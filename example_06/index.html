<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Machine Learning Part-04 - 茶桁.MAMT</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="茶桁.MAMT"><meta name="msapplication-TileImage" content="https://qiniu.hivan.me/picGo/20230601174411.png?imgNote"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="茶桁.MAMT"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="SVM"><meta property="og:type" content="blog"><meta property="og:title" content="Machine Learning Part-04"><meta property="og:url" content="https://hivan.me/example_06/"><meta property="og:site_name" content="茶桁.MAMT"><meta property="og:description" content="SVM"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://qiniu.hivan.me/MAMTimage-20210901170815036.png?img"><meta property="og:image" content="https://qiniu.hivan.me/MAMTimage-20210901170902373.png?img"><meta property="og:image" content="https://qiniu.hivan.me/MAMTimage-20210901170921575.png?img"><meta property="og:image" content="https://qiniu.hivan.me/MAMTimage-20210901171726065.png?img"><meta property="article:published_time" content="2021-09-02T12:59:46.686Z"><meta property="article:modified_time" content="2023-06-02T04:10:57.185Z"><meta property="article:author" content="Hivan Du"><meta property="article:tag" content="AI,人工智能,代码,大语言模型"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://qiniu.hivan.me/MAMTimage-20210901170815036.png?img"><meta property="twitter:creator" content="@hivan"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hivan.me/example_06/"},"headline":"Machine Learning Part-04","image":[],"datePublished":"2021-09-02T12:59:46.686Z","dateModified":"2023-06-02T04:10:57.185Z","author":{"@type":"Person","name":"Hivan Du"},"publisher":{"@type":"Organization","name":"茶桁.MAMT","logo":{"@type":"ImageObject","url":"https://hivan.me/img/logo.svg"}},"description":"SVM"}</script><link rel="canonical" href="https://hivan.me/example_06/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="茶桁.MAMT" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-09-02T12:59:46.686Z" title="9/2/2021, 8:59:46 PM">2021-09-02</time>发表</span></div></div><h1 class="title is-3 is-size-4-mobile">Machine Learning Part-04</h1><div class="content"><h2 id="svm">SVM</h2>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>label_a = np.random.normal(<span class="hljs-number">6</span>, <span class="hljs-number">2</span>, size = (<span class="hljs-number">50</span>,<span class="hljs-number">2</span>))<br>label_b = np.random.normal(-<span class="hljs-number">6</span>, <span class="hljs-number">2</span>, size = (<span class="hljs-number">50</span>,<span class="hljs-number">2</span>))<br><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>a = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br>b = [-<span class="hljs-number">1</span>,-<span class="hljs-number">2</span>, -<span class="hljs-number">3</span>]<br><br>plt.scatter(*<span class="hljs-built_in">zip</span>(*label_a))<br>plt.scatter(*<span class="hljs-built_in">zip</span>(*label_b))<br>plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/MAMTimage-20210901170815036.png?img"
alt="image-20210901170815036" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python">label_a_x = label_a[:, <span class="hljs-number">0</span>]<br>label_b_x = label_b[:, <span class="hljs-number">0</span>]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x, k, b</span>):<br>    <span class="hljs-keyword">return</span> k*x -b<br>  <br>k_and_b = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    k, b = (np.random.random(size = (<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)) * <span class="hljs-number">10</span> - <span class="hljs-number">5</span>)[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-keyword">if</span> np.<span class="hljs-built_in">max</span>(f(label_a_x, k, b)) &lt;= -<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> np.<span class="hljs-built_in">min</span>(f(label_b_x, k, b)) &gt;= <span class="hljs-number">1</span>:<br>        <span class="hljs-built_in">print</span>(k, b)<br>        k_and_b.append((k, b))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">-3.4732670434285517 -2.3248316389039325</span><br><span class="hljs-string">-3.654276254462583 0.01110189858052646</span><br><span class="hljs-string">-2.4609031871010014 -0.3932180655739925</span><br><span class="hljs-string">-2.9206497777762843 0.2595456609552631</span><br><span class="hljs-string">-4.07589152330003 -0.6463313059119606</span><br><span class="hljs-string">-3.1950366475236835 -1.8558958669742989</span><br><span class="hljs-string">-4.316670785852706 -3.1033030808371653</span><br><span class="hljs-string">-4.124339773909792 -1.5741734685470687</span><br><span class="hljs-string">-4.20817621470405 0.4368323022696625</span><br><span class="hljs-string">-3.7098120657624003 -0.38196175566618784</span><br><span class="hljs-string">-3.2053446683533315 0.12822700803583054</span><br><span class="hljs-string">-4.534694169094692 1.143734501297419</span><br><span class="hljs-string">-4.8124714209376425 0.8707258703100704</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>plt.scatter(*<span class="hljs-built_in">zip</span>(*label_a))<br>plt.scatter(*<span class="hljs-built_in">zip</span>(*label_b))<br><br><span class="hljs-keyword">for</span> k, b <span class="hljs-keyword">in</span> k_and_b:<br>    x = np.concatenate((label_a_x, label_b_x))<br>    plt.plot(x, f(x, k, b))<br><br>plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/MAMTimage-20210901170902373.png?img"
alt="image-20210901170902373" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(*<span class="hljs-built_in">zip</span>(*label_a))<br>plt.scatter(*<span class="hljs-built_in">zip</span>(*label_b))<br><br>k,b = <span class="hljs-built_in">sorted</span>(k_and_b, key = <span class="hljs-keyword">lambda</span> t: <span class="hljs-built_in">abs</span>(t[<span class="hljs-number">0</span>]))[<span class="hljs-number">0</span>]<br>x = np.concatenate((label_a_x, label_b_x))<br>plt.plot(x, f(x, k, b))<br>plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/MAMTimage-20210901170921575.png?img"
alt="image-20210901170921575" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_boston<br><br>datasets = load_boston()<br>data, target = datasets[<span class="hljs-string">&#x27;data&#x27;</span>], datasets[<span class="hljs-string">&#x27;target&#x27;</span>]<br><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>df = pd.DataFrame(data)<br>df.columns = datasets[<span class="hljs-string">&#x27;feature_names&#x27;</span>]<br><br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_select</span>(<span class="hljs-params">df, drop_num = <span class="hljs-number">4</span></span>):<br>    columns = random.sample(<span class="hljs-built_in">list</span>(df.columns), k = <span class="hljs-built_in">len</span>(df.columns) - drop_num)<br><br>    <span class="hljs-keyword">return</span> df[columns]<br>  <br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeRegressor<br><br>sample_x = random_select(df)<br>regressioner = DecisionTreeRegressor()<br>(X_train, X_test, y_train, y_test)  = train_test_split(sample_x, target, test_size = <span class="hljs-number">0.3</span>)<br>regressioner.fit(X_train, y_train)<br>regressioner.score(X_train, y_train)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">1.0</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>regressioner.score(X_test, y_test)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">0.8110635350395325</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_tree</span>(<span class="hljs-params">train_X, train_y, test_X, test_y, drop_n = <span class="hljs-number">4</span></span>):<br>    train_sample = random_select(train_X, drop_num = drop_n)<br><br>    regressioner = DecisionTreeRegressor()<br>    regressioner.fit(train_sample, train_y)<br><br>    train_score = regressioner.score(train_sample, train_y)<br>    test_score = regressioner.score(test_X[train_sample.columns], test_y)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;train score = &#123;&#125;; test score = &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(train_score, test_score))<br><br>    y_predicat = regressioner.predict(test_X[train_sample.columns])<br><br>    <span class="hljs-keyword">return</span> y_predicat<br>  <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_forest</span>(<span class="hljs-params">train_X, train_y, test_X, test_y, tree_n = <span class="hljs-number">4</span></span>):<br>    predicat = np.array([random_tree(train_X, train_y, test_X, test_y) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tree_n)])<br><br>    <span class="hljs-keyword">return</span> np.mean(predicat, axis = <span class="hljs-number">0</span>)<br>  <br>(X_train, X_test, y_train, y_test)  = train_test_split(df, target, test_size = <span class="hljs-number">0.3</span>)<br><br>forest_predict = random_forest(X_train, y_train, X_test, y_test)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">train score = 1.0; test score = 0.5367061884031776</span><br><span class="hljs-string">train score = 1.0; test score = 0.4983695562874999</span><br><span class="hljs-string">train score = 1.0; test score = 0.6715869370883646</span><br><span class="hljs-string">train score = 1.0; test score = 0.6210922529610217</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>forest_predict<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">array([10.925, 21.1  , 30.625, 28.025, 22.525, 17.65 , 20.6  , 17.325,</span><br><span class="hljs-string">       29.175, 14.95 , 40.775, 19.55 , 12.175, 23.675, 10.775, 22.1  ,</span><br><span class="hljs-string">       ...</span><br><span class="hljs-string">       15.575, 20.5  , 22.775, 30.725, 18.975, 16.45 , 22.05 , 18.925])</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> r2_score<br>r2_score(y_test, forest_predict)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">0.7840500839091215</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h2 id="entropy-熵">Entropy: 熵</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><span class="hljs-keyword">from</span> icecream <span class="hljs-keyword">import</span> ic<br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> lru_cache<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pr</span>(<span class="hljs-params">es</span>):<br>    counter = Counter(es)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_wrap</span>(<span class="hljs-params">e</span>):<br>        <span class="hljs-keyword">return</span> counter[e] / <span class="hljs-built_in">len</span>(es)<br>    <span class="hljs-keyword">return</span> _wrap<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">entropy</span>(<span class="hljs-params">elements</span>):<br>    <span class="hljs-comment"># Information Entropy</span><br>    p = pr(elements)<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(p(e) * np.log(p(e)) <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">set</span>(elements))<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gini</span>(<span class="hljs-params">elements</span>):<br>    p = pr(elements)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>-np.<span class="hljs-built_in">sum</span>(p(e) ** <span class="hljs-number">2</span> <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">set</span>(elements))<br>  <br>pure_func = gini<br><br>ic(pure_func([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]))<br>ic(pure_func([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]))<br>ic(pure_func([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">8</span>]))<br>ic(pure_func([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">9</span>]))<br>ic(pure_func([<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>]))<br>ic(pure_func([<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>]))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">ic| pure_func([1, 1, 1, 1, 1, 0]): 0.2777777777777777</span><br><span class="hljs-string">ic| pure_func([1, 1, 1, 1, 1, 1]): 0.0</span><br><span class="hljs-string">ic| pure_func([1, 2, 3, 4, 5, 8]): 0.8333333333333333</span><br><span class="hljs-string">ic| pure_func([1, 2, 3, 4, 5, 9]): 0.8333333333333333</span><br><span class="hljs-string">ic| pure_func([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;]): 0.44897959183673464</span><br><span class="hljs-string">ic| pure_func([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;d&#x27;]): 0.6122448979591837</span><br><span class="hljs-string">0.6122448979591837</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h2 id="random-forest">Random forest</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_boston<br><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeRegressor, DecisionTreeClassifier<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> r2_score<br><br><br>house = load_boston()<br>X = house.data<br>y = house.target<br><br>x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)<br><br>tree_reg = DecisionTreeRegressor()<br>tree_reg.fit(x_train, y_train)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;whole dataset train acc: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(tree_reg.score(x_train, y_train)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;whole dataset test acc: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(tree_reg.score(x_test, y_test)))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">whole dataset train acc: 1.0</span><br><span class="hljs-string">whole dataset test acc: 0.6776520888466615</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_forest</span>(<span class="hljs-params">train_x, train_y, test_x, test_y, drop_n=<span class="hljs-number">4</span></span>):<br>    random_features = np.random.choice(<span class="hljs-built_in">list</span>(train_x.columns), size=<span class="hljs-built_in">len</span>(train_x.columns)-drop_n)<br><br>    sample_x = train_x[random_features]<br>    sample_y = train_y<br><br>    reg = DecisionTreeRegressor()<br>    reg.fit(sample_x, sample_y)<br><br>    train_score = reg.score(sample_x, sample_y)<br>    test_score = reg.score(test_x[random_features], test_y)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;sub sample :: train score: &#123;&#125;, test score: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(train_score, test_score))<br><br>    y_predicated = reg.predict(test_x[random_features])<br><br>    <span class="hljs-keyword">return</span> y_predicated, test_score<br>  <br>with_feature_names = pd.DataFrame(X)<br>with_feature_names.columns = house[<span class="hljs-string">&#x27;feature_names&#x27;</span>]<br><br>x_train, x_test, y_train, y_test = train_test_split(with_feature_names, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)<br><br>tree_num = <span class="hljs-number">4</span><br>predicates = []<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tree_num):<br>    predicated, score = random_forest(x_train, y_train, x_test, y_test)<br>    predicates.append((predicated, score))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">sub sample :: train score: 1.0, test score: 0.5640870175410873</span><br><span class="hljs-string">sub sample :: train score: 1.0, test score: 0.29024437819534354</span><br><span class="hljs-string">sub sample :: train score: 1.0, test score: 0.37812117132843814</span><br><span class="hljs-string">sub sample :: train score: 1.0, test score: 0.5650888856735524</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>predicates_value = [v <span class="hljs-keyword">for</span> v, s <span class="hljs-keyword">in</span> predicates]<br>forest_scores = [s <span class="hljs-keyword">for</span> v, s <span class="hljs-keyword">in</span> predicates]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the score of forest is : &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(r2_score(y_test, np.mean(predicates_value, axis=<span class="hljs-number">0</span>))))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">the score of forest is : 0.680193104551715</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>weights = np.array(forest_scores) / np.<span class="hljs-built_in">sum</span>(forest_scores)<br><br>weights_score = np.zeros_like(np.mean(predicates_value, axis=<span class="hljs-number">0</span>))<br><span class="hljs-keyword">for</span> i, v <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(predicates_value):<br>    weights_score += v * weights[i]<br>    <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the score of weighted forest is : &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(r2_score(y_test, weights_score)))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">the score of weighted forest is : 0.6956613076019385</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><br></code></pre></td></tr></table></figure>
<h2 id="show-svm">Show SVM</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>label_a = np.random.normal(<span class="hljs-number">6</span>, <span class="hljs-number">2</span>, size=(<span class="hljs-number">50</span>, <span class="hljs-number">2</span>))<br>label_b = np.random.normal(-<span class="hljs-number">6</span>, <span class="hljs-number">2</span>, size=(<span class="hljs-number">50</span>, <span class="hljs-number">2</span>))<br><br>plt.scatter(*<span class="hljs-built_in">zip</span>(*label_a))<br>plt.scatter(*<span class="hljs-built_in">zip</span>(*label_b))<br><br>label_a_x = label_a[:, <span class="hljs-number">0</span>]<br>label_b_x = label_b[:, <span class="hljs-number">0</span>]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-keyword">return</span> w * x + b<br>  <br>k_and_b = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    k, b = (np.random.random(size=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)) * <span class="hljs-number">10</span> - <span class="hljs-number">5</span>)[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-keyword">if</span> np.<span class="hljs-built_in">max</span>(f(label_a_x, k, b)) &gt;= -<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> np.<span class="hljs-built_in">min</span>(f(label_b_x, k, b)) &gt;= <span class="hljs-number">1</span>:<br>        <span class="hljs-built_in">print</span>(k, b)<br>        k_and_b.append((k, b))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">0.17732109082579406 3.9508645615428843</span><br><span class="hljs-string">-0.8649868307954458 1.7349996177756957</span><br><span class="hljs-string">...</span><br><span class="hljs-string">-2.2969567032985783 2.171321001904926</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">for</span> k, b <span class="hljs-keyword">in</span> k_and_b:<br>    x = np.concatenate((label_a_x, label_b_x))<br>    plt.plot(x, f(x, k, b))<br>    <br><span class="hljs-built_in">print</span>(k_and_b)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">[(0.17732109082579406, 3.9508645615428843), (-0.8649868307954458, 1.7349996177756957), (-0.818317924604357, 0.352843348193578), (-0.19730603224472976, 4.002168852007262), </span><br><span class="hljs-string">...</span><br><span class="hljs-string">(-2.2969567032985783, 2.171321001904926)]</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>w, b = <span class="hljs-built_in">min</span>(k_and_b, key=<span class="hljs-keyword">lambda</span> k_b: k_b[<span class="hljs-number">0</span>])<br><br>all_x = np.concatenate((label_a_x, label_b_x))<br>plt.plot(all_x, f(all_x, w, b), <span class="hljs-string">&#x27;r-o&#x27;</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/MAMTimage-20210901171726065.png?img"
alt="image-20210901171726065" /></p>
<h2 id="integrated-learning">Integrated learning</h2>
<p>Ensemble learning is a machine learning paradigm that solves the same
problem by training multiple models. In contrast to ordinary machine
learning methods that try to learn a hypothesis from training data,
ensemble methods try to construct a set of hypotheses and use them in
combination. Next, we will use the decision tree and its integrated
version to model the classic data set Mnist and observe the differences
in different integration methods.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">!ls<br>!unzip mnist_test.csv.<span class="hljs-built_in">zip</span> &amp;&amp; unzip mnist_train.csv.<span class="hljs-built_in">zip</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier<br></code></pre></td></tr></table></figure>
<h4 id="build-a-data-set">Build a data set</h4>
<p>The Mnist data set used this time is not in the original format. In
order to more easily adapt to this training, the 28 * 28 pictures in the
original data set are <a
target="_blank" rel="noopener" href="https://numpy.org/doc/stable/reference%20/generated/numpy.ndarray.flatten.html">flatten</a>
operation, it becomes 784 features, the columns in the DataFrame below:
1x1, 1x2, ..., 28x28, representing the <em>i</em> row and <em>j</em>
column in the picture The pixel value of is a grayscale image, so the
pixel value is only 0 and 1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">train_df = df = pd.read_csv(<span class="hljs-string">&#x27;~/data/mnist_train.csv&#x27;</span>)<br>train_df.head()<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">	label	1x1	1x2	1x3	1x4	1x5	1x6	1x7	1x8	1x9	...	28x19	28x20	28x21	28x22	28x23	28x24	28x25	28x26	28x27	28x28</span><br><span class="hljs-string">0	5	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span><br><span class="hljs-string">1	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span><br><span class="hljs-string">2	4	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span><br><span class="hljs-string">3	1	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span><br><span class="hljs-string">4	9	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span><br><span class="hljs-string">5 rows × 785 columns</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>View training data information:, whether there is NaN, how many
pieces of data are there...</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python">train_df.info()<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span><br><span class="hljs-string">RangeIndex: 60000 entries, 0 to 59999</span><br><span class="hljs-string">Columns: 785 entries, label to 28x28</span><br><span class="hljs-string">dtypes: int64(785)</span><br><span class="hljs-string">memory usage: 359.3 MB</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>test_df = df = pd.read_csv(<span class="hljs-string">&#x27;~/data/mnist_test.csv&#x27;</span>)<br>test_df.head()<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">	label	1x1	1x2	1x3	1x4	1x5	1x6	1x7	1x8	1x9	...	28x19	28x20	28x21	28x22	28x23	28x24	28x25	28x26	28x27	28x28</span><br><span class="hljs-string">0	7	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span><br><span class="hljs-string">1	2	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span><br><span class="hljs-string">2	1	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span><br><span class="hljs-string">3	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span><br><span class="hljs-string">4	4	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span><br><span class="hljs-string">5 rows × 785 columns</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>test_df.info()<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span><br><span class="hljs-string">RangeIndex: 10000 entries, 0 to 9999</span><br><span class="hljs-string">Columns: 785 entries, label to 28x28</span><br><span class="hljs-string">dtypes: int64(785)</span><br><span class="hljs-string">memory usage: 59.9 MB</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>Build training and test data</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">X_train = train_df.iloc[:, <span class="hljs-number">1</span>:]<br>y_train = train_df.iloc[:, <span class="hljs-number">0</span>]<br><br>X_test = test_df.iloc[:, <span class="hljs-number">1</span>:]<br>y_test = test_df.iloc[:, <span class="hljs-number">0</span>]<br><br>(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">(((60000, 784), (60000,)), ((10000, 784), (10000,)))</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h4 id="decision-tree">Decision Tree</h4>
<p>First train a simple decision tree to see how it performs</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">dtc = DecisionTreeClassifier()<br>dtc.fit(X_train, y_train)<br><br>dtc.score(X_train, y_train)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">1.0</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>dtc.score(X_test, y_test)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">0.8753</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>dtc = DecisionTreeClassifier(min_samples_leaf=<span class="hljs-number">8</span>)<br>dtc.fit(X_train, y_train)<br><br>dtc.score(X_train, y_train), dtc.score(X_test, y_test)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">(0.9311666666666667, 0.8795)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>From the above results, we can see that by adjusting the parameter
<code>min_samples_leaf</code>, the overfitting situation has been
alleviated. What does this parameter mean? Why increasing it can
alleviate the overfitting problem? The meaning of
<code>min_samples_leaf</code> is the minimum number of samples contained
in the leaf nodes of the decision tree. By increasing this parameter,
the decision tree can not capture any of the subtle features of the
training data during training, resulting in excessive training data.
Fitting: The large number of samples of leaf nodes can also play a role
in voting and enhance the generalization performance of the model. You
can try to continue to increase the value of this parameter and try to
find the best parameter. In addition to this parameter, you can also try
to adjust the parameters such as <code>min_samples_split</code> and
<code>max_features</code>. For the specific meaning, please refer to <a
target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">sklearn
documentation</a></p>
<h3 id="second-question"><strong>Second question: </strong></h3>
<p><strong>Try to adjust other parameters to see the performance of the
decision tree on the test set</strong></p>
<h4 id="random-forest-1">Random Forest</h4>
<p>Take a look at the bagging version of the decision tree and how the
random forest performs!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">rfc = RandomForestClassifier(n_estimators = <span class="hljs-number">10</span>)<br>rfc.fit(X_train, y_train)<br><br>rfc.score(X_train, y_train), rfc.score(X_test, y_test)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">(0.99905, 0.9513)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>It is worthy of the integrated version. It basically achieves better
performance under the default parameters. The accuracy of the test set
is about 7% higher than that of the ordinary decision tree. However,
comparing the training and test results, it can be found that there is
still a certain degree of overfitting. , Try to adjust some parameters
below</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">rfc = RandomForestClassifier(n_estimators = <span class="hljs-number">20</span>)<br>rfc.fit(X_train, y_train)<br><br>rfc.score(X_train, y_train), rfc.score(X_test, y_test)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">(0.9999, 0.96)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>After increasing the parameter <code>n_estimators</code>, the
accuracy of the test set has increased by about 1%. The meaning of this
parameter is to train 20 decision trees at the same time, and finally
integrate the results. The increase of this parameter can be simply
regarded as voting The number of people increases, so the final result
will inevitably be more robust. You can try to continue to increase this
parameter, or adjust other parameters such as <code>max_samples</code>,
appropriately less than the total amount of training data, which can
increase the difference between different sub-models and further improve
the generalization performance. It can also adjust the parameters of the
base learner (decision tree). For the meaning of the parameters, see <a
target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">sklearn
documentation</a></p>
<h4 id="gbdt">GBDT</h4>
<p>Let's compare the performance of the boosting version of the decision
tree GBDT!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">gbc = GradientBoostingClassifier(n_estimators=<span class="hljs-number">10</span>)<br>gbc.fit(X_train, y_train)<br><br>gbc.score(X_train, y_train), gbc.score(X_test, y_test)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">(0.8423, 0.846)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>As expected, the performance has been greatly improved, and the
indicators of the training set are basically the same as those of the
test set, and there is no overfitting, so it should be possible to
continue to try to improve this parameter. Generally, in the absence of
overfitting, we only need to consider continuing to increase the
complexity of the model. This is the fastest way to improve performance.
When the complexity of the model increases to the point of over-fitting,
we then consider using some methods to reduce over-fitting.</p>
<h4 id="bagging">Bagging</h4>
<p>The aforementioned random forest and GBDT are ensemble learning
algorithms based on decision trees, but it should be noted that ensemble
learning is not exclusive to decision trees. Any other learner can be
used as a base learner for ensemble learning, such as Logistic
regression, support vector machine.</p>
<p>Bagging is short for "bootstrap aggregating". This is a
meta-algorithm, which takes M sub-samples (with replacement) from the
initial data set, and trains the prediction model on these sub-samples.
The final model is obtained by averaging all sub-models, which usually
produces better results. The main advantage of this technique is that it
combines regularization, all you need to do is choose good parameters
for the base learner.</p>
<p>The following uses the general api provided by sklearn to construct
an integrated learning algorithm</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Still use decision tree as base learner</span><br>bgc = BaggingClassifier(DecisionTreeClassifier(), max_samples=<span class="hljs-number">0.5</span>, max_features=<span class="hljs-number">1.0</span>, n_estimators=<span class="hljs-number">20</span>)<br>bgc.fit(X_train, y_train)<br><br>bgc.score(X_train, y_train), bgc.score(X_test, y_test)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">(0.9935166666666667, 0.9506)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="third-question">Third question</h3>
<p><strong>Logistic regression as a base learner</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">bgc = BaggingClassifier(LogisticRegression(max_iter = <span class="hljs-number">500</span>), max_samples=<span class="hljs-number">0.5</span>, max_features=<span class="hljs-number">1.0</span>, n_estimators=<span class="hljs-number">20</span>)<br>bgc.fit(X_train, y_train)<br><br>bgc.score(X_train, y_train), bgc.score(X_test, y_test)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">(0.9421166666666667, 0.9228)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>Above we have successfully used logistic regression as the base
learner to complete integrated learning. You can try to use only
logistic regression for training, and compare the performance of the
single model with the bagging version of logistic regression.</p>
<h4 id="boosting">Boosting</h4>
<p>Boosting refers to a series of algorithms that can transform a weak
learner into a strong learner. The main principle of boosting is to
combine a series of weak learners (only better than random guessing).
For those samples that were misclassified in the early stages of
training, the boosting algorithm will give more attention. Then combine
the predictions by weighted majority voting (classification) or weighted
sum (regression) to produce the final prediction.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">abc = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=<span class="hljs-number">10</span>, learning_rate=<span class="hljs-number">0.01</span>)<br>abc.fit(X_train, y_train)<br>abc.score(X_train, y_train), abc.score(X_test, y_test)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">(1.0, 0.875)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>Comparing the boosting integrated version of decision tree and
logistic regression, we can find that logistic regression has better
generalization ability, and decision tree is easier to overfit</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">abc = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=<span class="hljs-number">8</span>), n_estimators=<span class="hljs-number">10</span>, learning_rate=<span class="hljs-number">0.01</span>)<br>abc.fit(X_train, y_train)<br>abc.score(X_train, y_train), abc.score(X_test, y_test)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">(0.9981833333333333, 0.9532)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>In fact, over-fitting is not a bad thing. If your model cannot be
over-fitted, it means that it cannot fit the training data well.
Therefore, the decision tree is very over-fitted at the beginning, which
also shows its potential. , You can see that after the above parameters
are adjusted, the boosting version of the decision tree easily exceeds
the boosting version of the logistic regression</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Machine Learning Part-04</p><p><a href="https://hivan.me/example_06/">https://hivan.me/example_06/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hivan Du</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-09-02</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2023-06-02</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=6479444288ae9600196fa98e&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="https://afdian.net/item/72907364008511ee904852540025c377" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://qiniu.hivan.me/picGo/20230601221633.jpeg" alt="支付宝"></span></a><a class="button donate" href="https://www.buymeacoffee.com/hivandu" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="https://patreon.com/user?u=89473430" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="doo@hivan.me"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://qiniu.hivan.me/IMG_4603.JPG" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/example_05/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Machine Learning Part-03</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/example_08/"><span class="level-item">RNN</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://hivan.me/example_06/';
            this.page.identifier = 'example_06/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'hivan' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://www.gravatar.com/avatar/bdff168cf8a71c11d2712a1679a00c54?s=128" alt="茶桁"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">茶桁</p><p class="is-size-6 is-block">AI游民</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shang Hai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">164</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">21</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hivandu" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hivan"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/hivan"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com/hivan"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4NzE4MDQzMg==&amp;action=getalbum&amp;album_id=2932504849574543360&amp;scene=173&amp;from_msgid=2648747980&amp;from_itemidx=1&amp;count=3&amp;nolastread=1&amp;token=1758883909&amp;lang=zh_CN#wechat_redirect"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.zhihu.com/column/c_1424326166602178560" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">塌缩的奇点</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li><li><a class="level is-mobile" href="https://www.zhihu.com/column/hivandu" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">茶桁-知乎</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI%E7%A7%98%E7%B1%8D/"><span class="level-start"><span class="level-item">AI秘籍</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI%E7%A7%98%E7%B1%8D/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="level-start"><span class="level-item">从零开始接触人工智能大模型</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-04T08:13:00.000Z">2023-08-04</time></p><p class="title"><a href="/python-Built-in-functions/">7. Python的内置函数</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Python/">Python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-03T06:07:17.000Z">2023-08-03</time></p><p class="title"><a href="/Higher-order-functions/">6. Python的高阶函数</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Python/">Python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-02T11:23:14.000Z">2023-08-02</time></p><p class="title"><a href="/2023_8_3_Kalman/">卡尔曼滤波器的非数学介绍</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-01T15:43:14.000Z">2023-08-01</time></p><p class="title"><a href="/Modular-programming/">5. 模块化编程</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Python/">Python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-01T08:15:43.000Z">2023-08-01</time></p><p class="title"><a href="/Python-process-control/">4. Python的流程控制</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Python/">Python</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a><p class="is-size-7"><span>&copy; 2023 Hivan Du</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>