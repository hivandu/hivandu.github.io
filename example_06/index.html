<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>茶桁.MAMT</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Machine Learning Part-04SVM">
<meta property="og:type" content="article">
<meta property="og:title" content="茶桁.MAMT">
<meta property="og:url" content="https://hivan.me/example_06/index.html">
<meta property="og:site_name" content="茶桁.MAMT">
<meta property="og:description" content="Machine Learning Part-04SVM">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://qiniu.hivan.me/MAMTimage-20210901170815036.png?img">
<meta property="og:image" content="http://qiniu.hivan.me/MAMTimage-20210901170902373.png?img">
<meta property="og:image" content="http://qiniu.hivan.me/MAMTimage-20210901170921575.png?img">
<meta property="og:image" content="http://qiniu.hivan.me/MAMTimage-20210901171726065.png?img">
<meta property="article:published_time" content="2021-09-02T12:59:46.686Z">
<meta property="article:modified_time" content="2023-06-01T07:59:31.063Z">
<meta property="article:author" content="Hivan Du">
<meta property="article:tag" content="AI,人工智能,代码,大语言模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://qiniu.hivan.me/MAMTimage-20210901170815036.png?img">
  
    <link rel="alternate" href="/atom.xml" title="茶桁.MAMT" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">茶桁.MAMT</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">ChaHeng Notes，codding and writting ~</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://hivan.me"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-example_06" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/example_06/" class="article-date">
  <time class="dt-published" datetime="2021-09-02T12:59:46.686Z" itemprop="datePublished">2021-09-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Machine-Learning-Part-04"><a href="#Machine-Learning-Part-04" class="headerlink" title="Machine Learning Part-04"></a>Machine Learning Part-04</h1><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><span id="more"></span>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">label_a = np.random.normal(<span class="number">6</span>, <span class="number">2</span>, size = (<span class="number">50</span>,<span class="number">2</span>))</span><br><span class="line">label_b = np.random.normal(-<span class="number">6</span>, <span class="number">2</span>, size = (<span class="number">50</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">b = [-<span class="number">1</span>,-<span class="number">2</span>, -<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_a))</span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_b))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.hivan.me/MAMTimage-20210901170815036.png?img" alt="image-20210901170815036"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">label_a_x = label_a[:, <span class="number">0</span>]</span><br><span class="line">label_b_x = label_b[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, k, b</span>):</span><br><span class="line">    <span class="keyword">return</span> k*x -b</span><br><span class="line">  </span><br><span class="line">k_and_b = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    k, b = (np.random.random(size = (<span class="number">1</span>,<span class="number">2</span>)) * <span class="number">10</span> - <span class="number">5</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">max</span>(f(label_a_x, k, b)) &lt;= -<span class="number">1</span> <span class="keyword">and</span> np.<span class="built_in">min</span>(f(label_b_x, k, b)) &gt;= <span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(k, b)</span><br><span class="line">        k_and_b.append((k, b))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">-3.4732670434285517 -2.3248316389039325</span></span><br><span class="line"><span class="string">-3.654276254462583 0.01110189858052646</span></span><br><span class="line"><span class="string">-2.4609031871010014 -0.3932180655739925</span></span><br><span class="line"><span class="string">-2.9206497777762843 0.2595456609552631</span></span><br><span class="line"><span class="string">-4.07589152330003 -0.6463313059119606</span></span><br><span class="line"><span class="string">-3.1950366475236835 -1.8558958669742989</span></span><br><span class="line"><span class="string">-4.316670785852706 -3.1033030808371653</span></span><br><span class="line"><span class="string">-4.124339773909792 -1.5741734685470687</span></span><br><span class="line"><span class="string">-4.20817621470405 0.4368323022696625</span></span><br><span class="line"><span class="string">-3.7098120657624003 -0.38196175566618784</span></span><br><span class="line"><span class="string">-3.2053446683533315 0.12822700803583054</span></span><br><span class="line"><span class="string">-4.534694169094692 1.143734501297419</span></span><br><span class="line"><span class="string">-4.8124714209376425 0.8707258703100704</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_a))</span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, b <span class="keyword">in</span> k_and_b:</span><br><span class="line">    x = np.concatenate((label_a_x, label_b_x))</span><br><span class="line">    plt.plot(x, f(x, k, b))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.hivan.me/MAMTimage-20210901170902373.png?img" alt="image-20210901170902373"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_a))</span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_b))</span><br><span class="line"></span><br><span class="line">k,b = <span class="built_in">sorted</span>(k_and_b, key = <span class="keyword">lambda</span> t: <span class="built_in">abs</span>(t[<span class="number">0</span>]))[<span class="number">0</span>]</span><br><span class="line">x = np.concatenate((label_a_x, label_b_x))</span><br><span class="line">plt.plot(x, f(x, k, b))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.hivan.me/MAMTimage-20210901170921575.png?img" alt="image-20210901170921575"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"></span><br><span class="line">datasets = load_boston()</span><br><span class="line">data, target = datasets[<span class="string">&#x27;data&#x27;</span>], datasets[<span class="string">&#x27;target&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line">df.columns = datasets[<span class="string">&#x27;feature_names&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_select</span>(<span class="params">df, drop_num = <span class="number">4</span></span>):</span><br><span class="line">    columns = random.sample(<span class="built_in">list</span>(df.columns), k = <span class="built_in">len</span>(df.columns) - drop_num)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> df[columns]</span><br><span class="line">  </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line">sample_x = random_select(df)</span><br><span class="line">regressioner = DecisionTreeRegressor()</span><br><span class="line">(X_train, X_test, y_train, y_test)  = train_test_split(sample_x, target, test_size = <span class="number">0.3</span>)</span><br><span class="line">regressioner.fit(X_train, y_train)</span><br><span class="line">regressioner.score(X_train, y_train)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1.0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">regressioner.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0.8110635350395325</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_tree</span>(<span class="params">train_X, train_y, test_X, test_y, drop_n = <span class="number">4</span></span>):</span><br><span class="line">    train_sample = random_select(train_X, drop_num = drop_n)</span><br><span class="line"></span><br><span class="line">    regressioner = DecisionTreeRegressor()</span><br><span class="line">    regressioner.fit(train_sample, train_y)</span><br><span class="line"></span><br><span class="line">    train_score = regressioner.score(train_sample, train_y)</span><br><span class="line">    test_score = regressioner.score(test_X[train_sample.columns], test_y)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;train score = &#123;&#125;; test score = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(train_score, test_score))</span><br><span class="line"></span><br><span class="line">    y_predicat = regressioner.predict(test_X[train_sample.columns])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_predicat</span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_forest</span>(<span class="params">train_X, train_y, test_X, test_y, tree_n = <span class="number">4</span></span>):</span><br><span class="line">    predicat = np.array([random_tree(train_X, train_y, test_X, test_y) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(tree_n)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.mean(predicat, axis = <span class="number">0</span>)</span><br><span class="line">  </span><br><span class="line">(X_train, X_test, y_train, y_test)  = train_test_split(df, target, test_size = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">forest_predict = random_forest(X_train, y_train, X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">train score = 1.0; test score = 0.5367061884031776</span></span><br><span class="line"><span class="string">train score = 1.0; test score = 0.4983695562874999</span></span><br><span class="line"><span class="string">train score = 1.0; test score = 0.6715869370883646</span></span><br><span class="line"><span class="string">train score = 1.0; test score = 0.6210922529610217</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">forest_predict</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">array([10.925, 21.1  , 30.625, 28.025, 22.525, 17.65 , 20.6  , 17.325,</span></span><br><span class="line"><span class="string">       29.175, 14.95 , 40.775, 19.55 , 12.175, 23.675, 10.775, 22.1  ,</span></span><br><span class="line"><span class="string">       ...</span></span><br><span class="line"><span class="string">       15.575, 20.5  , 22.775, 30.725, 18.975, 16.45 , 22.05 , 18.925])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">r2_score(y_test, forest_predict)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0.7840500839091215</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h2 id="Entropy-熵"><a href="#Entropy-熵" class="headerlink" title="Entropy: 熵"></a>Entropy: 熵</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> icecream <span class="keyword">import</span> ic</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pr</span>(<span class="params">es</span>):</span><br><span class="line">    counter = Counter(es)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_wrap</span>(<span class="params">e</span>):</span><br><span class="line">        <span class="keyword">return</span> counter[e] / <span class="built_in">len</span>(es)</span><br><span class="line">    <span class="keyword">return</span> _wrap</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">entropy</span>(<span class="params">elements</span>):</span><br><span class="line">    <span class="comment"># Information Entropy</span></span><br><span class="line">    p = pr(elements)</span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(p(e) * np.log(p(e)) <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">set</span>(elements))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gini</span>(<span class="params">elements</span>):</span><br><span class="line">    p = pr(elements)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>-np.<span class="built_in">sum</span>(p(e) ** <span class="number">2</span> <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">set</span>(elements))</span><br><span class="line">  </span><br><span class="line">pure_func = gini</span><br><span class="line"></span><br><span class="line">ic(pure_func([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]))</span><br><span class="line">ic(pure_func([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">ic(pure_func([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">8</span>]))</span><br><span class="line">ic(pure_func([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">9</span>]))</span><br><span class="line">ic(pure_func([<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]))</span><br><span class="line">ic(pure_func([<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>]))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">ic| pure_func([1, 1, 1, 1, 1, 0]): 0.2777777777777777</span></span><br><span class="line"><span class="string">ic| pure_func([1, 1, 1, 1, 1, 1]): 0.0</span></span><br><span class="line"><span class="string">ic| pure_func([1, 2, 3, 4, 5, 8]): 0.8333333333333333</span></span><br><span class="line"><span class="string">ic| pure_func([1, 2, 3, 4, 5, 9]): 0.8333333333333333</span></span><br><span class="line"><span class="string">ic| pure_func([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;]): 0.44897959183673464</span></span><br><span class="line"><span class="string">ic| pure_func([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;d&#x27;]): 0.6122448979591837</span></span><br><span class="line"><span class="string">0.6122448979591837</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h2 id="Random-forest"><a href="#Random-forest" class="headerlink" title="Random forest"></a>Random forest</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor, DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">house = load_boston()</span><br><span class="line">X = house.data</span><br><span class="line">y = house.target</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">tree_reg = DecisionTreeRegressor()</span><br><span class="line">tree_reg.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;whole dataset train acc: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(tree_reg.score(x_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;whole dataset test acc: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(tree_reg.score(x_test, y_test)))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">whole dataset train acc: 1.0</span></span><br><span class="line"><span class="string">whole dataset test acc: 0.6776520888466615</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_forest</span>(<span class="params">train_x, train_y, test_x, test_y, drop_n=<span class="number">4</span></span>):</span><br><span class="line">    random_features = np.random.choice(<span class="built_in">list</span>(train_x.columns), size=<span class="built_in">len</span>(train_x.columns)-drop_n)</span><br><span class="line"></span><br><span class="line">    sample_x = train_x[random_features]</span><br><span class="line">    sample_y = train_y</span><br><span class="line"></span><br><span class="line">    reg = DecisionTreeRegressor()</span><br><span class="line">    reg.fit(sample_x, sample_y)</span><br><span class="line"></span><br><span class="line">    train_score = reg.score(sample_x, sample_y)</span><br><span class="line">    test_score = reg.score(test_x[random_features], test_y)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;sub sample :: train score: &#123;&#125;, test score: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(train_score, test_score))</span><br><span class="line"></span><br><span class="line">    y_predicated = reg.predict(test_x[random_features])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_predicated, test_score</span><br><span class="line">  </span><br><span class="line">with_feature_names = pd.DataFrame(X)</span><br><span class="line">with_feature_names.columns = house[<span class="string">&#x27;feature_names&#x27;</span>]</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(with_feature_names, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">tree_num = <span class="number">4</span></span><br><span class="line">predicates = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(tree_num):</span><br><span class="line">    predicated, score = random_forest(x_train, y_train, x_test, y_test)</span><br><span class="line">    predicates.append((predicated, score))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">sub sample :: train score: 1.0, test score: 0.5640870175410873</span></span><br><span class="line"><span class="string">sub sample :: train score: 1.0, test score: 0.29024437819534354</span></span><br><span class="line"><span class="string">sub sample :: train score: 1.0, test score: 0.37812117132843814</span></span><br><span class="line"><span class="string">sub sample :: train score: 1.0, test score: 0.5650888856735524</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">predicates_value = [v <span class="keyword">for</span> v, s <span class="keyword">in</span> predicates]</span><br><span class="line">forest_scores = [s <span class="keyword">for</span> v, s <span class="keyword">in</span> predicates]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;the score of forest is : &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(r2_score(y_test, np.mean(predicates_value, axis=<span class="number">0</span>))))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">the score of forest is : 0.680193104551715</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">weights = np.array(forest_scores) / np.<span class="built_in">sum</span>(forest_scores)</span><br><span class="line"></span><br><span class="line">weights_score = np.zeros_like(np.mean(predicates_value, axis=<span class="number">0</span>))</span><br><span class="line"><span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicates_value):</span><br><span class="line">    weights_score += v * weights[i]</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;the score of weighted forest is : &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(r2_score(y_test, weights_score)))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">the score of weighted forest is : 0.6956613076019385</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="Show-SVM"><a href="#Show-SVM" class="headerlink" title="Show SVM"></a>Show SVM</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">label_a = np.random.normal(<span class="number">6</span>, <span class="number">2</span>, size=(<span class="number">50</span>, <span class="number">2</span>))</span><br><span class="line">label_b = np.random.normal(-<span class="number">6</span>, <span class="number">2</span>, size=(<span class="number">50</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_a))</span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_b))</span><br><span class="line"></span><br><span class="line">label_a_x = label_a[:, <span class="number">0</span>]</span><br><span class="line">label_b_x = label_b[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, w, b</span>):</span><br><span class="line">    <span class="keyword">return</span> w * x + b</span><br><span class="line">  </span><br><span class="line">k_and_b = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    k, b = (np.random.random(size=(<span class="number">1</span>, <span class="number">2</span>)) * <span class="number">10</span> - <span class="number">5</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">max</span>(f(label_a_x, k, b)) &gt;= -<span class="number">1</span> <span class="keyword">and</span> np.<span class="built_in">min</span>(f(label_b_x, k, b)) &gt;= <span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(k, b)</span><br><span class="line">        k_and_b.append((k, b))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0.17732109082579406 3.9508645615428843</span></span><br><span class="line"><span class="string">-0.8649868307954458 1.7349996177756957</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">-2.2969567032985783 2.171321001904926</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, b <span class="keyword">in</span> k_and_b:</span><br><span class="line">    x = np.concatenate((label_a_x, label_b_x))</span><br><span class="line">    plt.plot(x, f(x, k, b))</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(k_and_b)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[(0.17732109082579406, 3.9508645615428843), (-0.8649868307954458, 1.7349996177756957), (-0.818317924604357, 0.352843348193578), (-0.19730603224472976, 4.002168852007262), </span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">(-2.2969567032985783, 2.171321001904926)]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">w, b = <span class="built_in">min</span>(k_and_b, key=<span class="keyword">lambda</span> k_b: k_b[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">all_x = np.concatenate((label_a_x, label_b_x))</span><br><span class="line">plt.plot(all_x, f(all_x, w, b), <span class="string">&#x27;r-o&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.hivan.me/MAMTimage-20210901171726065.png?img" alt="image-20210901171726065"></p>
<h2 id="Integrated-learning"><a href="#Integrated-learning" class="headerlink" title="Integrated learning"></a>Integrated learning</h2><p>Ensemble learning is a machine learning paradigm that solves the same problem by training multiple models. In contrast to ordinary machine learning methods that try to learn a hypothesis from training data, ensemble methods try to construct a set of hypotheses and use them in combination. Next, we will use the decision tree and its integrated version to model the classic data set Mnist and observe the differences in different integration methods.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">!ls</span><br><span class="line">!unzip mnist_test.csv.<span class="built_in">zip</span> &amp;&amp; unzip mnist_train.csv.<span class="built_in">zip</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier</span><br></pre></td></tr></table></figure>

<h4 id="Build-a-data-set"><a href="#Build-a-data-set" class="headerlink" title="Build a data set"></a>Build a data set</h4><p>The Mnist data set used this time is not in the original format. In order to more easily adapt to this training, the 28 * 28 pictures in the original data set are [flatten](<a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/reference">https://numpy.org/doc/stable/reference</a> &#x2F;generated&#x2F;numpy.ndarray.flatten.html) operation, it becomes 784 features, the columns in the DataFrame below: 1x1, 1x2, …, 28x28, representing the <em>i</em> row and <em>j</em> column in the picture The pixel value of is a grayscale image, so the pixel value is only 0 and 1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">train_df = df = pd.read_csv(<span class="string">&#x27;~/data/mnist_train.csv&#x27;</span>)</span><br><span class="line">train_df.head()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	label	1x1	1x2	1x3	1x4	1x5	1x6	1x7	1x8	1x9	...	28x19	28x20	28x21	28x22	28x23	28x24	28x25	28x26	28x27	28x28</span></span><br><span class="line"><span class="string">0	5	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">1	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">2	4	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">3	1	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">4	9	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">5 rows × 785 columns</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>View training data information:, whether there is NaN, how many pieces of data are there…</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">train_df.info()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span></span><br><span class="line"><span class="string">RangeIndex: 60000 entries, 0 to 59999</span></span><br><span class="line"><span class="string">Columns: 785 entries, label to 28x28</span></span><br><span class="line"><span class="string">dtypes: int64(785)</span></span><br><span class="line"><span class="string">memory usage: 359.3 MB</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">test_df = df = pd.read_csv(<span class="string">&#x27;~/data/mnist_test.csv&#x27;</span>)</span><br><span class="line">test_df.head()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	label	1x1	1x2	1x3	1x4	1x5	1x6	1x7	1x8	1x9	...	28x19	28x20	28x21	28x22	28x23	28x24	28x25	28x26	28x27	28x28</span></span><br><span class="line"><span class="string">0	7	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">1	2	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">2	1	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">3	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">4	4	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">5 rows × 785 columns</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">test_df.info()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span></span><br><span class="line"><span class="string">RangeIndex: 10000 entries, 0 to 9999</span></span><br><span class="line"><span class="string">Columns: 785 entries, label to 28x28</span></span><br><span class="line"><span class="string">dtypes: int64(785)</span></span><br><span class="line"><span class="string">memory usage: 59.9 MB</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>Build training and test data</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_train = train_df.iloc[:, <span class="number">1</span>:]</span><br><span class="line">y_train = train_df.iloc[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">X_test = test_df.iloc[:, <span class="number">1</span>:]</span><br><span class="line">y_test = test_df.iloc[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(((60000, 784), (60000,)), ((10000, 784), (10000,)))</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h4><p>First train a simple decision tree to see how it performs</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">dtc = DecisionTreeClassifier()</span><br><span class="line">dtc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">dtc.score(X_train, y_train)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1.0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">dtc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0.8753</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">dtc = DecisionTreeClassifier(min_samples_leaf=<span class="number">8</span>)</span><br><span class="line">dtc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">dtc.score(X_train, y_train), dtc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.9311666666666667, 0.8795)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>From the above results, we can see that by adjusting the parameter <code>min_samples_leaf</code>, the overfitting situation has been alleviated. What does this parameter mean? Why increasing it can alleviate the overfitting problem? The meaning of <code>min_samples_leaf</code> is the minimum number of samples contained in the leaf nodes of the decision tree. By increasing this parameter, the decision tree can not capture any of the subtle features of the training data during training, resulting in excessive training data. Fitting: The large number of samples of leaf nodes can also play a role in voting and enhance the generalization performance of the model. You can try to continue to increase the value of this parameter and try to find the best parameter. In addition to this parameter, you can also try to adjust the parameters such as <code>min_samples_split</code> and <code>max_features</code>. For the specific meaning, please refer to <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">sklearn documentation</a></p>
<h3 id="Second-question"><a href="#Second-question" class="headerlink" title="**Second question: **"></a>**Second question: **</h3><p><strong>Try to adjust other parameters to see the performance of the decision tree on the test set</strong></p>
<h4 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h4><p>Take a look at the bagging version of the decision tree and how the random forest performs!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rfc = RandomForestClassifier(n_estimators = <span class="number">10</span>)</span><br><span class="line">rfc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">rfc.score(X_train, y_train), rfc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.99905, 0.9513)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>It is worthy of the integrated version. It basically achieves better performance under the default parameters. The accuracy of the test set is about 7% higher than that of the ordinary decision tree. However, comparing the training and test results, it can be found that there is still a certain degree of overfitting. , Try to adjust some parameters below</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rfc = RandomForestClassifier(n_estimators = <span class="number">20</span>)</span><br><span class="line">rfc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">rfc.score(X_train, y_train), rfc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.9999, 0.96)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>After increasing the parameter <code>n_estimators</code>, the accuracy of the test set has increased by about 1%. The meaning of this parameter is to train 20 decision trees at the same time, and finally integrate the results. The increase of this parameter can be simply regarded as voting The number of people increases, so the final result will inevitably be more robust. You can try to continue to increase this parameter, or adjust other parameters such as <code>max_samples</code>, appropriately less than the total amount of training data, which can increase the difference between different sub-models and further improve the generalization performance. It can also adjust the parameters of the base learner (decision tree). For the meaning of the parameters, see <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">sklearn documentation</a></p>
<h4 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h4><p>Let’s compare the performance of the boosting version of the decision tree GBDT!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">gbc = GradientBoostingClassifier(n_estimators=<span class="number">10</span>)</span><br><span class="line">gbc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">gbc.score(X_train, y_train), gbc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.8423, 0.846)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>As expected, the performance has been greatly improved, and the indicators of the training set are basically the same as those of the test set, and there is no overfitting, so it should be possible to continue to try to improve this parameter. Generally, in the absence of overfitting, we only need to consider continuing to increase the complexity of the model. This is the fastest way to improve performance. When the complexity of the model increases to the point of over-fitting, we then consider using some methods to reduce over-fitting.</p>
<h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p>The aforementioned random forest and GBDT are ensemble learning algorithms based on decision trees, but it should be noted that ensemble learning is not exclusive to decision trees. Any other learner can be used as a base learner for ensemble learning, such as Logistic regression, support vector machine.</p>
<p>Bagging is short for “bootstrap aggregating”. This is a meta-algorithm, which takes M sub-samples (with replacement) from the initial data set, and trains the prediction model on these sub-samples. The final model is obtained by averaging all sub-models, which usually produces better results. The main advantage of this technique is that it combines regularization, all you need to do is choose good parameters for the base learner.</p>
<p>The following uses the general api provided by sklearn to construct an integrated learning algorithm</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Still use decision tree as base learner</span></span><br><span class="line">bgc = BaggingClassifier(DecisionTreeClassifier(), max_samples=<span class="number">0.5</span>, max_features=<span class="number">1.0</span>, n_estimators=<span class="number">20</span>)</span><br><span class="line">bgc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">bgc.score(X_train, y_train), bgc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.9935166666666667, 0.9506)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="Third-question"><a href="#Third-question" class="headerlink" title="Third question"></a>Third question</h3><p><strong>Logistic regression as a base learner</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bgc = BaggingClassifier(LogisticRegression(max_iter = <span class="number">500</span>), max_samples=<span class="number">0.5</span>, max_features=<span class="number">1.0</span>, n_estimators=<span class="number">20</span>)</span><br><span class="line">bgc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">bgc.score(X_train, y_train), bgc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.9421166666666667, 0.9228)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>Above we have successfully used logistic regression as the base learner to complete integrated learning. You can try to use only logistic regression for training, and compare the performance of the single model with the bagging version of logistic regression.</p>
<h4 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h4><p>Boosting refers to a series of algorithms that can transform a weak learner into a strong learner. The main principle of boosting is to combine a series of weak learners (only better than random guessing). For those samples that were misclassified in the early stages of training, the boosting algorithm will give more attention. Then combine the predictions by weighted majority voting (classification) or weighted sum (regression) to produce the final prediction.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">abc = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=<span class="number">10</span>, learning_rate=<span class="number">0.01</span>)</span><br><span class="line">abc.fit(X_train, y_train)</span><br><span class="line">abc.score(X_train, y_train), abc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(1.0, 0.875)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>Comparing the boosting integrated version of decision tree and logistic regression, we can find that logistic regression has better generalization ability, and decision tree is easier to overfit</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">abc = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=<span class="number">8</span>), n_estimators=<span class="number">10</span>, learning_rate=<span class="number">0.01</span>)</span><br><span class="line">abc.fit(X_train, y_train)</span><br><span class="line">abc.score(X_train, y_train), abc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.9981833333333333, 0.9532)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>In fact, over-fitting is not a bad thing. If your model cannot be over-fitted, it means that it cannot fit the training data well. Therefore, the decision tree is very over-fitted at the beginning, which also shows its potential. , You can see that after the above parameters are adjusted, the boosting version of the decision tree easily exceeds the boosting version of the logistic regression</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://hivan.me/example_06/" data-id="clid1n2wj004q2nj7adzmffeo" data-title="" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/example_08/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/example_03/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/develop/">develop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/software/">software</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/web/">web</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ChatGPT/" rel="tag">ChatGPT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chrome/" rel="tag">Chrome</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gmail/" rel="tag">Gmail</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Google/" rel="tag">Google</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Google-Plus/" rel="tag">Google Plus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gplus/" rel="tag">Gplus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mac/" rel="tag">Mac</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mail/" rel="tag">Mail</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Model/" rel="tag">Model</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NFC/" rel="tag">NFC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/O2O/" rel="tag">O2O</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PS/" rel="tag">PS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/QQ/" rel="tag">QQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SNS/" rel="tag">SNS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Stable-Diffusion/" rel="tag">Stable Diffusion</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translate/" rel="tag">Translate</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/allove/" rel="tag">allove</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/app/" rel="tag">app</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/avn/" rel="tag">avn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/" rel="tag">blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bluehost/" rel="tag">bluehost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/code/" rel="tag">code</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/css/" rel="tag">css</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/device/" rel="tag">device</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/feed/" rel="tag">feed</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flickr/" rel="tag">flickr</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/forward/" rel="tag">forward</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gmail/" rel="tag">gmail</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/habari/" rel="tag">habari</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/" rel="tag">hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ifttt/" rel="tag">ifttt</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/install/" rel="tag">install</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ireader/" rel="tag">ireader</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/istef/" rel="tag">istef</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/javascript/" rel="tag">javascript</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/location/" rel="tag">location</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/more/" rel="tag">more</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/net/" rel="tag">net</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nitrous/" rel="tag">nitrous</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/node/" rel="tag">node</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pageflakes/" rel="tag">pageflakes</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/picasaweb/" rel="tag">picasaweb</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/plugin/" rel="tag">plugin</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/reader/" rel="tag">reader</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/research/" rel="tag">research</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/search/" rel="tag">search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sms/" rel="tag">sms</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/telnet/" rel="tag">telnet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/twitter/" rel="tag">twitter</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/url/" rel="tag">url</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/version/" rel="tag">version</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/windows-8/" rel="tag">windows 8</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/wordpress/" rel="tag">wordpress</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%B8%96%E7%95%8C%E6%9D%AF/" rel="tag">世界杯</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BE%AE%E5%8D%9A/" rel="tag">微博</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%B6%E5%8C%BA/" rel="tag">时区</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A4%BE%E4%BA%A4/" rel="tag">社交</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%8A%B1%E5%84%BF%E5%BC%80%E4%BA%86/" rel="tag">花儿开了</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A1%A8%E6%83%85/" rel="tag">表情</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%87%E6%BB%A4%E5%99%A8/" rel="tag">过滤器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%9B%E5%BA%A6/" rel="tag">进度</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI/" style="font-size: 20px;">AI</a> <a href="/tags/ChatGPT/" style="font-size: 10px;">ChatGPT</a> <a href="/tags/Chrome/" style="font-size: 10px;">Chrome</a> <a href="/tags/Gmail/" style="font-size: 10px;">Gmail</a> <a href="/tags/Google/" style="font-size: 18px;">Google</a> <a href="/tags/Google-Plus/" style="font-size: 10px;">Google Plus</a> <a href="/tags/Gplus/" style="font-size: 10px;">Gplus</a> <a href="/tags/Mac/" style="font-size: 10px;">Mac</a> <a href="/tags/Mail/" style="font-size: 10px;">Mail</a> <a href="/tags/Model/" style="font-size: 10px;">Model</a> <a href="/tags/NFC/" style="font-size: 10px;">NFC</a> <a href="/tags/O2O/" style="font-size: 10px;">O2O</a> <a href="/tags/PS/" style="font-size: 10px;">PS</a> <a href="/tags/Python/" style="font-size: 16px;">Python</a> <a href="/tags/QQ/" style="font-size: 10px;">QQ</a> <a href="/tags/SNS/" style="font-size: 10px;">SNS</a> <a href="/tags/Stable-Diffusion/" style="font-size: 12px;">Stable Diffusion</a> <a href="/tags/Tool/" style="font-size: 10px;">Tool</a> <a href="/tags/Translate/" style="font-size: 10px;">Translate</a> <a href="/tags/allove/" style="font-size: 10px;">allove</a> <a href="/tags/app/" style="font-size: 10px;">app</a> <a href="/tags/avn/" style="font-size: 10px;">avn</a> <a href="/tags/blog/" style="font-size: 10px;">blog</a> <a href="/tags/bluehost/" style="font-size: 10px;">bluehost</a> <a href="/tags/code/" style="font-size: 10px;">code</a> <a href="/tags/css/" style="font-size: 10px;">css</a> <a href="/tags/device/" style="font-size: 10px;">device</a> <a href="/tags/feed/" style="font-size: 10px;">feed</a> <a href="/tags/flickr/" style="font-size: 10px;">flickr</a> <a href="/tags/forward/" style="font-size: 10px;">forward</a> <a href="/tags/gmail/" style="font-size: 10px;">gmail</a> <a href="/tags/habari/" style="font-size: 16px;">habari</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/ifttt/" style="font-size: 12px;">ifttt</a> <a href="/tags/install/" style="font-size: 10px;">install</a> <a href="/tags/ireader/" style="font-size: 10px;">ireader</a> <a href="/tags/istef/" style="font-size: 10px;">istef</a> <a href="/tags/javascript/" style="font-size: 10px;">javascript</a> <a href="/tags/location/" style="font-size: 10px;">location</a> <a href="/tags/more/" style="font-size: 10px;">more</a> <a href="/tags/net/" style="font-size: 10px;">net</a> <a href="/tags/nitrous/" style="font-size: 10px;">nitrous</a> <a href="/tags/node/" style="font-size: 10px;">node</a> <a href="/tags/pageflakes/" style="font-size: 10px;">pageflakes</a> <a href="/tags/picasaweb/" style="font-size: 10px;">picasaweb</a> <a href="/tags/plugin/" style="font-size: 10px;">plugin</a> <a href="/tags/reader/" style="font-size: 10px;">reader</a> <a href="/tags/research/" style="font-size: 10px;">research</a> <a href="/tags/search/" style="font-size: 10px;">search</a> <a href="/tags/sms/" style="font-size: 10px;">sms</a> <a href="/tags/telnet/" style="font-size: 10px;">telnet</a> <a href="/tags/twitter/" style="font-size: 12px;">twitter</a> <a href="/tags/url/" style="font-size: 10px;">url</a> <a href="/tags/version/" style="font-size: 10px;">version</a> <a href="/tags/windows-8/" style="font-size: 10px;">windows 8</a> <a href="/tags/wordpress/" style="font-size: 14px;">wordpress</a> <a href="/tags/%E4%B8%96%E7%95%8C%E6%9D%AF/" style="font-size: 10px;">世界杯</a> <a href="/tags/%E5%BE%AE%E5%8D%9A/" style="font-size: 10px;">微博</a> <a href="/tags/%E6%97%B6%E5%8C%BA/" style="font-size: 10px;">时区</a> <a href="/tags/%E7%A4%BE%E4%BA%A4/" style="font-size: 10px;">社交</a> <a href="/tags/%E8%8A%B1%E5%84%BF%E5%BC%80%E4%BA%86/" style="font-size: 10px;">花儿开了</a> <a href="/tags/%E8%A1%A8%E6%83%85/" style="font-size: 10px;">表情</a> <a href="/tags/%E8%BF%87%E6%BB%A4%E5%99%A8/" style="font-size: 10px;">过滤器</a> <a href="/tags/%E8%BF%9B%E5%BA%A6/" style="font-size: 10px;">进度</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">四月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">三月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">二月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">三月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">九月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">八月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">六月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">七月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">十一月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/06/">六月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/12/">十二月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/08/">八月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/03/">三月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/07/">七月 2012</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/04/">四月 2012</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/01/">一月 2012</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2011/11/">十一月 2011</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2011/10/">十月 2011</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2011/07/">七月 2011</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2010/12/">十二月 2010</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2010/04/">四月 2010</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2009/10/">十月 2009</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2009/02/">二月 2009</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2008/06/">六月 2008</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2006/10/">十月 2006</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2006/09/">九月 2006</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2006/06/">六月 2006</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/Use-AI-to-write-a-snake-game/">利用AI写一个『贪吃蛇游戏』</a>
          </li>
        
          <li>
            <a href="/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/">13 使用多步提示语让AI帮你写测试</a>
          </li>
        
          <li>
            <a href="/AI-create-a-excel-plugin/">12 AI帮你写个小插件，轻松处理Excel文件</a>
          </li>
        
          <li>
            <a href="/Save-costs-with-an-open-source-model/">11 用好开源模型节约成本</a>
          </li>
        
          <li>
            <a href="/Use-AI-to-index-and-analyze-documents-and-images/">10 利用AI索引并分析文献和图片</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 Hivan Du<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>