<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Machine Learning Part-04 - 茶桁.MAMT</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="茶桁.MAMT"><meta name="msapplication-TileImage" content="https://qiniu.hivan.me/picGo/20230601174411.png?imgNote"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="茶桁.MAMT"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="SVM"><meta property="og:type" content="blog"><meta property="og:title" content="Machine Learning Part-04"><meta property="og:url" content="https://hivan.me/example_06/"><meta property="og:site_name" content="茶桁.MAMT"><meta property="og:description" content="SVM"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://qiniu.hivan.me/MAMTimage-20210901170815036.png?img"><meta property="og:image" content="https://qiniu.hivan.me/MAMTimage-20210901170902373.png?img"><meta property="og:image" content="https://qiniu.hivan.me/MAMTimage-20210901170921575.png?img"><meta property="og:image" content="https://qiniu.hivan.me/MAMTimage-20210901171726065.png?img"><meta property="article:published_time" content="2021-09-02T12:59:46.686Z"><meta property="article:modified_time" content="2023-06-02T04:10:57.185Z"><meta property="article:author" content="Hivan Du"><meta property="article:tag" content="AI,人工智能,代码,大语言模型"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://qiniu.hivan.me/MAMTimage-20210901170815036.png?img"><meta property="twitter:creator" content="@hivan"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hivan.me/example_06/"},"headline":"Machine Learning Part-04","image":[],"datePublished":"2021-09-02T12:59:46.686Z","dateModified":"2023-06-02T04:10:57.185Z","author":{"@type":"Person","name":"Hivan Du"},"publisher":{"@type":"Organization","name":"茶桁.MAMT","logo":{"@type":"ImageObject","url":"https://hivan.me/img/logo.svg"}},"description":"SVM"}</script><link rel="canonical" href="https://hivan.me/example_06/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="茶桁.MAMT" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-09-02T12:59:46.686Z" title="9/2/2021, 8:59:46 PM">2021-09-02</time>发表</span></div></div><h1 class="title is-3 is-size-4-mobile">Machine Learning Part-04</h1><div class="content"><h2 id="svm">SVM</h2>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">label_a = np.random.normal(<span class="number">6</span>, <span class="number">2</span>, size = (<span class="number">50</span>,<span class="number">2</span>))</span><br><span class="line">label_b = np.random.normal(-<span class="number">6</span>, <span class="number">2</span>, size = (<span class="number">50</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">b = [-<span class="number">1</span>,-<span class="number">2</span>, -<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_a))</span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_b))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/MAMTimage-20210901170815036.png?img" alt="image-20210901170815036" /><figcaption aria-hidden="true">image-20210901170815036</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">label_a_x = label_a[:, <span class="number">0</span>]</span><br><span class="line">label_b_x = label_b[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, k, b</span>):</span><br><span class="line">    <span class="keyword">return</span> k*x -b</span><br><span class="line">  </span><br><span class="line">k_and_b = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    k, b = (np.random.random(size = (<span class="number">1</span>,<span class="number">2</span>)) * <span class="number">10</span> - <span class="number">5</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">max</span>(f(label_a_x, k, b)) &lt;= -<span class="number">1</span> <span class="keyword">and</span> np.<span class="built_in">min</span>(f(label_b_x, k, b)) &gt;= <span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(k, b)</span><br><span class="line">        k_and_b.append((k, b))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">-3.4732670434285517 -2.3248316389039325</span></span><br><span class="line"><span class="string">-3.654276254462583 0.01110189858052646</span></span><br><span class="line"><span class="string">-2.4609031871010014 -0.3932180655739925</span></span><br><span class="line"><span class="string">-2.9206497777762843 0.2595456609552631</span></span><br><span class="line"><span class="string">-4.07589152330003 -0.6463313059119606</span></span><br><span class="line"><span class="string">-3.1950366475236835 -1.8558958669742989</span></span><br><span class="line"><span class="string">-4.316670785852706 -3.1033030808371653</span></span><br><span class="line"><span class="string">-4.124339773909792 -1.5741734685470687</span></span><br><span class="line"><span class="string">-4.20817621470405 0.4368323022696625</span></span><br><span class="line"><span class="string">-3.7098120657624003 -0.38196175566618784</span></span><br><span class="line"><span class="string">-3.2053446683533315 0.12822700803583054</span></span><br><span class="line"><span class="string">-4.534694169094692 1.143734501297419</span></span><br><span class="line"><span class="string">-4.8124714209376425 0.8707258703100704</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_a))</span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, b <span class="keyword">in</span> k_and_b:</span><br><span class="line">    x = np.concatenate((label_a_x, label_b_x))</span><br><span class="line">    plt.plot(x, f(x, k, b))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/MAMTimage-20210901170902373.png?img" alt="image-20210901170902373" /><figcaption aria-hidden="true">image-20210901170902373</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_a))</span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_b))</span><br><span class="line"></span><br><span class="line">k,b = <span class="built_in">sorted</span>(k_and_b, key = <span class="keyword">lambda</span> t: <span class="built_in">abs</span>(t[<span class="number">0</span>]))[<span class="number">0</span>]</span><br><span class="line">x = np.concatenate((label_a_x, label_b_x))</span><br><span class="line">plt.plot(x, f(x, k, b))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/MAMTimage-20210901170921575.png?img" alt="image-20210901170921575" /><figcaption aria-hidden="true">image-20210901170921575</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"></span><br><span class="line">datasets = load_boston()</span><br><span class="line">data, target = datasets[<span class="string">&#x27;data&#x27;</span>], datasets[<span class="string">&#x27;target&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line">df.columns = datasets[<span class="string">&#x27;feature_names&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_select</span>(<span class="params">df, drop_num = <span class="number">4</span></span>):</span><br><span class="line">    columns = random.sample(<span class="built_in">list</span>(df.columns), k = <span class="built_in">len</span>(df.columns) - drop_num)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> df[columns]</span><br><span class="line">  </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line">sample_x = random_select(df)</span><br><span class="line">regressioner = DecisionTreeRegressor()</span><br><span class="line">(X_train, X_test, y_train, y_test)  = train_test_split(sample_x, target, test_size = <span class="number">0.3</span>)</span><br><span class="line">regressioner.fit(X_train, y_train)</span><br><span class="line">regressioner.score(X_train, y_train)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1.0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">regressioner.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0.8110635350395325</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_tree</span>(<span class="params">train_X, train_y, test_X, test_y, drop_n = <span class="number">4</span></span>):</span><br><span class="line">    train_sample = random_select(train_X, drop_num = drop_n)</span><br><span class="line"></span><br><span class="line">    regressioner = DecisionTreeRegressor()</span><br><span class="line">    regressioner.fit(train_sample, train_y)</span><br><span class="line"></span><br><span class="line">    train_score = regressioner.score(train_sample, train_y)</span><br><span class="line">    test_score = regressioner.score(test_X[train_sample.columns], test_y)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;train score = &#123;&#125;; test score = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(train_score, test_score))</span><br><span class="line"></span><br><span class="line">    y_predicat = regressioner.predict(test_X[train_sample.columns])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_predicat</span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_forest</span>(<span class="params">train_X, train_y, test_X, test_y, tree_n = <span class="number">4</span></span>):</span><br><span class="line">    predicat = np.array([random_tree(train_X, train_y, test_X, test_y) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(tree_n)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.mean(predicat, axis = <span class="number">0</span>)</span><br><span class="line">  </span><br><span class="line">(X_train, X_test, y_train, y_test)  = train_test_split(df, target, test_size = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">forest_predict = random_forest(X_train, y_train, X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">train score = 1.0; test score = 0.5367061884031776</span></span><br><span class="line"><span class="string">train score = 1.0; test score = 0.4983695562874999</span></span><br><span class="line"><span class="string">train score = 1.0; test score = 0.6715869370883646</span></span><br><span class="line"><span class="string">train score = 1.0; test score = 0.6210922529610217</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">forest_predict</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">array([10.925, 21.1  , 30.625, 28.025, 22.525, 17.65 , 20.6  , 17.325,</span></span><br><span class="line"><span class="string">       29.175, 14.95 , 40.775, 19.55 , 12.175, 23.675, 10.775, 22.1  ,</span></span><br><span class="line"><span class="string">       ...</span></span><br><span class="line"><span class="string">       15.575, 20.5  , 22.775, 30.725, 18.975, 16.45 , 22.05 , 18.925])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">r2_score(y_test, forest_predict)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0.7840500839091215</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="entropy-熵">Entropy: 熵</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> icecream <span class="keyword">import</span> ic</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pr</span>(<span class="params">es</span>):</span><br><span class="line">    counter = Counter(es)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_wrap</span>(<span class="params">e</span>):</span><br><span class="line">        <span class="keyword">return</span> counter[e] / <span class="built_in">len</span>(es)</span><br><span class="line">    <span class="keyword">return</span> _wrap</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">entropy</span>(<span class="params">elements</span>):</span><br><span class="line">    <span class="comment"># Information Entropy</span></span><br><span class="line">    p = pr(elements)</span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(p(e) * np.log(p(e)) <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">set</span>(elements))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gini</span>(<span class="params">elements</span>):</span><br><span class="line">    p = pr(elements)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>-np.<span class="built_in">sum</span>(p(e) ** <span class="number">2</span> <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">set</span>(elements))</span><br><span class="line">  </span><br><span class="line">pure_func = gini</span><br><span class="line"></span><br><span class="line">ic(pure_func([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]))</span><br><span class="line">ic(pure_func([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">ic(pure_func([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">8</span>]))</span><br><span class="line">ic(pure_func([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">9</span>]))</span><br><span class="line">ic(pure_func([<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]))</span><br><span class="line">ic(pure_func([<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>]))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">ic| pure_func([1, 1, 1, 1, 1, 0]): 0.2777777777777777</span></span><br><span class="line"><span class="string">ic| pure_func([1, 1, 1, 1, 1, 1]): 0.0</span></span><br><span class="line"><span class="string">ic| pure_func([1, 2, 3, 4, 5, 8]): 0.8333333333333333</span></span><br><span class="line"><span class="string">ic| pure_func([1, 2, 3, 4, 5, 9]): 0.8333333333333333</span></span><br><span class="line"><span class="string">ic| pure_func([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;]): 0.44897959183673464</span></span><br><span class="line"><span class="string">ic| pure_func([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;c&#x27;, &#x27;d&#x27;]): 0.6122448979591837</span></span><br><span class="line"><span class="string">0.6122448979591837</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="random-forest">Random forest</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor, DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">house = load_boston()</span><br><span class="line">X = house.data</span><br><span class="line">y = house.target</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">tree_reg = DecisionTreeRegressor()</span><br><span class="line">tree_reg.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;whole dataset train acc: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(tree_reg.score(x_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;whole dataset test acc: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(tree_reg.score(x_test, y_test)))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">whole dataset train acc: 1.0</span></span><br><span class="line"><span class="string">whole dataset test acc: 0.6776520888466615</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_forest</span>(<span class="params">train_x, train_y, test_x, test_y, drop_n=<span class="number">4</span></span>):</span><br><span class="line">    random_features = np.random.choice(<span class="built_in">list</span>(train_x.columns), size=<span class="built_in">len</span>(train_x.columns)-drop_n)</span><br><span class="line"></span><br><span class="line">    sample_x = train_x[random_features]</span><br><span class="line">    sample_y = train_y</span><br><span class="line"></span><br><span class="line">    reg = DecisionTreeRegressor()</span><br><span class="line">    reg.fit(sample_x, sample_y)</span><br><span class="line"></span><br><span class="line">    train_score = reg.score(sample_x, sample_y)</span><br><span class="line">    test_score = reg.score(test_x[random_features], test_y)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;sub sample :: train score: &#123;&#125;, test score: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(train_score, test_score))</span><br><span class="line"></span><br><span class="line">    y_predicated = reg.predict(test_x[random_features])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_predicated, test_score</span><br><span class="line">  </span><br><span class="line">with_feature_names = pd.DataFrame(X)</span><br><span class="line">with_feature_names.columns = house[<span class="string">&#x27;feature_names&#x27;</span>]</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(with_feature_names, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">tree_num = <span class="number">4</span></span><br><span class="line">predicates = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(tree_num):</span><br><span class="line">    predicated, score = random_forest(x_train, y_train, x_test, y_test)</span><br><span class="line">    predicates.append((predicated, score))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">sub sample :: train score: 1.0, test score: 0.5640870175410873</span></span><br><span class="line"><span class="string">sub sample :: train score: 1.0, test score: 0.29024437819534354</span></span><br><span class="line"><span class="string">sub sample :: train score: 1.0, test score: 0.37812117132843814</span></span><br><span class="line"><span class="string">sub sample :: train score: 1.0, test score: 0.5650888856735524</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">predicates_value = [v <span class="keyword">for</span> v, s <span class="keyword">in</span> predicates]</span><br><span class="line">forest_scores = [s <span class="keyword">for</span> v, s <span class="keyword">in</span> predicates]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;the score of forest is : &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(r2_score(y_test, np.mean(predicates_value, axis=<span class="number">0</span>))))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">the score of forest is : 0.680193104551715</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">weights = np.array(forest_scores) / np.<span class="built_in">sum</span>(forest_scores)</span><br><span class="line"></span><br><span class="line">weights_score = np.zeros_like(np.mean(predicates_value, axis=<span class="number">0</span>))</span><br><span class="line"><span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicates_value):</span><br><span class="line">    weights_score += v * weights[i]</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;the score of weighted forest is : &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(r2_score(y_test, weights_score)))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">the score of weighted forest is : 0.6956613076019385</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="show-svm">Show SVM</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">label_a = np.random.normal(<span class="number">6</span>, <span class="number">2</span>, size=(<span class="number">50</span>, <span class="number">2</span>))</span><br><span class="line">label_b = np.random.normal(-<span class="number">6</span>, <span class="number">2</span>, size=(<span class="number">50</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_a))</span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*label_b))</span><br><span class="line"></span><br><span class="line">label_a_x = label_a[:, <span class="number">0</span>]</span><br><span class="line">label_b_x = label_b[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, w, b</span>):</span><br><span class="line">    <span class="keyword">return</span> w * x + b</span><br><span class="line">  </span><br><span class="line">k_and_b = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    k, b = (np.random.random(size=(<span class="number">1</span>, <span class="number">2</span>)) * <span class="number">10</span> - <span class="number">5</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">max</span>(f(label_a_x, k, b)) &gt;= -<span class="number">1</span> <span class="keyword">and</span> np.<span class="built_in">min</span>(f(label_b_x, k, b)) &gt;= <span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(k, b)</span><br><span class="line">        k_and_b.append((k, b))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0.17732109082579406 3.9508645615428843</span></span><br><span class="line"><span class="string">-0.8649868307954458 1.7349996177756957</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">-2.2969567032985783 2.171321001904926</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, b <span class="keyword">in</span> k_and_b:</span><br><span class="line">    x = np.concatenate((label_a_x, label_b_x))</span><br><span class="line">    plt.plot(x, f(x, k, b))</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(k_and_b)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[(0.17732109082579406, 3.9508645615428843), (-0.8649868307954458, 1.7349996177756957), (-0.818317924604357, 0.352843348193578), (-0.19730603224472976, 4.002168852007262), </span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">(-2.2969567032985783, 2.171321001904926)]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">w, b = <span class="built_in">min</span>(k_and_b, key=<span class="keyword">lambda</span> k_b: k_b[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">all_x = np.concatenate((label_a_x, label_b_x))</span><br><span class="line">plt.plot(all_x, f(all_x, w, b), <span class="string">&#x27;r-o&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/MAMTimage-20210901171726065.png?img" alt="image-20210901171726065" /><figcaption aria-hidden="true">image-20210901171726065</figcaption>
</figure>
<h2 id="integrated-learning">Integrated learning</h2>
<p>Ensemble learning is a machine learning paradigm that solves the same problem by training multiple models. In contrast to ordinary machine learning methods that try to learn a hypothesis from training data, ensemble methods try to construct a set of hypotheses and use them in combination. Next, we will use the decision tree and its integrated version to model the classic data set Mnist and observe the differences in different integration methods.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">!ls</span><br><span class="line">!unzip mnist_test.csv.<span class="built_in">zip</span> &amp;&amp; unzip mnist_train.csv.<span class="built_in">zip</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier</span><br></pre></td></tr></table></figure>
<h4 id="build-a-data-set">Build a data set</h4>
<p>The Mnist data set used this time is not in the original format. In order to more easily adapt to this training, the 28 * 28 pictures in the original data set are <a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/reference%20/generated/numpy.ndarray.flatten.html">flatten</a> operation, it becomes 784 features, the columns in the DataFrame below: 1x1, 1x2, ..., 28x28, representing the <em>i</em> row and <em>j</em> column in the picture The pixel value of is a grayscale image, so the pixel value is only 0 and 1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">train_df = df = pd.read_csv(<span class="string">&#x27;~/data/mnist_train.csv&#x27;</span>)</span><br><span class="line">train_df.head()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	label	1x1	1x2	1x3	1x4	1x5	1x6	1x7	1x8	1x9	...	28x19	28x20	28x21	28x22	28x23	28x24	28x25	28x26	28x27	28x28</span></span><br><span class="line"><span class="string">0	5	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">1	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">2	4	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">3	1	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">4	9	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">5 rows × 785 columns</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>View training data information:, whether there is NaN, how many pieces of data are there...</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">train_df.info()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span></span><br><span class="line"><span class="string">RangeIndex: 60000 entries, 0 to 59999</span></span><br><span class="line"><span class="string">Columns: 785 entries, label to 28x28</span></span><br><span class="line"><span class="string">dtypes: int64(785)</span></span><br><span class="line"><span class="string">memory usage: 359.3 MB</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">test_df = df = pd.read_csv(<span class="string">&#x27;~/data/mnist_test.csv&#x27;</span>)</span><br><span class="line">test_df.head()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	label	1x1	1x2	1x3	1x4	1x5	1x6	1x7	1x8	1x9	...	28x19	28x20	28x21	28x22	28x23	28x24	28x25	28x26	28x27	28x28</span></span><br><span class="line"><span class="string">0	7	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">1	2	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">2	1	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">3	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">4	4	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0</span></span><br><span class="line"><span class="string">5 rows × 785 columns</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">test_df.info()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span></span><br><span class="line"><span class="string">RangeIndex: 10000 entries, 0 to 9999</span></span><br><span class="line"><span class="string">Columns: 785 entries, label to 28x28</span></span><br><span class="line"><span class="string">dtypes: int64(785)</span></span><br><span class="line"><span class="string">memory usage: 59.9 MB</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>Build training and test data</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_train = train_df.iloc[:, <span class="number">1</span>:]</span><br><span class="line">y_train = train_df.iloc[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">X_test = test_df.iloc[:, <span class="number">1</span>:]</span><br><span class="line">y_test = test_df.iloc[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(((60000, 784), (60000,)), ((10000, 784), (10000,)))</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h4 id="decision-tree">Decision Tree</h4>
<p>First train a simple decision tree to see how it performs</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">dtc = DecisionTreeClassifier()</span><br><span class="line">dtc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">dtc.score(X_train, y_train)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1.0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">dtc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0.8753</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">dtc = DecisionTreeClassifier(min_samples_leaf=<span class="number">8</span>)</span><br><span class="line">dtc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">dtc.score(X_train, y_train), dtc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.9311666666666667, 0.8795)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>From the above results, we can see that by adjusting the parameter <code>min_samples_leaf</code>, the overfitting situation has been alleviated. What does this parameter mean? Why increasing it can alleviate the overfitting problem? The meaning of <code>min_samples_leaf</code> is the minimum number of samples contained in the leaf nodes of the decision tree. By increasing this parameter, the decision tree can not capture any of the subtle features of the training data during training, resulting in excessive training data. Fitting: The large number of samples of leaf nodes can also play a role in voting and enhance the generalization performance of the model. You can try to continue to increase the value of this parameter and try to find the best parameter. In addition to this parameter, you can also try to adjust the parameters such as <code>min_samples_split</code> and <code>max_features</code>. For the specific meaning, please refer to <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">sklearn documentation</a></p>
<h3 id="second-question"><strong>Second question: </strong></h3>
<p><strong>Try to adjust other parameters to see the performance of the decision tree on the test set</strong></p>
<h4 id="random-forest-1">Random Forest</h4>
<p>Take a look at the bagging version of the decision tree and how the random forest performs!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rfc = RandomForestClassifier(n_estimators = <span class="number">10</span>)</span><br><span class="line">rfc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">rfc.score(X_train, y_train), rfc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.99905, 0.9513)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>It is worthy of the integrated version. It basically achieves better performance under the default parameters. The accuracy of the test set is about 7% higher than that of the ordinary decision tree. However, comparing the training and test results, it can be found that there is still a certain degree of overfitting. , Try to adjust some parameters below</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rfc = RandomForestClassifier(n_estimators = <span class="number">20</span>)</span><br><span class="line">rfc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">rfc.score(X_train, y_train), rfc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.9999, 0.96)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>After increasing the parameter <code>n_estimators</code>, the accuracy of the test set has increased by about 1%. The meaning of this parameter is to train 20 decision trees at the same time, and finally integrate the results. The increase of this parameter can be simply regarded as voting The number of people increases, so the final result will inevitably be more robust. You can try to continue to increase this parameter, or adjust other parameters such as <code>max_samples</code>, appropriately less than the total amount of training data, which can increase the difference between different sub-models and further improve the generalization performance. It can also adjust the parameters of the base learner (decision tree). For the meaning of the parameters, see <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">sklearn documentation</a></p>
<h4 id="gbdt">GBDT</h4>
<p>Let's compare the performance of the boosting version of the decision tree GBDT!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">gbc = GradientBoostingClassifier(n_estimators=<span class="number">10</span>)</span><br><span class="line">gbc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">gbc.score(X_train, y_train), gbc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.8423, 0.846)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>As expected, the performance has been greatly improved, and the indicators of the training set are basically the same as those of the test set, and there is no overfitting, so it should be possible to continue to try to improve this parameter. Generally, in the absence of overfitting, we only need to consider continuing to increase the complexity of the model. This is the fastest way to improve performance. When the complexity of the model increases to the point of over-fitting, we then consider using some methods to reduce over-fitting.</p>
<h4 id="bagging">Bagging</h4>
<p>The aforementioned random forest and GBDT are ensemble learning algorithms based on decision trees, but it should be noted that ensemble learning is not exclusive to decision trees. Any other learner can be used as a base learner for ensemble learning, such as Logistic regression, support vector machine.</p>
<p>Bagging is short for "bootstrap aggregating". This is a meta-algorithm, which takes M sub-samples (with replacement) from the initial data set, and trains the prediction model on these sub-samples. The final model is obtained by averaging all sub-models, which usually produces better results. The main advantage of this technique is that it combines regularization, all you need to do is choose good parameters for the base learner.</p>
<p>The following uses the general api provided by sklearn to construct an integrated learning algorithm</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Still use decision tree as base learner</span></span><br><span class="line">bgc = BaggingClassifier(DecisionTreeClassifier(), max_samples=<span class="number">0.5</span>, max_features=<span class="number">1.0</span>, n_estimators=<span class="number">20</span>)</span><br><span class="line">bgc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">bgc.score(X_train, y_train), bgc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.9935166666666667, 0.9506)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="third-question">Third question</h3>
<p><strong>Logistic regression as a base learner</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bgc = BaggingClassifier(LogisticRegression(max_iter = <span class="number">500</span>), max_samples=<span class="number">0.5</span>, max_features=<span class="number">1.0</span>, n_estimators=<span class="number">20</span>)</span><br><span class="line">bgc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">bgc.score(X_train, y_train), bgc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.9421166666666667, 0.9228)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>Above we have successfully used logistic regression as the base learner to complete integrated learning. You can try to use only logistic regression for training, and compare the performance of the single model with the bagging version of logistic regression.</p>
<h4 id="boosting">Boosting</h4>
<p>Boosting refers to a series of algorithms that can transform a weak learner into a strong learner. The main principle of boosting is to combine a series of weak learners (only better than random guessing). For those samples that were misclassified in the early stages of training, the boosting algorithm will give more attention. Then combine the predictions by weighted majority voting (classification) or weighted sum (regression) to produce the final prediction.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">abc = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=<span class="number">10</span>, learning_rate=<span class="number">0.01</span>)</span><br><span class="line">abc.fit(X_train, y_train)</span><br><span class="line">abc.score(X_train, y_train), abc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(1.0, 0.875)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>Comparing the boosting integrated version of decision tree and logistic regression, we can find that logistic regression has better generalization ability, and decision tree is easier to overfit</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">abc = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=<span class="number">8</span>), n_estimators=<span class="number">10</span>, learning_rate=<span class="number">0.01</span>)</span><br><span class="line">abc.fit(X_train, y_train)</span><br><span class="line">abc.score(X_train, y_train), abc.score(X_test, y_test)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(0.9981833333333333, 0.9532)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>In fact, over-fitting is not a bad thing. If your model cannot be over-fitted, it means that it cannot fit the training data well. Therefore, the decision tree is very over-fitted at the beginning, which also shows its potential. , You can see that after the above parameters are adjusted, the boosting version of the decision tree easily exceeds the boosting version of the logistic regression</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Machine Learning Part-04</p><p><a href="https://hivan.me/example_06/">https://hivan.me/example_06/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hivan Du</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-09-02</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2023-06-02</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=6479444288ae9600196fa98e&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="https://afdian.net/item/72907364008511ee904852540025c377" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://qiniu.hivan.me/picGo/20230601221633.jpeg" alt="支付宝"></span></a><a class="button donate" href="https://www.buymeacoffee.com/hivandu" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="https://patreon.com/user?u=89473430" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="doo@hivan.me"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://qiniu.hivan.me/IMG_4603.JPG" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/example_05/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Machine Learning Part-03</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/example_08/"><span class="level-item">RNN</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://hivan.me/example_06/';
            this.page.identifier = 'example_06/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'hivan' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://www.gravatar.com/avatar/bdff168cf8a71c11d2712a1679a00c54?s=128" alt="茶桁"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">茶桁</p><p class="is-size-6 is-block">AI游民</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shang Hai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">145</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">14</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hivandu" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hivan"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/hivan"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com/hivan"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4NzE4MDQzMg==&amp;action=getalbum&amp;album_id=2932504849574543360&amp;scene=173&amp;from_msgid=2648747980&amp;from_itemidx=1&amp;count=3&amp;nolastread=1&amp;token=1758883909&amp;lang=zh_CN#wechat_redirect"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.zhihu.com/column/c_1424326166602178560" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">塌缩的奇点</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li><li><a class="level is-mobile" href="https://www.zhihu.com/column/hivandu" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">茶桁-知乎</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="level-start"><span class="level-item">从零开始接触人工智能大模型</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-05T14:43:05.000Z">2023-07-05</time></p><p class="title"><a href="/2023%E5%B9%B4%E8%96%AA%E9%85%AC%E6%9C%80%E9%AB%98%E7%9A%84%E7%A7%91%E6%8A%80%E5%B7%A5%E4%BD%9C%E4%B8%AD%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E8%B5%AB%E7%84%B6%E5%9C%A8%E5%88%97/">2023年薪酬最高的科技工作中产品经理赫然在列</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-03T15:38:49.000Z">2023-07-03</time></p><p class="title"><a href="/%E6%A0%B9%E6%8D%AE%E5%9E%82%E7%9B%B4%E9%9C%80%E6%B1%82%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B/">18. 根据垂直需求微调模型</a></p><p class="categories"><a href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-30T11:48:02.000Z">2023-06-30</time></p><p class="title"><a href="/%E6%88%91%E4%BB%AC%E6%97%A0%E6%B3%95%E9%80%9A%E8%BF%87%E6%94%B9%E9%80%A0%E8%87%AA%E5%B7%B1%E6%91%86%E8%84%B1%E6%B0%94%E5%80%99%E5%8D%B1%E6%9C%BA/">观点：我们无法通过改造自己摆脱气候危机</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-14T14:36:13.000Z">2023-06-14</time></p><p class="title"><a href="/%E5%88%A9%E7%94%A8LangChain%E8%AE%A9AI%E5%81%9A%E5%86%B3%E7%AD%96/">17. 利用LangChain让AI做决策</a></p><p class="categories"><a href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-10T14:16:00.000Z">2023-06-10</time></p><p class="title"><a href="/Langchain%E8%AE%A9AI%E6%8B%A5%E6%9C%89%E8%AE%B0%E5%BF%86%E5%8A%9B/">16. Langchain让AI拥有记忆力</a></p><p class="categories"><a href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a><p class="is-size-7"><span>&copy; 2023 Hivan Du</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>