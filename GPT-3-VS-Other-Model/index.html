<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>04 GPT-3/4对比其他模型胜在哪？ | 茶桁.MAMT</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="大家好，我是茶桁。 在前两节课中，我们一起体验了 OpenAI 提供的 GPT-3.5 系列模型的两个核心接口。一个是获取文本的 Embedding 向量，另一个是根据提示语生成补全的文本内容。通过这两种方法，我们可以在零样本或少样本的情况下进行情感分析任务。然而，你可能会有两个疑问。首先，Embedding 不就是将文本转换为向量吗？为什么不直接使用开源模型（如Word2Vec、Bert）而要调">
<meta property="og:type" content="article">
<meta property="og:title" content="04 GPT-3&#x2F;4对比其他模型胜在哪？">
<meta property="og:url" content="https://hivan.me/GPT-3-VS-Other-Model/index.html">
<meta property="og:site_name" content="茶桁.MAMT">
<meta property="og:description" content="大家好，我是茶桁。 在前两节课中，我们一起体验了 OpenAI 提供的 GPT-3.5 系列模型的两个核心接口。一个是获取文本的 Embedding 向量，另一个是根据提示语生成补全的文本内容。通过这两种方法，我们可以在零样本或少样本的情况下进行情感分析任务。然而，你可能会有两个疑问。首先，Embedding 不就是将文本转换为向量吗？为什么不直接使用开源模型（如Word2Vec、Bert）而要调">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://qiniu.hivan.me/picGo/20230601165525.png?imgNote">
<meta property="og:image" content="http://qiniu.hivan.me/picGo/20230601165531.png?imgNote">
<meta property="og:image" content="http://qiniu.hivan.me/picGo/20230601165539.png?imgNote">
<meta property="og:image" content="http://qiniu.hivan.me/picGo/20230601165544.png?imgNote">
<meta property="article:published_time" content="2023-05-11T08:51:26.000Z">
<meta property="article:modified_time" content="2023-06-01T09:22:50.328Z">
<meta property="article:author" content="Hivan Du">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://qiniu.hivan.me/picGo/20230601165525.png?imgNote">
  
    <link rel="alternate" href="/atom.xml" title="茶桁.MAMT" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">茶桁.MAMT</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">ChaHeng Notes，codding and writting ~</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://hivan.me"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-GPT-3-VS-Other-Model" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/GPT-3-VS-Other-Model/" class="article-date">
  <time class="dt-published" datetime="2023-05-11T08:51:26.000Z" itemprop="datePublished">2023-05-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      04 GPT-3/4对比其他模型胜在哪？
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>大家好，我是茶桁。</p>
<p>在前两节课中，我们一起体验了 OpenAI 提供的 GPT-3.5 系列模型的两个核心接口。一个是获取文本的 Embedding 向量，另一个是根据提示语生成补全的文本内容。通过这两种方法，我们可以在零样本或少样本的情况下进行情感分析任务。然而，你可能会有两个疑问。首先，Embedding 不就是将文本转换为向量吗？为什么不直接使用开源模型（如Word2Vec、Bert）而要调用 OpenAI 的 API 呢？在我们的情感分析任务中，我们进行了一些简化处理。一方面，我们排除了相对中性的评分（3分）；另一方面，我们将1分、2分和4分、5分合并，将原本需要判断5个分类的问题简化了。那么，如果我们想要准确预测多个分类，是否也能如此简单呢？</p>
<span id="more"></span>

<p>在本节中，我们将通过代码和数据来回答第一个问题，尝试使用常见的开源模型，看看是否可以通过零样本学习的方式取得良好的效果。至于第二个问题，我们将在下节课中探讨，探索如何进一步利用 Embedding 结合机器学习算法来更好地处理情感分析问题。</p>
<h2 id="什么是预训练模型？"><a href="#什么是预训练模型？" class="headerlink" title="什么是预训练模型？"></a>什么是预训练模型？</h2><p>预训练模型是指通过在大规模文本数据上进行学习而生成的模型，能够将文本转化为语义丰富的向量表示。OpenAI 的 GPT-3 是一种超大规模的预训练模型，其英文全称为“Generative Pre-trained Transformer”，即生成式预训练 Transformer。通过预训练模型，我们可以在没有见过具体问题的情况下，利用大量可用的文本数据进行学习。</p>
<p>预训练模型的学习过程并不需要人工标注的数据。相反，它利用了大量的文本数据，如网页文章、维基百科内容、书籍电子版等，通过观察文本之间的内在关联来学习文本的语义。例如，在网络资料中，我们可以观察到许多类似的文本，如“小猫很可爱”和“小狗很可爱”。通过这些文本的前后关联，我们可以推断出小猫和小狗都是宠物，并且一般而言，人们对它们的情感是正面的。</p>
<p>这种隐含的关联信息可以帮助我们在情感分析等任务中填补少量用户评论和评分数据中缺失的“常识”。例如，如果文本中出现了“白日依山尽”，模型就能推测出接下来可能是“黄河入海流”。同样地，如果文本开头是“今天天气真”，那么后面很可能是“不错”，而较少可能是“糟糕”。这些文本关系最终通过一系列参数来体现。对于输入的文本，模型可以根据这些参数计算出一个向量，然后利用这个向量来推测文本的后续内容。</p>
<p>预训练模型在自然语言处理领域并不仅限于 OpenAI。早在2013年，Word2Vec就提出了类似的思想，通过在同一个句子中观察单词前后出现的单词，生成每个单词的向量表示。随后，在2018年，Google提出的BERT模型引起了整个行业的关注，BERT也成为了解决自然语言处理任务的重要预训练模型。在GPT-3论文发表之前，人们普遍认为BERT作为预训练模型在效果上优于GPT。然而，通过预训练模型，我们可以在语料文本越丰富的情况下拥有更多的参数，从而学习到更多的文本关系。因此，出现频率越高的文本关系，模型的预测能力也就越准确。随着时间的推移和研究的不断深入，预训练模型在自然语言处理领域的应用不断扩展。GPT-3的发布引发了巨大的关注，其规模庞大的预训练模型在生成式任务上取得了令人瞩目的成果。而BERT则在诸多下游任务中表现出色，成为许多实际应用的首选模型。</p>
<p>通过预训练模型，我们能够快速、高效地将文本转化为有意义的向量表示，从而在各种自然语言处理任务中取得良好的效果。随着预训练模型的不断发展和改进，我们可以期待更多的创新和突破，使得模型能够更准确地理解和处理各种复杂的自然语言表达。</p>
<p>预训练模型的引入为自然语言处理带来了巨大的进步和便利。它们通过利用大规模文本数据的内在关联，将语言转化为计算机能够理解和处理的形式，为我们解决各类自然语言处理任务提供了强大的基础。未来，随着技术的不断演进和研究的不断推进，预训练模型将继续发挥重要作用，推动自然语言处理领域的发展和创新。</p>
<h2 id="Fasttext、T5、GPT-3-模型对比"><a href="#Fasttext、T5、GPT-3-模型对比" class="headerlink" title="Fasttext、T5、GPT-3 模型对比"></a>Fasttext、T5、GPT-3 模型对比</h2><p>在探索预训练模型的应用中，我们不仅限于OpenAI的API，还可以考虑使用一些开源的模型。今天，我们将重点介绍两个开源预训练模型，并探讨它们在文本向量化方面的表现是否能与OpenAI的API媲美。</p>
<p>首先是Facebook的FastText，这个模型是基于Word2Vec思想的延伸，能够将单词表示成向量。通过FastText，我们可以将文本转化为向量表示，以便在后续的自然语言处理任务中使用。</p>
<p>第二个模型是Google的T5，T5的全称是Text-to-Text Transfer Transformer，它是一种适用于迁移学习的模型。迁移学习是指将T5生成的向量结果用于其他自然语言处理任务的机器学习过程。在许多最新的研究论文中，T5常被用作预训练模型进行微调和训练，或者作为基准模型用于对比和评估。</p>
<p>通过研究这两个开源预训练模型，我们可以更全面地了解它们在文本向量化方面的性能和适用性。无论是FastText还是T5，它们都为我们提供了更多选择和灵活性，让我们能够根据具体任务的需求来选择最适合的预训练模型。预训练模型的开源社区不断发展壮大，为我们提供了更多的工具和资源，助力我们在自然语言处理领域取得更好的效果。</p>
<h3 id="Fasttext-效果测试"><a href="#Fasttext-效果测试" class="headerlink" title="Fasttext 效果测试"></a>Fasttext 效果测试</h3><p>在开始使用FastText之前，我们需要先安装Python包管理工具中的FastText和Gensim两个包。如果您使用的是Conda作为Python包管理工具，可以按照以下示例代码安装所需的包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge fasttext gensim</span><br></pre></td></tr></table></figure>

<p>如果您使用的是PIP或其他Python包管理工具，可以将上述代码中的”conda”替换为相应的包管理工具名称，并执行相应的安装命令。</p>
<blockquote>
<p>如果你用conda无法安装，也可以尝试PIP安装，效果是一样的。</p>
</blockquote>
<p>通过执行上述代码，我们可以安装所需的FastText和Gensim包。这两个包提供了在文本向量化过程中使用的工具和函数。安装完成后，我们将能够顺利进行FastText的使用，并进行相关的自然语言处理任务。请确保在运行代码之前先完成包的安装。</p>
<p>接下来，我们需要将与FastText对应的模型下载到本地。这些开源库和相关论文由Facebook和Google等国外公司发布，因此其在英语处理方面的效果较好。我们将下载与英语相关的模型，名称为”cc.en.300.bin”。我在这里提供了相应模型的<a target="_blank" rel="noopener" href="https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz">下载链接</a>。</p>
<p>在这里，可以找到<a target="_blank" rel="noopener" href="https://fasttext.cc/docs/en/crawl-vectors.html">更多的Fasttext模型</a></p>
<p><img src="http://qiniu.hivan.me/picGo/20230601165525.png?imgNote" alt="notion image"></p>
<p>将模型下载下来之后解压缩，放在你的Notebook同目录下。</p>
<p><img src="http://qiniu.hivan.me/picGo/20230601165531.png?imgNote" alt="notion image"></p>
<p>代码的逻辑并不复杂。首先，我们使用Gensim库加载了预训练好的FastText模型。接下来，我们定义了一个函数来获取文本的向量表示。由于FastText学习的是单词的向量而不是句子的向量，同时我们想要测试零样本学习的效果，因此无法进一步根据评论数据对模型进行训练。因此，我们采用了一种常见的方法，将一句话中每个单词的向量相加并取平均，将得到的向量作为整个评论的表示。我们将这个操作封装成了一个名为 <code>get_fasttext_vector</code> 的函数，以供后续程序使用。</p>
<p>在这里，我们还是拿<a target="_blank" rel="noopener" href="https://www.notion.so/608ba17576684ea69d45fa987c2bfe7c">第二讲中的数据集</a>，用的是2.5W条亚马逊食物评论的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># load the FastText pre-trained model</span></span><br><span class="line">model = gensim.models.fasttext.load_facebook_model(<span class="string">&#x27;cc.en.300.bin&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_fasttext_vector</span>(<span class="params">line</span>):</span><br><span class="line">    vec = np.zeros(<span class="number">300</span>) <span class="comment"># Initialize an empty 300-dimensional vector</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> line.split():</span><br><span class="line">        vec += model.wv[word]</span><br><span class="line">    vec /= <span class="built_in">len</span>(line.split()) <span class="comment"># Take the average over all words in the line</span></span><br><span class="line">    <span class="keyword">return</span> vec</span><br></pre></td></tr></table></figure>

<p>而对应的零样本学习，我们还是和第 02 讲一样，将需要进行情感判断的评论分别与 “An Amazon review with a positive sentiment.” 以及 “An Amazon review with a negative sentiment.” 这两句话进行向量计算，算出它们之间的余弦距离。</p>
<p><strong>离前一个近，我们就认为是正面情感，离后一个近就是负面情感</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">positive_text = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Wanted to save some to bring to my Chicago family but my North Carolina family ate all 4 boxes before I could pack. These are excellent...could serve to anyone</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">negative_text = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">First, these should be called Mac - Coconut bars, as Coconut is the #2 ingredient and Mango is #3.  Second, lots of people don&#x27;t like coconut.  I happen to be allergic to it.  Word to Amazon that if you want happy customers to make things like this more prominent.  Thanks.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">positive_example_in_fasttext = get_fasttext_vector(positive_text)</span><br><span class="line">negative_example_in_fasttext = get_fasttext_vector(negative_text)</span><br><span class="line"></span><br><span class="line">positive_review_in_fasttext = get_fasttext_vector(<span class="string">&quot;An Amazon review with a positive sentiment.&quot;</span>)</span><br><span class="line">negative_review_in_fasttext = get_fasttext_vector(<span class="string">&#x27;An Amazon review with a negative sentiment.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> openai.embeddings_utils <span class="keyword">import</span> cosine_similarity</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_fasttext_score</span>(<span class="params">sample_embedding</span>):</span><br><span class="line">  <span class="keyword">return</span> cosine_similarity(sample_embedding, positive_review_in_fasttext) - cosine_similarity(sample_embedding, negative_review_in_fasttext)</span><br><span class="line"></span><br><span class="line">positive_score = get_fasttext_score(positive_example_in_fasttext)</span><br><span class="line">negative_score = get_fasttext_score(negative_example_in_fasttext)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Fasttext好评例子的评分 : %f&quot;</span> % (positive_score))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Fasttext差评例子的评分 : %f&quot;</span> % (negative_score))</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<blockquote>
<p>Fasttext好评例子的评分 : -0.000544 Fasttext差评例子的评分 : 0.000369</p>
</blockquote>
<p>在亚马逊食品评论数据集中，我们选择了一个正面评分为5分的例子和一个负面评分为1分的例子进行测试。然而，结果令人遗憾的是，通过零样本学习的方式，程序对这两个例子的判断都是错误的。然而，仔细思考一下，这样的结果其实是可以理解的。因为我们的方法是将整个句子的向量表示取平均，这意味着可能会出现之前提到的单词顺序不同的问题。例如，“not good, really bad” 和 “not bad, really good” 在这种情况下，意思完全不同，但向量表示完全相同。此外，我们对比的正面情感和负面情感两个句子之间只有一个单词的差异（如positive&#x2F;negative）。如果我们只考虑单词的出现情况，并将它们的向量取平均，这种策略如果真的能够取得良好的效果，那反而应该怀疑是否存在Bug。</p>
<h3 id="T5效果测试"><a href="#T5效果测试" class="headerlink" title="T5效果测试"></a>T5效果测试</h3><p>Fasttext并没有取得令人满意的结果，这可以理解，毕竟Word2Vec已经是10年前的技术了。那么，接下来我们来看看使用了目前最流行的Transformer结构的T5模型的效果如何。</p>
<p>T5模型的全称是Text-to-Text Transfer Transformer，意思是“文本到文本的迁移Transformer”。这个模型的设计初衷是为了方便在预训练之后将其迁移到其他任务上。当T5模型发布时，它在各种数据集上的评测表现都名列前茅。</p>
<p>T5模型中最大的模型有110亿个参数，同样基于Transformer。虽然相较于GPT-3的1750亿个参数要小很多，但对硬件性能的要求也不低。因此，我们首先测试一下T5-Small这个小型模型的效果。</p>
<p>在实际运行代码之前，我们需要安装相应的Python包。这里我们分别安装了SentencePiece和PyTorch。在安装PyTorch时，我还顺便安装了Torchvision，因为在后续课程中会用到它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge transformers</span><br><span class="line">conda install pytorch torchvision torchaudio -c pytorch-nightly</span><br><span class="line">conda install -c conda-forge sentencepiece</span><br></pre></td></tr></table></figure>

<blockquote>
<p>别问我为什么选择 <code>pytorch-nightly</code> , 简而言之，我用的是M1.</p>
</blockquote>
<p>我们需要执行的代码非常简单。首先，我们加载预训练的T5模型的分词器（Tokenizer）以及相应的模型。接下来，我们定义了一个名为get_t5_vector的函数，它接收一段文本输入，并使用分词器对其进行分词，生成一个序列。然后，我们将序列输入到模型的编码器部分进行编码。编码后的结果仍然是分词后的每个词的向量。与之前的Fasttext不同的是，这里的向量会随着位置和相邻词的不同而产生变化。因此，我们将这些向量取平均，作为整段文本的向量表示。需要注意的是，尽管我们进行了平均操作，但这里每个词的向量仍然包含了顺序信息的语义信息。值得注意的是，执行这段代码可能会稍慢一些。因为在首次加载模型时，Transformer库会将模型下载到本地并进行缓存，这个下载过程可能需要一些时间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> T5Tokenizer, T5Model</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># load the T5 tokenizer and model</span></span><br><span class="line">tokenizer = T5Tokenizer.from_pretrained(<span class="string">&#x27;t5-small&#x27;</span>, model_max_length=<span class="number">512</span>)</span><br><span class="line">model = T5Model.from_pretrained(<span class="string">&#x27;t5-small&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set the model to evaluation model</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># encode the input sentence</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_t5_vector</span>(<span class="params">line</span>):</span><br><span class="line">    input_ids = tokenizer.encode(line, return_tensors = <span class="string">&#x27;pt&#x27;</span>, max_length=<span class="number">512</span>, truncation = <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># generate the vector representation</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model.encoder(input_ids = input_ids)</span><br><span class="line">        vector = outputs.last_hidden_state.mean(dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> vector[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>有了模型和通过模型获取的向量数据，我们就可以再试一试前面的零样本学习的方式，来看看效果怎么样了。我们简单地把之前获取向量和计算向量的函数调用，都换成新的 get_t5_vector，运行一下就能看到结果了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">positive_review_in_t5 = get_t5_vector(<span class="string">&quot;An Amazon review with a positive sentiment.&quot;</span>)</span><br><span class="line">negative_review_in_t5 = get_t5_vector(<span class="string">&#x27;An Amazon review with a negative sentiment.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_t5</span>():</span><br><span class="line">  positive_example_in_t5 = get_t5_vector(positive_text)</span><br><span class="line">  negative_example_in_t5 = get_t5_vector(negative_text)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">get_t5_score</span>(<span class="params">sample_embedding</span>):</span><br><span class="line">    <span class="keyword">return</span> cosine_similarity(sample_embedding, positive_review_in_t5) - cosine_similarity(sample_embedding, negative_review_in_t5)</span><br><span class="line"></span><br><span class="line">  positive_score = get_t5_score(positive_example_in_t5)</span><br><span class="line">  negative_score = get_t5_score(negative_example_in_t5)</span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;T5好评例子的评分 : %f&quot;</span> % (positive_score))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;T5差评例子的评分 : %f&quot;</span> % (negative_score))</span><br><span class="line"></span><br><span class="line">test_t5()</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<blockquote>
<p>T5好评例子的评分 : -0.010294 T5差评例子的评分 : -0.008990</p>
</blockquote>
<p>然而，不幸的是，我们得到的结果并不太理想。两个例子都被错误地判断为负面情绪，并且正面评价的分数还更低。不过，不要着急，这可能是因为我们使用的模型太小了。毕竟，在T5论文中，110亿个参数的大模型在各项评估中表现卓越，而我们使用的是T5-Small，这个相同架构下参数只有6000万个的小模型。要训练110亿个参数需要很长时间。因此，为什么不尝试一下参数为2.2亿个的T5-Base模型呢？使用起来也很简单，只需要将模型名称从T5-Small更改为T5-Base，其他代码无需修改，然后重新运行一遍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = T5Tokenizer.from_pretrained(<span class="string">&#x27;t5-base&#x27;</span>, model_max_length=<span class="number">512</span>)</span><br><span class="line">model = T5Model.from_pretrained(<span class="string">&#x27;t5-base&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set the model to evaluation mode</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># encode the input sentence</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_t5_vector</span>(<span class="params">line</span>):</span><br><span class="line">    input_ids = tokenizer.encode(line, return_tensors=<span class="string">&#x27;pt&#x27;</span>, max_length=<span class="number">512</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># generate the vector representation</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model.encoder(input_ids=input_ids)</span><br><span class="line">        vector = outputs.last_hidden_state.mean(dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> vector[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">positive_review_in_t5 = get_t5_vector(<span class="string">&quot;An Amazon review with a positive sentiment.&quot;</span>)</span><br><span class="line">negative_review_in_t5 = get_t5_vector(<span class="string">&#x27;An Amazon review with a negative sentiment.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_t5()</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<blockquote>
<p>T5好评例子的评分 : 0.010347 T5差评例子的评分 : -0.023935</p>
</blockquote>
<p>这次的结果似乎符合我们的期望，好评被判断为正面情感，差评被判断为负面情感。然而，也许我们只是运气好，在这几个例子上看到了效果。因此，接下来让我们将整个数据集中的1分和2分的差评以及4分和5分的好评都提取出来进行观察。在OpenAI的API中，使用的Embedding可以达到95%以上的准确率，让我们看看使用这个参数为2.2亿的T5-Base模型会得到什么样的结果。</p>
<p>相应的代码并不复杂，基本上与第02讲中OpenAI提供的Embedding代码相似。只是通过pandas库，根据评论的文本字段，对T5模型进行计算，然后将结果存储到DataFrame的”t5_embedding”列中。</p>
<p>同样地，我们还需要使用T5模型获得”An Amazon review with a positive sentiment.”和”An Amazon review with a negative sentiment.”这两个句子的Embedding。然后，我们将用户评论的Embedding与这两个句子的Embedding计算余弦距离，以判断这些评论是正面还是负面。</p>
<p>最后，使用Scikit-learn库中的分类报告类将评估结果打印出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">datafile_path = <span class="string">&quot;./data/fine_food_reviews_with_embeddings_1k.csv&quot;</span></span><br><span class="line">df = pd.read_csv(datafile_path)</span><br><span class="line"></span><br><span class="line">df[<span class="string">&quot;t5_embedding&quot;</span>] = df.Text.apply(get_t5_vector)</span><br><span class="line"><span class="comment"># convert 5-star rating to binary sentiment</span></span><br><span class="line">df = df[df.Score != <span class="number">3</span>]</span><br><span class="line">df[<span class="string">&quot;sentiment&quot;</span>] = df.Score.replace(&#123;<span class="number">1</span>: <span class="string">&quot;negative&quot;</span>, <span class="number">2</span>: <span class="string">&quot;negative&quot;</span>, <span class="number">4</span>: <span class="string">&quot;positive&quot;</span>, <span class="number">5</span>: <span class="string">&quot;positive&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> PrecisionRecallDisplay</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_embeddings_approach</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">label_score</span>(<span class="params">review_embedding</span>):</span><br><span class="line">        <span class="keyword">return</span> cosine_similarity(review_embedding, positive_review_in_t5) - cosine_similarity(review_embedding, negative_review_in_t5)</span><br><span class="line"></span><br><span class="line">    probas = df[<span class="string">&quot;t5_embedding&quot;</span>].apply(<span class="keyword">lambda</span> x: label_score(x))</span><br><span class="line">    preds = probas.apply(<span class="keyword">lambda</span> x: <span class="string">&#x27;positive&#x27;</span> <span class="keyword">if</span> x&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;negative&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    report = classification_report(df.sentiment, preds)</span><br><span class="line">    <span class="built_in">print</span>(report)</span><br><span class="line"></span><br><span class="line">    display = PrecisionRecallDisplay.from_predictions(df.sentiment, probas, pos_label=<span class="string">&#x27;positive&#x27;</span>)</span><br><span class="line">    _ = display.ax_.set_title(<span class="string">&quot;2-class Precision-Recall curve&quot;</span>)</span><br><span class="line"></span><br><span class="line">evaluate_embeddings_approach()</span><br></pre></td></tr></table></figure>

<p>输出结果如图：</p>
<p><img src="http://qiniu.hivan.me/picGo/20230601165539.png?imgNote" alt="notion image"></p>
<p>实验结果显示，使用T5模型的效果还算不错，对于所有样本的准确率达到了90%。然而，在处理较为困难的差评判断时，T5模型的表现要比直接使用OpenAI提供的Embedding差很多，整体精度仅为60%。与此相比，当我们观察整体模型准确率时，OpenAI的Embedding能够达到96%，相比于这里的90%，要更好一些。</p>
<p>以下是我们上节做的测试结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">								precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">    negative       <span class="number">0.98</span>      <span class="number">0.73</span>      <span class="number">0.84</span>       <span class="number">136</span></span><br><span class="line">    positive       <span class="number">0.96</span>      <span class="number">1.00</span>      <span class="number">0.98</span>       <span class="number">789</span></span><br><span class="line"></span><br><span class="line">    accuracy                           <span class="number">0.96</span>       <span class="number">925</span></span><br><span class="line">   macro avg       <span class="number">0.97</span>      <span class="number">0.86</span>      <span class="number">0.91</span>       <span class="number">925</span></span><br><span class="line">weighted avg       <span class="number">0.96</span>      <span class="number">0.96</span>      <span class="number">0.96</span>       <span class="number">925</span></span><br></pre></td></tr></table></figure>

<p>当然，这个分数还是相当不错的，可以作为一个合格的情感分析分类器的基准线。毕竟，我们采用的是零样本分类的方法，没有对需要分类的数据进行任何训练，直接使用预训练模型提供的向量，并根据距离进行判断。因此，看起来更大规模的预训练模型确实很有用，可以取得更好的效果。此外，当因为成本或网络延迟等原因无法方便地使用OpenAI的API时，如果只需要获取文本的嵌入向量，使用T5这样的开源模型的效果也相当不错。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><img src="http://qiniu.hivan.me/picGo/20230601165544.png?imgNote" alt="notion image"></p>
<p>最后，我们来总结一下本讲的内容。在本讲中，我们使用了 Fasttext、T5-small 和 T5-base 这三个预训练模型进行了零样本分类的测试。在相同的食物评论数据集上，使用只学习了单词向量表示的 Fasttext，效果非常糟糕。当我们转而使用基于 Transformer 的 T5 模型时，即使是规模较小的 T5-small（6000 万参数），效果也不理想。然而，当我们尝试了具有 2.2 亿参数的 T5-base 模型时，结果有所改善。但是，这仍然远远落后于直接使用 OpenAI 的 API 的效果。这表明，模型的大小在情感分析这样简单的问题上也能产生显著差异。课后可以进行练习来进一步巩固所学知识。</p>
<h2 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h2><p>我们在尝试使用 T5-base 这个模型之后，下了个判断认为大一点的模型效果更好。不过，其实我们并没有在整个数据集上使用 T5-small 这个模型做评测，你能试着修改一下代码，用 T5-small 测试一下整个数据集吗？测试下来的效果又是怎样的呢？</p>
<p>我们使用 Fasttext 的时候，把所有的单词向量平均一下，用来做情感分析效果很糟糕。那么什么样的分类问题，可以使用这样的方式呢？给你一个小提示，你觉得什么样的文本分类，只关心出现的单词是什么，而不关心它们的顺序？</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://hivan.me/GPT-3-VS-Other-Model/" data-id="clid25e9u000l5xj74dbd6w6a" data-title="04 GPT-3/4对比其他模型胜在哪？" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/" rel="tag">AI</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/Text-Classification/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          05 为文本分类
        
      </div>
    </a>
  
  
    <a href="/Let-s-Build-a-Chatbot/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">03 提示语，做个聊天机器人</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/develop/">develop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/software/">software</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/web/">web</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ChatGPT/" rel="tag">ChatGPT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chrome/" rel="tag">Chrome</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gmail/" rel="tag">Gmail</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Google/" rel="tag">Google</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Google-Plus/" rel="tag">Google Plus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gplus/" rel="tag">Gplus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mac/" rel="tag">Mac</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mail/" rel="tag">Mail</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Model/" rel="tag">Model</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NFC/" rel="tag">NFC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/O2O/" rel="tag">O2O</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PS/" rel="tag">PS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/QQ/" rel="tag">QQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SNS/" rel="tag">SNS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Stable-Diffusion/" rel="tag">Stable Diffusion</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tool/" rel="tag">Tool</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translate/" rel="tag">Translate</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/allove/" rel="tag">allove</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/app/" rel="tag">app</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/avn/" rel="tag">avn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/" rel="tag">blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bluehost/" rel="tag">bluehost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/code/" rel="tag">code</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/css/" rel="tag">css</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/device/" rel="tag">device</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/feed/" rel="tag">feed</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flickr/" rel="tag">flickr</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/forward/" rel="tag">forward</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gmail/" rel="tag">gmail</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/habari/" rel="tag">habari</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/" rel="tag">hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ifttt/" rel="tag">ifttt</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/install/" rel="tag">install</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ireader/" rel="tag">ireader</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/istef/" rel="tag">istef</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/javascript/" rel="tag">javascript</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/location/" rel="tag">location</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/more/" rel="tag">more</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/net/" rel="tag">net</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nitrous/" rel="tag">nitrous</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/node/" rel="tag">node</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pageflakes/" rel="tag">pageflakes</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/picasaweb/" rel="tag">picasaweb</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/plugin/" rel="tag">plugin</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/reader/" rel="tag">reader</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/research/" rel="tag">research</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/search/" rel="tag">search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sms/" rel="tag">sms</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/telnet/" rel="tag">telnet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/twitter/" rel="tag">twitter</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/url/" rel="tag">url</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/version/" rel="tag">version</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/windows-8/" rel="tag">windows 8</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/wordpress/" rel="tag">wordpress</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%B8%96%E7%95%8C%E6%9D%AF/" rel="tag">世界杯</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BE%AE%E5%8D%9A/" rel="tag">微博</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%B6%E5%8C%BA/" rel="tag">时区</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A4%BE%E4%BA%A4/" rel="tag">社交</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%8A%B1%E5%84%BF%E5%BC%80%E4%BA%86/" rel="tag">花儿开了</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A1%A8%E6%83%85/" rel="tag">表情</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%87%E6%BB%A4%E5%99%A8/" rel="tag">过滤器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%9B%E5%BA%A6/" rel="tag">进度</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI/" style="font-size: 20px;">AI</a> <a href="/tags/ChatGPT/" style="font-size: 10px;">ChatGPT</a> <a href="/tags/Chrome/" style="font-size: 10px;">Chrome</a> <a href="/tags/Gmail/" style="font-size: 10px;">Gmail</a> <a href="/tags/Google/" style="font-size: 18px;">Google</a> <a href="/tags/Google-Plus/" style="font-size: 10px;">Google Plus</a> <a href="/tags/Gplus/" style="font-size: 10px;">Gplus</a> <a href="/tags/Mac/" style="font-size: 10px;">Mac</a> <a href="/tags/Mail/" style="font-size: 10px;">Mail</a> <a href="/tags/Model/" style="font-size: 10px;">Model</a> <a href="/tags/NFC/" style="font-size: 10px;">NFC</a> <a href="/tags/O2O/" style="font-size: 10px;">O2O</a> <a href="/tags/PS/" style="font-size: 10px;">PS</a> <a href="/tags/Python/" style="font-size: 16px;">Python</a> <a href="/tags/QQ/" style="font-size: 10px;">QQ</a> <a href="/tags/SNS/" style="font-size: 10px;">SNS</a> <a href="/tags/Stable-Diffusion/" style="font-size: 12px;">Stable Diffusion</a> <a href="/tags/Tool/" style="font-size: 10px;">Tool</a> <a href="/tags/Translate/" style="font-size: 10px;">Translate</a> <a href="/tags/allove/" style="font-size: 10px;">allove</a> <a href="/tags/app/" style="font-size: 10px;">app</a> <a href="/tags/avn/" style="font-size: 10px;">avn</a> <a href="/tags/blog/" style="font-size: 10px;">blog</a> <a href="/tags/bluehost/" style="font-size: 10px;">bluehost</a> <a href="/tags/code/" style="font-size: 10px;">code</a> <a href="/tags/css/" style="font-size: 10px;">css</a> <a href="/tags/device/" style="font-size: 10px;">device</a> <a href="/tags/feed/" style="font-size: 10px;">feed</a> <a href="/tags/flickr/" style="font-size: 10px;">flickr</a> <a href="/tags/forward/" style="font-size: 10px;">forward</a> <a href="/tags/gmail/" style="font-size: 10px;">gmail</a> <a href="/tags/habari/" style="font-size: 16px;">habari</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/ifttt/" style="font-size: 12px;">ifttt</a> <a href="/tags/install/" style="font-size: 10px;">install</a> <a href="/tags/ireader/" style="font-size: 10px;">ireader</a> <a href="/tags/istef/" style="font-size: 10px;">istef</a> <a href="/tags/javascript/" style="font-size: 10px;">javascript</a> <a href="/tags/location/" style="font-size: 10px;">location</a> <a href="/tags/more/" style="font-size: 10px;">more</a> <a href="/tags/net/" style="font-size: 10px;">net</a> <a href="/tags/nitrous/" style="font-size: 10px;">nitrous</a> <a href="/tags/node/" style="font-size: 10px;">node</a> <a href="/tags/pageflakes/" style="font-size: 10px;">pageflakes</a> <a href="/tags/picasaweb/" style="font-size: 10px;">picasaweb</a> <a href="/tags/plugin/" style="font-size: 10px;">plugin</a> <a href="/tags/reader/" style="font-size: 10px;">reader</a> <a href="/tags/research/" style="font-size: 10px;">research</a> <a href="/tags/search/" style="font-size: 10px;">search</a> <a href="/tags/sms/" style="font-size: 10px;">sms</a> <a href="/tags/telnet/" style="font-size: 10px;">telnet</a> <a href="/tags/twitter/" style="font-size: 12px;">twitter</a> <a href="/tags/url/" style="font-size: 10px;">url</a> <a href="/tags/version/" style="font-size: 10px;">version</a> <a href="/tags/windows-8/" style="font-size: 10px;">windows 8</a> <a href="/tags/wordpress/" style="font-size: 14px;">wordpress</a> <a href="/tags/%E4%B8%96%E7%95%8C%E6%9D%AF/" style="font-size: 10px;">世界杯</a> <a href="/tags/%E5%BE%AE%E5%8D%9A/" style="font-size: 10px;">微博</a> <a href="/tags/%E6%97%B6%E5%8C%BA/" style="font-size: 10px;">时区</a> <a href="/tags/%E7%A4%BE%E4%BA%A4/" style="font-size: 10px;">社交</a> <a href="/tags/%E8%8A%B1%E5%84%BF%E5%BC%80%E4%BA%86/" style="font-size: 10px;">花儿开了</a> <a href="/tags/%E8%A1%A8%E6%83%85/" style="font-size: 10px;">表情</a> <a href="/tags/%E8%BF%87%E6%BB%A4%E5%99%A8/" style="font-size: 10px;">过滤器</a> <a href="/tags/%E8%BF%9B%E5%BA%A6/" style="font-size: 10px;">进度</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">四月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">三月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">二月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">三月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">九月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">八月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">六月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">七月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">十一月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/06/">六月 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/12/">十二月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/08/">八月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/03/">三月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/07/">七月 2012</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/04/">四月 2012</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/01/">一月 2012</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2011/11/">十一月 2011</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2011/10/">十月 2011</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2011/07/">七月 2011</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2010/12/">十二月 2010</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2010/04/">四月 2010</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2009/10/">十月 2009</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2009/02/">二月 2009</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2008/06/">六月 2008</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2006/10/">十月 2006</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2006/09/">九月 2006</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2006/06/">六月 2006</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/Use-AI-to-write-a-snake-game/">利用AI写一个『贪吃蛇游戏』</a>
          </li>
        
          <li>
            <a href="/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/">13 使用多步提示语让AI帮你写测试</a>
          </li>
        
          <li>
            <a href="/AI-create-a-excel-plugin/">12 AI帮你写个小插件，轻松处理Excel文件</a>
          </li>
        
          <li>
            <a href="/Save-costs-with-an-open-source-model/">11 用好开源模型节约成本</a>
          </li>
        
          <li>
            <a href="/Use-AI-to-index-and-analyze-documents-and-images/">10 利用AI索引并分析文献和图片</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 Hivan Du<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>