<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>24. 深度学习进阶 - 矩阵运算的维度和激活函数 - 茶桁.MAMT</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="茶桁.MAMT"><meta name="msapplication-TileImage" content="https://qiniu.hivan.me/picGo/20230601174411.png?imgNote"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="茶桁.MAMT"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Hi，你好。我是茶桁。"><meta property="og:type" content="blog"><meta property="og:title" content="24. 深度学习进阶 - 矩阵运算的维度和激活函数"><meta property="og:url" content="https://hivan.me/24.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"><meta property="og:site_name" content="茶桁.MAMT"><meta property="og:description" content="Hi，你好。我是茶桁。"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2023-11-25T23:30:00.000Z"><meta property="article:modified_time" content="2023-11-29T06:18:52.997Z"><meta property="article:author" content="Hivan Du"><meta property="article:tag" content="AI"><meta property="twitter:card" content="summary"><meta property="twitter:creator" content="@hivan"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hivan.me/24.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"},"headline":"24. 深度学习进阶 - 矩阵运算的维度和激活函数","image":[],"datePublished":"2023-11-25T23:30:00.000Z","dateModified":"2023-11-29T06:18:52.997Z","author":{"@type":"Person","name":"Hivan Du"},"publisher":{"@type":"Organization","name":"茶桁.MAMT","logo":{"@type":"ImageObject","url":"https://hivan.me/img/logo.svg"}},"description":"Hi，你好。我是茶桁。"}</script><link rel="canonical" href="https://hivan.me/24.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?f91b64734fdc7bfb999e48f9248d44dd";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-ZFB6CVWZFJ" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-ZFB6CVWZFJ');</script><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="茶桁.MAMT" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-11-25T23:30:00.000Z" title="11/26/2023, 7:30:00 AM">2023-11-26</time>发表</span><span class="level-item"><a class="link-muted" href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a><span> / </span><a class="link-muted" href="/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/">核心能力基础</a></span></div></div><h1 class="title is-3 is-size-4-mobile">24. 深度学习进阶 - 矩阵运算的维度和激活函数</h1><div class="content"><p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030256914.png"
alt="Alt text" /></p>
<p>Hi，你好。我是茶桁。</p>
<span id="more"></span>
<p>咱们经过前一轮的学习，已经完成了一个小型的神经网络框架。但是这也只是个开始而已，在之后的课程中，针对深度学习我们需要进阶学习。</p>
<p>我们要学到超参数，优化器，卷积神经网络等等。看起来，任务还是蛮重的。</p>
<p>行吧，让我们开始。</p>
<h2 id="矩阵运算的维度">矩阵运算的维度</h2>
<p>首先，我们之前写了一份拓朴排序的代码。那我们是否了解在神经网络中拓朴排序的作用。我们前面讲过的内容大家可以回忆一下，拓朴排序在咱们的神经网络中的作用不是为了计算方便，是为了能计算。</p>
<p>换句话说，没有拓朴排序的话，根本就没法计算了。Tensorflow和PyTourh最大的区别就是，Tensorflow在运行之前必须得把拓朴排序建好，PyTorch是在运行的过程中自己根据我们的连接状况一边运行一边建立。但是它们都有拓朴排序。</p>
<p>拓朴排序后要进行计算，那就要提到维度问题，在进行机器学习的时候一定要确保我们矩阵运算的维度正确。我们来看一下示例就明白我要说的了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.from_numpy(np.random.random(size=(<span class="hljs-number">4</span>, <span class="hljs-number">10</span>)))<br><span class="hljs-built_in">print</span>(x.shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure>
<p>假如说，现在我们生成了一个4，10的矩阵，也就是4行10列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>linear = nn.Linear(in_features=<span class="hljs-number">10</span>, out_features=<span class="hljs-number">5</span>).double()<br><span class="hljs-built_in">print</span>(linear(x).shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<br></code></pre></td></tr></table></figure>
<p>然后我们来给他定义一个线性变化，<code>in_features=10</code>，这个就是必须的,
然后，out_features=5，假如把它分成5类。</p>
<p>这个时候，你看他就变成一个四行五列的一个东西了。</p>
<p>刚才我们说了，<code>in_features=10</code>是必须的，如果这个值我们设置成其他的，比如说8，那就不行了，运行不了。会收到警告：<code>mat1 and mat2 shapes cannot be multiplied (4x10 and 8x5)</code></p>
<p>我们再给它来一个Softmax</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">nonlinear = nn.Softmax()<br><span class="hljs-built_in">print</span>(nonlinear(linear(x)))<br></code></pre></td></tr></table></figure>
<p>这样，我们就得到了一个4*5的概率分布。</p>
<p>我们把这个非线性函数换一下，换成Sigmoid, 之前的Softmax赋值给yhat,
咱们做一个多层的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">yhat = nn.Softmax()<br>nonlinear = nn.Sigmoid()<br>linear2 = nn.linear(in_features=n, out_features=<span class="hljs-number">8</span>).double()<br><br><span class="hljs-built_in">print</span>(yhat(linear2(nonlinear(linear(x)))))<br></code></pre></td></tr></table></figure>
<p>好，这个时候，我还并没有给<code>in_features</code>赋值，我们来想想，这个时候应该赋值是多少？也就是说，我们现在的linear2到底传入的特征是多少？</p>
<p>我们这里定义的<code>linear</code>和<code>linear2</code>其实就是<code>w*x+b</code>。</p>
<p>那这里我们来推一下，第一次使用linear的时候，我们得到了4*5的矩阵对吧？nonlinear并没有改变矩阵的维度。现在linear2中，那我们<code>in_features</code>赋值就得是5对吧？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">yhat = nn.Softmax()<br>nonlinear = nn.Sigmoid()<br>linear2 = nn.linear(in_features=<span class="hljs-number">5</span>, out_features=<span class="hljs-number">8</span>).double()<br><br><span class="hljs-built_in">print</span>(yhat(linear2(nonlinear(linear(x)))).shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">8</span>])<br></code></pre></td></tr></table></figure>
<p>然后我们就得到了一个<code>4*8</code>的维度的矩阵。</p>
<p>那其实在PyTorch里提供了一种比较简单的方法，就叫做<code>Sequential</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">model = nn.Sequential(<br>    nn.Linear(in_features=<span class="hljs-number">10</span>, out_features=<span class="hljs-number">5</span>).double(),<br>    nn.Sigmoid(),<br>    nn.Linear(in_features=<span class="hljs-number">5</span>, out_features=<span class="hljs-number">8</span>).double(),<br>    nn.Softmax(),<br>)<br><br><span class="hljs-built_in">print</span>(model(x).shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">8</span>])<br></code></pre></td></tr></table></figure>
<p>这样，我们就把刚才几个函数方法按顺序都一个一个的写在<code>Sequential</code>里，那其实刚才的过程，也就是解释了这个方法的原理。</p>
<p>接着，我们来写一个<code>ytrue</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">ytrue = torch.randint(<span class="hljs-number">8</span>, (<span class="hljs-number">4</span>, ))<br>loss_fn = nn.CrossEntropyLoss()<br><br><span class="hljs-built_in">print</span>(model(x).shape)<br><span class="hljs-built_in">print</span>(ytrue.shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">8</span>])<br>torch.Size([<span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure>
<p>现在ytrue就是CrossEntropyLoss输入的一个label值。</p>
<p>然后我们就可以进行反向传播了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">loss.backward()<br><br><span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters():<br>  <span class="hljs-built_in">print</span>(p, p.grad)<br>  <br>---<br>Parameter containing:<br>tensor([...])<br>...<br></code></pre></td></tr></table></figure>
<p>求解反向传播之后就可以得到它的梯度了。然后再经过一轮一轮的训练，就可以把梯度稳定在某个值，这就是神经网络进行学习的一个过程。那主要是在这个过程中，一定要注意矩阵前后的大小。</p>
<h2 id="激活函数">激活函数</h2>
<p>然后我们来看看激活函数的重要性。</p>
<p>在我们之前的课程中，我们提到过一个概念「激活函数」，不知道大家还有没有印象。那么激活函数的作用是什么呢？
是实现非线性拟合对吧？</p>
<p>打比方来说，如果我们现在要拟合一个函数<code>f(x) = w*x+b</code>,
你把它再给送到一个g(x)，
再比如<code>g(x)=w2*x+b</code>，我们来做一个拟合，那么g(f(x)),
那是不是还是一样，<code>g(f(x)) = w2*(w*x+b)+b</code>,
然后就变成<code>w2*w*x + w2*b + b</code>，
那其实这个就还是一个线性函数。</p>
<p>我们每一段都给它进行一个线性变化，再进行一个非线性变化，再进行一个线性变化，一段一段这样折起来，理论上它可以拟合任何函数。</p>
<p>这个怎么理解？其实我们如何用已知的函数去拟合函数在高等数学里边是一个一直在学习，一直在研究的东西。学高数的同学应该知道，高数里面有一个著名的东西叫做傅立叶变化，这是一种线性积分变换，用于函数在时域和频域之间的变换。</p>
<p>我们给定任意一个复杂的函数，都可以通过sin和cos来把它拟合出来，其关键思想是任何连续、周期或非周期的函数都可以表示为正弦和余弦函数的组合。通过计算不同频率的正弦和余弦成分的系数an和bn，
我们可以了解一个函数的频谱特性，即它包含那些频率成分。</p>
<p><span class="math display">\[
\begin{align*}
f(x) = a_0 + \sum_{n=1}^0(a_n cos(2\pi nfx) + b_n sin(2\pi n fx))
\end{align*}
\]</span></p>
<p>除此之外，我们还有一个泰勒展开。我在数学篇的时候有仔细讲解过这个部分，大家可以回头去读一下我那篇文章，应该是数学篇第13节课，在那里我曾说过，所有的复杂函数都是用泰勒展开转换成多项式函数计算的。</p>
<p>之前有同学给我私信，也有同学在我文章下留言，说到某个位置看不懂了，还是数学拖了后腿。但是其实只是应用的话无所谓，但是如果想在这个方面有所建树，想要做些不一样的东西出来，还是要把数学的东西好好补一下的。</p>
<p>OK，那其实呢，我们的深度学习本质上其实就是在做这么一件事情，就是来自动拟合，到底是由什么构成的。</p>
<p>大家再来想一下，一个比较重要的，就是反向传播和前向传播。这个我们前面的课程里有详细的讲过，就是，我们的前向传播和反向传播的作用是什么。</p>
<p>那现在我们学完前几节了，回过头来我们想想，前向传播的作用是什么？反向传播的这个作用呢？</p>
<p>现在，假如说我已经训练出来了一个模型，我要用这个模型去预测。那么第一个问题是，预测的时候需不需要求loss？第二个是我需不需要做反向传播？</p>
<p>然后我们再来思考一个问题，如果我们需要求loss对于某个参数wi的偏导<span
class="math inline">\(\frac{\partial loss}{\partial
w_i}\)</span>，那么我们首先需要进行反向传播对吧？那我们在进行反向传播之前，能不能不进行前向传播？</p>
<p>也就是说，我们把这个模型放在这里，一个x，然后输入进去得到一个loss。那么咱们训练了一轮之后，我们能不能在求解的时候不进行前向传播，直接进行反向传播？</p>
<p>我们只要知道，求loss值需要预测值就明白了。</p>
<p>那我们继续来思考，loss值和precision、recall等等的关系是什么？这些是什么？我们之前学习过，这些是评测指标对吧？也就是再问，loss和评测指标的关系是什么？</p>
<p>也就是说，我们能不能用precision，能不能用precision来做我们的loss函数？不能对吧，无法求导。</p>
<p>所以在整个机器学习的过程中，如果要有反向传播、梯度下降，必须得是可导的。像我们所说的MSE是可导的，<code>cross-entropy</code>也是可以求导的。</p>
<p>那如果上过我之前课程的同学应该记得，可求导的的函数需要满足什么条件？光滑性和连续性对吧？连续性呢，是可求导的一个必要条件，但不是充分条件，还必须在某个点附近足够光滑，以使得导数存在。</p>
<p>对于loss函数的设定，第一点，一定是要能求偏导的。第二呢，就是它一定得是一个凸函数:Convex
functions。</p>
<p>那什么叫做凸函数呢？如果一个函数上的任意两点连线上的函数值都不低于这两点的函数值的线段，就称为凸函数。常见的比如线性函数，指数函数，幂函数，绝对值函数等都是凸函数。</p>
<p>想象一下，有一辆车，从a点开到b点，如果这个车在a点到b点的时候方向盘始终是打在一个方向的，那我们就说它是凸函数。</p>
<p>不过在一些情况下有些函数它不是凸函数，就在数学上专门有一个研究领域，Convex
optimization，凸优化其实就是解决对于这种函数怎么样快速的求出他的基值，另外一个就是对于这种非凸函数怎么把它变成凸函数。</p>
<p>不同的激活函数它有什么区别呢？在最早的时候，大家用的是<code>Sigmoid</code>：</p>
<p><span class="math display">\[
\begin{align*}
\sigma(z)=\frac{1}{1+e^{-z}}
\end{align*}
\]</span></p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030258830.png"
alt="Sigmoid" /></p>
<p>为什么最早用Sigmoid，这是因为Sigmoid有个天然的优势，就是它输出是0-1，而且它处处可导。</p>
<p>但是后来Sigmoid的结果有个e^x，指数运算就比较费时，这是第一个问题。第二个问题是Sigmoid的输出虽然是在0~1之间，但是平均值是0.5，对于程序来说，我们希望获得均值等于0，STD等于1。我们往往希望把它变成这样的一种函数，这样的话做梯度下降的时候比较好做。</p>
<p>于是就又提出来了一个更简单的方法，就是反正切函数：<code>Tanh</code>。</p>
<p><span class="math display">\[
\sigma(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\]</span></p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030258831.png"
alt="Tanh" /></p>
<p>它的形式和Sigmoid很像，不同的是平均值，它的平均值是0。现在这个用的也挺多。</p>
<p>但是Tanh和sigmoid一样都有一个小问题，就是它的绝大多数地方loss都等于0,
那么wi大部分时候就没有办法学习，也就不会更新。</p>
<p>为了解决这个问题，就是有人提出来了一种非常简单的方法，就是<code>ReLU</code>：</p>
<p><span class="math display">\[
\begin{align*}
ReLU(z) = \begin{cases} z, z&gt;0 \\ 0, otherwise \end{cases}
\end{align*}
\]</span></p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030258832.png"
alt="Alt text" /></p>
<p>这种方法看似非常简单，但其实非常好用。它就是当一个x值经过ReLU的时候，如果它大于0就还保持原来的值，如果不大于0就直接把它变成0。</p>
<p>这样大家可能会觉得x&lt;0时有这么多值没有办法求导，但其实比起sigmoid来说可求导的范围其实已经变多了。而且你会发现要对他x大于0的地方求偏导非常的简单，就直接等于1。</p>
<p>可以保证它肯定是可以做更新的，而且ReLU这种函数它是大量的被应用在卷积神经网络里边。</p>
<p>在咱们后面的课程中，会讲到卷积，它是有一个卷积核，[F1,F2,F3,F4]然后把它经过ReLU之后，可能会变成[F1，0，F3，0]。那我们只要更新F1，F3就可以了，下一次再经过某种方式，在重新把F2和F4我们重新计算一下。</p>
<p>也就是说现在的<code>wx+b</code>不像以前一样，只有一个<code>w</code>，如果x值等于0，那整个都等于0.
而是我们会有一个矩阵，它部分等于0也没关系。而且它的求导会变得非常的快，比求指数的导数快多了。</p>
<p>那其实这里还有一个小问题，面试的时候可能会问到，就是ReLU其实在0点的时候不可导，怎么办？</p>
<p>这个很简单，可以在函数里边直接设置一下，直接给他一个0的值就可以了，就是在代码里面加一句话。</p>
<p>再后来，又有人提出了一种方法：<code>LeakyRelU</code>：</p>
<p><span class="math display">\[
LeakyReLU(z) = \begin{cases} z, z&gt;0 \\ az, otherwise \end{cases}
\]</span></p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030258833.png"
alt="Alt text" /></p>
<p>它把小于0的这些地方，也加了一个很小的梯度，这样的话大于0的时候partial就恒等于1，小于的时候partial也恒等于一个值，比如定一个<code>a=0.2</code>,
都可以。那这样就可以实现处处有导数。</p>
<p>但是其实用的也不太多，因为我们事实上发现在这种卷积神经网络里边，我们每一次把部分的权重设置成0不更新，反而可以提升它的训练效率，我们反而可以每次把训练focus
on在几个参数上。</p>
<p>好，下节课，咱们来看看初始化的内容。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>24. 深度学习进阶 - 矩阵运算的维度和激活函数</p><p><a href="https://hivan.me/24. 深度学习进阶 - 矩阵运算的维度和激活函数/">https://hivan.me/24. 深度学习进阶 - 矩阵运算的维度和激活函数/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hivan Du</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2023-11-26</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2023-11-29</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/AI/">AI</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=6479444288ae9600196fa98e&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="https://afdian.net/item/72907364008511ee904852540025c377" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://qiniu.hivan.me/picGo/20230601221633.jpeg" alt="支付宝"></span></a><a class="button donate" href="https://www.buymeacoffee.com/hivandu" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="https://patreon.com/user?u=89473430" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="doo@hivan.me"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://qiniu.hivan.me/IMG_4603.JPG" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/25.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">25. 深度学习进阶 - 权重初始化，梯度消失和梯度爆炸</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/23.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%AE%8C%E6%88%90%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A1%86%E6%9E%B6/"><span class="level-item">23. 深度学习 - 多维向量自动求导</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://hivan.me/24.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/';
            this.page.identifier = '24. 深度学习进阶 - 矩阵运算的维度和激活函数/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'hivan' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://www.gravatar.com/avatar/bdff168cf8a71c11d2712a1679a00c54?s=128" alt="茶桁"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">茶桁</p><p class="is-size-6 is-block">AI游民</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shang Hai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">209</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">22</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hivandu" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hivan"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/hivan"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com/hivan"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4NzE4MDQzMg==&amp;action=getalbum&amp;album_id=2932504849574543360&amp;scene=173&amp;from_msgid=2648747980&amp;from_itemidx=1&amp;count=3&amp;nolastread=1&amp;token=1758883909&amp;lang=zh_CN#wechat_redirect"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.zhihu.com/column/c_1424326166602178560" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">塌缩的奇点</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li><li><a class="level is-mobile" href="https://www.zhihu.com/column/hivandu" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">茶桁-知乎</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI%E7%A7%98%E7%B1%8D/"><span class="level-start"><span class="level-item">AI秘籍</span></span><span class="level-end"><span class="level-item tag">67</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI%E7%A7%98%E7%B1%8D/BI/"><span class="level-start"><span class="level-item">BI</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/AI%E7%A7%98%E7%B1%8D/Math/"><span class="level-start"><span class="level-item">Math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/AI%E7%A7%98%E7%B1%8D/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li><li><a class="level is-mobile" href="/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"><span class="level-start"><span class="level-item">核心能力基础</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="level-start"><span class="level-item">从零开始接触人工智能大模型</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-06T23:30:00.000Z">2024-01-07</time></p><p class="title"><a href="/03.%20BI%20-%20XGBoost/">03. BI - XGBoost</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/BI/">BI</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-02T23:30:00.000Z">2024-01-03</time></p><p class="title"><a href="/02.%20BI%20-%20%E7%94%B7%E5%A5%B3%E5%A3%B0%E9%9F%B3%E8%AF%86%E5%88%AB/">02. BI - Project Two, 男女声音识别</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/BI/">BI</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-02T11:00:00.000Z">2024-01-02</time></p><p class="title"><a href="/Tea%20Truss&#039;s%20AI%20cheats%20math%20PDF%20release%20download/">茶桁的AI秘籍 - 数学篇 PDF发布下载</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Math/">Math</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-30T23:30:00.000Z">2023-12-31</time></p><p class="title"><a href="/01.%20BI%20-%20%E5%91%98%E5%B7%A5%E7%A6%BB%E8%81%8C%E9%A2%84%E6%B5%8B/">01. BI - Project one, 员工离职预测</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/BI/">BI</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-26T23:30:00.000Z">2023-12-27</time></p><p class="title"><a href="/33.%20Exercise:%20Captcha%20Recognition/">33. CV练习： 验证码识别</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/">核心能力基础</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a><p class="is-size-7"><span>&copy; 2024 Hivan Du</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>