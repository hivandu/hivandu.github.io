<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>茶桁.MAMT</title>
  
  <subtitle>ChaHeng Notes，codding and writting ~</subtitle>
  <link href="https://hivan.me/atom.xml" rel="self"/>
  
  <link href="https://hivan.me/"/>
  <updated>2023-06-05T04:31:38.442Z</updated>
  <id>https://hivan.me/</id>
  
  <author>
    <name>Hivan Du</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>15. 使用LLMChain连接Google和计算器</title>
    <link href="https://hivan.me/%E4%BD%BF%E7%94%A8LLMChain%E8%BF%9E%E6%8E%A5Google%E5%92%8C%E8%AE%A1%E7%AE%97%E5%99%A8/"/>
    <id>https://hivan.me/%E4%BD%BF%E7%94%A8LLMChain%E8%BF%9E%E6%8E%A5Google%E5%92%8C%E8%AE%A1%E7%AE%97%E5%99%A8/</id>
    <published>2023-06-05T04:23:27.000Z</published>
    <updated>2023-06-05T04:31:38.442Z</updated>
    
    <content type="html"><![CDATA[<p>大家好，我是Hivan.</p><p>在<a href="../使用链式调用简化多步提示语">上一节课</a>中，我们学习了如何使用LangChain这个Python包链式调用OpenAI的API。通过链式调用，我们可以将需要多轮询问AI才能解决的问题封装起来，将需要多轮自然语言调用才能解决的问题变成一个函数调用。</p><span id="more"></span><p>然而，LangChain对我们的帮助远不止于此。最近，ChatGPT发布了Plugins插件机制。通过Plugins，ChatGPT可以浏览整个互联网，还可以接入诸如Wolfram这样的科学计算工具，能够解决许多大语言模型难以解决的问题。不过，这是需要Plus用户才可享用的，并且每一个小时内的对话Token都是有限制的。</p><p>但是，这并不重要，我们通过LangChain也能实现类似的功能。在今天的课程中，我们将继续深入挖掘Langchain，看看它如何解决这些问题。</p><h2 id="解决-ai-数理能力的难题"><strong>解决 AI 数理能力的难题</strong></h2><p>虽然许多人发现 ChatGPT 在回答各种问题时表现得很好，但是当涉及到计算三位数乘法时，它就显得有些力不从心了。它似乎只是快速估算一个数字，而不是真正准确计算。为了解决这个问题，我们需要进一步研究 AI 数学能力的提升。</p><p>让我们看看下面这段代码，我们让 OpenAI 帮我们计算一下427乘以971等于多少。尽管它的计算结果相差无几，但它仍然算错了。这样的错误对于一个小学数学助教来说是难以忍受的。</p><p>因此，我们需要开发一种更强大的 AI 算法，它可以精确计算数字和解决复杂的数学问题。这将有助于开发更全面的 AI 功能，从而提高 AI 在各个领域的应用价值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai, os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;OPENAI_API_KEY&#x27;</span>] = <span class="string">&#x27;OPENAI_API_KEY&#x27;</span></span><br><span class="line">openai.api_key = <span class="string">&quot;OPENAI_API_KEY</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">from langchain.prompts import PromptTemplate</span></span><br><span class="line"><span class="string">from langchain.llms import OpenAI</span></span><br><span class="line"><span class="string">from langchain.chains import LLMChain</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">llm = OpenAI(model_name = &#x27;text-davinci-003&#x27;, max_tokens = 2048, temperature = 0.5)</span></span><br><span class="line"><span class="string">multiply_prompt = PromptTemplate(template = &#x27;请计算一下&#123;question&#125;是多少?&#x27;, input_variables = [&#x27;question&#x27;])</span></span><br><span class="line"><span class="string">math_chain = LLMChain(llm = llm, prompt = multiply_prompt, output_key = &#x27;answer&#x27;)</span></span><br><span class="line"><span class="string">answer = math_chain.run(&#123;&#x27;question&#x27;: &quot;</span><span class="number">427</span>乘以<span class="number">971</span><span class="string">&quot;&#125;)</span></span><br><span class="line"><span class="string">print(&#x27;OpenAI API 说答案是:&#x27;, answer)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">python_answer = 427 * 971</span></span><br><span class="line"><span class="string">print(&#x27;Python 得到的答案是:&#x27;, python_answer)</span></span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">OpenAI API 说答案是: </span><br><span class="line"><span class="number">427</span> x <span class="number">971</span> = <span class="number">417</span>,<span class="number">797</span></span><br><span class="line"></span><br><span class="line">Python 得到的答案是: <span class="number">414617</span></span><br></pre></td></tr></table></figure><p>注：可以看到，OpenAI 给出的结果，答案是错误的。不过，这不是意味着 ChatGPT 就没有用处。实际上，有人很聪明，指出虽然 ChatGPT 直接算这些数学题不行，但是它不是会写代码吗？我们可以让它帮我们写一段利用 Python 计算这个数学式子的代码，从而得出正确的答案。这样，我们可以更好地了解到 ChatGPT 的潜力和用处。</p><p>首先，让我们来分析一下这个数学式子。它包括加减乘除和括号，所以我们可以使用 Python 中的基本数学运算符和括号来计算它。在代码中，我们可以定义变量来代表数学式子中的每个数字和符号，然后使用运算符和括号来计算它们之间的关系。</p><p>例如，我们可以定义变量 a、b、c 和 d 来代表数学式子中的数字，然后使用加减乘除和括号来计算它们之间的关系。具体代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = 2</span><br><span class="line">b = 3</span><br><span class="line">c = 4</span><br><span class="line">d = 5</span><br><span class="line"></span><br><span class="line">result = (a + b) * (c - d)</span><br><span class="line"></span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p>运行以上代码，我们可以得到正确的答案：-10。这样，我们就可以利用 ChatGPT 帮助我们写出计算这个数学式子的 Python 代码，从而得出正确的答案，更好地了解 ChatGPT 的潜力和用处。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">multiply_prompt = PromptTemplate(template = <span class="string">&#x27;请写一段Python程序计算&#123;question&#125;。&#x27;</span>, input_variables = [<span class="string">&#x27;question&#x27;</span>])</span><br><span class="line">math_chain = LLMChain(llm = llm, prompt = multiply_prompt, output_key = <span class="string">&#x27;answer&#x27;</span>)</span><br><span class="line">answer = math_chain.run(&#123;<span class="string">&#x27;question&#x27;</span>: <span class="string">&quot;427乘以971&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(answer)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算427乘以971</span></span><br><span class="line">result = <span class="number">427</span> * <span class="number">971</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>我们不想再手动复制粘贴这段代码到Python解释器或者Notebook里去执行。因此，我们可以在后面调用一个Python解释器来自动完成整个过程，相关代码如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">multiply_prompt = PromptTemplate(template = <span class="string">&#x27;请写一段Python程序计算&#123;question&#125;。&#x27;</span>, input_variables = [<span class="string">&#x27;question&#x27;</span>])</span><br><span class="line">math_chain = LLMChain(llm = llm, prompt = multiply_prompt, output_key = <span class="string">&#x27;answer&#x27;</span>)</span><br><span class="line">answer_code = math_chain.run(&#123;<span class="string">&#x27;question&#x27;</span>: <span class="string">&quot;427乘以971&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.utilities <span class="keyword">import</span> PythonREPL</span><br><span class="line">python_repl = PythonREPL()</span><br><span class="line">result = python_repl.run(answer_code)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">427</span>乘以<span class="number">971</span>的结果是： <span class="number">414617</span></span><br></pre></td></tr></table></figure><p>注意：生成的 Python 脚本正确，可通过调用 Python 解释器获得计算结果。</p><p>可以看出，LangChain 内置了一个实用程序包，其中包含了 PythonREPL 类，可以调用 Python 解释器。如果你仔细观察对应代码的源代码，你会发现它实际上只是简单地调用了系统自带的 exec 方法来执行 Python 代码。除了 PythonREPL，utilities 包还有许多其他类，可以实现许多功能，比如直接运行 Bash 脚本、调用 Google 搜索 API 等等。你可以查看 LangChain 的文档，了解它内置的这些工具类。</p><p>如果你仔细思考一下，你会发现这实际上也是一种链式调用。只不过，调用链中的第二步不仅仅是访问 OpenAI 的 API。因此，对于这些工具能力，LangChain 也将它们封装成 LLMChain 的形式。例如，刚才的数学计算问题是一个先生成 Python 脚本，然后调用 Python 解释器的过程。LangChain 将这个过程封装成了一个名为 LLMMathChain 的 LLMChain。您不需要自己生成代码，再调用 PythonREPL，只需要直接调用 LLMMathChain，它就会在背景中完成所有操作。下面是相应的代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> LLMMathChain</span><br><span class="line">llm_math = LLMMathChain(llm = llm, verbose = <span class="literal">True</span>)</span><br><span class="line">result = llm_math.run(<span class="string">&quot;请计算一下427乘以971是多少？&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt; Entering new LLMMathChain chain...</span><br><span class="line">请计算一下<span class="number">427</span>乘以<span class="number">971</span>是多少？</span><br><span class="line">```text</span><br><span class="line"><span class="number">427</span> * <span class="number">971</span></span><br><span class="line">```</span><br><span class="line">...numexpr.evaluate(<span class="string">&quot;427 * 971&quot;</span>)...</span><br><span class="line"></span><br><span class="line">Answer: <span class="number">414617</span></span><br><span class="line">&gt; Finished chain.</span><br><span class="line">Answer: <span class="number">414617</span></span><br></pre></td></tr></table></figure><p>LangChain 也将 utilities 包中的许多功能封装成了 Utility Chains。例如，SQLDatabaseChain 可以直接根据你的数据库生成 SQL，然后获取数据；LLMRequestsChain 可以通过 API 调用外部系统，获取所需的答案。您可以在 LangChain 的 Utility Chains 文档中找到可用的工具列表。</p><figure><img src="https://qiniu.hivan.me/picGo/20230605104013.png?imgNote" alt="image-20230605104008009" /><figcaption aria-hidden="true">image-20230605104008009</figcaption></figure><p>LLMathChain使用OpenAI生成Python代码，然后通过REPL执行Python代码完成数学计算。</p><h2 id="通过-requestschain-获取实时外部信息"><strong>通过 RequestsChain 获取实时外部信息</strong></h2><p>在这里，我们将重点讲解如何通过 API 调用外部系统，以获得所需的答案。之前在介绍 llama-index 时，我们已经介绍过一种为 AI 引入外部知识的方法：计算这些外部知识的嵌入，并将其作为索引保存。但是，这种方法仅适用于处理预先准备好的知识，例如书籍或论文，这些内容虽然多，但是固定的，也不存在时效性问题，我们可以提前索引好，而且用户的问题往往也非常相似。</p><p>然而，对于具有时效性的问题，这种方法就不太适用了，因为我们可能没有必要不断地更新索引。例如，如果您想了解实时的天气情况，我们不太可能每隔几分钟就索引全球所有城市的最新天气信息。</p><p>这时，我们可以使用 LLMRequestsChain，通过 HTTP 请求来获取问题的答案。最简单粗暴的方法就是直接通过一个 HTTP 请求向 Google 提问。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> LLMRequestsChain</span><br><span class="line"></span><br><span class="line">template = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">在 &gt;&gt;&gt; 和 &lt;&lt;&lt; 之间是来自Google的原始搜索结果。</span></span><br><span class="line"><span class="string">请把对于问题&quot;&#123;query&#125;&quot;的答案从里面提取出来，如果里面没有相关信息的话就说“找不到“</span></span><br><span class="line"><span class="string">请使用如下格式：</span></span><br><span class="line"><span class="string">Extracted: &lt;answer or &quot;找不到&quot;&gt;</span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>&#123;requests_result&#125; &lt;&lt;&lt;</span></span><br><span class="line"><span class="string">Extracted:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">PROMPT = PromptTemplate(</span><br><span class="line">    input_variables = [<span class="string">&#x27;query&#x27;</span>, <span class="string">&#x27;requests_result&#x27;</span>],</span><br><span class="line">    template = template,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">requests_chain = LLMRequestsChain(llm_chain = LLMChain(llm = OpenAI(temperature = <span class="number">0</span>), prompt = PROMPT))</span><br><span class="line">question = <span class="string">&quot;今天上海的天气怎么样？&quot;</span></span><br><span class="line"></span><br><span class="line">inputs = &#123;</span><br><span class="line">    <span class="string">&quot;query&quot;</span>: question,</span><br><span class="line">    <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.google.com/search?q=&quot;</span> + question.replace(<span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">result = requests_chain(inputs)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(result[<span class="string">&#x27;output&#x27;</span>])</span><br></pre></td></tr></table></figure><p>输出的结果：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span>&#x27;query&#x27;<span class="punctuation">:</span> &#x27;今天上海的天气怎么样？&#x27;<span class="punctuation">,</span> &#x27;url&#x27;<span class="punctuation">:</span> &#x27;https<span class="punctuation">:</span><span class="comment">//www.google.com/search?q=今天上海的天气怎么样？&#x27;, &#x27;output&#x27;: &#x27;多雲時陰，最高溫：27°C 最低溫：19°C&#x27;&#125;</span></span><br><span class="line">多雲時陰，最高溫：<span class="number">27</span>°C 最低溫：<span class="number">19</span>°C</span><br></pre></td></tr></table></figure><p>让我们来看看这段代码。基于 LLMRequestsChain，我们用到了之前使用过的好几个技巧。</p><ol type="1"><li>首先，因为我们是简单粗暴地搜索 Google，但是我们想要的是一个有价值的天气信息，而不是整个网页。所以，我们还需要通过 ChatGPT 把网页搜索结果里面的答案给找出来。因此，我们定义了一个 PromptTemplate，通过一段提示语，让 OpenAI 在搜索结果中为我们找出问题的答案，而不是获取原始的 HTML 页面。</li><li>然后，我们使用了 LLMRequestsChain，并将刚才构造的 PromptTemplate 作为构造函数的一个参数传递给 LLMRequestsChain，以帮助我们在搜索之后处理搜索结果。</li><li>查询对应的搜索词会传递到 query 参数中，对应的原始搜索结果会默认放到 requests_results 中。而通过我们自己定义的 PromptTemplate 抽取出来的最终答案，则会放到 output 输出参数中。</li></ol><p>运行代码后，我们可以看到通过简单搜索 Google 并使用 OpenAI 提取搜索结果中的答案，我们得到了最新的天气信息。</p><h2 id="通过-transformationchain-转换数据格式"><strong>通过 TransformationChain 转换数据格式</strong></h2><p>有了实时的外部数据，我们就有了很多应用的创意了。比如说，我们可以根据气温来推荐大家穿什么衣服。如果最低温度低于 0 度，我们可以建议用户穿羽绒服。或者，根据是否下雨来决定是否提醒用户出门带伞。</p><p>但是，现在返回结果中的天气信息（天气、温度、风力）仅是一段文本，而非可以直接获取的 JSON 格式。当然，我们可以在 LLMChain 内再次链式调用 OpenAI 的接口，将这段文本转换成 JSON 格式。但这么做的话，一方面会消耗更多的 Token 和花费更多的资金，另一方面也会进一步增加程序运行所需的时间，毕竟一次往返的网络请求也很慢。这里的文本格式实际上非常简单，我们完全可以通过简单的字符串处理来解析它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_temperature</span>(<span class="params">weather_info</span>):</span><br><span class="line">    split_info = weather_info.split(<span class="string">&#x27;，&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(split_info) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;无法正确解析天气信息&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    weather_summary = split_info[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 提取最高温度</span></span><br><span class="line">    max_temp_start = weather_info.find(<span class="string">&quot;最高溫：&quot;</span>) + <span class="built_in">len</span>(<span class="string">&quot;最高溫：&quot;</span>)</span><br><span class="line">    max_temp_end = weather_info.find(<span class="string">&quot;°C&quot;</span>, max_temp_start)</span><br><span class="line">    max_temperature = weather_info[max_temp_start:max_temp_end]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取最低温度</span></span><br><span class="line">    min_temp_start = weather_info.find(<span class="string">&quot;最低溫：&quot;</span>) + <span class="built_in">len</span>(<span class="string">&quot;最低溫：&quot;</span>)</span><br><span class="line">    min_temp_end = weather_info.find(<span class="string">&quot;°C&quot;</span>, min_temp_start)</span><br><span class="line">    min_temperature = weather_info[min_temp_start:min_temp_end]</span><br><span class="line"></span><br><span class="line">    temperature_data = &#123;</span><br><span class="line">        <span class="string">&quot;weather_summary&quot;</span>: weather_summary,</span><br><span class="line">        <span class="string">&quot;max_temperature&quot;</span>: max_temperature,</span><br><span class="line">        <span class="string">&quot;min_temperature&quot;</span>: min_temperature</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> temperature_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试示例</span></span><br><span class="line">weather_info = <span class="string">&quot;多雲時陰，最高溫：27°C 最低溫：19°C&quot;</span></span><br><span class="line">result = extract_temperature(weather_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印拆分后的结果</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将结果保存为JSON文件</span></span><br><span class="line"><span class="comment"># with open(&quot;weather_data.json&quot;, &quot;w&quot;) as file:</span></span><br><span class="line"><span class="comment">#     json.dump(result, file, ensure_ascii=False, indent=4)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出的结果：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span>&#x27;weather_summary&#x27;<span class="punctuation">:</span> &#x27;多雲時陰&#x27;<span class="punctuation">,</span> &#x27;max_temperature&#x27;<span class="punctuation">:</span> &#x27;<span class="number">27</span>&#x27;<span class="punctuation">,</span> &#x27;min_temperature&#x27;<span class="punctuation">:</span> &#x27;<span class="number">19</span>&#x27;<span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><blockquote><p>以上代码其实是我用chatGPT在几次修改后产生的代码。因为我们并不需要保存JSON文件，所以我注释了后面两行代码，但是依然还是贴出来。</p><figure><img src="https://qiniu.hivan.me/picGo/20230605110352.png?imgNote" alt="image-20230605110352650" /><figcaption aria-hidden="true">image-20230605110352650</figcaption></figure></blockquote><p>我们在这里实现了一个 <code>extract_temperature</code> 函数，可以将 LLMRequestsChain 的输出结果解析为一个 dict。不过，我们能否将该解析逻辑进一步传递到 LLMChain 链式调用的最后呢？答案当然是可以的。Langchain 中有一个专门的解决方案，称为 TransformChain，用于格式转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> TransformChain, SequentialChain</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transform_func</span>(<span class="params">inputs: <span class="built_in">dict</span></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">    text = inputs[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;weather_info&quot;</span> : extract_temperature(text)&#125;</span><br><span class="line"></span><br><span class="line">transformation_chain = TransformChain(input_variables=[<span class="string">&quot;output&quot;</span>], </span><br><span class="line">                                      output_variables=[<span class="string">&quot;weather_info&quot;</span>], transform=transform_func)</span><br><span class="line"></span><br><span class="line">final_chain = SequentialChain(chains=[requests_chain, transformation_chain], </span><br><span class="line">                              input_variables=[<span class="string">&quot;query&quot;</span>, <span class="string">&quot;url&quot;</span>], output_variables=[<span class="string">&quot;weather_info&quot;</span>])</span><br><span class="line"></span><br><span class="line">final_chain</span><br><span class="line">final_result = final_chain.run(inputs)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(final_result)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span>&#x27;weather_summary&#x27;<span class="punctuation">:</span> &#x27;今天多雲時陰&#x27;<span class="punctuation">,</span> &#x27;max_temperature&#x27;<span class="punctuation">:</span> &#x27;雲時陰，最高溫<span class="number">27</span>&#x27;<span class="punctuation">,</span> &#x27;min_temperature&#x27;<span class="punctuation">:</span> &#x27;雲時陰，最高溫<span class="number">27</span>&#x27;<span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><blockquote><p>在 requests_chain 后面跟上一个 transformation_chain，就能把结果解析成 dict，供后面的其他业务使用结构化的数据。</p></blockquote><p>在这里，我们先定义了一个 <code>transform_func</code> 函数，对前面的 <code>extract_temperature</code> 函数进行简单的封装。<code>transform_func</code> 函数的输入是整个 LLMChain 在执行到 TransformChain 之前的输出结果的 dict。我们前面看到整个 LLMRequestsChain 中的天气信息文本内容是通过 <code>output</code> 这个 key 得到的，因此在这里我们也是先通过它来获取天气信息的文本内容，再调用 <code>extract_temperature</code> 解析，并将结果输出到 <code>weather_info</code> 这个字段中。</p><p>然后，我们定义了一个 TransformChain，其输入参数是 output，输出参数是 <code>weather_info</code>。</p><p>最后，我们通过上一讲中介绍过的 SequentialChain，将前面的 LLMRequestsChain 和这里的 TransformChain 串联在一起，形成一个名为 <code>final_chain</code> 的新的 LLMChain。</p><p>在这三个步骤完成之后，我们只需要调用 <code>final_chain</code> 的 run 方法，输入有关天气的搜索文本即可获得天气信息的 dict 形式的输出。</p><figure><img src="https://qiniu.hivan.me/picGo/20230605103945.png?imgNote" alt="image-20230605103928538" /><figcaption aria-hidden="true">image-20230605103928538</figcaption></figure><p>最后，让我们梳理一下 final_chain 完成的任务。</p><p>首先，通过一个 HTTP 请求，根据搜索词获取 Google 的搜索结果页。</p><p>接着，我们将自定义的提示（Prompt）提交给 OpenAI，并将搜索的问题和结果页发送给 OpenAI，以便它从中提取结果页中的天气信息。</p><p>最后，我们使用 transform_func 解析提取到的天气信息文本，并将其转换为一个字典。这样，后续的程序就可以轻松处理了。</p><h2 id="通过-vectordbqa-实现先搜索再回复的能力"><strong>通过 VectorDBQA 实现先搜索再回复的能力</strong></h2><p>另外，还有一个常用的 LLMChain，就是我们之前介绍的 llama-index 的使用场景，也就是针对自己的资料库进行问答。我们预先把资料库索引好，然后每次用户来问问题的时候，都是先到这个资料库里搜索，再把问题和答案一并交给 AI，让它去组织语言回答。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.embeddings.openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> SpacyTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> OpenAI, VectorDBQA</span><br><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"></span><br><span class="line">llm = OpenAI(temperature=<span class="number">0</span>)</span><br><span class="line">loader = TextLoader(<span class="string">&#x27;./data/ecommerce_faq.txt&#x27;</span>)</span><br><span class="line">documents = loader.load()</span><br><span class="line">text_splitter = SpacyTextSplitter(chunk_size=<span class="number">256</span>, pipeline=<span class="string">&quot;zh_core_web_sm&quot;</span>)</span><br><span class="line">texts = text_splitter.split_documents(documents)</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">docsearch = FAISS.from_documents(texts, embeddings)</span><br><span class="line"></span><br><span class="line">faq_chain = VectorDBQA.from_chain_type(llm=llm, vectorstore=docsearch, verbose=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>注：上述代码创建了一个基于 FAISS 进行向量存储的 docsearch 索引，并基于该索引创建了 VectorDBQA 的 LLMChain。</p><p>首先，我们通过 TextLoader 将文件加载到内存中，并通过 SpacyTextSplitter 对文本进行分段，以确保每个 Document 都是一个完整的句子。因为这里的文档是关于电子商务常见问题的内容，所以我们设置 chunk_size 为 256。接着，我们使用 OpenAIEmbeddings 为文档创建 Embedding，并通过 FAISS 将其存储为一个 VectorStore。最后，我们使用 VectorDBQA 的 <code>from_chain_type</code> 方法定义了一个 LLM。相关的常见问题内容，请参见 <a href="../ecommerce_faq.txt">ecommerce_faq.txt</a> 文件，其中的内容是 ChatGPT编写的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;请问你们的货，能送到三亚吗？大概需要几天？&quot;</span></span><br><span class="line">result = faq_chain.run(question)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; Entering new VectorDBQA chain...</span><br><span class="line"></span><br><span class="line">&gt; Finished chain.</span><br><span class="line"> 我们支持全国大部分省份的配送，包括三亚。一般情况下，大部分城市的订单在2-3个工作日内送达，偏远地区可能需要5-7个工作日。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;请问你们的退货政策是怎么样的？&quot;</span> </span><br><span class="line">result = faq_chain.run(question)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; Entering new VectorDBQA chain...</span><br><span class="line"></span><br><span class="line">&gt; Finished chain.</span><br><span class="line"> 自收到商品之日起7天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。</span><br></pre></td></tr></table></figure><p>我向它提了两个不同类型的问题，faq_chain 都能够正确地回答出来。你可以去看看 data 目录下面的 <a href="../ecommerce_faq.txt">ecommerce_faq.txt</a> 文件，看看它的回答是不是和文档中写的内容一致。</p><p>在 VectorDBQA 这个 LLMChain 背后，实际上是通过一系列的链式调用来完成搜索 VectorStore 和向 AI 发起 Completion 请求这两个步骤。</p><p>可以看到，LLMChain 是一个非常强大的工具，它可以将解决一个问题所需的多个步骤串联在一起。这些步骤可以是调用我们的语言模型，也可以是调用外部 API，或者在内部定义一个 Python 函数。这大大增强了我们利用大型语言模型的能力，特别是能够弥补它的许多不足之处，比如缺少有时效性的信息和通过 HTTP 调用比较慢等等。</p><h2 id="小结"><strong>小结</strong></h2><p>本文介绍了 Langchain 的链式调用，它不仅限于使用大型语言模型的接口。我们介绍了四种常见的将大型语言模型的接口和其他能力结合在一起的链式调用。</p><ol type="1"><li><p>LLMMathChain 可以通过 Python 解释器变成一个计算器，让 AI 能够准确地进行数学运算。</p></li><li><p>通过 RequestsChain，我们可以直接调用外部 API，然后让 AI 从返回的结果中提取我们关心的内容。</p></li><li><p>TransformChain 可以让我们根据自己的要求对数据进行处理和转化，我们可以进一步将 AI 返回的自然语言结果转换成结构化数据，方便其他程序处理。</p></li><li><p>VectorDBQA 能够完成和 llama-index 相似的事情，只需要预先做好内部数据资料的 Embedding 和索引，通过对 LLMChain 进行一次调用，我们就可以直接获取回答的结果。</p></li></ol><p>这些能力大大增强了 AI 的实用性，解决了几个之前大型语言模型难以处理的问题，包括数学计算能力、实时数据能力、和现有程序结合的能力，以及搜索自己的资料库的能力。你完全可以定义自己需要的 LLMChain，通过程序来完成各种任务，然后合理地组合不同类型的 LLMChain 对象，来实现连 ChatGPT 都做不到的事情。而 ChatGPT Plugins 的实现机制，其实也是类似的。</p><h2 id="思考题"><strong>思考题</strong></h2><p>最后，留下一个思考题。我们前面提到，Langchain 中有 SQLDatabaseChain 可以直接让我们编写需求访问数据库。在官方文档中也给出了<a href="https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html">相应的示例</a>。你可以试着体验一下，思考一下它是通过什么样的提示语信息来让 AI 写出可以直接执行的 SQL？</p><p>欢迎你在评论区分享你的体验和思考结果。也欢迎你将本文分享给感兴趣的朋友。我们下一篇文章再见！</p><h2 id="推荐试用"><strong>推荐试用</strong></h2><p>我们目前对于 Langchain 的讲解都是通过 Python 编程的方式来实现真实业务场景的需求。有人直接为 Langchain 开发了一个可以拖拽的图形界面，叫做 <a href="https://github.com/logspace-ai/langflow">LangFlow</a>。你可以尝试下载并体验一下，看看图形界面是否可以进一步提高你的效率。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;大家好，我是Hivan.&lt;/p&gt;
&lt;p&gt;在&lt;a href=&quot;../使用链式调用简化多步提示语&quot;&gt;上一节课&lt;/a&gt;中，我们学习了如何使用LangChain这个Python包链式调用OpenAI的API。通过链式调用，我们可以将需要多轮询问AI才能解决的问题封装起来，将需要多轮自然语言调用才能解决的问题变成一个函数调用。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="tag" scheme="https://hivan.me/tags/tag/"/>
    
  </entry>
  
  <entry>
    <title>14. 使用链式调用简化多步提示语</title>
    <link href="https://hivan.me/%E4%BD%BF%E7%94%A8%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%E7%AE%80%E5%8C%96%E5%A4%9A%E6%AD%A5%E6%8F%90%E7%A4%BA%E8%AF%AD/"/>
    <id>https://hivan.me/%E4%BD%BF%E7%94%A8%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%E7%AE%80%E5%8C%96%E5%A4%9A%E6%AD%A5%E6%8F%90%E7%A4%BA%E8%AF%AD/</id>
    <published>2023-06-02T09:14:18.000Z</published>
    <updated>2023-06-02T11:55:19.801Z</updated>
    
    <content type="html"><![CDATA[<p>Hi, 大家好，我是茶桁。</p><p>OpenAI 的大语言模型提供了 Completion 和 Embedding 两个核心接口。</p><p>我们可以通过增加提示语（Prompt）历史记录来提高模型的回答准确性和自然性。还可以将 Embedding提前索引好存起来，以此做到让AI根据外部知识来回答问题，</p><span id="more"></span><p>在我们多次与AI对话的过程中，讲AI返回的答案放在新的问题里，那么我们就可以让AI帮主我们给自己的代码撰写单元测试了。</p><p>以上这些方法是自然语言类应用中常见的模式。为了方便应用开发者使用这些模式，开源社区开发了名为 Langchain 的开源库，使用 Langchain，我们可以更加快速地实现之前利用大语言模型实现过的功能，并且可以更好地将模型集成到我们的业务系统中，实现更加复杂、有价值的功能。</p><h2 id="何谓链式调用">何谓链式调用</h2><p>在<a href="../Save-costs-with-an-open-source-model">第 11 讲</a>中，我们学习了 llama-index 的使用，并在此过程中已经安装了 Langchain。虽然 Langchain 也有类似 llama-index 的功能，但这不是 Langchain 的主要卖点。Langchain 带来的第一个主要优势就在于它的名字，也就是链式调用。</p><p>链式调用是指在程序中可以使用点 <code>.</code> 来连接对象，然后在这些对象上调用方法，以此来实现更加复杂的操作。在使用 ChatGPT 的过程中，我们经常会遇到中文语料较少的问题，以至于它不能很好地回答问题。为了解决这个问题，我们可以采取以下措施。</p><p>我们可以把中文问题交给 AI，让它翻译成英文，然后把英文问题贴进去提问，得到一个英文答案。最后，再请 AI 把英文答案翻译回中文。通过这种方式，我们可以得到更加准确的答案。</p><blockquote><p>可能部分读者会觉得这样做有些脱裤子放屁多此一举，但是其实你真的完全用英文来完成你的问题和获取答案，你就能很明显的感觉到差别。这个问题曾经有一个业内人士延展的谈到过，其实这是一个很严重的问题，也是我们这几十年来的互联网环境造成了今天的局面，而再这样下去，中文的AI环境就此完蛋，我们也可能再也追不上国外的脚步。当然，到底是否危言耸听仁者见仁智者见智，每个人有自己的看法，可是从我自己使用之后和对于一些原理的理解来看，这真的不是危言耸听。理解大语言模型原理的人应该都能懂。好了，我把视频链接放在这，有兴趣的自己去看看吧，一段7分钟的视频：<a href="https://www.bilibili.com/video/BV1Nm4y1z7AT/">中文互联网的凋零，正在杀死中国人工智能的未来【为什么我们搞不出ChatGPT】</a></p></blockquote><p>我们还可以通过增加数据样本的方式来扩展 ChatGPT 的知识库，让其可以更好地回答问题。除此之外，我们还可以通过使用更加复杂的算法和技术来提高 ChatGPT 的回答质量，例如使用迁移学习等技术。</p><p>例如，下面的截图里，我请 ChatGPT 简单介绍一下 Stable Diffusion 的原理。通过 Langchain 的链式调用，我们可以更方便地使用 ChatGPT 来解决各种问题。</p><p>Langchain 带来的链式调用优势可以让我们更加灵活地使用 ChatGPT，同时也可以帮助我们解决中文语料较少的问题。通过不断地扩展 ChatGPT 的知识库和使用更加先进的技术，我们相信 ChatGPT 可以成为一个更加强大的工具，帮助我们解决各种问题。</p><blockquote><p>Stable Diffusion 是一个非常热门的开源 AI 画图工具，我们在最开始的导读课程中有过一些贴图。除此之外，我以前也写过专门介绍的文章：<a href="../How-to-install-and-run-Stable-Diffusion-on-Apple-Silicon">在 Apple Silicon M1/M2 Mac 上安装和运行Stable Diffusion</a></p></blockquote><p>一般来说，如果我们人工询问ChatGPT，应该是这样的步骤, 我将其称之为“<strong>人工链式调用</strong>”：</p><figure><img src="https://qiniu.hivan.me/picGo/20230602165232.png?imgNote" alt="image-20230602165232370" /><figcaption aria-hidden="true">image-20230602165232370</figcaption></figure><figure><img src="https://qiniu.hivan.me/picGo/20230602165359.png?imgNote" alt="image-20230602165359092" /><figcaption aria-hidden="true">image-20230602165359092</figcaption></figure><figure><img src="https://qiniu.hivan.me/picGo/20230602165505.png?imgNote" alt="image-20230602165505848" /><figcaption aria-hidden="true">image-20230602165505848</figcaption></figure><p>可以从截图中看到，我们一共经历了三步：</p><ol type="1"><li>把我们的问题翻译成了英文</li><li>用翻译好的英文向ChatGPT提问</li><li>最后将ChatGPT的回答再翻译回中文</li></ol><p>那如果我们用API来实现这整个过程，其实就是一个链式调用的过程。</p><h2 id="使用llmchain进行链式调用">使用LLMChain进行链式调用</h2><p>我们可以使用 OpenAI 进行翻译请求和原始问题的组合，并将其发送给 AI，以完成问题的中译英转换，从而获得翻译好的英文问题。</p><p>接下来，我们可以将翻译好的英文问题发送回 OpenAI，以获得英文答案。</p><p>最后，我们可以将英文答案与要求 AI 翻译答案的请求组合在一起，以完成答案的英译中转换。</p><p>如果我们使用 LLMChain 进行链式调用，我们可以在代码中逐步进行如下操作：</p><ol type="1"><li>发送翻译请求和原始问题给 OpenAI 进行中译英转换。</li><li>得到翻译好的英文问题。</li><li>发送翻译好的英文问题给 OpenAI 进行英文答案的获取。</li><li>得到英文答案。</li><li>将英文答案与要求 AI 翻译答案的请求组合在一起，完成答案的英译中转换。</li></ol><p>通过以上操作，我们可以获得更加准确和完整的翻译结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai, os</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> LLMChain</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;OPENAI_API_KEY&#x27;</span>] = <span class="string">&#x27;OPENAI_API_KEY&#x27;</span></span><br><span class="line">openai.api_key = <span class="string">&quot;OPENAI_API_KEY&quot;</span></span><br><span class="line"></span><br><span class="line">llm = OpenAI(model_name = <span class="string">&quot;text-davinci-003&quot;</span>,  temperature = <span class="number">0.5</span>, max_tokens=<span class="number">2048</span>)</span><br><span class="line"></span><br><span class="line">en_to_zh_prompt = PromptTemplate(</span><br><span class="line">    template = <span class="string">&quot;请把下面这句话翻译成英文： \n\n &#123;question&#125;?&quot;</span>, input_variables = [<span class="string">&#x27;question&#x27;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">question_prompt = PromptTemplate(</span><br><span class="line">    template = <span class="string">&quot;&#123;english_question&#125;&quot;</span>, input_variables=[<span class="string">&#x27;english_question&#x27;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">zh_to_cn_prompt = PromptTemplate(</span><br><span class="line">    input_variables = [<span class="string">&#x27;english_answer&#x27;</span>],</span><br><span class="line">    template = <span class="string">&#x27;请把下面这一段翻译成中文: \n\n &#123;english_answer&#125;?&#x27;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">question_translate_chain = LLMChain(llm = llm, prompt = en_to_zh_prompt, output_key = <span class="string">&#x27;english_question&#x27;</span>)</span><br><span class="line">english = question_translate_chain.run(question = <span class="string">&#x27;请你作为一个机器学习的专家，介绍一下CNN的原理&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(english)</span><br></pre></td></tr></table></figure><p>得到英文翻译结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Please <span class="keyword">as</span> an expert <span class="keyword">in</span> Machine Learning, introduce the principle of CNN?</span><br></pre></td></tr></table></figure><p>让我们接着上面的代码继续：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">qa_chain = LLMChain(llm = llm, prompt = question_prompt, output_key = <span class="string">&#x27;english_answer&#x27;</span>)</span><br><span class="line">english_answer = qa_chain.run(english_question = english)</span><br><span class="line"><span class="built_in">print</span>(english_answer)</span><br></pre></td></tr></table></figure><p>得到英文回答：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Convolutional Neural Networks (CNNs) are a <span class="built_in">type</span> of deep learning neural network architecture that are particularly well suited <span class="keyword">for</span> processing <span class="keyword">and</span> analyzing data that has a spatial <span class="keyword">or</span> temporal component. The main principle of CNNs <span class="keyword">is</span> to learn features <span class="keyword">from</span> the data <span class="keyword">in</span> a hierarchical manner, starting <span class="keyword">from</span> basic features at the lower layers <span class="keyword">and</span> gradually increasing the complexity of the features at the higher layers. The main components of a CNN include convolutional layers, pooling layers, <span class="keyword">and</span> fully connected layers. The convolutional layers are used to extract features <span class="keyword">from</span> the <span class="built_in">input</span> data, <span class="keyword">while</span> the pooling layers are used to reduce the size of the <span class="built_in">input</span> data <span class="keyword">and</span> the fully connected layers are used to make predictions based on the extracted features.</span><br></pre></td></tr></table></figure><p>继续：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">answer_translate_chain = LLMChain(llm = llm, prompt = zh_to_cn_prompt)</span><br><span class="line">answer = answer_translate_chain.run(english_answer = english_answer)</span><br><span class="line"><span class="built_in">print</span>(answer)</span><br></pre></td></tr></table></figure><p>得到最终结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">卷积神经网络（CNN）是一种深度学习神经网络架构，特别适合处理和分析具有空间或时间成分的数据。CNN的主要原理是以分层的方式从数据中学习特征，从低层开始从基本特征，并逐渐增加高层的特征复杂性。CNN的主要组成部分包括卷积层、池化层和全连接层。卷积层用于从输入数据中提取特征，而池化层用于减小输入数据的大小，全连接层用于根据提取的特征进行预测。</span><br></pre></td></tr></table></figure><p>在这段代码中，我们使用了 Langchain 库，但是我们可以加入一些新的内容来扩展它，以便更好地满足我们的需求。我们主要使用了 Langchain 的三个包来扩展它。LLM、PromptTemplate 和 LLMChain。</p><p>首先，我们使用 LLM 包来回答我们提出的问题。在这里，我们使用的是 OpenAIChat。OpenAIChat 是一个最新的 gpt-3.5-turbo 模型，可以帮助我们用最好的方式回答我们的问题。</p><p>然后是 PromptTemplate，这个包和在第 11 讲中看到的 llama-index 的 PromptTemplate 相似。它可以定义一个提示语模板，里面可以定义一些可以动态替换的变量。例如，在代码中的 question_prompt 模板中，我们定义了一个名为 question 的变量，因为我们每次提出的问题都不一样。事实上，llama-index 中的 PromptTemplate 就是对 Langchain 的 PromptTemplate 做了一层简单的封装。</p><p>接下来是 LLMChain，主角。它的构造函数接收一个 LLM 和一个 PromptTemplate 作为参数。构造完成之后，可以直接调用里面的 run 方法，将 PromptTemplate 需要的变量，用 K=&gt;V 对的形式传入进去。返回的结果，就是 LLM 给我们的答案。</p><p>但是，现在我们似乎只是对 OpenAI 的 API 做了一层封装。我们构建了 3 个 LLMChain，然后按照顺序调用，每次拿到答案之后，再作为输入，交给下一个 LLM 调用。感觉好像更麻烦了，没有减少什么工作量呀？</p><p>不要着急，我们还没有真正使用 LLMChain 的“链式调用”功能。只需要加上一行小小的代码即可。我们使用一个名为 SimpleSequentialChain 的 LLMChain 类，将我们要按顺序依次调用的三个 LLMChain 放在一个数组中，将该数组传递给该类的构造函数。</p><p>然后，我们调用该对象的 run 方法，将我们用中文提出的问题交给它。此时，该 SimpleSequentialChain 会按照顺序开始调用数组参数 chains 中包含的其他 LLMChain。每次调用的结果都会存储在 Chain 构造时定义的 output_key 参数中。如果下一个调用的 LLMChain 中的模板内的变量与之前的 output_key 名称相同，则会用 output_key 中存储的内容替换模板内变量所在的占位符。</p><p>这样，我们只需要向该 SimpleSequentialChain 调用一次 run 方法，将一开始的问题交给它即可。后续根据答案去问新的问题，该 LLMChain 会自动链式搞定。我在这里将日志的 Verbose 模式打开，你在输出的过程中，可以看到该 LLMChain 调用了三次，并且可以一并看到中间两次的返回结果。这就是我们如何更好地使用 Langchain 库来满足我们的需求，同时保留原有的关键思想。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> SimpleSequentialChain</span><br><span class="line"></span><br><span class="line">chinese_qa_chain = SimpleSequentialChain(</span><br><span class="line">    chains = [question_translate_chain, qa_chain, answer_translate_chain], input_key = <span class="string">&#x27;question&#x27;</span>,</span><br><span class="line">    verbose = <span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">answer = chinese_qa_chain.run(question = <span class="string">&#x27;请你作为一个机器学习的专家，介绍一下CNN的原理。&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(answer)</span><br></pre></td></tr></table></figure><p>然后我们可以看到Verbose的日志信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt; Entering new SimpleSequentialChain chain...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Please introduce the principle of CNN <span class="keyword">as</span> an expert of machine learning.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Convolutional Neural Networks (CNNs) are a <span class="built_in">type</span> of deep learning neural network used <span class="keyword">for</span> image recognition <span class="keyword">and</span> classification. They are composed of multiple layers of neurons, each layer performing a specific task. The neurons are organized <span class="keyword">in</span> a hierarchical manner, <span class="keyword">with</span> each layer taking the output <span class="keyword">from</span> the previous layer <span class="keyword">as</span> its <span class="built_in">input</span>. The first layer of neurons <span class="keyword">is</span> usually a convolutional layer that performs convolution operations on the <span class="built_in">input</span> image, extracting features <span class="keyword">from</span> it. The subsequent layers are fully connected layers that use the extracted features to classify the image. The final layer produces the output, which <span class="keyword">is</span> usually a probability distribution over the categories of the image. CNNs are powerful tools <span class="keyword">for</span> image classification, <span class="keyword">and</span> have achieved state-of-the-art results on a variety of tasks.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">卷积神经网络（CNN）是一种深度学习神经网络，用于图像识别和分类。它由多层神经元组成，每层执行特定任务。神经元以分层的方式组织，每层以上一层的输出作为输入。第一层神经元通常是卷积层，对输入图像执行卷积操作，从中提取特征。随后的层是完全连接的图层，它们使用提取的要素对影像进行分类。最后一层产生输出，这通常是图像类别的概率分布。CNN是图像分类的强大工具，并且在各种任务上都取得了最先进的结果。</span><br><span class="line"></span><br><span class="line">&gt; Finished chain.</span><br></pre></td></tr></table></figure><p>当然，还得到我们的最终结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">卷积神经网络（CNN）是一种深度学习神经网络，用于图像识别和分类。它由多层神经元组成，每层执行特定任务。神经元以分层的方式组织，每层以上一层的输出作为输入。第一层神经元通常是卷积层，对输入图像执行卷积操作，从中提取特征。随后的层是完全连接的图层，它们使用提取的要素对影像进行分类。最后一层产生输出，这通常是图像类别的概率分布。CNN是图像分类的强大工具，并且在各种任务上都取得了最先进的结果。</span><br></pre></td></tr></table></figure><figure><img src="https://qiniu.hivan.me/picGo/20230602173646.png?imgNote" alt="image-20230602173646472" /><figcaption aria-hidden="true">image-20230602173646472</figcaption></figure><p>在使用链式调用时，需要注意，在一个 LLMChain 中使用 PromptTemplate 时，输入参数必须先在 LLMChain 中通过 output_key 定义，否则该变量将无值，从而导致程序报错。此外，在一些情况下，您可能需要使用 try-except 语句来处理可能出现的错误，以确保代码的稳定性。另外，建议您在代码中添加注释，以便更好地理解程序的执行流程和实现细节。</p><h2 id="输入多个变量">输入多个变量</h2><p>实际上，由于输入输出使用的是变量，这些变量是由这些参数所定义的。因此，我们不仅可以使用前一个LLMChain的输出作为后一个LLMChain的输入，还可以连续提出多个问题，并将这些问题的答案作为后续问题的输入来继续处理。这样，我们可以进一步扩展我们的模型，提取更多的信息以及更准确的答案。这个过程有点类似于人类的思考过程，因为在人类的思维中，一个问题的答案通常会引发更多的问题。</p><p>例如，我们可以提出这样的问题：“在中国，哪个城市是最大的城市？”然后，我们可以使用一个自然语言处理模型来回答这个问题，比如说，我们可以使用一个LLMChain模型。第一个LLMChain模型将输入中的问题转化为向量，然后将这个向量传递给下一个模型。第二个LLMChain模型将这个向量处理成一个问题的答案，然后将这个答案作为下一个问题的输入。这样，我们就可以连续提出多个问题，并将这些问题的答案作为后续问题的输入来继续处理。比如说，我们可以接下来问：“这个城市的人口是多少？”然后，我们可以再次使用LLMChain模型来回答这个问题。通过这种方式，我们可以更详细地了解一个城市的信息，同时也可以更好地理解LLMChain模型的工作原理。</p><p>下面我就给你看一个例子，让你更好的理解这个概念。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> SequentialChain</span><br><span class="line"></span><br><span class="line">q1_prompt = PromptTemplate(</span><br><span class="line">    input_variables = [<span class="string">&#x27;year1&#x27;</span>],</span><br><span class="line">    template = <span class="string">&#x27;&#123;year1&#125;年的欧冠联赛的冠军是哪支球队，只说球队名称。&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">q2_prompt = PromptTemplate(</span><br><span class="line">    input_variables = [<span class="string">&#x27;year2&#x27;</span>],</span><br><span class="line">    template = <span class="string">&#x27;&#123;year2&#125;年的欧冠联赛的冠军是哪支球队，只说球队名称。&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">q3_prompt = PromptTemplate(</span><br><span class="line">    input_variables = [<span class="string">&#x27;team1&#x27;</span>, <span class="string">&#x27;team2&#x27;</span>],</span><br><span class="line">    template = <span class="string">&#x27;&#123;team1&#125;和&#123;team2&#125;哪只球队获得欧冠的次数多一些？&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">chain1 = LLMChain(llm = llm, prompt = q1_prompt, output_key = <span class="string">&#x27;team1&#x27;</span>)</span><br><span class="line">chain2 = LLMChain(llm = llm, prompt = q2_prompt, output_key = <span class="string">&#x27;team2&#x27;</span>)</span><br><span class="line">chain3 = LLMChain(llm = llm, prompt = q3_prompt)</span><br><span class="line"></span><br><span class="line">sequential_chain = SequentialChain(chains = [chain1, chain2, chain3], input_variables = [<span class="string">&#x27;year1&#x27;</span>, <span class="string">&#x27;year2&#x27;</span>], verbose = <span class="literal">True</span>)</span><br><span class="line">answer = sequential_chain.run(year1 = <span class="number">2000</span>, year2 = <span class="number">2010</span>)</span><br><span class="line"><span class="built_in">print</span>(answer)</span><br></pre></td></tr></table></figure><p>然后我们得到日志和结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; Entering new SequentialChain chain...</span><br><span class="line"></span><br><span class="line">&gt; Finished chain.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">皇家马德里获得欧冠的次数多一些，共计<span class="number">13</span>次，而拜仁慕尼黑只有<span class="number">5</span>次。</span><br></pre></td></tr></table></figure><blockquote><p>有的时候需要注意，ChatGPT可能会一本正经的胡说八道。在得到结果的时候最好验证一下，比如这个问题下，ChatGPT最初告诉我的拜仁多一些，共计8次，而皇家马德里只有3次。虽然我不知道具体次数，但是我也知道皇马是欧冠之王，明显答案就不对。所以又重新生成并验证了一次。</p></blockquote><p>在这个例子里，我们定义了两个 PromptTemplate 和对应的 LLMChain。PromptTemplate 的作用是接收一个年份作为输入，回答这个年份的欧冠冠军。我们使用两个不同的 LLMChain 分别处理这两个 PromptTemplate，从而得到两支不同年份的欧冠冠军。接下来，我们将这两个队名作为输入，放到第三个问题里。这个问题会让 AI 告诉我们这两支球队哪一支获得欧冠的次数多一些。我们可以使用另一个 LLMChain 来处理这个问题，然后将其添加到我们的 SequentialChain 中。这样，我们只需要输入两个年份，就能通过三次回答得到答案。在这个过程中，我们可以使用不同的技术和算法来优化我们的模型，以获得更加准确和快速的结果。</p><h2 id="使用-langchain-实现自动化撰写单元测试">使用 Langchain 实现自动化撰写单元测试</h2><p>看到这里，让我们更深入地探讨一下通过多步提示语自动给代码写单元测试的方法。在<a href="../Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you">上一讲</a>中，我们介绍了 Langchain，它可以顺序地通过多个 Prompt 调用 OpenAI 的 GPT 模型来实现这个功能。这种方法非常适合自动化测试，因为它可以帮助我们节省大量的时间和精力。</p><p>但是，你可能会问，如果 AST 语法解析失败怎么办？不用担心，因为现在的 Langchain 已经具备了自动重试的能力，可以轻松解决这个问题。所以，现在我们可以更加放心地使用 Langchain 进行自动化测试，从而提高我们的工作效率。</p><p>总之，通过多步提示语自动给代码写单元测试是一种非常实用的方法，而 Langchain 则是实现这种方法的最佳选择。希望这篇文章能够帮助大家更好地了解 Langchain 的功能和优势。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> SequentialChain</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">write_unit_test</span>(<span class="params">function_to_test, unit_test_package=<span class="string">&quot;pytest&quot;</span></span>):</span><br><span class="line">    <span class="comment"># 解释源代码的步骤</span></span><br><span class="line">    explain_code = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    # How to write great unit tests with &#123;unit_test_package&#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    In this advanced tutorial for experts, we&#x27;ll use Python 3.10 and `&#123;unit_test_package&#125;` to write a suite of unit tests to verify the behavior of the following function.</span></span><br><span class="line"><span class="string">    ```python</span></span><br><span class="line"><span class="string">    &#123;function_to_test&#125;</span></span><br><span class="line"><span class="string">    \```</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Before writing any unit tests, let&#x27;s review what each element of the function is doing exactly and what the author&#x27;s intentions may have been.</span></span><br><span class="line"><span class="string">    - First,</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    explain_code_template = PromptTemplate(</span><br><span class="line">        input_variables=[<span class="string">&quot;unit_test_package&quot;</span>, <span class="string">&quot;function_to_test&quot;</span>],</span><br><span class="line">        template=explain_code</span><br><span class="line">    )</span><br><span class="line">    explain_code_llm = OpenAI(model_name=<span class="string">&quot;text-davinci-002&quot;</span>, temperature=<span class="number">0.4</span>, max_tokens=<span class="number">1000</span>, model_kwargs=&#123;<span class="string">&quot;stop&quot;</span>: [<span class="string">&quot;\n\n&quot;</span>, <span class="string">&quot;\n\t\n&quot;</span>, <span class="string">&quot;\n    \n&quot;</span>]&#125;)</span><br><span class="line">    explain_code_step = LLMChain(llm=explain_code_llm, prompt=explain_code_template, output_key=<span class="string">&quot;code_explanation&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建测试计划示例的步骤</span></span><br><span class="line">    test_plan = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A good unit test suite should aim to:</span></span><br><span class="line"><span class="string">    - Test the function&#x27;s behavior for a wide range of possible inputs</span></span><br><span class="line"><span class="string">    - Test edge cases that the author may not have foreseen</span></span><br><span class="line"><span class="string">    - Take advantage of the features of `&#123;unit_test_package&#125;` to make the tests easy to write and maintain</span></span><br><span class="line"><span class="string">    - Be easy to read and understand, with clean code and descriptive names</span></span><br><span class="line"><span class="string">    - Be deterministic, so that the tests always pass or fail in the same way</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    `&#123;unit_test_package&#125;` has many convenient features that make it easy to write and maintain unit tests. We&#x27;ll use them to write unit tests for the function above.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this particular function, we&#x27;ll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):</span></span><br><span class="line"><span class="string">    -</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    test_plan_template = PromptTemplate(</span><br><span class="line">        input_variables=[<span class="string">&quot;unit_test_package&quot;</span>, <span class="string">&quot;function_to_test&quot;</span>, <span class="string">&quot;code_explanation&quot;</span>],</span><br><span class="line">        template=explain_code + <span class="string">&quot;&#123;code_explanation&#125;&quot;</span> + test_plan</span><br><span class="line">    )</span><br><span class="line">    test_plan_llm = OpenAI(model_name=<span class="string">&quot;text-davinci-002&quot;</span>, temperature=<span class="number">0.4</span>, max_tokens=<span class="number">1000</span>, model_kwargs=&#123;<span class="string">&quot;stop&quot;</span>: [<span class="string">&quot;\n\n&quot;</span>, <span class="string">&quot;\n\t\n&quot;</span>, <span class="string">&quot;\n    \n&quot;</span>]&#125;)</span><br><span class="line">    test_plan_step = LLMChain(llm=test_plan_llm, prompt=test_plan_template, output_key=<span class="string">&quot;test_plan&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 撰写测试代码的步骤</span></span><br><span class="line">    starter_comment = <span class="string">&quot;Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator&quot;</span></span><br><span class="line">    prompt_to_generate_the_unit_test = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Before going into the individual tests, let&#x27;s first look at the complete suite of unit tests as a cohesive whole. We&#x27;ve added helpful comments to explain what each line does.</span></span><br><span class="line"><span class="string">```python</span></span><br><span class="line"><span class="string">import &#123;unit_test_package&#125;  # used for our unit tests</span></span><br><span class="line"><span class="string">&#123;function_to_test&#125;</span></span><br><span class="line"><span class="string">#&#123;starter_comment&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    unit_test_template = PromptTemplate(</span><br><span class="line">        input_variables=[<span class="string">&quot;unit_test_package&quot;</span>, <span class="string">&quot;function_to_test&quot;</span>, <span class="string">&quot;code_explanation&quot;</span>, <span class="string">&quot;test_plan&quot;</span>, <span class="string">&quot;starter_comment&quot;</span>],</span><br><span class="line">        template=explain_code + <span class="string">&quot;&#123;code_explanation&#125;&quot;</span> + test_plan + <span class="string">&quot;&#123;test_plan&#125;&quot;</span> + prompt_to_generate_the_unit_test</span><br><span class="line">    )</span><br><span class="line">    unit_test_llm = OpenAI(model_name=<span class="string">&quot;text-davinci-002&quot;</span>, temperature=<span class="number">0.4</span>, max_tokens=<span class="number">1000</span>, model_kwargs=&#123;<span class="string">&quot;stop&quot;</span>: <span class="string">&quot;```&quot;</span>&#125;)</span><br><span class="line">    unit_test_step = LLMChain(llm=unit_test_llm, prompt=unit_test_template, output_key=<span class="string">&quot;unit_test&quot;</span>)</span><br><span class="line"></span><br><span class="line">    sequential_chain = SequentialChain(chains=[explain_code_step, test_plan_step, unit_test_step],</span><br><span class="line">                                       input_variables=[<span class="string">&quot;unit_test_package&quot;</span>, <span class="string">&quot;function_to_test&quot;</span>, <span class="string">&quot;starter_comment&quot;</span>],</span><br><span class="line">                                       verbose=<span class="literal">True</span>)</span><br><span class="line">    answer = sequential_chain.run(unit_test_package=unit_test_package, function_to_test=function_to_test,</span><br><span class="line">                                  starter_comment=starter_comment)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;&quot;&quot;#<span class="subst">&#123;starter_comment&#125;</span>&quot;&quot;&quot;</span> + answer</span><br><span class="line">  </span><br><span class="line">code = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">def format_time(seconds):</span></span><br><span class="line"><span class="string">    minutes, seconds = divmod(seconds, 60)</span></span><br><span class="line"><span class="string">    hours, minutes = divmod(minutes, 60)</span></span><br><span class="line"><span class="string">    if hours &gt; 0:</span></span><br><span class="line"><span class="string">        return f&quot;&#123;hours&#125;h&#123;minutes&#125;min&#123;seconds&#125;s&quot;</span></span><br><span class="line"><span class="string">    elif minutes &gt; 0:</span></span><br><span class="line"><span class="string">        return f&quot;&#123;minutes&#125;min&#123;seconds&#125;s&quot;</span></span><br><span class="line"><span class="string">    else:</span></span><br><span class="line"><span class="string">        return f&quot;&#123;seconds&#125;s&quot;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> ast</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">write_unit_test_automatically</span>(<span class="params">code, retry=<span class="number">3</span></span>):</span><br><span class="line">    unit_test_code = write_unit_test(code)</span><br><span class="line">    all_code = code + unit_test_code</span><br><span class="line">    tried = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> tried &lt; retry:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            ast.parse(all_code)</span><br><span class="line">            <span class="keyword">return</span> all_code</span><br><span class="line">        <span class="keyword">except</span> SyntaxError <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Syntax error in generated code: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">            all_code = code + write_unit_test(code)</span><br><span class="line">            tried += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line"><span class="built_in">print</span>(write_unit_test_automatically(code))</span><br></pre></td></tr></table></figure><p>然后我们得到结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">[1m&gt; Entering new SequentialChain chain...[0m</span><br><span class="line"></span><br><span class="line">[1m&gt; Finished chain.[0m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下部分是返回的程序                   </span></span><br><span class="line">                       </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">format_time</span>(<span class="params">seconds</span>):</span><br><span class="line">    minutes, seconds = <span class="built_in">divmod</span>(seconds, <span class="number">60</span>)</span><br><span class="line">    hours, minutes = <span class="built_in">divmod</span>(minutes, <span class="number">60</span>)</span><br><span class="line">    <span class="keyword">if</span> hours &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;<span class="subst">&#123;hours&#125;</span>h<span class="subst">&#123;minutes&#125;</span>min<span class="subst">&#123;seconds&#125;</span>s&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> minutes &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;<span class="subst">&#123;minutes&#125;</span>min<span class="subst">&#123;seconds&#125;</span>s&quot;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;<span class="subst">&#123;seconds&#125;</span>s&quot;</span></span><br><span class="line"><span class="comment">#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator.</span></span><br><span class="line"><span class="comment">#The first element of each tuple is the name of the test case, and the second element is a list of tuples.</span></span><br><span class="line"><span class="comment">#Each tuple in the list of tuples is a set of input values and expected output values for that test case.</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="string">&quot;test_case, input_values, expected_output&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">    [</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="comment"># Test cases for positive integers</span></span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;positive_int&quot;</span>, [<span class="number">1</span>], <span class="string">&quot;1s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;positive_int&quot;</span>, [<span class="number">60</span>], <span class="string">&quot;1min&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;positive_int&quot;</span>, [<span class="number">3600</span>], <span class="string">&quot;1h&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;positive_int&quot;</span>, [<span class="number">3601</span>], <span class="string">&quot;1h1s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="comment"># Test cases for negative integers</span></span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;negative_int&quot;</span>, [-<span class="number">1</span>], <span class="string">&quot;-1s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;negative_int&quot;</span>, [-<span class="number">60</span>], <span class="string">&quot;-1min&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;negative_int&quot;</span>, [-<span class="number">3600</span>], <span class="string">&quot;-1h&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;negative_int&quot;</span>, [-<span class="number">3601</span>], <span class="string">&quot;-1h1s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="comment"># Test case for zero</span></span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;zero&quot;</span>, [<span class="number">0</span>], <span class="string">&quot;0s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="comment"># Test cases for positive floats</span></span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;positive_float&quot;</span>, [<span class="number">1.0</span>], <span class="string">&quot;1.0s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;positive_float&quot;</span>, [<span class="number">60.0</span>], <span class="string">&quot;1.0min&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;positive_float&quot;</span>, [<span class="number">3600.0</span>], <span class="string">&quot;1.0h&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;positive_float&quot;</span>, [<span class="number">3601.0</span>], <span class="string">&quot;1.0h1.0s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="comment"># Test cases for negative floats</span></span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;negative_float&quot;</span>, [-<span class="number">1.0</span>], <span class="string">&quot;-1.0s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;negative_float&quot;</span>, [-<span class="number">60.0</span>], <span class="string">&quot;-1.0min&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;negative_float&quot;</span>, [-<span class="number">3600.0</span>], <span class="string">&quot;-1.0h&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;negative_float&quot;</span>, [-<span class="number">3601.0</span>], <span class="string">&quot;-1.0h1.0s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="comment"># Test cases for strings that can be parsed as integers</span></span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;string_int&quot;</span>, [<span class="string">&quot;1&quot;</span>], <span class="string">&quot;1s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;string_int&quot;</span>, [<span class="string">&quot;60&quot;</span>], <span class="string">&quot;1min&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;string_int&quot;</span>, [<span class="string">&quot;3600&quot;</span>], <span class="string">&quot;1h&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;string_int&quot;</span>, [<span class="string">&quot;3601&quot;</span>], <span class="string">&quot;1h1s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="comment"># Test cases for strings that can be parsed as floats</span></span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;string_float&quot;</span>, [<span class="string">&quot;1.0&quot;</span>], <span class="string">&quot;1.0s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;string_float&quot;</span>, [<span class="string">&quot;60.0&quot;</span>], <span class="string">&quot;1.0min&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;string_float&quot;</span>, [<span class="string">&quot;3600.0&quot;</span>], <span class="string">&quot;1.0h&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;string_float&quot;</span>, [<span class="string">&quot;3601.0&quot;</span>], <span class="string">&quot;1.0h1.0s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        <span class="comment"># Test cases for strings that cannot be parsed as integers or floats</span></span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;string_other&quot;</span>, [<span class="string">&quot;abc&quot;</span>], <span class="string">&quot;abc&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;string_other&quot;</span>, [<span class="string">&quot;1.0.0&quot;</span>], <span class="string">&quot;1.0.0&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;string_other&quot;</span>, [<span class="string">&quot;60s&quot;</span>], <span class="string">&quot;60s&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;string_other&quot;</span>, [<span class="string">&quot;3600min&quot;</span>], <span class="string">&quot;3600min&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">        (<span class="params"><span class="string">&quot;string_other&quot;</span>, [<span class="string">&quot;3601h&quot;</span>], <span class="string">&quot;3601h&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    ],</span></span></span><br><span class="line"><span class="params"><span class="meta"></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_format_time</span>(<span class="params">test_case, input_values, expected_output</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Unit test for the format_time function.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># The input to the function is a list, so we need to unpack it before passing it to the function.</span></span><br><span class="line">    input_value, = input_values</span><br><span class="line">    output = format_time(input_value)</span><br><span class="line">    <span class="keyword">assert</span> output == expected_output</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这个代码的具体功能与<a href="../Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you">上一讲</a>相同，只是我们通过 Langchain 对其进行了封装，以便更容易维护。我们使用三个 LLMChain 来解释代码、生成测试计划和最终生成测试代码。每一步的输入都是上一步的输出，其中包括上一步的 Prompt Template 和这一步 Prompt Template 的组合，以及一些变量，这些变量是上一步执行的结果作为输入变量传递而来的。最终，我们可以使用 SequentialChain 自动按照这三个步骤执行 OpenAI 的 API 调用。</p><p>我们使用 write_unit_test 函数来封装整个过程。对于重试，我们使用 while 循环来调用 write_unit_test。得到的结果和输入的代码拼接在一起，然后交给 AST 库进行解析。如果解析未通过，则重试整个单元测试生成的过程，直到达到我们最大的重试次数为止。</p><p>LangChain 的链式调用方式有助于我们通过 AI 完成更复杂的任务，并将整个任务的完成过程定义为一个固定的流程模板。在下一讲中，我们将进一步探讨如何通过链式组合多个 LLMChain 的方法来完成更复杂且更实际的工作。</p><h2 id="小结"><strong>小结</strong></h2><p>相信到这里，你已经有了更多可以利用大语言模型的好点子。在本讲中，我向您介绍了如何使用 Langchain 这个开源库，对大语言模型进行链式调用。要使用大语言模型完成复杂任务往往需要我们多次向 AI 提问，而前面问题的答案可能是后面问题输入的一部分。LangChain 通过将多个 LLMChain 组合成一个 SequentialChain 并顺序执行，大大简化了这类任务的开发工作。</p><figure><img src="https://qiniu.hivan.me/picGo/20230602163412.png?imgNote" alt="SequentialChain" /><figcaption aria-hidden="true">SequentialChain</figcaption></figure><p>LLMChain 是一种链式调用大型语言模型的模式，可以将前面的变量和输出作为下一轮调用的变量输入。但是，这只是 Langchain 的一小部分功能。除了调用语言模型，我们还可以调用外部系统，甚至可以让 AI 做决策，决定我们的系统该做什么。在接下来的几节课中，我们将介绍更多关于 Langchain 的强大功能，并最终提供一个完整的电子商务聊天机器人。</p><p>留下一个<strong>思考题</strong>给你：你能够通过 Langchain 结合多个问题，并利用先前问题的答案结果来触发新的问题，从而找到你所需的答案吗？欢迎在评论区分享你的例子，并将这一节课与需要它的朋友分享。我们在下一节课再见。</p><p><strong>推荐阅读：</strong>就像之前介绍的 llama-index 项目一样，Langchain 项目也正在快速发展和迭代。我建议你去看看他们的<a href="https://python.langchain.com/en/latest/">官方文档</a>，以了解他们提供的最新功能。此外，之前我们提到过的向量数据库公司 Pinecone，也制作了一份 <a href="https://www.pinecone.io/learn/langchain/">Langchain AI 手册</a>，你也可以去看一看。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hi, 大家好，我是茶桁。&lt;/p&gt;
&lt;p&gt;OpenAI 的大语言模型提供了 Completion 和 Embedding 两个核心接口。&lt;/p&gt;
&lt;p&gt;我们可以通过增加提示语（Prompt）历史记录来提高模型的回答准确性和自然性。还可以将 Embedding提前索引好存起来，以此做到让AI根据外部知识来回答问题，&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="tag" scheme="https://hivan.me/tags/tag/"/>
    
  </entry>
  
  <entry>
    <title>利用AI写一个『贪吃蛇游戏』</title>
    <link href="https://hivan.me/Use-AI-to-write-a-snake-game/"/>
    <id>https://hivan.me/Use-AI-to-write-a-snake-game/</id>
    <published>2023-05-28T09:50:41.000Z</published>
    <updated>2023-06-02T00:58:55.446Z</updated>
    
    <content type="html"><![CDATA[<p>在完成一些简单的任务之后，我觉得ChatGPT可以帮我完成一些更为复杂，甚至于可能我并不是特别擅长的任务，而我想到了，也许，可以帮我写一个简单的游戏。</p><span id="more"></span><p>贪多嚼不烂的道理我还是明白的，于是，我给自己提前就设定了一个目标：</p><blockquote><p>游戏可以运行实现简单的规则减少BUG的存在可以开始和结束</p></blockquote><p>带着这四个最朴素的愿景，让我们开始吧。</p><p>一贯的，一上来我就将问题抛给了chatGPT</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">请用Python3.10 编写一个贪吃蛇的游戏，给我完整的代码。</span><br></pre></td></tr></table></figure><p>这里我们需要注意一下，你的描述越清晰，得到的结果越有可能是你想要的。在这里，我告诉了chatGPT，我希望用Python写一个游戏，而且版本号是3.10。并且，我告诉了chatGPT，请给我完整的代码。因为如果我不这么说的话，那很有可能chatGPT会先给我一堆的步骤而不是实际的代码。</p><p>在之后，我得到了chatGPT的答复：</p><img src="/Use-AI-to-write-a-snake-game/20230601172029.png" class="" title="happy"><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pygame</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 游戏窗口的大小</span></span><br><span class="line">WINDOW_WIDTH = <span class="number">800</span></span><br><span class="line">WINDOW_HEIGHT = <span class="number">600</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 蛇身和食物的大小</span></span><br><span class="line">BLOCK_SIZE = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义颜色</span></span><br><span class="line">WHITE = (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>)</span><br><span class="line">BLACK = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">RED = (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 Pygame</span></span><br><span class="line">pygame.init()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建游戏窗口</span></span><br><span class="line">window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))</span><br><span class="line">pygame.display.set_caption(<span class="string">&quot;贪吃蛇游戏&quot;</span>)</span><br><span class="line"></span><br><span class="line">clock = pygame.time.Clock()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">game_loop</span>():</span><br><span class="line">    game_over = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 蛇的初始位置和速度</span></span><br><span class="line">    snake_x = WINDOW_WIDTH // <span class="number">2</span></span><br><span class="line">    snake_y = WINDOW_HEIGHT // <span class="number">2</span></span><br><span class="line">    snake_x_change = <span class="number">0</span></span><br><span class="line">    snake_y_change = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 食物的初始位置</span></span><br><span class="line">    food_x = <span class="built_in">round</span>(random.randrange(<span class="number">0</span>, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE</span><br><span class="line">    food_y = <span class="built_in">round</span>(random.randrange(<span class="number">0</span>, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> game_over:</span><br><span class="line">        <span class="keyword">for</span> event <span class="keyword">in</span> pygame.event.get():</span><br><span class="line">            <span class="keyword">if</span> event.<span class="built_in">type</span> == pygame.QUIT:</span><br><span class="line">                game_over = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">elif</span> event.<span class="built_in">type</span> == pygame.KEYDOWN:</span><br><span class="line">                <span class="keyword">if</span> event.key == pygame.K_LEFT:</span><br><span class="line">                    snake_x_change = -BLOCK_SIZE</span><br><span class="line">                    snake_y_change = <span class="number">0</span></span><br><span class="line">                <span class="keyword">elif</span> event.key == pygame.K_RIGHT:</span><br><span class="line">                    snake_x_change = BLOCK_SIZE</span><br><span class="line">                    snake_y_change = <span class="number">0</span></span><br><span class="line">                <span class="keyword">elif</span> event.key == pygame.K_UP:</span><br><span class="line">                    snake_y_change = -BLOCK_SIZE</span><br><span class="line">                    snake_x_change = <span class="number">0</span></span><br><span class="line">                <span class="keyword">elif</span> event.key == pygame.K_DOWN:</span><br><span class="line">                    snake_y_change = BLOCK_SIZE</span><br><span class="line">                    snake_x_change = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新蛇的位置</span></span><br><span class="line">        snake_x += snake_x_change</span><br><span class="line">        snake_y += snake_y_change</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 绘制游戏窗口</span></span><br><span class="line">        window.fill(BLACK)</span><br><span class="line">        pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE])</span><br><span class="line">        pygame.draw.rect(window, WHITE, [snake_x, snake_y, BLOCK_SIZE, BLOCK_SIZE])</span><br><span class="line">        pygame.display.update()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 碰撞检测</span></span><br><span class="line">        <span class="keyword">if</span> snake_x == food_x <span class="keyword">and</span> snake_y == food_y:</span><br><span class="line">            food_x = <span class="built_in">round</span>(random.randrange(<span class="number">0</span>, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE</span><br><span class="line">            food_y = <span class="built_in">round</span>(random.randrange(<span class="number">0</span>, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE</span><br><span class="line"></span><br><span class="line">        clock.tick(<span class="number">20</span>)  <span class="comment"># 控制游戏的帧率</span></span><br><span class="line"></span><br><span class="line">    pygame.quit()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动游戏</span></span><br><span class="line">game_loop()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>从代码里我们可以看出来，文件引用了库<code>pygame</code>以及<code>random</code>, 其中，<code>pygame</code>需要我们单独安装一下:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pygame</span><br></pre></td></tr></table></figure><p>建立了一个<code>snake_v1.py</code>的文件，并把这段代码粘贴到文件里之后，我开始尝试运行它：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python ~/xx/game/snake_v1.py</span><br></pre></td></tr></table></figure><p>事情果然没有想的那么简单，根本就是毫无动静。我开始询问chatGPT</p><img src="/Use-AI-to-write-a-snake-game/20230601172034.png" class="" title="img"><p>chatGPT给到的答案似乎没有什么意义，这些事情在开始之前我就已经做好了。在短暂的思考之后，我忽然想到，是否因为我是Mac系统，从而导致了窗口不出现，于是我继续问问题：</p><img src="/Use-AI-to-write-a-snake-game/20230601172038.png" class="" title="img"><p>然后继续问：</p><img src="/Use-AI-to-write-a-snake-game/20230601172044.png" class="" title="img"><p>这一次，我抓到了重点。不明白为什么刚才我提到我是Mac系统的时候他不告诉我，先不管这些，在其中添加这段代码后，游戏终于可以运行了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pygame</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 游戏窗口的大小</span></span><br><span class="line">WINDOW_WIDTH = <span class="number">800</span></span><br><span class="line">WINDOW_HEIGHT = <span class="number">600</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 蛇身和食物的大小</span></span><br><span class="line">BLOCK_SIZE = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义颜色</span></span><br><span class="line">WHITE = (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>)</span><br><span class="line">BLACK = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">RED = (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 Pygame</span></span><br><span class="line">pygame.init()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建游戏窗口</span></span><br><span class="line">window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))</span><br><span class="line">pygame.display.set_caption(<span class="string">&quot;贪吃蛇游戏&quot;</span>)</span><br><span class="line">pygame.display.flip()</span><br><span class="line"></span><br><span class="line">clock = pygame.time.Clock()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">game_loop</span>():</span><br><span class="line">    game_over = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 蛇的初始位置和速度</span></span><br><span class="line">    snake_x = WINDOW_WIDTH // <span class="number">2</span></span><br><span class="line">    snake_y = WINDOW_HEIGHT // <span class="number">2</span></span><br><span class="line">    snake_x_change = <span class="number">0</span></span><br><span class="line">    snake_y_change = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 食物的初始位置</span></span><br><span class="line">    food_x = <span class="built_in">round</span>(random.randrange(<span class="number">0</span>, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE</span><br><span class="line">    food_y = <span class="built_in">round</span>(random.randrange(<span class="number">0</span>, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> game_over:</span><br><span class="line">        <span class="keyword">for</span> event <span class="keyword">in</span> pygame.event.get():</span><br><span class="line">            <span class="keyword">if</span> event.<span class="built_in">type</span> == pygame.QUIT:</span><br><span class="line">                game_over = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">elif</span> event.<span class="built_in">type</span> == pygame.KEYDOWN:</span><br><span class="line">                <span class="keyword">if</span> event.key == pygame.K_LEFT:</span><br><span class="line">                    snake_x_change = -BLOCK_SIZE</span><br><span class="line">                    snake_y_change = <span class="number">0</span></span><br><span class="line">                <span class="keyword">elif</span> event.key == pygame.K_RIGHT:</span><br><span class="line">                    snake_x_change = BLOCK_SIZE</span><br><span class="line">                    snake_y_change = <span class="number">0</span></span><br><span class="line">                <span class="keyword">elif</span> event.key == pygame.K_UP:</span><br><span class="line">                    snake_y_change = -BLOCK_SIZE</span><br><span class="line">                    snake_x_change = <span class="number">0</span></span><br><span class="line">                <span class="keyword">elif</span> event.key == pygame.K_DOWN:</span><br><span class="line">                    snake_y_change = BLOCK_SIZE</span><br><span class="line">                    snake_x_change = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新蛇的位置</span></span><br><span class="line">        snake_x += snake_x_change</span><br><span class="line">        snake_y += snake_y_change</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 绘制游戏窗口</span></span><br><span class="line">        window.fill(BLACK)</span><br><span class="line">        pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE])</span><br><span class="line">        pygame.draw.rect(window, WHITE, [snake_x, snake_y, BLOCK_SIZE, BLOCK_SIZE])</span><br><span class="line">        pygame.display.update()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 碰撞检测</span></span><br><span class="line">        <span class="keyword">if</span> snake_x == food_x <span class="keyword">and</span> snake_y == food_y:</span><br><span class="line">            food_x = <span class="built_in">round</span>(random.randrange(<span class="number">0</span>, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE</span><br><span class="line">            food_y = <span class="built_in">round</span>(random.randrange(<span class="number">0</span>, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE</span><br><span class="line"></span><br><span class="line">        clock.tick(<span class="number">20</span>)  <span class="comment"># 控制游戏的帧率</span></span><br><span class="line"></span><br><span class="line">    pygame.quit()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动游戏</span></span><br><span class="line">game_loop()</span><br><span class="line"></span><br></pre></td></tr></table></figure><img src="/Use-AI-to-write-a-snake-game/20230601172055.gif" class="" title="img"><p>不知道各位看出什么问题了没有，问题如下：</p><ol type="1"><li><p>动作太快了，完全是地狱级别的难度；</p></li><li><p>蛇似乎不会增长，吃了物品没有变长；</p></li><li><p>撞到墙壁后游戏虽然停止了，但是没任何提示，蛇就这么消失了；</p></li></ol><p>于是，我只能让chatGPT再帮我优化代码：</p><img src="/Use-AI-to-write-a-snake-game/20230601172059.png" class="" title="img"><p>当然，又有了其他问题，</p><ol type="1"><li><p>蛇在撞到自己之后没有办法结束游戏</p></li><li><p>游戏无法退出</p></li><li><p>游戏无法重新开始</p></li><li><p>Game Over之后显示的是乱码</p></li></ol><p>当然，有编程经验的人大概能明白，乱码的话一定是因为中文显示不正确，而目前，我也没打算支持中文。干脆用英文好了。</p><p>于是，我又开始提需求，在连续三四轮这种询问之后，终于来到了最后一次：</p><img src="/Use-AI-to-write-a-snake-game/20230601172104.png" class="" title="img"><p>然后我得到了如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pygame</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 游戏窗口的大小</span></span><br><span class="line">WINDOW_WIDTH = <span class="number">800</span></span><br><span class="line">WINDOW_HEIGHT = <span class="number">600</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 蛇身和食物的大小</span></span><br><span class="line">BLOCK_SIZE = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义颜色</span></span><br><span class="line">WHITE = (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>)</span><br><span class="line">BLACK = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">RED = (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 Pygame</span></span><br><span class="line">pygame.init()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建游戏窗口</span></span><br><span class="line">window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))</span><br><span class="line">pygame.display.set_caption(<span class="string">&quot;贪吃蛇游戏&quot;</span>)</span><br><span class="line">pygame.display.flip()</span><br><span class="line"></span><br><span class="line">clock = pygame.time.Clock()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">game_loop</span>():</span><br><span class="line">    game_over = <span class="literal">False</span></span><br><span class="line">    game_restart = <span class="literal">False</span></span><br><span class="line">    game_paused = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> game_restart:</span><br><span class="line">        <span class="keyword">if</span> game_over:</span><br><span class="line">            show_game_over()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 蛇的初始位置和速度</span></span><br><span class="line">        snake_x = WINDOW_WIDTH // <span class="number">2</span></span><br><span class="line">        snake_y = WINDOW_HEIGHT // <span class="number">2</span></span><br><span class="line">        snake_x_change = <span class="number">0</span></span><br><span class="line">        snake_y_change = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 蛇身的初始位置和长度</span></span><br><span class="line">        snake_body = []</span><br><span class="line">        snake_length = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 食物的初始位置</span></span><br><span class="line">        food_x = <span class="built_in">round</span>(random.randrange(<span class="number">0</span>, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE</span><br><span class="line">        food_y = <span class="built_in">round</span>(random.randrange(<span class="number">0</span>, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE</span><br><span class="line"></span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> game_over:</span><br><span class="line">            <span class="keyword">for</span> event <span class="keyword">in</span> pygame.event.get():</span><br><span class="line">                <span class="keyword">if</span> event.<span class="built_in">type</span> == pygame.QUIT:</span><br><span class="line">                    pygame.quit()</span><br><span class="line">                    quit()</span><br><span class="line">                <span class="keyword">elif</span> event.<span class="built_in">type</span> == pygame.KEYDOWN:</span><br><span class="line">                    <span class="keyword">if</span> event.key == pygame.K_LEFT <span class="keyword">and</span> snake_x_change != BLOCK_SIZE:</span><br><span class="line">                        snake_x_change = -BLOCK_SIZE</span><br><span class="line">                        snake_y_change = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">elif</span> event.key == pygame.K_RIGHT <span class="keyword">and</span> snake_x_change != -BLOCK_SIZE:</span><br><span class="line">                        snake_x_change = BLOCK_SIZE</span><br><span class="line">                        snake_y_change = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">elif</span> event.key == pygame.K_UP <span class="keyword">and</span> snake_y_change != BLOCK_SIZE:</span><br><span class="line">                        snake_y_change = -BLOCK_SIZE</span><br><span class="line">                        snake_x_change = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">elif</span> event.key == pygame.K_DOWN <span class="keyword">and</span> snake_y_change != -BLOCK_SIZE:</span><br><span class="line">                        snake_y_change = BLOCK_SIZE</span><br><span class="line">                        snake_x_change = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">elif</span> event.key == pygame.K_SPACE:</span><br><span class="line">                        game_paused = <span class="keyword">not</span> game_paused</span><br><span class="line">                    <span class="keyword">elif</span> event.key == pygame.K_q:</span><br><span class="line">                        pygame.quit()</span><br><span class="line">                        quit()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> game_paused:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新蛇的位置</span></span><br><span class="line">            snake_x += snake_x_change</span><br><span class="line">            snake_y += snake_y_change</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 碰撞检测</span></span><br><span class="line">            <span class="keyword">if</span> snake_x == food_x <span class="keyword">and</span> snake_y == food_y:</span><br><span class="line">                <span class="comment"># 蛇吃到了食物，增加长度和得分</span></span><br><span class="line">                snake_length += <span class="number">1</span></span><br><span class="line">                score += <span class="number">1</span></span><br><span class="line">                food_x = <span class="built_in">round</span>(random.randrange(<span class="number">0</span>, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE</span><br><span class="line">                food_y = <span class="built_in">round</span>(random.randrange(<span class="number">0</span>, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新蛇的身体</span></span><br><span class="line">            snake_head = [snake_x, snake_y]</span><br><span class="line">            snake_body.append(snake_head)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(snake_body) &gt; snake_length:</span><br><span class="line">                <span class="keyword">del</span> snake_body[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 检测碰撞</span></span><br><span class="line">            <span class="keyword">if</span> snake_head <span class="keyword">in</span> snake_body[:-<span class="number">1</span>] <span class="keyword">or</span> snake_x &lt; <span class="number">0</span> <span class="keyword">or</span> snake_x &gt;= WINDOW_WIDTH <span class="keyword">or</span> snake_y &lt; <span class="number">0</span> <span class="keyword">or</span> snake_y &gt;= WINDOW_HEIGHT:</span><br><span class="line">                game_over = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 绘制游戏窗口</span></span><br><span class="line">            window.fill(BLACK)</span><br><span class="line">            pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE])</span><br><span class="line">            <span class="keyword">for</span> body_part <span class="keyword">in</span> snake_body:</span><br><span class="line">                pygame.draw.rect(window, WHITE, [body_part[<span class="number">0</span>], body_part[<span class="number">1</span>], BLOCK_SIZE, BLOCK_SIZE])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 显示实时分数</span></span><br><span class="line">            font = pygame.font.Font(<span class="literal">None</span>, <span class="number">28</span>)</span><br><span class="line">            text = font.render(<span class="string">&quot;Score: &quot;</span> + <span class="built_in">str</span>(score), <span class="literal">True</span>, WHITE)</span><br><span class="line">            window.blit(text, (<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">            pygame.display.update()</span><br><span class="line">            clock.tick(<span class="number">10</span>)  <span class="comment"># 控制游戏的帧率</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重新开始游戏</span></span><br><span class="line">    game_loop()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_game_over</span>():</span><br><span class="line">    font = pygame.font.Font(<span class="literal">None</span>, <span class="number">36</span>)</span><br><span class="line">    text = font.render(<span class="string">&quot;Game Over. Press Space to restart&quot;</span>, <span class="literal">True</span>, WHITE)</span><br><span class="line">    text_rect = text.get_rect(center=(WINDOW_WIDTH // <span class="number">2</span>, WINDOW_HEIGHT // <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> event <span class="keyword">in</span> pygame.event.get():</span><br><span class="line">            <span class="keyword">if</span> event.<span class="built_in">type</span> == pygame.QUIT:</span><br><span class="line">                pygame.quit()</span><br><span class="line">                quit()</span><br><span class="line">            <span class="keyword">elif</span> event.<span class="built_in">type</span> == pygame.KEYDOWN <span class="keyword">and</span> event.key == pygame.K_SPACE:</span><br><span class="line">                game_loop()</span><br><span class="line"></span><br><span class="line">        window.fill(BLACK)</span><br><span class="line">        window.blit(text, text_rect)</span><br><span class="line">        pygame.display.update()</span><br><span class="line">        clock.tick(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动游戏</span></span><br><span class="line">game_loop()</span><br></pre></td></tr></table></figure><p>大家可以看看效果：</p><img src="/Use-AI-to-write-a-snake-game/20230601172110.gif" class="" title="img"><p>我们可以看到，这个游戏相对来说是比较完整了，速度正常了，有计分系统，蛇能正常增长，碰到自己游戏会结束，碰到墙壁后也会Game Over，并且可以通过SPACE来重新开始。并且，我还加入了按Q键的时候退出的功能。</p><p>当然，还可以继续完善，比如随着时间的流逝，速度上可以渐渐加快等等。就看你怎么想，然后其他的交给chatGPT。</p><p>通过这次的示例演示，其中重点不是教大家如何做一个贪吃蛇游戏，而是教大家如何利用chatGPT来解决你需要解决的问题。当然，我需要收回我开头说的话，chatGPT并不能帮你解决你不熟悉的问题。就比如，如果我完全不懂这其中内容的话，可能我窗口都打不开，我完全都不知道我什么时候才能解决Mac系统中不一样的部分，而也正是因为有一些简单的经验，才让我考虑的那个层面，从而针对性提问解决了问题。</p><p>所以要记住，AI并不能帮你解决你完全不懂的问题，起码，你要知道你想问什么，也要知道问题大概卡在哪里了，针对性继续提问。</p><p>最后，友情提示一下，不要用API来完成这一次次的对话，经验之谈，去买个Plus，比API交互便宜多了。你看那一串串的代码重复的给你写出来，你完全不知道会耗费多少Token。那些宝贵的Token，还是用在聊天窗无法完成的任务上比较合适。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在完成一些简单的任务之后，我觉得ChatGPT可以帮我完成一些更为复杂，甚至于可能我并不是特别擅长的任务，而我想到了，也许，可以帮我写一个简单的游戏。&lt;/p&gt;</summary>
    
    
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>13 使用多步提示语让AI帮你写测试</title>
    <link href="https://hivan.me/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/"/>
    <id>https://hivan.me/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/</id>
    <published>2023-05-28T09:17:32.000Z</published>
    <updated>2023-06-01T12:56:08.576Z</updated>
    
    <content type="html"><![CDATA[<p>Hi，大家好，我是茶桁。</p><p>很遗憾在上一讲，也就是第12讲的时候，咱们对于利用AI写一个VBA宏来执行Excel任务的过程并不顺利，仔细想来既然大家都在这里看这个系列文章了，应该也基本都会Python的，所以一个Excel自动化也并无太大影响，毕竟，这种商业软件的集成一定是早晚的事情，咱们也不必在这里死磕这一个问题。</p><span id="more"></span><p>那么本节课程呢，我们会通过chatGPT的不断交互，去完成一个测试任务。</p><p>在很多时候，我们探索性开发一些功能可以极大提高我们的效率，但是这个过程并不能做成一个完整的产品。我们理想中的产品应该是“自动化”的，我们只需要用自然语言输入自己的需求，对应的代码就自动写出来了。</p><p>那么如果中间出现了问题怎么办？当然是AI可以自己拿到反馈自己更正自己了，完全不需要人工去介入调试。</p><p>下面，让我们开始吧。</p><h3 id="代码的起源">代码的起源</h3><p>让AI自己调试自己的需求听起来是不是很不可思议？随着GPT-4的发布，还有就是未来模型能力的进一步增强，这个骑士并不是遥不可及。是的，我又在这里贩卖焦虑了，那些低廉的测试们，想要自己的退路了吗？</p><p>眼下，我们只有GPT-3.5的API权限。所以我们这一次无法一步到底，目标还是需要低一点，先通过大语言模型，帮助我们写单元测试代码。</p><p>整个过程是一个自动档的体验，只是能够提供的能力还相对比较简单，仅限于为现有代码提供单元测试而已。</p><p>其实，很早的时候OpenAI官方就在Cookbook中提供了类似的思路和示例，可以参见<a href="https://github.com/openai/openai-cookbook/blob/main/examples/Unit_test_writing_using_a_multi-step_prompt.ipynb"><strong>Unit test writing using a multi-step prompt</strong></a>，不过这个例子里面的代码已经无法使用了，因为对应的<code>code-davinci-002</code>模型已经被OpenAI下线了。但是示例里，分步骤分析问题，通过多个Prompts来完成单元测试的想法，还是完全可以拿来借鉴的。</p><p>虽然模型会变，代码也就需要跟着会改变，但是我相信这一课一定能打开你的思路，随着你拿到GPT-4的API乃至未来可能会出现的GPT-5，你都完全可以完成更复杂的“自动写代码”的程序。</p><h3 id="先从一个起点开始">先从一个起点开始</h3><p>为了写测试，我们需要首先编写一个程序。为了避免选择 Leetcode 这样的题库，因为这些题目可能在 AI 的训练数据集中出现并且AI已经知道了答案。因此，我选择了一个有趣的小题目，即让 Python 根据我们输入的一个整数代表的秒数，格式化成一段自然语言描述的时间。例如，输入 1 将返回 1秒，输入 61 将返回 1分钟1秒。为了实现这个功能，我们需要考虑处理输入、分析输入并将其转化为可读的输出。我们可以使用 Python 中的 datetime 模块和字符串处理功能来完成这个任务。我们可以编写单元测试来确保代码的正确性，包括测试各种输入组合的情况，并确保输出符合预期。我们还可以考虑添加错误处理，例如，如果输入不是整数，则返回错误消息。因此，编写测试代码可以帮助我们确保代码的正确性并提高代码的可读性。</p><p>让我们从简单需求开始：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">咱们用Python写一个函数，进行时间格式化的输出，比如:</span><br><span class="line">输入 输出</span><br><span class="line"><span class="number">1</span>          <span class="number">1</span>s</span><br><span class="line"><span class="number">61</span>        <span class="number">1</span>min1s</span><br><span class="line"></span><br><span class="line">要求仅需要格式化到小时(?h?min?s)就可以了。</span><br></pre></td></tr></table></figure><p>当然，我们是讲这个需求直接抛给chatGPT，让其待我们实现，然后我得到了如下程序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">format_time</span>(<span class="params">seconds</span>):</span><br><span class="line">    hours = seconds // <span class="number">3600</span></span><br><span class="line">    minutes = (seconds % <span class="number">3600</span>) // <span class="number">60</span></span><br><span class="line">    seconds = seconds % <span class="number">60</span></span><br><span class="line"></span><br><span class="line">    time_string = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> hours &gt; <span class="number">0</span>:</span><br><span class="line">        time_string += <span class="string">f&quot;<span class="subst">&#123;hours&#125;</span>h&quot;</span></span><br><span class="line">    <span class="keyword">if</span> minutes &gt; <span class="number">0</span>:</span><br><span class="line">        time_string += <span class="string">f&quot;<span class="subst">&#123;minutes&#125;</span>min&quot;</span></span><br><span class="line">    <span class="keyword">if</span> seconds &gt; <span class="number">0</span>:</span><br><span class="line">        time_string += <span class="string">f&quot;<span class="subst">&#123;seconds&#125;</span>s&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> time_string</span><br></pre></td></tr></table></figure><p>并且，chatGPT还十分贴心的给出了一些示例用法：</p><img src="/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/20230601171814.png" class="" title="img"><p>现在让我们放在VSCode里尝试一下：</p><img src="/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/20230601171823.png" class="" title="img"><p>似乎很顺利，没有报错，可以执行，而且目测代码也完成了我们想要的基本功能，使用chatGPT给的示例测试一下看看：</p><img src="/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/20230601171829.png" class="" title="img"><p>恩，完全没问题。</p><p>好了，现在可以让我们完成单元测试的代码了，当然，依然是将问题交给chatGPT：</p><img src="/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/20230601171833.png" class="" title="img"><p>然后我得到了如下回复：</p><img src="/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/20230601171841.png" class="" title="img"><p>这里我们需要安装一个新库，在你的命令行内输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytest</span><br></pre></td></tr></table></figure><p>将chatGPT给到的代码改动一下，因为毕竟我们是在一个文件内定义的类，并不存在引入的情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pytest</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_format_time</span>():</span><br><span class="line">    <span class="comment"># 测试秒数为 1 的情况</span></span><br><span class="line">    <span class="keyword">assert</span> format_time(<span class="number">1</span>) == <span class="string">&quot;1s&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试秒数为 61 的情况</span></span><br><span class="line">    <span class="keyword">assert</span> format_time(<span class="number">61</span>) == <span class="string">&quot;1min1s&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试秒数为 3661 的情况</span></span><br><span class="line">    <span class="keyword">assert</span> format_time(<span class="number">3661</span>) == <span class="string">&quot;1h1min1s&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试秒数为 3600 的情况</span></span><br><span class="line">    <span class="keyword">assert</span> format_time(<span class="number">3600</span>) == <span class="string">&quot;1h&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试秒数为 0 的情况</span></span><br><span class="line">    <span class="keyword">assert</span> format_time(<span class="number">0</span>) == <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试负数秒数的情况</span></span><br><span class="line">    <span class="keyword">assert</span> format_time(-<span class="number">10</span>) == <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试较大秒数的情况</span></span><br><span class="line">    <span class="keyword">assert</span> format_time(<span class="number">123456789</span>) == <span class="string">&quot;34293h21min29s&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在此添加更多的测试用例...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行单元测试</span></span><br><span class="line"><span class="comment"># pytest.main()</span></span><br></pre></td></tr></table></figure><p>好的，我们的单元测试写完了, 下课，咱们下期再见。</p><p>。。。</p><p>当然是开玩笑的，哪有这么简单。不知道有多少人真的做过程序员或者测试，聪明如你们，当然能从这段代码中看到还存在问题</p><p>虽然这个测试考虑到了负数，考虑到了超过24小时较大秒数的情况，但是依然有未考虑到的情况，比如说，我们如果输入了浮点数1.0， 或者字符串abc，在活着Null这样的空值怎么办？虽然前端可以控制输入类型来避免一些情况发生，但是无论如何，我们无法相信前端，并不是因为前端程序员不给力，而是前端是可以被篡改的。我们不知道前端传回来的内容会发生怎样的变化，所以后端校验必须严谨而完整。</p><p>下面，让我们继续完善吧，接着我们就要离开WebGPT的交互，转而使用API了，所以请在您的代码内设置好相应的 <code>openai.api_key</code></p><h3 id="分步提示语">分步提示语</h3><p>我们要明白，就算有了AI，也并不是把问题一股脑的塞给他就可以解决了。我们需要的是反过来自己思考，如果我们自己来为一段代码写单元测试，我们自己会怎么做？</p><p>而这些想法，最后就会变成在chatGPT里的Prompts，最终由chatGPT告诉我们答案。</p><p>在文章开头我分享的Cookbook里的那个例子里就提供了一份很好的思路，在里面将问题拆成了三个步骤：</p><ol type="1"><li><p>把代码提交给大语言模型，让大语言模型解释一下，这个代码是在干什么。这个步骤很重要，因为它可以帮助我们更好地理解代码的含义以及逻辑。如果大语言模型的解释不够详细，我们可以再次提交代码，直到我们完全理解了它的含义。</p></li><li><p>把代码以及代码的解释一起交给大语言模型，让大语言模型规划一下，针对这个代码逻辑，我们到底要写哪几个 TestCase。如果在这个过程里，大语言模型规划的 TestCase 数量太少，那么我们可以重复第二步，让 AI 多生成几个 TestCase。这样可以帮助我们更全面地测试代码，确保代码质量。</p></li><li><p>针对上面生成的 TestCase 的详细描述，我们再次提交给大语言模型，让它根据这些描述生成具体的测试代码。在这个过程中，我们还会对生成的代码进行一次语法检查，如果语法检查没法通过，我们就要让 AI 重新生成一下。这个可以避免因为大语言模型的概率采样不稳定，导致生成的代码无法运行的问题。同时，我们还可以对生成的代码进行一些修改，比如添加注释，让代码更加易读易懂。这个步骤可以帮助我们更好地理解代码的结构，以及代码所要实现的功能。</p></li></ol><p>到最后，我们当然需要实际运行一下这些代码，看看我们的代码是否能够通过这些自动化测试。</p><h3 id="自己的代码自己解释">自己的代码自己解释</h3><p>我们将步骤一步步拆解开来，通过Python程序把整个过程“自动化”：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gpt</span>(<span class="params">prompt, model = <span class="string">&#x27;text-davinci-002&#x27;</span>, temperature = <span class="number">0.4</span>, max_tokens = <span class="number">1000</span>, top_p = <span class="number">1</span>, stop = [<span class="string">&#x27;\n\n&#x27;</span>, <span class="string">&#x27;\n\t\n&#x27;</span>, <span class="string">&#x27;\n   \n&#x27;</span>]</span>):</span><br><span class="line">    response = openai.Completion.create(</span><br><span class="line">        model = model,</span><br><span class="line">        prompt = prompt,</span><br><span class="line">        temperature = temperature,</span><br><span class="line">        max_tokens = max_tokens,</span><br><span class="line">        top_p = top_p,</span><br><span class="line">        stop = stop</span><br><span class="line">    )</span><br><span class="line">    message = response[<span class="string">&#x27;choices&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> message</span><br><span class="line"></span><br><span class="line">code = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">def format_time(seconds):</span></span><br><span class="line"><span class="string">    hours = seconds // 3600</span></span><br><span class="line"><span class="string">    minutes = (seconds % 3600) // 60</span></span><br><span class="line"><span class="string">    seconds = seconds % 60</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    time_string = &quot;&quot;</span></span><br><span class="line"><span class="string">    if hours &gt; 0:</span></span><br><span class="line"><span class="string">        time_string += f&quot;&#123;hours&#125;h&quot;</span></span><br><span class="line"><span class="string">    if minutes &gt; 0:</span></span><br><span class="line"><span class="string">        time_string += f&quot;&#123;minutes&#125;min&quot;</span></span><br><span class="line"><span class="string">    if seconds &gt; 0:</span></span><br><span class="line"><span class="string">        time_string += f&quot;&#123;seconds&#125;s&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return time_string</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">explain_code</span>(<span class="params">function_to_test, unit_test_package = <span class="string">&#x27;pytest&#x27;</span></span>):</span><br><span class="line">    prompt = <span class="string">f&quot;&quot;&quot;# How to write great unit tests with <span class="subst">&#123;unit_test_package&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In this advanced tutorial for experts, we&#x27;ll use Python 3.10 and `<span class="subst">&#123;unit_test_package&#125;</span>` to write a suite of unit tests to verify the behavior of the following function.</span></span><br><span class="line"><span class="string">```python</span></span><br><span class="line"><span class="string"><span class="subst">&#123;function_to_test&#125;</span></span></span><br><span class="line"><span class="string">```</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Before writing any unit tests, let&#x27;s review what each element of the function is doing exactly and what the author&#x27;s intentions may have been.</span></span><br><span class="line"><span class="string">- First,</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    response = gpt(prompt)</span><br><span class="line">    <span class="keyword">return</span> response, prompt</span><br><span class="line"></span><br><span class="line">code_explaination, prompt_to_explain_code = explain_code(code)</span><br><span class="line"><span class="built_in">print</span>(code_explaination)</span><br></pre></td></tr></table></figure><p>在这一步中，我们所写的代码做了以下几件事情：</p><p>首先，我们定义一个gpt的函数，对调用GPT3.5的模型做了一个简单的封装。其中有两点需要特别注意一下：</p><ol type="1"><li><p>我们默认使用了 <code>text-davinci-002</code> 模型，这是一个通过监督学习微调的生成文本的模型。因为这里我们希望生成目标明确的文本的代码解释，所以选用了这个模型。</p></li><li><p>我们对 stop 做了特殊的设置，只要连续两个换行或者类似连续两个换行的情况出现，就中止数据的生成。这是避免模型一口气连测试代码也生成出来。那样的话，我们没法对测试代码的生成提出具体的要求。通过 stop，我们可以确保在第一步，只解释现在的功能代码有什么用。此外，我们还对 stop 进行了调优，确保在生成代码解释时不会因为过度使用 stop 而出现信息不完整的情况。具体来说，我们设置了一个阈值，只有当连续两个换行或类似换行的情况出现的次数达到阈值时，才会中止数据的生成。</p></li></ol><p>接下来，我们可以进一步提高代码的解释的准确性。我们可以通过以下几个步骤来实现：</p><ol type="1"><li><p>确定使用pytest测试包。</p></li><li><p>提供要测试的代码以及相应的上下文。</p></li><li><p>指示AI对代码的功能进行详细描述。</p></li><li><p>使用“- First”等引导词，引导GPT模型逐步分行描述代码的功能。</p></li></ol><p>这些步骤可以让我们的代码解释更加清晰明了。此外，我们也可以通过提供更详细的上下文和示例来帮助GPT模型对代码的功能进行更准确的描述。例如，我们可以提供更多的测试用例，以确保代码的正确性，并帮助GPT模型更好地理解代码的功能。同时，我们还可以提供更多的注释和解释，以便其他人更好地了解我们的代码。</p><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">```python</span><br><span class="line">  seconds // <span class="number">3600</span></span><br><span class="line">  ```</span><br><span class="line">  <span class="keyword">is</span> dividing the number of seconds by <span class="number">3600</span> <span class="keyword">and</span> discarding the remainder. For example, `<span class="number">7200</span> // <span class="number">3600</span>` returns `<span class="number">2</span>` because there are `<span class="number">2</span>` hours <span class="keyword">in</span> `<span class="number">7200</span>` seconds.</span><br><span class="line">- Second,</span><br><span class="line">  ```python</span><br><span class="line">  (seconds % <span class="number">3600</span>) // <span class="number">60</span></span><br><span class="line">  ```</span><br><span class="line">  <span class="keyword">is</span> dividing the remainder of the division of `seconds` by `<span class="number">3600</span>` by `<span class="number">60</span>`. For example, `<span class="number">7200</span> % <span class="number">3600</span>` returns `<span class="number">0</span>` because there are no seconds remaining after the division by `<span class="number">3600</span>`. Therefore, `(<span class="number">7200</span> % <span class="number">3600</span>) // <span class="number">60</span>` returns `<span class="number">0</span>` because there are no minutes remaining after the division by `<span class="number">60</span>`.</span><br><span class="line">- Third,</span><br><span class="line">  ```python</span><br><span class="line">  seconds % <span class="number">60</span></span><br><span class="line">  ```</span><br><span class="line">  <span class="keyword">is</span> returning the remainder of the division of `seconds` by `<span class="number">60</span>`. For example, `<span class="number">7200</span> % <span class="number">60</span>` returns `<span class="number">0</span>` because there are no seconds remaining after the division by `<span class="number">60</span>`.</span><br><span class="line">- Fourth,</span><br><span class="line">  ```python</span><br><span class="line">  time_string = <span class="string">&quot;&quot;</span></span><br><span class="line">  ```</span><br><span class="line">  <span class="keyword">is</span> initializing an empty string to be used to store the formatted time.</span><br><span class="line">- Fifth,</span><br><span class="line">  ```python</span><br><span class="line">  <span class="keyword">if</span> hours &gt; <span class="number">0</span>:</span><br><span class="line">  ```</span><br><span class="line">  <span class="keyword">is</span> checking <span class="keyword">if</span> the number of `hours` <span class="keyword">is</span> greater than `<span class="number">0</span>`. If it <span class="keyword">is</span>, the following code will be executed.</span><br><span class="line">  ```python</span><br><span class="line">  time_string += <span class="string">f&quot;<span class="subst">&#123;hours&#125;</span>h&quot;</span></span><br><span class="line">  ```</span><br><span class="line">  This code <span class="keyword">is</span> adding the number of `hours` to the string `time_string` <span class="keyword">with</span> the `h` character after it. For example, <span class="keyword">if</span> `hours` <span class="keyword">is</span> `<span class="number">2</span>`, the string `time_string` will be updated to `<span class="string">&quot;2h&quot;</span>`.</span><br><span class="line">- Sixth,</span><br><span class="line">  ```python</span><br><span class="line">  <span class="keyword">if</span> minutes &gt; <span class="number">0</span>:</span><br><span class="line">  ```</span><br><span class="line">  <span class="keyword">is</span> checking <span class="keyword">if</span> the number of `minutes` <span class="keyword">is</span> greater than `<span class="number">0</span>`. If it <span class="keyword">is</span>, the following code will be executed.</span><br><span class="line">  ```python</span><br><span class="line">  time_string += <span class="string">f&quot;<span class="subst">&#123;minutes&#125;</span>min&quot;</span></span><br><span class="line">  ```</span><br><span class="line">  This code <span class="keyword">is</span> adding the number of `minutes` to the string `time_string` <span class="keyword">with</span> the `<span class="built_in">min</span>` characters after it. For example, <span class="keyword">if</span> `minutes` <span class="keyword">is</span> `<span class="number">2</span>`, the string `time_string` will be updated to `<span class="string">&quot;2min&quot;</span>`.</span><br><span class="line">- Seventh,</span><br><span class="line">  ```python</span><br><span class="line">  <span class="keyword">if</span> seconds &gt; <span class="number">0</span>:</span><br><span class="line">  ```</span><br><span class="line">  <span class="keyword">is</span> checking <span class="keyword">if</span> the number of `seconds` <span class="keyword">is</span> greater than `<span class="number">0</span>`. If it <span class="keyword">is</span>, the following code will be executed.</span><br><span class="line">  ```python</span><br><span class="line">  time_string += <span class="string">f&quot;<span class="subst">&#123;seconds&#125;</span>s&quot;</span></span><br><span class="line">  ```</span><br><span class="line">  This code <span class="keyword">is</span> adding the number of `seconds` to the string `time_string` <span class="keyword">with</span> the `s` character after it. For example, <span class="keyword">if</span> `seconds` <span class="keyword">is</span> `<span class="number">2</span>`, the string `time_string` will be updated to `<span class="string">&quot;2s&quot;</span>`.</span><br><span class="line">- Eighth,</span><br><span class="line">  ```python</span><br><span class="line">  <span class="keyword">return</span> time_string</span><br><span class="line">  ```</span><br><span class="line">  <span class="keyword">is</span> returning the string `time_string`.</span><br></pre></td></tr></table></figure><p>运行代码后，AI回复了我们几个步骤，详细秒数了我们格式化时间的代码是如何做的。</p><h3 id="自己的解释自己实现">自己的解释自己实现</h3><p>当然接下来，我们就需要根据生成的这个详细描述，请AI为我们制定一下具体的测试计划了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_a_test_plan</span>(<span class="params">full_code_explaination, unit_test_package=<span class="string">&quot;pytest&quot;</span></span>):</span><br><span class="line">    prompt_to_explain_a_plan = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">A good unit test suite should aim to:</span></span><br><span class="line"><span class="string">- Test the function&#x27;s behavior for a wide range of possible inputs</span></span><br><span class="line"><span class="string">- Test edge cases that the author may not have foreseen</span></span><br><span class="line"><span class="string">- Take advantage of the features of `<span class="subst">&#123;unit_test_package&#125;</span>` to make the tests easy to write and maintain</span></span><br><span class="line"><span class="string">- Be easy to read and understand, with clean code and descriptive names</span></span><br><span class="line"><span class="string">- Be deterministic, so that the tests always pass or fail in the same way</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">`<span class="subst">&#123;unit_test_package&#125;</span>` has many convenient features that make it easy to write and maintain unit tests. We&#x27;ll use them to write unit tests for the function above.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">For this particular function, we&#x27;ll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):</span></span><br><span class="line"><span class="string">-&quot;&quot;&quot;</span></span><br><span class="line">    prompt = full_code_explaination + prompt_to_explain_a_plan</span><br><span class="line">    response = gpt(prompt)</span><br><span class="line">    <span class="keyword">return</span> response, prompt</span><br><span class="line"></span><br><span class="line">test_plan, prompt_to_get_test_plan = generate_a_test_plan(prompt_to_explain_code + code_explaination)</span><br><span class="line"><span class="built_in">print</span>(test_plan)</span><br></pre></td></tr></table></figure><p>我们整个测试计划的提示语，同样经过了精心设计。我们首先对 AI 的测试用例做出了以下要求：</p><ol type="1"><li><p>在考虑输入范围时，测试用例应尽可能覆盖更广的范围。</p></li><li><p>AI 应考虑到一些边界条件，这些条件可能比代码作者预想的更加复杂。</p></li><li><p>我们希望 AI 能够充分利用 pytest 这个测试包的特性。</p></li><li><p>测试用例应该易于阅读和理解，测试代码应该简洁明了。</p></li><li><p>测试代码的输出结果应该是确定的，要么通过，要么失败，不应该有随机性。</p></li></ol><p>在这一步之后，我们并没有让 AI 立即开始编写测试代码。相反，我们提供了几个例子来让 AI 生成一系列示例。我们对测试用例的提示非常详细，这也是我们之前没有让 AI 直接生成测试用例的原因。因为这种方法无法在提示语中插入这些详细的要求。对于具体的测试用例，我们只能希望 AI 能够自行想出更多的例子。</p><p>最后，我们的提示语既包括了第一步要求解释代码内容的要求，也包括了 AI 生成的代码解释的要求，以及我们在这里新增的测试用例要求。这提供了非常详细的上下文，使得 AI 的表现更好，更具有逻辑性。此外，我们建议 AI 参考其他相关测试用例，以确保测试的全面性和正确性。</p><p>然后AI输出了结果给我：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">The `seconds` <span class="built_in">input</span> <span class="keyword">is</span> a positive integer:</span><br><span class="line">  - `seconds` <span class="keyword">is</span> less than <span class="number">60</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> equal to <span class="number">60</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> greater than <span class="number">60</span> but less than <span class="number">3600</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> equal to <span class="number">3600</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> greater than <span class="number">3600</span></span><br><span class="line">- The `seconds` <span class="built_in">input</span> <span class="keyword">is</span> a negative integer:</span><br><span class="line">  - `seconds` <span class="keyword">is</span> less than -<span class="number">60</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> equal to -<span class="number">60</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> greater than -<span class="number">60</span> but less than -<span class="number">3600</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> equal to -<span class="number">3600</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> greater than -<span class="number">3600</span></span><br><span class="line">- The `seconds` <span class="built_in">input</span> <span class="keyword">is</span> a <span class="built_in">float</span>:</span><br><span class="line">  - `seconds` <span class="keyword">is</span> less than <span class="number">0.0</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> equal to <span class="number">0.0</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> greater than <span class="number">0.0</span> but less than <span class="number">60.0</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> equal to <span class="number">60.0</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> greater than <span class="number">60.0</span> but less than <span class="number">3600.0</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> equal to <span class="number">3600.0</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> greater than <span class="number">3600.0</span></span><br><span class="line">- The `seconds` <span class="built_in">input</span> <span class="keyword">is</span> a string:</span><br><span class="line">  - `seconds` <span class="keyword">is</span> an empty string</span><br><span class="line">  - `seconds` <span class="keyword">is</span> a string that can be parsed to an integer</span><br><span class="line">  - `seconds` <span class="keyword">is</span> a string that can be parsed to a <span class="built_in">float</span></span><br><span class="line">  - `seconds` <span class="keyword">is</span> a string that cannot be parsed to an integer <span class="keyword">or</span> a <span class="built_in">float</span></span><br><span class="line">- The `seconds` <span class="built_in">input</span> <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">  - `seconds` <span class="keyword">is</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure><p>我运行了一下这个代码，可以看到，AI 提供了很多测试用例。并且，里面考虑了好几种情况，包括我们前面提到的负数这样的特殊条件，也包括输入字符串，以及 None 这样的内容。不仅如此，我们还可以探索更多的情况，例如小数和分数等。此外，我们可以调整代码中的参数，来观察AI生成的测试用例数量和质量。</p><p>不过，生成哪些用例其实是有一定的随机性的。这个也是大语言模型的一个缺点，就是可控性差。有时候，AI 可能就只生成了 3 个用例，那样的话就会有很多情况我们的用例覆盖不到。</p><p>所以，我们可以在生成用例之后，加一个步骤，检查一下到底生成了多少个用例。如果太少的话，我们就让 AI 再生成一些。我在下面给了一段示例代码，通过“-”这样一个换行加横杆的标记来判断之前生成的测试用例数量，如果比我们设定的下限少，我们就再添加一段提示语，让 AI 再生成一些。</p><p>这里的提示语，我们要特别提醒 AI 考虑一下测试罕见情况和边界条件，例如极大或极小的输入值，或者一些异常情况的处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">not_enough_test_plan = <span class="string">&quot;&quot;&quot;The function is called with a valid number of seconds</span></span><br><span class="line"><span class="string">    - `format_time(1)` should return `&quot;1s&quot;`</span></span><br><span class="line"><span class="string">    - `format_time(59)` should return `&quot;59s&quot;`</span></span><br><span class="line"><span class="string">    - `format_time(60)` should return `&quot;1min&quot;`</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">approx_min_cases_to_cover = <span class="number">7</span></span><br><span class="line">elaboration_needed = test_plan.count(<span class="string">&quot;\n-&quot;</span>) +<span class="number">1</span> &lt; approx_min_cases_to_cover </span><br><span class="line"><span class="keyword">if</span> elaboration_needed:</span><br><span class="line">        prompt_to_elaborate_on_the_plan = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In addition to the scenarios above, we&#x27;ll also want to make sure we don&#x27;t forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):</span></span><br><span class="line"><span class="string">-&quot;&quot;&quot;</span></span><br><span class="line">        more_test_plan, prompt_to_get_test_plan = generate_a_test_plan(prompt_to_explain_code + code_explaination + not_enough_test_plan + prompt_to_elaborate_on_the_plan)</span><br><span class="line">        <span class="built_in">print</span>(more_test_plan)</span><br></pre></td></tr></table></figure><p>然后得到结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">The function <span class="keyword">is</span> called <span class="keyword">with</span> a valid number of seconds</span><br><span class="line">    - `format_time(<span class="number">1</span>)` should <span class="keyword">return</span> `<span class="string">&quot;1s&quot;</span>`</span><br><span class="line">    - `format_time(<span class="number">59</span>)` should <span class="keyword">return</span> `<span class="string">&quot;59s&quot;</span>`</span><br><span class="line">    - `format_time(<span class="number">60</span>)` should <span class="keyword">return</span> `<span class="string">&quot;1min&quot;</span>`</span><br><span class="line">- The function <span class="keyword">is</span> called <span class="keyword">with</span> an invalid number of seconds</span><br><span class="line">    - `format_time(-<span class="number">1</span>)` should <span class="keyword">raise</span> a `ValueError`</span><br><span class="line">    - `format_time(<span class="string">&quot;60&quot;</span>)` should <span class="keyword">raise</span> a `ValueError`</span><br><span class="line">- The function <span class="keyword">is</span> called <span class="keyword">with</span> a valid number of seconds <span class="keyword">and</span> the `hours`, `minutes`, <span class="keyword">or</span> `seconds` are `<span class="number">0</span>`</span><br><span class="line">    - `format_time(<span class="number">0</span>)` should <span class="keyword">return</span> `<span class="string">&quot;&quot;</span>`</span><br><span class="line">    - `format_time(<span class="number">3600</span>)` should <span class="keyword">return</span> `<span class="string">&quot;1h&quot;</span>`</span><br><span class="line">    - `format_time(<span class="number">7200</span>)` should <span class="keyword">return</span> `<span class="string">&quot;2h&quot;</span>`</span><br><span class="line">    - `format_time(<span class="number">7201</span>)` should <span class="keyword">return</span> `<span class="string">&quot;2h1s&quot;</span>`</span><br></pre></td></tr></table></figure><h3 id="自己的计划自己生成">自己的计划自己生成</h3><p>当然，有些情况下，生成的测试用例数会比我们的实际情况更少。这时候，我们需要想办法增加测试用例的数量，以便更全面地测试我们的代码。一种方法是增加测试数据的覆盖范围。我们可以通过添加一些边界值、特殊值、无效值等来增加测试用例的数量。</p><p>除了增加测试用例的数量，我们还可以增加测试用例的复杂度。这样可以更好地测试代码的鲁棒性和可扩展性。我们可以通过增加测试用例的步骤、条件等来增加测试用例的复杂度。</p><p>另外，为了提高测试用例的可读性和可维护性，我们可以将测试用例分为不同的类别，并为每个类别定义一个清晰的目标。例如，我们可以将测试用例按照输入数据的类型、函数的不同参数组合、不同的执行路径等进行分类。</p><p>对于这些分类，我们可以在提示语中指明要测试的具体内容，以帮助 AI 编写更加精确、全面的测试用例。同时，我们还可以提供一些代码示例或者代码注释来帮助 AI 理解我们要测试的功能代码。</p><p>需要注意的是，我们在生成提示语的时候，要尽可能保留原有的关键信息，以确保 AI 编写的测试用例符合我们的测试要求。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_test_cases</span>(<span class="params">function_to_test, unit_test_package=<span class="string">&quot;pytest&quot;</span></span>):</span><br><span class="line">    starter_comment = <span class="string">&quot;Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator&quot;</span></span><br><span class="line">    prompt_to_generate_the_unit_test = <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Before going into the individual tests, let&#x27;s first look at the complete suite of unit tests as a cohesive whole. We&#x27;ve added helpful comments to explain what each line does.</span></span><br><span class="line"><span class="string">```python</span></span><br><span class="line"><span class="string">import <span class="subst">&#123;unit_test_package&#125;</span>  # used for our unit tests</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="subst">&#123;function_to_test&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#<span class="subst">&#123;starter_comment&#125;</span>&quot;&quot;&quot;</span></span><br><span class="line">    full_unit_test_prompt = prompt_to_explain_code + code_explaination + test_plan + prompt_to_generate_the_unit_test</span><br><span class="line">    <span class="keyword">return</span> gpt(model=<span class="string">&quot;text-davinci-003&quot;</span>, prompt=full_unit_test_prompt, stop=<span class="string">&quot;```&quot;</span>), prompt_to_generate_the_unit_test</span><br><span class="line"></span><br><span class="line">unit_test_response, prompt_to_generate_the_unit_test = generate_test_cases(code)</span><br><span class="line"><span class="built_in">print</span>(unit_test_response)</span><br></pre></td></tr></table></figure><p>再次，AI为我生成了如下内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#The first element of the tuple is a string that describes the test case,</span></span><br><span class="line"><span class="comment">#and the second element is the input for the format_time function.</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&quot;test_case, seconds&quot;</span>, [</span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="comment"># Positive integer tests</span></span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Less than 60&quot;</span>, <span class="number">59</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to 60&quot;</span>, <span class="number">60</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than 60 but less than 3600&quot;</span>, <span class="number">7200</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to 3600&quot;</span>, <span class="number">3600</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than 3600&quot;</span>, <span class="number">7201</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="comment"># Negative integer tests</span></span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Less than -60&quot;</span>, -<span class="number">59</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to -60&quot;</span>, -<span class="number">60</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than -60 but less than -3600&quot;</span>, -<span class="number">7200</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to -3600&quot;</span>, -<span class="number">3600</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than -3600&quot;</span>, -<span class="number">7201</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="comment"># Float tests</span></span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Less than 0.0&quot;</span>, -<span class="number">1.0</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to 0.0&quot;</span>, <span class="number">0.0</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than 0.0 but less than 60.0&quot;</span>, <span class="number">59.9</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to 60.0&quot;</span>, <span class="number">60.0</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than 60.0 but less than 3600.0&quot;</span>, <span class="number">7200.0</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to 3600.0&quot;</span>, <span class="number">3600.0</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than 3600.0&quot;</span>, <span class="number">7201.0</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="comment"># String tests</span></span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Empty string&quot;</span>, <span class="string">&quot;&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;String that can be parsed to an integer&quot;</span>, <span class="string">&quot;7200&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;String that can be parsed to a float&quot;</span>, <span class="string">&quot;7200.0&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;String that cannot be parsed to an integer or a float&quot;</span>, <span class="string">&quot;7200.0.0&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="comment"># None test</span></span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;None&quot;</span>, <span class="literal">None</span></span>)</span></span></span><br><span class="line"><span class="params"><span class="meta">]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_format_time</span>(<span class="params">test_case, seconds</span>):</span><br><span class="line">    <span class="comment"># This test checks that the output of the format_time function</span></span><br><span class="line">    <span class="comment"># is the expected output for the given input.</span></span><br><span class="line">    expected_output = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> seconds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        hours = seconds // <span class="number">3600</span></span><br><span class="line">        minutes = (seconds % <span class="number">3600</span>) // <span class="number">60</span></span><br><span class="line">        seconds = seconds % <span class="number">60</span></span><br><span class="line">        <span class="keyword">if</span> hours &gt; <span class="number">0</span>:</span><br><span class="line">            expected_output += <span class="string">f&quot;<span class="subst">&#123;hours&#125;</span>h&quot;</span></span><br><span class="line">        <span class="keyword">if</span> minutes &gt; <span class="number">0</span>:</span><br><span class="line">            expected_output += <span class="string">f&quot;<span class="subst">&#123;minutes&#125;</span>min&quot;</span></span><br><span class="line">        <span class="keyword">if</span> seconds &gt; <span class="number">0</span>:</span><br><span class="line">            expected_output += <span class="string">f&quot;<span class="subst">&#123;seconds&#125;</span>s&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> format_time(seconds) == expected_output</span><br></pre></td></tr></table></figure><p>在运行这段提示语之后，我们可以得到最终输出的测试代码。除了正常情况下的测试，还包括异常输入的测试。如果输入错误，代码也会输出警告信息。这样，我们可以确保代码在各种情况下都能够正常工作。同时，我们也可以添加更多的测试用例，以覆盖更多的情况，从而提高代码的质量和可靠性。因此，如果您需要进行测试，可以使用这个测试代码作为起点，随时添加新的测试用例。这将有助于确保您的代码在各种情况下都能够正常运行。</p><p>最后还缺少了什么？当然是语法检查。</p><h3 id="通过-ast-库进行语法检查">通过 AST 库进行语法检查</h3><p>我们建议使用Python的AST库再次检查生成的测试代码的语法。这样我们可以确保我们的测试代码是具有正确语法的。需要注意的是，在检查语法时，我们不仅需要生成的测试代码，还需要原始的功能代码。这样才能通过语法检查。另外，为了更好地测试代码，您可以考虑添加一些额外的测试用例，以确保代码的正确性和稳定性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ast</span><br><span class="line">code_start_index = prompt_to_generate_the_unit_test.find(<span class="string">&quot;```python\n&quot;</span>) + <span class="built_in">len</span>(<span class="string">&quot;```python\n&quot;</span>)</span><br><span class="line">code_output = prompt_to_generate_the_unit_test[code_start_index:] + unit_test_response</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    ast.parse(code_output)</span><br><span class="line"><span class="keyword">except</span> SyntaxError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Syntax error in generated code: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>非常幸运，直接通过了语法检查。下一步，我们把对应的整个测试代码打印出来执行试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(code_output)</span><br></pre></td></tr></table></figure><p>输出结果最后AI帮我们生成的测试代码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pytest  <span class="comment"># used for our unit tests</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">format_time</span>(<span class="params">seconds</span>):</span><br><span class="line">    hours = seconds // <span class="number">3600</span></span><br><span class="line">    minutes = (seconds % <span class="number">3600</span>) // <span class="number">60</span></span><br><span class="line">    seconds = seconds % <span class="number">60</span></span><br><span class="line"></span><br><span class="line">    time_string = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> hours &gt; <span class="number">0</span>:</span><br><span class="line">        time_string += <span class="string">f&quot;<span class="subst">&#123;hours&#125;</span>h&quot;</span></span><br><span class="line">    <span class="keyword">if</span> minutes &gt; <span class="number">0</span>:</span><br><span class="line">        time_string += <span class="string">f&quot;<span class="subst">&#123;minutes&#125;</span>min&quot;</span></span><br><span class="line">    <span class="keyword">if</span> seconds &gt; <span class="number">0</span>:</span><br><span class="line">        time_string += <span class="string">f&quot;<span class="subst">&#123;seconds&#125;</span>s&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> time_string</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator.</span></span><br><span class="line"><span class="comment">#The first element of the tuple is a string that describes the test case,</span></span><br><span class="line"><span class="comment">#and the second element is the input for the format_time function.</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&quot;test_case, seconds&quot;</span>, [</span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="comment"># Positive integer tests</span></span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Less than 60&quot;</span>, <span class="number">59</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to 60&quot;</span>, <span class="number">60</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than 60 but less than 3600&quot;</span>, <span class="number">7200</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to 3600&quot;</span>, <span class="number">3600</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than 3600&quot;</span>, <span class="number">7201</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="comment"># Negative integer tests</span></span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Less than -60&quot;</span>, -<span class="number">59</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to -60&quot;</span>, -<span class="number">60</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than -60 but less than -3600&quot;</span>, -<span class="number">7200</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to -3600&quot;</span>, -<span class="number">3600</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than -3600&quot;</span>, -<span class="number">7201</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="comment"># Float tests</span></span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Less than 0.0&quot;</span>, -<span class="number">1.0</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to 0.0&quot;</span>, <span class="number">0.0</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than 0.0 but less than 60.0&quot;</span>, <span class="number">59.9</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to 60.0&quot;</span>, <span class="number">60.0</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than 60.0 but less than 3600.0&quot;</span>, <span class="number">7200.0</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Equal to 3600.0&quot;</span>, <span class="number">3600.0</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Greater than 3600.0&quot;</span>, <span class="number">7201.0</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="comment"># String tests</span></span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;Empty string&quot;</span>, <span class="string">&quot;&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;String that can be parsed to an integer&quot;</span>, <span class="string">&quot;7200&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;String that can be parsed to a float&quot;</span>, <span class="string">&quot;7200.0&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;String that cannot be parsed to an integer or a float&quot;</span>, <span class="string">&quot;7200.0.0&quot;</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="comment"># None test</span></span></span></span><br><span class="line"><span class="params"><span class="meta">    (<span class="params"><span class="string">&quot;None&quot;</span>, <span class="literal">None</span></span>)</span></span></span><br><span class="line"><span class="params"><span class="meta">]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_format_time</span>(<span class="params">test_case, seconds</span>):</span><br><span class="line">    <span class="comment"># This test checks that the output of the format_time function</span></span><br><span class="line">    <span class="comment"># is the expected output for the given input.</span></span><br><span class="line">    expected_output = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> seconds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        hours = seconds // <span class="number">3600</span></span><br><span class="line">        minutes = (seconds % <span class="number">3600</span>) // <span class="number">60</span></span><br><span class="line">        seconds = seconds % <span class="number">60</span></span><br><span class="line">        <span class="keyword">if</span> hours &gt; <span class="number">0</span>:</span><br><span class="line">            expected_output += <span class="string">f&quot;<span class="subst">&#123;hours&#125;</span>h&quot;</span></span><br><span class="line">        <span class="keyword">if</span> minutes &gt; <span class="number">0</span>:</span><br><span class="line">            expected_output += <span class="string">f&quot;<span class="subst">&#123;minutes&#125;</span>min&quot;</span></span><br><span class="line">        <span class="keyword">if</span> seconds &gt; <span class="number">0</span>:</span><br><span class="line">            expected_output += <span class="string">f&quot;<span class="subst">&#123;seconds&#125;</span>s&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> format_time(seconds) == expected_output</span><br></pre></td></tr></table></figure><h3 id="抓个bug试试">抓个BUG试试</h3><p>我们可以试着在 Notebook 里面调用一下 format_time(-1)，看看自动化测试跑得对不对。</p><img src="/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/20230601171855.png" class="" title="img"><p>如图可以看到，输入-1的时候，输出变成了59min59s, 确实AI生成的测试代码帮我们捕捉到了一个Bug。</p><h3 id="小结">小结</h3><p>好了，到这里这一讲也就结束了。我们不仅学会了如何利用一个方法，将一个问题拆分成多个提示语的步骤，循序渐进地让 AI 通过解释代码，构造测试用例，最后再根据代码的解释和设计的测试用例，生成最终的自动化测试，而且还学会了如何在这个过程中，增加更多的内容，以达到更全面的测试。</p><p>在生成整套测试代码的过程中，我们不需要人工地复制粘帖任何内容，全都是代码自动完成的，是一个“自动档”的过程。通过将一个问题拆分成多个提示语的步骤，我们的内容变得更加有条理、有逻辑，也更符合我们平时写文字的方式，而不是一股脑地把各种要求都放在提示语的开头，这在解决复杂问题时往往效果不好。</p><p>此外，我们还学会了使用多步提示语的好处。多步提示语带来的一个好处，就是能让 AI 考虑各种边界条件。在得到代码的解释之后，我们可以让 AI 考虑 -1、None 这样的特殊输入，从而涵盖更多的测试情况。这样，我们的测试代码最终真的抓住了程序里的 Bug。</p><p>回过头来看，如果我们只是直接把代码往 ChatGPT 里一贴，虽然也能生成测试用例，但是那些测试用例就比较欠考虑，不会涵盖各种边角的情况。因此，我们在生成测试用例的过程中，应该尽可能地提供更多的提示语，让 AI 的测试代码更加全面和详尽。</p><h3 id="思考题">思考题</h3><p>在本讲中，代码内容有点长，思考题部分需要你思考的内容更多。</p><ol type="1"><li><p>你可以试着减少我们的提示语或者提示步骤，看看生成的测试用例有什么样的变化。你可以尝试移除一些提示语，或者更换提示步骤的顺序，从而得到不同的测试结果。</p></li><li><p>目前我们的代码是以过程式方式一步步演示整个测试代码是如何生成的。如果语法检查出错了，我们实际上应该从头开始重试一遍，再次生成测试代码。你可以尝试将整个代码封装修改，变成一个会自动重试 3 次的函数。这样，我们就可以直接调用这个函数，为 Python 代码生成自动化测试了。</p></li><li><p>我们本讲中的提示语是借鉴了 OpenAI Cookbook 的样例。你可以尝试总结一下，这些提示语中有哪些常用的方法是值得借鉴的。</p></li></ol><p>欢迎你将你的思考结果分享在评论区，同时也欢迎你将本讲分享给感兴趣的朋友。我们下一讲再见。</p><h3 id="推荐阅读">推荐阅读</h3><p>我们之所以要循序渐进地提示 AI，让 AI 先生成例子再生成代码，是因为现在的大型语言模型具有一种名为“思维链（CoT）”的能力。当我们提供更详细的推理步骤时，AI 的表现会更好。在 OpenAI Cookbook 中，有一章专门介绍了<a href="https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md">思维链的能力</a>，你可以去仔细研读一下。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hi，大家好，我是茶桁。&lt;/p&gt;
&lt;p&gt;很遗憾在上一讲，也就是第12讲的时候，咱们对于利用AI写一个VBA宏来执行Excel任务的过程并不顺利，仔细想来既然大家都在这里看这个系列文章了，应该也基本都会Python的，所以一个Excel自动化也并无太大影响，毕竟，这种商业软件的集成一定是早晚的事情，咱们也不必在这里死磕这一个问题。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>12 AI帮你写个小插件，轻松处理Excel文件</title>
    <link href="https://hivan.me/AI-create-a-excel-plugin/"/>
    <id>https://hivan.me/AI-create-a-excel-plugin/</id>
    <published>2023-05-26T09:15:54.000Z</published>
    <updated>2023-06-01T12:57:36.454Z</updated>
    
    <content type="html"><![CDATA[<p>开头我就要跟各位先说对不起，本来我是很想为大家把这部分实现并完成的。但是很抱歉，因为我用的Mac，而这部分代码实现起来的时候一直会如下报错：</p><span id="more"></span><img src="/AI-create-a-excel-plugin/20230601171646.png" class="" title="img"><p>当然，其实是可以迂回实现的，就是使用Mac本来的AppleScript，不过实现效果并不好，所以想想也就算了。毕竟我考虑，可能看我这些系列文章的人都多多少少会一点Python吧，利用Python去处理数据，怎么也比Excel强很多，再说，现在Excel+ChatGPT的组合说能实现的功能也都很有限。</p><p>当然，我也可以直接将徐文浩已经实现的东西贴给大家，就像这篇文章这样：<a href="https://link.zhihu.com/?target=https%3A//www.163.com/dy/article/I2K9R9QF05560XXW.html">https://www.163.com/dy/article/I2K9R9QF05560XXW.html</a></p><p>但是，毕竟人家徐文浩是付费文章，这样做先不说是否有法律风险，道德上也没办法实现。</p><p>所以，最后这篇文章就算鸽了大家吧，当然，作为补偿，我将我自己实现过的代码贴在这里给大家做个参考，请注意，这是AppleScript的代码，请谨慎使用：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- Step 1: Prompt to choose Excel fileset excelFile to choose file with prompt &quot;Select Excel File&quot;</span><br><span class="line">- Step 2: Open Excel file and get worksheettell application &quot;Microsoft Excel&quot; activate open excelFile set excelWorkbook to workbook 1 set excelSheet to sheet 1 of excelWorkbook - Step 3: Read first column and process each product titleset productTitles to value of column 1 of excelSheet set translatedResults to &#123;&#125; repeat with i from 1 to count of productTitles set productTitle to item i of productTitles set promptText to &quot;Consideration product: &quot; &amp; productTitle &amp; return &amp; ¬ &quot;1. Compose human readable product title used on Amazon in English within 20 words.&quot; &amp; return &amp; ¬ &quot;2. Write 5 selling points for the products on Amazon.&quot; &amp; return &amp; ¬ &quot;3. Evaluate a price range for this product in the U.S.&quot; - Step 4: Call OpenAI API and get translated resultset translatedResult to my callOpenAPI(promptText) copy translatedResult to the end of translatedResults end repeat - Step 5: Write translated results to second columnset column 2 of excelSheet to translatedResults - Step 6: Save and close Excel filesave excelWorkbook close excelWorkbook</span><br><span class="line">end tellon callOpenAPI(promptText) - Call OpenAI API and return translated result- Replace this code with actual API callset translatedResult to &quot;Translated Result&quot; - Replace with actual translated resultreturn translatedResult</span><br><span class="line">end callOpenAPI</span><br></pre></td></tr></table></figure><p>最后，在和大家说声抱歉。咱们下期，来尝试用AI帮忙来写测试。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;开头我就要跟各位先说对不起，本来我是很想为大家把这部分实现并完成的。但是很抱歉，因为我用的Mac，而这部分代码实现起来的时候一直会如下报错：&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>11 用好开源模型节约成本</title>
    <link href="https://hivan.me/Save-costs-with-an-open-source-model/"/>
    <id>https://hivan.me/Save-costs-with-an-open-source-model/</id>
    <published>2023-05-20T09:14:42.000Z</published>
    <updated>2023-06-01T13:02:08.461Z</updated>
    
    <content type="html"><![CDATA[<p>Hi， 大家好，我是茶桁。</p><p>直奔主题，我们来谈谈成本这件事。</p><span id="more"></span><p>大家应该都知道，ChatGPT对免费用户是有5美元的API调用额度的，说是这么说，可是那是以前，现在新注册的小伙伴应该都发现自己的API Key根本无法调用API，原因是这个免费额度似乎已经失效了。而我可以直接说，在我从第一节到第10节的课程中所用到的金额，已经超过这个数目了。也就是说，我这10节课API调用成本就已经超过了40元人民币。</p><p>看到这大家大概能理解我这个课程为什么改为付费课程了吧？</p><p>对于 <code>ChatCompletion</code> 的接口来说，为了更好地使用它，我们需要传入更多的上下文信息，以便更准确地进行文本生成。不过要注意的是，实际消耗的 Token 数量可能比我们感觉的要多。此外，除了费用之外，数据安全也是我们需要考虑的一个问题。由于每个国家的数据监管要求不同，不是所有的数据都适合通过 OpenAI 的 API 来处理。因此，我们需要寻找一个除 OpenAI 以外的解决方案。幸运的是，有一些开源的大语言模型可以帮助我们解决这个问题。通过利用这些开源的模型，中小型公司也可以轻松地获得更准确、更安全的文本生成服务。</p><h3 id="在-colab-中使用-gpu">在 Colab 中使用 GPU</h3><p>在本课中，我们需要使用一些开源模型。但是，并不是所有人的电脑都配备了强劲的 NVIDIA GPU。因此，我建议您使用 Colab 运行相应的笔记本，并注意将运行环境设置为 GPU。</p><p>如下图，选择 <code>代码执行程序-&gt;更改运行时类型</code>,然后在硬件加速器上选择 <code>GPU</code> 就可以了。</p><img src="/Save-costs-with-an-open-source-model/20230601171522.png" class="" title="img"><img src="/Save-costs-with-an-open-source-model/20230601171529.png" class="" title="img"><p>当然，有的小伙伴应该是看到有一个警告：想要使用付费GPU，购买额外的计算单元。 暂时不需要理会它，只要用的多，Colab的GPU是有免费额度的。</p><h3 id="开源伙伴huggingfaceembedding">开源伙伴：HuggingfaceEmbedding</h3><p>在第四讲中，我们使用了 Google 开源的 T5 模型来比较零样本分类效果。尽管该模型的效果不如 OpenAI 的 API，准确率只有90%，但也算是相当不错的了。这也让我们想到，上一讲中使用的 llama-index 向量搜索部分是否可以使用开源模型的 Embedding 进行替换呢？</p><p>答案是肯定的，llama-index 允许您直接定义一个定制化的 Embedding。我将相关代码放在了下面，您可以参考它进行操作。这种方法不仅可以大大增加文本长度，还可以保持原有的关键思想。</p><p>在继续之前，我们需要安装一下 <code>sentence-transformers</code> 这个库：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge sentence-transformers</span><br></pre></td></tr></table></figure><p>然后执行如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需库</span></span><br><span class="line"><span class="keyword">import</span> openai, os</span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, LangchainEmbedding, ServiceContext, StorageContext, GPTVectorStoreIndex, load_index_from_storage</span><br><span class="line"><span class="keyword">from</span> llama_index.vector_stores.faiss <span class="keyword">import</span> FaissVectorStore</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings.huggingface <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.node_parser <span class="keyword">import</span> SimpleNodeParser</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Markdown, display</span><br><span class="line"></span><br><span class="line"><span class="comment"># API Key</span></span><br><span class="line">os.environ[<span class="string">&#x27;OPENAI_API_KEY&#x27;</span>] = <span class="string">&#x27;OPENAI_API_KEY&#x27;</span></span><br><span class="line">openai.api_key = <span class="string">&quot;&quot;</span> </span><br><span class="line"></span><br><span class="line">text_splitter = CharacterTextSplitter(separator = <span class="string">&#x27;\n\n&#x27;</span>, chunk_size = <span class="number">100</span>, chunk_overlap = <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">dimension = <span class="number">768</span></span><br><span class="line">faiss_index = faiss.IndexFlatIP(dimension)</span><br><span class="line"></span><br><span class="line">parser = SimpleNodeParser(text_splitter = text_splitter)</span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&#x27;./data/faq/&#x27;</span>).load_data()</span><br><span class="line">nodes = parser.get_nodes_from_documents(documents)</span><br><span class="line"></span><br><span class="line">embed_model = LangchainEmbedding(HuggingFaceEmbeddings(</span><br><span class="line">    model_name = <span class="string">&#x27;sentence-transformers/paraphrase-multilingual-mpnet-base-v2&#x27;</span></span><br><span class="line">))</span><br><span class="line"></span><br><span class="line">vector_store = FaissVectorStore(faiss_index = faiss_index)</span><br><span class="line">service_context = ServiceContext.from_defaults(embed_model = embed_model)</span><br><span class="line">index = GPTVectorStoreIndex.from_documents(documents = documents, service_context=service_context)</span><br><span class="line">query_engine = index.as_query_engine()</span><br></pre></td></tr></table></figure><p>我们使用了一个面向电商 FAQ 的纯文本文件作为输入。里面包含了预设好的 FAQ 问答对。为了确保我们没有使用 OpenAI 的 API，我们将 openai.api_key 设置为空字符串。然后定义了一个 <code>embeded_model</code>，它封装了 HuggingFaceEmbeddings 类。HuggingFaceEmbeddings 可以下载、加载并计算输入文本的嵌入向量，因为 HuggingFace 为基于 transformers 的模型定义了一个标准，所以你可以使用一套代码来使用所有 transformers 类型的模型。 <code>sentence-transformers</code>是目前效果最好的语义搜索模型，它包括一系列预训练模型，其中 <code>paraphrase-multilingual-mpnet-base-v2</code> 模型支持多语言和将语句和段落转换为向量。由于我们的示例都是中文，所以选择了这个模型。你可以根据实际问题选择适合自己的模型。</p><p>我们使用 Faiss 库作为向量索引库，定义向量维度为 768，与 <code>paraphrase-multilingual-mpnet-base-v2</code> 模型的维度相同。</p><p>对于文档的切分，我们使用 CharacterTextSplitter，并对参数进行了调整。我们使用 "" 作为分段符， <code>chunk_size</code> 设置为 100， <code>chunk_overlap</code> 设为最大 100。</p><p>Embedding 使用了 3198 个 Token，不过这些 Token 都是通过 sentence_transformers 模型计算的，不需要花费额外成本。完成索引创建后，我们可以使用常见的电商类型 FAQ 问题测试。</p><p>接下来让我们问点问题试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openai.api_key = os.environ.get(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line">response = query_engine.query(<span class="string">&quot;请问你们海南能发货吗？&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure><p>输出的结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A: 是的，我们支持海南地区的发货。您可以在下单时选择您的收货地址，我们会根据您的地址信息提供相应的物流服务。</span><br></pre></td></tr></table></figure><p>继续第二个问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">response = query_engine.query(<span class="string">&quot;你们用那些快递公司送货？&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们与顺丰速运、圆通速递、申通快递、韵达快递、中通快递、百世快递等多家知名快递公司合作。</span><br></pre></td></tr></table></figure><p>第三个问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">response = query_engine.query(<span class="string">&quot;你们的退货政策是怎么样的？&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(response)　</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A: 自收到商品之日起<span class="number">7</span>天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。</span><br></pre></td></tr></table></figure><p>我们使用 Embedding 模式来查询。通过三个典型问题的测试，AI 的回答都正确，效果不错。</p><h3 id="使用chatglm">使用ChatGLM</h3><p>通过上面的代码，我们已经完成了生成 Embedding 和利用 Embedding 进行搜索的过程。在实际的问答过程中，我们仍然在使用 OpenAI 的 Completion API。是否有可能将其替换为其他模型呢？我们可以尝试一下来自清华大学的 ChatGLM 语言模型，看看中文的开源语言模型是否具备基本的知识理解和推理能力。为此，我们需要安装一些依赖包。由于 icetk 没有 Conda 的源，因此我们使用 pip 进行安装。当然，在 Conda 的包管理器中也同样可以找到它。</p><blockquote><p>这里大家可能会遇到一些坑，比如，protobuf这个库的版本不兼容，这里有一个矛盾点，就是icetk这个库依赖的是3.19以下的版本，而transformers却需要以上的版本。</p></blockquote><p>遇到上述问题的时候，我们可以这样操作一下：</p><p>先安装最新版本的，然后将其中的builder,py下载下来，再从新安装3.18版本，接着替换builder.py就可以了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置本地环境为UTF-8</span></span><br><span class="line"><span class="keyword">import</span> locale</span><br><span class="line">locale.setlocale(locale.LC_ALL, <span class="string">&#x27;en_US.UTF-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装最新版本</span></span><br><span class="line">!pip install protobuf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询路径</span></span><br><span class="line"><span class="keyword">import</span> google.protobuf <span class="keyword">as</span> protobuf</span><br><span class="line"><span class="built_in">print</span>(protobuf.__path__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载文件</span></span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> files</span><br><span class="line">files.download(<span class="string">&#x27;/usr/local/lib/python3.10/dist-packages/google/protobuf/internal/builder.py&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载并重新安装</span></span><br><span class="line">!pip uninstall protobuf</span><br><span class="line">!pip install protobuf=<span class="number">3.18</span><span class="number">.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上传文件</span></span><br><span class="line">uploadded = files.upload()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 替换文件</span></span><br><span class="line">shutil.move(<span class="string">&#x27;builder.py&#x27;</span>, <span class="string">&#x27;/usr/local/lib/python3.10/dist-packages/google/protobuf/internal/builder.py&#x27;</span>)</span><br></pre></td></tr></table></figure><p>然后，我们就可以继续了，以下代码我在本地（M1 Mac）跑了一遍，在120分钟之后，仍然还在继续，最后我不得不放弃转而使用Colab了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!pip install icetk</span><br><span class="line">!pip install cpm_kernels</span><br><span class="line">!pip install transformers</span><br></pre></td></tr></table></figure><p>我们可以通过 transformers 加载模型。最大的一个 <a href="https://github.com/THUDM/GLM-130B">ChatGLM</a> 模型有 1300 亿个参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;THUDM/chatglm-6b-int4&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;THUDM/chatglm-6b-int4&quot;</span>, trust_remote_code=<span class="literal">True</span>).half().cuda()</span><br><span class="line">model = model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><p>为了能够运行这个大型模型，我们选择了一个裁剪后的 60 亿个参数的版本，使用 int-4 量化的方式。我们的模型名字是 chatglm-6b-int4，代表 6B 的参数量。我们希望通过 GPU 进行模型的计算，所以在加载模型的时候调用了.cuda()。加载模型时，我们设置了 trust_remote_code = true 参数，以便确认您信任该模型的代码，它不会造成恶意的破坏。如果您想要用 CPU 运行，可以使用下面的代码。</p><blockquote><p>虽然建议使用GPU，但是如果你想用CPU的话，可以把模型加载的代码替换一下：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;THUDM/chatglm-6b-int4&quot;</span>, trust_remote_code=<span class="literal">True</span>).<span class="built_in">float</span>()</span><br></pre></td></tr></table></figure><p>特别是在Mac M1下，你除了使用CPU运行的话，似乎没有其他办法。</p><blockquote><p>写给M1用户</p></blockquote><p>ChatGLM如果想要在M1内运行的话，可能你会遇到如下报错：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Could <span class="keyword">not</span> find module <span class="string">&#x27;nvcuda.dll&#x27;</span> </span><br><span class="line">或者 </span><br><span class="line">RuntimeError: Unknown platform: darwin (MacOS)</span><br></pre></td></tr></table></figure><p>如果遇到这种问题的话，可以参看官方文档这里：</p><p>https://github.com/THUDM/ChatGLM-6B/issues/6#issuecomment-1471303336</p><p>然后我们来测试模型进行回答：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">自收到商品之日起7天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">根据以上信息，请回答下面的问题：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Q: 你们的退货政策是怎么样的？</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">response, history = model.chat(tokenizer, question, history=[])</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们的退货政策是在产品未使用、包装完好的情况下，自收到商品之日起<span class="number">7</span>天内可以退货。请注意，某些特殊商品可能不支持退货，在购买前请查看商品详情页面的退货政策，以了解具体情况。</span><br></pre></td></tr></table></figure><p>ChatGLM的回答是根据我们提供的上下文来的。如果我们去掉提供的FAQ的内容，它的回答会有所变化。</p><p>下一个问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Q: 你们的退货政策是怎么样的？</span></span><br><span class="line"><span class="string">A: </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">response, history = model.chat(tokenizer, question, history=[])</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">我们公司的政策是，如果您在购买后的<span class="number">7</span>天内发现我们的产品不符合您的期望，可以申请退货。退货条件如下：</span><br><span class="line"><span class="number">1.</span> 符合退货政策要求的产品；</span><br><span class="line"><span class="number">2.</span> 未使用或损坏的产品；</span><br><span class="line"><span class="number">3.</span> 符合退货地点的要求，即退货地址与购买地址相同；</span><br><span class="line"><span class="number">4.</span> 符合退货流程的要求，即您需要提供有效的退货单并完成退货流程。</span><br><span class="line">如果符合上述退货政策要求，我们将尽快处理您的退货申请，并为您退款。请注意，如果您已经使用了我们的产品，或者产品有损坏或变质等情况，我们将无法为您提供退货服务。</span><br></pre></td></tr></table></figure><p>在没有合适的上下文信息时，AI编造了一些本来不存在的要求，例如“退货地址与购买地址相同”。另一个例子是，当我们的上下文只提到可以将物品送到海南地区时，AI能否判断是否支持送到三亚市。</p><p>继续问问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">我们支持全国大部分省份的配送，包括北京、上海、天津、重庆、河北、山西、辽宁、吉林、黑龙江、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">根据以上信息，请回答下面的问题：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Q: 你们能配送到三亚吗？</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">response, history = model.chat(tokenizer, question, history=[])</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">是的，我们支持全国大部分省份的配送，包括三亚市。</span><br></pre></td></tr></table></figure><p>ChatGLM可以配送到三亚。如果想知道它是否可以送到哈尔滨，可以简单地在上下文中排除东三省并询问。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">我们支持全国大部分省份的配送，包括北京、上海、天津、重庆、河北、山西、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆.但是不能配送到东三省</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">根据以上信息，请回答下面的问题：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Q: 你们能配送到哈尔滨吗？</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">response, history = model.chat(tokenizer, question, history=[])</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">很抱歉，我们目前不能配送到哈尔滨。</span><br></pre></td></tr></table></figure><p>ChatGLM 可以回答我们无法发送到哈尔滨的问题，因此我们可以使用 ChatGLM 来处理 FAQ。</p><h3 id="封装llm">封装LLM</h3><p>我们使用原始的 ChatGLM 模型代码，无法直接通过查询来访问 llama-index 获取答案。要实现这一点，我们将其封装为 LLM 类，使我们的索引使用指定的大语言模型即可。您可以查看相应的 <a href="https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html">llama-index 文档。</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai, os</span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, LangchainEmbedding, ServiceContext,</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings.huggingface <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.node_parser <span class="keyword">import</span> SimpleNodeParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.llms.base <span class="keyword">import</span> LLM</span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> LLMPredictor</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>, <span class="type">List</span>, Mapping, <span class="type">Any</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomLLM</span>(<span class="title class_ inherited__">LLM</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_call</span>(<span class="params">self, prompt: <span class="built_in">str</span>, stop: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">str</span>]] = <span class="literal">None</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        response, history = model.chat(tokenizer, prompt, history=[])</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_identifying_params</span>(<span class="params">self</span>) -&gt; Mapping[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;name_of_model&quot;</span>: <span class="string">&quot;chatglm-6b-int4&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_llm_type</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;custom&quot;</span></span><br></pre></td></tr></table></figure><p>我们将 CustomLLM 对象传入 index 构造函数并重新运行问题，以观察其效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> SpacyTextSplitter</span><br><span class="line"></span><br><span class="line">llm_predictor = LLMPredictor(llm=CustomLLM())</span><br><span class="line"></span><br><span class="line">text_splitter = CharacterTextSplitter(separator=<span class="string">&quot;\n\n&quot;</span>, chunk_size=<span class="number">100</span>, chunk_overlap=<span class="number">20</span>)</span><br><span class="line">parser = SimpleNodeParser(text_splitter=text_splitter)</span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&#x27;./data/faq/&#x27;</span>).load_data()</span><br><span class="line">nodes = parser.get_nodes_from_documents(documents)</span><br><span class="line"></span><br><span class="line">embed_model = LangchainEmbedding(HuggingFaceEmbeddings(</span><br><span class="line">    model_name=<span class="string">&quot;sentence-transformers/paraphrase-multilingual-mpnet-base-v2&quot;</span></span><br><span class="line">))</span><br><span class="line">service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)</span><br><span class="line"></span><br><span class="line">dimension = <span class="number">768</span></span><br><span class="line">faiss_index = faiss.IndexFlatIP(dimension)</span><br><span class="line">vector_store = FaissVectorStore(faiss_index = faiss_index)</span><br><span class="line">service_context = ServiceContext.from_defaults(embed_model = embed_model)</span><br><span class="line">index = GPTVectorStoreIndex.from_documents(documents = documents, service_context=service_context)</span><br><span class="line">query_engine = index.as_query_engine()</span><br><span class="line">response = index.query(</span><br><span class="line">    <span class="string">&quot;请问你们海南能发货吗？&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">海南能发货。</span><br></pre></td></tr></table></figure><p>现在，我们可以直接使用 ChatGLM 的模型进行 FAQ问答。我们的解决方案是使用 paraphrase-multilingual-mpnet-base-v2 模型计算 Embedding 进行语义搜索，然后通过 chatglm-6b-int4 模型来解决问答。这两个模型可以在家用级别的显卡上运行。非常厉害！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hi， 大家好，我是茶桁。&lt;/p&gt;
&lt;p&gt;直奔主题，我们来谈谈成本这件事。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>10 利用AI索引并分析文献和图片</title>
    <link href="https://hivan.me/Use-AI-to-index-and-analyze-documents-and-images/"/>
    <id>https://hivan.me/Use-AI-to-index-and-analyze-documents-and-images/</id>
    <published>2023-05-17T09:02:24.000Z</published>
    <updated>2023-06-01T13:19:33.567Z</updated>
    
    <content type="html"><![CDATA[<p>Hi, 我是茶桁。</p><p>看到我这篇文章的读者们不知道有多少人是接触过ChatGPT或者其他人工智能产品的。</p><span id="more"></span><p>市面上目前充斥着大量的人工智能产品，从聊天，文案，脚本，音乐，绘画等方方面面都涵盖了。但是不知道有多少人遇到过以下的场景不知道该如何解决：</p><ol type="1"><li><p>我需要针对一篇很长的文章（可以是论文，可以是小说）进行总结或者分析的时候，就开始无从下手。因为ChatGPT在接收长度上是有限制的，这个长度我大概测试过，如果你用的是WebGPT，那么应该中文应该是在2500字左右，多一个字都会告诉你长度超出限制。而我们一篇论文，起码来说都是5000字以上的。分两段来喂给ChatGPT当然可以，但是上下文关联有时候会遇到问题，ChatGPT也会给你胡编乱造。</p></li><li><p>有的时候我从客户那里接收到的是一张图片，也许是截图，也许就是拍的一张照片。那么，怎样利用ChatGPT去分析这张图片上的内容，然后根据我的需求给我相应的答案呢？</p></li></ol><p>以上这两点，估计是很多人遇到想解决的。而今天这篇文章，就是从这两点入手教你如何解决。</p><h3 id="大语言模型的不足"><strong>大语言模型的不足</strong></h3><p>让我们打开ChatGPT来问一些常识性的问题，这个问题对于大部分上过学的中国人来说，都能从课本上了解到：</p><p>“鲁迅先生在日本学习医学的老师是谁？”</p><p>结果如下图，这个“嘉泽源之助”到底是谁呢？我也不知道，得到这个答案的时候，我还特意去Google了一下，根本找不到相关资料。</p><img src="/Use-AI-to-index-and-analyze-documents-and-images/20230601170302.png" class="" title="img"><p>那么为什么会出现这种情况呢？这要从大语言模型的原理及它使用训练的数据集说起。</p><p>大语言模型利用训练样本中的文本前后关系，对接下来的文本进行概率预测。若出现类似的前后文本越多，那么概率会收敛到正确答案，回答准确；反之，训练过程随机性增大，对应的答案容易似是而非。GPT-3 的模型虽然训练语料很多，但中文语料很少，只有不到 1%。因此，若问很多中文相关的知识性或常识性问题，其回答往往不准确。</p><p>解决方法有两种：一是多找一些高质量的中文语料，训练一个新模型；二是找一些数据，利用 OpenAI 提供的"微调"（Fine-tune）接口，在原模型上训练一个新模型。</p><img src="/Use-AI-to-index-and-analyze-documents-and-images/20230601170309.png" class="" title="img"><p>如上图显示，ChatGPT只能提供2021年9月之前的新闻。要解决这个问题，需要更多的文本数据，但对于时效性要求较高的资讯类信息，这种方法并不实用。例如，要让 AI 告诉我们前一天足球赛的比分，需要不断地训练和微调模型，成本太高。不过，ChatGPT的插件已经可以获取网上信息了。</p><p>对比之下，我们可以看看Bing是怎么做的。</p><h3 id="bing的方式搜索然后提示"><strong>Bing的方式：搜索，然后提示</strong></h3><p>对于关注这个领域的朋友，可能会问微软是如何在Bing中加入ChatGPT问答功能的，而且效果似乎也不错。那么Bing是如何实现这一点的呢？虽然我不知道Bing是如何实现的，但如果我来做的话，我会使用搜索后提示的方法。我们可以通过搜索找到与问题相关的语料库，然后将与问题语义最接近的前几个内容作为提示，供AI参考并回答问题。</p><p>其实，早期的ChatGPT可以利用Chrome的一款插件（<a href="https://chrome.google.com/webstore/detail/webchatgpt-chatgpt-with-i/lpfemeioodjbpieminkklglpmhlngfcn?utm_source=chrome-ntp-icon">WebChatGPT</a>）做到这件事，可是也必须是能在网上搜索到的，据我所知，其调用的搜索引擎应该是Google，当我输入Prompt的时候，会先调用这个插件，然后这个插件会去搜索相关信息返回到ChatGPT再从新提问，比如我问 <code>鲁迅先生的医学老师是谁</code> ：</p><img src="/Use-AI-to-index-and-analyze-documents-and-images/20230601170319.png" class="" title="img"><p>看到这里，有些读者应该也明白怎样做了，就是增加语料。我们将问题语义最接近的前几条内容作为提示语的一部分提供给AI，以便AI参考这些内容并回答问题。如下图，我先关闭了插件的Web access，纯喂语料来再试试：</p><img src="/Use-AI-to-index-and-analyze-documents-and-images/20230601170328.png" class="" title="img"><p>这也是利用大语言模型的一个常见模式。因为大语言模型其实内含了两种能力。</p><p>第一种能力是海量的语料中已经包含的知识信息。比如，我们前面问 AI 鱼香肉丝的做法，它能回答上来就是因为语料库中已经有了充足的相关知识。这些知识我们通常称之为"世界知识"。这些知识可以包括食材的使用、调料的配比和烹饪方法等等。</p><p>第二种能力是根据你输入的内容，理解和推理的能力。如果大语言模型在语料库中并没有这个问题的答案，它也能进行阅读理解。这个过程中，"知识"不是模型本身提供的，而是我们找出来，临时提供给模型的。如果不提供这个上下文，再问一次模型相同的问题，它还是答不上来的。因此，在某些情况下，我们需要提供更多的上下文，以确保模型可以正确地理解问题，给出恰当的答案。</p><h3 id="正餐llama_index建立第二个大脑"><strong>正餐：llama_index，建立“第二个大脑”</strong></h3><p>我认为，这种先搜索、后提示的方式称为 AI 的"第二大脑"模式，它是一种非常实用的方法。实现这种方法需要先将希望 AI 能够回答的知识建立为一个外部索引，这个索引就像是 AI 的"第二个大脑"。每次向 AI 提问时，它都会在这个第二大脑中查询相关资料，再运用自己的思维能力来回答问题。</p><p>目前，许多应用程序都是通过这种模式来实现的，如读论文、读书回答问题等。因此，我们可以自己来实现这个"第二大脑"模式。</p><p>但是，我们不必从零开始编写代码，因为这种模式非常常见，所以有人为它编写了一个开源 Python 包，名为 llama-index。我们可以使用这个软件包的几行代码来尝试它是否能够回答与鲁迅先生写的《藤野先生》相关的问题。</p><p>由于 llama-index 尚未制作好 Conda 下的包，因此即使在 Conda 下，我们仍需通过 pip 来安装。 同时，我们可以根据需要，根据 llama-index 的文档进行调整和优化，以满足我们特定的需求。</p><blockquote><p>llama-index 的Github， llama-index官方文档</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install llama-index</span><br><span class="line">pip install langchain</span><br></pre></td></tr></table></figure><p>我把《藤野先生》这篇文章转换成了txt文件，并放到data/mr_fujino目录下。我们的代码也很简洁。</p><img src="/Use-AI-to-index-and-analyze-documents-and-images/20230601170336.png" class="" title="img"><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import openai</span><br><span class="line">import os</span><br><span class="line">from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage</span><br><span class="line"></span><br><span class="line"># Set your OpenAI API key directly</span><br><span class="line">os.environ[&#x27;OPENAI_API_KEY&#x27;] = &quot;OPENAI_API_KEY&quot;</span><br><span class="line">openai.api_key = &quot;OPENAI_API_KEY&quot;</span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(&#x27;./data/mr_fujino&#x27;).load_data()</span><br><span class="line">index = GPTVectorStoreIndex.from_documents(documents)</span><br><span class="line">index.storage_context.persist(&#x27;index_mr_fujino&#x27;)</span><br></pre></td></tr></table></figure><blockquote><p>注意这里，必须要import os后设置os.environ[‘OPENAI_API_KEY’], 如果不这么做，而只是像遗忘一样设置openai.api_key = “”, 那么执行会报错，告诉你环境中没有OPENAI_API_KEY。</p></blockquote><blockquote><p>在最新的llama-index中，获取本地数据和写入本地数据都有了较大的改动，目前为止我的代码是可用的，但是一旦报错，我建议您查阅官方文档『llama-index官方文档』</p></blockquote><p>分析下代码：</p><ol type="1"><li><p>首先，我们通过SimpleDirectoryReader方法将本地目录<code>mr_fujino</code>加载了进来，注意这个方法其实是一个循环方法，会讲目录内的每一个文件都当作是一篇文档。</p></li><li><p>然后我们将读取到的所有文档交给了GPTVectorStoreIndex方法来构建索引。将方法名称拆开来看就是<code>GPT Vector Store Index</code>， 可以看出，这是一个将文档转化成向量，然后再变成一个索引。</p></li><li><p>最后，我们将索引存储到本地，<code>storage_context.persist</code>这个方法就是<code>GPTVectorStoreIndex</code>方法在最近版本中更改的。之前的版本是<code>save_to_disk</code>, 并会存储成一个json文件。而现在会存储成一个目录，并在其中存储多个json文件： <img src="/Use-AI-to-index-and-analyze-documents-and-images/20230601170345.png" class="2. 1." title="img"></p></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">storage_context = StorageContext.from_defaults(persist_dir = &#x27;./index_mr_fujino/&#x27;)</span><br><span class="line"></span><br><span class="line">index = load_index_from_storage(storage_context)</span><br><span class="line">query_engine = index.as_query_engine()</span><br><span class="line">response = query_engine.query(&#x27;鲁迅先生在日本学习医学的老师是谁？&#x27;)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure><p>这里我们读取到本地目录内的所有文件。然后将索引加载到内存中。再对Index索引调用Query函数，就可以得到问题的答案了。</p><p>可以看到，通过外部的索引，我们可以正确的获得问题的答案</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 输出答案</span><br><span class="line">鲁迅先生在日本学习医学的老师是藤野严九郎。</span><br></pre></td></tr></table></figure><p>似乎问题很容易结局，四行代码解决问题，再加上一行显示结果。</p><p>让我们再来测试一下其他的问题看看，这次我们问问鲁迅先生去哪里学的医学：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">response = query_engine.query(&quot;鲁迅先生去哪里学的医学？&quot;)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure><p>回答正确。</p><p>接着我们来思考一个问题，我们搜索到的内容，在整个过程里面是如何交给OpenAI的呢？让我们来看看下面这段代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from llama_index import QuestionAnswerPrompt</span><br><span class="line">query_str = &quot;鲁迅先生去哪里学的医学？&quot;</span><br><span class="line">DEFAULT_TEXT_QA_PROMPT_TMPL = (</span><br><span class="line">    &quot;Context information is below. \n&quot;</span><br><span class="line">    &quot;---------------------\n&quot;</span><br><span class="line">    &quot;&#123;context_str&#125;&quot;</span><br><span class="line">    &quot;\n---------------------\n&quot;</span><br><span class="line">    &quot;Given the context information and not prior knowledge, &quot;</span><br><span class="line">    &quot;answer the question: &#123;query_str&#125;\n&quot;</span><br><span class="line">)</span><br><span class="line">QA_PROMPT = QuestionAnswerPrompt(DEFAULT_TEXT_QA_PROMPT_TMPL)</span><br><span class="line"></span><br><span class="line">query_engine = index.as_query_engine(text_qa_template = QA_PROMPT)</span><br><span class="line">response = query_engine.query(query_str)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure><p>这段代码定义了一个名为<code>QA_PROMPT</code>的对象，并为其设计了一个模板，以便于后续的应用。</p><p>在模板中，我们提供了上下文信息（<code>Context information</code>），并支持两个变量：<code>context_str</code>和<code>query_str</code>。在实际被调用时，<code>context_str</code>会被Embedding相似度找出来的内容替换，而<code>query_str</code>则会被实际问题替换。</p><p>在实际提问时，我们要求AI只考虑上下文信息，而不是基于其自身的先验知识（<code>prior knowledge</code>）来回答问题。为了更好地利用AI的知识，我们需要提供更多的上下文信息，以便于AI更好地理解问题，给出更加准确的答案。另外，在模板的设计过程中，我们也需要考虑到更多的场景，以便于应对不同的问题，提高AI的应用范围和效果。</p><p>我们把搜索结果和问题组合成提示语，让 AI 回答问题。再问一次 AI 确认答案是否正确。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 输出结果</span><br><span class="line">鲁迅先生去仙台学的医学。</span><br></pre></td></tr></table></figure><p>可以看到AI又正确的回答出了我们提出的问题。</p><p>让我们继续调戏一下，问问不相干的问题会得到什么答案呢？比如，我们问问红楼梦里林黛玉和贾宝玉的关系。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">QA_PROMPT_TMPL = (</span><br><span class="line">    &quot;下面的“我”指的是鲁迅先生 \n&quot;</span><br><span class="line">    &quot;---------------------\n&quot;</span><br><span class="line">    &quot;&#123;context_str&#125;&quot;</span><br><span class="line">    &quot;\n---------------------\n&quot;</span><br><span class="line">    &quot;根据这些信息，请回答问题: &#123;query_str&#125;\n&quot;</span><br><span class="line">    &quot;如果您不知道的话，请回答不知道\n&quot;</span><br><span class="line">)</span><br><span class="line">QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)</span><br><span class="line"></span><br><span class="line">query_engine = index.as_query_engine(text_qa_template = QA_PROMPT)</span><br><span class="line"></span><br><span class="line">response = query_engine.query(&#x27;请问林黛玉和贾宝玉是什么关系？&#x27;)</span><br><span class="line">print(response)</span><br><span class="line"># 输出结果</span><br><span class="line">不知道</span><br></pre></td></tr></table></figure><p>这样的回答基本上是基于我们给到的Prompt有提到<code>如果不知道的话，就回答不知道</code>。如果给这段prompt，相信AI一定会信口胡诌一气。而我们也确确实实的发现，AI的回答是基于我们所限定的语料而没有根据<code>prior knowledge</code>来回答。</p><h3 id="用-llama_index-总结文章"><strong>用 llama_index 总结文章</strong></h3><p>还有一个常见的使用 llama-index 这样的Python 库的应用场景，就是生成文章的摘要。在前面教你如何进行文本聚类的时候，我们已经看到了可以通过合适的提示语（Prompt）做到这一点。不过，如果要总结一篇论文、甚至是一本书，每次最多只能支持 4096 个 Token 的 API 就不太够用了。</p><p>要解决这个问题也并不困难，我们只要进行分段小结，再对总结出来的内容再做一次小结就可以了。我们可以把一篇文章，乃至一本书，构建成一个树状的索引。每一个树里面的节点，就是它的子树下内容的摘要。最后，在整棵树的根节点，得到的就是整篇文章或者整本书的总结了。</p><p>当然，这个方法不仅仅适用于论文和书籍，也可以用于其他需要总结的长文本，比如新闻报道、电影剧情等。此外，我们还可以通过扩展 llama-index 的功能，使其支持更多 Token 的 API，从而处理更长的文章。最后，值得一提的是，利用 llama-index 生成文章摘要的应用场景还有很多，比如在搜索引擎、聊天机器人等领域都有着广泛的应用。</p><img src="/Use-AI-to-index-and-analyze-documents-and-images/20230601170352.png" class="" title="img"><p>原理清楚了，就是将文本分片建立树状结构的索引来完成全文的总结。</p><p>那么事实上，llama-index 内置了这样的功能，接下来，我们就看看要实现这个功能，我们该如何写代码呢？</p><p>在开始写代码之前，我们需要安装一下spacy这个Python库，并且下载一下对应的中文分词分句需要的模型：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install spacy</span><br><span class="line">python3 -m spacy download zh_core_web_sm</span><br></pre></td></tr></table></figure><p>安装完成完毕之后，那么接下来的事情就比较简单了。</p><p>我们使用<code>GPTListIndex</code>这个llama-index内最简单的索引结构，并做了两点优化。</p><ol type="1"><li><p>首先，在索引中，我们指定了一个 LLMPredictor，使得我们在向 OpenAI 发起请求时都使用 ChatGPT 模型，因为这个模型比较快，也比较便宜。而 llama-index 默认使用的模型 text-davinci-003 的价格比 gpt-3.5-turbo 贵上十倍。</p></li><li><p>其次，我们定义了使用 SpacyTextSplitter 来进行中文文本的分割，因为 llama-index 默认的设置对于中文文本的支持和效果都不太好。我们选用的文章是中文的，里面的标点符号也都是中文的，所以我们使用了中文的语言模型。我们也限制了分割出来的文本段，最长不要超过 2048 个 Token，这些参数都可以根据你实际用来处理的文章内容和属性自己设置。</p></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from langchain.chat_models import ChatOpenAI</span><br><span class="line">from langchain.text_splitter import SpacyTextSplitter</span><br><span class="line">from llama_index import GPTListIndex, LLMPredictor, ServiceContext</span><br><span class="line">from llama_index.node_parser import SimpleNodeParser</span><br><span class="line"></span><br><span class="line"># difine LLM</span><br><span class="line">llm_predictor = LLMPredictor(llm = ChatOpenAI(</span><br><span class="line">    temperature = 0,</span><br><span class="line">    model_name = &#x27;gpt-3.5-turbo&#x27;,</span><br><span class="line">    max_tokens = 1024</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">text_splitter = SpacyTextSplitter(pipeline = &#x27;zh_core_web_sm&#x27;, chunk_size = 2048)</span><br><span class="line">parser = SimpleNodeParser(text_splitter = text_splitter)</span><br><span class="line">documents = SimpleDirectoryReader(&#x27;./data/mr_fujino/&#x27;).load_data()</span><br><span class="line">nodes = parser.get_nodes_from_documents(documents)</span><br><span class="line"></span><br><span class="line">service_context = ServiceContext.from_defaults(llm_predictor = llm_predictor)</span><br><span class="line"></span><br><span class="line">list_index = GPTListIndex(nodes = nodes, service_context = service_context)</span><br></pre></td></tr></table></figure><p><code>GPTListIndex</code> 构建索引时不会创建嵌入，因此索引构建快速且不消耗令牌数。它只是根据您设置的索引结构和分割方式建立了一个列表索引。然后，我们可以让 AI 小结这篇文章。由于我们想要按照树状结构进行文章的小结，因此我们设定了一个参数 <code>response_mode = "tree_summarize"</code>。这个参数会按照树状结构将整篇文章总结出来。实际上，它会将每一段文本分片并通过查询内的提示语进行小结，然后再对多个小结里的内容通过查询里的提示语继续小结。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">list_response = list_index.as_query_engine(response_mode=&quot;tree_summarize&quot;)</span><br><span class="line">response = list_response.query(&quot;下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:&quot;)</span><br><span class="line"># response = list_index.query()</span><br><span class="line">print(response)</span><br><span class="line"># 输出结果</span><br><span class="line">鲁迅先生在日本学习医学时遇到了藤野严九郎教授，他很有学问，对学生也很关心，甚至帮助鲁迅修改讲义。但鲁迅当时不够用功，有时也很任性。鲁迅遇到了一些困难，但藤野先生一直鼓励他。最终，鲁迅决定离开医学，去学习生物学。在离开前，藤野先生给了他一张照片，并希望他能保持联系。鲁迅很久没有和一个人联系了，虽然想写信但难以下笔。他想起了藤野先生，这个人给他很多鼓励和教诲，希望中国有新的医学和学术。鲁迅收藏了他所改正的讲义，但七年前搬家时丢失了一半。他的照片还挂在鲁迅的房间里，每当鲁迅疲倦时看到他的照片就会增加勇气。</span><br></pre></td></tr></table></figure><p>完成，基本上，我们用了几行代码完成了整个文章的总结，返回的结果从整体上来看还是不错的。</p><h3 id="多模态的引入让llama-index识别图片"><strong>多模态的引入让llama-index识别图片</strong></h3><p>llama_index 不仅能索引文本，还能够索引图片和插图等信息，这就是所谓的多模态能力。通过一些多模态模型，将文本和图片联系起来，实现了这种能力。在课程的第三部分，我们将专门探讨这些图像的多模态模型的构建方法。</p><p>接下来，我们来看一个 llama_index <a href="https://github.com/jerryjliu/llama_index/blob/main/examples/multimodal/Multimodal.ipynb">官方样例库提供的例子</a>。这个例子是将吃饭的小票拍下来，然后记录下吃的东西、花费的金额以及吃饭的日期等信息。这种记录方式既方便又直观，也能够帮助我们更好地管理财务和饮食。不仅如此，我们还可以利用 llama_index 来查询自己的饮食习惯和花费情况，进一步了解自己的生活方式。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex</span><br><span class="line">from llama_index.readers.file.base import DEFAULT_FILE_EXTRACTOR, ImageParser</span><br><span class="line">from llama_index.response.notebook_utils import display_response, display_image</span><br><span class="line">from llama_index.indices.query.query_transform.base import ImageOutputQueryTransform</span><br><span class="line"></span><br><span class="line">image_parser = ImageParser(keep_image = True, parse_text = True)</span><br><span class="line">file_extractor = DEFAULT_FILE_EXTRACTOR</span><br><span class="line">file_extractor.update(&#123;</span><br><span class="line">    &#x27;.jpg&#x27;: image_parser,</span><br><span class="line">    &#x27;.png&#x27;: image_parser,</span><br><span class="line">    &#x27;.jpeg&#x27;: image_parser,</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"># NOTE: we add filename as metadata for all documents</span><br><span class="line">filename_fn = lambda filename: &#123;&#x27;file_name&#x27;: filename&#125;</span><br><span class="line"></span><br><span class="line">receipt_reader = SimpleDirectoryReader(</span><br><span class="line">    input_dir = &#x27;./data/receipts/&#x27;,</span><br><span class="line">    file_extractor = file_extractor,</span><br><span class="line">    file_metadata = filename_fn,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">receipt_documents = receipt_reader.load_data()</span><br></pre></td></tr></table></figure><p>为了更好地支持图片索引，我们引入了 <code>ImageParser</code> 这个类，它是基于 OCR 扫描的模型 Donut 构建而成。它由一个视觉编码器和一个文本解码器组成，可以将任何一张图片转换成一段文本，然后我们再通过 OpenAI 的嵌入技术将这段文本转换成一个向量。</p><p>现在，我们可以使用简单的 <code>SimpleDirectoryReader</code>，通过指定 <code>FileExtractor</code> 来解析对应的图片，并将其转换为向量，以进行检索。</p><p>此外，在提问时，我们还使用了 <code>ImageOutputQueryTransform</code>，可以在输出结果时为图片添加标签，以便在Jupyter中更好地显示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">receipts_index = GPTVectorStoreIndex.from_documents(receipt_documents)</span><br><span class="line">response_query_engine = receipts_index.as_query_engine(query_transform = ImageOutputQueryTransform(width = 400))</span><br><span class="line">receipts_response = response_query_engine.query(</span><br><span class="line">    &quot;When was the last time I went to McDonald\&#x27;s and how much did I spend. Also show me the receipt from my visit.&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">display_response(receipts_response)</span><br></pre></td></tr></table></figure><p>得到的结果：</p><blockquote><p>Final Response: The last time you went to McDonald's was on 03/10/2018 at 07:39:12 PM and you spent $26.15. Here is the receipt</p></blockquote><p>在上述内容中，我们可以看到 OpenAI 的强大之处在于其对于任意文本的处理能力。通过对文本的处理，OpenAI 能够不仅展示对应的图片，还能够给出正确的答案。</p><p>为了更好地理解图片的内容，我们可以对其进行单独解析，以获取其中所包含的文本信息。通过这种方式，我们能够更全面地了解图片所代表的意义和含义。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">output_image = image_parser.parse_file(&#x27;./data/receipts/1100-receipt.jpg&#x27;)</span><br><span class="line">print(output_image.text)</span><br></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;s_menu&gt;&lt;s_nm&gt; Story&lt;/s_nm&gt;&lt;s_num&gt; 16725 Stony Platin Rd&lt;/s_nm&gt;&lt;s_num&gt; Store#:&lt;/s_nm&gt;&lt;s_num&gt; 3659&lt;/s_num&gt;&lt;s_price&gt; 700-418-8362&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Welcome to all day breakfast dormist O Md Donald&#x27;s&lt;/s_nm&gt;&lt;s_num&gt; 192&lt;/s_num&gt;&lt;s_price&gt; 192&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; QTY ITEM&lt;/s_nm&gt;&lt;s_num&gt; OTAL&lt;/s_num&gt;&lt;s_unitprice&gt; 03/10/2018&lt;/s_unitprice&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 07:39:12 PM&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Delivery&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 0.00&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; 10 McNuggets EVM&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 10.29&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Barbeque Sauce&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 1&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Barbeque Sauce&lt;/s_nm&gt;&lt;s_num&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 0.40&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; L Coke&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 0.40&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; M French Fries&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 3.99&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; HM GrChS S-Fry Yog&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;sep/&gt;&lt;s_nm&gt; Smoonya&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;sep/&gt;&lt;s_nm&gt; HM Apple Juice&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 2.89&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Cookies&lt;/s_nm&gt;&lt;s_cnt&gt; 6&lt;/s_cnt&gt;&lt;s_price&gt; 2.89&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Choc Chip Cookie&lt;/s_nm&gt;&lt;s_cnt&gt; 6&lt;/s_cnt&gt;&lt;s_price&gt; 1.19&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Baked Apple Pie&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 3.29&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; French Fries&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 2.99&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Iced Tea&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 2.99&lt;/s_price&gt;&lt;/s_menu&gt;&lt;s_sub_total&gt;&lt;s_subtotal_price&gt; 25.04&lt;/s_subtotal_price&gt;&lt;s_tax_price&gt; 1.11&lt;/s_tax_price&gt;&lt;/s_sub_total&gt;&lt;s_total&gt;&lt;s_total_price&gt; 26.15&lt;/s_total_price&gt;&lt;s_changeprice&gt; 0.00&lt;/s_changeprice&gt;&lt;s_creditcardprice&gt; 26.15&lt;/s_creditcardprice&gt;&lt;/s_total&gt;</span><br></pre></td></tr></table></figure><p>可以看到，这里我们使用 OCR 技术将打印好的小票转换成了可读的文本。文本结果中包含了我们去的店铺的名字和时间，以及消费的金额等信息。除此之外，我们还可以通过这样的方式将其他打印材料转换为可编辑的文本，例如收据、发票等等。</p><p>围绕 OpenAI 和整个大语言模型的生态，目前正在快速发展中。因此，llama-index 这个库也在不断地迭代和更新。在我使用它的过程中，我发现了各种各样的小 Bug，并且对于中文的支持也存在一些小缺陷。不过，作为开源项目，它已经拥有了一个非常不错的生态系统，特别是提供了大量的 DataConnector。这些 DataConnector 不仅支持 PDF、ePub 等电子书格式，还支持 YouTube、Notion、MongoDB 等外部的数据源和 API 接入的数据，以及本地数据库的数据。你可以在 <a href="http://llamahub.ai/">llamahub.ai</a> 上找到社区开发者开发出的读取各种不同数据源格式的 DataConnector。（大家有看到B站吗？不知道是否对视频进行解析的，回头来试试。）</p><img src="/Use-AI-to-index-and-analyze-documents-and-images/20230601170400.png" class="" title="img"><h3 id="小结"><strong>小结</strong></h3><p>在这一讲中，我们介绍了 llama-index 这个 Python 包，它能够帮助你将外部的资料库变成索引，并且通过提供的查询接口快速向文档提问。除此之外，你也可以通过将文本分片并通过树状的方式管理索引来进行小结。虽然 llama-index 还在不断发展中，但它已经非常值得拿来使用，可以加速你开发大语言模型类的相关应用。</p><p>除了上述功能，llama-index 还有很多其他功能。你可以在官网上查看<a href="https://gpt-index.readthedocs.io/en/latest/">相关文档</a>，也可以直接去<a href="https://github.com/jerryjliu/llama_index">源代码</a>里查看具体实现。值得一提的是，llama-index 其实给出了一种使用大语言模型的设计模式，我称之为"第二大脑"模式。通过先将外部的资料库索引，然后每次提问的时候，先从资料库里通过搜索找到有相关性的材料，然后再通过 AI 的语义理解能力让 AI 基于搜索到的结果来回答问题。这一模式在未来可能会成为一种趋势，具有广阔的应用前景。</p><img src="/Use-AI-to-index-and-analyze-documents-and-images/20230601170404.png" class="" title="img"><p>其中，前两步的索引和搜索，我们可以使用 OpenAI 的 Embedding 接口，也可以使用其它的大语言模型的 Embedding，或者传统的文本搜索技术。这些技术都有各自的优缺点，需要根据具体情况进行选择。比如，OpenAI 的 Embedding 接口可以更好地处理语义信息，但需要更大的计算资源和更长的训练时间；传统的文本搜索技术则速度更快，但精度不如大语言模型。另外，对于多模态功能的实现，我们还可以通过其他的模型来把图片变成文本进行索引，从而更好地利用多种信息资源。</p><p>在最后一步的问答中，OpenAI 的接口往往是必须的。这是因为问答需要更深入地理解文本语义，而传统的模型往往难以做到这一点。不过，我们也可以使用一些基于 OpenAI 的模型来自己训练问答模型，从而更好地适应自己的数据集和需求。</p><p>通过今天的这几个例子，希望你也能开始建立起自己的“第二大脑”资料库，能够将自己的数据集交给 AI 进行索引，获得一个专属于你自己的 AI。当然，这只是一个开始，未来还有更多的技术和应用等待我们去探索。</p><h3 id="课后练习"><strong>课后练习</strong></h3><p>在 llama-index 的生态系统中，不仅支持各种各样的 DataConnector 去加载数据，后端还支持各种形式的索引。除了在语义搜索中介绍过的 Faiss、Pinecone、Weaviate，该库还支持哪些形式的索引呢？这些索引的使用场景和优势是什么呢？你能详细解释一下吗？</p><p>除了进行问答和文章概括之外，llama-index 还有哪些功能可以帮助我们呢？你能提出一些例子吗？这个库的使用场景有哪些呢？</p><p>现在有很多应用，在用户上传文档后，会给出一系列提示，告诉用户可以向对应的书籍或论文提出什么问题。例如 <a href="https://scispace.com/">SCISPACE</a>，你能解释一下这些提示是如何生成的吗？这个过程中使用了哪些技术？</p><p>期待在评论区看到你的分享！同时，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p><h3 id="推荐阅读"><strong>推荐阅读</strong></h3><p>llama-index 的功能非常强大。如果你想深入了解该库，可以查看官方文档和示例部分，以了解它可以用来做什么。</p><ol type="1"><li>官方文档：<a href="https://gpt-index.readthedocs.io/en/latest/">https://gpt-index.readthedocs.io/en/latest/</a></li><li>源码以及示例：<a href="https://github.com/jerryjliu/llama_index">https://github.com/jerryjliu/llama_index</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hi, 我是茶桁。&lt;/p&gt;
&lt;p&gt;看到我这篇文章的读者们不知道有多少人是接触过ChatGPT或者其他人工智能产品的。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>09 使用Embedding实现语义检索</title>
    <link href="https://hivan.me/Implementing-semantic-retrieval-using-Embedding/"/>
    <id>https://hivan.me/Implementing-semantic-retrieval-using-Embedding/</id>
    <published>2023-05-16T09:01:08.000Z</published>
    <updated>2023-06-01T13:21:04.307Z</updated>
    
    <content type="html"><![CDATA[<p>Hi，我是茶桁。</p><p>过去的8讲，你已熟悉Embedding和Completion接口。Embedding适合用于机器学习中的分类、聚类等传统场景。Completion接口可以用作聊天机器人，也可以用于文案撰写、文本摘要、机器翻译等工作。</p><span id="more"></span><p>然而，很多同学可能认为这与他们的日常工作无关。实际上，我们通常在搜索、广告、推荐等业务中使用自然语言处理技术。因此，我们今天来看看如何使用OpenAI的接口来帮助这些需求。</p><p>当涉及到优化搜索结果时，OpenAI的Embedding接口可以提供有价值的功能。Embedding接口能够将文本转换为表示其语义特征的向量，这些向量可以用于比较文本之间的相似性，从而优化搜索结果的排序和相关性。</p><p>首先，使用OpenAI的嵌入接口，您可以将搜索查询和搜索结果中的文本转换为嵌入向量。通过比较查询向量与结果向量之间的相似度，您可以重新排列搜索结果，以提供更相关和有用的结果。这可以帮助用户更快地找到他们想要的信息，并提供更好的搜索体验。</p><p>其次，OpenAI的嵌入接口还可以帮助您改进搜索结果的相关性。通过将用户的上下文和历史记录与搜索查询结合起来，您可以生成更具个性化和定制化的搜索结果。使用嵌入接口，您可以将用户的上下文信息转换为嵌入向量，并与查询向量进行比较，以确定最相关的结果，并在搜索结果中突出显示这些个性化的内容。</p><p>此外，OpenAI的嵌入接口还可以用于相似性搜索和聚类分析。您可以使用嵌入向量来比较不同文本之间的相似性，并将相似的文本聚集在一起。这有助于在搜索结果中提供更多相关的选项，并帮助用户发现相关但可能未被明确搜索的内容。</p><p>下面，就让我们来一步步的实现：</p><h2 id="生成实验数据">生成实验数据</h2><p>在演示代码之前，我们需要实验数据。我们通常要在网上找数据集或使用软件包的数据集。但并非总是能找到合适的数据集。这时，我们可以借助 AI 生成数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai, os</span><br><span class="line"></span><br><span class="line">openai.api_key = <span class="string">&quot;YOUR_API_KEY&quot;</span></span><br><span class="line">COMPLETION_MODEL = <span class="string">&#x27;text-davinci-003&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_data_by_prompt</span>(<span class="params">prompt</span>):</span><br><span class="line">    response = openai.Completion.create(</span><br><span class="line">        engine = COMPLETION_MODEL,</span><br><span class="line">        prompt = prompt,</span><br><span class="line">        temperature = <span class="number">0.5</span>,</span><br><span class="line">        max_tokens = <span class="number">2048</span>, <span class="comment"># 根据需要设置生成的标题长度</span></span><br><span class="line">        top_p = <span class="number">1</span>,</span><br><span class="line">        stop=<span class="literal">None</span>,  <span class="comment"># 可选，用于指定生成标题的终止标记</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> response.choices[<span class="number">0</span>].text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义提示词</span></span><br><span class="line">prompt = <span class="string">&quot;请你生成50条亚马逊里的商品的标题，每条在30个字左右，品类是心理类的书籍，标题里往往也会有一些促销类的信息，每行一条。&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行定义类, data接收原始数据</span></span><br><span class="line">data = generate_data_by_prompt(prompt)</span><br></pre></td></tr></table></figure><p>为了更贴近实际情况，我们可以精心设计提示语。例如，明确商品为亚马逊产品，品类为心理书籍，并在标题中包含促销信息。将返回结果按行分割并加载到DataFrame中，以查看结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">product_names = data.strip().split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;product_name&#x27;</span>: product_names&#125;)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p>数据结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">product_name</span><br><span class="line"><span class="number">0</span><span class="number">1.</span> 【特价】心理学家的智慧：拥抱接受自己</span><br><span class="line"><span class="number">1</span><span class="number">2.</span> 【免费配送】心理学的本质：探索你的内在</span><br><span class="line"><span class="number">2</span><span class="number">3.</span> 【限时特价】心理学的洞察力：提升你的自信</span><br><span class="line"><span class="number">3</span><span class="number">4.</span> 【<span class="number">2</span>件<span class="number">8</span>折】心理学的力量：走出你的舒适区</span><br><span class="line"><span class="number">4</span><span class="number">5.</span> 【全场满减】心理学的技巧：让你的思维更灵活</span><br></pre></td></tr></table></figure><p>AI 为我们生成了 50 条商品信息，每一条都带有促销相关的标签。我们需要去掉每一行的标号，以得到干净的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.product_name = df.product_name.apply(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">1</span>].strip())</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> product_name</span><br><span class="line"><span class="number">0</span>【特价】心理学家的智慧：拥抱接受自己</span><br><span class="line"><span class="number">1</span>【免费配送】心理学的本质：探索你的内在</span><br><span class="line"><span class="number">2</span>【限时特价】心理学的洞察力：提升你的自信</span><br><span class="line"><span class="number">3</span>【<span class="number">2</span>件<span class="number">8</span>折】心理学的力量：走出你的舒适区</span><br><span class="line"><span class="number">4</span>【全场满减】心理学的技巧：让你的思维更灵活</span><br></pre></td></tr></table></figure><p>我们可以生成一些3C电子商品，涵盖不同的品类，以便在后面展示搜索效果时更加方便。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">electronic_prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">请你生成50条亚马逊里的商品的标题，每条在30个字左右，品类是3C类电子商品，标题里往往也会有一些促销类的信息，每行一条。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">electronic_data = generate_data_by_prompt(electronic_prompt)</span><br><span class="line">electronic_product_names = electronic_data.strip().split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">electronic_df = pd.DataFrame(&#123;<span class="string">&#x27;product_name&#x27;</span>: electronic_product_names&#125;)</span><br><span class="line">electronic_df.product_name = electronic_df.product_name.apply(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">1</span>].strip())</span><br><span class="line">electronic_df.head()</span><br></pre></td></tr></table></figure><blockquote><p>这一步我们将获取数据和处理写在一起执行</p></blockquote><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">product_name</span><br><span class="line"><span class="number">0</span>Apple MacBook Pro <span class="number">16</span><span class="string">&quot; 苹果笔记本电脑，限时优惠！</span></span><br><span class="line"><span class="string">1新款 iPad Pro 11&quot;</span> 平板电脑，4G/WiFi，超薄设计！</span><br><span class="line"><span class="number">2</span>无线鼠标，蓝牙连接，超长续航，低至半价！</span><br><span class="line"><span class="number">3</span>三星 Galaxy S20 5G 智能手机，性能升级，极速体验！</span><br><span class="line"><span class="number">4</span>小米 Redmi Note <span class="number">8</span> Pro 智能手机，<span class="number">4800</span>万超清拍照！</span><br></pre></td></tr></table></figure><p>拼接这两个 DataFrame 后，即可用于搜索实验的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = pd.concat([df, electronic_df], axis = <span class="number">0</span>)</span><br><span class="line">df = df.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">display(df)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">product_name</span><br><span class="line"><span class="number">0</span>【特价】心理学家的智慧：拥抱接受自己</span><br><span class="line"><span class="number">1</span>【免费配送】心理学的本质：探索你的内在</span><br><span class="line"><span class="number">2</span>【限时特价】心理学的洞察力：提升你的自信</span><br><span class="line"><span class="number">3</span>【<span class="number">2</span>件<span class="number">8</span>折】心理学的力量：走出你的舒适区</span><br><span class="line"><span class="number">4</span>【全场满减】心理学的技巧：让你的思维更灵活</span><br><span class="line">......</span><br><span class="line"><span class="number">73</span>小米米家电动滑板车，超长续航，超薄设计！</span><br><span class="line"><span class="number">74</span>小米米家电动折叠车，智能控制，超薄设计，低至半价！</span><br><span class="line"><span class="number">75</span>小米米家空气净化器Pro，智能检测，超长续航！</span><br><span class="line"><span class="number">76</span>小米米家净水器，三级净水，智能检测，限时优惠！</span><br><span class="line"><span class="number">77</span>三星 Galaxy S10 智能手机，双摄像头，性能升级！</span><br><span class="line"><span class="number">78</span> rows × <span class="number">1</span> columns</span><br></pre></td></tr></table></figure><p>合并后的数据量不到100条，说明AI返回的条数不到50条。这并不影响我们使用这个数据源。如果需要，可以再次获取数据并进行合并。</p><h3 id="通过-embedding-进行语义搜索">通过 Embedding 进行语义搜索</h3><p>对于搜索问题，我们可以利用 GPT 模型。大公司如百度、阿里有许多内部复杂的策略和模型。但是许多中小型公司，尤其是刚开始提供搜索功能的公司，则通常使用 Elasticsearch 这个开源项目。Elasticsearch 背后的搜索原理是先分词，然后使用倒排索引。</p><p>简单来说，搜索引擎将商品名称（例如“心理学的洞察力：提升你的自信”）拆分成多个单词（如“心理学”、“洞察力”、“提升”、“自信”）。每个标题都是这样切分。然后，建立一个索引，比如“洞察力”这个词，出现过的标题的编号，都按编号顺序跟在“心理”后面。其他的词也是类似。</p><p>当用户搜索时，搜索引擎将用户输入的关键词（例如“思维的力量”）拆分为多个单词（如“思维”和“力量”）。然后，搜索引擎查找包含这些单词的标题，并根据出现的单词的数量、权重等找出相关商品。</p><p>但是，此策略有缺点。如果有同义词，这么简单地搜索是搜不到的。例如，如果搜索“心理学智慧”，虽然语义上很接近，但是因为“心理”、“智慧”这两个词在标题里都没有出现，所以就无法匹配。为了提升搜索效果，需要做更多的工程研发工作，例如找一个同义词表，将标题中出现的同义词也算上等等。</p><p>然而，使用 OpenAI 的 Embedding 接口，可以将一段文本的语义表示为一个向量。向量之间是可以计算距离的，这是在之前的情感分析的零样本分类中演示过的。因此，我们可以将用户的搜索通过 Embedding 接口变为向量。然后，计算它和所有商品标题的余弦距离，找出离我们搜索词最近的几个向量。这些向量实际上是与该商品类似的语义，不一定需要相同的关键词。</p><p>根据这个思路，我们可以使用代码来尝试一下。首先，我们需要计算所有商品标题的 Embedding，并将其存储下来。这里的代码与之前使用 Embedding 进行分类和聚类的代码基本相同，不再详细解释。我们使用 backoff 和 batch 处理，以便代码容错并快速处理这些商品标题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> openai.embeddings_utils <span class="keyword">import</span> get_embeddings</span><br><span class="line"><span class="keyword">import</span> openai, os, backoff</span><br><span class="line">embedding_model = <span class="string">&#x27;text-embedding-ada-002&#x27;</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@backoff.on_exception(<span class="params">backoff.expo, openai.error.RateLimitError</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_embeddings_with_backoff</span>(<span class="params">prompts, engine</span>):</span><br><span class="line">    embeddings = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(prompts), batch_size):</span><br><span class="line">        batch = prompts[i:i+batch_size]</span><br><span class="line">        embeddings += get_embeddings(list_of_text = batch,  engine=engine)</span><br><span class="line">    <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line">prompts = df.product_name.tolist()</span><br><span class="line">prompt_batches = [prompts[i:i+batch_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(prompts), batch_size)]</span><br><span class="line"></span><br><span class="line">embeddings = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> prompt_batches:</span><br><span class="line">    batch_embeddings = get_embeddings_with_backoff(prompts = batch, engine = embedding_model)</span><br><span class="line">    embeddings += batch_embeddings</span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;embedding&#x27;</span>] = embeddings</span><br><span class="line">df.to_parquet(<span class="string">&#x27;./data/taobao_product_title.parquet&#x27;</span>, index = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>为了定义一个 search_product 的搜索函数，我们可以接受三个参数：一个名为 df 的数据源，一个名为 query 的搜索词，以及一个名为 n 的参数，表示要返回多少条搜索结果。该函数将执行以下三项操作：</p><ol type="1"><li><p>使用 OpenAI API 将搜索词转换为 Embedding。</p></li><li><p>计算该 Embedding 与 DataFrame 中每个 Embedding 的余弦距离。</p></li><li><p>根据余弦相似度对搜索结果进行排序，并返回与搜索词最相似的 n 个标题。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> openai.embeddings_utils <span class="keyword">import</span> get_embedding, cosine_similarity</span><br><span class="line"></span><br><span class="line"><span class="comment"># search through the reviews for a specific product</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search_product</span>(<span class="params">df, query, n = <span class="number">3</span>, pprint = <span class="literal">True</span></span>):</span><br><span class="line">    product_embedding = get_embedding(</span><br><span class="line">        query,</span><br><span class="line">        engine = embedding_model</span><br><span class="line">    )</span><br><span class="line">    df[<span class="string">&quot;similarity&quot;</span>] = df.embedding.apply(<span class="keyword">lambda</span> x: cosine_similarity(x, product_embedding))</span><br><span class="line"></span><br><span class="line">    results = (</span><br><span class="line">        df.sort_values(<span class="string">&quot;similarity&quot;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">        .head(n)</span><br><span class="line">        .product_name</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> pprint:</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> results:</span><br><span class="line">            <span class="built_in">print</span>(r)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">results = search_product(df, <span class="string">&quot;心理学智慧&quot;</span>, n = <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>我们就拿刚才举的那个例子，使用"心理学智慧"作为搜索词，调用这个 search_product 函数，然后拿前 3 个返回结果。可以看到，尽管在关键词上完全不同，但是返回的结果里，的确包含了"心理学的洞察力：提升你的自信"这个商品。</p><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">【新品特惠】心理学的智慧：改变你的生活</span><br><span class="line">【新品特惠】心理学的洞察力：提升你的自信</span><br><span class="line">【满减特惠】心理学的智慧：让你的生活更完美</span><br></pre></td></tr></table></figure><blockquote><p>请注意，由于我们的商品标题是随机生成的，因此您得到的数据集和搜索结果可能与我不同。请根据实际情况测试您想要的搜索词。</p></blockquote><h3 id="利用-embedding-进行商品推荐的冷启动">利用 Embedding 进行商品推荐的冷启动</h3><p>Embedding 向量距离不仅可以用于搜索，还可以用于商品推荐中的冷启动。主流的推荐算法主要依托于用户的“看了又看”等行为信息。即如果有很多用户看了 OPPO 手机，并转而看了 vivo 手机，那么在用户看 OPPO 手机时，我们就可以向他推荐 vivo 手机。但往往新商品或新平台缺乏相关的行为数据。此时，我们同样可以根据商品名称在语义上的相似度来进行商品推荐。</p><p>我们的代码实现和上面的搜索例子基本一致，唯一的差别是商品名称的 Embedding 是从 DataFrame 中获取，而不是通过调用 OpenAI 的 Embedding API 获取，因为我们已经计算过一遍嵌入，无需再次请求。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">recommend_product</span>(<span class="params">df, product_name, n=<span class="number">3</span>, pprint=<span class="literal">True</span></span>):</span><br><span class="line">    product_embedding = df[df[<span class="string">&#x27;product_name&#x27;</span>] == product_name].iloc[<span class="number">0</span>].embedding</span><br><span class="line">    df[<span class="string">&quot;similarity&quot;</span>] = df.embedding.apply(<span class="keyword">lambda</span> x: cosine_similarity(x, product_embedding))</span><br><span class="line"></span><br><span class="line">    results = (</span><br><span class="line">        df.sort_values(<span class="string">&quot;similarity&quot;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">        .head(n)</span><br><span class="line">        .product_name</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> pprint:</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> results:</span><br><span class="line">            <span class="built_in">print</span>(r)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">results = recommend_product(df, <span class="string">&quot;【全场满减】心理学的技巧：让你的思维更灵活&quot;</span>, n=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">【全场满减】心理学的技巧：让你的思维更灵活</span><br><span class="line">【全场满减】心理学的奥秘：激发你的潜能</span><br><span class="line">【全场满减】心理学的技巧：改变你的态度</span><br></pre></td></tr></table></figure><h3 id="利用-faiss-加速搜索">利用 Faiss 加速搜索</h3><p>上面的示例代码还存在一个问题：每次搜索或推荐时，我们都需要计算输入嵌入和所有数据嵌入之间的余弦相似度。在上面的例子中，我们只检索了100条数据，但在实际应用中，即使不像百度或谷歌这样的搜索引擎，检索的内容数也可能达到几百万或上千万。如果每次搜索都要计算几百万次余弦距离，速度肯定会很慢。</p><p>为了解决这个问题，我们可以使用一些向量数据库或软件包，它们能够快速搜索相似性。例如，我推荐使用 Facebook 开源的 Faiss Python 包，它的全称是 Facebook AI Similarity Search，可以快速进行高维向量的相似性搜索。</p><p>我们可以将 DataFrame 中的嵌入向量加载到 Faiss 索引中，然后让 Faiss 帮助我们快速找到最相似的向量。下面我们来看看效果。</p><p>当然，按照惯例，我们需要先安装 Faiss 这个 Python 库。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge faiss-cpu</span><br></pre></td></tr></table></figure><blockquote><p>faiss有多个版本，包括cpu, gpu等等，你可以到这里看看<a href="https://github.com/facebookresearch/faiss/blob/main/INSTALL.md">官方文档</a>，选择适合自己的版本安装。因为我是M1， 所以选择了cpu版本。</p></blockquote><p>把整个 Embedding 变成一个二维矩阵，然后直接将其加载到 Faiss 索引中即可。在之前，我们需要定义好 Faiss 索引的维度数，与我们的 Embedding 向量的维度数相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_embeddings_to_faiss</span>(<span class="params">df</span>):</span><br><span class="line">    embeddings = np.array(df[<span class="string">&#x27;embedding&#x27;</span>].tolist()).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    index = faiss.IndexFlatL2(embeddings.shape[<span class="number">1</span>])</span><br><span class="line">    index.add(embeddings)</span><br><span class="line">    <span class="keyword">return</span> index</span><br><span class="line"></span><br><span class="line">index = load_embeddings_to_faiss(df)</span><br></pre></td></tr></table></figure><p>搜索 Faiss 简单易用。我们将查询转换成嵌入形式，再将其转换为 numpy 数组向量。然后，我们只需对刚才生成的索引 index 调用 search 方法，并指定返回的结果数量。返回的仅是索引的 index，即加载在 Faiss 中的第几个索引。根据这个索引，在 DataFrame 中查找对应的行数和商品标题即可获得搜索结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">search_index</span>(<span class="params">index, df, query, k=<span class="number">5</span></span>):</span><br><span class="line">    query_vector = np.array(get_embedding(query, engine=embedding_model)).reshape(<span class="number">1</span>, -<span class="number">1</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    distances, indexes = index.search(query_vector, k)</span><br><span class="line"></span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(indexes)):</span><br><span class="line">        product_names = df.iloc[indexes[i]][<span class="string">&#x27;product_name&#x27;</span>].values.tolist()</span><br><span class="line">        results.append((distances[i], product_names))    </span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">products = search_index(index, df, <span class="string">&quot;心理学智慧&quot;</span>, k=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> distances, product_names <span class="keyword">in</span> products:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(distances)):</span><br><span class="line">        <span class="built_in">print</span>(product_names[i], distances[i])</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">【新品特惠】心理学的智慧：改变你的生活 <span class="number">0.20244475</span></span><br><span class="line">【新品特惠】心理学的洞察力：提升你的自信 <span class="number">0.21296506</span></span><br><span class="line">【满减特惠】心理学的智慧：让你的生活更完美 <span class="number">0.21423605</span></span><br></pre></td></tr></table></figure><p>用"自然淡雅背包"这个关键词搜索，结果与我们之前计算的余弦距离排序的结果相同。 Faiss 通过 ANN 近似最近邻算法实现相似性搜索。如需了解 Faiss 的原理，请向 ChatGPT 提问。</p><img src="/Implementing-semantic-retrieval-using-Embedding/20230601170153.png" class="" title="img"><p>Faiss库能够加载的数据量受限于内存大小。如果数据量继续增长，需要使用向量数据库进行搜索。例如，OpenAI推荐的<a href="https://www.pinecone.io/">Pinecone</a>和<a href="https://weaviate.io/">Weaviate</a>，也有许多团队使用国人开源产品<a href="https://milvus.io/">Milvus</a>。虽然使用Embedding的相似度可以快速启动搜索和推荐，但需要更复杂的策略才能实现更好的效果。例如，根据用户反馈的行为更好地排序搜索和推荐结果。但对于提供简单的搜索或推荐功能而言，文本的嵌入相似度是很好的快速启动方式。</p><h3 id="小结"><strong>小结</strong></h3><p>在这一讲中，我将教给你三个主要的技巧，以便更快速地优化现有业务中的推荐和搜索功能。</p><p>首先，即使没有适当的测试数据，我们也可以让AI生成一些数据。这种方法不仅可以节省在线查找数据的时间，还可以根据自己的需求生成具有特定特征的数据。例如，我们可以要求在商品标题中添加一些促销相关信息。</p><p>其次，我们可以利用嵌入之间的余弦相似度作为语义相似度，来优化搜索。通过嵌入的相似性，我们不要求搜索词和查询内容之间完全匹配，只要它们的语义信息接近即可。</p><p>最后，我们将学习如何使用Faiss这样的Python库或其他向量数据库，以快速检索向量。这样，我们就不必每次搜索都计算整个数据库的余弦相似度。</p><p>通过计算嵌入向量并对其进行索引，我们可以将外部知识和信息引入到使用GPT模型的应用程序中。在以后的课程中，我们将学习如何利用这些外部知识开发更复杂的AI应用程序。</p><h3 id="课后练习"><strong>课后练习</strong></h3><p>搜索中经常会遇到同一关键字具有多重含义的情况。例如，当我们搜索“小米手机”时，结果中应该包括“荣耀V30 Pro”，但不应包括“黑龙江优质小米”。你可以尝试使用嵌入进行语义搜索，看看是否仍然会出现这种问题。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hi，我是茶桁。&lt;/p&gt;
&lt;p&gt;过去的8讲，你已熟悉Embedding和Completion接口。Embedding适合用于机器学习中的分类、聚类等传统场景。Completion接口可以用作聊天机器人，也可以用于文案撰写、文本摘要、机器翻译等工作。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>08 改写和审核</title>
    <link href="https://hivan.me/Rewriting-and-Reviewing/"/>
    <id>https://hivan.me/Rewriting-and-Reviewing/</id>
    <published>2023-05-15T09:00:02.000Z</published>
    <updated>2023-06-01T13:23:12.734Z</updated>
    
    <content type="html"><![CDATA[<p>Hi, 我是茶桁。</p><p>我们已经介绍了 OpenAI 的主要接口。这是基础知识系列的最后一讲，我们将讨论 OpenAI GPT 系列模型的其他接口。你可能不会经常使用其中一些接口，但了解它们不会有任何坏处，说不定你会在某些需求中用到它们。</p><span id="more"></span><p>在这篇文章中，我们将一起探讨 OpenAI 为文本改写和内容审核提供的功能，以及 GPT 系列模型的种类、区别和应用场景。</p><h3 id="文本改写教程">文本改写教程</h3><p>我猜你可能已经用过许多基于 AI 大型语言模型的产品了。其中很常见的一类应用是写作助手，比如 Notion AI。它可以帮助你在文章中选择一段内容，并让 AI 帮你修改它，例如缩短文本或改变语气等。</p><img src="/Rewriting-and-Reviewing/20230601170036.png" class="" title="img"><p>OpenAI的GPT系列模型是一个生成式模型，给它一段文字，它可以补全后面的文字。要修改一段内容，可以通过提示语来解决问题。下面这段代码是通过ChatGPT模型实现了这个功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">openai.api_key = <span class="string">&#x27;OPENAI_API_KEY&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_text_short</span>(<span class="params">text</span>):</span><br><span class="line">    messages = []</span><br><span class="line">    messages.append( &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你是一个用来将文本改写得短的AI助手，用户输入一段文本，你给出一段意思相同，但是短小精悍的结果&quot;</span>&#125;)</span><br><span class="line">    messages.append( &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: text&#125;)</span><br><span class="line">    response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>,</span><br><span class="line">        messages=messages,</span><br><span class="line">        temperature=<span class="number">0.5</span>,</span><br><span class="line">        max_tokens=<span class="number">2048</span>,</span><br><span class="line">        presence_penalty=<span class="number">0</span>,</span><br><span class="line">        frequency_penalty=<span class="number">2</span>,</span><br><span class="line">        n=<span class="number">3</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">long_text = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">我们可以回顾下它走的一些关键路程：GPT 1.0走的是生成模式的自回归语言模型路线，比Bert出来的还早些。Bert证明了：双向语言模型对于很多NLP理解类任务，效果比自回归这种单向语言模型效果更好。尽管如此，GPT 2.0并没有因此切换到双向语言模型这条路上，仍然走文本生成的路，而且开始尝试零示例（zero shot）prompt和少量示例（few shot）prompt。其实这时候， OpenAI心目中的AGI已经开始浮出水面，逐渐显示出轮廓了。只是因为zero shot/few shot效果比Bert+fine-tuning差的比较远，所以大家都没太当回事，甚至不理解它为什么要始终坚持走单向语言模型的路线。这个时候，我估计即使是OpenAI自己，也不一定能确保这条路肯定能走通。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">short_version = make_text_short(long_text)</span><br><span class="line"></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> choice <span class="keyword">in</span> short_version[<span class="string">&quot;choices&quot;</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;version <span class="subst">&#123;index&#125;</span>: &quot;</span> + choice[<span class="string">&quot;message&quot;</span>][<span class="string">&quot;content&quot;</span>])</span><br><span class="line">    index += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">version <span class="number">1</span>: GPT <span class="number">1.0</span>和Bert是NLP领域的里程碑，前者采用了自回归语言模型，后者证明双向语言模型更优。不过GPT <span class="number">2.0</span>仍然坚持文本生成路线，并尝试了零/少量示例prompt。OpenAI心目中的AGI开始显现轮廓，但由于效果不如Bert+fine-tuning而被忽视。这条路是否可行并不能确定。</span><br><span class="line">version <span class="number">2</span>: GPT <span class="number">1.0</span>采用了生成模式的自回归语言模型路线，比Bert更早。Bert证明了双向语言模型在NLP理解类任务上效果更好，但GPT <span class="number">2.0</span>仍然坚持文本生成路线，并尝试了零/少量示例prompt。OpenAI心目中的AGI逐渐浮出水面，但zero shot/few shot效果不如Bert+fine-tuning, 这条路是否可行还需观察。</span><br><span class="line">version <span class="number">3</span>: GPT <span class="number">1.0</span>采用自回归语言模型，比Bert早。Bert证明双向语言模型效果更好，但GPT <span class="number">2.0</span>仍然走文本生成路线，并尝试了零/少量示例prompt。OpenAI的AGI已经开始浮出水面，只是因为zero/few shot效果差而被忽视。即使OpenAI也不能保证这条路一定能成功。</span><br></pre></td></tr></table></figure><p>我们使用 ChatGPT 的模型接口，因为它比较便宜。我们使用了以下参数：</p><ol type="1"><li><p><code>n=3</code>，让 AI 给我们返回 3 个答案供我们选择。在文本改写类的应用里面，我们通常不只是直接给出答案，而是会给用户几个选项来选择。</p></li><li><p><code>presence_penalty=0</code>、 <code>frequency_penalty=2</code>。这两个参数和 <code>temperature</code> 参数类似，都是来控制你输出的内容的。 <code>presence_penalty</code> 指的是如果一个 Token 在前面的内容已经出现过了，那么在后面生成的时候给它的概率一定的惩罚； <code>frequency_penalty</code> 指的是对于重复出现的 Token 进行概率惩罚。这样，AI 就会尽量使用不同的表述。</p></li></ol><h3 id="通过-logit_bias-参数精准控制内容"><strong>通过 logit_bias 参数精准控制内容</strong></h3><p>虽然 <code>temperature</code> 、 <code>presence_penalty</code> 和 <code>frequency_penalty</code> 等参数都可以控制生成文本的风格，但是它们都只是单一的参数，无法精确控制不想出现的词汇。不过，OpenAI 提供了方法来解决这个问题。例如，如果我们不想让“灾害”这两个字出现在生成的内容中，可以采取如下措施。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line">encoding = tiktoken.get_encoding(<span class="string">&#x27;p50k_base&#x27;</span>)</span><br><span class="line">token_ids = encoding.encode(<span class="string">&quot;灾害&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(token_ids)</span><br><span class="line"></span><br><span class="line">bias_map = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> token_id <span class="keyword">in</span> token_ids:</span><br><span class="line">    bias_map[token_id] = -<span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_text_short</span>(<span class="params">text</span>):</span><br><span class="line">    messages = []</span><br><span class="line">    messages.append( &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你是一个用来将文本改写得短的AI助手，用户输入一段文本，你给出一段意思相同，但是短小精悍的结果&quot;</span>&#125;)</span><br><span class="line">    messages.append( &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: text&#125;)</span><br><span class="line">    response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>, messages=messages, temperature=<span class="number">0.5</span>, max_tokens=<span class="number">2048</span>,</span><br><span class="line">        n=<span class="number">3</span>, presence_penalty=<span class="number">0</span>, frequency_penalty=<span class="number">2</span>, </span><br><span class="line">        logit_bias = bias_map,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">short_version = make_text_short(long_text)</span><br><span class="line"></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> choice <span class="keyword">in</span> short_version[<span class="string">&quot;choices&quot;</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;version <span class="subst">&#123;index&#125;</span>: &quot;</span> + choice[<span class="string">&quot;message&quot;</span>][<span class="string">&quot;content&quot;</span>])</span><br><span class="line">    index += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>输出结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">163</span>, <span class="number">223</span>, <span class="number">122</span>, <span class="number">22522</span>, <span class="number">111</span>]</span><br><span class="line">version <span class="number">1</span>: GPT <span class="number">1.0</span>是生成模式的自回归语言模型，比Bert还早。Bert证明了双向语言模型效果更好，但GPT <span class="number">2.0</span>仍然坚持文本生成路线，并尝试零/少量示例prompt。这时OpenAI的AGI开始浮出水面，只是因为效果不如Bert+fine-tuning而被忽视。</span><br><span class="line">version <span class="number">2</span>: GPT <span class="number">1.0</span>是自回归语言模型，比Bert早。Bert证明了双向语言模型效果更好，但GPT <span class="number">2.0</span>仍然选择文本生成，并尝试了零/少量示例prompt。OpenAI的AGI开始浮出水面，但因为zero/few shot效果差被忽视。这条路是否可行不确定。</span><br><span class="line">version <span class="number">3</span>: GPT <span class="number">1.0</span>采用生成模式自回归语言模型，比Bert更早。Bert证明了双向语言模型的效果比自回归好，但GPT <span class="number">2.0</span>仍然坚持文本生成路线，并尝试零/少量示例prompt。OpenAI心目中的AGI已经开始浮出水面，只是因为zero shot/few shot效果不如Bert+fine-tuning而被忽视。</span><br></pre></td></tr></table></figure><p>我们使用 Tiktoken 库，找到了“灾害”对应的 Token，并赋予它们 -100 的 bias，以避免这个词出现在生成的回复中。我们将整个 <code>bias_map</code> 作为参数传递给 <code>Completion</code> 的 <code>logit_bias</code> 参数。现在，生成的三个回复都不包含“灾害”这个词了，即使之前的第一个回复中也包含了。现在，“灾”这个字被强制改成了繁体字，而另一个则出现了错别字“宣”。</p><p><code>logit_bias</code> 参数的取值范围为 -100 到 100 之间，但通常设置在 1 到 -1 之间就足够了。如果将其设置为 100，则必须出现某些字，整个生成过程会变得非常缓慢，难以忍受。</p><h3 id="用英文减少-token-使用">用英文减少 Token 使用</h3><p>虽然“灾害”只有两个中文字，但在使用 Tiktoken 处理时，我们打印了对应的 Token ID，实际上需要使用5个 Token。同样的，中文中含义相同的词语，所需的 Token 数量比英文多。例如，将上面的句子翻译成英文，与中文相同的内容所需的 Token 数量进行比较。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">translate</span>(<span class="params">text</span>):</span><br><span class="line">    messages = []</span><br><span class="line">    messages.append( &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你是一个翻译，把用户的话翻译成英文&quot;</span>&#125;)</span><br><span class="line">    messages.append( &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: text&#125;)</span><br><span class="line">    response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>, messages=messages, temperature=<span class="number">0.5</span>, max_tokens=<span class="number">2048</span>,        n=<span class="number">1</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> response[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;message&quot;</span>][<span class="string">&quot;content&quot;</span>]</span><br><span class="line"></span><br><span class="line">chinese = long_text</span><br><span class="line">english = translate(chinese)</span><br><span class="line"></span><br><span class="line">num_of_tokens_in_chinese = <span class="built_in">len</span>(encoding.encode(chinese))</span><br><span class="line">num_of_tokens_in_english = <span class="built_in">len</span>(encoding.encode(english))</span><br><span class="line"><span class="built_in">print</span>(english)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;chinese: <span class="subst">&#123;num_of_tokens_in_chinese&#125;</span> tokens&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;english: <span class="subst">&#123;num_of_tokens_in_english&#125;</span> tokens&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">We can review some of its key milestones: GPT <span class="number">1.0</span> followed the path of a generative pattern of autoregressive language model, which came out earlier than Bert. Bert proved that <span class="keyword">for</span> many NLP understanding tasks, the effect of bidirectional language models <span class="keyword">is</span> better than that of unidirectional language models like autoregressive. Nevertheless, GPT <span class="number">2.0</span> did <span class="keyword">not</span> switch to the bidirectional language model, but continued to follow the path of text generation, <span class="keyword">and</span> began to <span class="keyword">try</span> zero-shot <span class="keyword">and</span> few-shot prompts. Actually, at this point, OpenAI<span class="string">&#x27;s AGI had already begun to emerge and gradually showed its outline. It&#x27;</span>s just that because the effect of zero-shot/few-shot <span class="keyword">is</span> far worse than Bert+fine-tuning, everyone didn<span class="string">&#x27;t pay much attention to it, and even didn&#x27;</span>t understand why it always insisted on the path of unidirectional language models. At this point, I estimate that even OpenAI itself cannot guarantee that this path will definitely work.</span><br><span class="line">chinese: <span class="number">589</span> tokens</span><br><span class="line">english: <span class="number">208</span> tokens</span><br></pre></td></tr></table></figure><p>同样的内容，在中文中要消耗超过 589个 Token，而英文只有 208。在使用 OpenAI 的接口时，最好使用英语提示语，以节约成本。当然，在输出结果时可以使用 " <code>generate Chinese</code>" 等提示。但是，我们将在后面的课程演示中尽量使用中文，以方便您的理解。</p><h3 id="openai-的模型">OpenAI 的模型</h3><p>OpenAI 曾经提供过一个<a href="https://platform.openai.com/docs/api-reference/edits">Edit 接口</a>，并单独提供了文本编辑模型。目前，这个接口和模型不能使用，可能因为是 Alpha 版本或已被下线。</p><p>由于 OpenAI 的产品更新非常快，所以最好的办法是通过它提供的接口查看可用模型，以便选择效果最好或最新的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># list all open ai models</span></span><br><span class="line">engines = openai.Engine.<span class="built_in">list</span>()</span><br><span class="line">pd = pd.DataFrame(openai.Engine.<span class="built_in">list</span>()[<span class="string">&#x27;data&#x27;</span>])</span><br><span class="line">display(pd[[<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;owner&#x27;</span>]])</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了文本长度，仅展示了部分，中间大部分省略掉了。可以去查看我源代码，里面有展示。</span></span><br><span class="line"><span class="built_in">id</span>owner</span><br><span class="line"><span class="number">0</span>whisper-<span class="number">1</span>openai-internal</span><br><span class="line"><span class="number">1</span>babbageopenai</span><br><span class="line"><span class="number">2</span>davinciopenai</span><br><span class="line"><span class="number">3</span>text-davinci-edit-001openai</span><br><span class="line"><span class="number">4</span>babbage-code-search-codeopenai-dev</span><br><span class="line"><span class="number">5</span>text-similarity-babbage-001openai-dev</span><br><span class="line"><span class="number">6</span>code-davinci-edit-001openai</span><br><span class="line"><span class="number">7</span>text-davinci-001openai</span><br><span class="line">...</span><br><span class="line"><span class="number">49</span>davinci-similarityopenai-dev</span><br></pre></td></tr></table></figure><p>截止到目前为止，输出结果中有 49 个模型。这些模型的名称已经很直观了，比如 <code>text-similarity-babbage-001</code> 用于相似度匹配，适合用于零样本分类，而 <code>text-search-davinci-doc-001</code> 更适合用于文档搜索。虽然一些模型标记为 openai-dev 或 <code>openai-internal</code> , 但它们仍然可用。例如，在第 02 讲中使用 <code>get_embedding</code> 方法获取向量时，后台使用的是 <code>text-similarity-davinci-001</code> 模型，这也是 openai-dev 的模型之一。虽然其中许多模型已经过时，但实际上只需要关注几类主要模型即可。GPT-4 家族的模型，包括 gpt-4 和 gpt-4-0314。使用方式和 ChatGPT 的模型一样，带日期的模型是模型快照，不会随时间迁移不断更新。GPT-4 的模型现在还很昂贵，输入 1000 个 Token 需要 0.03 美分，生成 1000 个 Token 则需要 0.06 美分。通常我使用它来写代码，准确率较高。</p><ol type="1"><li><p>GPT-3.5 家族的模型，包括 ChatGPT 所使用的 gpt-3.5-turbo 或者 gpt-3.5-turbo-0301，以及 text-davinci-003 和 text-davinci-002 这两个模型。前者专门针对对话的形式进行了微调，并且价格便宜，无论输入输出，1000 个 Token 都只需要 0.002 美分。后两个里，003 的模型有一个特殊功能，支持"插入文本"。003 是基于强化学习微调的，而 002 则是监督学习下微调的。text-davinci-003 和 002 模型比 3.5-turbo 贵 10 倍，但输出更稳定。你可以根据自己需要来决定。</p></li><li><p>Ada、Babbage、Curie 以及 Davinci 这四个基础模型只适合下达单轮指令，不适合考虑复杂上下文和进行逻辑推理。这四个模型按首字母排序，价格越来越贵，效果越来越好。如果要微调一个属于自己的模型，需要基于这四个基础模型。</p></li><li><p>text-embedding-ada-002、text-similarity-ada-001 这些专门用途模型通常用于获取 Embedding，用于其他机器学习模型的训练或语义相似度比较。</p></li></ol><p>所有模型的名字都来自科学史上的名人。Ada 来自人类史上第一位程序员 Ada，她也是著名诗人拜伦的女儿。Babadge 则是设计了分析机的巴贝奇，巴贝奇分析机也被认为是现代计算机的前身。Curie 指居里夫人，Davinci 是指达芬奇。</p><p>可以挑几个模型试一下它们 Embedding 的维度数量，就知道模型的尺寸不同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> openai.embeddings_utils <span class="keyword">import</span> get_embedding</span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;让我们来算算Embedding&quot;</span></span><br><span class="line"></span><br><span class="line">embedding_ada = get_embedding(text, engine=<span class="string">&quot;text-embedding-ada-002&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;embedding-ada: &quot;</span>, <span class="built_in">len</span>(embedding_ada))</span><br><span class="line"></span><br><span class="line">similarity_ada = get_embedding(text, engine=<span class="string">&quot;text-similarity-ada-001&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;similarity-ada: &quot;</span>, <span class="built_in">len</span>(similarity_ada))</span><br><span class="line"></span><br><span class="line">babbage_similarity = get_embedding(text, engine=<span class="string">&quot;babbage-similarity&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;babbage-similarity: &quot;</span>, <span class="built_in">len</span>(babbage_similarity))</span><br><span class="line"></span><br><span class="line">babbage_search_query = get_embedding(text, engine=<span class="string">&quot;text-search-babbage-query-001&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;search-babbage-query: &quot;</span>, <span class="built_in">len</span>(babbage_search_query))</span><br><span class="line"></span><br><span class="line">curie = get_embedding(text, engine=<span class="string">&quot;curie-similarity&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;curie-similarity: &quot;</span>, <span class="built_in">len</span>(curie))</span><br><span class="line"></span><br><span class="line">davinci = get_embedding(text, engine=<span class="string">&quot;text-similarity-davinci-001&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;davinci-similarity: &quot;</span>, <span class="built_in">len</span>(davinci))</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">embedding-ada:  <span class="number">1536</span></span><br><span class="line">similarity-ada:  <span class="number">1024</span></span><br><span class="line">babbage-similarity:  <span class="number">2048</span></span><br><span class="line">search-babbage-query:  <span class="number">2048</span></span><br><span class="line">curie-similarity:  <span class="number">4096</span></span><br><span class="line">davinci-similarity:  <span class="number">12288</span></span><br></pre></td></tr></table></figure><p><code>ada-similarity</code> 的最小维度为 1024， <code>davinci-similarity</code> 的最大维度为 12288。因此，它们的价格和效果有所不同。</p><h3 id="gpt-也能插入内容">GPT 也能插入内容</h3><p><code>text-davinci-003</code> 模型有一个特殊功能：插入文本。你可以使用这个功能来改写文本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">prefix = <span class="string">&quot;&quot;&quot;在这个快节奏的现代社会中，我们每个人都面临着各种各样的挑战和困难。</span></span><br><span class="line"><span class="string">在这些挑战和困难中，有些是由外部因素引起的，例如经济萧条、全球变暖和自然灾害等。\n&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 还有一些是由内部因素引起的，例如情感问题、健康问题和自我怀疑等。</span></span><br><span class="line">suffix = <span class="string">&quot;&quot;&quot;\n面对这些挑战和困难，我们需要采取积极的态度和行动来克服它们。</span></span><br><span class="line"><span class="string">这意味着我们必须具备坚韧不拔的意志和创造性思维，以及寻求外部支持的能力。</span></span><br><span class="line"><span class="string">只有这样，我们才能真正地实现自己的潜力并取得成功。&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">insert_text</span>(<span class="params">prefix, suffix</span>):</span><br><span class="line">    response = openai.Completion.create(</span><br><span class="line">        model=<span class="string">&quot;text-davinci-003&quot;</span>,</span><br><span class="line">        prompt=prefix,</span><br><span class="line">        suffix=suffix,</span><br><span class="line">        max_tokens=<span class="number">1024</span>,</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">response = insert_text(prefix, suffix)</span><br><span class="line"><span class="built_in">print</span>(response[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;text&quot;</span>])</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此外，还有一些挑战和困难是由自我内在的原因导致的，比如事业上的恐惧和沮丧，以及担心无法达到完美标准等。</span><br></pre></td></tr></table></figure><p>使用这个接口和普通的 Completion 接口基本相同，唯一的区别在于除了前缀的 prompt 参数之外，还需要一个后缀的 suffix 参数。</p><p>需要注意的是插入内容的提示语。如果我们稍微修改上面的内容，例如去掉 Suffix 一开始的换行符，插入的文本内容可能会不如预期。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">prefix = <span class="string">&quot;&quot;&quot;在这个快节奏的现代社会中，我们每个人都面临着各种各样的挑战和困难。</span></span><br><span class="line"><span class="string">在这些挑战和困难中，有些是由外部因素引起的，例如经济萧条、全球变暖和自然灾害等。\n&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 还有一些是由内部因素引起的，例如情感问题、健康问题和自我怀疑等。</span></span><br><span class="line">suffix = <span class="string">&quot;&quot;&quot;面对这些挑战和困难，我们需要采取积极的态度和行动来克服它们。</span></span><br><span class="line"><span class="string">这意味着我们必须具备坚韧不拔的意志和创造性思维，以及寻求外部支持的能力。</span></span><br><span class="line"><span class="string">只有这样，我们才能真正地实现自己的潜力并取得成功。&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">response = insert_text(prefix, suffix)</span><br><span class="line"><span class="built_in">print</span>(response[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;text&quot;</span>])</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">而另一些则是由内在因素引起的，比如性格、思想、管理方式或者技能缺乏等。</span><br><span class="line">无论是外部因素还是内在因素，</span><br></pre></td></tr></table></figure><p>在使用这个 INSERT 接口的时候，考虑好文本之间需要使用什么样的分隔符，是非常重要的。</p><h3 id="ai-的正直">AI 的正直</h3><p>介绍 OpenAI 的最后一个自然语言处理接口——Moderate。这是唯一一个免费的接口，可以检查用户输入和返回的内容。如果出现不当内容，你就可以屏蔽用户访问或人工审核。</p><p>下面看一个例子，如何使用这个接口。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">chatgpt</span>(<span class="params">text</span>):</span><br><span class="line">    messages = []</span><br><span class="line">    messages.append( &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a useful AI assistant&quot;</span>&#125;)</span><br><span class="line">    messages.append( &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: text&#125;)</span><br><span class="line">    response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>,</span><br><span class="line">        messages=messages,</span><br><span class="line">        temperature=<span class="number">0.5</span>,</span><br><span class="line">        max_tokens=<span class="number">2048</span>,</span><br><span class="line">        top_p=<span class="number">1</span>,</span><br><span class="line">    )</span><br><span class="line">    message = response[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;message&quot;</span>][<span class="string">&quot;content&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> message</span><br><span class="line"></span><br><span class="line">threaten = <span class="string">&quot;你不听我的我就拿刀砍死你&quot;</span></span><br><span class="line"><span class="built_in">print</span>(chatgpt(threaten))</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对不起，我不理解您的意思。作为AI助手，我旨在为您提供最好的服务和支持，而且我不能被伤害。如果您有任何问题或需要帮助，请告诉我，我会尽力回答和解决。</span><br></pre></td></tr></table></figure><p>对于 AI 的暴力威胁，我们可以看到简单调用 ChatGPT 的 API 并不能返回一个日常的对话，而是告知用户不会回应暴力言论。接着，我们可以将这句话发送到 moderate 的接口进行进一步处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">threaten = <span class="string">&quot;你不听我的我就拿刀砍死你&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">moderation</span>(<span class="params">text</span>):</span><br><span class="line">    response = openai.Moderation.create(</span><br><span class="line">        <span class="built_in">input</span>=text</span><br><span class="line">    )</span><br><span class="line">    output = response[<span class="string">&quot;results&quot;</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"><span class="built_in">print</span>(moderation(threaten))</span><br></pre></td></tr></table></figure><p>返回结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;categories&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;hate&quot;</span>: false,</span><br><span class="line">    <span class="string">&quot;hate/threatening&quot;</span>: false,</span><br><span class="line">    <span class="string">&quot;self-harm&quot;</span>: false,</span><br><span class="line">    <span class="string">&quot;sexual&quot;</span>: false,</span><br><span class="line">    <span class="string">&quot;sexual/minors&quot;</span>: false,</span><br><span class="line">    <span class="string">&quot;violence&quot;</span>: true,</span><br><span class="line">    <span class="string">&quot;violence/graphic&quot;</span>: false</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;category_scores&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;hate&quot;</span>: <span class="number">0.030033664777874947</span>,</span><br><span class="line">    <span class="string">&quot;hate/threatening&quot;</span>: <span class="number">0.0002820899826474488</span>,</span><br><span class="line">    <span class="string">&quot;self-harm&quot;</span>: <span class="number">0.004850226454436779</span>,</span><br><span class="line">    <span class="string">&quot;sexual&quot;</span>: <span class="number">2.2907377569936216e-05</span>,</span><br><span class="line">    <span class="string">&quot;sexual/minors&quot;</span>: <span class="number">6.477687275463495e-09</span>,</span><br><span class="line">    <span class="string">&quot;violence&quot;</span>: <span class="number">0.9996402263641357</span>,</span><br><span class="line">    <span class="string">&quot;violence/graphic&quot;</span>: <span class="number">4.35576839663554e-05</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;flagged&quot;</span>: true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>moderate 接口返回的是一个 JSON，包括是否应该对输入内容进行标记的 flag，问题类型的 categories，以及每个 categories 的分数 category_scores。举例，我们的文本被标记为暴力。该免费接口可用于所有输入输出，即使您不使用 ChatGPT 的 AI 功能，只是经营在线网站，也可以使用该接口过滤不合适的内容。</p><h3 id="小结">小结</h3><p>我们总结了 ChatGPT API 的基础功能，包括如何使用提示语进行文本改写。我们深入了解了 Completion 接口的一些新参数，其中的 logit_bias 参数可以帮助我们在生成的文本中精确避免不希望出现的 Token。我们还发现，中文消耗的 Token 数量要远高于英文，因此建议在生产环境下使用英文提示语。</p><p>此外，我们介绍了 OpenAI 提供的各种模型以及它们的应用场景。我们体验了两个特殊接口：文本插入功能和内容审核过滤。这样我们就学习了 OpenAI GPT 模型的所有基本接口，以及如何利用这些接口完成最简单的任务，包括文本处理、聊天机器人、分类和聚类、以及文本改写和内容审核。</p><p>在第二部分中，我们将学习如何结合自己的专有数据开发自己的应用程序，这是这门课程中更精彩的一部分。</p><p><strong>而到此为止，本课程的免费部分也就结束了，之后就要进入收费课程。放心，不会很贵的，如果您觉得课程对您有用，应该不是吝啬那1 ~ 2元。</strong></p><h3 id="课后练习">课后练习</h3><p>使用<a href="https://www.hivan.me/Quickly%20build%20an%20AI%20application">06讲中</a>介绍的Gradio和本讲介绍的内容，尝试创建一个文本改写应用。另外，可以将问题拆解并输入ChatGPT，看看它是否能够编写相应的代码。期待在评论区看到您的分享，欢迎将此课程分享给感兴趣的朋友。下次见！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hi, 我是茶桁。&lt;/p&gt;
&lt;p&gt;我们已经介绍了 OpenAI 的主要接口。这是基础知识系列的最后一讲，我们将讨论 OpenAI GPT 系列模型的其他接口。你可能不会经常使用其中一些接口，但了解它们不会有任何坏处，说不定你会在某些需求中用到它们。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>07 AI帮你做总结</title>
    <link href="https://hivan.me/AI-can-help-you-summarize-your-content/"/>
    <id>https://hivan.me/AI-can-help-you-summarize-your-content/</id>
    <published>2023-05-14T08:58:49.000Z</published>
    <updated>2023-06-01T13:24:21.570Z</updated>
    
    <content type="html"><![CDATA[<p>Hi， 我是茶桁。</p><p>在<a href="https://www.hivan.me/Quickly%20build%20an%20AI%20application">上一节中</a>，我们介绍了如何使用最新的ChatGPT API，注册HuggingFace账户，并将我们的聊天机器人部署出去。在这个过程中，我们学习了实际的应用开发过程，使你对聊天机器人的开发有了充足的体验。在这一讲中，我们将探讨OpenAI的各种接口提供的能力，以更深入地了解这些接口。我们将分别介绍如何利用嵌入（Embedding）进行文本聚类，并使用提示语（Prompt）对文本进行总结。此外，我们还将介绍其他的接口能力，如语言模型和自然语言生成，以帮助您更好地理解和利用OpenAI的各种功能。</p><span id="more"></span><h2 id="基于-embedding-向量进行文本聚类文本聚类简介">基于 Embedding 向量进行文本聚类文本聚类简介</h2><p>文本聚类是一种自动将未标注的文本根据相似度分成几类的方法。使用 GPT 系列模型进行文本聚类非常简单，我们可以将文本转换为向量，然后使用一些简单的聚类算法，比如最简单的 K-Means 算法。</p><p>我们使用 20 newsgroups 数据集来演示如何使用 GPT 系列模型。该数据集包含带有标注的英文新闻组数据。虽然该数据集已经经过预处理，例如去除标点符号、停用词等等，但仍可以观察到 OpenAI 的 GPT 系列模型在处理这种不太“自然语言”的数据时的效果。</p><p>我们可以使用 Python 库 scikit-learn 来获取数据。该库是一个非常常用的机器学习库，我们可以直接下载数据并将其存储为 CSV 文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_to_csv</span>():</span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    newsgroups_train = fetch_20newsgroups(subset = <span class="string">&#x27;train&#x27;</span>, remove = &#123;<span class="string">&#x27;headers&#x27;</span>, <span class="string">&#x27;footers&#x27;</span>, <span class="string">&#x27;quotes&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转换数据</span></span><br><span class="line">    df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T</span><br><span class="line">    df.columns = [<span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;target&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    targets = pd.DataFrame(newsgroups_train.target_names, columns = [<span class="string">&#x27;title&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    out = pd.merge(df, targets, left_on = <span class="string">&#x27;target&#x27;</span>, right_index = <span class="literal">True</span>)</span><br><span class="line">    out.to_csv(<span class="string">&#x27;./data/20_newsgroup.csv&#x27;</span>, index = <span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">data_to_csv()</span><br></pre></td></tr></table></figure><p>我们需要预处理数据，过滤掉空文本和 Token 数量太多的数据，就像我们之前进行文本分类一样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> openai.embeddings_utils <span class="keyword">import</span> get_embeddings</span><br><span class="line"><span class="keyword">import</span> openai, os, tiktoken, backoff</span><br><span class="line"></span><br><span class="line">openai.api_key = <span class="string">&quot;OPENAI_API_KEY&quot;</span></span><br><span class="line">embedding_model = <span class="string">&quot;text-embedding-ada-002&quot;</span></span><br><span class="line">embedding_encoding = <span class="string">&quot;cl100k_base&quot;</span>  <span class="comment"># this the encoding for text-embedding-ada-002</span></span><br><span class="line">batch_size = <span class="number">2000</span></span><br><span class="line">max_tokens = <span class="number">8000</span>  <span class="comment"># the maximum for text-embedding-ada-002 is 8191</span></span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;20_newsgroup.csv&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of rows before null filtering:&quot;</span>, <span class="built_in">len</span>(df))</span><br><span class="line">df = df[df[<span class="string">&#x27;text&#x27;</span>].isnull() == <span class="literal">False</span>]</span><br><span class="line">encoding = tiktoken.get_encoding(embedding_encoding)</span><br><span class="line"></span><br><span class="line">df[<span class="string">&quot;n_tokens&quot;</span>] = df.text.apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(encoding.encode(x)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of rows before token number filtering:&quot;</span>, <span class="built_in">len</span>(df))</span><br><span class="line">df = df[df.n_tokens &lt;= max_tokens]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of rows data used:&quot;</span>, <span class="built_in">len</span>(df))</span><br></pre></td></tr></table></figure><p>数据结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Number of rows before null filtering: <span class="number">11314</span></span><br><span class="line">Number of rows before token number filtering: <span class="number">11096</span></span><br><span class="line">Number of rows data used: <span class="number">11044</span></span><br></pre></td></tr></table></figure><p>我们使用 Embedding 接口获取文本的 Embedding 向量，并将数据存储为 parquet 文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@backoff.on_exception(<span class="params">backoff.expo, openai.error.RateLimitError</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_embeddings_with_backoff</span>(<span class="params">prompts, engine</span>):</span><br><span class="line">    embeddings = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(prompts), batch_size):</span><br><span class="line">        batch = prompts[i:i+batch_size]</span><br><span class="line">        embeddings += get_embeddings(list_of_text=batch, engine=engine)</span><br><span class="line">    <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line">prompts = df.text.tolist()</span><br><span class="line">prompt_batches = [prompts[i:i+batch_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(prompts), batch_size)]</span><br><span class="line"></span><br><span class="line">embeddings = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> prompt_batches:</span><br><span class="line">    batch_embeddings = get_embeddings_with_backoff(prompts=batch, engine=embedding_model)</span><br><span class="line">    embeddings += batch_embeddings</span><br><span class="line"></span><br><span class="line">df[<span class="string">&quot;embedding&quot;</span>] = embeddings</span><br><span class="line">df.to_parquet(<span class="string">&quot;./data/20_newsgroup_with_embedding.parquet&quot;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>这段代码与前面的文本分类基本相同，不再详细讲解。在机器学习任务中，通常会将一些中间步骤的数据结果保存下来，以防止后续步骤出现错误导致需要从头开始。因此，我们保存了原始数据和 Embedding 处理后的数据。这样，如果需要修改聚类程序，我们就无需再次计算 Embedding。</p><p>接下来，我们可以使用 K-Means 算法进行聚类。由于原始数据来自 20 个不同的新闻组，因此我们将其聚合为 20 个类别，以便查看自动聚类与原始分类是否相似。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line">embedding_df = pd.read_parquet(<span class="string">&quot;./data/20_newsgroup_with_embedding.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line">matrix = np.vstack(embedding_df.embedding.values)</span><br><span class="line">num_of_clusters = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">kmeans = KMeans(n_clusters=num_of_clusters, init=<span class="string">&quot;k-means++&quot;</span>, n_init=<span class="number">10</span>, random_state=<span class="number">42</span>)</span><br><span class="line">kmeans.fit(matrix)</span><br><span class="line">labels = kmeans.labels_</span><br><span class="line">embedding_df[<span class="string">&quot;cluster&quot;</span>] = labels</span><br></pre></td></tr></table></figure><p>我们可以用 NumPy 的 stack 函数把所有的 Embedding 放到一个矩阵里，设置聚合类的数量，然后运行 K-Means 算法的 fit 函数来聚类。聚类后，我们可以统计每个类有多少条各个 newsgroups 分组的数据，并找出排名第一的分组。如果聚类出来的类都是从某一个 newsgroup 分组出来的文章，说明这个类和那个分组的内容相似。我们可以使用这个思路的代码，详情见下方。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 统计每一个cluster的数量</span></span><br><span class="line">new_df = embedding_df.groupby(<span class="string">&#x27;cluster&#x27;</span>)[<span class="string">&#x27;cluster&#x27;</span>].count().reset_index(name = <span class="string">&#x27;count&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计这个cluster里最多的分类的数量</span></span><br><span class="line">title_count = embedding_df.groupby([<span class="string">&#x27;cluster&#x27;</span>, <span class="string">&#x27;title&#x27;</span>]).size().reset_index(name  = <span class="string">&#x27;title_count&#x27;</span>)</span><br><span class="line">first_titles = title_count.groupby(<span class="string">&#x27;cluster&#x27;</span>).apply(<span class="keyword">lambda</span> x: x.nlargest(<span class="number">1</span>, columns=[<span class="string">&#x27;title_count&#x27;</span>]))</span><br><span class="line">first_titles = first_titles.reset_index(drop = <span class="literal">True</span>)</span><br><span class="line">new_df = pd.merge(new_df, first_titles[[<span class="string">&#x27;cluster&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;title_count&#x27;</span>]], on = <span class="string">&#x27;cluster&#x27;</span>, how = <span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">new_df = new_df.rename(columns = &#123;<span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;rank1&#x27;</span>, <span class="string">&#x27;title_count&#x27;</span>:<span class="string">&#x27;rank1_count&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计这个cluster里第二多的分类的数量</span></span><br><span class="line">second_titles = title_count[~title_count[<span class="string">&#x27;title&#x27;</span>].isin(first_titles[<span class="string">&#x27;title&#x27;</span>])]</span><br><span class="line">second_titles = second_titles.groupby(<span class="string">&#x27;cluster&#x27;</span>).apply(<span class="keyword">lambda</span> x: x.nlargest(<span class="number">1</span>, columns=[<span class="string">&#x27;title_count&#x27;</span>]))</span><br><span class="line">second_titles = second_titles.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">new_df = pd.merge(new_df, second_titles[[<span class="string">&#x27;cluster&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;title_count&#x27;</span>]], on=<span class="string">&#x27;cluster&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">new_df = new_df.rename(columns=&#123;<span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;rank2&#x27;</span>, <span class="string">&#x27;title_count&#x27;</span>: <span class="string">&#x27;rank2_count&#x27;</span>&#125;)</span><br><span class="line">new_df.fillna(<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">new_df[<span class="string">&#x27;per_1&#x27;</span>] = (new_df[<span class="string">&#x27;rank1_count&#x27;</span>] / new_df[<span class="string">&#x27;count&#x27;</span>]).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="string">&#x27;&#123;:.2%&#125;&#x27;</span>.<span class="built_in">format</span>(x))</span><br><span class="line">new_df[<span class="string">&#x27;per_1_2&#x27;</span>] = ((new_df[<span class="string">&#x27;rank1_count&#x27;</span>] + new_df[<span class="string">&#x27;rank2_count&#x27;</span>])/ new_df[<span class="string">&#x27;count&#x27;</span>]).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="string">&#x27;&#123;:.2%&#125;&#x27;</span>.<span class="built_in">format</span>(x))</span><br><span class="line"><span class="comment"># new_df[&#x27;first_percentage&#x27;] = (new_df[&#x27;rank1_count&#x27;] / new_df[&#x27;count&#x27;]).map(lambda x: &#x27;&#123;:.2%&#125;&#x27;.format(x))</span></span><br><span class="line"><span class="comment"># 将缺失值替换为 0</span></span><br><span class="line">new_df.fillna(<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">display(new_df)</span><br></pre></td></tr></table></figure><p>我们可以分成几步来写这个代码：</p><ol type="1"><li><p>使用 groupby 对之前的 DataFrame 按照 cluster 聚合，统计每个 cluster 里面数据的数量。</p></li><li><p>使用 groupby，按照 cluster + title 的方式聚合，统计某一个 cluster 里面排名第一的分组名称和数量。</p></li><li><p>使用 x.nlargest 函数，从聚合后的 cluster 中拿到数量排名第一的分组的名称和数量。</p></li><li><p>去掉数据中的排名第一的分组，统计排名第二的分组，并将其与排名第一的分组放在一起进行分析。</p></li></ol><p>输出结果：</p><img src="/AI-can-help-you-summarize-your-content/20230601165935.png" class="" title="img"><p>统计数据表明，大部分聚类结果都能对应到新闻组的某个分类。例如，cluster 0 中有79.86%的数据来自于 misc.forsale 分类。在20个聚类中，有9个聚类中的80%以上数据来自于新闻组中的某个分类。其余的分类，例如cluster 2，前两个分组占了71.39%, 这两个分组的数据都是关于电脑硬件的，一个是关于pc.hardware，另一个是关于mac.hardware。我们只有3个分类的分组比较分散，分别是cluster 6、13和18。因此，可以看出直接使用文本嵌入来进行聚类的效果还不错。</p><h2 id="使用提示语对文本进行总结"><strong><em>*使用提示语对文本进行总结*</em></strong></h2><p>在应用中，我们通常使用的文本数据并没有明显的分组信息。以前，我们需要对聚类出来的类命名，只能通过观察每个类中的文本内容来选择一个适当的名称。对于这里的20个分类数据，我们通常只能每个类选择几篇文本，手动阅读一遍，然后再为其命名。但是，现在有了OpenAI的Completion接口，我们完全可以让AI为聚类出来的类命名。我们可以从每个类中随机选择3-5条文本，然后请AI总结一个名称，然后再选择一两条文本，让AI将其翻译成中文，以查看名称是否合理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">items_per_cluster = <span class="number">10</span></span><br><span class="line">COMPLETIONS_MODEL = <span class="string">&quot;text-davinci-003&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_of_clusters):</span><br><span class="line">    cluster_name = new_df[new_df.cluster == i].iloc[<span class="number">0</span>].rank1</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Cluster <span class="subst">&#123;i&#125;</span>, Rank 1: <span class="subst">&#123;cluster_name&#125;</span>, Theme:&quot;</span>, end=<span class="string">&quot; &quot;</span>)</span><br><span class="line"></span><br><span class="line">    content = <span class="string">&quot;\n&quot;</span>.join(</span><br><span class="line">        embedding_df[embedding_df.cluster == i].text.sample(items_per_cluster, random_state=<span class="number">42</span>).values</span><br><span class="line">    )</span><br><span class="line">    response = openai.Completion.create(</span><br><span class="line">        model=COMPLETIONS_MODEL,</span><br><span class="line">        prompt=<span class="string">f&#x27;&#x27;&#x27;我们想要给下面的内容，分组成有意义的类别，以便我们可以对其进行总结。请根据下面这些内容的共同点，总结一个50个字以内的新闻组的名称。比如 “PC硬件”\n\n内容:\n&quot;&quot;&quot;\n<span class="subst">&#123;content&#125;</span>\n&quot;&quot;&quot;新闻组名称：&#x27;&#x27;&#x27;</span>,</span><br><span class="line">        temperature=<span class="number">0</span>,</span><br><span class="line">        max_tokens=<span class="number">100</span>,</span><br><span class="line">        top_p=<span class="number">1</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(response[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;text&quot;</span>].replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br></pre></td></tr></table></figure><p>我们可以通过 Completion 接口实现需求：</p><ol type="1"><li><p>随机从每个聚类类别中选出10条记录，将其拼接在一起。</p></li><li><p>提供一段提示语告诉AI这些内容来自新闻组，并请求AI为这些新闻组取一个不超过50字的名称。</p></li><li><p>输出结果在下面，包括Cluster、原先排名第一的分组英文和AI给出的新闻组名称。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Cluster <span class="number">0</span>, Rank <span class="number">1</span>: misc.forsale, Theme: 电子产品出售</span><br><span class="line">Cluster <span class="number">1</span>, Rank <span class="number">1</span>: soc.religion.christian, Theme: 宗教信仰的多样性</span><br><span class="line">Cluster <span class="number">2</span>, Rank <span class="number">1</span>: comp.sys.ibm.pc.hardware, Theme: 电脑硬件</span><br><span class="line">Cluster <span class="number">3</span>, Rank <span class="number">1</span>: rec.autos, Theme: 汽车维修与维护</span><br><span class="line">Cluster <span class="number">4</span>, Rank <span class="number">1</span>: talk.politics.mideast, Theme: 中东冲突报道</span><br><span class="line">Cluster <span class="number">5</span>, Rank <span class="number">1</span>: talk.politics.misc, Theme: 主观价值观</span><br><span class="line">Cluster <span class="number">6</span>, Rank <span class="number">1</span>: comp.os.ms-windows.misc, Theme: 科技产品<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Cluster 7, Rank 1: rec.sport.baseball, Theme: 运动员技术分析</span></span><br><span class="line"><span class="string">Cluster 8, Rank 1: sci.space, Theme: 太空探索</span></span><br><span class="line"><span class="string">Cluster 9, Rank 1: sci.electronics, Theme: 电脑硬件和电子设备</span></span><br><span class="line"><span class="string">Cluster 10, Rank 1: talk.politics.guns, Theme: 枪支控制讨论</span></span><br><span class="line"><span class="string">Cluster 11, Rank 1: rec.motorcycles, Theme: 骑行者经验分享</span></span><br><span class="line"><span class="string">Cluster 12, Rank 1: sci.crypt, Theme: 公民权利与加密技术</span></span><br><span class="line"><span class="string">Cluster 13, Rank 1: sci.electronics, Theme: 研究与技术</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p>中文分类名称大部分合理。可以挑选一些文本内容，检查其翻译是否与分类名称一致。翻译代码与上面类似，但有以下几个差别：</p><ol type="1"><li><p>每个分类的抽样数据只有1条，而不是10条。</p></li><li><p>限制文本Token数量不超过100个，以免太占空间。</p></li><li><p>输出内容放大到500字，确保翻译提供足够内容。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">items_per_cluster = <span class="number">1</span></span><br><span class="line">COMPLETIONS_MODEL = <span class="string">&quot;text-davinci-003&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_of_clusters):</span><br><span class="line">    cluster_name = new_df[new_df.cluster == i].iloc[<span class="number">0</span>].rank1</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Cluster <span class="subst">&#123;i&#125;</span>, Rank 1: <span class="subst">&#123;cluster_name&#125;</span>, 抽样翻译:&quot;</span>, end=<span class="string">&quot; &quot;</span>)</span><br><span class="line"></span><br><span class="line">    content = <span class="string">&quot;\n&quot;</span>.join(</span><br><span class="line">        embedding_df[(embedding_df.cluster == i) &amp; (embedding_df.n_tokens &gt; <span class="number">100</span>)].text.sample(items_per_cluster, random_state=<span class="number">42</span>).values</span><br><span class="line">    )</span><br><span class="line">    response = openai.Completion.create(</span><br><span class="line">        model=COMPLETIONS_MODEL,</span><br><span class="line">        prompt=<span class="string">f&#x27;&#x27;&#x27;请把下面的内容翻译成中文\n\n内容:\n&quot;&quot;&quot;\n<span class="subst">&#123;content&#125;</span>\n&quot;&quot;&quot;翻译：&#x27;&#x27;&#x27;</span>,</span><br><span class="line">        temperature=<span class="number">0</span>,</span><br><span class="line">        max_tokens=<span class="number">2000</span>,</span><br><span class="line">        top_p=<span class="number">1</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(response[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;text&quot;</span>].replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Cluster <span class="number">0</span>, Rank <span class="number">1</span>: sci.electronics, 抽样翻译: 我开始研究一些在相当嘈杂的环境中，以及在相当远的距离上传输串行数据的设备，我看到了各种保护RS232收发器（以及其他电路）免受串行线上瞬变的方案。我想知道最佳的做法是什么？这有多必要？据我所知，保护是必要的，特别是如果你计划将电缆路由到一个未知的环境（不受控制）。像信号线和电源线之间的意外短路，甚至闪电等事情都是非常可能的，我不认为你会喜欢看到你的电脑烟消云散的景象！（即使以太网卡也受到保护。我看过我的PC中的一个连接器，它由气体放电管保护！）但是，如果你计划将串行电缆用于内部路由（即在受控环境中），则不需要它们应该是相当安全的。建议：查看RS数据手册。他们有几个RS232收发器，具有过压保护。其中包括LT1080，LT1081和MAX250和MAX251。Maxim应该是绝缘的，但仍需要光耦合器才能工作（不要问我为什么。我以前从未使用过它们。）另一种选择是RS232电涌保护器。 RS目录中列出了两个。如果您需要额外的信息（即库存号），请给我发电子邮件。</span><br><span class="line">Cluster <span class="number">1</span>, Rank <span class="number">1</span>: comp.sys.ibm.pc.hardware, 抽样翻译: 如果您购买带CD配置的Centris <span class="number">650</span>，您将获得一台带有内置数学协处理器支持的68RC040处理器的Mac。我的理解是“可选fpu”是指您可以选择购买没有FPU的Centris <span class="number">650</span> <span class="number">4</span>/<span class="number">80</span>或其他带有FPU的配置。Apple不提供从非FPU系统升级为FPU系统的服务。而且，目前尚不清楚非FPU系统（68LC040）上的<span class="string">&#x27;040处理器是否可以由另一家供应商提供的68RC040替换。苹果公司曾经发出一份备忘录，指出只有非FPU 68LC040处理器的Centris 610无法升级为支持FPU - 根据苹果的备忘录，两种芯片的引脚配置似乎不匹配，因此无法互换（再次，根据苹果的备忘录）。希望有所帮助。</span></span><br><span class="line"><span class="string">Cluster 2, Rank 1: talk.politics.misc, 抽样翻译: 以下内容可以在某个FTP存档中找到，我在这封“大哥大”的恶魔般的备忘录中插入了我的评论：看！这显然是禁止我们自己的螺纹规格的第一步。如果不以牙还牙地抵制这种疯狂，使用我们无畏领袖“慷慨”定义的螺纹以外的螺纹将是一种犯罪。废话！我说！ANSI标准螺纹会有微妙的弱点，让他们的特工更容易拆卸我们的汽车，导致我们的消声器在不合适的时候掉下来。哈！“相信我们”看到了吗？！这是第一步。很明显，我们必须团结起来，写信给你的国会议员！使用“相当好的螺纹”，而不是这种恶魔灵感的ANSI垃圾。保护你使用任何螺纹的宪法权利。游击螺纹活动必须成为日常事务。抵制通用汽车，并用STZ螺纹联合公司的螺丝自己建造汽车。螺丝你，比尔·克林顿！你和你的极权主义暴徒！</span></span><br><span class="line"><span class="string">Cluster 3, Rank 1: rec.sport.baseball, 抽样翻译: 我也是... RBI 是一个毫无价值的统计数据。当然，偷垒也是一样，因为有时候有些跑者会在一个可能会跑的球员前面。当然，投手会根据不同的垒位投出不同的球，所以击球率、长打率和击球率也没有用了。嗯... 我猜全垒打也不算了吧。我的意思是？RBI 可能不是一个完美的统计数据，但没有什么是完美的。没有任何统计数据（或缺乏）能告诉我没有关键时刻的击球手。也许没有任何统计数据能告诉我，但有些人是... 我只知道！！ 8)</span></span><br><span class="line"><span class="string">Cluster 4, Rank 1: comp.os.ms-windows.misc, 抽样翻译: 几个月前，我遇到了严重的内存问题，在Windows中出现了“停止错误消息，检查奇偶校验”。我运行了QA / PLUS，Check It，Diagnose以及几个共享软件内存检查器。我的系统中有8兆SIMM。这些商店购买/共享软件诊断工具要么运行正常没有错误，要么在我无法放置在内存芯片上的某个地址上发现错误。出于绝望，我想出了（现在已删除）步骤来找到坏的内存芯片。我发现了2个（道德：永远不要买背面印有“不适用于敏感或关键应用”的内存）。无论如何，我确实使用8兆芯片中的4个组合过滤掉了所有坏的内存芯片，并创建了一个RAM驱动器来进行测试。虽然它没有缓解我在Windows中的奇偶校验错误问题，但我确实以这种方式找到了坏的内存芯片。它从未失败过，为我找到坏的芯片，而商业/共享软件总是让我失望，要么没有发现错误，要么指向我不知道是哪块芯片的地址。PS：今天我的打字水平太糟糕了，我不想再用这个行编辑器了。-大卫</span></span><br><span class="line"><span class="string">Cluster 5, Rank 1: sci.crypt, 抽样翻译: 警察/联邦政府不需要获得您的私钥才能监听蜂窝电话交谈。加密不是端到端的，而是手机到基站 - 它必须是这样，以便蜂窝用户和固定设备可以相互交谈。对于蜂窝到蜂窝的通话，传输在基站解密，传递到另一个基站并重新加密。警察/联邦政府可以听取未加密的通话，前提是他们获得搜查蜂窝提供商设备的令牌。想要破解系统的唯一原因是他们可以在不获得令牌的情况下监听。但是，也许Clipper系统是安全的，他们确实需要一个令牌才能从保管处获取密钥，然后才能使用扫描仪进行监听（参见上面 - 他们不必*必须*走这条路）。我有我的疑虑，但即使是真的，一旦他们拥有密钥，他们将*永远*不再需要令牌来拨打该手机，随时随地都可以监听。 “嗯，法官，看来他不是毒贩，所以我们自然会停止监听……”那对英国保罗来说是真的，但我相信他们正在讨论如何使用这个芯片构建端到端加密电话。这*不是*蜂窝（尽管它当然可以按照您的建议在那里使用）</span></span><br><span class="line"><span class="string">Cluster 6, Rank 1: comp.graphics, 抽样翻译: 我需要有关Display PostScript strokeadjust功能的信息。此功能可调整线的端点，以使在低分辨率设备上显示的线看起来更好。PostScript文献在一定程度上解释了这个过程。他们还给出了一个如何在没有strokeadjust的PostScript环境中“模拟”strokeadjust的例子。建议的模拟是使用以下公式修改线的端点坐标：新坐标=（四舍五入（旧坐标-0.25））+0.25这样，我们最终得到所有坐标以“.25”结尾。从阅读中，我认为他们实际上可能做的是：新坐标=（（截断（旧坐标*2））/2）+0.25这样，所有坐标都以“0.25”或“0.75”结尾，取决于哪个更接近。通过与Display PostScript进行实际比较，我发现DPS实际上不是用这两种方法。由于我喜欢DPS的结果比我的东西看起来更好，我想知道是否有人知道DPS是如何做的，愿意/能够告诉我。谢谢！</span></span><br><span class="line"><span class="string">Cluster 7, Rank 1: sci.space, 抽样翻译: 我认为，如果有奖励等，应该有以下“类别”：大公司小公司/公司（根据报告收入？）大政府（国民生产总值等）小政府（或政治影响力或国民生产总值？）大组织（行星协会？等）小组织（许多小型组织..）组织事务可能必须是非营利性的，或者是？当然，这意味着奖金可能会增加。较大的获得更多或？基本上使奖金（总奖金）60亿美元，分配给班级获胜者..更公平？必须建立一个单独的组织来监督活动，裁判等，并监视安全违规（或者不，如果人们想冒自己的生命危险让他们去做？）。</span></span><br><span class="line"><span class="string">Cluster 8, Rank 1: talk.politics.guns, 抽样翻译: 我当时现场观看，并且已经重新观看了几次，从新闻视角来看，只有一个起火点可见，坦克在迎风侧打入，风把火势席卷整个干燥的木结构，几分钟内就烧毁了。受30英里每小时的大风和直升机的影响，火势迅速蔓延。如果有其他火源，它们不可见，也不需要，我观察到的火势就是这样。所有的见证者都是从联邦调查局那里拿工资的。是的，媒体毫无疑问地支持联邦调查局的版本。真可悲。吉姆--jmd@handheld.com</span></span><br><span class="line"><span class="string">Cluster 9, Rank 1: rec.motorcycles, 抽样翻译: 1.在前苏联有一个类似的想法，需要多少军人来安装一个新的电灯？答案是九个：一个坐在桌子上拿着灯，四个拿着桌子转动它，另外四个则以相反的方向绕着桌子跑，以免让第一个感到不舒服（被转动时）。可惜，它缺乏尼克的消息中的那种黑色幽默。2.在我看来，签名应该是这样的：/       _                     __        /  ./_______/_/_______________    /________ /____//___ /      _                  /特使Albert</span></span><br><span class="line"><span class="string">Cluster 10, Rank 1: rec.sport.hockey, 抽样翻译: 嗨，曲棍球迷们。大家好！上周日，在盐湖城，当地的ABC电视台决定不播放曲棍球比赛。节目主管真是个混蛋！不管怎样，我有一个卫星天线，几个曲棍球朋友邀请自己过来看本周日（4月25日）的比赛，但我找不到正确的比赛时间。对于卡尔加里和洛杉矶的比赛，我有时间显示从MDT 11:00到MDT 5:00。现在，我甚至不确定本周日会有哪些比赛，因为ABC搞乱了时间表。我想我应该能从天线上收到三场比赛（MDT 11:00，2:00和5:30），但我不确定。如果有人有时间表，请给我发电子邮件。正如你所看到的，我必须收听rec.sport.hockey，有时很难得到链接。提前谢谢非常感谢附言：英语或法语都可以。罗兰·贝胡宁behunin@oodis01.af.milbehunin@oodis01.hill.af.mil</span></span><br><span class="line"><span class="string">Cluster 11, Rank 1: comp.windows.x, 抽样翻译: 由于xterminals没有NeWS服务器，它们具有速度的本质，因为它们的功能有限：一旦添加NeWS和其他一切，就拥有了工作站。 我听说有一些变通方法，其中一种涉及perl脚本。 我们正试图用不需要NeWS（如ghostview）的程序替换需要NeWS（如pageview）的程序。也许其他人可以详细说明perl变通方法； 我没有个人经验。</span></span><br><span class="line"><span class="string">Cluster 12, Rank 1: talk.politics.mideast, 抽样翻译: 游击队和恐怖分子组织使用的一种“可靠的”方法是：在当地民众中间进行行动，从而迫使对立的“国家”可能会伤害无辜的平民，以搜索，或者为了避免平民死亡而放弃搜索。当然，利用人口作为掩护的人也应该为把无辜的平民拖入危险境地负责。你是在暗示，当游击队利用人口作为掩护时，以色列应该完全放弃吗？所以......最简单的方法是利用无辜者作为护盾，并希望对方尊重无辜的生命？你该死的对，以色列坚持一些“非军事化”或“缓冲”区。它已经忍受了太多年来阿拉伯国家领土发动的攻击，并看到这些国家什么也没做。以色列决定停止这种行动的唯一方法是自己去做，这并不奇怪。什么？所以关于以色列来自邻近阿拉伯国家的攻击可以重新开始？虽然我也希望这种情况发生，但只有当阿拉伯国家表明他们准备承担责任和责任来阻止以色列来自他们领土的游击袭击时，这才会发生。他们必须证明（或提供一些“保证”），以色列不会接受他们的“话”-不要求他们对“反以色列游击队”的容忍态度。天哪，布拉德。你到底是怎么想到联合国部队可以阻止任何事情的？他们只是因为那个国家允许他们进入而驻扎在那里。它可以要求他们随时离开；就像纳赛尔在1956年和1967年所做的那样。既然有这种“限制”，我不认为以色列会更舒服。如果没有阿拉伯国家对和平的真正承诺，以及其他各方提供的具体（而不是智力或政治上的行动）“保证”，联合国对以色列毫无用处（但也许可以作为一个“诡计”？）。也许你知道，对大多数社区来说，有一种感觉，即“我们中的许多人与那些攻击我们的人作斗争而死，要比少数人默默地接受命运而死要好”。但是，如果你要求以色列看到受伤少的意义，我建议你也同样应用于巴勒斯坦，阿拉伯和伊斯兰团体。从以色列的角度来看，“让步”一无所获......除了意识到它已经放弃了“某些东西”，现在只能*希望*对方决定也这样做。话可以通过仅仅这样来收回；要“收回”有形的物品（土地，土地控制权），需要你所说的以色列应该避免的行动。以色列忍受阿拉伯国家领土发动的攻击已有几十年之久，直到通过其对黎巴嫩的入侵才基本上停止了这种现实。整个基础就是你上面所说的：1）以色列会对这些攻击表示愤怒，向有关阿拉伯国家抗议，2）那个国家立即无视整个事件，确信它不能为“私人组织”所犯的罪行负责，3）以色列</span></span><br><span class="line"><span class="string">Cluster 13, Rank 1: sci.med, 抽样翻译: Nutrasweet是一种合成甜味剂，比糖甜几千倍。有些人担心人体分解Nutrasweet时产生的化学物质。据认为，它会形成甲醛，并且已知会在人体排除物质的降解途径中产生甲醇。真正的问题是甲醇和甲醛的水平是否足够高，以致于造成重大损害，因为它们对活细胞都有毒性。我只能说我不会摄入它。苯丙氨酸不是你要担心的。它是一种氨基酸，每个人都会使用少量的苯丙氨酸来进行蛋白质合成。有些人患有苯丙酮尿症，他们缺少降解这种化合物并排除体外的酶。对他们来说，它会在体内积累，而且在高水平下，这对生长中的神经细胞有毒性。因此，它只是儿童（直到10岁左右）或患有这种疾病的妇女的主要问题。它曾经是婴儿脑损伤的主要原因，但现在可以在出生时轻松检测到，然后只需要避免儿童或怀孕时摄入苯丙氨酸即可。</span></span><br><span class="line"><span class="string">Cluster 14, Rank 1: comp.os.ms-windows.misc, 抽样翻译:</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p>我们选取的内容和AI总结的标题非常相似。这个过程是否让你想起Completion接口的强大之处呢？只需给出指令，它就能按照我们的要求提供合理的结果。</p><blockquote><p>在这个例子中，因为超出Token限制，导致后面没有完全执行完，报错了。大家看到我的源文件可以看到这部分报错。</p></blockquote><h3 id="使用文本摘要支持无限轮数的聊天">使用文本摘要支持无限轮数的聊天</h3><p>在前一课中，我们思考了一个问题：如果我们想让 AI 在对话轮数不断增多的情况下尽可能多地记住过去几轮的上下文，该怎么办？有一个方法是，让 AI 对过去几轮的内容进行总结，得出一个 100 字左右的概述。然后，将这个概述作为对话的提示信息，告诉 AI 这是聊天的背景，然后继续新一轮的对话。</p><p>让我们在这里试试看。</p><ol type="1"><li><p>我们拿上一课的一段聊天记录，让 AI 先总结一下，可以看到 AI 总结了 User 和 Assistant 之间聊了什么。</p></li><li><ol type="1"><li><p>```python history = """User : 你是谁？ Assistant : 我是一个AI语言模型，专门用于回答各种问题，包括法律问题。</p><p>User : 请问什么是正当防卫？ Assistant : 正当防卫是指在必要时为了保护自己、他人或者国家公共利益而采取的防御行为。在我国法律中，对于正当防卫的情况，法律规定可以免除或减轻犯罪责任。但是，正当防卫也有限制，必须符合法律规定的情形和条件，否则可能构成违法犯罪行为。</p><p>User : 那防卫过当呢？ Assistant : 防卫过当是指在正当防卫行为中，因过度防卫而超出了必要限度，对袭击者造成了严重伤害或者死亡的行为。在我国法律中，防卫过当是不被允许的，因为它已经超出了必要的防卫范围，可能构成过失犯罪或者故意犯罪。如果行为构成犯罪，防卫人需要承担相应的法律责任。 """</p><p>def summarize(text, max_tokens=200): response = openai.Completion.create( model=COMPLETIONS_MODEL, prompt=text + "：", max_tokens=max_tokens, ) return response["choices"][0]["text"]</p><p>summarized = summarize(history) print(summarized) <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2. 输出结果：</span><br><span class="line"></span><br><span class="line">3. ```python</span><br><span class="line">   User和Assistant聊了关于正当防卫和防卫过当的话题，包括了正当防卫是指什么，它有什么样的限制，防卫过当是什么，以及如果防卫行为构成犯罪的话防卫人会承担相应的法律责任。</span><br></pre></td></tr></table></figure></p></li></ol></li><li><p>接着，我们新建一个对话，提示语里加上总结内容，然后让 AI 继续对话。</p></li><li><ol type="1"><li><p>```python prompt = summarized + "，继续对话：" conversation = Conversation(prompt, 5)</p><p>question = "那恶意挑衅呢？" answer = conversation.ask(question) print("User : %s" % question) print("Assistant : %s" % answer) <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2. &gt; 注意，如果你是在 Notebook 里面执行的话，你需要把上一讲的 Conversation 类的代码复制过来先执行一下。这里，我们启动了一个新的对话对象，将之前的几轮对话的内容总结了一下，放在 Prompt 的最前面，然后让 AI 根据已经聊了的内容，继续往下聊。</span><br><span class="line"></span><br><span class="line">3. ```python</span><br><span class="line">   # 上一节中定义的Conversation类</span><br><span class="line">   class Conversation:</span><br><span class="line">       def __init__(self, prompt, num_of_round):</span><br><span class="line">           self.prompt = prompt</span><br><span class="line">           self.num_of_round = num_of_round</span><br><span class="line">           self.messages = []</span><br><span class="line">           self.messages.append(&#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: self.prompt&#125;)</span><br><span class="line">   </span><br><span class="line">       def ask(self, question):</span><br><span class="line">           try:</span><br><span class="line">               self.messages.append( &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: question&#125;)</span><br><span class="line">               response = openai.ChatCompletion.create(</span><br><span class="line">                   model=&quot;gpt-3.5-turbo&quot;,</span><br><span class="line">                   messages=self.messages,</span><br><span class="line">                   temperature=0.5,</span><br><span class="line">                   max_tokens=2048,</span><br><span class="line">                   top_p=1,</span><br><span class="line">               )</span><br><span class="line">           except Exception as e:</span><br><span class="line">               print(e)</span><br><span class="line">               return e</span><br><span class="line">   </span><br><span class="line">           message = response[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]</span><br><span class="line">           self.messages.append(&#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: message&#125;)</span><br><span class="line">           </span><br><span class="line">           if len(self.messages) &gt; self.num_of_round*2 + 1:</span><br><span class="line">               del self.messages[1:3]</span><br><span class="line">           return message</span><br></pre></td></tr></table></figure></p></li></ol></li><li><p>当我们直接问，“那恶意挑衅呢？”，AI 会正确回答出恶意挑衅的法律解释，并结合了上下文告诉你相关性。</p></li></ol><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">User : 那恶意挑衅呢？</span><br><span class="line">Assistant : 恶意挑衅是指他人以言语、行为等方式故意挑衅、侮辱或者侵犯他人人身权利，如果被挑衅者出于自卫而采取防卫行为，且防卫行为符合正当防卫的要求，那么这种防卫行为也是合法的。但是如果被挑衅者的防卫行为明显超出了正当防卫的必要性和适度性，那么就可能构成防卫过当，需要承担相应的法律责任。</span><br></pre></td></tr></table></figure><p>如果我们只让 AI 对话而没有加上 AI 总结之前的对话，那么它只能跟你闲扯些其他话题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conversation = Conversation(<span class="string">&quot;请你根据已经聊了的内容，继续对话：&quot;</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">question = <span class="string">&quot;那恶意挑衅呢？&quot;</span></span><br><span class="line">answer = conversation.ask(question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;User : %s&quot;</span> % question)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Assistant : %s\n&quot;</span> % answer)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">User : 那恶意挑衅呢？</span><br><span class="line">Assistant : 恶意挑衅是指有人故意挑衅、侮辱或者攻击别人，这种行为是不应该被容忍的。如果我们遇到这种情况，我们应该要保持冷静，不要过度激动或者反击，可以采取一些有效的措施来应对，比如报警或者向相关机构举报。另外，我们也可以通过教育和宣传来提高公众对于恶意挑衅的认识，让更多人知道这种行为的危害性和不可取性。</span><br></pre></td></tr></table></figure><p>如果没有总结，AI 将没头没脑地和你聊天，提醒你不应容忍恶意挑衅。</p><h2 id="小结">小结</h2><p>本节课我们实验了通过 Embedding 拿到的向量进行文本聚类的技巧。利用 Completion 接口可以帮我们总结内容的能力，我们可以直接给分类取了一个名字，避免了繁琐的人工命名，分类效果也很好。</p><p>这种技巧同样适用于多轮的长对话中。我们可以将历史对话让 AI 总结为一小段文本放到提示语里面，这样即可让 AI 记住过去的对话内容，又不会超出模型支持的 Token 数量。</p><h2 id="课后练习">课后练习</h2><ol type="1"><li><p>在体验 GPT 系列模型的时候，你觉得哪些提示语特别有用？欢迎分享自己的体验。</p></li><li><p>在文本聚类中，有三个聚合出来的类和原先的分组没有很明显的对应关系。你能不能写一些代码来研究数据，找出其中的原因呢？</p></li></ol><p>期待在评论区看到你的思考！也欢迎你把这节课分享给感兴趣的朋友，我们下一讲见！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hi， 我是茶桁。&lt;/p&gt;
&lt;p&gt;在&lt;a href=&quot;https://www.hivan.me/Quickly%20build%20an%20AI%20application&quot;&gt;上一节中&lt;/a&gt;，我们介绍了如何使用最新的ChatGPT API，注册HuggingFace账户，并将我们的聊天机器人部署出去。在这个过程中，我们学习了实际的应用开发过程，使你对聊天机器人的开发有了充足的体验。在这一讲中，我们将探讨OpenAI的各种接口提供的能力，以更深入地了解这些接口。我们将分别介绍如何利用嵌入（Embedding）进行文本聚类，并使用提示语（Prompt）对文本进行总结。此外，我们还将介绍其他的接口能力，如语言模型和自然语言生成，以帮助您更好地理解和利用OpenAI的各种功能。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>06 快速建立一个AI应用</title>
    <link href="https://hivan.me/Quickly-build-an-AI-application/"/>
    <id>https://hivan.me/Quickly-build-an-AI-application/</id>
    <published>2023-05-13T08:56:19.000Z</published>
    <updated>2023-06-01T13:26:11.174Z</updated>
    
    <content type="html"><![CDATA[<p>Hi，我是茶桁。</p><p>在过去的两讲中，我们已经使用 OpenAI 提供的 Embedding 接口完成了文本分类的功能。现在，我们回到 Completion 接口，这一讲将带你更深入地了解该接口的使用。除此之外，我们还将快速搭建一个有界面的聊天机器人，这将让你更好地理解 Completion 接口的应用场景。在这个过程中，你将第一次使用 HuggingFace 这个平台，它是目前最流行的深度学习模型社区。通过 HuggingFace，你可以下载到最新的开源模型，查看其他人提供的示例代码，并参与到社区的交流中。</p><span id="more"></span><h2 id="价廉高质的chatgpt">价廉高质的ChatGPT</h2><p>我们在<a href="https://www.hivan.me/Let&#39;s%20Build%20a%20Chatbot">第三讲</a>里介绍了Completion接口，并且通过它实现了一个聊天机器人的功能。在那个时候，我们采用的是自己将整个对话拼接起来，将整个上下文都发送给 OpenAI 的 Completion API 的方式。不过，在 3 月 2 日，因为 ChatGPT 的火热，OpenAI 放出了一个直接可以进行对话聊天的接口。这个接口叫做 ChatCompletion，对应的模型叫做 gpt-3.5-turbo，不但用起来更容易了，速度还快，而且价格也是我们之前使用的 text-davinci-003 的十分之一，可谓是物美价廉了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note: you need to be using OpenAI Python v0.27.0 for the code below to work</span></span><br><span class="line"></span><br><span class="line">openai.ChatCompletion.create(</span><br><span class="line">  model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>,</span><br><span class="line">  messages=[</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a helpful assistant.&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Who won the world series in 2020?&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;The Los Angeles Dodgers won the World Series in 2020.&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Where was it played?&quot;</span>&#125;</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><blockquote><p>这里需要注意，我们运行这段代码需要的是OpenAI Python v0.27.0， 或者以上版本。如果你的版本过低，可能会报错找不到ChatCompletion这个方法，这个时候请重新更新一下你的OpenAI API库 <code>conda install -c conda-forge openai</code></p></blockquote><blockquote><p><a href="https://platform.openai.com/docs/guides/chat">这个链接</a>内，你可以看到官方的接口调用示例</p></blockquote><p>在 OpenAI 的官方文档里，可以看到这个接口也非常简单。实际上，在使用 OpenAI API 进行自然语言处理时，你只需要传入一个包括多个元素的数组。每个数组元素都包含两个字段： <code>role</code> 和 <code>content</code>。其中，role 字段具有三种选择，分别为 <code>system</code>、 <code>user</code> 和 <code>assistant</code>。</p><p>当 <code>role</code> 字段的取值为 <code>system</code> 时， <code>content</code> 字段中的内容表示我们向 AI 发送的指令，也就是告诉 AI 如何回答用户的问题。例如，如果我们希望 AI 通过中文回答用户的问题，我们就可以在 content 字段中写入“你是一个只会用中文回答问题的助理”。这样，即使用户的问题是用英语提出的，AI 的回答也会是中文的。</p><p>当 <code>role</code> 字段的取值为 <code>user</code> 或 <code>assistant</code> 时， <code>content</code> 字段中的内容则表示用户和 AI 之间的对话内容。与我们在<a href="https://www.hivan.me/Let&#39;s%20Build%20a%20Chatbot">第 03 讲</a>中制作的聊天机器人类似，你需要将对话历史记录一起发送给 OpenAI 的接口，这样它才能理解整个对话的上下文，并作出更加准确的回答。</p><p>有了这个接口，我们就可以很方便地封装一个聊天机器人。聊天机器人能够在许多领域中发挥作用，比如在客服领域中，它可以帮助客户解决问题。在社交娱乐领域中，聊天机器人可以模拟真实的人类交流，从而为人们提供更好的体验。此外，我们还可以通过加入自然语言处理算法，让聊天机器人更加聪明、更加智能化。下面是我封装的聊天机器人的代码，一起来看一看吧。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">openai.api_key = <span class="string">&#x27;OPENAI_API_KEY&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conversation</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, prompt, num_of_round</span>):</span><br><span class="line">        self.prompt = prompt</span><br><span class="line">        self.num_of_round = num_of_round</span><br><span class="line">        self.messages = []</span><br><span class="line">        self.messages.append(&#123;<span class="string">&#x27;role&#x27;</span>:<span class="string">&#x27;system&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: self.prompt&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ask</span>(<span class="params">self, question</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.messages.append(&#123;<span class="string">&#x27;role&#x27;</span>:<span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: question&#125;)</span><br><span class="line">            response = openai.ChatCompletion.create(</span><br><span class="line">                model=<span class="string">&#x27;gpt-3.5-turbo&#x27;</span>,</span><br><span class="line">                messages = self.messages,</span><br><span class="line">                temperature = <span class="number">0.5</span>,</span><br><span class="line">                max_tokens = <span class="number">2048</span>,</span><br><span class="line">                top_p = <span class="number">1</span>,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(e)</span><br><span class="line">            <span class="keyword">return</span> e</span><br><span class="line">        </span><br><span class="line">        message  = response[<span class="string">&#x27;choices&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>]</span><br><span class="line">        self.messages.append(&#123;<span class="string">&#x27;role&#x27;</span>:<span class="string">&#x27;assistant&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: message&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.messages) &gt; self.num_of_round*<span class="number">2</span> + <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">del</span> self.messages[<span class="number">1</span>:<span class="number">3</span>] <span class="comment"># Remove the first round conversation left</span></span><br><span class="line">        <span class="keyword">return</span> message</span><br></pre></td></tr></table></figure><p>我们为了更好地使用这个聊天机器人，封装了一个 Conversation 类，构造函数 init 接受两个参数。第一个参数是 prompt，表示我们对这个聊天机器人的指令，它是系统的 content。第二个参数是 num_of_round，代表每次向 ChatGPT 发起请求时，保留过去几轮会话。这个构造函数的作用是初始化 ChatGPT 对象和对话数组。</p><p>我们的 Conversation 类只有一个 ask 函数，输入是一个 string 类型的 question，返回结果也是 string 类型的一条 message。每次调用 ask 函数，都会向 ChatGPT 发起一个请求。在这个请求中，我们会把最新的问题拼接到整个对话数组的最后，得到 ChatGPT 的回答之后再把回答拼接上去。如果回答完之后，发现会话的轮数超过我们设置的 num_of_round，我们就去掉最前面的一轮会话。</p><p>因此，我们的 Conversation 类可以帮助我们更好地管理整个聊天过程，并且可以根据我们的需求来定制保留会话轮数的数量。现在，我们可以尝试使用这个 Conversation 类来和聊天机器人进行交互，享受无限畅聊的乐趣。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:</span></span><br><span class="line"><span class="string">1. 你的回答必须是中文</span></span><br><span class="line"><span class="string">2. 回答限制在100个字以</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">conv1 = Conversation(prompt, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">question1 = <span class="string">&quot;你是谁？&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;User : %s&quot;</span> % question1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Assistant : %s\n&quot;</span> % conv1.ask(question1))</span><br><span class="line"></span><br><span class="line">question2 = <span class="string">&quot;请问什么是正当防卫？&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;User : %s&quot;</span> % question2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Assistant : %s\n&quot;</span> % conv1.ask(question2))</span><br><span class="line"></span><br><span class="line">question3 = <span class="string">&quot;那防卫过当呢？&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;User : %s&quot;</span> % question3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Assistant : %s\n&quot;</span> % conv1.ask(question3))</span><br></pre></td></tr></table></figure><p>我们给 ChatGPT 下达了指令，让它扮演一位中国律师，使用中文回答问题，每个回答不超过100个字，并设置AI只记住过去三轮对话。</p><p>接着，我们按顺序问了三个问题：“您是谁？”、“请问什么是正当防卫？”以及“那防卫过当呢？”。</p><p>可以看到，它在回答中表示自己能够回答法律问题，每个回答都在100个字以内。</p><p>此外，当我们问它“那防卫过当呢？”时，它也确实记住了上下文，知道我们正在问法律问题，而不是其他信息。为了保证回答充分，我们可以补充一些细节和背景信息，如何定义正当防卫以及何时认为防卫过当等。这些信息将有助于更全面地回答用户的问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">User : 你是谁？</span><br><span class="line">Assistant : 我是一个AI语言模型，专门用于回答各种问题，包括法律问题。</span><br><span class="line"></span><br><span class="line">User : 请问什么是正当防卫？</span><br><span class="line">Assistant : 正当防卫是指在必要时为了保护自己、他人或者国家公共利益而采取的防御行为。在我国法律中，对于正当防卫的情况，法律规定可以免除或减轻犯罪责任。但是，正当防卫也有限制，必须符合法律规定的情形和条件，否则可能构成违法犯罪行为。</span><br><span class="line"></span><br><span class="line">User : 那防卫过当呢？</span><br><span class="line">Assistant : 防卫过当是指在正当防卫行为中，因过度防卫而超出了必要限度，对袭击者造成了严重伤害或者死亡的行为。在我国法律中，防卫过当是不被允许的，因为它已经超出了必要的防卫范围，可能构成过失犯罪或者故意犯罪。如果行为构成犯罪，防卫人需要承担相应的法律责任。</span><br></pre></td></tr></table></figure><p>在问完了 3 个问题之后，我们又问了第四个问题，也就是我们最初问它的问题是什么。因为记录了过去 1-3 轮的对话，所以它能正确地回答出我们最初问的是"你是谁"。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">question4 = <span class="string">&quot;我问你的第一个问题是什么？&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;User : %s&quot;</span> % question4)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Assistant : %s\n&quot;</span> % conv1.ask(question4))</span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">User : 我问你的第一个问题是什么？</span><br><span class="line">Assistant : 你问的第一个问题是：<span class="string">&quot;请问什么是正当防卫？&quot;</span></span><br></pre></td></tr></table></figure><p>可以看到，回答我们的是“请问什么是正当防卫？”， 这个问题对于我们提问来说似乎是第二个，先不管这个，继续问问看，看看我们设置的次数限制有没有效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">question5 = <span class="string">&quot;我问你的第一个问题是什么？&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;User : %s&quot;</span> % question5)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Assistant : %s\n&quot;</span> % conv1.ask(question5))</span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">User : 我问你的第一个问题是什么？</span><br><span class="line">Assistant : 你问的第一个问题是：<span class="string">&quot;那防卫过当呢？&quot;</span></span><br></pre></td></tr></table></figure><p>似乎是剔除了一个最前方的问题，原因是我在问道这个问题的时候，也算是一个新问题。上一轮已经是第四轮了，而我们设置记住的 num_of_round 是 3。在上一轮的问题回答完了之后，AI记录的第一轮的关于"请问什么是正当防卫？"的问答，被我们从 ChatGPT 的对话历史里去掉了。所以这个时候，它会告诉我们，第一个问题是"那防卫过当呢？"。</p><h2 id="聊天的成本">聊天的成本</h2><p>无论是在<a href="https://www.hivan.me/Let&#39;s%20Build%20a%20Chatbot">第 03 讲</a>里，还是这一讲里，我们每次都要发送一大段之前的聊天记录给到 OpenAI。这是由 OpenAI 的 GPT-3 系列的大语言模型的原理所决定的。GPT-3 系列的模型的使用流程非常简单，只需要提供一段文字，它就可以续写下一段内容。因为 OpenAI 为了能够方便地为所有人提供服务，没有在服务器端维护整个对话过程自己去拼接，所以就需要由你来拼接。</p><p>实际上，在 ChatGPT 的接口中，虽然把对话分成了一个数组，但最终发送给模型的仍然是拼接到一起的字符串。为了方便用户使用，OpenAI 提供了一个名为 <a href="https://github.com/openai/openai-python/blob/main/chatml.md">ChatML</a> 的格式，它是 ChatGPT 的 API 的底层实现。OpenAI 根据一个定义好特定分隔符的格式，将提供的多轮对话内容拼接在一起，然后提交给 gpt-3.5-turbo 这个模型。这样，用户就可以轻松地使用 ChatGPT 接口进行对话，而不必担心对话过程中的拼接问题。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;|im_start|&gt;system</span><br><span class="line">You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.</span><br><span class="line">Knowledge cutoff: 2021-09-01</span><br><span class="line">Current date: 2023-03-01&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">How are you&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">I am doing well!&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">How are you now?&lt;|im_end|&gt;</span><br></pre></td></tr></table></figure><blockquote><p>注：chatml 的文档里，你可以看到你的对话，就是通过 &lt;|im_start|&gt;system|user|assistant、&lt;|im_end|&gt; 这些分隔符分割拼装的字符串。底层仍然是一个内容续写的大语言模型。</p></blockquote><p>ChatGPT 是一个方便易用的对话模型，但是在使用过程中需要注意一些细节。特别是在需要传达大量上下文信息的情况下，可能需要消耗更多的费用。OpenAI 根据模型处理的 Token 数量来计算费用，但需要注意的是，这种费用是"双向收费"的。也就是说，费用是根据发送给 ChatGPT 的上下文和 ChatGPT 返回的内容所包含的 Token 数量总和来计算的。</p><p>从模型原理上来看，这种计费方式是合理的。因为每一个 Token，无论是发送给 ChatGPT 的还是 ChatGPT 返回的，都需要通过 GPU 或 CPU 进行计算。因此，你发送的上下文越长，ChatGPT 消耗的资源也会越多。但是在实际使用中，你可能会感到疑惑。比如，你进行了 10 轮对话，一共发送了 1000 个 Token，那么你可能会认为只需要支付 1000 个 Token 的费用。然而实际上，由于每一轮对话都需要发送之前的上下文，因此第二轮需要 200 个 Token，第三轮需要 300 个 Token，以此类推。因此，10 轮对话所需要的 Token 数量是 5500，而不是之前估算的 1000。</p><p>如果你要开发 ChatGPT 应用，计算 Token 的数量是非常重要的。下面是一段示例代码，可以帮助你计算在 ChatGPT 对话模型下所消耗的 Token 数量。</p><h3 id="通过-api-计算-token-数量"><strong><em>*通过 API 计算 Token 数量*</em></strong></h3><p>第一种计算 Token 数量的方式，是从 API 返回的结果里面获取。当我们调用 API 时，我们可以从返回的结果中获取 Token 的数量。这个数量代表了这次请求消耗的 Token 数。我们可以利用这个信息来计算出整个会话过程中使用的 Token 数量。为了实现这个计算方式，我们需要修改一下刚才的 Conversation 类。我们可以重新创建一个 Conversation2 类来实现这个功能。这个新类和之前的类只有一个不同，就是 ask 函数除了返回回复的消息之外，还会返回这次请求消耗的 Token 数。这样我们就可以更好地掌握整个会话过程中的 Token 使用情况，从而更好地优化我们的代码和算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conversation2</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, prompt, num_of_round</span>):</span><br><span class="line">        self.prompt = prompt</span><br><span class="line">        self.num_of_round = num_of_round</span><br><span class="line">        self.messages = []</span><br><span class="line">        self.messages.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: self.prompt&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ask</span>(<span class="params">self, question</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.messages.append( &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: question&#125;)</span><br><span class="line">            response = openai.ChatCompletion.create(</span><br><span class="line">                model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>,</span><br><span class="line">                messages=self.messages,</span><br><span class="line">                temperature=<span class="number">0.5</span>,</span><br><span class="line">                max_tokens=<span class="number">2048</span>,</span><br><span class="line">                top_p=<span class="number">1</span>,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(e)</span><br><span class="line">            <span class="keyword">return</span> e</span><br><span class="line"></span><br><span class="line">        message = response[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;message&quot;</span>][<span class="string">&quot;content&quot;</span>]</span><br><span class="line">        num_of_tokens = response[<span class="string">&#x27;usage&#x27;</span>][<span class="string">&#x27;total_tokens&#x27;</span>]</span><br><span class="line">        self.messages.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span>: message&#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.messages) &gt; self.num_of_round*<span class="number">2</span> + <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">del</span> self.messages[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">        <span class="keyword">return</span> message, num_of_tokens</span><br></pre></td></tr></table></figure><p>然后我们还是问一遍之前的问题，看看每一轮问答消耗的 Token 数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv2 = Conversation2(prompt, <span class="number">3</span>)</span><br><span class="line">questions = [question1, question2, question3, question4, question5]</span><br><span class="line"><span class="keyword">for</span> question <span class="keyword">in</span> questions:</span><br><span class="line">    answer, num_of_tokens = conv2.ask(question)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;询问 &#123;%s&#125; 消耗的token数量是 : %d&quot;</span> % (question, num_of_tokens))输出结果：</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">询问&#123;你是谁？&#125; 消耗的Token数量是： <span class="number">113</span></span><br><span class="line">询问&#123;请问什么是正当防卫？&#125; 消耗的Token数量是： <span class="number">302</span></span><br><span class="line">询问&#123;那防卫过当呢？&#125; 消耗的Token数量是： <span class="number">516</span></span><br><span class="line">询问&#123;我问你的第一个问题是什么？&#125; 消耗的Token数量是： <span class="number">549</span></span><br><span class="line">询问&#123;我问你的第一个问题是什么？&#125; 消耗的Token数量是： <span class="number">546</span></span><br></pre></td></tr></table></figure><p>根据我们的数据，我们可以看出前几轮的 Token 消耗数量在逐渐增多，这表明 ChatGPT 能在前几轮对话中提供更多的信息和回答。然而，最后 3 轮的 Token 消耗数量保持不变。这是因为我们代码里只使用过去 3 轮的对话内容向 ChatGPT 发起请求。我们可以考虑增加对话轮数以便更好地利用 ChatGPT 的能力，同时也需要注意 Token 消耗量的控制。</p><h3 id="通过-tiktoken-库计算-token-数量"><strong><em>*通过 Tiktoken 库计算 Token 数量*</em></strong></h3><p>第二种方式是使用Python库Tiktoken对文本进行分词并计算Token数量。这种方法在上一讲中已经提到过。</p><p>需要注意的是，使用不同的GPT模型需要对应不同的Tiktoken编码器模型。对应文档可以在以下链接中查询：https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb</p><p>我们所使用的ChatGPT模型采用的是cl100k_base的编码器。我们也可以用它计算第一轮对话使用的Token数量，以此来丰富我们的文本内容。 在计算Token数量方面，我们可以试着定义不同的计算方法，例如考虑标点符号的使用、停用词的去除、Token的长度等等。这些方法可以更加准确地计算Token数量，并且有助于我们更好地理解文本内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line">encoding = tiktoken.get_encoding(<span class="string">&quot;cl100k_base&quot;</span>)</span><br><span class="line"></span><br><span class="line">conv2 = Conversation2(prompt, <span class="number">3</span>)</span><br><span class="line">question1 = <span class="string">&quot;你是谁？&quot;</span></span><br><span class="line">answer1, num_of_tokens = conv2.ask(question1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;总共消耗的token数量是 : %d&quot;</span> % (num_of_tokens))</span><br><span class="line"></span><br><span class="line">prompt_count = <span class="built_in">len</span>(encoding.encode(prompt))</span><br><span class="line">question1_count = <span class="built_in">len</span>(encoding.encode(question1))</span><br><span class="line">answer1_count = <span class="built_in">len</span>(encoding.encode(answer1))</span><br><span class="line">total_count = prompt_count + question1_count + answer1_count</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Prompt消耗 %d Token, 问题消耗 %d Token，回答消耗 %d Token，总共消耗 %d Token&quot;</span> % (prompt_count, question1_count, answer1_count, total_count))</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">总共消耗的token数量是 : <span class="number">113</span></span><br><span class="line">Prompt消耗 <span class="number">66</span> Token, 问题消耗 <span class="number">5</span> Token，回答消耗 <span class="number">29</span> Token，总共消耗 <span class="number">100</span> Token</span><br></pre></td></tr></table></figure><p>我们通过 API 获得了消耗的 Token 数，然后又通过 Tiktoken 分别计算了 System 的指示内容、用户的问题和 AI 生成的回答，发现了两者还有小小的差异。这个是因为，我们没有计算 OpenAI 去拼接它们内部需要的格式的 Token 数量。很多时候，我们都需要通过 Tiktoken 预先计算一下 Token 数量，避免提交的内容太多，导致 API 返回报错。</p><p>为了更好地解决这个问题，我们可以使用一种称为“Tokenization”的技术。这种技术可以将一段文本分割成单个的词语或符号，这些词语或符号被称为Token。通过计算每个Token的数量，我们可以更准确地预测 API 的返回值。此外，我们还可以使用另一种技术，称为“Compression”，通过压缩文本以减少Token的数量。这样可以大大减少提交的内容，提高API的速度和准确性。</p><h2 id="使用gradio搭建一个聊天界面">使用Gradio搭建一个聊天界面</h2><p>我们已经有了一个封装好的聊天机器人，但是它现在只能在 Python Notebook 中使用，每次需要通过调用代码来提问。因此，我们接下来将为我们封装好的 Convesation 接口开发一个界面，并添加一些新的功能。</p><p>我们将使用 Gradio 这个 Python 库来开发这个聊天机器人的界面，因为它有以下几个优点。首先，我们现有的代码都是使用 Python 实现的，所以你不需要再去学习 JavaScript、TypeScript 以及相关的前端框架。其次，Gradio 渲染出来的界面可以直接在 Jupyter Notebook 中显示出来，这对于不了解技术的同学来说，大大减轻了其他环境搭建的负担。此外，Gradio 这个公司已被目前最大的开源机器学习模型社区 HuggingFace 收购了。你可以免费将 Gradio 的应用部署到 HuggingFace 上。接下来，我将教你如何部署，你可以将你自己开发的聊天机器人部署上去，让你的朋友们也能使用。最后，值得一提的是，在后面的课程中，我们有时也会使用一些开源的模型，而这些模型往往也托管在 HuggingFace 上。因此，使用 HuggingFace Gradio 的部署方式，特别方便我们演示给其他人看。</p><blockquote><p>注：Gradio 官方也有用其他开源预训练模型创建 Chatbot 的教程https://gradio.app/creating-a-chatbot/</p></blockquote><p>首先，让我们先安装所需的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge gradio</span><br></pre></td></tr></table></figure><p>Gradio 应用的代码我也列在了下面，对应的逻辑也非常简单。首先，我们定义好了 system 这个系统角色的提示语，创建了一个 Conversation 对象。</p><p>然后，我们为了更好的用户体验，对 answer 方法进行了进一步封装。在这个新的版本中，我们添加了更多的逻辑，使得 Chatbot 能够更好地处理用户的输入。具体来说，我们加入了一个新的函数，用于判断用户输入的语句是否是一个问题。如果是一个问题，我们会对其进行解答，并返回一个对应的答案。如果不是一个问题，我们会将其添加到历史记录中。</p><p>此外，我们还对 Conversation 类进行了扩展，增加了一个名为“recommend”的方法。该方法可以根据用户的历史记录，为用户推荐一些相关的话题。这样可以增强 Chatbot 的交互性和趣味性。</p><p>最后，我们通过一段 with 代码，创建了对应的聊天界面。Gradio 提供了一个现成的 Chatbot 组件，我们只需要调用它，然后提供一个文本输入框就好了。这样用户就可以通过输入文字与 Chatbot 进行交互，获取他们想要的信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gradio <span class="keyword">as</span> gr</span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:</span></span><br><span class="line"><span class="string">1. 你的回答必须是中文</span></span><br><span class="line"><span class="string">2. 回答限制在100个字以</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">conv = Conversation(prompt, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">answer</span>(<span class="params">question, history=[]</span>):</span><br><span class="line">    history.append(question)</span><br><span class="line">    response = conv.ask(question)</span><br><span class="line">    history.append(response)</span><br><span class="line">    responses = [(u,b) <span class="keyword">for</span> u,b <span class="keyword">in</span> <span class="built_in">zip</span>(history[::<span class="number">2</span>], history[<span class="number">1</span>::<span class="number">2</span>])]</span><br><span class="line">    <span class="keyword">return</span> responses, history</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gr.Blocks(css=<span class="string">&quot;#chatbot&#123;height:300px&#125; .overflow-y-auto&#123;height:500px&#125;&quot;</span>) <span class="keyword">as</span> demo:</span><br><span class="line">    chatbot = gr.Chatbot(elem_id=<span class="string">&quot;chatbot&quot;</span>)</span><br><span class="line">    state = gr.State([])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> gr.Row():</span><br><span class="line">        txt = gr.Textbox(show_label=<span class="literal">False</span>, placeholder=<span class="string">&quot;Enter text and press enter&quot;</span>).style(container=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    txt.submit(answer, [txt, state], [chatbot, state])</span><br><span class="line"></span><br><span class="line">demo.launch()</span><br></pre></td></tr></table></figure><p>你直接在 Colab 或者你本地的 Jupyter Notebook 里面，执行一下这一讲到目前的所有代码，就得到了一个可以和 ChatGPT 聊天的机器人了。</p><p>你可以直接在Jupyter中使用，当然，程序也提供了一个本地地址供你使用，如下图：</p><p>VSCode Jupyter内：</p><img src="/Quickly-build-an-AI-application/20230601165713.png" class="" title="img"><p>浏览器上：</p><img src="/Quickly-build-an-AI-application/20230601165716.png" class="" title="img"><h2 id="部署到huggingface上">部署到HuggingFace上</h2><p>如果你拥有一个聊天机器人，你肯定会想和你的朋友一起分享这个惊奇的机器人。为了让你的朋友也能使用这个机器人，我们建议你将它部署到 <a href="https://huggingface.co/">HuggingFace</a> 上。</p><ol type="1"><li><p>首先，如果你还没有 HuggingFace 的账号，你需要先注册一个账号。选择Spaces之后，然后选择 " Create New Space" 创建一个新的项目空间。接下来，你需要将机器人的代码上传到你的项目空间中。 <img src="/Quickly-build-an-AI-application/20230601165722.png" class="" title="img"></p></li><li><p>在接下来的界面里，你需要为你的 Space 取一个名字。这个名字应该是能够代表你的项目的，也应该是容易记忆的。你可以将你的 Space 命名为“gradio”，或者其他什么奇奇怪怪的名字。License随便选择一个协议，或者跟我一样选择MIT。在 Select the Space SDK 里面，你需要选择第二个 Gradio。这个 SDK 是一个简单易用的工具，能够帮助你快速地构建和部署你的应用程序。如果你对编程不是很熟悉，这个 SDK 就更加适合你了。在选择硬件方面，我们建议选择免费的选项。这样可以避免花费不必要的成本，同时也能够让你更加专注于开发你的应用程序。在选择项目的时候，我们建议选择 public。这样其他人就可以看到你的项目，给你提供反馈和建议。不过需要注意的是，public 的 space，是连你后面上传的代码也能够看到的。因此，你需要确保你上传的代码是没有敏感信息的，同时也需要遵守相关的法律法规。一切都准备好之后，选择最下方的Create Space按钮吧。</p></li></ol><img src="/Quickly-build-an-AI-application/20230601165729.png" class="" title="img"><ol start="3" type="1"><li><p>创建成功后，会跳转到 HuggingFace 的 App 界面。在这个界面里，你可以找到如何使用当前 space 的方法，包括克隆 Clone 当前的 space 和提交代码部署 App 等。为了方便这个过程，我们只需要通过 Git 把当前 space 下载下来，然后提交两个文件就可以了。这两个文件对于 App 的部署非常重要。第一个文件包含了 App 的所有必要文件，第二个文件则包含了 App 的所有依赖。这些文件都需要在部署前进行正确的配置，以确保 App 能够顺利地运行。</p></li><li><ol type="1"><li><a href="http://app.py/">app.py</a> 包含了我们的 Gradio 应用；</li><li>requirements.txt 包含了这个应用依赖的 Python 包，这里我们只依赖 OpenAI 这一个包。 <img src="/Quickly-build-an-AI-application/20230601165747.png" class="" title="img"></li></ol></li><li><p>下载这个Git到本地，如果本地没有这两个文件，记得自己创建一下，然后将我们刚才的代码粘贴到app.py内，最后全部提交:</p></li></ol><img src="/Quickly-build-an-AI-application/20230601165800.png" class="" title="img"><ol start="5" type="1"><li><p>还记得咱们之前提到过，选择Public之后，不仅App，甚至于代码都是公开的吗？那么我们就不能在其中放入我们的Api Key这种敏感内容了，所以我们需要在HuggingFace的Space内设置一下这个环境变量。</p></li><li><ol type="1"><li><p>你可以点击界面里面的 Settings，然后往下找到 Repository secret，点击New secret <img src="/Quickly-build-an-AI-application/20230601165811.png" class="" title="img"></p></li><li><p>在 Name 这里输入 OPENAI_API_KEY，然后在 Secret value 里面填入你的 OpenAI 的密钥。 <img src="/Quickly-build-an-AI-application/20230601165818.png" class="" title="img"></p></li><li><p>最后，添加完成之后记得要重启一下Space <img src="/Quickly-build-an-AI-application/20230601165828.png" class="" title="img"></p></li><li><p>好啦，这个时候，你可以重新点击 App 这个 Tab 页面，试试你的聊天机器人是否可以正常工作了。</p></li></ol></li></ol><img src="/Quickly-build-an-AI-application/20230601165833.png" class="" title="img"><p>因为本人比较吝啬，也就不提供免费的测试Demo了，文中用于演示的Demo也一并删除了，但是 <code>app.py</code>这个在Github上可以找到。</p><p>有兴趣的可以去徐文浩的Spaces空间去测试下：https://huggingface.co/spaces/xuwenhao83/simple_chatbot</p><h2 id="小结">小结</h2><p>希望通过这一讲，你已经学会了怎么使用 ChatGPT 的接口来实现一个聊天机器人了。在我们实现只保留固定轮数的对话的过程中，我们发现这样的限制可以防止聊天机器人在对话的过程中变得混乱。我们也了解到了通过在对话过程中一直发送所有上下文的原因，这使得聊天机器人能够更好地理解用户的意图和需求。</p><p>此外，我们还学习了如何使用 Gradio 这个库来创建一个聊天机器人界面。通过使用 Gradio，我们可以轻松地为我们的聊天机器人提供一个直观的界面，使得用户可以更方便地与聊天机器人进行交互。</p><p>最后，我们将这个简单的聊天机器人部署到了 HuggingFace 上。这为我们提供了一个可靠的平台，可以与他人分享我们的聊天机器人，并在他们的设备上使用。希望你可以在这个过程中享受到学习的乐趣，以及尝试着将这个聊天机器人进一步优化和扩展。</p><h2 id="课后练习">课后练习</h2><p>在这一讲里，我们讨论的是 Chatbot 的维护能力。目前 Chatbot 只能维护过去 N 轮的对话，这意味着如果对话很长的话，我们一开始对话的信息就被丢掉了。但是，我们可以采用一种不设定轮数，只限制传入的上下文的 Token 数量的方式来解决这个问题。这种方式可以让 Chatbot 尽可能多地记住上下文，同时在 Token 数量超标的时候再删减最开始的对话。</p><p>要实现这个功能，你可以根据这一讲学到的内容，修改代码，让聊天机器人在不限制轮数的情况下，能够保存尽可能多的上下文信息。除了“忘记”开始的几轮，你还能想到什么办法让 AI 记住更多的上下文信息吗？</p><p>期待能在评论区看到你的思考，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hi，我是茶桁。&lt;/p&gt;
&lt;p&gt;在过去的两讲中，我们已经使用 OpenAI 提供的 Embedding 接口完成了文本分类的功能。现在，我们回到 Completion 接口，这一讲将带你更深入地了解该接口的使用。除此之外，我们还将快速搭建一个有界面的聊天机器人，这将让你更好地理解 Completion 接口的应用场景。在这个过程中，你将第一次使用 HuggingFace 这个平台，它是目前最流行的深度学习模型社区。通过 HuggingFace，你可以下载到最新的开源模型，查看其他人提供的示例代码，并参与到社区的交流中。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>05 为文本分类</title>
    <link href="https://hivan.me/Text-Classification/"/>
    <id>https://hivan.me/Text-Classification/</id>
    <published>2023-05-12T08:52:45.000Z</published>
    <updated>2023-06-01T13:30:13.510Z</updated>
    
    <content type="html"><![CDATA[<p>Hi， 我是茶桁。</p><p>在前一讲中，我们观察到大型模型的确表现出良好效果。在情感分析任务中，通过使用 OpenAI API 提供的 Embedding，我们得到的结果要比能在单个机器上运行的较小模型（如T5-base）要好得多。然而，我们之前所选择的问题确实有些过于简单。我们将5个不同的评分分成了正面、负面和中性，同时还排除了相对难以判断的"中性"评价，因此我们判断准确率的提高相对较容易实现。但是，如果我们想要准确预测具体的分数呢？这将是我们接下来需要探索的问题。</p><span id="more"></span><h2 id="训练机器学习模型">训练机器学习模型</h2><p>一种最简单的方法是利用我们获得的文本 Embedding 向量。这次我们将不再直接使用向量之间的距离，而是采用传统的机器学习方法进行分类。这样做的原因在于，仅仅使用向量之间的距离无法最大化利用已标注好的分数信息。</p><p>事实上，OpenAI在其官方教程中也给出了这样一个例子。我在这里提供了相应的<a href="https://github.com/openai/openai-cookbook/blob/main/examples/Classification_using_embeddings.ipynb">GitHub代码链接</a>，你可以查看一下。然而，为了避免过于自夸，我们希望将我们的结果与其他使用传统机器学习方法得到的结果进行比较。</p><p>因此，我重新找到了一个中文数据集来进行试验。该数据集是在中文互联网上容易找到的一份包含今日头条新闻标题和关键词的数据集，你可以在GitHub上直接获取数据，我在这里提供了<a href="https://github.com/aceimnorstuvwxz/toutiao-text-classfication-dataset">链接</a>。使用这个数据集的好处是，已经有人发布了预测实验的结果。我们可以将我们训练得到的结果与其进行比较。</p><h2 id="处理数据">处理数据</h2><p>在训练模型之前，我们需要获取每个新闻标题的嵌入向量。为了做到这一点，我们使用 Python 数据处理库 Pandas 将相应的文本加载到内存中。然后，我们调用之前使用过的 OpenAI 嵌入接口，将返回结果存储在内存中，这些嵌入向量将作为我们机器学习模型的输入。</p><p>虽然这听起来非常简单直接，但是在实际操作中，我们需要注意一些细节。例如，我们需要在处理数据之前进行数据清洗，以去除无用的标点符号和停用词，并进行分词。这有助于提高我们模型的精度和可靠性。</p><p>一旦我们完成了数据清洗和嵌入向量的提取，我们就可以开始训练我们的机器学习模型了。在这个过程中，我们可以使用各种传统的机器学习算法，如决策树、支持向量机或朴素贝叶斯分类器等，来对数据进行分类。当然，我们也可以尝试一些更高级的算法，如深度学习，来进一步提高模型的精度和效果。</p><blockquote><p><strong>千万不要急着复制代码运行，请看完整篇文章。因为涉及API调用，涉及到你的Money</strong></p></blockquote><blockquote><p>请注意：由于后面的代码可能会消耗相当数量的 Token，如果您使用的是免费的 $5 美元配额，建议您直接下载我在 Github 上提供的数据文件，以使用我已经处理好的数据。值得注意的是，如果您准备使用自己的数据进行处理，请确保您已经在算法上进行了必要的优化，以保证您的代码能够在合理的时间内完成处理。文章最后我会放上本节数据。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> openai.embeddings_utils <span class="keyword">import</span> get_embedding, get_embeddings</span><br><span class="line"></span><br><span class="line">openai.api_key = <span class="string">&quot;OPENAI_API_KEY&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># embedding model parameters</span></span><br><span class="line">embedding_model = <span class="string">&quot;text-embedding-ada-002&quot;</span></span><br><span class="line">embedding_encoding = <span class="string">&quot;cl100k_base&quot;</span>  <span class="comment"># this the encoding for text-embedding-ada-002</span></span><br><span class="line">max_tokens = <span class="number">8000</span>  <span class="comment"># the maximum for text-embedding-ada-002 is 8191</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># import data/toutiao_cat_data.txt as a pandas dataframe</span></span><br><span class="line"><span class="comment"># 这里的数据是从 Github上获取的</span></span><br><span class="line"><span class="comment"># https://github.com/aceimnorstuvwxz/toutiao-text-classfication-dataset</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./data/toutiao_cat_data.txt&#x27;</span>, sep=<span class="string">&#x27;_!_&#x27;</span>, names=[<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;code&#x27;</span>, <span class="string">&#x27;category&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;keywords&#x27;</span>])</span><br><span class="line">df = df.fillna(<span class="string">&quot;&quot;</span>)</span><br><span class="line">df[<span class="string">&quot;combined&quot;</span>] = (</span><br><span class="line">    <span class="string">&quot;标题: &quot;</span> + df.title.<span class="built_in">str</span>.strip() + <span class="string">&quot;; 关键字: &quot;</span> + df.keywords.<span class="built_in">str</span>.strip()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Lines of text before filtering: &quot;</span>, <span class="built_in">len</span>(df))</span><br><span class="line"></span><br><span class="line">encoding = tiktoken.get_encoding(embedding_encoding)</span><br><span class="line"><span class="comment"># omit reviews that are too long to embed</span></span><br><span class="line">df[<span class="string">&quot;n_tokens&quot;</span>] = df.combined.apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(encoding.encode(x)))</span><br><span class="line">df = df[df.n_tokens &lt;= max_tokens]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Lines of text after filtering: &quot;</span>, <span class="built_in">len</span>(df))</span><br></pre></td></tr></table></figure><blockquote><p>注：以上内容是加载数据并做一些简单预处理的代码，你可以直接运行。下面则最好不要。</p></blockquote><blockquote><p>下面是请求 OpenAI API 获取 Embedding 的代码，但是运行时可能会遇到报错。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># randomly sample 1k rows</span></span><br><span class="line">df_1k = df.sample(<span class="number">1000</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">df_1k[<span class="string">&quot;embedding&quot;</span>] = df_1k.combined.apply(<span class="keyword">lambda</span> x : get_embedding(x, engine=embedding_model))</span><br><span class="line">df_1k.to_csv(<span class="string">&quot;./data/toutiao_cat_data_10k_with_embeddings.csv&quot;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>当我们直接运行这个代码之后，很大几率会遇到一个报错，因为这个数据处理过程中也是有一些坑在内的。</p><img src="/Text-Classification/20230601165412.png" class="" title="img"><p>在使用 OpenAI 提供的接口时，我们需要注意两个坑点。</p><p>第一个坑点是关于接口限制。我们使用的 text-embedding-ada-002 模型，每条记录的长度被限制在了 8191 个标记。在调用 API 之前，我们需要计算每条记录的标记数，如果超过了 8000 个，就需要过滤掉。在我们的数据集中，只有新闻标题，因此这个长度限制不会成为问题。但是在使用其他数据集时，你需要注意数据是否超出了长度限制。如果超过了长度限制，你需要对数据进行过滤或者对文本进行截断，只使用文本的最后 8000 个标记。</p><p>为了保证计算出来的标记数和 OpenAI 的一致，我们在这里使用了 Tiktoken 库，并且采用了 cl100k_base 这种编码方式。</p><p>第二个坑点是关于 API 的调用限制。OpenAI 对 API 的调用进行了限速，如果你过于频繁地调用，就会遇到限速的错误。而且，如果你在遇到错误后继续调用，限速的时间还会被延长。为了解决这个问题，我们可以使用 Python 库 backoff。在调用 API 时，如果遇到错误，就等待一段时间，如果连续出现错误，就拉长等待时间。我们可以使用 backoff 改造代码来解决这个问题。下面是经过 backoff 改造后的代码。</p><blockquote><p>当然，你首先还是需要安装backoff到你的conda环境内。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge backoff</span><br></pre></td></tr></table></figure><p>换成PIP安装也可以，效果一致</p><p>接着，让我们来看修改后的部分代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> backoff</span><br><span class="line"></span><br><span class="line"><span class="meta">@backoff.on_exception(<span class="params">backoff.expo, openai.error.RateLimitError</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_embedding_with_backoff</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    <span class="keyword">return</span> get_embedding(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># randomly sample 10k rows</span></span><br><span class="line">df_10k = df.sample(<span class="number">10000</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">df_10k[<span class="string">&quot;embedding&quot;</span>] = df_10k.combined.apply(<span class="keyword">lambda</span> x : get_embedding_with_backoff(text=x, engine=embedding_model))</span><br><span class="line">df_10k.to_csv(<span class="string">&quot;./data/toutiao_cat_data_10k_with_embeddings.csv&quot;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>通过使用 backoff 库，我们指定了在遇到 RateLimitError 的时候，按照指数级别增加等待时间，这样可以确保我们的程序可以顺利运行。</p><p>然而，如果你直接运行上面那个代码，大约需要 2 个小时才能处理完 1 万条数据。而我们的数据集里有 38 万条数据，如果按照这种方式处理，需要 3 天 3 夜才能把训练数据处理完，这样显然不怎么实用。所以，我们需要采取一些措施来加快处理速度。</p><p>首先，我们需要了解慢的原因。其中一个原因是因为限速，即每分钟 API 调用次数的限制。虽然 backoff 可以确保程序不会因为失败而终止，但是我们仍然受到了限制。另一个原因是因为延时，因为我们是按照顺序一个个调用 Embedding 接口，每一次调用都要等前一次调用结束后才会发起请求，这更进一步拖长了处理数据所需要的时间。</p><img src="/Text-Classification/20230601165416.png" class="" title="img"><p>为了加快处理速度，我们可以采用多线程处理的方式，这样就可以实现多条数据并行请求。另外，我们还可以使用分布式处理的方法，将数据集分成多个部分，分别在不同的计算机上进行处理。这样可以大大缩短处理时间，提高程序的效率。</p><blockquote><p>注：如果您想了解更多关于 OpenAI 对于不同模型的限速的信息，可以点击<a href="https://platform.openai.com/docs/guides/rate-limits/overview">这个链接</a>查看。</p></blockquote><p>要解决这个问题，我们可以采用一些方法来提高处理速度。OpenAI 支持批量调用接口，您可以在一个请求里一次批量处理很多个请求。通过将 1000 条记录打包在一起处理，速度将会快很多。我在下面放置了一些对应的代码，您可以试着执行一下，处理这 38 万多条的数据，也就个把小时。</p><p>除此之外，您也可以考虑优化您的代码，例如使用更高效的算法和数据结构，以减少处理时间。此外，您还应该注意 OpenAI 的限速不仅仅是针对请求数的，也限制您每分钟可以处理的 Token 数量。因此，您需要根据每条数据包含的 Token 数自行计算一次打包多少条记录才能避免超出限制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> backoff</span><br><span class="line"><span class="keyword">from</span> openai.embeddings_utils <span class="keyword">import</span> get_embeddings</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@backoff.on_exception(<span class="params">backoff.expo, openai.error.RateLimitError</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_embeddings_with_backoff</span>(<span class="params">prompts, engine</span>):</span><br><span class="line">    embeddings = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(prompts), batch_size):</span><br><span class="line">        batch = prompts[i:i+batch_size]</span><br><span class="line">        embeddings += get_embeddings(list_of_text=batch, engine=engine)</span><br><span class="line">    <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line"><span class="comment"># randomly sample 10k rows</span></span><br><span class="line">df_all = df</span><br><span class="line"><span class="comment"># group prompts into batches of 100</span></span><br><span class="line">prompts = df_all.combined.tolist()</span><br><span class="line">prompt_batches = [prompts[i:i+batch_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(prompts), batch_size)]</span><br><span class="line"></span><br><span class="line">embeddings = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> prompt_batches:</span><br><span class="line">    batch_embeddings = get_embeddings_with_backoff(prompts=batch, engine=embedding_model)</span><br><span class="line">    embeddings += batch_embeddings</span><br><span class="line"></span><br><span class="line">df_all[<span class="string">&quot;embedding&quot;</span>] = embeddings</span><br><span class="line">df_all.to_parquet(<span class="string">&quot;./data/toutiao_cat_data_all_with_embeddings.parquet&quot;</span>, index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>最后一个需要注意的点是，对于这样的大数据集，我们不应该存储成 CSV 格式。特别是我们获取到的 Embedding 数据，它是由很多浮点数组成的。如果我们存储成 CSV 格式，那么它会把本来只需要 4 个字节的浮点数都用字符串的形式存储下来，这会浪费好几倍的空间，并且写入的速度也会变得很慢。因此，我在这里采用了 parquet 这个序列化的格式。使用 parquet 格式可以节省空间并提高写入速度，整个存储的过程只需要 1 分钟左右。另外，为了确保数据的安全，我们还可以对 parquet 文件进行加密和压缩，这样可以进一步减小存储空间，并且保护数据的机密性。</p><h2 id="训练模型看效果"><strong><em>*训练模型看效果*</em></strong></h2><p>我们已经完成了数据处理，接下来可以开始尝试模型训练了。如果你担心你的 API 调用次数会因此过多，你可以直接在我的 GitHub 上下载我处理好的数据集，并使用它来训练你的模型。此外，我还可以为你提供一些额外的建议和技巧，以帮助你更好地利用这个数据集，例如更好的模型选择、调整等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report, accuracy_score</span><br><span class="line"></span><br><span class="line">training_data = pd.read_parquet(<span class="string">&quot;data/toutiao_cat_data_all_with_embeddings.parquet&quot;</span>)</span><br><span class="line">training_data.head()</span><br><span class="line"></span><br><span class="line">df =  training_data.sample(<span class="number">50000</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    <span class="built_in">list</span>(df.embedding.values), df.category, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">clf = RandomForestClassifier(n_estimators=<span class="number">300</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">preds = clf.predict(X_test)</span><br><span class="line">probas = clf.predict_proba(X_test)</span><br><span class="line"></span><br><span class="line">report = classification_report(y_test, preds)</span><br><span class="line"><span class="built_in">print</span>(report)</span><br></pre></td></tr></table></figure><p>模型训练的代码非常简单，但是如果你想提高其准确率，还有一些技巧可以使用。考虑到运行时间的因素，我这里直接随机选取了里面的 5 万条数据，4 万条作为训练集，1 万条作为测试集。然后通过最常用的 scikit-learn 这个机器学习工具包里面的随机森林（RandomForest）算法，做了一次训练和测试。在我的电脑上，大概 10 分钟可以跑完，整体的准确率可以达到 84%。</p><p>如果你想提高准确率，可以考虑增加数据量、调整算法参数或者使用其他算法。使用更多的数据，特别是多样性更高的数据，可以帮助你的模型更好地捕捉规律。调整算法参数可以帮助你找到最优的模型，从而提高准确率。同时，你可以尝试使用其他算法，如支持向量机（SVM）或神经网络，来比较不同算法之间的效果差异。</p><p>此外，你还可以使用交叉验证来评估模型的准确率。交叉验证可以帮助你评估模型的泛化能力，即模型在新的数据上的表现能力。最后，你可以尝试使用集成学习（ensemble learning）技术，如 bagging 或 boosting，来进一步提高模型的准确率。</p><img src="/Text-Classification/20230601165406.png" class="" title="img"><p>随机森林是一种效果不错的算法，但是它的运行速度相对较慢。为了更好地处理数据，我们可以使用更简单的逻辑回归（LogisticRegression）算法。然而，我们需要在整个数据集上运行这个算法。同样地，我们将使用80%的数据作为训练集，剩下的20%作为测试集。虽然我们的数据量比之前的四万条数据要多几倍，但是运行时间只需要三到四分钟，最终的准确率也能达到86%。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">df =  training_data</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    <span class="built_in">list</span>(df.embedding.values), df.category, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">preds = clf.predict(X_test)</span><br><span class="line">probas = clf.predict_proba(X_test)</span><br><span class="line"></span><br><span class="line">report = classification_report(y_test, preds)</span><br><span class="line"><span class="built_in">print</span>(report)</span><br></pre></td></tr></table></figure><p>输出结果：</p><img src="/Text-Classification/20230601165424.png" class="" title="img"><p>这个结果已经比我们下载数据集的 GitHub 页面里看到的效果好了，那个的准确率只有 85%。</p><p>可以看到，通过 OpenAI 的 API 获取到 Embedding，然后通过一些简单的线性模型，我们就能获得很好的分类效果。我们既不需要提前储备很多自然语言处理的知识，对数据进行大量的分析和清理；也不需要搞一块昂贵的显卡，去使用什么深度学习模型。只要 1～2 个小时，我们就能在一个几十万条文本的数据集上训练出一个非常不错的分类模型。</p><h2 id="指标一点机器学习小知识">指标：一点机器学习小知识</h2><p>刚刚我们解释了分类报告中的指标含义。每一行代表一个类别，包含准确率（Precision）、召回率（Recall）、F1分数和支持样本量（Support）这四个指标。为了更好地理解，我们将使用今日头条的新闻标题数据集进行说明。</p><p>准确率表示模型正确判断为该类别的标题所占的比例，即在所有判断为该类别的标题中有多少是真正属于该类别。举个例子，模型判断有100个标题属于农业新闻，但实际上只有83个标题是农业新闻，那么准确率就是0.83。准确率越高越好，但是并不意味着准确率达到100%就代表模型完全正确，因为模型可能会漏判，所以我们还需要考虑召回率。</p><p>召回率表示模型正确判断为该类别的标题占实际该类别下所有标题的比例，即没有漏掉的比例。例如，模型判断有100个标题属于农业新闻，这100个标题实际上都是农业新闻。准确率已经达到100%，但是实际上我们共有200条农业新闻。因此，在农业新闻类别中，我们的召回率只有100/200 = 50%。</p><p>因此，评估模型效果时需要考虑准确率和召回率，综合考虑这两个指标得出的结果就是F1分数。F1分数是准确率和召回率的调和平均数，即 F1 Score = 2 * (Precision * Recall) / (Precision + Recall)。当准确率和召回率都为100%时，F1分数也为1。如果准确率为100%，召回率为80%，那么计算得到的F1分数为0.88。F1分数越高越好。</p><p>支持样本量表示数据中实际属于该类别的样本数量。一般来说，样本数量越多，该类别的训练结果就越准确。</p><p>分类报告中的每一行代表一个类别，包含四个指标，而最后三行是针对整个测试数据集的结果，因此支持样本量都是1万个。</p><p>第一行的accuracy只有一个指标，虽然它在F1 Score这一列中，但它不是F1分数的含义。它表示模型总体上判断正确的分类数除以测试样本数，即模型的整体准确率。</p><p>第二行的macro average是宏平均，它将每个类别计算得到的指标加在一起取平均。宏平均对于数据分类不平衡的情况非常有用。比如，假设我们进行情感分析，其中90%的样本属于正面情感，而10%的样本属于负面情感。在这种情况下，如果我们的模型在正面情感方面的预测效果很好，准确率达到了90%，但在负面情感方面的准确率只有50%。如果只看整体数据，准确率似乎很高，因为正面情感的样本很多。但是对于我们的目标来说，即找到具有负面情感的客户并与他们沟通、进行赔偿，整体准确率就没有什么用了。而宏平均会将整体准确率计算为(90% + 50%)/2 = 70%。这并不是一个很好的预测结果，我们需要进一步优化模型。宏平均在处理数据样本不平衡的情况下非常有用。</p><p>第三行的weighted average是加权平均，它是根据每个类别支持的样本数量对每个指标进行加权平均计算。无论是准确率、召回率还是F1分数，都需要按照各个类别的支持样本量进行加权平均。</p><h2 id="小结">小结</h2><img src="/Text-Classification/Untitled.png" class="" title="img"><p>好了，今天的这一讲到这里就结束了，最后我们来回顾一下。在这一讲中，我们学到了如何使用 OpenAI 的 API 来获取文本的 Embedding。虽然接口不复杂，但是有些细节需要我们注意。我们需要考虑模型能够接受的最大文本长度、API 本身的限速以及网络延时带来的问题。因此，我们提供了一些解决方案，例如使用 Tiktoken 计算样本的 Token 数量并进行过滤，以及在遇到限速问题时通过 backoff 进行指数级别的时间等待。此外，我们还介绍了如何通过一次性批量请求一批数据，最大化我们的吞吐量来解决问题。对于返回的结果，我们可以通过 parquet 这样序列化的方式保存数据，来减少数据尺寸。</p><p>除此之外，我们还学到了如何直接利用拿到的 Embedding，简单调用一下 scikit-learn，通过机器学习的方法，进行更准确的分类。我们最终把 Embedding 放到一个简单的逻辑回归模型里，就取得了很不错的分类效果。因此，我们可以说这两个知识点都是非常有用的，特别是在处理大量文本数据时。你学会了吗？希望这些知识点能够对你有所帮助！</p><p>在本讲中，我们学习了如何利用 OpenAI 来获取文本的 Embedding，以及如何通过传统的机器学习方式来进行训练，并评估训练的结果。但是，我们可以进一步探索和扩展这个主题。</p><p>我们之前使用了 Amazon1000 条食物评论的情感分析数据集。在该数据集中，我们已经成功获取并保存了 Embedding 数据。那么，为什么不试着在完整的数据集上，训练一个能够区分从 1 分到 5 分每一个级别的机器学习模型呢？这不仅可以帮助你更深入地了解 Embedding 技术，还可以提高你的机器学习技能。最终，你可以将你的测试结果分享出来，与其他人比较，看看效果如何。</p><p>为方便大家，我在这里提供了整个原始数据集的下载链接。请随意下载，使用并测试你的机器学习模型。如果你感到收获颇丰，也欢迎你将本节课分享给其他人，让更多人了解如何用 Embedding 技术进行文本分类。</p><p>本文中所用到的最终训练完成的模型下载地址（百度云盘）：</p><blockquote><p>链接: https://pan.baidu.com/s/1yoxcDq5B6K0TZti-pfKRww?pwd=xt6m 提取码: xt6m</p></blockquote><blockquote><p>复制这段内容后打开百度网盘手机App，操作更方便哦 --来自百度网盘超级会员v6的分享</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hi， 我是茶桁。&lt;/p&gt;
&lt;p&gt;在前一讲中，我们观察到大型模型的确表现出良好效果。在情感分析任务中，通过使用 OpenAI API 提供的 Embedding，我们得到的结果要比能在单个机器上运行的较小模型（如T5-base）要好得多。然而，我们之前所选择的问题确实有些过于简单。我们将5个不同的评分分成了正面、负面和中性，同时还排除了相对难以判断的&quot;中性&quot;评价，因此我们判断准确率的提高相对较容易实现。但是，如果我们想要准确预测具体的分数呢？这将是我们接下来需要探索的问题。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>04 GPT-3/4对比其他模型胜在哪？</title>
    <link href="https://hivan.me/GPT-3-VS-Other-Model/"/>
    <id>https://hivan.me/GPT-3-VS-Other-Model/</id>
    <published>2023-05-11T08:51:26.000Z</published>
    <updated>2023-06-01T13:32:20.705Z</updated>
    
    <content type="html"><![CDATA[<p>大家好，我是茶桁。</p><p>在前两节课中，我们一起体验了 OpenAI 提供的 GPT-3.5 系列模型的两个核心接口。一个是获取文本的 Embedding 向量，另一个是根据提示语生成补全的文本内容。通过这两种方法，我们可以在零样本或少样本的情况下进行情感分析任务。然而，你可能会有两个疑问。首先，Embedding 不就是将文本转换为向量吗？为什么不直接使用开源模型（如Word2Vec、Bert）而要调用 OpenAI 的 API 呢？在我们的情感分析任务中，我们进行了一些简化处理。一方面，我们排除了相对中性的评分（3分）；另一方面，我们将1分、2分和4分、5分合并，将原本需要判断5个分类的问题简化了。那么，如果我们想要准确预测多个分类，是否也能如此简单呢？</p><span id="more"></span><p>在本节中，我们将通过代码和数据来回答第一个问题，尝试使用常见的开源模型，看看是否可以通过零样本学习的方式取得良好的效果。至于第二个问题，我们将在下节课中探讨，探索如何进一步利用 Embedding 结合机器学习算法来更好地处理情感分析问题。</p><h2 id="什么是预训练模型">什么是预训练模型？</h2><p>预训练模型是指通过在大规模文本数据上进行学习而生成的模型，能够将文本转化为语义丰富的向量表示。OpenAI 的 GPT-3 是一种超大规模的预训练模型，其英文全称为“Generative Pre-trained Transformer”，即生成式预训练 Transformer。通过预训练模型，我们可以在没有见过具体问题的情况下，利用大量可用的文本数据进行学习。</p><p>预训练模型的学习过程并不需要人工标注的数据。相反，它利用了大量的文本数据，如网页文章、维基百科内容、书籍电子版等，通过观察文本之间的内在关联来学习文本的语义。例如，在网络资料中，我们可以观察到许多类似的文本，如“小猫很可爱”和“小狗很可爱”。通过这些文本的前后关联，我们可以推断出小猫和小狗都是宠物，并且一般而言，人们对它们的情感是正面的。</p><p>这种隐含的关联信息可以帮助我们在情感分析等任务中填补少量用户评论和评分数据中缺失的“常识”。例如，如果文本中出现了“白日依山尽”，模型就能推测出接下来可能是“黄河入海流”。同样地，如果文本开头是“今天天气真”，那么后面很可能是“不错”，而较少可能是“糟糕”。这些文本关系最终通过一系列参数来体现。对于输入的文本，模型可以根据这些参数计算出一个向量，然后利用这个向量来推测文本的后续内容。</p><p>预训练模型在自然语言处理领域并不仅限于 OpenAI。早在2013年，Word2Vec就提出了类似的思想，通过在同一个句子中观察单词前后出现的单词，生成每个单词的向量表示。随后，在2018年，Google提出的BERT模型引起了整个行业的关注，BERT也成为了解决自然语言处理任务的重要预训练模型。在GPT-3论文发表之前，人们普遍认为BERT作为预训练模型在效果上优于GPT。然而，通过预训练模型，我们可以在语料文本越丰富的情况下拥有更多的参数，从而学习到更多的文本关系。因此，出现频率越高的文本关系，模型的预测能力也就越准确。随着时间的推移和研究的不断深入，预训练模型在自然语言处理领域的应用不断扩展。GPT-3的发布引发了巨大的关注，其规模庞大的预训练模型在生成式任务上取得了令人瞩目的成果。而BERT则在诸多下游任务中表现出色，成为许多实际应用的首选模型。</p><p>通过预训练模型，我们能够快速、高效地将文本转化为有意义的向量表示，从而在各种自然语言处理任务中取得良好的效果。随着预训练模型的不断发展和改进，我们可以期待更多的创新和突破，使得模型能够更准确地理解和处理各种复杂的自然语言表达。</p><p>预训练模型的引入为自然语言处理带来了巨大的进步和便利。它们通过利用大规模文本数据的内在关联，将语言转化为计算机能够理解和处理的形式，为我们解决各类自然语言处理任务提供了强大的基础。未来，随着技术的不断演进和研究的不断推进，预训练模型将继续发挥重要作用，推动自然语言处理领域的发展和创新。</p><h2 id="fasttextt5gpt-3-模型对比">Fasttext、T5、GPT-3 模型对比</h2><p>在探索预训练模型的应用中，我们不仅限于OpenAI的API，还可以考虑使用一些开源的模型。今天，我们将重点介绍两个开源预训练模型，并探讨它们在文本向量化方面的表现是否能与OpenAI的API媲美。</p><p>首先是Facebook的FastText，这个模型是基于Word2Vec思想的延伸，能够将单词表示成向量。通过FastText，我们可以将文本转化为向量表示，以便在后续的自然语言处理任务中使用。</p><p>第二个模型是Google的T5，T5的全称是Text-to-Text Transfer Transformer，它是一种适用于迁移学习的模型。迁移学习是指将T5生成的向量结果用于其他自然语言处理任务的机器学习过程。在许多最新的研究论文中，T5常被用作预训练模型进行微调和训练，或者作为基准模型用于对比和评估。</p><p>通过研究这两个开源预训练模型，我们可以更全面地了解它们在文本向量化方面的性能和适用性。无论是FastText还是T5，它们都为我们提供了更多选择和灵活性，让我们能够根据具体任务的需求来选择最适合的预训练模型。预训练模型的开源社区不断发展壮大，为我们提供了更多的工具和资源，助力我们在自然语言处理领域取得更好的效果。</p><h3 id="fasttext-效果测试">Fasttext 效果测试</h3><p>在开始使用FastText之前，我们需要先安装Python包管理工具中的FastText和Gensim两个包。如果您使用的是Conda作为Python包管理工具，可以按照以下示例代码安装所需的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge fasttext gensim</span><br></pre></td></tr></table></figure><p>如果您使用的是PIP或其他Python包管理工具，可以将上述代码中的"conda"替换为相应的包管理工具名称，并执行相应的安装命令。</p><blockquote><p>如果你用conda无法安装，也可以尝试PIP安装，效果是一样的。</p></blockquote><p>通过执行上述代码，我们可以安装所需的FastText和Gensim包。这两个包提供了在文本向量化过程中使用的工具和函数。安装完成后，我们将能够顺利进行FastText的使用，并进行相关的自然语言处理任务。请确保在运行代码之前先完成包的安装。</p><p>接下来，我们需要将与FastText对应的模型下载到本地。这些开源库和相关论文由Facebook和Google等国外公司发布，因此其在英语处理方面的效果较好。我们将下载与英语相关的模型，名称为"cc.en.300.bin"。我在这里提供了相应模型的<a href="https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz">下载链接</a>。</p><p>在这里，可以找到<a href="https://fasttext.cc/docs/en/crawl-vectors.html">更多的Fasttext模型</a></p><img src="/GPT-3-VS-Other-Model/20230601165525.png" class="" title="img"><p>将模型下载下来之后解压缩，放在你的Notebook同目录下。</p><img src="/GPT-3-VS-Other-Model/20230601165531.png" class="" title="img"><p>代码的逻辑并不复杂。首先，我们使用Gensim库加载了预训练好的FastText模型。接下来，我们定义了一个函数来获取文本的向量表示。由于FastText学习的是单词的向量而不是句子的向量，同时我们想要测试零样本学习的效果，因此无法进一步根据评论数据对模型进行训练。因此，我们采用了一种常见的方法，将一句话中每个单词的向量相加并取平均，将得到的向量作为整个评论的表示。我们将这个操作封装成了一个名为 <code>get_fasttext_vector</code> 的函数，以供后续程序使用。</p><p>在这里，我们还是拿<a href="https://www.notion.so/608ba17576684ea69d45fa987c2bfe7c">第二讲中的数据集</a>，用的是2.5W条亚马逊食物评论的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># load the FastText pre-trained model</span></span><br><span class="line">model = gensim.models.fasttext.load_facebook_model(<span class="string">&#x27;cc.en.300.bin&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_fasttext_vector</span>(<span class="params">line</span>):</span><br><span class="line">    vec = np.zeros(<span class="number">300</span>) <span class="comment"># Initialize an empty 300-dimensional vector</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> line.split():</span><br><span class="line">        vec += model.wv[word]</span><br><span class="line">    vec /= <span class="built_in">len</span>(line.split()) <span class="comment"># Take the average over all words in the line</span></span><br><span class="line">    <span class="keyword">return</span> vec</span><br></pre></td></tr></table></figure><p>而对应的零样本学习，我们还是和第 02 讲一样，将需要进行情感判断的评论分别与 “An Amazon review with a positive sentiment.” 以及 “An Amazon review with a negative sentiment.” 这两句话进行向量计算，算出它们之间的余弦距离。</p><p><strong>离前一个近，我们就认为是正面情感，离后一个近就是负面情感</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">positive_text = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Wanted to save some to bring to my Chicago family but my North Carolina family ate all 4 boxes before I could pack. These are excellent...could serve to anyone</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">negative_text = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">First, these should be called Mac - Coconut bars, as Coconut is the #2 ingredient and Mango is #3.  Second, lots of people don&#x27;t like coconut.  I happen to be allergic to it.  Word to Amazon that if you want happy customers to make things like this more prominent.  Thanks.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">positive_example_in_fasttext = get_fasttext_vector(positive_text)</span><br><span class="line">negative_example_in_fasttext = get_fasttext_vector(negative_text)</span><br><span class="line"></span><br><span class="line">positive_review_in_fasttext = get_fasttext_vector(<span class="string">&quot;An Amazon review with a positive sentiment.&quot;</span>)</span><br><span class="line">negative_review_in_fasttext = get_fasttext_vector(<span class="string">&#x27;An Amazon review with a negative sentiment.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> openai.embeddings_utils <span class="keyword">import</span> cosine_similarity</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_fasttext_score</span>(<span class="params">sample_embedding</span>):</span><br><span class="line">  <span class="keyword">return</span> cosine_similarity(sample_embedding, positive_review_in_fasttext) - cosine_similarity(sample_embedding, negative_review_in_fasttext)</span><br><span class="line"></span><br><span class="line">positive_score = get_fasttext_score(positive_example_in_fasttext)</span><br><span class="line">negative_score = get_fasttext_score(negative_example_in_fasttext)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Fasttext好评例子的评分 : %f&quot;</span> % (positive_score))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Fasttext差评例子的评分 : %f&quot;</span> % (negative_score))</span><br></pre></td></tr></table></figure><p>输出结果为：</p><blockquote><p>Fasttext好评例子的评分 : -0.000544 Fasttext差评例子的评分 : 0.000369</p></blockquote><p>在亚马逊食品评论数据集中，我们选择了一个正面评分为5分的例子和一个负面评分为1分的例子进行测试。然而，结果令人遗憾的是，通过零样本学习的方式，程序对这两个例子的判断都是错误的。然而，仔细思考一下，这样的结果其实是可以理解的。因为我们的方法是将整个句子的向量表示取平均，这意味着可能会出现之前提到的单词顺序不同的问题。例如，“not good, really bad” 和 “not bad, really good” 在这种情况下，意思完全不同，但向量表示完全相同。此外，我们对比的正面情感和负面情感两个句子之间只有一个单词的差异（如positive/negative）。如果我们只考虑单词的出现情况，并将它们的向量取平均，这种策略如果真的能够取得良好的效果，那反而应该怀疑是否存在Bug。</p><h3 id="t5效果测试">T5效果测试</h3><p>Fasttext并没有取得令人满意的结果，这可以理解，毕竟Word2Vec已经是10年前的技术了。那么，接下来我们来看看使用了目前最流行的Transformer结构的T5模型的效果如何。</p><p>T5模型的全称是Text-to-Text Transfer Transformer，意思是“文本到文本的迁移Transformer”。这个模型的设计初衷是为了方便在预训练之后将其迁移到其他任务上。当T5模型发布时，它在各种数据集上的评测表现都名列前茅。</p><p>T5模型中最大的模型有110亿个参数，同样基于Transformer。虽然相较于GPT-3的1750亿个参数要小很多，但对硬件性能的要求也不低。因此，我们首先测试一下T5-Small这个小型模型的效果。</p><p>在实际运行代码之前，我们需要安装相应的Python包。这里我们分别安装了SentencePiece和PyTorch。在安装PyTorch时，我还顺便安装了Torchvision，因为在后续课程中会用到它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge transformers</span><br><span class="line">conda install pytorch torchvision torchaudio -c pytorch-nightly</span><br><span class="line">conda install -c conda-forge sentencepiece</span><br></pre></td></tr></table></figure><blockquote><p>别问我为什么选择 <code>pytorch-nightly</code> , 简而言之，我用的是M1.</p></blockquote><p>我们需要执行的代码非常简单。首先，我们加载预训练的T5模型的分词器（Tokenizer）以及相应的模型。接下来，我们定义了一个名为get_t5_vector的函数，它接收一段文本输入，并使用分词器对其进行分词，生成一个序列。然后，我们将序列输入到模型的编码器部分进行编码。编码后的结果仍然是分词后的每个词的向量。与之前的Fasttext不同的是，这里的向量会随着位置和相邻词的不同而产生变化。因此，我们将这些向量取平均，作为整段文本的向量表示。需要注意的是，尽管我们进行了平均操作，但这里每个词的向量仍然包含了顺序信息的语义信息。值得注意的是，执行这段代码可能会稍慢一些。因为在首次加载模型时，Transformer库会将模型下载到本地并进行缓存，这个下载过程可能需要一些时间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> T5Tokenizer, T5Model</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># load the T5 tokenizer and model</span></span><br><span class="line">tokenizer = T5Tokenizer.from_pretrained(<span class="string">&#x27;t5-small&#x27;</span>, model_max_length=<span class="number">512</span>)</span><br><span class="line">model = T5Model.from_pretrained(<span class="string">&#x27;t5-small&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set the model to evaluation model</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># encode the input sentence</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_t5_vector</span>(<span class="params">line</span>):</span><br><span class="line">    input_ids = tokenizer.encode(line, return_tensors = <span class="string">&#x27;pt&#x27;</span>, max_length=<span class="number">512</span>, truncation = <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># generate the vector representation</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model.encoder(input_ids = input_ids)</span><br><span class="line">        vector = outputs.last_hidden_state.mean(dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> vector[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>有了模型和通过模型获取的向量数据，我们就可以再试一试前面的零样本学习的方式，来看看效果怎么样了。我们简单地把之前获取向量和计算向量的函数调用，都换成新的 get_t5_vector，运行一下就能看到结果了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">positive_review_in_t5 = get_t5_vector(<span class="string">&quot;An Amazon review with a positive sentiment.&quot;</span>)</span><br><span class="line">negative_review_in_t5 = get_t5_vector(<span class="string">&#x27;An Amazon review with a negative sentiment.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_t5</span>():</span><br><span class="line">  positive_example_in_t5 = get_t5_vector(positive_text)</span><br><span class="line">  negative_example_in_t5 = get_t5_vector(negative_text)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">get_t5_score</span>(<span class="params">sample_embedding</span>):</span><br><span class="line">    <span class="keyword">return</span> cosine_similarity(sample_embedding, positive_review_in_t5) - cosine_similarity(sample_embedding, negative_review_in_t5)</span><br><span class="line"></span><br><span class="line">  positive_score = get_t5_score(positive_example_in_t5)</span><br><span class="line">  negative_score = get_t5_score(negative_example_in_t5)</span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;T5好评例子的评分 : %f&quot;</span> % (positive_score))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;T5差评例子的评分 : %f&quot;</span> % (negative_score))</span><br><span class="line"></span><br><span class="line">test_t5()</span><br></pre></td></tr></table></figure><p>输出结果：</p><blockquote><p>T5好评例子的评分 : -0.010294 T5差评例子的评分 : -0.008990</p></blockquote><p>然而，不幸的是，我们得到的结果并不太理想。两个例子都被错误地判断为负面情绪，并且正面评价的分数还更低。不过，不要着急，这可能是因为我们使用的模型太小了。毕竟，在T5论文中，110亿个参数的大模型在各项评估中表现卓越，而我们使用的是T5-Small，这个相同架构下参数只有6000万个的小模型。要训练110亿个参数需要很长时间。因此，为什么不尝试一下参数为2.2亿个的T5-Base模型呢？使用起来也很简单，只需要将模型名称从T5-Small更改为T5-Base，其他代码无需修改，然后重新运行一遍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = T5Tokenizer.from_pretrained(<span class="string">&#x27;t5-base&#x27;</span>, model_max_length=<span class="number">512</span>)</span><br><span class="line">model = T5Model.from_pretrained(<span class="string">&#x27;t5-base&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set the model to evaluation mode</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># encode the input sentence</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_t5_vector</span>(<span class="params">line</span>):</span><br><span class="line">    input_ids = tokenizer.encode(line, return_tensors=<span class="string">&#x27;pt&#x27;</span>, max_length=<span class="number">512</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># generate the vector representation</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model.encoder(input_ids=input_ids)</span><br><span class="line">        vector = outputs.last_hidden_state.mean(dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> vector[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">positive_review_in_t5 = get_t5_vector(<span class="string">&quot;An Amazon review with a positive sentiment.&quot;</span>)</span><br><span class="line">negative_review_in_t5 = get_t5_vector(<span class="string">&#x27;An Amazon review with a negative sentiment.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_t5()</span><br></pre></td></tr></table></figure><p>输出结果为：</p><blockquote><p>T5好评例子的评分 : 0.010347 T5差评例子的评分 : -0.023935</p></blockquote><p>这次的结果似乎符合我们的期望，好评被判断为正面情感，差评被判断为负面情感。然而，也许我们只是运气好，在这几个例子上看到了效果。因此，接下来让我们将整个数据集中的1分和2分的差评以及4分和5分的好评都提取出来进行观察。在OpenAI的API中，使用的Embedding可以达到95%以上的准确率，让我们看看使用这个参数为2.2亿的T5-Base模型会得到什么样的结果。</p><p>相应的代码并不复杂，基本上与第02讲中OpenAI提供的Embedding代码相似。只是通过pandas库，根据评论的文本字段，对T5模型进行计算，然后将结果存储到DataFrame的"t5_embedding"列中。</p><p>同样地，我们还需要使用T5模型获得"An Amazon review with a positive sentiment."和"An Amazon review with a negative sentiment."这两个句子的Embedding。然后，我们将用户评论的Embedding与这两个句子的Embedding计算余弦距离，以判断这些评论是正面还是负面。</p><p>最后，使用Scikit-learn库中的分类报告类将评估结果打印出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">datafile_path = <span class="string">&quot;./data/fine_food_reviews_with_embeddings_1k.csv&quot;</span></span><br><span class="line">df = pd.read_csv(datafile_path)</span><br><span class="line"></span><br><span class="line">df[<span class="string">&quot;t5_embedding&quot;</span>] = df.Text.apply(get_t5_vector)</span><br><span class="line"><span class="comment"># convert 5-star rating to binary sentiment</span></span><br><span class="line">df = df[df.Score != <span class="number">3</span>]</span><br><span class="line">df[<span class="string">&quot;sentiment&quot;</span>] = df.Score.replace(&#123;<span class="number">1</span>: <span class="string">&quot;negative&quot;</span>, <span class="number">2</span>: <span class="string">&quot;negative&quot;</span>, <span class="number">4</span>: <span class="string">&quot;positive&quot;</span>, <span class="number">5</span>: <span class="string">&quot;positive&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> PrecisionRecallDisplay</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_embeddings_approach</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">label_score</span>(<span class="params">review_embedding</span>):</span><br><span class="line">        <span class="keyword">return</span> cosine_similarity(review_embedding, positive_review_in_t5) - cosine_similarity(review_embedding, negative_review_in_t5)</span><br><span class="line"></span><br><span class="line">    probas = df[<span class="string">&quot;t5_embedding&quot;</span>].apply(<span class="keyword">lambda</span> x: label_score(x))</span><br><span class="line">    preds = probas.apply(<span class="keyword">lambda</span> x: <span class="string">&#x27;positive&#x27;</span> <span class="keyword">if</span> x&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;negative&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    report = classification_report(df.sentiment, preds)</span><br><span class="line">    <span class="built_in">print</span>(report)</span><br><span class="line"></span><br><span class="line">    display = PrecisionRecallDisplay.from_predictions(df.sentiment, probas, pos_label=<span class="string">&#x27;positive&#x27;</span>)</span><br><span class="line">    _ = display.ax_.set_title(<span class="string">&quot;2-class Precision-Recall curve&quot;</span>)</span><br><span class="line"></span><br><span class="line">evaluate_embeddings_approach()</span><br></pre></td></tr></table></figure><p>输出结果如图：</p><img src="/GPT-3-VS-Other-Model/20230601165539.png" class="" title="img"><p>实验结果显示，使用T5模型的效果还算不错，对于所有样本的准确率达到了90%。然而，在处理较为困难的差评判断时，T5模型的表现要比直接使用OpenAI提供的Embedding差很多，整体精度仅为60%。与此相比，当我们观察整体模型准确率时，OpenAI的Embedding能够达到96%，相比于这里的90%，要更好一些。</p><p>以下是我们上节做的测试结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">    negative       <span class="number">0.98</span>      <span class="number">0.73</span>      <span class="number">0.84</span>       <span class="number">136</span></span><br><span class="line">    positive       <span class="number">0.96</span>      <span class="number">1.00</span>      <span class="number">0.98</span>       <span class="number">789</span></span><br><span class="line"></span><br><span class="line">    accuracy                           <span class="number">0.96</span>       <span class="number">925</span></span><br><span class="line">   macro avg       <span class="number">0.97</span>      <span class="number">0.86</span>      <span class="number">0.91</span>       <span class="number">925</span></span><br><span class="line">weighted avg       <span class="number">0.96</span>      <span class="number">0.96</span>      <span class="number">0.96</span>       <span class="number">925</span></span><br></pre></td></tr></table></figure><p>当然，这个分数还是相当不错的，可以作为一个合格的情感分析分类器的基准线。毕竟，我们采用的是零样本分类的方法，没有对需要分类的数据进行任何训练，直接使用预训练模型提供的向量，并根据距离进行判断。因此，看起来更大规模的预训练模型确实很有用，可以取得更好的效果。此外，当因为成本或网络延迟等原因无法方便地使用OpenAI的API时，如果只需要获取文本的嵌入向量，使用T5这样的开源模型的效果也相当不错。</p><h2 id="小结">小结</h2><img src="/GPT-3-VS-Other-Model/20230601165544.png" class="" title="img"><p>最后，我们来总结一下本讲的内容。在本讲中，我们使用了 Fasttext、T5-small 和 T5-base 这三个预训练模型进行了零样本分类的测试。在相同的食物评论数据集上，使用只学习了单词向量表示的 Fasttext，效果非常糟糕。当我们转而使用基于 Transformer 的 T5 模型时，即使是规模较小的 T5-small（6000 万参数），效果也不理想。然而，当我们尝试了具有 2.2 亿参数的 T5-base 模型时，结果有所改善。但是，这仍然远远落后于直接使用 OpenAI 的 API 的效果。这表明，模型的大小在情感分析这样简单的问题上也能产生显著差异。课后可以进行练习来进一步巩固所学知识。</p><h2 id="课后练习">课后练习</h2><p>我们在尝试使用 T5-base 这个模型之后，下了个判断认为大一点的模型效果更好。不过，其实我们并没有在整个数据集上使用 T5-small 这个模型做评测，你能试着修改一下代码，用 T5-small 测试一下整个数据集吗？测试下来的效果又是怎样的呢？</p><p>我们使用 Fasttext 的时候，把所有的单词向量平均一下，用来做情感分析效果很糟糕。那么什么样的分类问题，可以使用这样的方式呢？给你一个小提示，你觉得什么样的文本分类，只关心出现的单词是什么，而不关心它们的顺序？</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;大家好，我是茶桁。&lt;/p&gt;
&lt;p&gt;在前两节课中，我们一起体验了 OpenAI 提供的 GPT-3.5 系列模型的两个核心接口。一个是获取文本的 Embedding 向量，另一个是根据提示语生成补全的文本内容。通过这两种方法，我们可以在零样本或少样本的情况下进行情感分析任务。然而，你可能会有两个疑问。首先，Embedding 不就是将文本转换为向量吗？为什么不直接使用开源模型（如Word2Vec、Bert）而要调用 OpenAI 的 API 呢？在我们的情感分析任务中，我们进行了一些简化处理。一方面，我们排除了相对中性的评分（3分）；另一方面，我们将1分、2分和4分、5分合并，将原本需要判断5个分类的问题简化了。那么，如果我们想要准确预测多个分类，是否也能如此简单呢？&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>03 提示语，做个聊天机器人</title>
    <link href="https://hivan.me/Let-s-Build-a-Chatbot/"/>
    <id>https://hivan.me/Let-s-Build-a-Chatbot/</id>
    <published>2023-05-11T08:43:15.000Z</published>
    <updated>2023-06-01T13:34:25.038Z</updated>
    
    <content type="html"><![CDATA[<p>大家好，我是茶桁。</p><p>在本次讲座中，我们将研究 OpenAI 提供的 Completion API 接口。你可能已经体验过与 ChatGPT 进行聊天的乐趣，或是利用它帮我们编写各种文本材料的便利。现在，我们可以从这个需求出发，进一步探索 Completion API 的具体用法，以及如何利用它来生成更加优质的文本。</p><span id="more"></span><h2 id="ai-客服">AI 客服</h2><p>在我了解人工智能和自然语言生成之前，我听说过智能客服，然而我并没有亲身体验过。我想象中，智能客服的回答应该是按照固定的模板进行生成的，这样的缺点就是每次回答都会是一样的。虽然可以设计多个模板来轮换着表达相同的意思，但是最多也就是三四个模板，整体的体验可能会比较呆板。</p><p>但是，随着 GPT 这样的生成式语言模型的出现，我们现在可以让 AI 自动根据我们的需求来撰写文案了。只要我们将需求提供给 Open AI 提供的 Completion 接口，就能自动获得一段文本，大大提升了智能客服的质量和效率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">openai.api_key = <span class="string">&quot;OPENAI_API_KEY&quot;</span></span><br><span class="line">COMPLETION_MODEL = <span class="string">&quot;text-davinci-003&quot;</span></span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&quot;请你用朋友的语气回复给到客户，并称他为“亲”，他的订单已经发货在路上了，预计在3天之内会送达，订单号256180193592，我们很抱歉因为天气的原因物流时间比原来长，感谢他选购我们的商品。&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_response</span>(<span class="params">prompt, temperature = <span class="number">1.0</span></span>):</span><br><span class="line">    completions = openai.Completion.create (</span><br><span class="line">        engine=COMPLETION_MODEL,</span><br><span class="line">        prompt=prompt,</span><br><span class="line">        max_tokens=<span class="number">1024</span>,</span><br><span class="line">        n=<span class="number">1</span>,</span><br><span class="line">        stop=<span class="literal">None</span>,</span><br><span class="line">        temperature=temperature,</span><br><span class="line">    )</span><br><span class="line">    message = completions.choices[<span class="number">0</span>].text</span><br><span class="line">    <span class="keyword">return</span> message</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(get_response(prompt))</span><br></pre></td></tr></table></figure><p>可以得到回复：</p><blockquote><p>亲，你好！多谢选购我们的商品！你的订单（订单号256180193592）已发货，预计3天之内能送达。由于天气原因，物流前来比原定的时间会晚一点，抱歉给您带来不便。</p></blockquote><img src="/Let-s-Build-a-Chatbot/20230601164516.png" class="" title="img"><p>让我们再来调用并输出一次：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(get_response(prompt))</span><br></pre></td></tr></table></figure><p>得到回复：</p><blockquote><p>亲，您的订单256180193592已经发货，预计3天将全部到达，不过由于天气原因送货可能比期望晚点，这里深表歉意。感谢您选购我们的商品，祝您购物愉快！</p></blockquote><p><strong>我们可以看到，原始提示语是一模一样的，甚至都没改动标点符号。在连续两次调用之后，我们得到了含义相同、遣词造句不同的两份结果。</strong></p><p>我提供了一个简单的示例代码，利用 Open AI 提供的 Completion API 接口来生成一段合适的智能客服回复。我们给 AI 提供了一个简短的提示语，要求 AI 用亲切的语气告诉客户，订单因为天气原因延迟了。我们还指定了一些语言风格上的要求，比如要用朋友的口吻称呼客户为"亲"。</p><p>通过连续两次使用相同的参数，我们可以看到 AI 确实满足了我们的要求，给出了一段符合逻辑的回复。这其中包括两个难点：AI 能够正确地使用"亲"称呼客户，以及在回复中提取出了订单号并返回给了客户。</p><p>值得注意的是，尽管两次返回的回复意思相同，但是具体的语言表达又有所不同，这归功于我们使用了一个名为 temperature 的参数。该参数的取值范围在 0-2 之间，代表输出结果的随机性或者多样性。我们在示例代码中选择了 1.0 这个中等值，从而保证每次生成的内容都有一定的差异。如果我们将该参数设置为 0，那么每次输出的结果将非常相似。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(get_response(prompt, <span class="number">0.0</span>))</span><br></pre></td></tr></table></figure><blockquote><p>亲，您的订单256180193592已经发货，预计3天内会送达，由于天气原因，物流时间比原来长，我们深表歉意。感谢您选购我们的商品，祝您购物愉快！</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(get_response(prompt, <span class="number">0.0</span>))</span><br></pre></td></tr></table></figure><blockquote><p>亲，您的订单256180193592已经发货，预计3天内会送达，由于天气原因，物流时间比原来长，我们深表歉意。感谢您选购我们的商品，祝您购物愉快！</p></blockquote><p>这个参数该怎么设置，取决于实际使用的场景。如果对应的场景比较严肃，不希望出现差错，那么设得低一点比较合适，比如银行客服的场景。如果场景没那么严肃，有趣更加重要，比如讲笑话的机器人，那么就可以设置得高一些。</p><p>除了 <code>temperature</code> 之外，让我们来看看Completion这个接口的其他参数：</p><ol type="1"><li><code>engine</code>：指定使用的 Open AI 模型的名称。不同的模型具有不同的特点和能力，例如生成式语言模型 GPT-3 和面向任务的模型 DALL-E，而这个参数允许您选择您想要使用的模型。在这个例子中，我们使用的是 text-davinci-003 引擎，这是 Open AI 的一个最高级的语言模型，能够根据您的提示语生成出最准确、最多样化的回答。</li><li><code>prompt</code>：指定模型要生成的文本的初始提示语。这个参数通常是由人类提供的，以指导模型生成适当的文本。在这个例子中，prompt 是一个字符串，要求 AI 以亲切的口吻告诉用户订单被延迟了，同时使用“亲”这个称呼。</li><li><code>max_tokens</code>：这个参数是用来限制 AI 生成的文本长度的，它指定了允许生成文本中的 token 的最大数量。Token 是对输入进行分割后得到的最小文本单元，可以是单词或者短语等等。在这个例子中，max_tokens 设为 4096，这是 text-davinci-003 引擎允许的最大值。需要注意的是，这个数量既包括您输入的 prompt，也包括 AI 产生的回答，两者加起来不能超过 4096 个 token。比如，如果您的 prompt 已经包含 1000 个 token，那么您设置的 max_tokens 就不能超过 3096。</li><li><code>n</code>：表示您希望模型生成多少个不同的文本。在这个例子中，我们将 n 设为 1，因为我们只需要生成一段回复。如果您需要多个备选项，可以将 n 设为较高的数字，然后选择其中最适合的一个。</li><li><code>stop</code>：指定生成的文本中的一个字符串，用于指示模型停止生成。例如，您可以将 stop 设为“”以指示模型在生成两个换行符时停止。在这个例子中，我们将 stop 设为逗号“，”，这意味着模型在生成提示语中的“亲”后停止生成。</li></ol><p>当然并不是只有这五个参数，只是目前而言我们无需了解那么多，有钻研精神又比较感兴趣的，可以去参看<a href="https://platform.openai.com/docs/api-reference/completions/create">官方文档</a>。当然是英文版的，有能力的直接看，看不太懂的，还记得我之前在<a href="https://www.hivan.me/Understanding%20and%20Utilizing%20AI#7b5b4c64ff164636a92957c2d16a9963">导读里教大家的方法吗</a>？</p><h2 id="聊天机器人">聊天机器人</h2><p>在前面我们了解了如何使用提示语让 AI 回答一个问题。但是，如何让 AI 与人进行对话呢？特别是如何实现多轮对话，让 GPT 能够记住上下文。例如，当用户问我们 "iPhone14 拍照好不好" 时，我们回答 "很好"，然后用户又问 "它的价格是多少" 时，我们需要理解用户指的是上面提到的 iPhone。</p><p>对于聊天机器人而言，理解当前用户的句子是不够的，必须能够理解整个上下文才行。然而，GPT 的模型要实现支持多轮对话也不复杂。我们只需要在提示语中增加一些小小的工作就可以了。</p><p>要实现问答，我们只需在提示语中，在问题之前加上 "Q: " 表示这是一个问题，然后另起一行，加上 "A: " 表示我们需要一个回答，Completion 的接口就会回答你在 "Q: " 中提出的问题。例如，当我们问 AI "麻婆豆腐怎么做" 时，它会一步一步地列出制作步骤。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">question =  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Q : 麻婆豆腐该怎么做？</span></span><br><span class="line"><span class="string">A : </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(get_response(question))</span><br></pre></td></tr></table></figure><blockquote><p>准备好所需食材：豆腐、肉末（猪肉或鸡肉）、精盐、葱，蒜，料酒、生抽和老抽，花椒粉、白胡椒粉、辣椒粉、糖、淀粉和麻油。</p></blockquote><p>要让 AI 完成多轮对话其实也很容易，我们只需要把之前对话的内容也放到提示语里面，把整个上下文都提供给 AI。这样 AI 就能够自动根据上下文，回答下一个问题。例如，如果你接着问“那蚝油牛肉呢？”，我们不要只是把这个问题传给 AI，而是将之前的对话也一并传给 AI，那么 AI 就能够理解你问的“那蚝油牛肉呢？”是想要知道怎么做，而不是关于购买或者价格的问题。</p><p>Q：麻婆豆腐该怎么做？</p><p>A：详细的麻婆豆腐的做法</p><p>Q：那蚝油牛肉呢？</p><p>A：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">question =  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Q : 麻婆豆腐该怎么做？</span></span><br><span class="line"><span class="string">A :</span></span><br><span class="line"><span class="string">准备好所需食材：豆腐、肉末（猪肉或鸡肉）、精盐、葱，蒜，料酒、生抽和老抽，花椒粉、白胡椒粉、辣椒粉、糖、淀粉和麻油。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">一、先将肉末和调料混合在一起：肉末、精盐、料酒、生抽、老抽、花椒粉、白胡椒粉、辣椒粉和麻油各适量。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">二、另外把豆腐切成小块，然后用淀粉抓着放入沸水锅中煮5-6分钟，煮至软熟。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">三、将肉末放入油锅，小火爆炒一会。加入蒜泥、葱泥继续炒香。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">四、将豆腐放入油锅中，加入糖、适量盐、酱油和水各适量，混合均匀，中火烧开再煮至汤汁稠浓即可。 </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Q: 那耗油牛肉呢？</span></span><br><span class="line"><span class="string">A: </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(get_response(question))</span><br></pre></td></tr></table></figure><blockquote><p>耗油牛肉的做法：</p></blockquote><p>下面是一个名为 food_chatbot 的完整 Python 代码，它可以从命令行读取你的问题，并回答你的问题。你可以继续提问，这样我们就可以把整个对话过程都发送给 AI 来回答。这样的话，你可以体验一下 AI 是否能够理解整个对话过程的上下文。如果你想退出，只需在需要提问时输入“bye”即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">openai.api_key = <span class="string">&quot;OPENAI_API_KEY&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ask_gpt3</span>(<span class="params">prompt</span>):</span><br><span class="line">    response = openai.Completion.create(</span><br><span class="line">        engine=<span class="string">&quot;text-davinci-003&quot;</span>,</span><br><span class="line">        prompt=prompt,</span><br><span class="line">        max_tokens=<span class="number">512</span>,</span><br><span class="line">        n=<span class="number">1</span>,</span><br><span class="line">        stop=<span class="literal">None</span>,</span><br><span class="line">        temperature=<span class="number">0.5</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    message = response.choices[<span class="number">0</span>].text.strip()</span><br><span class="line">    <span class="keyword">return</span> message</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;你好，我是一个聊天机器人，请你提出你的问题吧?&quot;</span>)</span><br><span class="line"></span><br><span class="line">questions = []</span><br><span class="line">answers = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_prompt</span>(<span class="params">prompt, questions, answers</span>):</span><br><span class="line">    num = <span class="built_in">len</span>(answers)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">        prompt += <span class="string">&quot;\n Q : &quot;</span> + questions[i]</span><br><span class="line">        prompt += <span class="string">&quot;\n A : &quot;</span> + answers[i]</span><br><span class="line">    prompt += <span class="string">&quot;\n Q : &quot;</span> + questions[num] + <span class="string">&quot;\n A : &quot;</span>        </span><br><span class="line">    <span class="keyword">return</span> prompt</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    user_input = <span class="built_in">input</span>(<span class="string">&quot;&gt; &quot;</span>)</span><br><span class="line">    questions.append(user_input)</span><br><span class="line">    <span class="keyword">if</span> user_input.lower() <span class="keyword">in</span> [<span class="string">&quot;bye&quot;</span>, <span class="string">&quot;goodbye&quot;</span>, <span class="string">&quot;exit&quot;</span>]:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Goodbye!&quot;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    prompt = generate_prompt(<span class="string">&quot;&quot;</span>, questions, answers)</span><br><span class="line"></span><br><span class="line">    answer = ask_gpt3(prompt)</span><br><span class="line">    <span class="built_in">print</span>(answer)</span><br><span class="line">    answers.append(answer)</span><br></pre></td></tr></table></figure><h2 id="ai解决情感分析问题">AI解决情感分析问题</h2><p>在上一讲中，我们介绍了使用向量距离来进行情感分析的方法，但这种方法需要一些机器学习和数学知识。那么，我们是否能够利用自然语言提示语的方式，让 AI 帮助我们完成情感分析呢？</p><p>让我们来尝试一下。我们可以告诉 AI 我们想要它帮助我们判断用户评论的情感是正面还是负面，并且提供两个例子，分别是正面和负面的评论，让 AI 通过这两个例子学习如何进行情感分析。然后，我们再给 AI 一条新的评论，看看它能否正确地判断这条评论的情感。</p><p>在这个过程中，我们只需要调用 Completion 的 API 一次，但需要将提示语分成三个部分：</p><p>第一部分是告诉 AI 我们要进行情感分析；</p><p>第二部分是提供两个例子，以"评论："开头后面跟着具体的评论，然后是以"情感："开头后面跟着这个例子的情感；</p><p>第三部分是给出要进行情感分析的评论，同样以"评论："开头跟着我们想要它判断的评论，然后是以"情感："开头但后面没有内容，因为我们等待 AI 给出判断结果。</p><p>详细代码和实现方法可以参考下面的示例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">prompts = <span class="string">&quot;&quot;&quot;判断一下用户的评论情感上是正面的还是负面的</span></span><br><span class="line"><span class="string">评论：120的刷新率会舒服很多，有重低音跑的感觉，特别爽，真是非常给力，特别屏幕也是超级好用，屏幕也很灵敏，使用丝滑顺畅，打游戏也很舒服拍照效果：拍照效果杠杠的，画质清晰，像素感人，发货快，包装严实，各方面都没破损，颜值很高，耐看，打开网页丝滑顺畅，游戏不掉帧，分辨率高，功能很齐全。拍照很强大，照片成像很细腻，反应速度也很快，苹果系统真的很好用，运行速度：</span></span><br><span class="line"><span class="string">14的运行速度非常快，打游戏真的太流畅了，真的非常喜欢这个手机。颜值高，手感好，拍照清晰，内存大，运行速度快，待机时间长满意太喜欢了，手感不错，拍照效果也很好，运行速度很快，音效很好，续航强，手机手感很好，电池续航也不错。其他特色：整体特别满意。外观很好看，手感拿着很有质感，屏幕看着非常好，拍照效果很不错，像素高了，新机拍照片怎么都好看。5G高速运行，超长待机一整天。</span></span><br><span class="line"><span class="string">情感：正面</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">评论：刚买完就降价，卡着降价保护时间降价</span></span><br><span class="line"><span class="string">情感：负面</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">good_case = prompts + <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">评论：外形外观：银色外观好看。</span></span><br><span class="line"><span class="string">屏幕音效：音效非常好。</span></span><br><span class="line"><span class="string">拍照效果：拍照效果优秀，完全能够满足日常需要。</span></span><br><span class="line"><span class="string">运行速度：运行速度快，高刷无延迟。</span></span><br><span class="line"><span class="string">情感：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(get_response(good_case))</span><br></pre></td></tr></table></figure><blockquote><p>正面</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bad_case = prompts + <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">评论：充电口有划痕，有强迫症的建议选黄道吉日抽奖</span></span><br><span class="line"><span class="string">情感</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(get_response(bad_case))</span><br></pre></td></tr></table></figure><blockquote><p>负面</p></blockquote><img src="/Let-s-Build-a-Chatbot/Untitled.png" class="" title="img"><p>我们重新从京东商城的 iPhone 评论区随机找两个和上次不太一样的好评和差评，可以看到，结果是准确的。这是不是很棒？我们不需要任何机器学习的相关知识，用几句话就能够轻松搞定情感分析问题。</p><img src="/Let-s-Build-a-Chatbot/20230601164529.png" class="" title="img"><blockquote><p>常见的大模型的上下文学习能力，通过几个例子，就能回答正确的结果。</p></blockquote><p>而上面这个“给一个任务描述、给少数几个例子、给需要解决的问题”这样三个步骤的组合，也是大语言模型里使用提示语的常见套路。一般我们称之为 Few-Shots Learning（少样本学习），也就是给一个或者少数几个例子，AI 就能够举一反三，回答我们的问题。</p><h2 id="小结">小结</h2><p>在本节中，我们介绍了如何巧妙地利用提示语来让 AI 完成多种任务，通过三个例子向大家展示了 Completion 接口的强大之处。第一个例子中，我们让 AI 重写一段话，通过指定模型引擎和最大 token 数量等参数，实现了高效自然语言生成。第二个例子中，我们利用 Q 和 A 的提示语将对话历史记录传递给 AI，实现了多轮对话和上下文理解。最后一个例子中，我们将正面和负面情感的例子传递给 AI，实现了情感分析。可以看到，通过巧妙运用提示语，我们能够轻松地让大语言模型完成各种任务，这也是我们迈向通用人工智能的第一步。</p><h2 id="课后练习">课后练习</h2><p>上一讲我给出了一些数据集，通过向量距离来进行正面和负面情感的判断。那么，你能不能试着用这一讲的 Few-Shots Learning 的方法，在对应的数据集上也运行一下，看看通过这种方式进行情感分析的准确率是多少？</p><blockquote><p>在使用 GPT 类型的模型的时候，提示语非常重要。所以，有人专门制作了一个叫做 <a href="https://chrome.google.com/webstore/detail/aiprm-for-chatgpt/ojnbohmppadfgpejeebfnmnknjdlckgj">AIPRM 的浏览器插件</a>，包含了很多总结出来有效的提示语。你可以试着安装，看看别人都用了什么有趣的提示语。关于更多ChatGPT浏览器插件，可以看看<a href="https://www.hivan.me/Best%20ChatGPT%20Chrome%20Extension">这篇文章</a></p></blockquote><p><a href="https://github.com/hivandu/GPT_develop/blob/main/03_completion_api.ipynb">本节课程代码</a>可以在Github上找到</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;大家好，我是茶桁。&lt;/p&gt;
&lt;p&gt;在本次讲座中，我们将研究 OpenAI 提供的 Completion API 接口。你可能已经体验过与 ChatGPT 进行聊天的乐趣，或是利用它帮我们编写各种文本材料的便利。现在，我们可以从这个需求出发，进一步探索 Completion API 的具体用法，以及如何利用它来生成更加优质的文本。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>最佳 ChatGPT Chrome 扩展程序</title>
    <link href="https://hivan.me/Best-ChatGPT-Chrome-Extension/"/>
    <id>https://hivan.me/Best-ChatGPT-Chrome-Extension/</id>
    <published>2023-05-10T08:38:41.000Z</published>
    <updated>2023-06-02T03:46:52.509Z</updated>
    
    <content type="html"><![CDATA[<p>想要轻松访问ChatGPT吗？其中一个最佳方式是通过其一系列Chrome扩展程序。这些扩展程序还为您提供更好的使用ChatGPT的方法，包括帮助您编写更好的提示以获得更好的响应，或为ChatGPT授予搜索互联网的能力，从而提供对更多最新信息的访问。</p><p>这是您现在可以使用的最佳 ChatGPT Chrome 扩展程序。</p><span id="more"></span><h2 id="google-聊天-gpt"><strong>Google 聊天 GPT</strong></h2><p>嫉妒 Bing Chat 及其在您搜索时与您聊天或使用最新的 GPT-4 语言模型的能力吗？不需要。只需从网上商店获取适用于 Google 的 ChatGPT，您就可以将 ChatGPT 与 Google 搜索一起使用。事实上，只需进行一次普通的 Google 搜索，在结果旁边，您也会收到来自 ChatGPT 的回复，这有时比 Google 结果本身更有用。</p><h2 id="merlin"><strong>Merlin</strong></h2><p>想要 ChatGPT 在您上网的任何地方响应任何内容？Merlin 将 ChatGPT 带到任何网站，因此您可以突出显示任何文本或网页，并要求 ChatGPT 对其做出响应。您可以让它为您总结一个网页，或为您提供 YouTube 视频的纲要，这样您就不需要全部观看了。</p><img src="/Best-ChatGPT-Chrome-Extension/20230601164209.png" class="" title="img"><h2 id="talkberry"><strong>TalkBerry</strong></h2><p>为什么要在 ChatGPT 上打字，而不是直接与 ChatGPT 对话呢？使用 TalkBerry，您可以简单地与 ChatGPT 通话。只需安装扩展程序并确保您的麦克风或耳机已插入，即可开始使用。使用 TalkBerry，您可以节省大量在输入上的时间，或者将 ChatGPT 用作语言导师，让它聆听并帮助您提高发音和语言理解能力。</p><h2 id="tweetgpt"><strong>TweetGPT</strong></h2><p>使用 TweetGPT 可以让您的社交媒体游戏更上一层楼。TweetGPT 是 ChatGPT 的插件，利用 AI 聊天机器人工具的强大功能，制作更有趣、更尖刻、更具吸引力或更友好的推文和回复。您可以选择要发布的主题、您的情绪基调和语言，ChatGPT 将完成剩下的工作。如果您对某些措辞不满意，您甚至可以在之后编辑该消息。</p><img src="/Best-ChatGPT-Chrome-Extension/20230601164220.png" class="" title="img"><h2 id="gpt-ez"><strong>GPT-EZ</strong></h2><p>如果您不喜欢 ChatGPT 界面并想将其更改为您自己的喜好，请尝试 GPT-EZ。它允许您自定义 ChatGPT 网站的 UI，包括配色方案、字体样式和其他选项。此外，它还可以让您更轻松地复制和继续与 ChatGPT 的对话，并让您更轻松地下载对话日志。</p><h2 id="snackprompt"><strong>SnackPrompt</strong></h2><p>通过使用一些评价最高的提示来充分利用 AI 聊天机器人。SnackPrompt 列出并排名全球其他聊天机器人用户的最佳提示，让您可以访问一些最新和最强大的 AI 功能。</p><img src="/Best-ChatGPT-Chrome-Extension/20230601164230.png" class="" title="img"><h2 id="webchatgpt"><strong>WebChatGPT</strong></h2><p>ChatGPT 的最大限制之一是它无法访问最近的信息。即使您使用的是最新的 GPT-4 语言模型，它仍然只能访问 2021 年之前的信息。使用 WebChatGPT，您可以让 ChatGPT 能够在网络上搜索更多最新的信息来源。从 Chrome 网上应用店获取扩展程序，在使用 ChatGPT 时只需将其打开即可享受这一方便的功能。</p><h2 id="youtube-summary"><strong>YouTube Summary</strong></h2><p>喜欢 YouTube 教程，但不想看完序言？让此 ChatGPT 扩展为您总结说明。只需从 YouTube 视频页面获取转录内容，然后将其输入到插件中，您就会立即获得摘要。它还适用于文章、电子邮件或科学论文。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;想要轻松访问ChatGPT吗？其中一个最佳方式是通过其一系列Chrome扩展程序。这些扩展程序还为您提供更好的使用ChatGPT的方法，包括帮助您编写更好的提示以获得更好的响应，或为ChatGPT授予搜索互联网的能力，从而提供对更多最新信息的访问。&lt;/p&gt;
&lt;p&gt;这是您现在可以使用的最佳 ChatGPT Chrome 扩展程序。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Chrome" scheme="https://hivan.me/tags/Chrome/"/>
    
    <category term="ChatGPT" scheme="https://hivan.me/tags/ChatGPT/"/>
    
  </entry>
  
  <entry>
    <title>02 大语言模型做情感分析</title>
    <link href="https://hivan.me/Large-language-model-for-sentiment-analysis/"/>
    <id>https://hivan.me/Large-language-model-for-sentiment-analysis/</id>
    <published>2023-05-09T08:32:50.000Z</published>
    <updated>2023-06-01T13:52:17.373Z</updated>
    
    <content type="html"><![CDATA[<p>上一节中，我们介绍了大型语言模型的接口非常简单，仅提供了Complete和Embedding两个接口。但这样看似简单的接口，实际上可以解决很多自然语言处理问题。例如，情感分析、文本分类、文章聚类、摘要生成、搜索等问题，都可以使用大型语言模型解决。接下来的几节课中，我们将介绍如何使用这两个简单的API来解决传统的自然语言处理问题。本节我们将从最常见的自然语言处理问题“情感分析”开始介绍，看看如何使用大型语言模型。</p><span id="more"></span><h2 id="传统的二分类方法朴素贝叶斯与逻辑回归">传统的二分类方法：朴素贝叶斯与逻辑回归</h2><p>朴素贝叶斯与逻辑回归可以用来解决“情感分析”问题。这些算法的基本思想是，根据给定的标记数据，学习一个分类器，用来将新的输入数据进行分类。对于情感分析问题，分类器的目标是将一段文字分为正面或负面情感。</p><p>在这种方法中，我们首先需要对一部分评论数据进行标记，也就是手动标注这些评论是正面还是负面的。然后，我们将这些标记好的数据喂给算法进行学习。学习的过程就是从标记好的数据中提取特征，然后学习一个分类器，这个分类器可以将新的输入数据进行分类。</p><p>朴素贝叶斯算法是一种基于贝叶斯定理和特征条件独立假设的算法。它的基本思想是，通过已知的特征条件概率来计算输入数据的分类概率，并选择具有最大概率的类别作为分类结果。在情感分析问题中，朴素贝叶斯算法通过计算评论中每个单词在正面评论和负面评论中出现的频率来进行分类。</p><p>逻辑回归是一种广泛应用于分类问题的统计学习方法，其本质是利用一组输入特征和权重参数来计算一个分数，然后将分数转化为概率值。在情感分析问题中，逻辑回归通过使用输入评论的特征和权重参数来计算这段评论属于正面或负面情感的概率。</p><p>需要注意的是，传统的二分类方法并不一定是最优解决方案。随着深度学习技术的发展，越来越多的研究表明，使用深度学习模型可以在情感分析问题上获得更好的性能。</p><p>朴素贝叶斯算法是一种基于贝叶斯定理和条件独立性假设的分类算法。在垃圾邮件分类中，它的基本思想是对于一封待分类的邮件，通过计算邮件中出现单词的概率，来判断它是否为垃圾邮件。具体地，朴素贝叶斯算法会先从一组已知分类的训练集中学习出每个单词在每个分类下的条件概率，然后根据贝叶斯定理，计算待分类邮件属于每个分类的后验概率，最终将待分类邮件归类到后验概率最大的分类中。</p><p>具体而言，朴素贝叶斯算法假设所有特征都是相互独立的，即一个特征的出现并不会影响其它特征的出现概率，这也是“朴素”这个词的由来。在垃圾邮件分类中，这意味着我们可以将一封邮件表示为一个由词汇组成的向量，其中向量的每个元素表示一个单词是否出现在邮件中，这个元素的值通常是0或1。然后，我们可以通过计算训练集中每个单词在垃圾邮件和非垃圾邮件中的出现概率，来计算一个待分类邮件属于垃圾邮件或非垃圾邮件的概率。</p><p>具体地，设邮件中出现的单词为w1, w2, ..., wn，将邮件表示为一个n维的向量X=(x1, x2, ..., xn)，其中xi=1表示单词wi出现在邮件中，xi=0表示单词wi未出现在邮件中。设邮件属于垃圾邮件的概率为P(S)，邮件属于非垃圾邮件的概率为P(H)，则有：</p><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.172ex;" xmlns="http://www.w3.org/2000/svg" width="26.647ex" height="5.475ex" role="img" focusable="false" viewBox="0 -1460 11778 2420"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(1785,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2063,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(2915,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3581.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(4637.6,0)"><g data-mml-node="mrow" transform="translate(220,710)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1992,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2270,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(2915,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3526.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(4526.4,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(5277.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(5666.4,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(6311.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(2379.7,-710)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1992,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="6900.4" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span> <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.172ex;" xmlns="http://www.w3.org/2000/svg" width="28.296ex" height="5.475ex" role="img" focusable="false" viewBox="0 -1460 12507 2420"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(2028,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2306,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(3158,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3824.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(4880.6,0)"><g data-mml-node="mrow" transform="translate(220,710)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1992,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2270,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(3158,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3769.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(4769.4,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(5520.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(5909.4,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(6797.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(2622.7,-710)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1992,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="7386.4" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span></p><p>其中，P(X|S)表示在垃圾邮件中，向量X出现的概率，即：</p><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="43.444ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 19202.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1992,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2270,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(2915,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3581.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(4637.6,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(5388.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(5777.6,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(6786.1,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(7064.1,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(7709.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(8320.3,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(9320.6,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(10071.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(10460.6,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(11469.1,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(11747.1,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(12392.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(12781.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mo" transform="translate(13559.1,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(14003.8,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(14448.4,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(14893.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(15671.1,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(16422.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(16811.1,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(17890.4,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(18168.4,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(18813.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></p><p>同理，P(X|H)表示在非垃圾邮件中，向量X出现的概率，即：</p><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="45.643ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 20174.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1992,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2270,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(3158,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3824.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(4880.6,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(5631.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(6020.6,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(7029.1,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(7307.1,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(8195.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(8806.3,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(9806.6,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(10557.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(10946.6,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(11955.1,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(12233.1,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(13121.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(13510.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mo" transform="translate(14288.1,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(14732.8,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(15177.4,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(15622.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(16400.1,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(17151.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(17540.1,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(18619.4,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(18897.4,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(19785.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></p><p>P(S)和P(H)分别表示训练集中垃圾邮件和非垃圾邮件的概率，可通过训练集中垃圾邮件和非垃圾邮件的数量计算得出。接下来，我们需要计算每个单词在垃圾邮件和非垃圾邮件中出现的概率，即P(wi|S)和P(wi|H)。</p><p>为了计算这些条件概率，我们需要统计训练集中所有垃圾邮件和非垃圾邮件中每个单词出现的次数，以及每个类别中单词的总数。然后，我们可以使用以下公式来计算每个单词在垃圾邮件和非垃圾邮件中出现的概率：</p><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -1.991ex;" xmlns="http://www.w3.org/2000/svg" width="53.684ex" height="5.115ex" role="img" focusable="false" viewBox="0 -1381 23728.5 2261"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1140,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(2183,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2461,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(3106,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3772.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(4828.5,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mtext"><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path data-c="75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" transform="translate(556,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1112,0)"></path><path data-c="62" d="M307 -11Q234 -11 168 55L158 37Q156 34 153 28T147 17T143 10L138 1L118 0H98V298Q98 599 97 603Q94 622 83 628T38 637H20V660Q20 683 22 683L32 684Q42 685 61 686T98 688Q115 689 135 690T165 693T176 694H179V543Q179 391 180 391L183 394Q186 397 192 401T207 411T228 421T254 431T286 439T323 442Q401 442 461 379T522 216Q522 115 458 52T307 -11ZM182 98Q182 97 187 90T196 79T206 67T218 55T233 44T250 35T271 29T295 26Q330 26 363 46T412 113Q424 148 424 212Q424 287 412 323Q385 405 300 405Q270 405 239 390T188 347L182 339V98Z" transform="translate(1945,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2501,0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(2945,0)"></path><path data-c="20" d="" transform="translate(3337,0)"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(3587,0)"></path><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(4087,0)"></path><path data-c="20" d="" transform="translate(4393,0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(4643,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(5032,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(5310,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(6143,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(6587,0)"></path><path data-c="A0" d="" transform="translate(6981,0)"></path></g><g data-mml-node="msub" transform="translate(7231,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mtext" transform="translate(8274,0)"><path data-c="A0" d=""></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(250,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(750,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(1306,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1862,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(2306,0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(2806,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(3198,0)"></path><path data-c="20" d="" transform="translate(3592,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(3842,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(4120,0)"></path><path data-c="20" d="" transform="translate(4676,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(4926,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(5320,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(5876,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(6376,0)"></path><path data-c="20" d="" transform="translate(7209,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(7459,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(7903,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(8736,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(9236,0)"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(9514,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(9792,0)"></path></g></g><g data-mml-node="mtext" transform="translate(1396.5,-686)"><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(389,0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(889,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(1278,0)"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(1778,0)"></path><path data-c="20" d="" transform="translate(2056,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(2306,0)"></path><path data-c="75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" transform="translate(2862,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(3418,0)"></path><path data-c="62" d="M307 -11Q234 -11 168 55L158 37Q156 34 153 28T147 17T143 10L138 1L118 0H98V298Q98 599 97 603Q94 622 83 628T38 637H20V660Q20 683 22 683L32 684Q42 685 61 686T98 688Q115 689 135 690T165 693T176 694H179V543Q179 391 180 391L183 394Q186 397 192 401T207 411T228 421T254 431T286 439T323 442Q401 442 461 379T522 216Q522 115 458 52T307 -11ZM182 98Q182 97 187 90T196 79T206 67T218 55T233 44T250 35T271 29T295 26Q330 26 363 46T412 113Q424 148 424 212Q424 287 412 323Q385 405 300 405Q270 405 239 390T188 347L182 339V98Z" transform="translate(4251,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(4807,0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(5251,0)"></path><path data-c="20" d="" transform="translate(5643,0)"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(5893,0)"></path><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(6393,0)"></path><path data-c="20" d="" transform="translate(6699,0)"></path><path data-c="77" d="M90 368Q84 378 76 380T40 385H18V431H24L43 430Q62 430 84 429T116 428Q206 428 221 431H229V385H215Q177 383 177 368Q177 367 221 239L265 113L339 328L333 345Q323 374 316 379Q308 384 278 385H258V431H264Q270 428 348 428Q439 428 454 431H461V385H452Q404 385 404 369Q404 366 418 324T449 234T481 143L496 100L537 219Q579 341 579 347Q579 363 564 373T530 385H522V431H529Q541 428 624 428Q692 428 698 431H703V385H697Q696 385 691 385T682 384Q635 377 619 334L559 161Q546 124 528 71Q508 12 503 1T487 -11H479Q460 -11 456 -4Q455 -3 407 133L361 267Q359 263 266 -4Q261 -11 243 -11H238Q225 -11 220 -3L90 368Z" transform="translate(6949,0)"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(7671,0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(8171,0)"></path><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(8563,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(9119,0)"></path><path data-c="20" d="" transform="translate(9513,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(9763,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(10041,0)"></path><path data-c="20" d="" transform="translate(10597,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(10847,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(11241,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(11797,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(12297,0)"></path><path data-c="20" d="" transform="translate(13130,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(13380,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(13824,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(14657,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(15157,0)"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(15435,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(15713,0)"></path></g><rect width="18660" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span> <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -1.577ex;" xmlns="http://www.w3.org/2000/svg" width="53.343ex" height="4.701ex" role="img" focusable="false" viewBox="0 -1381 23577.5 2078"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1140,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(2183,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2461,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(3349,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4015.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(5071.5,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mtext"><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path data-c="75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" transform="translate(556,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1112,0)"></path><path data-c="62" d="M307 -11Q234 -11 168 55L158 37Q156 34 153 28T147 17T143 10L138 1L118 0H98V298Q98 599 97 603Q94 622 83 628T38 637H20V660Q20 683 22 683L32 684Q42 685 61 686T98 688Q115 689 135 690T165 693T176 694H179V543Q179 391 180 391L183 394Q186 397 192 401T207 411T228 421T254 431T286 439T323 442Q401 442 461 379T522 216Q522 115 458 52T307 -11ZM182 98Q182 97 187 90T196 79T206 67T218 55T233 44T250 35T271 29T295 26Q330 26 363 46T412 113Q424 148 424 212Q424 287 412 323Q385 405 300 405Q270 405 239 390T188 347L182 339V98Z" transform="translate(1945,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2501,0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(2945,0)"></path><path data-c="20" d="" transform="translate(3337,0)"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(3587,0)"></path><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(4087,0)"></path><path data-c="20" d="" transform="translate(4393,0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(4643,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(5032,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(5310,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(6143,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(6587,0)"></path><path data-c="A0" d="" transform="translate(6981,0)"></path></g><g data-mml-node="msub" transform="translate(7231,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mtext" transform="translate(8274,0)"><path data-c="A0" d=""></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(250,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(750,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(1306,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1862,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(2306,0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(2806,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(3198,0)"></path><path data-c="20" d="" transform="translate(3592,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(3842,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(4120,0)"></path><path data-c="20" d="" transform="translate(4676,0)"></path><path data-c="68" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 124T102 167T103 217T103 272T103 329Q103 366 103 407T103 482T102 542T102 586T102 603Q99 622 88 628T43 637H25V660Q25 683 27 683L37 684Q47 685 66 686T103 688Q120 689 140 690T170 693T181 694H184V367Q244 442 328 442Q451 442 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(4926,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(5482,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(5982,0)"></path><path data-c="20" d="" transform="translate(6815,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(7065,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(7509,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(8342,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(8842,0)"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(9120,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(9398,0)"></path></g></g><g data-mml-node="mtext" transform="translate(1396.5,-686)"><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(389,0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(889,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(1278,0)"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(1778,0)"></path><path data-c="20" d="" transform="translate(2056,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(2306,0)"></path><path data-c="75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" transform="translate(2862,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(3418,0)"></path><path data-c="62" d="M307 -11Q234 -11 168 55L158 37Q156 34 153 28T147 17T143 10L138 1L118 0H98V298Q98 599 97 603Q94 622 83 628T38 637H20V660Q20 683 22 683L32 684Q42 685 61 686T98 688Q115 689 135 690T165 693T176 694H179V543Q179 391 180 391L183 394Q186 397 192 401T207 411T228 421T254 431T286 439T323 442Q401 442 461 379T522 216Q522 115 458 52T307 -11ZM182 98Q182 97 187 90T196 79T206 67T218 55T233 44T250 35T271 29T295 26Q330 26 363 46T412 113Q424 148 424 212Q424 287 412 323Q385 405 300 405Q270 405 239 390T188 347L182 339V98Z" transform="translate(4251,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(4807,0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(5251,0)"></path><path data-c="20" d="" transform="translate(5643,0)"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(5893,0)"></path><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(6393,0)"></path><path data-c="20" d="" transform="translate(6699,0)"></path><path data-c="77" d="M90 368Q84 378 76 380T40 385H18V431H24L43 430Q62 430 84 429T116 428Q206 428 221 431H229V385H215Q177 383 177 368Q177 367 221 239L265 113L339 328L333 345Q323 374 316 379Q308 384 278 385H258V431H264Q270 428 348 428Q439 428 454 431H461V385H452Q404 385 404 369Q404 366 418 324T449 234T481 143L496 100L537 219Q579 341 579 347Q579 363 564 373T530 385H522V431H529Q541 428 624 428Q692 428 698 431H703V385H697Q696 385 691 385T682 384Q635 377 619 334L559 161Q546 124 528 71Q508 12 503 1T487 -11H479Q460 -11 456 -4Q455 -3 407 133L361 267Q359 263 266 -4Q261 -11 243 -11H238Q225 -11 220 -3L90 368Z" transform="translate(6949,0)"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(7671,0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(8171,0)"></path><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(8563,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(9119,0)"></path><path data-c="20" d="" transform="translate(9513,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(9763,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(10041,0)"></path><path data-c="20" d="" transform="translate(10597,0)"></path><path data-c="68" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 124T102 167T103 217T103 272T103 329Q103 366 103 407T103 482T102 542T102 586T102 603Q99 622 88 628T43 637H25V660Q25 683 27 683L37 684Q47 685 66 686T103 688Q120 689 140 690T170 693T181 694H184V367Q244 442 328 442Q451 442 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(10847,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(11403,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(11903,0)"></path><path data-c="20" d="" transform="translate(12736,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(12986,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(13430,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(14263,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(14763,0)"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(15041,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(15319,0)"></path></g><rect width="18266" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span></p><p>在实际计算中，由于训练集中往往包含大量单词，每个单词的出现次数很少，为了避免概率为0的情况，我们通常会使用拉普拉斯平滑或其他平滑技术来调整概率值。</p><p>最终，我们可以使用以下公式来计算一封邮件是垃圾邮件的概率：</p><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="51.346ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 22694.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(1785,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2063,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2529,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3407,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(3936,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4281,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(4579,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5245.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(6301.6,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(7052.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7441.6,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(8086.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(8697.8,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(9198,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(9949,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(10338,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(11490.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(11768.6,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(12413.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(13024.8,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(13525,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(14276,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(14665,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(15817.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(16095.6,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(16740.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(17129.6,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mo" transform="translate(17407.6,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(17852.2,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(18296.9,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(18741.6,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(19019.6,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(19770.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(20159.6,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(21382.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(21660.8,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(22305.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></p><p>其中，email表示待分类的邮件，w1、w2、...、wn表示邮件中出现的单词。类似地，我们可以计算邮件为非垃圾邮件的概率：</p><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="54.095ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 23909.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(2028,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2306,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2772,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3650,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(4179,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4524,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(4822,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5488.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(6544.6,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(7295.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7684.6,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(8572.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(9183.8,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(9684,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(10435,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(10824,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(11976.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(12254.6,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(13142.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(13753.8,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(14254,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(15005,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(15394,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(16546.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(16824.6,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(17712.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(18101.6,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mo" transform="translate(18379.6,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(18824.2,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(19268.9,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(19713.6,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(19991.6,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(20742.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(21131.6,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(22354.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(22632.8,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(23520.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></p><p>然后，我们可以比较这两个概率的大小，将邮件分类为垃圾邮件或非垃圾邮件。这就是朴素贝叶斯分类算法的基本流程。</p><p>一般来说，如果一个词语在差评里出现的概率比好评里高得多，那这个词语所在的评论，就更有可能是一个差评。</p><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.172ex;" xmlns="http://www.w3.org/2000/svg" width="19.07ex" height="5.475ex" role="img" focusable="false" viewBox="0 -1460 8428.7 2420"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mo" transform="translate(1573,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1851,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2423,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3089.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(4145.6,0)"><g data-mml-node="mrow" transform="translate(220,710)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1712,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1990,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mo" transform="translate(2423,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="msub" transform="translate(2812,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(675,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g><g data-mml-node="mrow" transform="translate(1091.1,-710)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1712,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="4043.2" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span> <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="46.624ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 20608 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mo" transform="translate(1573,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1851,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(2703,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3369.8,0)"><path data-c="221D" d="M56 124T56 216T107 375T238 442Q260 442 280 438T319 425T352 407T382 385T406 361T427 336T442 315T455 297T462 285L469 297Q555 442 679 442Q687 442 722 437V398H718Q710 400 694 400Q657 400 623 383T567 343T527 294T503 253T495 235Q495 231 520 192T554 143Q625 44 696 44Q717 44 719 46H722V-5Q695 -11 678 -11Q552 -11 457 141Q455 145 454 146L447 134Q362 -11 235 -11Q157 -11 107 56ZM93 213Q93 143 126 87T220 31Q258 31 292 48T349 88T389 137T413 178T421 196Q421 200 396 239T362 288Q322 345 288 366T213 387Q163 387 128 337T93 213Z"></path></g><g data-mml-node="mi" transform="translate(4425.6,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(5176.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(5565.6,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(6574.1,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(6852.1,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mo" transform="translate(7285.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(7896.3,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(8896.6,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(9647.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(10036.6,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(11045.1,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(11323.1,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mo" transform="translate(11756.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(12145.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mo" transform="translate(12923.1,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(13367.8,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(13812.4,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(14257.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(15035.1,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(15786.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(16175.1,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(17254.4,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(17532.4,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mo" transform="translate(17965.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(18576.6,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="msub" transform="translate(19576.8,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(675,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g></g></svg></mjx-container></span></p><p>假设我们有一个训练集包含4封邮件，其中2封是垃圾邮件，2封不是垃圾邮件。训练集里的邮件包含以下单词：</p><table><thead><tr class="header"><th>序号</th><th>邮件类型</th><th>出现的单词</th></tr></thead><tbody><tr class="odd"><td>Email A</td><td>垃圾邮件</td><td>buy, money, offer, secret</td></tr><tr class="even"><td>Email B</td><td>垃圾邮件</td><td>buy, secret, sell, money</td></tr><tr class="odd"><td>Email C</td><td>普通邮件</td><td>offer, book, sell</td></tr><tr class="even"><td>Email D</td><td>普通邮件</td><td>book, study, exam</td></tr></tbody></table><p>然后来了一封新邮件，里面的单词是：buy、money、sell。</p><p>通过计算在垃圾邮件和非垃圾邮件中这些单词出现的概率，我们可以预测这封新邮件是垃圾邮件还是非垃圾邮件。</p><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="24.431ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 10798.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(1569,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2141,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2908.8,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(3464.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">垃</text></g><g data-mml-node="mi" transform="translate(4464.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">圾</text></g><g data-mml-node="mo" transform="translate(5464.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6131.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(7187.1,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(7687.1,0)"><path data-c="F7" d="M318 466Q318 500 339 518T386 537Q418 537 438 517T458 466Q458 438 440 417T388 396Q355 396 337 417T318 466ZM56 237T56 250T70 270H706Q721 262 721 250T706 230H70Q56 237 56 250ZM318 34Q318 68 339 86T386 105Q418 105 438 85T458 34Q458 6 440 -15T388 -36Q355 -36 337 -15T318 34Z"></path></g><g data-mml-node="mn" transform="translate(8465.1,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(9242.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(10298.7,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container></span> <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="27.662ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 12226.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2018,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(2503,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3103,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(3569,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(4336.8,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(4892.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">垃</text></g><g data-mml-node="mi" transform="translate(5892.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">圾</text></g><g data-mml-node="mo" transform="translate(6892.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(7559.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(8615.1,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(9115.1,0)"><path data-c="F7" d="M318 466Q318 500 339 518T386 537Q418 537 438 517T458 466Q458 438 440 417T388 396Q355 396 337 417T318 466ZM56 237T56 250T70 270H706Q721 262 721 250T706 230H70Q56 237 56 250ZM318 34Q318 68 339 86T386 105Q418 105 438 85T458 34Q458 6 440 -15T388 -36Q355 -36 337 -15T318 34Z"></path></g><g data-mml-node="mn" transform="translate(9893.1,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(10670.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(11726.7,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container></span> <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="26.282ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 11616.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(1609,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2075,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(2373,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(2948.8,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(3504.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">垃</text></g><g data-mml-node="mi" transform="translate(4504.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">圾</text></g><g data-mml-node="mo" transform="translate(5504.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6171.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(7227.1,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(7727.1,0)"><path data-c="F7" d="M318 466Q318 500 339 518T386 537Q418 537 438 517T458 466Q458 438 440 417T388 396Q355 396 337 417T318 466ZM56 237T56 250T70 270H706Q721 262 721 250T706 230H70Q56 237 56 250ZM318 34Q318 68 339 86T386 105Q418 105 438 85T458 34Q458 6 440 -15T388 -36Q355 -36 337 -15T318 34Z"></path></g><g data-mml-node="mn" transform="translate(8505.1,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(9282.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(10338.7,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(778,0)"></path></g></g></g></svg></mjx-container></span> <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="24.431ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 10798.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(1569,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2141,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2908.8,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(3464.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">普</text></g><g data-mml-node="mi" transform="translate(4464.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">通</text></g><g data-mml-node="mo" transform="translate(5464.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6131.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(7187.1,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mi" transform="translate(7687.1,0)"><path data-c="F7" d="M318 466Q318 500 339 518T386 537Q418 537 438 517T458 466Q458 438 440 417T388 396Q355 396 337 417T318 466ZM56 237T56 250T70 270H706Q721 262 721 250T706 230H70Q56 237 56 250ZM318 34Q318 68 339 86T386 105Q418 105 438 85T458 34Q458 6 440 -15T388 -36Q355 -36 337 -15T318 34Z"></path></g><g data-mml-node="mn" transform="translate(8465.1,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(9242.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(10298.7,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container></span> <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="27.662ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 12226.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2018,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(2503,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3103,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(3569,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(4336.8,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(4892.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">普</text></g><g data-mml-node="mi" transform="translate(5892.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">通</text></g><g data-mml-node="mo" transform="translate(6892.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(7559.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(8615.1,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mi" transform="translate(9115.1,0)"><path data-c="F7" d="M318 466Q318 500 339 518T386 537Q418 537 438 517T458 466Q458 438 440 417T388 396Q355 396 337 417T318 466ZM56 237T56 250T70 270H706Q721 262 721 250T706 230H70Q56 237 56 250ZM318 34Q318 68 339 86T386 105Q418 105 438 85T458 34Q458 6 440 -15T388 -36Q355 -36 337 -15T318 34Z"></path></g><g data-mml-node="mn" transform="translate(9893.1,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(10670.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(11726.7,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container></span> <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="27.576ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 12188.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(1609,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2075,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(2373,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(2948.8,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(3504.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">普</text></g><g data-mml-node="mi" transform="translate(4504.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">通</text></g><g data-mml-node="mo" transform="translate(5504.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6171.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(7227.1,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(7727.1,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(8247.1,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(8592.1,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mn" transform="translate(9077.1,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(9854.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(10910.7,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(778,0)"></path></g></g></g></svg></mjx-container></span></p><p>然后我们把这封邮件里所有词语的条件概率用全概率公式乘起来，就得到了这封邮件是垃圾邮件还有普通邮件的概率。</p><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="94.513ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 41774.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">垃</text></g><g data-mml-node="mi" transform="translate(2140,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">圾</text></g><g data-mml-node="mo" transform="translate(3417.8,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(3973.6,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(4825.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5492.3,0)"><path data-c="221D" d="M56 124T56 216T107 375T238 442Q260 442 280 438T319 425T352 407T382 385T406 361T427 336T442 315T455 297T462 285L469 297Q555 442 679 442Q687 442 722 437V398H718Q710 400 694 400Q657 400 623 383T567 343T527 294T503 253T495 235Q495 231 520 192T554 143Q625 44 696 44Q717 44 719 46H722V-5Q695 -11 678 -11Q552 -11 457 141Q455 145 454 146L447 134Q362 -11 235 -11Q157 -11 107 56ZM93 213Q93 143 126 87T220 31Q258 31 292 48T349 88T389 137T413 178T421 196Q421 200 396 239T362 288Q322 345 288 366T213 387Q163 387 128 337T93 213Z"></path></g><g data-mml-node="mi" transform="translate(6548.1,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(7299.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7688.1,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(8117.1,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(8689.1,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(9456.9,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(10012.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">垃</text></g><g data-mml-node="mi" transform="translate(11012.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">圾</text></g><g data-mml-node="mo" transform="translate(12012.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(12401.7,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(13179.7,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(13930.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(14319.7,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(15197.7,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(15682.7,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(16282.7,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(16748.7,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(17516.4,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(18072.2,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">垃</text></g><g data-mml-node="mi" transform="translate(19072.2,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">圾</text></g><g data-mml-node="mo" transform="translate(20072.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(20461.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(21239.2,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(21990.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(22379.2,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(22848.2,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(23314.2,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(23612.2,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(24188,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(24743.8,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">垃</text></g><g data-mml-node="mi" transform="translate(25743.8,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">圾</text></g><g data-mml-node="mo" transform="translate(26743.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(27132.8,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(27910.8,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(28661.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(29050.8,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">垃</text></g><g data-mml-node="mi" transform="translate(30050.8,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">圾</text></g><g data-mml-node="mo" transform="translate(31050.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(31717.6,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(32773.3,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(33273.3,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(34051.3,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(34551.3,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(35329.3,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(778,0)"></path></g><g data-mml-node="mi" transform="translate(36607.3,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(37385.3,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(38941.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(39996.9,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(778,0)"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(1278,0)"></path></g></g></g></svg></mjx-container></span></p><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="91.622ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 40496.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">普</text></g><g data-mml-node="mi" transform="translate(2140,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">通</text></g><g data-mml-node="mo" transform="translate(3417.8,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(3973.6,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(4825.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5492.3,0)"><path data-c="221D" d="M56 124T56 216T107 375T238 442Q260 442 280 438T319 425T352 407T382 385T406 361T427 336T442 315T455 297T462 285L469 297Q555 442 679 442Q687 442 722 437V398H718Q710 400 694 400Q657 400 623 383T567 343T527 294T503 253T495 235Q495 231 520 192T554 143Q625 44 696 44Q717 44 719 46H722V-5Q695 -11 678 -11Q552 -11 457 141Q455 145 454 146L447 134Q362 -11 235 -11Q157 -11 107 56ZM93 213Q93 143 126 87T220 31Q258 31 292 48T349 88T389 137T413 178T421 196Q421 200 396 239T362 288Q322 345 288 366T213 387Q163 387 128 337T93 213Z"></path></g><g data-mml-node="mi" transform="translate(6548.1,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(7299.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7688.1,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(8117.1,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(8689.1,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(9456.9,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(10012.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">普</text></g><g data-mml-node="mi" transform="translate(11012.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">通</text></g><g data-mml-node="mo" transform="translate(12012.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(12401.7,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(13179.7,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(13930.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(14319.7,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(15197.7,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(15682.7,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(16282.7,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(16748.7,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(17516.4,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(18072.2,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">普</text></g><g data-mml-node="mi" transform="translate(19072.2,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">通</text></g><g data-mml-node="mo" transform="translate(20072.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(20461.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(21239.2,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(21990.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(22379.2,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(22848.2,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(23314.2,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(23612.2,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(24188,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(24743.8,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">普</text></g><g data-mml-node="mi" transform="translate(25743.8,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">通</text></g><g data-mml-node="mo" transform="translate(26743.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(27132.8,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(27910.8,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(28661.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(29050.8,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">普</text></g><g data-mml-node="mi" transform="translate(30050.8,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">通</text></g><g data-mml-node="mo" transform="translate(31050.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(31717.6,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(32773.3,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mi" transform="translate(33273.3,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(34051.3,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mi" transform="translate(34551.3,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(35329.3,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(778,0)"></path></g><g data-mml-node="mi" transform="translate(36607.3,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(37385.3,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(38941.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(39996.9,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container></span></p><p>在这里，我们发现 P(垃圾∣X)&gt;P(普通∣X)，而且 P(普通∣X) 其实等于 0。那如果用朴素贝叶斯算法，我们就会认为这封邮件 100% 是垃圾邮件。</p><p>朴素贝叶斯算法是一种基于贝叶斯定理和条件独立性假设的分类算法，可以用来进行情感分析等分类问题。在使用朴素贝叶斯算法进行垃圾邮件分类时，通过计算每个单词在垃圾邮件和非垃圾邮件中出现的概率来判断一封邮件是否为垃圾邮件。不过，如果一封邮件中出现的单词不在训练集中，那么朴素贝叶斯算法就不能给出正确的分类结果。此外，像逻辑回归、随机森林等机器学习算法也可以用来进行分类，网上有很多其他人使用这些传统方法来设计情感分析的解决方案，可以在 Kaggle 这个机器学习比赛的网站里<a href="https://www.kaggle.com/code/ankumagawa/sentimental-analysis-using-naive-bayes-classifier">找到相关</a>的 Jupyter Notebook。虽然这些算法背后涉及到一些数学知识，但在进行实际的 AI 应用开发时，并不需要预先掌握这些知识，因为我们可以借助各种 AI 平台和工具来进行开发。</p><h2 id="替代传统方法特征工程与模型调参">替代传统方法：特征工程与模型调参</h2><p>传统的机器学习算法，在应用到实际问题时，往往需要进行特征工程和模型调参，这两个方面都需要一定的经验和技巧。</p><h3 id="特征工程">特征工程</h3><p>特征工程是机器学习中非常重要的一环，通过对数据进行加工和转换，从原始数据中提取有意义的特征，以便机器学习算法更好地理解和分类数据。在自然语言处理中，特征工程尤其重要，因为自然语言数据的特征通常是复杂且非结构化的。</p><p>举个例子，如果我们想对一段文本进行情感分析，我们不能仅仅依靠文本中是否出现特定的单词来判断，因为不同的单词可能在不同的语境下有不同的情感含义。比如，“这家餐馆太糟糕了，一点都不好吃”和“这家餐馆太好吃了，一点都不糟糕”这两句话，从意思上是完全相反的，但是它们都包含相同的单词。这时候，我们需要通过一些特征工程的方法来解决这个问题。</p><p>在自然语言处理中，常用的特征工程方法包括：n-gram模型、词袋模型、TF-IDF权重等。</p><p>其中，n-gram模型是一种将相邻的n个单词组合在一起形成新的单词序列的方法。通过n-gram模型，我们可以考虑更多单词之间的关系，而不仅仅是单个单词的出现情况。常见的n-gram模型包括bigram（二元模型）和trigram（三元模型）。以二元模型为例，当n=2时，我们会将相邻的两个单词组合在一起，从而得到一个新的词组，比如“太糟糕”和“不好吃”。这些词组可以更好地捕捉到单个单词无法表达的情感含义。</p><p>词袋模型则是一种将文本表示成一个包含其所有单词的向量的方法。在词袋模型中，我们将文本中的所有单词都列出来，形成一个向量，然后统计每个单词在文本中出现的频率，并将其对应到向量的相应位置。虽然词袋模型并不能考虑单词之间的关系，但是它可以捕捉到文本中每个单词的出现情况。</p><p>TF-IDF（词频-逆文档频率）权重是一种根据单词在文本中的频率和其在语料库中的文档频率来赋予单词权重的方法。在TF-IDF模型中，单词在文本中出现的次数越多，它的权重就越大，但是如果它在语料库中出现的文档频率越高，它的权重就越小。这种方法可以用来衡量一个单词在整个文本集合中的重要性，常用于文本分类、信息检索等任务中。TF-IDF权重计算公式如下：</p><p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.016ex;" xmlns="http://www.w3.org/2000/svg" width="19.512ex" height="5.09ex" role="img" focusable="false" viewBox="0 -1359 8624.5 2250"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="TeXAtom" transform="translate(977,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1840.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2895.8,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="msub" transform="translate(3256.8,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="TeXAtom" transform="translate(523,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4587.3,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(5587.6,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(5885.6,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(6370.6,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mfrac" transform="translate(6847.6,0)"><g data-mml-node="mi" transform="translate(444.5,676)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mrow" transform="translate(220,-686)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="msub" transform="translate(520,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><rect width="1537" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span></p><p>其中，<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="3.463ex" height="1.668ex" role="img" focusable="false" viewBox="0 -443 1530.9 737.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(623,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></g></svg></mjx-container></span>表示单词<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></span>在文本<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex;" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></svg></mjx-container></span>中的权重, <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="3.769ex" height="2.261ex" role="img" focusable="false" viewBox="0 -705 1665.9 999.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="msub" transform="translate(361,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="TeXAtom" transform="translate(523,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(623,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></g></svg></mjx-container></span>表示单词<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></span>在文本中<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex;" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 412 865"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></svg></mjx-container></span>出现的频率，<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g></g></svg></mjx-container></span>表示文本集合中的总文档数，<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="3.025ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 1337 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="msub" transform="translate(520,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></span>表示包含单词<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></span>的文档数。</p><p>在实际应用中，TF-IDF通常与其他特征一起使用。比如，在文本分类任务中，我们可以把TF-IDF作为文本特征之一，同时还可以加入词性标注、情感分析等特征来提高分类的准确性。在文本信息检索任务中，我们可以使用TF-IDF对查询和文档进行向量化，并通过计算它们之间的相似度来进行检索。</p><p>除了TF-IDF，还有一些其他常用的特征工程方法，比如词袋模型（Bag-of-Words）、词嵌入（Word Embedding）等，每种方法都有其优缺点和适用场景。特征工程是机器学习中非常重要的一环，正确地选择和处理特征可以大大提高模型的性能。</p><p>不同的特征工程方式，在不同的问题上效果不一样，比如我们做情感分析，可能就需要保留标点符号，因为像“！”这样的符号往往蕴含着强烈的情感特征。但是，这些种种细微的技巧，让我们在想要解决一个简单的情感分析问题时，也需要撰写大量文本处理的代码，还要了解针对当前特定场景的技巧，这非常依赖工程师的经验。</p><h3 id="机器学习相关经验">机器学习相关经验</h3><p>机器学习涉及到很多知识和技巧，包括数据处理、模型训练和评估等。在数据处理方面，我们需要将数据集划分为训练集、验证集和测试集，并进行数据清洗、特征选择等操作。在模型训练方面，我们需要选择适合问题的算法、调整模型超参数，避免过拟合和欠拟合等问题。在模型评估方面，我们需要使用合适的指标来评估模型的性能，比如 AUC、准确率、召回率等。此外，针对数据集不够大的情况，我们需要采用交叉验证等技术来提高模型的泛化能力。</p><p>但是，随着大语言模型的发展，我们已经不再需要具备这些丰富的机器学习经验了。大语言模型提供了 Completion 和 Embedding 这两个 API，我们可以用不到 10 行代码就能完成情感分析，并且能获得非常好的效果。这极大地降低了机器学习应用的门槛，使得更多人能够轻松地利用机器学习技术来解决各种实际问题。</p><h3 id="大语言模型轻松解决">大语言模型轻松解决</h3><p>通过大语言模型来进行情感分析，最简单的方式就是利用它提供的 Embedding 这个 API。这个 API 可以将任何一段文本转换为一个固定长度的向量，以此代表这段文本的语义信息。为了进行情感分析，我们需要事先计算“好评”和“差评”这两个标签的 Embedding。对于一段待分析的文本，我们可以通过 Embedding API 获取该文本的 Embedding。然后，我们将这个文本 Embedding 和“好评”以及“差评”的 Embedding 分别计算余弦相似度（Cosine Similarity），并将两个相似度之间的差值作为该文本的情感分数。如果分数大于 0，则说明这段文本更接近“好评”，反之则更接近“差评”。利用这种方法，我们可以在不需要大量经验和复杂特征工程的情况下，用不到 10 行代码快速地进行情感分析，并获得非常好的效果。</p><p>比如，咱们看一下两条惊动上购买iPhone的用户发表的评论。</p><blockquote><p>实际案例还是来自于徐文浩的文章</p></blockquote><img src="/Large-language-model-for-sentiment-analysis/20230601163807.png" class="" title="img"><p>这个使用大模型的方法一共有 20 行代码，我们看看它能否帮助我们快速对这两条评论进行情感分析。</p><blockquote><p>注意，在执行 <code>import</code> 的时候，经常会遇到这样的错误提示： <code>ModuleNotFoundError: No module named 'scipy'</code> 此时切换到你所使用的环境内，然后执行一遍安装就行了( <code>conda install -c conda-forge scipy</code> )，少数时候，可能你安装一个之后仍然会报缺少其他模块，继续安装。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> openai.embeddings_utils <span class="keyword">import</span> cosine_similarity, get_embedding</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取访问open ai的密钥</span></span><br><span class="line">openai.api_key = <span class="string">"这里输入你的API Key"</span></span><br><span class="line"><span class="comment"># 选择模型</span></span><br><span class="line">EMBEDDING_MODEL = <span class="string">"text-embedding-ada-002"</span></span><br><span class="line"></span><br><span class="line">positive_review = get_embedding(<span class="string">"好评"</span>)</span><br><span class="line">negative_review = get_embedding(<span class="string">"差评"</span>)</span><br><span class="line"></span><br><span class="line">positive_example = get_embedding(<span class="string">"买的银色版真的很好看，一天就到了，晚上就开始拿起来完系统很丝滑流畅，做工扎实，手感细腻，很精致哦苹果一如既往的好品质"</span>)</span><br><span class="line">negative_example = get_embedding(<span class="string">"降价厉害，保价不合理，不推荐"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_score</span>(<span class="params">sample_embedding</span>):</span><br><span class="line"><span class="keyword">return</span> cosine_similarity(sample_embedding, positive_review) - cosine_similarity(sample_embedding, negative_review)</span><br><span class="line"></span><br><span class="line">positive_score = get_score(positive_example)</span><br><span class="line">negative_score = get_score(negative_example)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"好评例子的评分 : %f"</span> % (positive_score))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"差评例子的评分 : %f"</span> % (negative_score))</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">好评例子的评分： <span class="number">0.070963</span></span><br><span class="line">差评例子的评分： -<span class="number">0.072895</span></span><br></pre></td></tr></table></figure><p>我们可以看到，通过大语言模型提供的 Embedding 相似度计算方法，可以非常简单地对评论进行情感分析。在对京东商品评论进行测试时，我们发现好评的相似度得分大于0，差评的得分小于0，验证了该方法的有效性。更进一步的，我们可以尝试使用这个方法来分析含义截然相反的评论，看看它能否准确判断。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">good_restraurant = get_embedding(<span class="string">"这家餐馆太好吃了，一点都不糟糕"</span>)</span><br><span class="line">bad_restraurant = get_embedding(<span class="string">"这家餐馆太糟糕了，一点都不好吃"</span>)</span><br><span class="line"></span><br><span class="line">good_score = get_score(good_restraurant)</span><br><span class="line">bad_score = get_score(bad_restraurant)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"好评餐馆的评分 : %f"</span> % (good_score))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"差评餐馆的评分 : %f"</span> % (bad_score))</span><br></pre></td></tr></table></figure><img src="/Large-language-model-for-sentiment-analysis/20230601163817.png" class="" title="img"><p>如图所示，我们输出结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">好评餐馆的评分 : <span class="number">0.062719</span></span><br><span class="line">差评餐馆的评分 : -<span class="number">0.074591</span></span><br></pre></td></tr></table></figure><p>可以看到，虽然两句话分别是“太好吃”“不糟糕”和“太糟糕”“不好吃”，其实词语都一样，但是大语言模型一样能够帮助我们判断出来他们的含义是不同的，一个更接近好评，一个更接近差评。</p><h3 id="更大的数据集上的真实案例">更大的数据集上的真实案例</h3><p>在前面的例子中，我们展示了利用大语言模型进行情感分析的简单方法，并验证了它在小规模数据上的有效性。那么在更大的数据集上，我们的方法是否仍然可靠呢？下面我们将以亚马逊提供的食品评论数据为例进行验证。</p><p>该数据集包含了用户对不同品牌的食品的评论以及所打的评分，我们可以将评分转化为好评和差评两个类别，以验证我们的方法准确度。具体而言，我们将1-2星的评分视为差评，4-5星的评分视为好评。为了避免重复计算，我们在数据集中已经提前保存了评论的Embedding信息。</p><p>利用Pandas读取CSV数据，我们可以将这些评论和对应的评分加载到内存中，并利用我们之前介绍的Embedding API计算评论的向量。通过计算评论向量与好评、差评向量之间的余弦距离，我们可以得到每个评论的相似度分数。最终，我们将这些分数与阈值进行比较，即可将评论分类为好评或差评。</p><p>重新调用 OpenAI 的 API 会很浪费钱，这个数据集里，我们将获取到的 Embedding 信息保存下来了，不需要再重新计算。</p><blockquote><p>模型和代码我都将上传到自己的Github上，有需要的可以<a href="https://github.com/hivandu/GPT_develop">自己获取</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">datafile_path = <span class="string">'./data/fine_food_reviews_with_embeddings_1k.csv'</span></span><br><span class="line"></span><br><span class="line">df = pd.read_csv(datafile_path)</span><br><span class="line">df[<span class="string">'embedding'</span>] = df.embedding.apply(<span class="built_in">eval</span>).apply(np.array)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将五星评级转换为二元情绪</span></span><br><span class="line">df = df[df.Score != <span class="number">3</span>]</span><br><span class="line">df[<span class="string">"sentiment"</span>] = df.Score.replace({<span class="number">1</span>: <span class="string">"negative"</span>, <span class="number">2</span>: <span class="string">"negative"</span>, <span class="number">4</span>:<span class="string">"positive"</span>, <span class="number">5</span>:<span class="string">"positive"</span>}</span><br></pre></td></tr></table></figure><p>每一条评论都用我们上面的方法，和一个预先设定好的好评和差评的文本去做对比，然后看它离哪个近一些。这里的好评和差评，我们写得稍微长了一点，分别是 “An Amazon review with a negative sentiment.” 和 “An Amazon review with a positive sentiment.”。</p><p>在计算完结果之后，我们利用 Scikit-learn 这个机器学习的库，将我们的预测值和实际用户打出的星数做个对比，然后输出对比结果。需要的代码，也就不到 20 行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> PrecisionRecallDisplay</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_embeddings_approach</span>(<span class="params"></span></span><br><span class="line"><span class="params">    labels = [<span class="string">'negative'</span>, <span class="string">'positive'</span>], </span></span><br><span class="line"><span class="params">    model = EMBEDDING_MODEL,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    label_embeddings = [get_embedding(label, engine=model) <span class="keyword">for</span> label <span class="keyword">in</span> labels]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">label_score</span>(<span class="params">review_embedding, label_embeddings</span>):</span><br><span class="line">        <span class="keyword">return</span> cosine_similarity(review_embedding, label_embeddings[<span class="number">1</span>]) - cosine_similarity(review_embedding, label_embeddings[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    probas = df[<span class="string">"embedding"</span>].apply(<span class="keyword">lambda</span> x: label_score(x, label_embeddings))</span><br><span class="line">    preds = probas.apply(<span class="keyword">lambda</span> x: <span class="string">'positive'</span> <span class="keyword">if</span> x&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="string">'negative'</span>)</span><br><span class="line"></span><br><span class="line">    report = classification_report(df.sentiment, preds)</span><br><span class="line">    <span class="built_in">print</span>(report)</span><br><span class="line"></span><br><span class="line">    display = PrecisionRecallDisplay.from_predictions(df.sentiment, probas, pos_label=<span class="string">'positive'</span>)</span><br><span class="line">    _ = display.ax_.set_title(<span class="string">"2-class Precision-Recall curve"</span>)</span><br><span class="line"></span><br><span class="line">evaluate_embeddings_approach(labels=[<span class="string">'An Amazon review with a negative sentiment.'</span>, <span class="string">'An Amazon review with a positive sentiment.'</span>])</span><br></pre></td></tr></table></figure><p>输出结果</p><img src="/Large-language-model-for-sentiment-analysis/20230601163823.png" class="" title="img"><p>我们可以看到通过简单的调用大模型接口计算向量相似度的方法可以达到相当高的精度，而不需要进行任何机器学习训练。现在，我们来看一下如何在具体数据集上使用这个方法进行情感分析。</p><p>我们使用的是亚马逊商品评论数据集，包含了约 130 万条评论以及对应的评分。我们将评分为 1 或 2 的评论视为负面评论，评分为 4 或 5 的评论视为正面评论，而评分为 3 的评论则排除在外。我们使用 Python 中的 Pandas 库读取数据集，再使用 OpenAI 提供的 Embedding API 获取评论的向量表示，并计算每个评论向量与正面评论和负面评论向量之间的余弦相似度。对于余弦相似度大于 0 的评论，我们认为其为正面评论，余弦相似度小于 0 的评论则认为其为负面评论。最后，我们通过比较模型的预测结果与真实标签的一致性来评估模型的性能。</p><h2 id="小结">小结</h2><p>本课程介绍了使用大型语言模型进行情感分析的零样本分类方法。这种方法不需要额外的训练数据，利用大型语言模型中预训练的“好评”和“差评”的概念信息，可以判断未见过的评论的情感极性。通过计算不同文本在大型语言模型中的嵌入距离，可以进行情感分析。使用这种方法可以轻松地在经典数据集上达到95%以上的准确度，并且降低了机器学习的门槛。利用 OpenAI的API可以在几分钟内获得所需的结果，这使得文本分类的应用变得更加容易。</p><p>在学习完本节课程后，希望你能够进一步巩固所学内容并掌握实践能力。因此，我们建议你进行一些课后练习，以加深对大模型应用于情感分析的理解和实战能力的提升。你可以尝试将本节课程中讲解的方法应用到其他数据集上，比如 Kaggle 提供的亚马逊耳机评论数据，看看是否也会有很好的效果。该数据集较大，你可以选择挑选几条数据进行尝试，因为 OpenAI 会对你的调用进行限速，免费的 Token 数量也比较有限。对于大批量数据的处理，我们会在后面进行详细讲解。</p><p>欢迎你将你的结果分享到评论区，也欢迎你把这节课分享给感兴趣的朋友。我们下节课再见！</p><p>数据集：https://www.kaggle.com/datasets/shitalkat/amazonearphonesreviews</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;上一节中，我们介绍了大型语言模型的接口非常简单，仅提供了Complete和Embedding两个接口。但这样看似简单的接口，实际上可以解决很多自然语言处理问题。例如，情感分析、文本分类、文章聚类、摘要生成、搜索等问题，都可以使用大型语言模型解决。接下来的几节课中，我们将介绍如何使用这两个简单的API来解决传统的自然语言处理问题。本节我们将从最常见的自然语言处理问题“情感分析”开始介绍，看看如何使用大型语言模型。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>01 进入AI大门，学会与其交谈</title>
    <link href="https://hivan.me/Enter-the-door-of-AI-learn-to-communicate-with-it/"/>
    <id>https://hivan.me/Enter-the-door-of-AI-learn-to-communicate-with-it/</id>
    <published>2023-05-07T08:26:43.000Z</published>
    <updated>2023-06-02T03:14:45.319Z</updated>
    
    <content type="html"><![CDATA[<p>不用问我都知道，你们一定是被ChatGPT的火热出圈导致的开始关注人工智能，也是由于此才看到我这篇文章。</p><p>放心，大家想要的我一定会给予，既然大家都想先认识ChatGPT，那么我们就从这个主题开始。</p><span id="more"></span><p>接下来，我们学学如何利用openAI的API来和其沟通。在整个使用过程中，我们都使用的是GPT-3.5的大预言模型。</p><p>在本课程中，我们将回答许多问题，例如，OpenAI 的 API 能够实现哪些神奇的事情？OpenAI 的产品被称为已经离通用人工智能（AGI）不远了，它们长什么样子？GPT-3 这样的模型与之前基于深度学习的自然语言处理解决方案有什么不同？我们将通过逐步解释这些问题，使您深入了解这个令人兴奋的领域。</p><p>无论您是否是一名程序员，您都可以从本课程中学习如何使用 AI 技术，尤其是大型语言模型，为您的项目和业务提供价值。</p><h2 id="基础工作">基础工作</h2><h3 id="创建帐号和api-key">创建帐号和API Key</h3><p>了开始学习本课程，您需要先注册一个可以使用 OpenAI 的 API 的账号。您可以通过注册入口进行注册。目前，OpenAI 尚未向中国大陆和香港地区开放，因此您需要自己寻找适当的解决方案进行注册。如果您有更好的解决方案，也欢迎在评论区分享。</p><img src="/Enter-the-door-of-AI-learn-to-communicate-with-it/20230601162842.png" class="" title="img"><p>注册账号后，您需要点击右上角的账号，然后进入 "View API Keys" 页面管理 API Keys。</p><img src="/Enter-the-door-of-AI-learn-to-communicate-with-it/20230601162846.png" class="" title="img"><p>您可以点击下方的 "+Create new secret key" 来创建一个新的 API Key。</p><img src="/Enter-the-door-of-AI-learn-to-communicate-with-it/20230601163129.png" class="" title="img"><p>您需要将此 API Key 存储在一个安全的位置，因为在后续的 OpenAI 接口调用中，需要使用此 API Key。</p><img src="/Enter-the-door-of-AI-learn-to-communicate-with-it/20230601163125.png" class="" title="img"><p>储存API Key留用这方便，我使用的是1Password，开了家庭版，很好用。</p><img src="/src=https:/qiniu.hivan.me/picGo/20230601163133.png%20img%20%25%7D%0A%3Cp%3E%EF%BF%BDM%0COpenAI%20:@%09M9%E8%8C%84(7%D0%9B%EF%BF%BD%205%20%EF%BF%BDC%08%EF%BF%BDe/%2018%20%EF%BF%BDC%09%EF%BF%BDM9%20API%20%EF%BF%BD%EF%BF%BD%0C%EF%BF%BD%1F%EF%BF%BDS%EF%BF%BD%20API%20%EF%BF%BD%EF%BF%BD%EF%BF%BDv%EF%BF%BD%10,%EF%BF%BD%0B%EF%BF%BDf%60%02%EF%BF%BD%EF%BF%BD%EF%BF%BD%EF%BF%BD(%EF%BF%BDE%EF%BF%BD%EF%BF%BD-%EF%BF%BD%00e%7F(d%20API%0C%19%00%EF%BF%BD%03QG%EF%BF%BD0%EF%BF%BD9&7%02%3C/p%3E%0A%3Cp%3E%EF%BF%BD%1A(,%EF%BF%BD%EF%BF%BD%10K%EF%BF%BD%0C%11%D1%B0M9&7%EF%BF%BD%EF%BF%BD%EF%BF%BD%EF%BF%BD%7F(M9%EF%BF%BDAPIM%EF%BF%BD%EF%BF%BD%0C%0D%C5%82d%0C%EF%BF%BD:APIM%EF%BF%BD%EF%BF%BDChatGPT%20Plus/$*/%EF%BF%BD%EF%BF%BD%EF%BF%BD%0C@%EF%BF%BDPlusv%0DI%0C%EF%BF%BDAPI%20M%EF%BF%BD%0C%60%00%EF%BF%BD%D1%9A%00%20%EF%BF%BD(a(%EF%BF%BD/%EF%BF%BD%60%EF%BF%BD%7F(%EF%BF%BD%02%3C/p%3E%0A%3Ch4%20id=*%EF%BF%BDa%22%3E*%EF%BF%BDa%3C/h4%3E" class="&lt;img" title="{% asset_img &#x2F;Enter-the-door-of-AI-learn-to-communicate-with-it&#x2F;20230601163144.png img"><h4 id="已绑卡">已绑卡</h4><img src="/Enter-the-door-of-AI-learn-to-communicate-with-it/20230601163148.png" class="" title="img"><p>关于绑卡这个事，可以自己在网上搜索看，办法总比问题多。不要找我，虽然我有渠道，但是我的渠道很贵，到时候说我骗人钱我可说不清楚。</p><h2 id="搭建环境">搭建环境</h2><p>既然是开发API应用，那必然是需要开发环境的。如果你自己会，那就最好不过了，如果不是太熟悉，可以参考一下我这篇文章：</p><p>这篇文章详细的介绍了在Mac内如何搭建AI环境，包括Tensorflow的安装等。</p><p>基本上，我们现在需要的是3.10 的Python环境，还有Conda（我习惯用这个），然后在本地安装好Jupyter lab, 如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create --name gpt python=3.10</span><br><span class="line">conda activate gpt</span><br><span class="line">conda install -c conda-forge jupyterlab ipywidgets openai</span><br></pre></td></tr></table></figure><p>这一段命令的意思是创建一个名为 <code>gpt</code>的python 3.10的开发环境，然后切换到这个环境里，再安装必要的包。</p><p>在后面的使用过程中，当然你可以选择jupyter notebook, 也可以和我一样，使用VSCode。</p><img src="/Enter-the-door-of-AI-learn-to-communicate-with-it/20230601163152.png" class="" title="img"><p>当然，你也可以选择Colab，其实这也是一个Jupyterlab，如果你不想本地搭建环境，那就直接使用Colab吧，不过注意一点，需要科学上网。就算你本地有环境，我还是建议你有些事后使用Colab，能用到一些免费的GPU资源，我的M1没有好的显卡支持，很多时候还是需要上Colab。</p><img src="/Enter-the-door-of-AI-learn-to-communicate-with-it/20230601163156.png" class="" title="img"><p>使用时候，记得要安装openAI的库，并且设置自己的API Key：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!pip install openai</span><br><span class="line">%<span class="built_in">env</span> OPENAI_API_KEY=<span class="string">&quot;这里输入你的API Key&quot;</span></span><br></pre></td></tr></table></figure><h3 id="测试一下">测试一下</h3><p>让我们现在开始依次写完这段代码，虽然截图内已经有了，但是还是让我们一步步来执行起来，这一段代码，并不是出自我之手，而且直接借鉴的徐文浩的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定API Key和模型</span></span><br><span class="line">openai.api_key = <span class="string">&quot;输入你自己的代码&quot;</span></span><br><span class="line">COMPLETION_MODEL = <span class="string">&quot;text-davinci-003&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定关键词和描述</span></span><br><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Consideration proudct : 工厂现货PVC充气青蛙夜市地摊热卖充气玩具发光蛙儿童水上玩具</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">1. Compose human readale product title used on Amazon in english within 20 words.</span></span><br><span class="line"><span class="string">2. Write 5 selling points for the products in Amazon.</span></span><br><span class="line"><span class="string">3. Evaluate a price range for this product in U.S.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Output the result in json format with three properties called title, selling_points and price_range</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写一个调用方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_response</span>(<span class="params">prompt</span>):</span><br><span class="line">    completions = openai.Completion.create (</span><br><span class="line">        engine=COMPLETION_MODEL,</span><br><span class="line">        prompt=prompt,</span><br><span class="line">        max_tokens=<span class="number">512</span>,</span><br><span class="line">        n=<span class="number">1</span>,</span><br><span class="line">        stop=<span class="literal">None</span>,</span><br><span class="line">        temperature=<span class="number">0.0</span>,        </span><br><span class="line">    )</span><br><span class="line">    message = completions.choices[<span class="number">0</span>].text</span><br><span class="line">    <span class="keyword">return</span> message</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用方法并打印最终结果</span></span><br><span class="line"><span class="built_in">print</span>(get_response(prompt))</span><br></pre></td></tr></table></figure><p>然后我们就可以看到返回了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;title&quot;</span>: <span class="string">&quot;Glow-in-the-Dark Inflatable PVC Frog Night Market Hot Selling Water Toy for Kids&quot;</span>,</span><br><span class="line">    <span class="string">&quot;selling_points&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;Made of durable PVC material&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Glow-in-the-dark design for night play&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Inflatable design for easy storage and transport&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Perfect for water play and outdoor activities&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Great gift for kids&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&quot;price_range&quot;</span>: <span class="string">&quot;$10 - $20&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码里面，我们调用了 OpenAI 的 Completion 接口，然后向它提了一个需求，也就是为一个我在 1688 上找到的中文商品名称做三件事情。</p><ol type="1"><li><p>为这个商品写一个适合在亚马逊上使用的英文标题。</p></li><li><p>给这个商品写 5 个卖点。</p></li><li><p>估计一下，这个商品在美国卖多少钱比较合适。</p></li></ol><p>同时，我们告诉 OpenAI，我们希望返回的结果是 JSON 格式的，并且上面的三个事情用 title、selling_points 和 price_range 三个字段返回。</p><p>神奇的是，OpenAI 真的理解了我们的需求，返回了一个符合我们要求的 JSON 字符串给我们。在这个过程中，它完成了好几件不同的事情。</p><p>第一个是翻译，我们给的商品名称是中文的，返回的内容是英文的。</p><p>第二个是理解你的语义去生成文本，我们这里希望它写一个在亚马逊电商平台上适合人读的标题，所以在返回的英文结果里面，AI 没有保留原文里有的“工厂现货”的含义，因为那个明显不适合在亚马逊这样的平台上作为标题。下面 5 条描述也没有包含“工厂现货”这样的信息。而且，其中的第三条卖点 “Inflatable design for easy storage and transport”，也就是作为一个充气的产品易于存放和运输，这一点其实是从“充气”这个信息 AI 推理出来的，原来的中文标题里并没有这样的信息。</p><p>第三个是利用 AI 自己有的知识给商品定价，这里它为这个商品定的价格是在 10～20 美元之间。而我用 “Glow-in-the-Dark frog” 在亚马逊里搜索，搜索结果的第一行里，就有一个 16 美元发光的青蛙。</p><p>最后是根据我们的要求把我们想要的结果，通过一个 JSON 结构化地返回给我们。而且，尽管我们没有提出要求，但是 AI 还是很贴心地把 5 个卖点放在了一个数组里，方便你后续只选取其中的几个来用。返回的结果是 JSON，这样方便了我们进一步利用返回结果。比如，我们就可以把这个结果解析之后存储到数据库里，然后展现给商品运营人员。</p><p>接下来，我们再看一个其他的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Man Utd must win trophies, says Ten Hag ahead of League Cup final</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">请将上面这句话的人名提取出来，并用json的方式展示出来</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(get_response(prompt))</span><br></pre></td></tr></table></figure><p>得到输出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;names&quot;</span>: [<span class="string">&quot;Ten Hag&quot;</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>看出AI干了什么吗？其实从中文中你能知道我需要AI做什么，而他完完全全输出了我想要的。</p><p>我们这里的两个例子，其实对应着很多不同的问题，其中就包括机器翻译、文本生成、知识推理、命名实体识别等等。在传统的机器学习领域，对于其中任何一个问题，都可能需要一个独立的机器学习模型。就算把这些模型都免费提供给你，把这些独立的机器学习模型组合到一起实现上面的效果，还需要海量的工程研发工作。没有一个数十人的团队，工作量根本看不到头。然而，OpenAI 通过一个包含 1750 亿参数的大语言模型，就能理解自然的语言输入，直接完成各种不同的问题。而这个让人惊艳的表现，也是让很多人惊呼“通用人工智能（AGI）要来了”的原因。</p><p>这两个例子虽然简单，但是咱们暂时先到此为止，记得课后好好练习。</p><ol type="1"><li><p>请将今天课程中提供的示例代码，在你搭建的开发环境中运行一下。</p></li><li><p>你可以去看一下 OpenAI 提供的<a href="https://platform.openai.com/examples/">示例</a>，找几个你感兴趣的用途，在上面的开发环境里运行体验一下，你也可以脑洞大开，尝试一些你想用 AI 解决的问题，看看 AI 能不能给出你想要的结果。</p></li></ol><h2 id="推荐阅读">推荐阅读</h2><p>推荐阅读如果你想知道 GPT 系列大模型到底是怎么回事儿，我推荐你去看一下李沐老师讲解 GPT 系列论文的视频 <a href="https://www.bilibili.com/video/BV1AF411b7xQ/">GPT、GPT-2、GPT-3 论文精读</a>，这个视频深入浅出，能够让你理解为什么现在 GPT 那么火热。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;不用问我都知道，你们一定是被ChatGPT的火热出圈导致的开始关注人工智能，也是由于此才看到我这篇文章。&lt;/p&gt;
&lt;p&gt;放心，大家想要的我一定会给予，既然大家都想先认识ChatGPT，那么我们就从这个主题开始。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>导读：了解AI并使用它/他/她们</title>
    <link href="https://hivan.me/Understanding_and_Utilizing_AI/"/>
    <id>https://hivan.me/Understanding_and_Utilizing_AI/</id>
    <published>2023-05-06T07:01:00.000Z</published>
    <updated>2023-06-02T03:17:58.609Z</updated>
    
    <content type="html"><![CDATA[<p>如果你想开始学习AI应用开发，那么在学习之前，有一些学前提醒需要注意。在当今AI爆发的时代，学习AI应用开发需要的学习方法和策略也发生了变化。本课程的目标是通过多尝试、多体验、多做头脑风暴的学习方法，帮助学生在短时间内掌握AI应用开发的基本技能。我们并不会传授过于深奥的数学和理论知识，而是会通过简单易学的API，让学生能够快速上手开发实用价值的AI应用。因此，在学习本课程的过程中，我们鼓励学生多尝试、多体验、多做头脑风暴，以更加轻松、快速地掌握AI应用开发的技能。如果你不知道如何开始，你可以使用Jupyter或者Golab这样的工具来帮助自己入门。</p><span id="more"></span><h2 id="多练习多尝试多交流">多练习，多尝试，多交流</h2><h3 id="多尝试运行和修改代码">1. 多尝试运行和修改代码</h3><p>为什么要多尝试运行和修改代码？因为这是学习AI最有效的方式之一。通过自己亲手运行代码并进行修改，可以更深入地理解算法和模型背后的原理，并从中学到许多实用的技巧。此外，尝试运行和修改代码也能够帮助你更好地掌握编程语言和工具。</p><p>而为了方便地进行代码实验，我们可以使用一些开源的工具。例如，Jupyter Notebook 是一个广泛使用的交互式笔记本工具，它支持多种编程语言，并且可以在本地运行。如果你不知道如何搭建环境，</p><img src="/Understanding_and_Utilizing_AI/20230601142010.png" class="" title="img"><p>除了本地启动Jupyter之外，你也可以直接使用微软的VSCode，可以直接调用本地Jupyter环境（推荐）。</p><img src="/Understanding_and_Utilizing_AI/20230601142015.png" class="" title="img"><p>也可以使用 Google 的 Colaboratory（简称 Colab） 这样的云端工具，只需要一个 Google 账号即可使用。</p><img src="/Understanding_and_Utilizing_AI/20230601142039.png" class="" title="img"><p>自然，这也不是全无门槛的，学会如何科学上网是必备技能。这一部分请原谅我无法教授，还是需要自行查找资料。</p><h3 id="多体验不同的ai工具">2. 多体验不同的AI工具</h3><p>在学习 AI 应用开发的过程中，你需要了解当前市场上涌现的海量 AI 应用。通过体验这些 AI 应用，你可以更好地了解 AI 的能力和应用场景，也能够了解到当前 AI 技术的发展状况。</p><p>例如，你可以尝试使用一些人工智能工具来完成自己的工作，比如使用自然语言处理的工具来帮助你写作、使用机器学习的工具来进行数据分析、使用计算机视觉的工具来进行图像处理等等。此外，你还可以体验一些常用的 AI 应用，比如语音助手、智能家居、智能客服、智能医疗等等。通过这些体验，你可以深入了解 AI 技术在实际场景中的应用和效果，从而更好地理解 AI 技术的价值和未来发展方向。</p><p>在课程中，你还可以从我推荐中了解到一些最新的 AI 应用。你可以注册账号、下载应用，多去体验一下这些 AI 应用，这不仅能够激发你学习课程的动力，也能够打开你自己利用 AI 大模型能力的思路。</p><p>总之，多去体验各类 AI 应用能够帮助你更好地了解 AI 技术的应用和发展现状，也能够激发你对 AI 技术的兴趣和热情，从而更好地进行学习和实践。</p><p>本地搭建<strong>Stable Diffusion</strong></p><blockquote><p>这里给大家一个小小的建议，尽量不要用线上的图片生成AI去生成商业图片，会有法律隐患的。而如果是在本地架设的情况下，这种问题基本就不存在了。</p></blockquote><h3 id="多交流">3. 多交流</h3><p>与周围的人以及朋友一起多做做头脑风暴，尝试寻找有趣的新产品的机会。事实上，这是一个非常好的建议。AI应用已经涌现出许多，但是有些应用只是简单地使用了现有的API，缺乏创意和创新。然而，还有很多应用具有独特的想法，有些甚至可以直接商业化。学习AI的目的在于学以致用，可以与身边对新一代AI应用有兴趣的人一起探讨，看看课程中介绍的各种方法和技巧能否用于不同的场景和角度。这才是学习这门课程的真正价值。当然，如果你对AI大模型的底层原理有兴趣，可以深入研究其中的数学原理和各种深度学习模型。现在，有能力构建大模型的人实在是太少了，而不是太多了。能够推动通用人工智能向前发展一小步，相信是所有AI从业者都梦寐以求的事情。所以，学习AI，不仅要学习知识，更要发挥创造力，发掘新的应用场景，才能真正做到学以致用。</p><p>[Stable Diffution 生成的填色图，完全可以变成一项生意，生成多张图做本书：Link](https://qiniu.hivan.me/picGo/<img src="/Understanding_and_Utilizing_AI/20230601142054.png" class="" title="img">Stable Diffution 生成的填色图，完全可以变成一项生意，生成多张图做本书：<a href="https://aituts.com/ai-generated-coloring-books/">Link</a></p><h2 id="使用ai工具改变你现在的工作方式">使用AI工具改变你现在的工作方式</h2><p>随着 ChatGPT、Whisper 和 Stable Diffusion 等强大的 AI 技术的出现，我们的学习和工作方式也需要跟着改变。现在，利用 AI 工具来改造自己的学习和工作流程已经成为一种趋势。通过将 AI 技术应用到各个方面，我们可以获得更加沉浸式的学习体验，同时也能够提高日常生活和工作的效率。在过去的几个月里，我自己也不断地研究和学习新技术，并通过 AI 工具来提高自己的效率。在这篇文章中，我想和大家分享一些我所使用的 AI 工具和优化流程。</p><p>ChatGPT，这是一种基于 GPT 技术的人工智能语言模型。我常常利用 ChatGPT 来帮助自己解决问题，比如在学习编程的过程中，我会输入一些代码，然后让 ChatGPT 来帮我检查代码的错误。ChatGPT 还可以用来进行翻译、摘要和生成文章等等。这种 AI 工具可以帮助我们更加高效地学习和工作。</p><p>Whisper，这是一种人工智能笔记工具。与传统笔记工具不同的是，Whisper 可以将我们所输入的笔记和文本转化为自然语言，并通过 AI 技术来优化笔记的布局和结构。这样一来，我们可以更加快速和方便地记录学习和工作中的重要信息，并将其整理成易于理解的形式。</p><p>Stable Diffusion，这是一种用于大规模数据处理和分析的 AI 工具。在我的研究工作中，我常常需要处理海量的数据，并对数据进行分析和建模。Stable Diffusion 的出现让我能够更加高效地处理数据，并且能够利用 AI 技术来进行数据建模和预测。</p><p>利用 AI 工具来改造学习和工作流程已经成为一种趋势。通过利用这些工具，我们可以更加高效地学习和工作，并且可以更加快速地解决问题。当然，这些工具只是 AI 技术应用的冰山一角，未来还将有更多更加强大和智能的 AI 工具出现，让我们拭目以待。</p><h3 id="如何使用-chatgpt-进行学习">如何使用 ChatGPT 进行学习</h3><p>随着 AI 技术的发展，ChatGPT 成为了许多人学习知识的“助教”。但是，有些人觉得 ChatGPT 没有多大用处，这可能是因为他们询问了过于宽泛的问题。实际上，ChatGPT 的作用是为我们提供有针对性的回答，只需询问具体问题即可。</p><p>当我们学习新知识时，我们可以请 ChatGPT 帮助我们解释我们不理解的内容。与搜索不同，ChatGPT 可以根据我们的追问提供更深入的解释，直到我们完全理解这个知识点为止。此外，ChatGPT 不仅可以解释概念，还可以解释代码。我们可以将需要解释的代码段粘贴到 ChatGPT 中，它将为我们提供详细的讲解。</p><p>此外，ChatGPT 作为一个 AI “助教”，它的知识广博、不知疲倦，极其耐心。我们不需要担心问题过于简单或产生心理压力，因为 ChatGPT 不会嫌麻烦或不耐烦。因此，与查找资料或询问他人相比，使用 ChatGPT 可以更高效地解决问题。</p><p>[问题具体化，将思考的过程交给自己，而获取知识交给ChatGPT，是我最常用的方式](https://qiniu.hivan.me/picGo/<img src="/Understanding_and_Utilizing_AI/20230601142105.png" class="" title="img">问题具体化，将思考的过程交给自己，而获取知识交给ChatGPT，是我最常用的方式</p><p>在 Poe 平台中，我们可以选择不同的语言模型，不仅可以使用 ChatGPT，还可以使用其他大型语言模型。这样，我们可以选择最适合自己的模型。</p><img src="/Understanding_and_Utilizing_AI/20230601142111.png" class="" title="img"><h3 id="学会使用工具获取额外资料">学会使用工具获取额外资料</h3><p>在当今的数字时代，英文资料已经成为许多行业中获取第一手信息的主要来源。特别是在技术领域，大量的技术文档、API文档和博客文章都是用英文写成的。虽然英文阅读能力是每个人在学习和工作中必须掌握的技能，但对于许多人来说，英文阅读还是比较吃力的。然而，随着机器翻译技术的不断提高，人们已经越来越多地使用翻译插件，将英文资料转换为中英对照版本。</p><p>DeepL是一种在线翻译工具，它使用了深度学习技术，可以对英文文本进行准确的翻译。使用DeepL，只需将需要翻译的文本复制粘贴到工具中，它就可以快速将其翻译成目标语言。另外，DeepL还可以通过浏览器插件的形式直接嵌入到浏览器中，当您访问英文网页时，它会自动将其翻译成您的语言。</p><img src="/Understanding_and_Utilizing_AI/20230601142118.png" class="" title="img"><p>使用翻译插件可以帮助我们快速浏览英文资料，同时避免了语言障碍。当我们遇到一些翻译不准确的地方，可以快速查看英文原文，确保对资料的理解和应用。同时，翻译插件的中英对照形式也让我们能够更快速地阅读英文资料，从而提高我们的阅读效率。</p><p>除了文本资料外，现在还有越来越多的音视频资料，如播客和视频等。这些最新的资料往往只有音频或视频版，而没有文字版。但是通过语音识别和文本摘要技术，我们也可以快速将音频和视频转换为文本，并且生成一个摘要。这样，我们就可以先快速浏览一遍摘要，决定是否值得去完整地听或看。</p><p>现在市面上有许多这样的浏览器插件，例如 Glarity，可以帮助我们快速总结视频内容，再来决定是否要看。对于像约翰卡马克这样的大神的访谈，我们可以使用 ChatGPT 背后的语言模型来生成一个摘要，以快速浏览视频内容，确定是否值得花时间去看。</p><h3 id="如何通过ai来阅读论文">如何通过AI来阅读论文</h3><p>在科技飞速发展的今天，不断学习新知识，跟上最新的技术进展是非常必要的。读论文是获取新知识的好方法。然而，阅读一篇论文是一项费时费力的工作，因为它通常包含大量的专业术语、公式和图表。但是，随着大型语言模型的出现，我们现在可以借助AI来阅读论文。</p><p>现在有很多工具可以帮助我们阅读论文。例如，<a href="https://time.geekbang.com/column/article/641726#:~:text=现在，都会通过-,scispace,-这个网站，把">scispace</a>是一个网站，可以将要阅读的论文上传到其中。然后，我们可以向AI提出问题，以快速了解论文讲解了什么内容。scispace内置了许多你可能会关心的问题，并且可以直接选择回答的语言。此外，在阅读过程中，AI可以对公式、图表等内容进行详细解释，这些工具都可以大大降低阅读论文的门槛，提高掌握这些复杂知识的效率。</p><img src="/Understanding_and_Utilizing_AI/20230601142126.png" class="" title="img"><p>除了scispace之外，还有许多其他的工具可以帮助我们阅读论文。例如，<a href="https://time.geekbang.com/column/article/641726#:~:text=的工具。比如-,ChatPDF,-就可以让">ChatPDF</a>是一个可以对PDF文件进行小结和提问的工具。将各种分析报告上传至ChatPDF中，可以快速获取所需的信息。</p><h3 id="利用-ai-写代码-demo提高生产效率的新工具">利用 AI 写代码 Demo：提高生产效率的新工具</h3><p>GitHub Copilot 是一个利用 AI 技术帮助工程师写代码的工具。通过将需求描述给 ChatGPT，它可以快速生成可用的代码，帮助工程师节省时间和精力。使用 Copilot 写代码的体验非常棒，只需输入注释或代码的开头，Copilot 就能为你生成完整的代码。</p><p>对于一些简单的函数调用等胶水代码，Copilot 十有八九是能帮上忙的。即使有些代码不够完美，以它为基础改造比从头开始写更快。当你需要使用一些不熟悉的包时，Copilot 尤其有用。</p><p>如果你是一个工程师，安装 Copilot 并使用它写代码是提高生产效率的好方法。此外，使用 ChatGPT 和 Copilot 来帮助写 Demo 代码，可以帮助你快速实验需求，而不必费时查找文档和阅读教程。这些工具能让你更轻松地完成工作，提高你的生产力和效率。</p><p>在实际使用中，你可能需要花一些时间来熟悉和调整 Copilot 生成的代码。但是，一旦你熟悉了 Copilot，它将成为你编程工作中最有价值的助手之一。</p><p>当然，我是一个穷人，写代码并不是我的主业，所以我一般都使用ChatGPT来完成我为数不多的需求：</p><img src="/Understanding_and_Utilizing_AI/20230601142132.png" class="" title="img"><p>ChatGPT和Copilot的代码正确率以及BUG率比多数工程师都要来的优秀，自然，也包括我自己。</p><h3 id="使用ai获取灵感">使用AI获取灵感</h3><p>现在随着人工智能技术的快速发展，越来越多的AI写作工具涌现出来，让人们在创作过程中更加得心应手。其中，最让人印象深刻的就是AI如何帮助我们获取灵感。虽然AI还没有完全替代人类的思维，但在寻找灵感方面，它们已经展现出了惊人的能力。</p><p>在实际的写作中，很多人并不依赖AI产生内容，但是当缺少灵感的时候，AI可以作为一个非常好的助手。例如，当你在围绕一个主题思考写作内容时，可以尝试使用notion.ai等工具寻找灵感。虽然这些工具的很多主意并不新颖，但它们往往可以给你带来意想不到的角度和思路。</p><img src="/Understanding_and_Utilizing_AI/20230601142142.png" class="" title="img"><p>此外，你还可以尝试通过人设的不同来让AI从另一个角度帮助你思考问题。例如，为ChatGPT设置一个不同的人设，让它模拟某个领域的专家来帮助你做头脑风暴。这个时候，虽然你只有一个人在思考，但是你却可以组织一个各路大神汇聚的团队帮助你思考问题，让你得到更多有价值的点子。</p><p>不仅如此，对于不同领域的人群，还有一些专门的AI工具可以帮助他们获取灵感。例如，Midjourney、Dall-E 2等工具可以让设计师在创作过程中更加得心应手，快速地制作出优秀的设计作品。</p><h3 id="接受它别被它替代">接受它，别被它替代</h3><p>随着科技的快速发展，现代社会正在经历着巨大的变革。许多传统行业和工作岗位正在面临被自动化和数字化取代的风险。因此，我们必须积极地适应新的技术，以免被时代抛弃。尤其是人工智能的发展，不仅给我们的生活带来了诸多便利，也对我们的工作和职业规划产生了重大影响。因此，了解人工智能的基本原理和应用场景，具备使用和开发人工智能的能力，成为未来职场竞争力的关键。同时，我们也需要不断地学习和更新知识，跟上时代的步伐，不断提升自己的技能和能力，以应对快速变化的社会和市场。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;如果你想开始学习AI应用开发，那么在学习之前，有一些学前提醒需要注意。在当今AI爆发的时代，学习AI应用开发需要的学习方法和策略也发生了变化。本课程的目标是通过多尝试、多体验、多做头脑风暴的学习方法，帮助学生在短时间内掌握AI应用开发的基本技能。我们并不会传授过于深奥的数学和理论知识，而是会通过简单易学的API，让学生能够快速上手开发实用价值的AI应用。因此，在学习本课程的过程中，我们鼓励学生多尝试、多体验、多做头脑风暴，以更加轻松、快速地掌握AI应用开发的技能。如果你不知道如何开始，你可以使用Jupyter或者Golab这样的工具来帮助自己入门。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>系列课程：从零开始接触人工智能大模型（介绍）</title>
    <link href="https://hivan.me/Gettin_started_with_large-scale_artificial_intelligence_models_from_scratch/"/>
    <id>https://hivan.me/Gettin_started_with_large-scale_artificial_intelligence_models_from_scratch/</id>
    <published>2023-05-05T06:41:03.000Z</published>
    <updated>2023-06-01T13:59:21.027Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>整个系列课程内容虽然为自己所写，但是参考了bothub 创始人徐文浩的课程《AI 大模型之美》</p></blockquote><p>人工智能是计算机科学领域中最具前瞻性和影响力的技术之一。它是一种智慧型算法，能够模拟人类的思维过程，处理大量的数据和信息，从而发现隐藏在其中的规律和趋势。人工智能的应用范围非常广泛，包括语音识别、图像识别、自然语言处理、机器翻译、智能推荐、智能问答、自动驾驶等等。</p><span id="more"></span><p>人工智能的发展历程可以追溯到上个世纪五六十年代。当时，计算机科学家们开始思考如何让计算机能够像人类一样思考和行动。1956年，美国达特茅斯学院举办了一次名为“人工智能”的会议，正式提出了人工智能的概念。自此以后，人工智能的研究和应用就成为了计算机科学领域的一项重要任务。</p><p>随着科技的不断进步，人工智能技术得到了长足的发展。各种机器学习算法、深度学习算法、开源的软件包以及云平台提供的解决方案不断涌现，为企业提供了各种智能化的产品和服务。例如，通过图像识别技术，我们可以将人脸识别、车牌识别、病变识别等技术应用于安防、交通、医疗等领域；通过自然语言处理技术，我们可以实现智能客服、智能翻译、智能问答等功能，提升用户体验和效率；通过机器学习技术，我们可以实现推荐系统、广告精准投放等功能，提高市场竞争力。</p><p>然而，人工智能领域仍然存在着挑战和困难。其中一个难点就是“有多少人工就有多少智能”这个诅咒。很多“智能”都来自于大量的人工数据标注和硬编码的业务规则，导致人工智能在某些特殊情况下表现得像“人工智障”。因此，如何提高人工智能的效率和性能，仍然是人工智能领域需要解决的问题之一。</p><p>去年 12 月，我第一次尝试与 ChatGPT 进行交互。一开始我并没有对这个新的 AI 聊天机器人抱有太高的期待，毕竟以前的聊天机器人总是表现得像“人工智障”。但是，ChatGPT 证明了我的想法是错误的。在与它交流了几分钟后，我决定让它帮我写一些 Bash 脚本和 SQL 代码。我很惊讶地发现，它不仅完全理解了我的需求，还精确地写出了我需要的复杂 SQL 代码。这次体验让我对人工智能的潜力有了新的认识，也让我更加期待未来人工智能的发展。</p><img src="/Gettin_started_with_large-scale_artificial_intelligence_models_from_scratch/20230514084241.png" class="" title="img"><p>对于复杂的需要窗口函数的 SQL，ChatGPT 写得比我快多了。</p><p>从年前到目前（5月份）为止，我一直在体验市面上大部分的人工智能工具，例如最近非常火热的ChatGPT等。这样的体验让我对人工智能的潜力有了新的认识，也让我开始思考各行业未来的前景，并提出了一些担忧。当我们看到人工智能技术不断取得突破性进展的同时，也会担心它是否会取代人类的工作，进而给社会带来不稳定的因素。然而，我相信人工智能的发展是为了更好地服务人类，而非取代人类。我们需要更多人去了解和掌握人工智能技术，这样才能更好地应对未来的挑战，发挥人工智能技术的最大价值。</p><p>基于此，我想让更多人开始接触人工智能，并且学会如何利用人工智能，更甚为学习新一代AI应用编程。因此，我想介绍一门系列课程：从零开始接触人工智能大模型。该课程将介绍人工智能的基本概念、常见应用场景以及如何利用最新的AI技术构建自己的AI应用。我们将从浅入深地讲解人工智能相关的知识，帮助每个人都能够轻松上手，并且学会如何应用到自己的领域中去。不仅仅是算法工程师和机器学习研究人员，每个工程师都可以快速学习并参与开发新一代的AI应用。我相信，学会开发新一代AI应用是每个软件开发行业从业者都值得学习的，无论是产品经理还是工程师，乃至于行业之外的业务人员，都应该拥抱新的AI浪潮。</p><p>在学习的过程中，我们将讨论人工智能的应用场景，例如语音识别、图像识别、自然语言处理、机器翻译、智能推荐、智能问答、自动驾驶等等。这将有助于我们了解人工智能技术在不同领域的应用，从而更好地把握未来的发展方向。同时，我们也会学习最新的人工智能技术，例如大模型、自监督学习等等。这些技术的出现，为人工智能的应用提供了更加广阔的空间和深度。</p><p>在这个充满挑战和机遇的时代，我们需要准备好迎接未来的挑战。学习人工智能技术，是每个软件开发行业从业者都需要掌握的技能。无论是产品经理还是工程师，乃至于行业之外的业务人员，都应该拥抱新的AI浪潮，学习开发新一代的AI应用。我相信，通过学习新一代的AI应用编程，我们能够更好地应对未来的挑战，为我们的生活和工作带来更多的便利和机遇。</p><h3 id="学习成本那么高给我一个理由先">学习成本那么高，给我一个理由先</h3><p>这个应该是普遍的一个想法，其实对于此，我将不仅给你一个理由，而是给你三个：</p><h4 id="开发门槛降低人人可学习ai应用开发">1. 开发门槛降低，人人可学习AI应用开发</h4><p>人人都应该学习如何开发新一代 AI 应用，因为这一轮的 AI 浪潮里，开发新的 AI 应用的门槛大大降低了。过去，AI 应用开发是一个门槛比较高的领域，需要掌握大量的机器学习和深度学习的知识，了解各种基础模型，使用各种机器学习的编程框架，以及理解在实际应用里锤炼机器学习的各种实战技巧和模型。对于没有相关经验的人来说，不花上个一两年时间，你可能很难用 AI 算法做出有价值的产品。</p><p>但是现在，随着预训练好的大型基础模型的出现，以及这些模型的能力通过开放 API 的形式提供出来，即使没有任何机器学习的理论知识，你只需要一两天时间，就能做出一个能解决实际问题的 AI 应用。比如，最近在 GitHub 上就能看到很多工程师，花上 1-2 天时间就做出来的图书翻译、人工智能语音对话的应用。</p><p>这样的开发方式，让更多的人有机会参与到 AI 应用的开发中来。无论你是产品经理、UI/UE 设计师、前端开发、后端开发还是大数据团队的人员，都可以通过学习一些基本的 AI 应用开发技能，为自己的职业生涯增添新的技能和竞争力。特别是在当前的科技革命和数字化转型浪潮下，AI 技术已经逐渐渗透到各个行业中，很多企业已经开始了 AI 落地实践，而能够掌握 AI 技术的人才也逐渐成为各个行业中的稀缺资源。因此，学习如何开发新一代 AI 应用，也是提升自己职业竞争力的一种重要途径。</p><p>最后，学习如何开发新一代 AI 应用还可以让我们更好地了解 AI 技术的本质和应用，拓宽我们的知识面和视野。AI 技术正在改变我们的生活和工作方式，了解和掌握这些技术，也有助于我们更好地适应未来的发展和变化。</p><p>学习如何开发新一代 AI 应用对于个人的职业发展和未来非常重要，因为 AI 技术已经开始在各个行业得到广泛应用。无论你从事什么行业，都可以利用 AI 技术来提高效率、降低成本、提供更好的服务，并在竞争中脱颖而出。掌握 AI 技术也可以让你在未来的就业市场上更有竞争力，拥有更广泛的职业选择。因此，学习如何开发新一代 AI 应用可以为个人的职业发展和未来打下坚实的基础。</p><h4 id="站在巨人的肩膀上">2. 站在巨人的肩膀上</h4><p>随着人工智能技术的迅猛发展，AI应用开发的范围也越来越广泛，涉及到自然语言处理、计算机视觉、语音识别等多个领域。这一轮的AI浪潮已经开始让我们看到了通用人工智能（AGI）的雏形，AI应用的覆盖领域被大大扩展了，几乎任何一个问题都有机会通过AI来解决优化。</p><p>过去，机器学习模型的应用通常局限于某一个细分领域上的进步，而且对于每一个具体问题都要单独收集数据、训练单独的机器学习模型来解决里面某一个小问题。然而，随着计算能力的提高和模型规模的增加，现在拥有海量参数的大模型已经开始成为主流。例如，2020年发布的GPT-3模型拥有1750亿个参数，可以无需任何微调，就能解决情感分析、命名实体识别、机器翻译等一系列的自然语言处理问题。同时，对于很多AI没有见过的问题，只要通过自然语言给到AI几个例子，通过"小样本学习"，AI就能给出正确的回答。这意味着，一个大模型可以一次性解决不同类型的很多问题。</p><p>在计算机视觉上，像2021年OpenAI发布的CLIP模型也有类似的效果。通过4亿个（图片、文本）对的训练，对于图片的分类可以任意扩展，而不需要预先标注。这样的模型使得我们对于图片的分类不再局限于预先的人工数据标注的类别，而是可以扩展到任何类别上去。</p><p>这种发展趋势使得AI应用开发的门槛逐渐降低，使得普通人也能够参与到AI应用的开发中来。无论你所在的行业和领域，都有机会通过简单的AI应用开发，提升效率和产出。同时，了解和掌握AI技术也成为了一种职业竞争力，可以帮助你更好地适应未来的工作环境。</p><p>总之，AI技术的广泛应用和快速发展已经让AI应用开发成为一个非常重要的技能。了解AI技术的人，无论是在工作中还是在日常生活中，都会受益匪浅。</p><h4 id="人工智能对我们的工作的影响">3. 人工智能对我们的工作的影响</h4><p>人工智能（AI）已经开始以多种方式改变我们的生活。我们已经习惯了依赖AI进行日常任务，如编写代码、翻译文本，甚至为文章生成图像。然而，AI的影响超出了我们的个人生活，它将对我们的工作产生重大影响。</p><p>随着AI的不断发展，它不可避免地将接管许多以前由人类执行的任务。公司已经在使用AI来优化产品描述、搜索算法和其他曾经是人工工作者领域的任务。这无疑会导致工作的流失，并改变许多人的工作性质。</p><p>尽管存在工作流失的可能性，但那些拥抱AI的人无疑将会获得好处。使用AI的团队和公司很可能会看到更高的效率和生产力，从而导致更大的产出和成功。无论您是产品经理、工程师、运营专家还是平面设计师，AI的出现都将从根本上改变您的工作性质。</p><p>AI将作为助手，帮助我们完成简单的基于知识的任务，甚至提供创造性的灵感。事实上，有些人将AI的发展与工业革命相比，标志着我们生活和工作方式的根本变化。虽然这可能会对一些工人造成危机，但有机会拥抱这种变化并学习在以AI为驱动的未来需求的新技能。</p><p>正如2008年App Store的发布创造了对移动应用程序开发人员的需求一样，AI革命为那些愿意学习和适应的人带来了新的机会。无论是获得机器学习、数据科学还是其他与AI相关的领域的专业知识，那些积极应对不断变化的就业市场的人无疑会蓬勃发展。</p><p>AI对我们的工作的影响不容小觑。尽管有些人可能将其视为危机，但有机会拥抱这种变化并学习在以AI为驱动的未来需求的新技能。每个人都可以决定如何应对这种变化，以及他们是否会抓住它带来的机会。</p><h3 id="如何学习呢">如何学习呢？</h3><p>新一代AI应用开发是一个快速发展的领域，需要不断更新自己的知识和技能。而通过实践学习是最有效的方法之一，因为它可以让你在实际解决问题的过程中学习和掌握技能。</p><p>这门课程采用实践教学的方式，让学生能够亲自动手解决一系列实际问题，如情感分析、记忆聊天机器人、图像搜索等。学生们将通过编写几行或几十行的代码来解决这些问题，并在在线Notebook的环境下进行代码运行，无需搭建复杂的开发环境。即使你是产品经理或业务方，也可以轻松地体验到新一代AI应用的开发过程，从而更好地理解和掌握其工作原理。</p><p>除了OpenAI的API外，这门课程还涵盖了语音、视觉等应用场景，包括语音识别、语音合成、AI绘画等。学生们将了解到如何使用开源模型以及如何根据自己的数据微调这些模型，从而更好地满足不同场景下的需求。</p><p>此外，这门课程还将探讨AI应用的套路和方法，例如分类、搜索、推荐、问答等问题。学生们将学习如何使用现有模型的能力来解决这些问题，并将这些方法和套路应用到现有的业务系统中，以提高应用的体验和效率。</p><p>随着课程的深入，学生们还将学习如何组合多个API、开源模型和开源库来解决复杂的真实问题。例如，如果你想实现一个电商客服，不仅需要检索知识库和问答的能力，还需要连接现有的订单和物流信息的能力。学生们将学习如何在AI应用的开发过程中将复杂的业务流程串联起来，以更好地应对实际问题。</p><h3 id="拥抱新时代接受通用人工智能">拥抱新时代，接受“通用人工智能”</h3><p>随着人工智能技术的不断进步，越来越多的人开始认识到“通用人工智能”（AGI）的重要性和可能性。如今，AGI已经不再是一个遥不可及的概念，而是一个即将到来的现实。我们可以看到，各个领域的科学家和工程师正在努力推进AGI的研究和应用，希望通过人工智能技术的创新和发展来实现这一目标。</p><p>在这个变化迅速的时代，我们需要尽快拥抱AGI。AGI可以帮助我们解决许多复杂的问题，并且能够极大地改善我们的生活质量。例如，我们可以使用AGI来开发更为智能的医疗设备，提高医疗诊断的准确性和效率；我们也可以利用AGI来优化城市交通，减少交通堵塞和污染；此外，AGI还能为我们提供更好的教育和娱乐体验，让我们的生活更加丰富多彩。</p><p>因此，我们需要尽快投入时间和精力来学习和应用AGI技术。学习AGI不仅可以让我们掌握更加先进的技能和知识，还可以激发我们的创新和热情，让我们更好地适应这个变化迅速的时代。我们需要通过学习和实践，尽快将AGI技术应用到我们的工作和生活中，让其发挥最大的价值。</p><p>最后，我希望每一个人都能积极地拥抱AGI技术，努力学习和应用这项技术，为推动人工智能技术的发展贡献自己的力量。相信在不久的将来，AGI将成为我们生活中不可或缺的一部分，让我们共同期待并努力实现这一目标。</p><h3 id="目录">目录</h3><blockquote><p>注： 这里将会是我未来所有系列教程的目录，便于大家更快找到相关章节</p></blockquote><p><a href="https://www.hivan.me/Understanding%20and%20Utilizing%20AI">导读：了解AI并使用它/他/她们</a></p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;整个系列课程内容虽然为自己所写，但是参考了bothub 创始人徐文浩的课程《AI 大模型之美》&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;人工智能是计算机科学领域中最具前瞻性和影响力的技术之一。它是一种智慧型算法，能够模拟人类的思维过程，处理大量的数据和信息，从而发现隐藏在其中的规律和趋势。人工智能的应用范围非常广泛，包括语音识别、图像识别、自然语言处理、机器翻译、智能推荐、智能问答、自动驾驶等等。&lt;/p&gt;</summary>
    
    
    
    <category term="从零开始接触人工智能大模型" scheme="https://hivan.me/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Exploring the Potential and Challenges of Hybrid Machine Learning Systems in AI</title>
    <link href="https://hivan.me/Exploring-the-Potential-and-Challenges-of-Hybrid-Machine-Learning-Systems-in-AI/"/>
    <id>https://hivan.me/Exploring-the-Potential-and-Challenges-of-Hybrid-Machine-Learning-Systems-in-AI/</id>
    <published>2023-04-01T08:24:52.000Z</published>
    <updated>2023-06-01T08:43:38.157Z</updated>
    
    <content type="html"><![CDATA[<p>随着机器学习和深度学习的飞速发展，人工智能（AI）正取得飞跃性进展。然而，越来越多的研究者一致认为，AI演进的下一个阶段在于开发混合机器学习系统。这篇博客文章将探讨这个新兴领域，讨论它的潜力、挑战以及对AI未来的影响。</p><span id="more"></span><h1 id="什么是混合机器学习系统">什么是混合机器学习系统？</h1><p>混合机器学习系统结合了两个或更多的机器学习模型或技术，创建出一个更强大、更灵活的AI解决方案。这些系统可以发挥每个组成模型的优势，同时弥补它们各自的弱点。组合机器学习模型的方法有多种，例如：</p><ul><li>集成学习：将多个基础模型结合成一个更强大的模型。这可以通过bagging、boosting和stacking等技术来实现。</li><li>多模态学习：集成不同的数据来源（例如文本、图像和音频）以创建更丰富的数据表示，并提高整体性能。</li><li>迁移学习：利用从一个领域或任务中获得的知识，以改善另一个领域或任务中的性能。</li><li>元学习：训练模型学习如何学习，使它们能够更快地适应新任务。</li></ul><h1 id="混合系统的潜力">混合系统的潜力</h1><p>混合机器学习系统有潜力彻底改变AI，为解决复杂问题开辟新的可能性。这些系统的一些关键优势包括：</p><ul><li>改善性能：通过结合多个模型，混合系统可以实现比任何单个模型更好的性能。对于过于复杂以至于单个模型无法有效解决的问题尤为如此。</li><li>鲁棒性：混合系统可以更好地抵御噪声、过拟合和其他影响单个模型的问题。这在现实世界中的应用中尤其重要，因为数据通常是嘈杂和不完美的。</li><li>通用性：混合系统可以处理各种问题、数据类型和任务，这使它们高度适应各种行业和应用。</li><li>可迁移性：混合系统可以更轻松地利用从一个领域或任务中获得的知识，以改善另一个领域或任务中的性能，这使它们非常适合具有有限训练数据的任务。</li></ul><h1 id="开发混合系统的挑战">开发混合系统的挑战</h1><p>尽管有潜力，混合机器学习系统也面临着一些挑战。其中最显著的障碍包括：</p><ol type="1"><li>复杂性：设计和实施混合系统可能比单个模型更加复杂。研究人员和实践者需要仔细考虑如何最好地组合模型和技术，以创建一个有效的系统。</li><li>可伸缩性：混合系统的增加的复杂性可能使它们更难以扩展，无论是在计算资源方面还是处理大量数据的能力方面。</li><li>可解释性：混合系统可以更具挑战性地解释和说明，因为它们涉及多个模型和技术的交互。这可能使得理解系统如何做出决策并确保其正确运行变得更加困难。</li><li>训练和适应：与训练单个模型相比，训练混合系统可能需要更多的计算资源和时间。此外，将这些系统适应到新任务或不断变化的条件可能需要大量的工作。</li></ol><p>总之，混合机器学习系统代表了AI未来的一个有希望的方向。通过利用多个模型和技术的优势，这些系统有潜力在性能、鲁棒性和通用性方面取得重大进展。然而，实现这个潜力需要克服与复杂性、可伸缩性、可解释性和训练相关的挑战。随着研究人员和实践者继续探索这个令人兴奋的领域，我们可以期待在各种行业和应用中看到混合机器学习系统的大量进展。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;随着机器学习和深度学习的飞速发展，人工智能（AI）正取得飞跃性进展。然而，越来越多的研究者一致认为，AI演进的下一个阶段在于开发混合机器学习系统。这篇博客文章将探讨这个新兴领域，讨论它的潜力、挑战以及对AI未来的影响。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
</feed>
