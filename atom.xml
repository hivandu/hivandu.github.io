<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>茶桁.MAMT</title>
  
  <subtitle>ChaHeng Notes，codding and writting ~</subtitle>
  <link href="https://hivan.me/atom.xml" rel="self"/>
  
  <link href="https://hivan.me/"/>
  <updated>2023-12-16T08:13:56.991Z</updated>
  <id>https://hivan.me/</id>
  
  <author>
    <name>Hivan Du</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>29. 深度学习进阶 - 卷积的原理</title>
    <link href="https://hivan.me/29.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%8E%9F%E7%90%86/"/>
    <id>https://hivan.me/29.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%8E%9F%E7%90%86/</id>
    <published>2023-12-12T23:30:00.000Z</published>
    <updated>2023-12-16T08:13:56.991Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246431.png"alt="Alt text" /></p><span id="more"></span><p>Hi,你好。我是茶桁。</p><p>在结束了RNN的学习之后，咱们今天开始来介绍一下CNN。</p><p>CNN是现代的机器深度学习一个很核心的内容，就假如说咱们做图像分类、图像分割，图像的切分等等。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246432.png"alt="Alt text" /></p><p>其实这些过程就是你让计算机能够自动识别，不仅能够识别图像里有什么，还能识别图像里这些东西分别是在什么地方。这种复杂操作其实都是基于啊CNN的变体。要给计算机有识别图像的能力。</p><p>再比方说大无人驾驶汽车，它要识别行人在哪里。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246433.png"alt="Alt text" /></p><p>再比如安防的摄像头，要能够检测出来我们人在哪里。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246434.png"alt="Alt text" /></p><p>这些事情背后都是计算机视觉的问题。</p><p>大概一九五几年、六几年的时候，哈佛大学曾经做过一个研究，给猫的大脑上装了一些电极，让这个猫去看前面的一个幻灯片，然后通过切换幻灯片的内容，然后观察猫的大脑哪些地方活跃。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246435.png"alt="Alt text" /></p><p>就发现两个特点，第一个它有一种一层一层的特性，比方说我换了颜色，它固定的就这几层会活跃，离眼睛远的地方会活跃。如果换了线条，颜色没变，会是另外的一层区域会活跃，不同层其实对于不同的特定变化是不一样的。</p><p>第二个发现，越靠近眼睛的地方，越低级的层次的变化会越明显，比如线条颜色。眼睛越远的距离，线条和颜色没变，但是眼睛变大了或者变小了，那么这些地方它会更明显。</p><p>也就是说，第一个它是有分层的，第二个，它不同的这个层的抽象性是不一样的，对于什么东西的感受力是不一样的。</p><p>沿着这个思路，人们当时就提出来了一些方法。当时人们做计算机视觉，主流不是机器学习。但是人们提出来一个一个这样的filter：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">filter</span> = np.array([<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>],<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>],<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>]<br>])<br></code></pre></td></tr></table></figure><p>这样的filter是人刻意的，主观的提出来的。他们把这个filter去应用到一个一个的图像上。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246436.png"alt="Alt text" /></p><p>比方说我们的图像是<code>a b c d e f g h i j k l</code>，然后按4*4的矩阵相乘，再加起来，比如<spanclass="math inline">\(aw+bx+ey+fz\)</span>，这样就得到了一个新的内容。大家把这个操作就叫做卷积操作。</p><p>看个示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image = np.array([<br>    [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>],<br>])<br>plt.imshow(image)<br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246437.png"alt="Alt text" /></p><p>我们可以看到，这个矩阵的前三列全是10，后两列都是0，最后生成的图像有一个明显的分界，伴随着两个不同的颜色。</p><p>我们现在给这个图像矩阵加上一个filter, 然后按上面的方法进行操作：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246438.png"alt="Alt text" /></p><p>那左上角的3*3的小矩阵的运算结果就是0。</p><p>那同理，我们以此往后算，第二个结果是39, 第三个结果是39....大家后面可以自行计算一下，最后的计算结果就是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">[[<span class="hljs-number">0</span>, <span class="hljs-number">39</span>, <span class="hljs-number">39</span>, <span class="hljs-number">0</span>],<br>[<span class="hljs-number">0</span>, <span class="hljs-number">39</span>, <span class="hljs-number">39</span>, <span class="hljs-number">0</span>],<br>[<span class="hljs-number">0</span>, <span class="hljs-number">39</span>, <span class="hljs-number">39</span>, <span class="hljs-number">0</span>]]<br></code></pre></td></tr></table></figure><p>我们可以看出来，当分割的小矩阵内数据相同的时候，值为0，如果说矩阵内的这个部分图像差距不是很大，那它也是近乎接近于0，意味着差别很小。如果说分割的这个小矩阵左右两边是相反数的时候，两边的差别是最大的，不管最后相加的值是正的还是负的，绝对值下应该是最大的。这个地方其实是图像竖着的边缘。</p><p>那如果我们将filter改一下，改成下面这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">[[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>[-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>]]<br></code></pre></td></tr></table></figure><p>如果是这样，计算的结果就是图像横向的边缘的绝对值最大。</p><p>基于这种原理，我们就可以找到图像所有竖向和横向的边沿，给它拿出来。</p><p>这整个的一个过程，就叫做卷积:convolution。convolution就是两个东西之间互相起作用。最早是出现在信号处理上，两个信号把它做一个合并。</p><p>卷积的操作是为了干什么呢？卷积的操作是用来提取图片的某种特征，抓取图片特征。在上个世纪后期，计算机视觉的老科学家们提出了大量的kernel，当时叫做算子，现在叫做卷积核。</p><p>卷积的操作就是给定一个图片，然后给定一个卷积核，和卷积核一样大小的窗口里边的每个值相乘，相乘之后再做相加。</p><p>假如咱们有一张图片，一般来说，咱们现实生活中图片往往是三维，通常是红绿蓝(RGB)，然后我们让这张图片和这个filter去做相乘的操作。</p><p>这三个层里面每一层都会和filter做一个相乘的操作，咱们就假设这三个层分别为:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">[[a11, a12, a13],[a21, a22, a23],[a31, a32, a33]],<br>[[b11, b12, b13],[b21, b22, b23],[b31, b32, b33]]<br>[[c11, c12, c13],[c21, c22, c23],[c31, c32, c33]]<br></code></pre></td></tr></table></figure><p>然后再假设filter为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[[f11, f12, f13], [f21, f22, f23], [f31, f32, f33]]<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246439.png"alt="Alt text" /></p><p>那这个filter会分别和这三个层进行卷积操作，产生的卷积结果为v1, v2, v3,然后这三个结果再进行相加，最后会产生一个新的层。</p><p>我们来看一下下面这张图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246440.png"alt="Alt text" /></p><p>这张图显示的是一层的情况，一个filter大小的矩阵被卷积成了一个点，然后这个操作不只是针对一层的，而是对整个一个纵向体积内的所有层都做这样一个操作：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246441.png"alt="Alt text" /></p><p>途中最底下是我们的图片的RGB分层，再经过和filter相乘之后向上会卷积成一个点，那向上之后的Map1,Map2,...原因是每一层都是一个不同的filter计算的结果，这里存在很多个filter，然后分别计算产生了这样一个叠加层。</p><p>再做下一次运算的时候也是一样，这些Map的纵向上经过和filter运算依然会被卷积成一个点。</p><p>就着上面那个简单的图形，咱们来做个演示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv</span>(<span class="hljs-params">image, <span class="hljs-built_in">filter</span></span>):<br>    h = <span class="hljs-built_in">filter</span>.shape[<span class="hljs-number">0</span>]<br>    w = <span class="hljs-built_in">filter</span>.shape[<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(image.shape[<span class="hljs-number">0</span>]):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(image.shape[<span class="hljs-number">1</span>]):<br>            window = image[i: i+h, j: j+w]<br>            <span class="hljs-built_in">print</span>(window)<br><span class="hljs-built_in">filter</span> = np.array([<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>],<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>],<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>]<br>])<br><br>conv(image, <span class="hljs-built_in">filter</span>)<br><br>---<br>[[<span class="hljs-number">10</span> <span class="hljs-number">10</span> <span class="hljs-number">10</span>]<br> [<span class="hljs-number">10</span> <span class="hljs-number">10</span> <span class="hljs-number">10</span>]<br> [<span class="hljs-number">10</span> <span class="hljs-number">10</span> <span class="hljs-number">10</span>]]<br>...<br>[[-<span class="hljs-number">3</span>]<br> [-<span class="hljs-number">3</span>]<br> [-<span class="hljs-number">3</span>]]<br>...<br>[[<span class="hljs-number">10</span> -<span class="hljs-number">3</span> -<span class="hljs-number">3</span>]]<br>[[-<span class="hljs-number">3</span> -<span class="hljs-number">3</span> -<span class="hljs-number">3</span>]]<br>[[-<span class="hljs-number">3</span> -<span class="hljs-number">3</span>]]<br>[[-<span class="hljs-number">3</span>]]<br><br></code></pre></td></tr></table></figure><p>输入一个图片的数据,拿到filter的高宽，然后让filter沿着图片从上到下，从左到右移动。</p><p>我们打印结果能看到，运行到中间的时候会出现一串的[[-3], [-3],[-3]]。因为i会一直运行边上，那么如果要做卷积的话，大小要和filter一直一样，所以咱们在这里需要给他减去一个filter。就是不要运行后边这几个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">...<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(image.shape[<span class="hljs-number">0</span>] - h+<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(image.shape[<span class="hljs-number">1</span>] - w+<span class="hljs-number">1</span>):<br>            ...<br></code></pre></td></tr></table></figure><p>这样就可以了。</p><p>我们每一次其实就是从左到右，从上到下裁剪出来一个一个的window。</p><p>我们让这个window和filter相乘后再相加，我们可以得到什么结果？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i ...:<br>    <span class="hljs-keyword">for</span> j ...<br>        ...<br>        result = np.<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">filter</span> * window)<br>        <span class="hljs-built_in">print</span>(result)<br><br>---<br><span class="hljs-number">0</span><br><span class="hljs-number">39</span><br>...<br><span class="hljs-number">39</span><br><span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>就是计算卷积的结果。</p><p>那我们可以将其改成矩阵的形式, 然后咱们打印出来看看是个啥：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv</span>(<span class="hljs-params">image, <span class="hljs-built_in">filter</span></span>):<br>    r_height = image.shape[<span class="hljs-number">0</span>] - h+<span class="hljs-number">1</span><br>    r_width = image.shape[<span class="hljs-number">1</span>] - w+<span class="hljs-number">1</span><br>    result = np.zeros(shape=(r_height, r_width))<br>    <span class="hljs-keyword">for</span> i ...:<br>        <span class="hljs-keyword">for</span> j ...:<br>            result[i][j] = np.<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">filter</span> * window)<br>    <span class="hljs-keyword">return</span> result<br><br>result = conv(image, <span class="hljs-built_in">filter</span>)<br>plt.imshow(result)<br>plt.show()<br>---<br>array([[ <span class="hljs-number">0.</span>, <span class="hljs-number">39.</span>, <span class="hljs-number">39.</span>,  <span class="hljs-number">0.</span>],<br>       [ <span class="hljs-number">0.</span>, <span class="hljs-number">39.</span>, <span class="hljs-number">39.</span>,  <span class="hljs-number">0.</span>],<br>       [ <span class="hljs-number">0.</span>, <span class="hljs-number">39.</span>, <span class="hljs-number">39.</span>,  <span class="hljs-number">0.</span>],<br>       [ <span class="hljs-number">0.</span>, <span class="hljs-number">39.</span>, <span class="hljs-number">39.</span>,  <span class="hljs-number">0.</span>]])<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246442.png"alt="Alt text" /></p><p>那变成这样的原因是因为原来的图像中间有一个边缘，现在这张图显示的是图片边缘的部分被高亮。</p><p>这样一张图片可能并不太能理解，我拿我的头像来做这个示例好了：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246443.jpg"alt="Alt text" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">myself = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./assets/chaheng.jpg&#x27;</span>).convert(<span class="hljs-string">&#x27;L&#x27;</span>)<br>...<br>plt.imshow(result, cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br></code></pre></td></tr></table></figure><p>为了更明显一点，我将图像改成灰度显示。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246444.png"alt="Alt text" /></p><p>我们可以看到卷积之后的效果，明显边缘都被显示出来了。但是我们也注意到了，竖向的边缘都很明显，但是横向的边缘并不清楚。我们再来对横向进行一下卷积,我们先要增加一个处理多个filter的方法，将原来的conv方法改为single_conv，表示处理单个：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">single_conv</span>(<span class="hljs-params">...</span>):<br>    ...<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv</span>(<span class="hljs-params">image, filters</span>):<br>    results = [single_conv(image, f) <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> filters]<br>    <span class="hljs-keyword">return</span> results<br></code></pre></td></tr></table></figure><p>然后我们的调用需要改一下传递的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">results = conv(image, [h_filter, w_filter])<br></code></pre></td></tr></table></figure><p>既然要传递两个filter,那我们就需要再定义一个横向的filter，然后一起传进去：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 原来的filter</span><br>h_filter = np.array([...])<br><br><span class="hljs-comment"># 新定义的横向filter</span><br>w_filter = np.array([<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>    [-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>]<br>])<br></code></pre></td></tr></table></figure><p>接着我们将原图，竖向的卷积结果和横向的卷积结果都打印出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>plt.imshow(image)<br>plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>plt.imshow(results[<span class="hljs-number">0</span>])<br>plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br>plt.imshow(results[<span class="hljs-number">1</span>])<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246445.png"alt="Alt text" /></p><p>原图变成这个颜色的原因是我在PIL读取图像的时候，将其转为了灰度。我们可以看到第二张图片和第三张图明显在边缘上的区别，一个像是灯光从左边打过来的，一个像是灯光从上面打下来的。</p><p>中间和右边这个，其实都是把边缘提出来了。因为卷积核的不同，中间这个图把竖着的边缘明显提取的比较准确，右边的把横向的提取的比较准确。</p><p>这也是为什么我们之前看得那张图里会有那么多的Map:</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246446.png"alt="Alt text" /></p><p>它的每一层都是一个不同的filter提取出来的，有这么多filter的原因则是每一个filter提取出来的特征都是不一样的。</p><p>我们来看我们刚才定义的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">single</span>(<span class="hljs-params">image, <span class="hljs-built_in">filter</span></span>):<br>    ...<br></code></pre></td></tr></table></figure><p>我们把输入卷积的时候的image这个参数叫做<code>input channel</code>。那在此时此刻，我们这个图像如果是RGB的，它就是三维的，那么<code>input channel</code>就等于3。</p><p><code>filters</code>的个数，就叫做<code>output channel</code>。原因就在于，有多少个<code>filter</code>，那我们的results就有多厚。比如说我们有4个<code>filter</code>,那输出的result就有四层。然后可以接着对results继续应用<code>filter</code>做卷积，那在这一轮的<code>input channel</code>就等于一次的<code>output channel</code>,也就是4。</p><p>这个，就是卷积的原理。</p><p>好，这节课就到这里了，下节课咱们继续学习卷积，来看看在神经网络里如何应用。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246431.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>28. 深度学习进阶 - LSTM</title>
    <link href="https://hivan.me/28.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20LSTM/"/>
    <id>https://hivan.me/28.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20LSTM/</id>
    <published>2023-12-09T23:30:00.000Z</published>
    <updated>2023-12-16T08:14:04.718Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054870.png"alt="Alt text" /></p><span id="more"></span><p>Hi, 你好。我是茶桁。</p><p>我们上一节课，用了一个示例来展示了一下我们为什么要用RNN神经网络，它和全连接的神经网络具体有什么区别。</p><p>这节课，我们就着上一节课的内容继续往后讲，没看过上节课的，建议回头去好好看看，特别是对代码的进程顺序好好的弄清楚。</p><p>全连接的模型得很仔细的去改变它的结构，然后再给它加很多东西，效果才能变好：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">self.linear_with_tanh = nn.Sequential(<br>    nn.Linear(<span class="hljs-number">10</span>, self.hidden_size),<br>    nn.Tanh(),<br>    nn.Linear(self.hidden_size, self.hidden_size),<br>    nn.Tanh(),<br>    nn.Linear(self.hidden_size, output_size)<br>)<br></code></pre></td></tr></table></figure><p>但是对于RNN模型来说，我们只用了两个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">self.rnn = nn.RNN(x_size, hidden_size, n_layers, batch_first=<span class="hljs-literal">True</span>)<br>self.out = nn.Linear(hidden_size, output_size) <br></code></pre></td></tr></table></figure><p>这是一个很本质的问题,也比较重要。为什么RNN的模型这么简单，它的效果比更复杂的全连接要好呢？</p><p>这个和我们平时生活中做各种事情其实都很类似，他背后的原因是他的信息保留的更多。RNN模型厉害的本质是在运行的过程中把更多的信息记录下来，而全连接没有记录。</p><p>对于RNN模型，还有两个点大家需要注意。</p><p>第一个，有一种叫做stacked的RNN的模型。我们RNN模型每一次输出都有一个output和hidden，把outputs和hidden作为它的输入再传给另外一个RNN模型，模型就变得更复杂，理论上可以解决些更复杂的场景。我们把这种就叫做stackedRNN。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054871.png"alt="Alt text" /></p><p>还有一种形式，Bidirectional RNN，双向RNN。有一个很著名的文本模型Bert,那个B就是双向的意思。</p><p>我们回过头来看上节课我们讲过的两种网络：</p><p><span class="math display">\[\begin{align*}h_t &amp; = \sigma_h(W_hx_t + U_hh_{t-1} + b_h) \\y_t &amp; = \sigma_y(W_yh_t + b_y)\end{align*}\]</span></p><p>在这个里面，每一时刻的y_t只和y_{t-1}有关系，如果把所有的x一次性给到模型的时候，其实我们在这里可以给它加一个东西：</p><p><span class="math display">\[\begin{align*}h_t &amp; = \sigma_h(W_hx_t + U_hh_{t-1} + V_h * h_{t+1} + b_h)\end{align*}\]</span></p><p>还可以写成这样，那这样的话它实现的就是每一时刻的t既和前一次有关系和后一刻有关系。这样我们每一次的值不仅和前面有关，还和后面有关。就叫做双向RNN。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054872.png"alt="Alt text" /></p><p>对于RNN来说，它有一个很严重的问题，就是之前说过的，它的vanishing和exploding的问题会很明显,也就是梯度消失和爆炸问题。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054873.png"alt="Alt text" /></p><p>想一下，现在如果有一个loss，那它最终的loss是不是对于{x1, x2, ...,xn}都有关系，比方说现在要求<span class="math inline">\(\frac{\partialloss}{\partial w_1}\)</span>, 假如说现在h是100， 那这种调用关系就是</p><p><span class="math display">\[\begin{align*}\frac{\partial loss}{\partial w_1} = \frac{\partial h_{100}}{\partialh_{99}} \cdot \frac{\partial h_{99}}{\partial h_{98}} \cdot ... \cdot\frac{\partial h_{0}}{\partial w_{1}}\end{align*}\]</span></p><p>loss对于w1求偏导的时候，其实loss最先接受的是离他最近的,假如说是h100。h100调用了h99,h99调用h98，就这个调用过程，这一串东西会变得很长。</p><p>我们之前课程说过一些情况，怎么去解决这个问题呢？对于RNN模型来说梯度爆炸很好解决，就直接设定一个阈值就可以了，起码也是能学习的。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054874.png"alt="Alt text" /></p><p>要讲的是想一种方法怎么样来解决梯度消失的问题。这个梯度消失的解决方法，就叫LSTM。要解决梯度消失，就是要用LSTM:Long Short-TermMemory，长短记忆模型，既能保持长信息，又能保持短信息。</p><p>在之前那个很长的过程中，怎么样能够让它不消散呢？LSTM的核心思想是通过门控机制来控制信息的流动和及已的更新，包含了InputGate, Forget Gate，Cell State以及OutputGate。这些会一起协作来处理序列数据。</p><p>其中Input Gate控制着新信息的输入，以及信息对细胞状态的影响。 ForgetGate控制着细胞状态中哪些信息应该被易王，CellState用于传递信息，是LSTM的核心，OutputGate控制着细胞状态如何影响输出。</p><p>这里每一个门控单元都由一个Sigmoid激活函数来控制信息的流动，以及一个Tanh激活函数来确定信息的值。</p><p><span class="math display">\[\begin{align*}Input Gate \\i_t &amp; = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\C&#39;t &amp; = \tanh(W_c \cdot [h{t-1}, x_t] + b_c) \\C_t &amp; = f_t \cdot C_{t-1} + i_t \cdot C&#39;_t \\Forget Gate \\f_t &amp; = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \\C_t &amp; = f_t \cdot C_{t-1} + i_t \cdot C&#39;_t \\Output Gate \\o_t &amp; = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\h_t &amp; = o_t \cdot \tanh(C_t)\end{align*}\]</span></p><p>其中，<span class="math inline">\(h_{t-1}\)</span>是前一个时间步的隐藏状态，<span class="math inline">\(x_t\)</span>是当前时间步的输入，<span class="math inline">\(W_i, W_f, W_o,W_c\)</span> 是权重矩阵，<span class="math inline">\(b_i, b_f, b_o,b_c\)</span> 是偏置。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054875.png"alt="Alt text" /></p><p>LSTM输入的是一个序列数据，可以是文本、时间序列，音频信号等等。那每个时间步的输入是序列中的饿一个元素，比如一个单词、一个时间点的观测值等等。</p><p>假设我们有一个序列 x = [x1, x2, ..., xt]，其中t就代表的是时间步。</p><p>xt进来的时候, 之前我们是只接收一个hidden state,现在我们多接收了一个<spanclass="math inline">\(C_{t-1}\)</span>，这个就是我们的Cell，这一步的<spanclass="math inline">\(C_{t-1}\)</span>其实就是上一步的<spanclass="math inline">\(C_t\)</span>。</p><p>在训练开始时，需要初始化LSTM单元的隐藏状态h0和细胞状态c0。通常我们初始化它们为全零向量。</p><p>最开始的时候，我们要进入Input Gate, 对于每个时间步t,计算输入门的激活值<spanclass="math inline">\(i_t\)</span>，控制新信息的输入。使用Sigmoid函数来计算输入门的值：</p><p><span class="math display">\[i_t = \sigma (W_i \cdot [h_{t-1}, x_t] + b_i)\]</span></p><p>然后，计算新的侯选值<span class="math inline">\(C&#39;_t\)</span>，这是在当前时间步考虑的新信息。使用tanh激活函数来计算侯选值：</p><p><span class="math display">\[C&#39;_t = tanh(W_c \cdot [h_{t-1}, x_t] + b_c)\]</span></p><p>接下来我们就要更新细胞状态了，细胞状态<spanclass="math inline">\(C_t\)</span>更新是通过遗忘门<spanclass="math inline">\(f_t\)</span>和输入门<spanclass="math inline">\(i_t\)</span>控制的。遗忘门控制着哪些信息应该被遗忘，输入门控制新信息对细胞状态的影响：</p><p><span class="math display">\[C_t = f_t \cdot C_{t-1} + i_t \cdot C&#39;_t\]</span></p><p>那遗忘门决定哪些信息应该被遗忘，使用的就是Sigmoid函数计算遗忘门的激活值。</p><p><span class="math display">\[f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)\]</span></p><p>接着，计算输出门<span class="math inline">\(O_t\)</span>,控制着细胞状态如何影响输出和隐藏状态。一样，我们还是使用Sigmoid函数计算。</p><p><span class="math display">\[o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)\]</span></p><p>使用输出门的值<spanclass="math inline">\(o_t\)</span>来计算最终的隐藏状态<spanclass="math inline">\(h_t\)</span>和输出。隐藏状态和输出都是根据细胞状态和输出门的值来计算的：</p><p><span class="math display">\[h_t = o_t \cdot tanh(C_t)\]</span></p><p>接下来就容易了，我们迭代重复上述过程，处理序列中的每一个时间步，直到处理完整个序列。</p><p>LSTM的输出可以是隐藏状态<span class="math inline">\(h_t\)</span>,也可以是细胞状态<span class="math inline">\(C_t\)</span>，具体是取决于应用的需求。</p><p>后来大家就发现了一种改进的LSTM，其中门控机制允许细胞状态窥视现前的细胞状态的信息，而不仅仅是根据当前时间步的输入和隐藏状态来决定。这个机制在LSTM单源种引入了额外的权重和连接，以允许细胞状态在门控过程中访问现前的细胞状态，我们称之为窥视孔连接:Peephole connections。</p><p><span class="math display">\[\begin{align*}f_t = \sigma(W_f \cdot [C_{t-1}, h_{t-1}, x_t] + b_f) \\i_t = \sigma(W_i \cdot [C_{t-1}, h_{t-1}, x_t] + b_i) \\o_t = \sigma(W_o \cdot [C_{t-1}, h_{t-1}, x_t] + b_o) \\\end{align*}\]</span></p><p>之前，我们是xt和x_{t-1}决定的f，那现在又把c_{t-1}加上了。就是多加了一些信息。</p><p>除此之外它有一个方法GRU，这个是2014年提出来的，Geted RecurrentUnit，它是LSTM的一个简化版本。</p><p>它最核心的内容：</p><p><span class="math display">\[\begin{align*}h_t = (1-z_t) \cdot h_{t-1} + z_t \cdot h&#39;_t\end{align*}\]</span></p><p>咱们刚刚是<span class="math inline">\(C_t = f_t \cdot C_{t-1} + i_t\cdotC&#39;_t\)</span>，也就是遗忘加上输入，那我们对过去保留越多的时候，输入就会越小，那对过去保留越小的时候，输入就会越大。</p><p>所以既然f也是1-0，i也是0-1，f大的时候i就小，f小的时候i就大，那么能不能写成f=(1-i)？</p><p>于是，GRU就这样实现了, 它其实最核心的就做了这样一件事， f=(1-i)。</p><p><span class="math display">\[\begin{align*}z_t &amp; = \sigma(W_z \cdot [h_{t-1}, x_t]) \\r_t &amp; = \sigma(W_r \cdot [h_{t-1}, x_t]) \\h&#39;_t &amp; = \tanh(W \cdot [r_t \cdot h_{t-1}, x_t]) \\h_t &amp; = (1-z_t) \cdot h_{t-1} + z_t \cdot h&#39;_t\end{align*}\]</span></p><p>这个z其实和i是一样的东西，只是原作者为了发表论文方便而改了个名称。</p><p><ahref="https://arxiv.org/pdf/1406.1078v3.pdf">https://arxiv.org/pdf/1406.1078v3.pdf</a></p><p><span class="math inline">\(r_t\)</span>是来控制上一时刻的<spanclass="math inline">\(h_t\)</span>在我们此时此刻的重要性、影响程度。那我们可以将<spanclass="math inline">\(r_t \cdot h_{t-1}\)</span>看成是关于及已的，<spanclass="math inline">\(1-z_t\)</span>也是关于记忆的。</p><p>GRU这样做之后有什么好处呢?</p><p>原来我们有三个门: f, i, o，那现在变成了两个，z和r。为什么就更好了呢？我们在PyTorch里面往往用的是GRU。</p><p>大家想一下，是不是少了一个门其实就少了一个矩阵？我们看公式的时候，<spanclass="math inline">\(W_f\)</span>是一个数学符号，但是在背后其实是一个矩阵，是一个矩阵的话少了一个矩阵意味着参数就少多了，运算就更快了等等。</p><p>但其实这些都不是最关键的，最关键的是减少过拟合了。我们之前的课程中一再强调，过拟合之所以产生，最主要的原因是数据不够或者说是模型太复杂。</p><p>但是在现有的数据情况下，为了让数据发挥出最大效力，你把需要训练的模型变简单，参数变少，就没有那么复杂了。</p><p>关于RNN模型，我们后面还会介绍一些具体的示例。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054870.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>27. 深度学习进阶 - 为什么RNN</title>
    <link href="https://hivan.me/27.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E4%B8%BA%E4%BB%80%E4%B9%88RNN/"/>
    <id>https://hivan.me/27.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E4%B8%BA%E4%BB%80%E4%B9%88RNN/</id>
    <published>2023-12-05T23:30:00.000Z</published>
    <updated>2023-12-16T08:14:08.561Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="../../../../../../Volumes/Media%20Database/%E5%9B%BE%E7%89%87/Screenshots/202311051508354.png"alt="Alt text" /></p><span id="more"></span><p>Hi，你好。我是茶桁。</p><p>这节课开始，我们将会讲一个比较重要的一种神经网络，它对应了咱们整个生活中很多类型的一种问题结构，它就是咱们的RNN网络。</p><p>咱们首先回忆一下，上节课咱们学到了一些深度学习的一些进阶基础。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501000.png" /></p><p>学了很多神经网络的Principles,就是它的一些很重要的概念，比方层数维度。再然后咱们讲了Optimizer，一些优化方式。还有weights的initialization，初始化等等。</p><p>那么大家具备了这些知识之后，那我们基本上已经能够解决常见的大概90%的机器学习问题了。</p><p>我们现实生活中绝大多数的机器学习问题，或者说识别问题都可以把它抽象成要么是分类，要么是回归问题。</p><h2 id="一个柯基的例子">一个柯基的例子</h2><p>我们来一个例子，比方说一张图片里这个是什么动物，这显然是一个分类问题。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501001.png"alt="Corgi" /></p><p>但是我们对这个图片的多个物体是什么，还有位置标注出来，那这个在里面前面会有一段是一个分类问题，后面还有一个长的向量，又会是一个回归问题。</p><p>我们只要知道分类和回归最大的区别就是一个返回的是一个类别，另外一个返回的是一个真正的数值。</p><p>那么接下来我们要正是的讲一下两种神经网络，RNN和CNN。这两个的目的是用来加速解决我们之前遇到的分类问题，或者回归问题。</p><p>在这些LSTM和CNN之类的高级的方法出现之前，其实我们用最直接的神经网络是可以解决所有的问题。</p><p>我们还是来看上面的那个例子，还是那张图片，如果要去分类看这图片里的是什么动物，我们把它形式化的表述一下。</p><p>假设我们这张图片现在是258*258的，那每一张图片进来之后，这个图片的饿背后其实都是一个向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 258 * 258</span><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>example_img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;assets/Corgi.png&#x27;</span>)<br>example_img = np.array(example_img)<br><br><span class="hljs-built_in">print</span>(example_img.shape)<br><br>---<br>(<span class="hljs-number">429</span>, <span class="hljs-number">696</span>, <span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p>我们可以看到这张图片在计算机里保存的时候是<code>(429, 696, 3)</code>这样的一组数字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.imshow(example_img)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501002.png"alt="Alt text" /></p><p>我们用plt展示出来，就是这样。</p><p>我们现在就可以讲整个图片变成一个向量，然后把它从立方体变的拉平：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">example_img = example_img.reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(example_img)<br><br>---<br>[[<span class="hljs-number">120</span> <span class="hljs-number">150</span>  <span class="hljs-number">88</span> ...  <span class="hljs-number">43</span>  <span class="hljs-number">39</span>  <span class="hljs-number">38</span>]]<br></code></pre></td></tr></table></figure><p>那现在，我们要给这个图片做分类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dimension, categorical</span>):<br>        <span class="hljs-built_in">super</span>(Model, self).__init__()<br>        self.linear = nn.Linear(in_features=input_dimension, out_features=categorical)<br>        self.softmax = nn.Softmax()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        predict = self.softmax(self.linear(x))<br>        <span class="hljs-keyword">return</span> predict<br>        ...<br></code></pre></td></tr></table></figure><p>这里我们暂停一下，来说说这段代码中的<code>super(...)</code>，为了避免有些小伙伴Python基础不太好，这里说明一下。</p><p>如果有从我Python基础课就看过来的小伙伴，应该知道我在面向对象的时候应该是讲过这个方法。这个是为了在继承父类的时候，我们在重写父类方法的时候，依然可以调用父类方法。方式就是<code>super().父类方法名()</code>。有需要补Python基础的可以回头将我写的Python基础课程好好再看一遍。</p><p>好，我们继续回过头来讲，我们定义好这个Model之后，将图片数据变成一个PyTorch能够处理的一个example，当作训练数据传入train_x。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">train_x = torch.from_numpy(example_img)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;shape:&#123;&#125;, \ntrain_x:&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(train_x.shape, train_x))<br><br>---<br>shape:torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">895752</span>]), <br>train_x:tensor([[<span class="hljs-number">120</span>, <span class="hljs-number">150</span>,  <span class="hljs-number">88</span>,  ...,  <span class="hljs-number">43</span>,  <span class="hljs-number">39</span>,  <span class="hljs-number">38</span>]], dtype=torch.uint8)<br></code></pre></td></tr></table></figure><p>然后进入线性函数，传入<code>in_features</code>为<code>train_x.shape[1]</code>，把它变成一个10分类，再把test_model运行一下，将我们的<code>train_x</code>输入进去就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">test_model = Model(input_dimension=train_x.shape[<span class="hljs-number">1</span>], categorical=<span class="hljs-number">10</span>)<br>output = test_model(train_x.<span class="hljs-built_in">float</span>())<br></code></pre></td></tr></table></figure><p>这样的话,我们就可以产生出一个Softmax，有了这个Softmax，在这我们如果有很多个x，它就会对应我们很多个已知的y。</p><p>然后我们在这里定义一个loss：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">criterion = torch.nn.CrossEntropyLoss()<br></code></pre></td></tr></table></figure><p>再之后我们在做线性的时候之前，肯定是有一些ytrue数据的，肯定是知道它的y的，写个循环它就不断的可以去训练。</p><p>接着我们可以得到这个它的权重，那么在这里这是一张图片，如果这个图片要做回归，要给这个图片打分，那么将<code>out_features</code>换成1就可以了。</p><p>我们在Model里不断的去改它的东西，让它的输出能够满足就可以了。</p><p>不管是用户数据还是气象数据、天文数据、图片、文字，我们都可以把它变成这样的一个x向量。变成x向量之后只要送到一个模型里面，这个模型它能够去做优化，做些调整。那么它就能够去不断的去做优化。</p><p>当然，我们这里还缺一个optimizer：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = torch.optim.SGD(test_model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br></code></pre></td></tr></table></figure><p>我们定义了一个SGD优化器，learning_rate设置了一下，给了一个初始的学习率。</p><p>然后呢再不断的去循环它就可以了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义虚拟的y</span><br>lable = np.random.randint(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">10</span>)<br>train_y = torch.from_numpy(np.array([lable])).<span class="hljs-built_in">float</span>()<br><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    y_true = train_y<br>    y_predict = test_model(train_x.<span class="hljs-built_in">float</span>())<br><br>    <span class="hljs-built_in">print</span>(y_true.shape)<br>    <span class="hljs-built_in">print</span>(y_predict.shape)<br><br>    <span class="hljs-built_in">print</span>(loss)<br></code></pre></td></tr></table></figure><p>我们现在可以将criterion假如到循环里来计算一下loss了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    ...<br>    loss = criterion(y_predict, y_true)<br></code></pre></td></tr></table></figure><p>就是说，我们之前学习的这些内容，不管是图片还是用户的数据、或者文字，其实都是可以变成一个向量，再把向量送入到定义好的模型里，求出它的结果。</p><p>再经过反复的运作，反复的调试来更新它的数据。</p><h2 id="为什么rnn-or-cnn">为什么RNN or CNN</h2><p>那为什么我们还要学习RNN和CNN这些东西呢？我们刚开始学的<code>wx+b</code>的形式，可以把任意的x变成其它的一个output，但是它在解决一些问题的时候效果就不是太好。</p><p>比方说啊，我们要识别一个图像到底是什么的时候，wx+b它是给每一个x一个权重,<span class="math inline">\(w x_i + b\)</span>, 然后最后产出一个值。</p><p>但是图像我们是希望给中间一个区域一个平分，可是现在是一个点一个点的。</p><p>例如我们输入是一个x，输出是一个y。x它包含了多个x:{x1, x2, x3, ...,xn}，那y的输出呢，它是和多个x有关系。如果是在一个曲线上，我们取几个点,{output1, output2, output3}, 那么这个output3就不止和<spanclass="math inline">\(\vec x_3\)</span>有关系，它和前面的output2,output1都有关系。</p><p>也就是说，当下这一时刻的数据其实不仅取不仅取决于今天发生的一些事情，还取决于昨天前天，甚至大前天发生的事情。</p><p>但是我们如果直接进行<code>wxi+b</code>的话，这里xi=x3，wx3+b我们期望输出一个output3，这样就忽略了前边的这些事情。</p><p>与此类似的还有我们写文章，当前这个字和前面是什么字应该是有依赖关系的。其实把它抽象一下的话，会发现在现实生活中其实有很多种依赖关系。</p><p>我们之前讲的wx+b，其实是一对一。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501003.png"alt="Alt text" /></p><p>虽然x的维度可能会很大，y输出的维度也可能很大，但是它一个x就只对应输出一个y。</p><p>而除了one to one 之外，我们还有一些其他的类别：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501004.png"alt="Alt text" /></p><p><code>one to many</code>，就是x输入之后，最后会输出多个y。比方说咱们输入的是一个类别，输出的是一篇文章，分别是第一个单词，第二个单词和第三个单词。</p><p>我们会发现，这三个输出的单词前后是有相关性的。这种就属于是一对多，输出的的这些内容是独立的个体，但是它们之间有相关性。</p><p>后面的<code>many to one</code>，典型的一个应用，你给他输入一句话，输出这个地方，这句话到底是表示正向的还是负向的。那么这句话其实每个单词之间是有依赖关系的，而输出的是一个值。</p><p>那<code>many to many</code>里，前边输入的这个input是一个序列，有依赖关系。输出也是一个序列，有依赖关系。那么这会是一个什么？比方我们的机器翻译，就有可能是这样一个关系，对吧？还有比方说我们会去做那个文本的阅读理解，文本的摘要。</p><p>那还有一个<code>many to many</code>和第一个有什么区别呢？它其实只是更加的实时，比如说同声传译。</p><p>对于这些所有的问题我们给它抽象一下，它每一步的输出就像我们之前学过递归函数一样，是和前一步的输出有关系，还和当前这一步的输入有关系,我们其实学过最典型的一个依赖关系就是这样，就是斐波那契数列或者求阶乘：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">fib</span>(<span class="hljs-params">n</span>):<br>    <span class="hljs-keyword">if</span> n == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> n == <span class="hljs-number">1</span>: <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> fib(n-<span class="hljs-number">1</span>) + fib(n-<span class="hljs-number">2</span>)<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">fac</span>(<span class="hljs-params">n</span>):<br>    <span class="hljs-keyword">if</span> n == <span class="hljs-number">0</span>: <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>    <span class="hljs-keyword">else</span>: <span class="hljs-keyword">return</span> n*fac(n-<span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>): <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#123;&#125;\t&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(fib(i), fac(i)))<br></code></pre></td></tr></table></figure><p>那么这个怎么实现的？我们要实现这个有多种方法，我们可以来看一个具体的案例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RNN</span>(nn.Module):<br>    <span class="hljs-comment"># implement RNN from scratch rather than ysubf nn.RNN</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size</span>):<br>        <span class="hljs-built_in">super</span>(RNN, self).__init__()<br><br>        self.hidden_size = hidden_size<br>        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)<br>        self.i2o = nn.Linear(input_size + hidden_size, output_size)<br>        self.softmax = nn.LogSoftmax(dim=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_tensor, hidden_tensor</span>):<br>        combined = torch.cat((input_tensor, hidden_tensor), <span class="hljs-number">1</span>)<br><br>        hidden = self.i2h(combined)<br>        output = self.i2o(combined)<br></code></pre></td></tr></table></figure><p>这是一个非常经典的RNN的模型，我们来一起来分析它的构成。</p><p>在构造函数内，输入了一个<code>input_size</code>（x向量），还有一个<code>hidden_size</code>。然后在下面做了一个<code>i2h</code>的线性变化，这个线性变化它接受一个的两个参数，<code>in_features</code>是<code>input_size + hidden_size</code>,<code>out_features</code>是<code>hidden_size</code>。</p><p>现在有一个<span class="math inline">\(\vec x\)</span>和一个<spanclass="math inline">\(\vec h\)</span>，将两个向量相加输入进入，然后会输出一个<span class="math inline">\(vech\)</span>一样大小的东西。</p><p>然后下面还有一个<code>i2o</code>，它是将<code>input_size + hidden_size</code>输入之后，输出一个<code>output_size</code>一样大小的东西。</p><p>在输出这两个之后，我们将<code>output_size</code>大小的这个向量，输入到<code>Softmax</code>里面，就会变成一个概率分布。</p><p>然后它继续forward的时候，继续向前运算的时候，它的输入是input和hidden，那它在这里，如果我们要求训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">line_tensor, category_tensor</span>):<br>    hidden = rnn.init_hidden()<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(line_tensor.size()[<span class="hljs-number">0</span>]):<br>        output, hidden = rnn(line_tensor[i], hidden)<br></code></pre></td></tr></table></figure><p>这里它有很多的tensor，比如我们的<code>x:[x1, x2, ..., xn]</code>,这个tensor就是这些个x。那么它在做训练的第一步会取最前面的这个x向量，这个x向量刚开始会有一个随机的hidden向量，这个时候关键的地方就来了，就是它不断的重复:<code>output, hidden = rnn(line_tensor[i], hidden)</code>,我们来看，这个hidden就会一次一次的送进去做更新。</p><p>hidden一开始是随机的，之后t时刻的hidden的值是由上一时刻，也就是t-1时刻的x和hidden来影响的。</p><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livescript">h0 -&gt; random<br><span class="hljs-function"><span class="hljs-params">(x0, h0)</span> -&gt;</span> output1, h1<br><span class="hljs-function"><span class="hljs-params">(x1, h1)</span> -&gt;</span> output2, h2<br>...<br></code></pre></td></tr></table></figure><p>这样，输出的output2不仅是x1的影响，也是受到x0的影响的，这样前后的关系就被连接起来了。</p><p>就比如说我们输入的是一段文字，就比说<code>ChaHeng</code>，输入<code>C</code>的时候，我们会得到一个hidden，然后计算<code>h</code>时候，我们又会得到一个hidden,一直到最后一个<code>g</code>，那我们算这一步的时候，它既包含了<code>g</code>这个字母，还包含了之前<code>n</code>的hidden向量。那<code>n</code>再往上，一直到<code>C</code>都相关，这样它就实现了传递的效果。</p><p>那这个做法有两个人分别提出来了两种。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501005.png"alt="Alt text" /></p><p>之前，我们将神经网络建模为:</p><p><span class="math display">\[\begin{align*}y_t = \sigma(Wx_t + b) \\y_{t+1} = \sigma(Wx_{t+1} + b)\end{align*}\]</span></p><p>现在我们将其更新为两两种方法，一个是Elman network:</p><p><span class="math display">\[\begin{align*}h_t &amp; = \sigma_h(W_hx_t + U_hh_{t-1}+b_h) \\y_t &amp; = \sigma_y(W_yh_t + b_y)\end{align*}\]</span></p><p>还有一个是Jordan networks:</p><p><span class="math display">\[\begin{align*}h_t &amp; = \sigma_h(W_hx_t + U_hy_{t-1}+b_h) \\y_t &amp; = \sigma_y(W_yh_t + b_y)\end{align*}\]</span></p><p>我们看一下区别，其实就是为了加上非线性变化。给h加了一个非线性变化，再给y加了一个非线性变化。</p><p>这两个人都是很著名的计算机科学家，他们提出来的模型有区别，一个是一直在传递这个h，一个是一直在传递y。但是都实现了yt时刻和xt有关，也和x_{t-1}有关。这两个都实现了这样的一种功能，只不过它们中间一直传递的东西不太一样。</p><p>这个就是RNN的内核，它的内核就是这个东西。</p><p>我们接着，就来看一个案例，这个案例中的数据是一个盈利数据,还是老样子，数据集我就放在文末了。</p><p>我们这里是一个两个月每天的盈利指数，其中2点几的是盈利比较多，1点几的就是盈利比较少的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">timeserise_revenue = pd.read_csv(<span class="hljs-string">&#x27;~/mount/Sync/data/AI_Cheats/time_serise_revenue.csv&#x27;</span>)<br>sales_data = pd.read_csv(<span class="hljs-string">&#x27;~/mount/Sync/data/AI_Cheats/time_serise_sale.csv&#x27;</span>)<br><br>timeserise_revenue.drop(axis=<span class="hljs-number">1</span>, columns=<span class="hljs-string">&#x27;Unnamed: 0&#x27;</span>, inplace=<span class="hljs-literal">True</span>)<br>sales_data.drop(axis=<span class="hljs-number">1</span>, columns=<span class="hljs-string">&#x27;Unnamed: 0&#x27;</span>, inplace=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>数据上我就不展示了，大家自己拿到后查看一下。我们现在要做的是，是想根据它前十天的一个数据，来预测一下第11天的数据。</p><p>很简单的方法咱们可以写一个全连接的网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FullyConnected</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x_size, hidden_size, output_size</span>):<br>        <span class="hljs-built_in">super</span>(FullyConnected, self).__init__()<br>        self.hidden_size = hidden_size<br><br>        self.linear_with_tanh = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">10</span>, self.hidden_size),<br>            nn.Tanh(),<br>            nn.Linear(self.hidden_size, self.hidden_size),<br>            nn.Tanh(),<br>            nn.Linear(self.hidden_size, output_size)<br>        )<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        yhat = self.linear_with_tanh(x)<br>        <br>        <span class="hljs-keyword">return</span> yhat<br></code></pre></td></tr></table></figure><p>我们输入10个值对它进行线性变化，再给它进行一个非线性变化，然后重复一遍，最后再来一次线性变化，这样就是最简单的一种线性和非线性变化的网络。</p><p>然后我们处理一下数据，设置一下相关参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">sales_data.drop(axis=<span class="hljs-number">1</span>, columns=<span class="hljs-string">&#x27;Unnamed: 0&#x27;</span>, inplace=<span class="hljs-literal">True</span>)<br>source_data = sales_data<br><br>n_epochs = <span class="hljs-number">30</span><br>hidden_size = <span class="hljs-number">2</span> <span class="hljs-comment"># try to change this parameters </span><br>n_layers = <span class="hljs-number">1</span><br>batch_size = <span class="hljs-number">5</span><br>seq_length = <span class="hljs-number">10</span><br>n_sample_size = <span class="hljs-number">50</span><br><br>x_size = <span class="hljs-number">1</span><br><br>fc_model = FullyConnected(x_size, hidden_size, output_size=seq_length)<br>fc_model = fc_model.double()<br><br>criterion = nn.MSELoss()<br>optimizer = optim.SGD(fc_model.parameters(), lr=<span class="hljs-number">0.01</span>)<br><br>fc_losses = np.zeros(n_epochs) <br><br>plt.imshow(fc_model.state_dict()[<span class="hljs-string">&#x27;linear_with_tanh.0.weight&#x27;</span>])<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501006.png"alt="Alt text" /></p><p>显示了一下一开始的权重。</p><p>之后我们来看一下整个的训练过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">data_loader = torch.utils.data.DataLoader(source_data.values, batch_size=seq_length, shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<br>    epoch_losses = []<br>    <span class="hljs-keyword">for</span> iter_, t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):<br>        random_index = random.randint(<span class="hljs-number">0</span>, t.shape[-<span class="hljs-number">1</span>] - seq_length - <span class="hljs-number">1</span>)<br>        train_x = t[:, random_index: random_index+seq_length]<br>        train_y = t[:, random_index + <span class="hljs-number">1</span>: random_index + seq_length + <span class="hljs-number">1</span>]<br><br>        outputs = fc_model(train_x.double())<br><br>        optimizer.zero_grad()<br>        loss = criterion(outputs, train_y)<br>        loss.backward()<br>        optimizer.step()<br><br>        epoch_losses.append(loss.detach())<br>    fc_losses[epoch] = np.mean(epoch_losses)<br></code></pre></td></tr></table></figure><p>传入的<code>data_loader</code>是每一次随机的取期望的10个数字,这个数字我们就会根据序列来取出x和y,然后把x送到模型里边得到outputs,得到outputs之后又出现熟悉的面孔,我们求它的loss，再通过它的loss做反向传播。</p><p>optimizer做step，就是做全程的更新。</p><p>之后我们可以将每次循环的结果打印出来看看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<br>    ...<br>    <span class="hljs-keyword">for</span> iter_, t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):<br>        ...<br>        <span class="hljs-keyword">if</span> iter_  == <span class="hljs-number">0</span>:<br>            plt.clf()<br>            plt.ion()<br>            plt.title(<span class="hljs-string">&quot;Epoch &#123;&#125;, iter &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch, iter_))<br>            plt.plot(torch.flatten(outputs.detach()),<span class="hljs-string">&#x27;r-&#x27;</span>,linewidth=<span class="hljs-number">1</span>,label=<span class="hljs-string">&#x27;Output&#x27;</span>)<br>            plt.plot(torch.flatten(train_y),<span class="hljs-string">&#x27;c-&#x27;</span>,linewidth=<span class="hljs-number">1</span>,label=<span class="hljs-string">&#x27;Label&#x27;</span>)<br>            plt.plot(torch.flatten(train_x),<span class="hljs-string">&#x27;g-&#x27;</span>,linewidth=<span class="hljs-number">1</span>,label=<span class="hljs-string">&#x27;Input&#x27;</span>)<br>            plt.draw()<br>            plt.pause(<span class="hljs-number">0.05</span>)<br></code></pre></td></tr></table></figure><p>我们就不全展示了，大家可以自行去运行一下。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501007.png"alt="Alt text" /></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501008.png"alt="Alt text" /></p><p>红色是预测值，绿色是输入值，蓝色是实际值。这里我只放了第一张和第30张，也就是本次循环的最后一张。</p><p>那一开始，预测出来值没有和我们实际的值相符，到了30的相较而言是比较相符了。</p><p>我们看看它的loss是否如预期的下降了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.plot(fc_losses)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501009.png"alt="Alt text" /></p><p>看完全连接的模型，再来看看RNN的模型，做一个非常简单的RNN模型，那首先还是定义模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleRNN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x_size, hidden_size, n_layers, batch_size, output_size</span>):<br>        <span class="hljs-built_in">super</span>(SimpleRNN, self).__init__()<br>        self.hidden_size = hidden_size<br>        self.n_layers = n_layers<br>        self.batch_size = batch_size<br>        self.rnn = nn.RNN(x_size, hidden_size, n_layers, batch_first=<span class="hljs-literal">True</span>)<br>        self.out = nn.Linear(hidden_size, output_size) <span class="hljs-comment"># 10 in and 10 out</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, hidden=<span class="hljs-literal">None</span></span>):<br>        hidden = self.__init__hidden()<br>        output, hidden = self.rnn(inputs.<span class="hljs-built_in">float</span>(), hidden.<span class="hljs-built_in">float</span>())<br>        output = self.out(output.<span class="hljs-built_in">float</span>());<br><br>        <span class="hljs-keyword">return</span> output, hidden<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__hidden</span>(<span class="hljs-params">self</span>):<br>        hidden = torch.zeros(self.n_layers, self.batch_size, self.hidden_size, dtype=torch.float64)<br>        <span class="hljs-keyword">return</span> hidden<br></code></pre></td></tr></table></figure><p>我们输入的是<code>x_size</code>，然后然后定义一个<code>hidden_size</code>。这里注意啊，<code>hidden_size</code>是可以改的，越大可以表示的中间层的信息就越多，但意味着需要更多的数据去训练它。</p><p>然后在forward里，可以看到每一步会输出一个output，到最后一步的时候我们把output做一个线性变化，就可以变成期望的这个结果。</p><p>那这个RNN模型其实非常的简单，就是进了一个RNN，然后做了一个线性变化，把output做成线性变化。</p><p>然后我们来看看具体表现如何，那首先一样的是定义参数，数据可以用上一次整理过的数据，不需要再做一次了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">n_epochs = <span class="hljs-number">30</span><br>hidden_size = <span class="hljs-number">2</span> <span class="hljs-comment"># try to change this parameters </span><br>n_layers = <span class="hljs-number">1</span><br>batch_size = <span class="hljs-number">5</span><br>seq_length = <span class="hljs-number">10</span><br>n_sample_size = <span class="hljs-number">50</span><br><br>x_size = <span class="hljs-number">1</span><br>output_size = <span class="hljs-number">1</span><br><br>hidden = <span class="hljs-literal">None</span><br><br>rnn_model = SimpleRNN(x_size, hidden_size, n_layers, seq_length, output_size)<br><br>criterion = nn.MSELoss()<br>optimizer = optim.SGD(rnn_model.parameters(), lr=<span class="hljs-number">0.01</span>)<br><br>rnn_losses = np.zeros(n_epochs) <br></code></pre></td></tr></table></figure><p>然后我们就可以来跑一下了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">data_loader = torch.utils.data.DataLoader(source_data.values, batch_size=seq_length, shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<br>    <span class="hljs-keyword">for</span> iter_, t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):<br>        <span class="hljs-keyword">if</span> t.shape[<span class="hljs-number">0</span>] != seq_length: <span class="hljs-keyword">continue</span> <br><br>        random_index = random.randint(<span class="hljs-number">0</span>, t.shape[-<span class="hljs-number">1</span>] - seq_length - <span class="hljs-number">1</span>)<br>        train_x = t[:, random_index: random_index+seq_length]<br>        train_y = t[:, random_index + <span class="hljs-number">1</span>: random_index + seq_length + <span class="hljs-number">1</span>]<br><br>        outputs, hidden = rnn_model(train_x.double().unsqueeze(<span class="hljs-number">2</span>), hidden)<br><br>        optimizer.zero_grad()<br>        loss = criterion(outputs.double(), train_y.double().unsqueeze(<span class="hljs-number">2</span>))<br>        loss.backward()<br>        optimizer.step()<br><br>        epoch_losses.append(loss.detach())<br>    rnn_losses[epoch] = np.mean(epoch_losses)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501010.png"alt="Alt text" /></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501011.png"alt="Alt text" /></p><p>那RNN模型其实从第三轮的时候效果就已经出现了，我们的x一样，改变了一个模型之后拟合的效果就不一样了。</p><p>我们来看看它的loss：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501012.png"alt="Alt text" /></p><p>RNN模型跑下来，loss是下降到了0.67左右，那我们之前的全连接模型的loss是在0.8以上，还是有一些区别的。我们可以将两个模型的loss打印到一张图上，就更能看出来两个模型的区别了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.plot(rnn_losses, c=<span class="hljs-string">&#x27;red&#x27;</span>)<br>plt.plot(fc_losses, c=<span class="hljs-string">&#x27;green&#x27;</span>)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501013.png"alt="Alt text" /></p><p>就可以看到，非常明显。</p><p>举这个例子作用是想说明，<code>wx+b</code>加上非线性变化这种形式其实也能解决问题，但是遇到时间相关，序列相关的问题的时候，解决效果就没有RN模型这么好。</p><p>为什么没有RNN模型好呢？因为RNN模型在这个过程中每一步把前一步的hidden的影响给它保留了下来。就是说它每一步的输出的时候不是单纯的考虑这一步的输出，把之前每一步的x的值其实都保留下来了。这个区别就是为什么要有RNN，以及大家之后什么时候用RNN。</p><p>因为我这边只是做个测试，所以仅仅做了30次epoch，那之后，大家可以尝试一下将epoch改成200或者更多，来看看具体loss会下降到什么程度。</p><p>好，文章最后，就是本文所用的数据集了：</p><blockquote><p>time_serise_revenue.csv</p></blockquote><p>链接: https://pan.baidu.com/s/1dL9XdBgoi3nC2VOC6w_wnw?pwd=qmw6提取码: qmw6 --来自百度网盘超级会员v6的分享</p><blockquote><p>time_serise_sale.csv</p></blockquote><p>链接: https://pan.baidu.com/s/12wMJHzSZk91YPFcaG-K6Eg?pwd=1kmp提取码: 1kmp --来自百度网盘超级会员v6的分享</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;../../../../../../Volumes/Media%20Database/%E5%9B%BE%E7%89%87/Screenshots/202311051508354.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>26. 深度学习进阶 - 深度学习的优化方法</title>
    <link href="https://hivan.me/26.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
    <id>https://hivan.me/26.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</id>
    <published>2023-12-01T23:30:00.000Z</published>
    <updated>2023-12-16T08:14:13.145Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311040226671.png"alt="Alt text" /></p><span id="more"></span><p>Hi, 你好。我是茶桁。</p><p>上一节课中我们预告了，本节课是一个难点，同时也是一个重点，大家要理解清楚。</p><p>我们在做机器学习的时候，会用不同的优化方法。</p><h2 id="sgd">SGD</h2><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311040226673.png"alt="Alt text" /></p><p>上图中左边就是Batch Gradient Descent，中间是Mini-Batch GradientDescent, 最右边则是Stochastic Gradient Descent。</p><p>我们还是直接上代码写一下来看。首先我们先定义两个随机值，一个x，一个ytrue：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>x = np.random.random(size=(<span class="hljs-number">100</span>, <span class="hljs-number">8</span>))<br>ytrue = torch.from_numpy(np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">5</span>, size=(<span class="hljs-number">100</span>, <span class="hljs-number">1</span>)))<br></code></pre></td></tr></table></figure><p>x是一个100<em>8的随机值，ytrue是100</em>1的随机值，在0到5之间，这100个x对应着这100个ytrue的输入。</p><p>然后我们来定义一个Sequential,在里面按顺序放一个线性函数，一个Sigmoid激活函数，然后再来一个线性函数，别忘了咱们上节课所讲的，要注意x的维度大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">linear = torch.nn.Linear(in_features=<span class="hljs-number">8</span>, out_features=<span class="hljs-number">1</span>)<br>sigmoid = torch.nn.Sigmoid()<br>linear2 = torch.nn.Linear(in_features=<span class="hljs-number">1</span>, out_features=<span class="hljs-number">1</span>)<br><br>train_x = torch.from_numpy(x)<br><br>model = torch.nn.Sequential(linear, sigmoid, linear2).double()<br></code></pre></td></tr></table></figure><p>我们先来看一下训练x和ytrue值的大小：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(model(train_x).shape)<br><span class="hljs-built_in">print</span>(ytrue.shape)<br><br>---<br>torch.Size([<span class="hljs-number">100</span>, <span class="hljs-number">1</span>])<br>torch.Size([<span class="hljs-number">100</span>, <span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>然后我们就可以来求loss了，先拿到预测值，然后将预测值和真实值一起放进去求值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_fn = torch.nn.MSELoss()<br>yhat = model(train_x)<br>loss = loss_fn(yhat, ytrue)<br><span class="hljs-built_in">print</span>(loss)<br><br>---<br><span class="hljs-number">36.4703</span><br></code></pre></td></tr></table></figure><p>我们现在可以定义一个optimer，来尝试进行优化，我们来将之前的所做的循环个100次，在其中我们加上反向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">optimer = torch.optim.SGD(model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br><br><span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    yhat = model(train_x)<br>    loss = loss_fn(yhat, ytrue)<br>    loss.backward()<br>    <span class="hljs-built_in">print</span>(loss)<br>    optimer.step()<br><br>---<br>tensor(<span class="hljs-number">194.9302</span>, dtype=torch.float64, grad_fn=&lt;MseLossBackward0&gt;)<br>...<br>tensor(<span class="hljs-number">1.9384</span>, dtype=torch.float64, grad_fn=&lt;MseLossBackward0&gt;)<br><br></code></pre></td></tr></table></figure><p>可以看到，loss会一直降低。从194一直降低到了2左右。</p><p>在求解loss的时候，我们用到了所有的<code>train_x</code>，那这种方式就叫做Batchgradient Descent，批量梯度下降。</p><p>它会对整个数据集计算损失函数相对于模型参数的梯度。梯度是一个矢量，包含了每个参数相对与损失函数的变化率。</p><p>这个方法会使用计算得到的梯度来更新模型的参数。更新规则通常是按照一下方式进行：</p><p><span class="math display">\[\begin{align*}w_{t+1} = w_t - \eta \triangledown w_t\end{align*}\]</span></p><p><span class="math inline">\(w_{t+1}\)</span>是模型参数，<spanclass="math inline">\(\eta\)</span>是学习率， <spanclass="math inline">\(\triangledownw_t\)</span>是损失函数相对于参数的梯度。</p><p>但是在实际的情况下这个方法可能会有一个问题，比如说，我们在随机x的时候参数不是100，而是10^8，维度还是8维。假如它的维度很大，那么会出现的情况就是把x给加载到模型里面运算的时候，消耗的内存就会非常非常大，所需要的运算空间就非常大。</p><p>这也就是这个方法的一个缺点，计算成本非常高，由于需要计算整个训练数据集的梯度，因此在大规模数据集上的计算成本较高。而且可能会卡在局部最小值，难以逃离。说实话，我上面演示的数据也是尝试了几次之后拿到一次满意的，也遇到了在底部震荡的情况。</p><p>在这里可以有一个很简单的方法，我们规定每次就取20个:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span> // <span class="hljs-number">20</span>):<br>        batch_index = np.random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(train_x)), size=<span class="hljs-number">20</span>)<br><br>        yhat = model(train_x[batch_index])<br>        loss = loss_fn(yhat, ytrue[batch_index])<br>        loss.backward()<br>        <span class="hljs-built_in">print</span>(loss)<br>        optimer.step()<br></code></pre></td></tr></table></figure><p>这样做loss也是可以下降的，那这种方法就叫做Mini Batch。</p><p>还有一种方法很极端，就是Stochhastic GradientDescent，就是每次只取一个个数字:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span> // <span class="hljs-number">1</span>):<br>        ...<br></code></pre></td></tr></table></figure><p>这种方法很极端，但是可以每次都可以运行下去。那大家就知道，有这三种不同的优化方式。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311040226674.png"alt="Alt text" /></p><p>这样的话，我们来看一下，上图中的蓝色，绿色和紫色，分别对应哪种训练方式？</p><p>紫色的是Stochastic GradientDescent，因为它每次只取一个点，所以它的loss变化会很大，随机性会很强。换句话说，这一次取得数据好，可能loss会下降，如果数据取得不好，它的这个抖动会很大。</p><p>绿色就是Mini-Batch,我们刚才20个、20个的输入进去，是有的时候涨，有的时候下降。</p><p>最后蓝色的就是Batch Gradient Descent，因为它x最多，所以下降的最稳定。</p><p>但是因为每次x特别多内存，那有可能就满了。内存如果满了，机器就没有时间去运行程序，就会变得特别的慢。</p><h2 id="momentum">MOMENTUM</h2><p>我们上面讲到的了这个式子：</p><p><span class="math display">\[\begin{align*}w_{t+1} = w_t - \eta \triangledown w_t\end{align*}\]</span></p><p>这个是最原始的Grady descent,我们会发现一个问题，就是本来在等高线上进行梯度下降的时候，它找到的不是最快的下降的那条线，在实际情况中，数据量会很多，数量会很大。比方说做图片什么的，动辄几兆几十兆，如果要再加载几百个这个进去，那就会很慢。这个梯度往往可能会变的抖动会很大。</p><p>那有人就想了一个办法去减少抖动。就是我们每一次在计算梯度下降方向的时候，连带着上一次的方向一起考虑，然后取一个比例改变了原本的方向。那这样的话，整个梯度下降的线就会平缓了，抖动也就没有那么大，这个就叫做Momentum,动量。</p><p><span class="math display">\[\begin{align*}v_t &amp; = \gamma \cdot v_{t-1} + \eta \triangledown w_t \\w_{t+1} &amp; = w_t - v_t\end{align*}\]</span></p><p>动量在物理学中就是物体沿某个方向运动的能量。</p><p>之前我们每次的wt是直接去减去学习率乘以梯度，现在还考虑了v{t-1}的值，乘上一个gamma，这个值就是我们刚才说的取了一个比例。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311040226675.png"alt="Alt text" /></p><p>就像这个图一样，原来是红色，加了动量之后就变成蓝色，可以看到更平稳一些。</p><h2 id="rms-prop">RMS-PROP</h2><p>除了动量法之外呢，还有一个RMS-PROP方法，全称为Root mean squareprop。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311040226676.png"alt="Alt text" /></p><p><span class="math display">\[\begin{align*}S_{\frac{\partial loss}{\partial w}} &amp; = \beta S_{\frac{\partialloss}{\partial w}} + (1 - \beta)||\frac{\partial loss}{\partial w} ||^2\\S_{\frac{\partial loss}{\partial b}} &amp; = \beta S_{\frac{\partialloss}{\partial b}} + (1 - \beta)||\frac{\partial loss}{\partial b} ||^2\\w &amp; = w - \alpha \frac{\frac{\partial loss}{\partialw}}{\sqrt{S_{\frac{\partial loss}{\partial w}}}} \\b &amp; = b - \alpha \frac{\frac{\partial loss}{\partialb}}{\sqrt{S_{\frac{\partial loss}{\partial b}}}}\end{align*}\]</span></p><p>这个方法看似复杂，其实也是非常简单。这些方法在PyTorch里其实都有包含，我们可以直接调用。我们在这里还是要理解一下它的原理，之后做事的时候也并不需要真的取从头写这些玩意。</p><p>在讲它之前，我们再回头来说一下刚刚求解的动量法，动量法其实已经做的比较好了，但是还是有一个问题，它每次的rate是人工定义的。也就是我们上述公式中的<spanclass="math inline">\(\gamma\)</span>,这个比例是人工定义的，那在RMS-PROP中就写了一个动态的调整方法。</p><p>这个动态的调整方法就是我们每一次在进行调整w或者b的时候，都会除以一个根号下的<spanclass="math inline">\(S_{\frac{\partial loss}{\partialw}}\)</span>，我们往上看，如果<span class="math inline">\(\frac{\partialloss}{\partial w}\)</span>比较大的话，那么<spanclass="math inline">\(S_{\frac{\partial loss}{\partialw}}\)</span>也就将会比较大，那放在下面的式子中，根号下，也就是<spanclass="math inline">\(\sqrt{S_{\frac{\partial loss}{\partialw}}}\)</span>在分母上，那么w就会更小，反之则会更大。</p><p>所以说，当这一次的梯度很大的时候，这样一个方法就让<spanclass="math inline">\(\frac{\partial loss}{\partialw}\)</span>其实变小了，对b来说也是一样的情况。</p><p>也就说，如果上一次的方向变化的很严重，那么这一次就会稍微的收敛一点，就会动态的有个缩放。那么如果上一次变化的很小，那为了加速它，这个值反而就会变大一些。</p><p>所以说他是实现了一个动态的学习率的变化，当然它前面还有一个初始值，这个<spanclass="math inline">\(\gamma\)</span>需要人为设置，但是在这个<spanclass="math inline">\(\gamma\)</span>基础上它实现了动态的学习速率的变化。</p><p>动态的学习速率考察两个值，一个是前一时刻的变化的快慢，另一个就是它此时此刻变化的快慢。这个就叫做RMS。</p><h2 id="adam">ADAM</h2><p>那我们在这里，其实还有一个方法：ADAM。 <span class="math display">\[\begin{align*}V_{dw} &amp; = \beta_1V_{dw} + (1-\beta_1)dw \\V_{db} &amp; = \beta_1V_{db} + (1-\beta_1)db \\S_{dw} &amp; = \beta_2S_{dw} + (1-\beta_2)||dw||^2 \\S_{db} &amp; = \beta_2S_{db} + (1-\beta_2)||db||^2 \\&amp; V_{dw}^{corrected} = \frac{V_{dw}}{1-\beta_1^t} \\&amp; V_{db}^{corrected} = \frac{V_{db}}{1-\beta_1^t} \\&amp; S_{dw}^{corrected} = \frac{S_{dw}}{1-\beta_2^t} \\&amp; S_{db}^{corrected} = \frac{S_{db}}{1-\beta_2^t} \\w &amp; = w -\alpha\frac{V_{db}^{corrected}}{\sqrt{S_{dw}^{corrected}}+\varepsilon}\\b &amp; = b -\alpha\frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\varepsilon}\\\end{align*}\]</span></p><p>刚刚讲过的RMS特点其实是动态的调整了我们的学习率，之前讲Momentum其实还保持了上一时刻的方向，RMS就没有解决这个问题，RMS把上一时刻的方向给弄没了。</p><p>RMS，它的定义其实就没有考虑上次的方向，它只考虑上次变化的大小。而现在提出来这个ADAM，这个ADAM的意思就是AdaptiveMomentum,还记不记得咱们讲随机森林和Adaboost那一节，我们讲过Adaboost就是AdaptiveBoosting，这里的Adaptive其实就是一个意思，就是自适应动量，也叫动态变化动量。</p><p>ADAM就结合了RMS和动量的两个优点。第一个是他在分母上也加了一个根号下的数，也就做了RMS做的事，然后在分子上还有一个数，这个数就保留了上一时刻的数，比如<spanclass="math inline">\(V_{dw}^{corrected}\)</span>，就保留了上一时刻的V，就保留了上一时刻的方向。</p><p>所以ADAM既是动态的调整了学习率，又保留了上一时刻的方向。</p><p>那除此之外，其实还有一个AdaGrad和L-BFGS方法，不过常用的方法也就是上面详细讲的这几种。</p><p>到此为止，我们进阶神经网络的基础知识就都差不多具备了，接下来我们就该来讲解下卷机和序列，比如说LSTM和RNN、CNN的东西。在这些结束之后，我们还会有Attention机制，Transformer机制，YOLO机制，Segmentation机制，还有强化深度学习其实都是基于这些东西。</p><p>那我们下节课，就先从RNN来说开去。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311040226671.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>25. 深度学习进阶 - 权重初始化，梯度消失和梯度爆炸</title>
    <link href="https://hivan.me/25.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/"/>
    <id>https://hivan.me/25.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/</id>
    <published>2023-11-28T23:30:00.000Z</published>
    <updated>2023-11-29T06:18:47.001Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311031851045.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><span id="more"></span><p>咱们这节课会讲到权重初始化、梯度消失和梯度爆炸。咱们先来看看权重初始化的内容。</p><h2 id="权重初始化">权重初始化</h2><p>机器学习在我们使用的过程中的初始值非常的重要。就比如最简单的<code>wx+b</code>，现在要拟合成一个yhat，w如果初始的过大或者初始的过小其实都会比较有影响。</p><p>假设举个极端情况，就是w拟合的时候刚刚就拟合到了离x很近的地方，我们想象一下，这个时候是不是学习起来就会很快？所以对于深度学习模型权重的初始化是一个非常重要的事情，甚至有人就说把初始化做好了，其实绝大部分事情就已经解决了。</p><p>那么我们怎么样获得一个比较好的初始化的值？首先有这么几个原则</p><ul><li>我们的权重值不能设置为0。</li><li>尽量将权重变成一个随机化的正态分布。而且有更大的X输入，那我们的权重就应该更小。</li></ul><p><span class="math display">\[\begin{align*}loss &amp; = \sum(\hat y - y_i)^2 \\&amp; = \sum(\sum w_ix_i - y_i)^2\end{align*}\]</span></p><p>我们看上面的式子，yhat就是w_i*x_i,这个时候x_i可能是几百万，也可能是几百。我们w_i取值在(-n,n)之间，那当x_i维度特别大的时候，那yhat值算出来的也就会特别大。所以，x_i的维度特别大的时候，我们期望w_i值稍微小一些，否则加出来的yhat可能就会特别大，那最后求出来的loss也会特别大。</p><p>如果loss值特别大，可能就会得到一个非常的梯度。那我们知道，学习的梯度特别大的话，就会发生比较大的震荡。</p><p>所以有一个原则，就是当x的dimension很大的时候,我们期望的它的权重越小。</p><p>那后来就有人提出来了一个比较重要的初始化方法，Xavier初始化。这个方法特别适用于sigmoid激活函数或反正切tanh激活函数，它会根据前一层和当前层的神经元数量来选择初始化的范围，以确保权重不会过大或过小。<span class="math display">\[\begin{align*}均值为0和标准差的正态分布: \sigma &amp; =\sqrt{\frac{2}{n_{inputs}+n_{outputs}}} \\-r和+r之间的均匀分布：r &amp; = \sqrt{\frac{6}{n_{inputs}+n_{outputs}}}\end{align*}\]</span></p><p>然后W的均匀分布就会是这样： <span class="math display">\[W \sim U \Bigg \vert -\frac{\sqrt 6}{\sqrt{n_j + n_{j+1}}}, \frac{\sqrt6}{\sqrt{n_j + n_{j+1}}}\]</span></p><p>这个是一个比较有名的初始化方法，如果要做函数的初始化的话，PyTorch在init里面有一个方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nn.init.xavier_uniform_(tensor, gain=<span class="hljs-number">1.0</span>)<br></code></pre></td></tr></table></figure><p>比如，我们看这样例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.xavier_uniform_(w, gain=nn.calculate_gain(<span class="hljs-string">&#x27;relu&#x27;</span>))<br></code></pre></td></tr></table></figure><blockquote><p>注意:init方法里还有其他的一些方法，大家可以查阅PyTorch的相关文档：https://pytorch.org/docs/stable/nn.init.html</p></blockquote><h2 id="梯度消失与梯度爆炸">梯度消失与梯度爆炸</h2><p>当我们的模型层数特别多的时候</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311031849640.png"alt="Alt text" /></p><p>就比如我们上节课用到的Sequential，我们可以在里面写如非常多的一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">model = nn.Sequential(<br>    nn.Linear(in_features=<span class="hljs-number">10</span>, out_features=<span class="hljs-number">5</span>).double(),<br>    nn.Sigmoid(),<br>    nn.Linear(in_features=<span class="hljs-number">5</span>, out_features=<span class="hljs-number">8</span>).double(),<br>    nn.Sigmoid(),<br>    nn.Linear(in_features=<span class="hljs-number">8</span>, out_features=<span class="hljs-number">8</span>).double(),<br>    nn.Sigmoid(),<br>    ...<br>    nn.Linear(in_features=<span class="hljs-number">8</span>, out_features=<span class="hljs-number">8</span>).double(),<br>    nn.Softmax(),<br>)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311031849642.png"alt="Alt text" /></p><p>这样，在做偏导的时候我们其中几个值特别小，那两个一乘就会乘出来一个特别特别小的数字。最后可能会导致一个结果，<spanclass="math inline">\(\frac{\partial loss}{\partialwi}\)</span>的值就会极小，它的更新就会特别的慢。我们把这种东西就叫做梯度消失，也有人叫梯度弥散。</p><p>以Sigmoid函数为例，其导数为</p><p><span class="math display">\[\begin{align*}\sigma &#39;(x) = \sigma(x)(1-\sigma(x))\end{align*}\]</span></p><p>在x趋近正无穷或者负无穷时，导数接近0。当这种小梯度在多层网络中相乘的时候，梯度会迅速减小，导致梯度消失。</p><p>除此之外还有一种情况叫梯度爆炸，剃度爆炸类似，当模型的层很多的时候，如果其中某两个值很大，例如两个10<sup>2，当这两个乘起来就会变成10</sup>4。乘下来整个loss很大，又会产生一个结果，我们来看这样一个场景：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311031849643.png"alt="Alt text" /></p><p>假如说对于上图中这个函数来说，横轴为x,竖轴为loss，对于这个xi来说，这个地方<spanclass="math inline">\(\frac{\partial loss}{\partialxi}\)</span>已经是一个特别大的数字了。</p><p>假设咱们举个极端的情况（<strong>忽略图中竖轴上的数字</strong>），我们现在loss等于x^4：<spanclass="math inline">\(loss=x^4\)</span>，然后现在<spanclass="math inline">\(\frac{\partial loss}{\partialx^4}\)</span>就等于<spanclass="math inline">\(4x^3\)</span>，我们假设x在A点，当x=10的时候，那<spanclass="math inline">\(4\times x^3 = 4000\)</span>，那我们计算新的xi，就是<span class="math inline">\(x_i = x_i - \alpha\cdot \frac{\partial loss}{\partialx_i}\)</span>，现在给alpha一个比较小的数，我们假设是0.1，那式子就变成<spanclass="math inline">\(10 - 0.1 \times 4000\)</span>，结果就是-390。</p><p>我们把它变到-390之后，本来我们本来做梯度下降更新完，xi期望的是loss要下降，但是我们结合图像来看，xi=-390的时候，loss就变得极其的巨大了，然后我们在继续，(-390)^4，这个loss就已经爆炸了。</p><p>再继续的时候，会发现会在极值上跳来跳去，loss就无法进行收敛了。所以我们也要拒绝这种情况的发生。</p><p>那梯度消失和梯度爆炸这两个问题该如何解决呢？我们来看第一种解决方法：<code>Batch normalization</code>，批量归一化。</p><p>那这个方法的核心思想是对神经网络的每一层的输入进行归一化，使其具有零均值和单位方差。</p><p>那么首先，对于每个mini-batch中的输入数据，计算均值和方差。<spanclass="math inline">\(B = \{x_1...m\}\)</span>; 要学习的参数: <spanclass="math inline">\(\gamma,\beta\)</span>。</p><p><span class="math display">\[\begin{align*}\mu_B &amp; = \frac{1}{m}\sum^m_{i=1}x_i \\\sigma ^2_B &amp; = \frac{1}{m}\sum_{i=1}^m(x_i-\mu_B)^2 \\&amp; \mu 为均值mean， \sigma为方差\end{align*}\]</span></p><p>这里和咱们之前讲x做normalization的时候其实是特别相似，基本上就是一件事。</p><p>然后我们使用均值和方差对输入进行归一化，使得其零均值和单位方差，即将输入标准化为xhat。</p><p><span class="math display">\[\begin{align*}\hat x_i = \frac{x_i - \mu_B}{\sqrt{\sigma ^2_B + \varepsilon}}\end{align*}\]</span></p><p>接着我们对归一化后的输入应用缩放和平移操作，以允许网络学习最佳的变换。</p><p><span class="math display">\[\begin{align*}y_i = \gamma \hat x_i + \beta \equiv BN_{\gamma,\beta}(x_i)\end{align*}\]</span></p><p>输出为<span class="math inline">\(\{y_i =BN_{\gamma,\beta}(x_i)\}\)</span>。</p><p>最后将缩放和平移后的数据传递给激活函数进行非线性变换。</p><p>它会输入一个小批量的x值，</p><p>经过反复的梯度下降，会得到一个gamma和beta，能够知道在这一步x要怎么样进行缩放，在缩放之前会经历刚开始的时候那个normalization一样，把把过小值会变大，把过大值会变小。</p><p>我们在之前的课程中演示过，没看过和忘掉的同学可以往前翻看一下。</p><p>然后在经过这两个可学习的参数进行一个变化，这样它可以做到在每一层x变化不会极度的增大或者极度的缩小，可以让我们的权值保持的比较稳定。</p><p>那除了Batch normalization之外，还有一个方法叫Gradient clipping，它是可以直接将过大的梯度值变小。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311031849644.png"alt="Alt text" /></p><p>它其实很简单，也叫做梯度减脂。</p><p>如果我们求解出来<span class="math inline">\(\frac{\partialloss}{\partialw_i}\)</span>很大，假设原来等于400，我们定义了一个100，那超过100的部分，就全部设置成100。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">train_loss.backward()<br>pt.nn.units.clip_grad_value_(model.parameters(), <span class="hljs-number">100</span>)<br>optimizer.step()<br></code></pre></td></tr></table></figure><p>简单粗暴。那其实梯度爆炸还是比较容易解决的，比较复杂的其实是梯度消失的问题。</p><p>梯度爆炸为什么比较容易解决？梯度爆炸起码是有导数的，只要把这个导数给它放的特别小就行了，有导数起码保证wi可以更新。</p><p>假设alpha，我们的learning_rate等于0.01，乘上一个100，可以保证每次可以有个变化。但是每次这个梯度特别小，假如都快接近于0了，那么1e-10,就算乘上100倍，最后还是一个特别小的数字。所以相较而言，梯度爆炸就更好解决一些，方法更粗暴一些。</p><p>补充一个知识点，这个虽然现在已经用不到了，但是对我们的理解还是有帮助的。方法比较古老。</p><p>就是当我们发现梯度有问题的时候，大概在10年前，那个时候神经网络的模块也不太丰富，很多新出的model，做神经网络的人，一些导数，传播什么的都需要自己写，就我们前几节课写那个神经网络框架的时候做的事。</p><p>有的时候导数写错了，就有一种方法叫做gradient checking，梯度检查。</p><p>这个使用场景非常的少，当你自己发明了一个新的模块，加到这个模型里面的时候会遇到。</p><p>其实很简单，就是把最终的<span class="math inline">\(\frac{\partialloss}{\partialw_i}\)</span>，求解出来的偏导总是不收敛，可能是这个偏导有问题，那么有可能求导的函数写错了。</p><p>那在这个时候就可以做个简单的变化：</p><p><span class="math display">\[\begin{align*}\frac{\partial loss(\theta+\varepsilon)-\partial loss(\theta -\varepsilon)}{2\varepsilon}\end{align*}\]</span></p><p>这其中<span class="math inline">\(\partial loss(\theta +\varepsilon)\)</span>和<span class="math inline">\(\partial loss(\theta- \varepsilon)\)</span>是在参数<spanclass="math inline">\(\theta\)</span>,其实也就是我们的wi上添加和减去微小扰动theta后的损失函数值。</p><p>然后我们计算数值梯度和反向传播计算得到的梯度之间的差异。通常这是通过计算它们之间的差异来完成，然后将其与一个小的阈值，比如1e-7进行比较。如果差异非常小（小于阈值），则可以认为梯度计算是正确的，否则可能就需要从新写一下偏导函数了。</p><p>这个比较难，但不是一个重点，当且仅当自己要发明一个模型的时候。</p><p>那接下来我们来看一下关于Learning_rate和Early Stopping的问题。</p><p>理论上，如果深度学习效果不好，那么我们可以将learningrate调小，可以让所有模型效果变得更好，它可以让所有的loss下降。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311031849645.png"alt="Alt text" /></p><p>但是如果你的learningrate变得特别小，假如说是1e-9，那这样的结果就是w的变化会非常的慢，训练时间就变得很长。为了解决这个问题，就有一些比较简单的方法。</p><p>第一个，我们可以把learningrate和loss设置成一个相关的函数，例如说loss越小的时候，Learningrate越小，或者随着epoch的增大，loss越小。这个就叫learningrate的decay。</p><p>将learningrate或者训练次数和loss设置成一个相关的函数，那么越到后面效果越好的时候，learningrate就会越小。</p><p>还有，我们可能会发现loss连续k次不下降，那我们就可以提前结束训练过程，这个就是EarlyStopping。</p><p>也就是当你发现loss连续k次不下降，或者甚至于在上升，那么这个时候，就可以将最优的这个值给它记录下来。</p><p>咱们可能会经常出现的情况就是值在那里震荡，本来呢已经快接近于最优点了，可是震荡了几次之后，还可能震荡出去了，loss变大了。或者就一直在这个震荡里边出不去，这个时候多学习也没有用，所以就可以早点停止，这个就是EarlyStopping，中文有人称呼它为早停方法。</p><p>好，下节课，咱们要讲一个重点，也是一个难点。就是咱们做机器学习的时候，不同的优化方法。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311031851045.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>24. 深度学习进阶 - 矩阵运算的维度和激活函数</title>
    <link href="https://hivan.me/24.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>https://hivan.me/24.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</id>
    <published>2023-11-25T23:30:00.000Z</published>
    <updated>2023-11-29T06:18:52.997Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030256914.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><span id="more"></span><p>咱们经过前一轮的学习，已经完成了一个小型的神经网络框架。但是这也只是个开始而已，在之后的课程中，针对深度学习我们需要进阶学习。</p><p>我们要学到超参数，优化器，卷积神经网络等等。看起来，任务还是蛮重的。</p><p>行吧，让我们开始。</p><h2 id="矩阵运算的维度">矩阵运算的维度</h2><p>首先，我们之前写了一份拓朴排序的代码。那我们是否了解在神经网络中拓朴排序的作用。我们前面讲过的内容大家可以回忆一下，拓朴排序在咱们的神经网络中的作用不是为了计算方便，是为了能计算。</p><p>换句话说，没有拓朴排序的话，根本就没法计算了。Tensorflow和PyTourh最大的区别就是，Tensorflow在运行之前必须得把拓朴排序建好，PyTorch是在运行的过程中自己根据我们的连接状况一边运行一边建立。但是它们都有拓朴排序。</p><p>拓朴排序后要进行计算，那就要提到维度问题，在进行机器学习的时候一定要确保我们矩阵运算的维度正确。我们来看一下示例就明白我要说的了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.from_numpy(np.random.random(size=(<span class="hljs-number">4</span>, <span class="hljs-number">10</span>)))<br><span class="hljs-built_in">print</span>(x.shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure><p>假如说，现在我们生成了一个4，10的矩阵，也就是4行10列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>linear = nn.Linear(in_features=<span class="hljs-number">10</span>, out_features=<span class="hljs-number">5</span>).double()<br><span class="hljs-built_in">print</span>(linear(x).shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<br></code></pre></td></tr></table></figure><p>然后我们来给他定义一个线性变化，<code>in_features=10</code>，这个就是必须的,然后，out_features=5，假如把它分成5类。</p><p>这个时候，你看他就变成一个四行五列的一个东西了。</p><p>刚才我们说了，<code>in_features=10</code>是必须的，如果这个值我们设置成其他的，比如说8，那就不行了，运行不了。会收到警告：<code>mat1 and mat2 shapes cannot be multiplied (4x10 and 8x5)</code></p><p>我们再给它来一个Softmax</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">nonlinear = nn.Softmax()<br><span class="hljs-built_in">print</span>(nonlinear(linear(x)))<br></code></pre></td></tr></table></figure><p>这样，我们就得到了一个4*5的概率分布。</p><p>我们把这个非线性函数换一下，换成Sigmoid, 之前的Softmax赋值给yhat,咱们做一个多层的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">yhat = nn.Softmax()<br>nonlinear = nn.Sigmoid()<br>linear2 = nn.linear(in_features=n, out_features=<span class="hljs-number">8</span>).double()<br><br><span class="hljs-built_in">print</span>(yhat(linear2(nonlinear(linear(x)))))<br></code></pre></td></tr></table></figure><p>好，这个时候，我还并没有给<code>in_features</code>赋值，我们来想想，这个时候应该赋值是多少？也就是说，我们现在的linear2到底传入的特征是多少？</p><p>我们这里定义的<code>linear</code>和<code>linear2</code>其实就是<code>w*x+b</code>。</p><p>那这里我们来推一下，第一次使用linear的时候，我们得到了4*5的矩阵对吧？nonlinear并没有改变矩阵的维度。现在linear2中，那我们<code>in_features</code>赋值就得是5对吧？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">yhat = nn.Softmax()<br>nonlinear = nn.Sigmoid()<br>linear2 = nn.linear(in_features=<span class="hljs-number">5</span>, out_features=<span class="hljs-number">8</span>).double()<br><br><span class="hljs-built_in">print</span>(yhat(linear2(nonlinear(linear(x)))).shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">8</span>])<br></code></pre></td></tr></table></figure><p>然后我们就得到了一个<code>4*8</code>的维度的矩阵。</p><p>那其实在PyTorch里提供了一种比较简单的方法，就叫做<code>Sequential</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">model = nn.Sequential(<br>    nn.Linear(in_features=<span class="hljs-number">10</span>, out_features=<span class="hljs-number">5</span>).double(),<br>    nn.Sigmoid(),<br>    nn.Linear(in_features=<span class="hljs-number">5</span>, out_features=<span class="hljs-number">8</span>).double(),<br>    nn.Softmax(),<br>)<br><br><span class="hljs-built_in">print</span>(model(x).shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">8</span>])<br></code></pre></td></tr></table></figure><p>这样，我们就把刚才几个函数方法按顺序都一个一个的写在<code>Sequential</code>里，那其实刚才的过程，也就是解释了这个方法的原理。</p><p>接着，我们来写一个<code>ytrue</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">ytrue = torch.randint(<span class="hljs-number">8</span>, (<span class="hljs-number">4</span>, ))<br>loss_fn = nn.CrossEntropyLoss()<br><br><span class="hljs-built_in">print</span>(model(x).shape)<br><span class="hljs-built_in">print</span>(ytrue.shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">8</span>])<br>torch.Size([<span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure><p>现在ytrue就是CrossEntropyLoss输入的一个label值。</p><p>然后我们就可以进行反向传播了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">loss.backward()<br><br><span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters():<br>  <span class="hljs-built_in">print</span>(p, p.grad)<br>  <br>---<br>Parameter containing:<br>tensor([...])<br>...<br></code></pre></td></tr></table></figure><p>求解反向传播之后就可以得到它的梯度了。然后再经过一轮一轮的训练，就可以把梯度稳定在某个值，这就是神经网络进行学习的一个过程。那主要是在这个过程中，一定要注意矩阵前后的大小。</p><h2 id="激活函数">激活函数</h2><p>然后我们来看看激活函数的重要性。</p><p>在我们之前的课程中，我们提到过一个概念「激活函数」，不知道大家还有没有印象。那么激活函数的作用是什么呢？是实现非线性拟合对吧？</p><p>打比方来说，如果我们现在要拟合一个函数<code>f(x) = w*x+b</code>,你把它再给送到一个g(x)，再比如<code>g(x)=w2*x+b</code>，我们来做一个拟合，那么g(f(x)),那是不是还是一样，<code>g(f(x)) = w2*(w*x+b)+b</code>,然后就变成<code>w2*w*x + w2*b + b</code>，那其实这个就还是一个线性函数。</p><p>我们每一段都给它进行一个线性变化，再进行一个非线性变化，再进行一个线性变化，一段一段这样折起来，理论上它可以拟合任何函数。</p><p>这个怎么理解？其实我们如何用已知的函数去拟合函数在高等数学里边是一个一直在学习，一直在研究的东西。学高数的同学应该知道，高数里面有一个著名的东西叫做傅立叶变化，这是一种线性积分变换，用于函数在时域和频域之间的变换。</p><p>我们给定任意一个复杂的函数，都可以通过sin和cos来把它拟合出来，其关键思想是任何连续、周期或非周期的函数都可以表示为正弦和余弦函数的组合。通过计算不同频率的正弦和余弦成分的系数an和bn，我们可以了解一个函数的频谱特性，即它包含那些频率成分。</p><p><span class="math display">\[\begin{align*}f(x) = a_0 + \sum_{n=1}^0(a_n cos(2\pi nfx) + b_n sin(2\pi n fx))\end{align*}\]</span></p><p>除此之外，我们还有一个泰勒展开。我在数学篇的时候有仔细讲解过这个部分，大家可以回头去读一下我那篇文章，应该是数学篇第13节课，在那里我曾说过，所有的复杂函数都是用泰勒展开转换成多项式函数计算的。</p><p>之前有同学给我私信，也有同学在我文章下留言，说到某个位置看不懂了，还是数学拖了后腿。但是其实只是应用的话无所谓，但是如果想在这个方面有所建树，想要做些不一样的东西出来，还是要把数学的东西好好补一下的。</p><p>OK，那其实呢，我们的深度学习本质上其实就是在做这么一件事情，就是来自动拟合，到底是由什么构成的。</p><p>大家再来想一下，一个比较重要的，就是反向传播和前向传播。这个我们前面的课程里有详细的讲过，就是，我们的前向传播和反向传播的作用是什么。</p><p>那现在我们学完前几节了，回过头来我们想想，前向传播的作用是什么？反向传播的这个作用呢？</p><p>现在，假如说我已经训练出来了一个模型，我要用这个模型去预测。那么第一个问题是，预测的时候需不需要求loss？第二个是我需不需要做反向传播？</p><p>然后我们再来思考一个问题，如果我们需要求loss对于某个参数wi的偏导<spanclass="math inline">\(\frac{\partial loss}{\partialw_i}\)</span>，那么我们首先需要进行反向传播对吧？那我们在进行反向传播之前，能不能不进行前向传播？</p><p>也就是说，我们把这个模型放在这里，一个x，然后输入进去得到一个loss。那么咱们训练了一轮之后，我们能不能在求解的时候不进行前向传播，直接进行反向传播？</p><p>我们只要知道，求loss值需要预测值就明白了。</p><p>那我们继续来思考，loss值和precision、recall等等的关系是什么？这些是什么？我们之前学习过，这些是评测指标对吧？也就是再问，loss和评测指标的关系是什么？</p><p>也就是说，我们能不能用precision，能不能用precision来做我们的loss函数？不能对吧，无法求导。</p><p>所以在整个机器学习的过程中，如果要有反向传播、梯度下降，必须得是可导的。像我们所说的MSE是可导的，<code>cross-entropy</code>也是可以求导的。</p><p>那如果上过我之前课程的同学应该记得，可求导的的函数需要满足什么条件？光滑性和连续性对吧？连续性呢，是可求导的一个必要条件，但不是充分条件，还必须在某个点附近足够光滑，以使得导数存在。</p><p>对于loss函数的设定，第一点，一定是要能求偏导的。第二呢，就是它一定得是一个凸函数:Convexfunctions。</p><p>那什么叫做凸函数呢？如果一个函数上的任意两点连线上的函数值都不低于这两点的函数值的线段，就称为凸函数。常见的比如线性函数，指数函数，幂函数，绝对值函数等都是凸函数。</p><p>想象一下，有一辆车，从a点开到b点，如果这个车在a点到b点的时候方向盘始终是打在一个方向的，那我们就说它是凸函数。</p><p>不过在一些情况下有些函数它不是凸函数，就在数学上专门有一个研究领域，Convexoptimization，凸优化其实就是解决对于这种函数怎么样快速的求出他的基值，另外一个就是对于这种非凸函数怎么把它变成凸函数。</p><p>不同的激活函数它有什么区别呢？在最早的时候，大家用的是<code>Sigmoid</code>：</p><p><span class="math display">\[\begin{align*}\sigma(z)=\frac{1}{1+e^{-z}}\end{align*}\]</span></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030258830.png"alt="Sigmoid" /></p><p>为什么最早用Sigmoid，这是因为Sigmoid有个天然的优势，就是它输出是0-1，而且它处处可导。</p><p>但是后来Sigmoid的结果有个e^x，指数运算就比较费时，这是第一个问题。第二个问题是Sigmoid的输出虽然是在0~1之间，但是平均值是0.5，对于程序来说，我们希望获得均值等于0，STD等于1。我们往往希望把它变成这样的一种函数，这样的话做梯度下降的时候比较好做。</p><p>于是就又提出来了一个更简单的方法，就是反正切函数：<code>Tanh</code>。</p><p><span class="math display">\[\sigma(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\]</span></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030258831.png"alt="Tanh" /></p><p>它的形式和Sigmoid很像，不同的是平均值，它的平均值是0。现在这个用的也挺多。</p><p>但是Tanh和sigmoid一样都有一个小问题，就是它的绝大多数地方loss都等于0,那么wi大部分时候就没有办法学习，也就不会更新。</p><p>为了解决这个问题，就是有人提出来了一种非常简单的方法，就是<code>ReLU</code>：</p><p><span class="math display">\[\begin{align*}ReLU(z) = \begin{cases} z, z&gt;0 \\ 0, otherwise \end{cases}\end{align*}\]</span></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030258832.png"alt="Alt text" /></p><p>这种方法看似非常简单，但其实非常好用。它就是当一个x值经过ReLU的时候，如果它大于0就还保持原来的值，如果不大于0就直接把它变成0。</p><p>这样大家可能会觉得x&lt;0时有这么多值没有办法求导，但其实比起sigmoid来说可求导的范围其实已经变多了。而且你会发现要对他x大于0的地方求偏导非常的简单，就直接等于1。</p><p>可以保证它肯定是可以做更新的，而且ReLU这种函数它是大量的被应用在卷积神经网络里边。</p><p>在咱们后面的课程中，会讲到卷积，它是有一个卷积核，[F1,F2,F3,F4]然后把它经过ReLU之后，可能会变成[F1，0，F3，0]。那我们只要更新F1，F3就可以了，下一次再经过某种方式，在重新把F2和F4我们重新计算一下。</p><p>也就是说现在的<code>wx+b</code>不像以前一样，只有一个<code>w</code>，如果x值等于0，那整个都等于0.而是我们会有一个矩阵，它部分等于0也没关系。而且它的求导会变得非常的快，比求指数的导数快多了。</p><p>那其实这里还有一个小问题，面试的时候可能会问到，就是ReLU其实在0点的时候不可导，怎么办？</p><p>这个很简单，可以在函数里边直接设置一下，直接给他一个0的值就可以了，就是在代码里面加一句话。</p><p>再后来，又有人提出了一种方法：<code>LeakyRelU</code>：</p><p><span class="math display">\[LeakyReLU(z) = \begin{cases} z, z&gt;0 \\ az, otherwise \end{cases}\]</span></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030258833.png"alt="Alt text" /></p><p>它把小于0的这些地方，也加了一个很小的梯度，这样的话大于0的时候partial就恒等于1，小于的时候partial也恒等于一个值，比如定一个<code>a=0.2</code>,都可以。那这样就可以实现处处有导数。</p><p>但是其实用的也不太多，因为我们事实上发现在这种卷积神经网络里边，我们每一次把部分的权重设置成0不更新，反而可以提升它的训练效率，我们反而可以每次把训练focuson在几个参数上。</p><p>好，下节课，咱们来看看初始化的内容。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311030256914.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>23. 深度学习 - 多维向量自动求导</title>
    <link href="https://hivan.me/23.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%AE%8C%E6%88%90%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A1%86%E6%9E%B6/"/>
    <id>https://hivan.me/23.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%AE%8C%E6%88%90%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A1%86%E6%9E%B6/</id>
    <published>2023-11-21T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:17.365Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509025.png"alt="茶桁的 AI 秘籍 核心能力 23" /></p><span id="more"></span><p>Hi, 你好。我是茶桁。</p><p>前面几节课中，我们从最初的理解神经网络，到讲解函数，多层神经网络，拓朴排序以及自动求导。可以说，最难的部分已经过去了，这节课到了我们来收尾的阶段，没错，生长了这么久，终于到迎接成果的时候了。</p><p>好，让我们开始。</p><blockquote><p>我们还是用上一节课的代码：<code>21.ipynb</code>。</p></blockquote><p>我们上一节课中，实现了自动计算的部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.name))<br>    node.backward()<br></code></pre></td></tr></table></figure><p>结果我就不打印了，节省篇幅。</p><p>那我们到这一步之后，咱们就已经获得了偏导，现在要考虑的问题就是去更新它，去优化它的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">learning_rate = <span class="hljs-number">1e-5</span><br><br><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.value = node.value + -<span class="hljs-number">1</span> * node.gradients[node] * learning_rate<br></code></pre></td></tr></table></figure><p>node 的值去更新，就应该等于它本身的值加上一个 -1乘以它的偏导在乘以一个<code>learning_rate</code>,我们对这个是不是已经很熟悉了？我们从第 8节线性回归的时候就一直在接触这个公式。</p><p>只不过在这个地方，x, y的值也要更新吗？它们的值是不应该去更新的，那要更新的应该是 k, b的值。</p><p>那么在这个地方该怎么办呢？其实很简单，我们添加一个判断就可以了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    <span class="hljs-keyword">if</span> node.is_trainable:<br>        node.value = node.value + -<span class="hljs-number">1</span> * node.gradients[node] * learning_rate<br></code></pre></td></tr></table></figure><p>然后我们给之前定义的类上加一个变量用于判断。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">..., is_trainable=<span class="hljs-literal">False</span></span>):<br>        ...<br>        self.is_trainable = is_trainable<br><br></code></pre></td></tr></table></figure><p>在这里我们默认是不可以训练的，只有少数的一些是需要训练的。</p><p>然后我们在初始化的部分把这个定义的值加上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">node_k = Placeholder(name=<span class="hljs-string">&#x27;k&#x27;</span>, is_trainable=<span class="hljs-literal">True</span>)<br>node_b = Placeholder(name=<span class="hljs-string">&#x27;b&#x27;</span>, is_trainable=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>对了，我们还需要将 Placeholder 做些改变：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Placeholder</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">..., is_trainable=<span class="hljs-literal">False</span></span>):<br>        Node.__init__(.., is_trainable=is_trainable)<br>        ...<br>    ...<br></code></pre></td></tr></table></figure><p>这就意味着，运行 for 循环的时候只有 k 和 b的值会更新，我们再加几句话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    <span class="hljs-keyword">if</span> node.is_trainable:<br>        ...<br>        cmp = <span class="hljs-string">&#x27;large&#x27;</span> <span class="hljs-keyword">if</span> node.gradients[node] &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;small&#x27;</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#123;&#125;的值&#123;&#125;，需要更新。&#x27;</span>.<span class="hljs-built_in">format</span>(node.name, cmp))<br><br>---<br>k的值small，需要更新。<br>b的值small，需要更新。<br></code></pre></td></tr></table></figure><p>我们现在将 forward, backward 和 optimize的三个循环封装乘三个方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">graph_sorted_nodes</span>):<br>    <span class="hljs-comment"># Forward</span><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>        node.forward()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">graph_sorted_nodes</span>):<br>    <span class="hljs-comment"># Backward</span><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.name))<br>        node.backward()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">optimize</span>(<span class="hljs-params">graph_sorted_nodes, learning_rate=<span class="hljs-number">1e-3</span></span>):<br>    <span class="hljs-comment"># optimize</span><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>        <span class="hljs-keyword">if</span> node.is_trainable:<br>            node.value = node.value + -<span class="hljs-number">1</span> * node.gradients[node] * learning_rate<br>            cmp = <span class="hljs-string">&#x27;large&#x27;</span> <span class="hljs-keyword">if</span> node.gradients[node] &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;small&#x27;</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#123;&#125;的值&#123;&#125;，需要更新。&#x27;</span>.<span class="hljs-built_in">format</span>(node.name, cmp))<br><br></code></pre></td></tr></table></figure><p>然后我们再来定义一个 epoch 方法，将 forward 和 backward放进去一起执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">run_one_epoch</span>(<span class="hljs-params">graph_sorted_nodes</span>):<br>    forward(graph_sorted_nodes)<br>    backward(graph_sorted_nodes)<br></code></pre></td></tr></table></figure><p>这样，我们完成一次完整的求值 - 求导 - 更新，就可以写成这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">run_one_epoch(sorted_nodes)<br>optimize(sorted_nodes)<br></code></pre></td></tr></table></figure><p>为了更好的观察，我们将所有的 print 都删掉，然后在 backward方法中写一个观察 loss 的打印函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">graph_sorted_nodes</span>):<br>    <span class="hljs-comment"># Backward</span><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(node, Loss):<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;loss value: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.value))<br>        node.backward()<br></code></pre></td></tr></table></figure><p>然后我们来对刚才完整的过程做个循环：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 完整的一次求值 - 求导 - 更新：</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    run_one_epoch(sorted_nodes)<br>    optimize(sorted_nodes, learning_rate=<span class="hljs-number">1e-1</span>)<br><br>---<br>loss value: <span class="hljs-number">0.12023025149136042</span><br>loss value: <span class="hljs-number">0.11090709486917472</span><br>loss value: <span class="hljs-number">0.10118818479676453</span><br>loss value: <span class="hljs-number">0.09120180962480523</span><br>loss value: <span class="hljs-number">0.08111466190584131</span><br>loss value: <span class="hljs-number">0.0711246044819575</span><br>loss value: <span class="hljs-number">0.061446239826641165</span><br>loss value: <span class="hljs-number">0.05229053883349982</span><br>loss value: <span class="hljs-number">0.043842158831920566</span><br>loss value: <span class="hljs-number">0.036239620745126</span><br></code></pre></td></tr></table></figure><p>可以看到 loss 在一点点的下降。当然，这样循环 10次我们还能观察出来，但是我们如果要成百上千次的去计算它，这样可就不行了，那我们需要将history 存下来，然后用图来显示出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_history = []<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    ...<br>    _loss_node = sorted_nodes[-<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">isinstance</span>(_loss_node, Loss)<br>    loss_history.append(_loss_node.value)<br>    optimize(sorted_nodes, learning_rate=<span class="hljs-number">1e-1</span>)<br><br>plt.plot(loss_history)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509027.png"alt="Alt text" /></p><p>我们现在可以验证一下，我们拟合的 yhat 和真实的 y之间差距有多大，首先我们当然是要获取到每个值的下标，然后用 sigmoid函数来算一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">sorted_nodes<br><br>---<br>[k, y, x, b, Linear, Sigmoid, Loss]<br></code></pre></td></tr></table></figure><p>通过下标来进行计算，k 是 0，x 是 2，b 是 3，y 是 1：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br><br><span class="hljs-comment"># k*x+b</span><br>sigmoid_x = sorted_nodes[<span class="hljs-number">0</span>].value * sorted_nodes[<span class="hljs-number">2</span>].value + sorted_nodes[<span class="hljs-number">3</span>].value<br><span class="hljs-built_in">print</span>(sigmoid(sigmoid_x))<br><br><span class="hljs-comment"># y</span><br><span class="hljs-built_in">print</span>(sorted_nodes[<span class="hljs-number">1</span>].value)<br><br>---<br><span class="hljs-number">0.891165479601981</span><br><span class="hljs-number">0.8988713384533658</span><br></code></pre></td></tr></table></figure><p>可以看到，非常的接近。那说明我们拟合的情况还是不错的。</p><p>好，这里总结一下，就是我们有了拓朴排序，就能向前去计算它的值，通过向前计算的值就可以向后计算它的值。那现在其实我们已经完成了一个mini的深度学习框架的核心内容，咱们能够定义节点，能够前向传播运算，能够反向传播运算，能更新梯度了。</p><p>那接下来是不是就结束了呢？很遗憾，并没有，接着咱们还要考虑如何处理多维数据。咱们现在看到的数据都是x、k、b 的输入，也就是都是一维的。</p><p>然而咱们真实世界中大多数场景下其实都是多维度的，其实都是多维数组。那么多维数组的还需要更新些什么，和现在有什么区别呢？</p><p>我们来接着往后看，因为基本上写法和现在这些几乎完全一样，那我也就不这么细致的讲了。</p><p>为了和之前代码做一个区分，所以我将多维向量计算的代码从新开了个文件，放在了<code>23.ipynb</code>里，小伙伴可以去下载到本地研习。</p><p>那么多维和现在最大的区别在哪里呢？就在于计算的时候，我们就要用到矩阵运算了。只是值变成了矩阵，运算变成的了矩阵运算。好，我们从Node开始来改动它，没什么变化的地方我就直接用<code>...</code>来省略了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>=[]</span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">raise</span> <span class="hljs-literal">NotImplemented</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">raise</span> <span class="hljs-literal">NotImplemented</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Placeholder</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        Node.__init__(self)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, value=<span class="hljs-literal">None</span></span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients = &#123;self:<span class="hljs-number">0</span>&#125;<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.outputs:<br>            grad_cost = n.gradients[self]<br>            self.gradients[self] = grad_cost * <span class="hljs-number">1</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, k, b</span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients = &#123;n: np.zeros_like(n.value) <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.inputs&#125;<br><br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.outputs:<br>            grad_cost = n.gradients[self]<br><br>            self.gradients[self.inputs[<span class="hljs-number">0</span>]] = np.dot(grad_cost, self.inputs[<span class="hljs-number">1</span>].value.T)<br>            self.gradients[self.inputs[<span class="hljs-number">1</span>]] = np.dot(self.inputs[<span class="hljs-number">0</span>].value.T, grad_cost)<br>            self.gradients[self.inputs[<span class="hljs-number">2</span>]] = np.<span class="hljs-built_in">sum</span>(grad_cost, axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-literal">False</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sigmoid</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, node</span>):<br>        Node.__init__(self, [node])<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_sigmoid</span>(<span class="hljs-params">self, x</span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.partial = self._sigmoid(self.x) * (<span class="hljs-number">1</span> - self._sigmoid(self.x))<br>        self.gradients = &#123;n: np.zeros_like(n.value) <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.inputs&#125;<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.outputs:<br>            grad_cost = n.gradients[self]  <br>            self.gradients[self.inputs[<span class="hljs-number">0</span>]] = grad_cost * self.partial<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MSE</span>(<span class="hljs-title class_ inherited__">Node</span>): <span class="hljs-comment"># 也就是之前的 Loss 类</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, y, a</span>):<br>        Node.__init__(self, [y, a])<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        y = self.inputs[<span class="hljs-number">0</span>].value.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        a = self.inputs[<span class="hljs-number">1</span>].value.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">assert</span>(y.shape == a.shape)<br>        self.m = self.inputs[<span class="hljs-number">0</span>].value.shape[<span class="hljs-number">0</span>]<br>        self.diff = y - a<br>        self.value = np.mean(self.diff**<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = (<span class="hljs-number">2</span> / self.m) * self.diff<br>        self.gradients[self.inputs[<span class="hljs-number">1</span>]] = (-<span class="hljs-number">2</span> / self.m) * self.diff<br></code></pre></td></tr></table></figure><p>类完成之后，我们还有一些其他的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_and_backward</span>(<span class="hljs-params">graph</span>): <span class="hljs-comment"># run_one_epoch</span><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> graph:<br>        n.forward()<br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span>  graph[::-<span class="hljs-number">1</span>]:<br>        n.backward()<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">toplogic</span>(<span class="hljs-params">graph</span>):<br>    ...<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_feed_dict_to_graph</span>(<span class="hljs-params">feed_dict</span>):<br>    ...<br><span class="hljs-comment"># 将 sorted_nodes 赋值从新定义了一个方法</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">topological_sort_feed_dict</span>(<span class="hljs-params">feed_dict</span>):<br>    graph = convert_feed_dict_to_graph(feed_dict)<br>    <span class="hljs-keyword">return</span> toplogic(graph)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">optimize</span>(<span class="hljs-params">trainables, learning_rate=<span class="hljs-number">1e-2</span></span>):<br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> trainables:<br>        node.value += -<span class="hljs-number">1</span> * learning_rate * node.gradients[node]<br></code></pre></td></tr></table></figure><p>这样就完成了。可以发现基本上代码没有什么变动，变化比较大的都是各个类中的backward 方法，因为要将其变成使用矩阵运算。</p><p>我们来尝试着用一下这个多维算法，我们还是用波士顿房价的那个数据来做一下尝试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python">X_ = data[<span class="hljs-string">&#x27;data&#x27;</span>]<br>y_ = data[<span class="hljs-string">&#x27;target&#x27;</span>]<br><br><span class="hljs-comment"># Normalize data</span><br>X_ = (X_ - np.mean(X_, axis=<span class="hljs-number">0</span>)) / np.std(X_, axis=<span class="hljs-number">0</span>)<br><br>n_features = X_.shape[<span class="hljs-number">1</span>]<br>n_hidden = <span class="hljs-number">10</span><br>W1_ = np.random.randn(n_features, n_hidden)<br>b1_ = np.zeros(n_hidden)<br>W2_ = np.random.randn(n_hidden, <span class="hljs-number">1</span>)<br>b2_ = np.zeros(<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># Neural network</span><br>X, y = Placeholder(), Placeholder()<br>W1, b1 = Placeholder(), Placeholder()<br>W2, b2 = Placeholder(), Placeholder()<br><br>l1 = Linear(X, W1, b1)<br>s1 = Sigmoid(l1)<br>l2 = Linear(s1, W2, b2)<br>cost = MSE(y, l2)<br><br>feed_dict = &#123;<br>    X: X_,<br>    y: y_,<br>    W1: W1_,<br>    b1: b1_,<br>    W2: W2_,<br>    b2: b2_<br>&#125;<br><br>epochs = <span class="hljs-number">5000</span><br><span class="hljs-comment"># Total number of examples</span><br>m = X_.shape[<span class="hljs-number">0</span>]<br>batch_size = <span class="hljs-number">16</span><br>steps_per_epoch = m // batch_size<br><br>graph = topological_sort_feed_dict(feed_dict)<br>trainables = [W1, b1, W2, b2]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Total number of examples = &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(m))<br></code></pre></td></tr></table></figure><p>我们在中间定义了 l1, s1, l2, cost,分别来实例化四个类。然后我们就需要根据数据来进行迭代计算了，定义一个losses 来保存历史数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">losses = []<br><br>epochs = <span class="hljs-number">100</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    loss = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(steps_per_epoch):<br>        <span class="hljs-comment"># Step 1</span><br>        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)<br><br>        X.value = X_batch<br>        y.value = y_batch<br><br>        <span class="hljs-comment"># Step 2</span><br>        forward_and_backward(graph) <span class="hljs-comment"># set output node not important.</span><br><br>        <span class="hljs-comment"># Step 3</span><br>        rate = <span class="hljs-number">1e-2</span><br>    <br>        optimize(trainables, rate)<br><br>        loss += graph[-<span class="hljs-number">1</span>].value<br>    <br>    <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>: <br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Epoch: &#123;&#125;, Loss: &#123;:.3f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(i+<span class="hljs-number">1</span>, loss/steps_per_epoch))<br>        losses.append(loss/steps_per_epoch)<br><br>---<br>Epoch: <span class="hljs-number">1</span>, Loss: <span class="hljs-number">194.170</span><br>...<br>Epoch: <span class="hljs-number">4901</span>, Loss: <span class="hljs-number">3.137</span><br></code></pre></td></tr></table></figure><p>可以看到它 loss下降的非常快，还记得咱们刚开始的时候在训练波士顿房价数据的时候，那个loss 下降到多少？最低是不是就下降到在第一节课的时候我们的 lose最多下降到了多少 47.34 对吧？那现在呢？直接下降到了3，这是为什么？因为我们的维度多了，维度多了它就准确了。这说明什么？说明大家去谈恋爱的时候，不要盯着对象的一个方面，多方面考察，才能知道这个人是否合适。</p><p>好，现在看起来效果是很好，但是我们想知道到底拟合出来的什么函数，那怎么办？咱们把这个维度降低成三维空间就可以看了。</p><p>现在咱们这个波士顿的所有数据实际上是一个 15 维的数据，15维的数据你根本看不了，咱们现在只要把 x这个里边取一点值，在这个里边稍微把值给它变一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X_ = dataframe[[<span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>]]<br>y_ = data[<span class="hljs-string">&#x27;target&#x27;</span>]<br></code></pre></td></tr></table></figure><p>在咱们之前的课程中对其进行计算的时候就分析过，RM 和 LSTAT是影响最大的两个特征，我们还是来用这个。然后我们将刚才的代码从新运行一遍：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">losses = []<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm_notebook(<span class="hljs-built_in">range</span>(epochs)):<br>    ...<br><br>---<br>Epoch: <span class="hljs-number">1</span>, Loss: <span class="hljs-number">150.122</span><br>...<br>Epoch: <span class="hljs-number">4901</span>, Loss: <span class="hljs-number">16.181</span><br></code></pre></td></tr></table></figure><p>这次下降的就没上次好了。</p><p>现在我们可视化一下这个三维空间来看看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mpl_toolkits.mplot3d <span class="hljs-keyword">import</span> Axes3D<br><br>predicate_results = []<br><span class="hljs-keyword">for</span> rm, ls <span class="hljs-keyword">in</span> X_.values:<br>    X.value = np.array([[rm, ls]])<br>    forward_and_backward(graph)<br>    predicate_results.append(graph[-<span class="hljs-number">2</span>].value[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br><br>%matplotlib widget<br><br>fig = plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>))<br>ax = fig.add_subplot(<span class="hljs-number">111</span>, projection=<span class="hljs-string">&#x27;3d&#x27;</span>)<br><br>X_ = dataframe[[<span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>]].values[:, <span class="hljs-number">0</span>]<br>Y_ = dataframe[[<span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>]].values[:, <span class="hljs-number">1</span>]<br><br>Z = predicate_results<br><br>rm_and_lstp_price = ax.plot_trisurf(X_, Y_, Z, color=<span class="hljs-string">&#x27;green&#x27;</span>)<br><br>ax.set_xlabel(<span class="hljs-string">&#x27;RM&#x27;</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;% of lower state&#x27;</span>)<br>ax.set_zlabel(<span class="hljs-string">&#x27;Predicated-Price&#x27;</span>)<br></code></pre></td></tr></table></figure><p>然后我们就能看到一个数据的三维图形，因为我们开启了widget，所以可以进行拖动。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509029.png"alt="Alt text" /></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509030.png"alt="Alt text" /></p><p>从图形上看，确实符合房间越多，低收入人群越少，房价越高的特性。</p><p>那现在计算机确实帮我们自动的去找到了一个函数，这个函数到底怎么设置咱们都不用关心，它自动就给你求解出来，这个就是深度学习的意义。咱们经过这一系列写出来的东西其实就已经能够做到。</p><p>我觉得这个真的有一种数学之美，它从最简单的东西出发，最后做成了这样一个复杂的东西。确实很深其，并且还都在我们的掌握之中。</p><p>好，大家下来以后记得要多多自己敲代码，多分析其中的一些过程和原理。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509025.png&quot;
alt=&quot;茶桁的 AI 秘籍 核心能力 23&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>22. 深度学习 - 自动求导</title>
    <link href="https://hivan.me/22.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <id>https://hivan.me/22.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%9E%E7%8E%B0/</id>
    <published>2023-11-18T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:21.030Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311012100649.png"alt="Alt text" /></p><span id="more"></span><p>Hi，你好。我是茶桁。</p><p>咱们接着上节课内容继续讲，我们上节课已经了解了拓朴排序的原理，并且简单的模拟实现了。我们这节课就来开始将其中的内容变成具体的计算过程。</p><p><code>linear, sigmoid</code>和<code>loss</code>这三个函数的值具体该如何计算呢？</p><p>我们现在似乎大脑已经有了一个起比较模糊的印象，可以通过它的输入来计算它的点。</p><p>让我们先把最初的父类 Node 改造一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs=[], name=<span class="hljs-literal">None</span></span>):<br>        ...<br>        self.value = <span class="hljs-literal">None</span><br>    <br>    ...<br></code></pre></td></tr></table></figure><p>然后再复制出一个，和<code>Placeholder</code>一样，我们需要继承Node，并且改写这个方法自己独有的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, k, b, name=<span class="hljs-literal">None</span></span>):<br>        Node.__init__(self, inputs=[x, k, b], name=name)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        x, k, b = self.inputs[<span class="hljs-number">0</span>], self.inputs[<span class="hljs-number">1</span>], self.inputs[<span class="hljs-number">2</span>]<br>        self.value = k.value * x.value + b.value<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;我是&#123;&#125;, 我没有人类爸爸，需要自己计算结果&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.value))<br>    ...<br></code></pre></td></tr></table></figure><p>我们新定义的这个类叫<code>Linear</code>, 它会接收 x, k, b。它继承了Node。这个里面的 forward该如何计算呢？我们需要每一个节点都需要一个值，一个变量，因为我们初始化的时候接收的x,k,b 都赋值到了 inputs里，这里我们将其取出来就行了，然后就是线性方程的公式<code>k*x+b</code>，赋值到它自己的value 上。</p><p>然后接着呢，就轮到 Sigmoid 了，一样的，我们定义一个子类来继承Node:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sigmoid</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, name=<span class="hljs-literal">None</span></span>):<br>        Node.__init__(self, inputs=[x], name=name)<br>        self.x = self.inputs[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_sigmoid</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        self.value = self._sigmoid(self.x.value)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;我是&#123;&#125;, 我自己计算了结果&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.value))<br>    ...<br></code></pre></td></tr></table></figure><p>Sigmoid 函数只接收一个参数，就是 x，其公式为1/(1+e^{-x})，我们在这里定义一个新的方法来计算，然后在 forward里把传入的 x取出来，再将其送到这个方法里进行计算，最后将结果返回给它自己的value。</p><p>那下面自然是 Loss 函数了，方式也是一模一样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Loss</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, y, yhat, name=<span class="hljs-literal">None</span></span>):<br>        Node.__init__(self, inputs = [y, yhat], name=name)<br>        self.y = self.inputs[<span class="hljs-number">0</span>]<br>        self.yhat = self.inputs[<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        y_v = np.array(self.y.value)<br>        yhat_v = np.array(self.y_hat.value)<br>        self.value = np.mean((y.value - yhat.value) ** <span class="hljs-number">2</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;我是&#123;&#125;, 我自己计算了结果&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.value))<br><br>    ...<br></code></pre></td></tr></table></figure><p>那我们这里定义成 Loss其实并不确切，因为我们虽然喊它是损失函数，但是其实损失函数的种类也非常多。而这里，我们用的MSE。所以我们应该定义为<code>MSE</code>，不过为了避免歧义，这里还是沿用Loss 好了。</p><p>定义完类之后，我们参数调用的类名也就需要改一下了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">...<br>node_linear = Linear(x=node_x, k=node_k, b=node_b, name=<span class="hljs-string">&#x27;linear&#x27;</span>)<br>node_sigmoid = Sigmoid(x=node_linear, name=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>node_loss = Loss(y=node_y, yhat=node_sigmoid, name=<span class="hljs-string">&#x27;loss&#x27;</span>)<br></code></pre></td></tr></table></figure><p>好，这个时候我们基本完成了，计算之前让我们先看一下<code>sorted_node</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">sorted_node<br><br>---<br>[Placeholder: y,<br> Placeholder: k,<br> Placeholder: x,<br> Placeholder: b,<br> Linear: Linear,<br> Sigmoid: Sigmoid,<br> MSE: Loss]<br></code></pre></td></tr></table></figure><p>没有问题，我们现在可以模拟神经网络的计算过程了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.forward()<br><br>---<br>我是x, 我已经被人类爸爸赋值为<span class="hljs-number">3</span><br>我是b, 我已经被人类爸爸赋值为<span class="hljs-number">0.3737660632429008</span><br>我是k, 我已经被人类爸爸赋值为<span class="hljs-number">0.35915077292816744</span><br>我是y, 我已经被人类爸爸赋值为<span class="hljs-number">0.6087876106387002</span><br>我是Linear, 我没有人类爸爸，需要自己计算结果<span class="hljs-number">1.4512183820274032</span><br>我是Sigmoid, 我没有人类爸爸，需要自己计算结果<span class="hljs-number">0.8101858733432837</span><br>我是Loss, 我没有人类爸爸，需要自己计算结果<span class="hljs-number">0.04056126022042443</span><br></code></pre></td></tr></table></figure><p>咱们这个整个过程就像是数学老师推公式一样，因为这个比较复杂。你不了解这个过程就求解不出来。</p><p>这就是为什么我一直坚持要手写代码的原因。<code>c+v</code>大法确实好，但是肯定是学的不够深刻。表面的东西懂了，但是更具体的为什么不清楚。</p><p>我们可以看到，我们现在已经将 Linear、Sigmoid 和 Loss都将值计算出来了。那我们现在已经实现了从 x 到 loss 的前向传播</p><p>现在我们有了loss，那就又要回到我们之前机器学习要做的事情了，就是将损失函数 loss的值降低。</p><p>之前咱们讲过，要将 loss的值减小，那我们就需要求它的偏导，我们前面课程的求导公式这个时候就需要拿过来了。</p><p>然后我们需要做的事情并不是完成求导就好了，而是要实现「链式求导」。</p><p>那从 Loss 开始反向传播的时候该做些什么？先让我们把“口号”喊出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">...</span>):<br>        ...<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.inputs:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;获取∂&#123;&#125; / ∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, n.name))<br></code></pre></td></tr></table></figure><p>这样修改一下Node，然后在其中假如一个反向传播的方法，将口号喊出来。</p><p>然后我们来看一下口号喊的如何，用<code>[::-1]</code>来实现反向获取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    node.backward()<br><br>---<br>获取∂Loss / ∂y<br>获取∂Loss / ∂Sigmoid<br>获取∂Sigmoid / ∂Linear<br>获取∂Linear / ∂x<br>获取∂Linear / ∂k<br>获取∂Linear / ∂b<br></code></pre></td></tr></table></figure><p>这样看着似乎不是太直观，我们再将 node的名称加上去来看就明白很多：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    <span class="hljs-built_in">print</span>(node.name)<br>    node.backward()<br>---<br>Loss<br>获取∂Loss / ∂y<br>获取∂Loss / ∂Sigmoid<br>Sigmoid<br>获取∂Sigmoid / ∂Linear<br>Linear<br>获取∂Linear / ∂x<br>获取∂Linear / ∂k<br>获取∂Linear / ∂b<br>...<br></code></pre></td></tr></table></figure><p>最后的<code>k, y, x, b</code>我就用...代替了，主要是函数。</p><p>那我们就清楚的看到，Loss 获取了两个偏导，然后传到了 Sigmoid，Sigmoid获取到一个，再传到Linear，获取了三个。那现在其实我们只要把这些值能乘起来就可以了。我们要计算步骤都有了，只需要把它乘起来就行了。</p><p>我们先是需要一个变量，用于存储 Loss 对某个值的偏导</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">...</span>):<br>        ...<br>        self.gradients = <span class="hljs-built_in">dict</span>()<br>    ...<br></code></pre></td></tr></table></figure><p>然后我们倒着来看，先来看 Loss:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Loss</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">0</span>].name)<br>        self.gradients[self.inputs[<span class="hljs-number">1</span>]] = <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">1</span>].name)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[0]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">0</span>]]))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[1]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">1</span>]]))<br></code></pre></td></tr></table></figure><p>眼尖的小伙伴应该看出来了，我现在依然还是现在里面进行「喊口号」的动作。主要是先来看一下过程。</p><p>刚才每个 node 都有一个 gradients，它代表的是对某个节点的偏导。</p><p>现在这个节点 self 就是 loss，然后我们<code>self.inputs[0]</code>就是y, <code>self.inputs[1]</code>就是 yhat,也就是<code>node_sigmoid</code>。那么我们现在这个<code>self.gradients[self.inputs[n]]</code>其实就分别是<code>∂loss/∂y</code>和<code>∂loss/∂yhat</code>，我们把对的值分别赋值给它们。</p><p>然后我们再来看 Sigmoid：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sigmoid</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">0</span>].name)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[0]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">0</span>]]))<br></code></pre></td></tr></table></figure><p>我们依次来看哈，这个时候的 self 就是 Sigmoid了，这个时候的<code>sigmoid.inputs[0]</code>应该是 Linear对吧，然后我们整个<code>self.gradients[self.inputs[0]]</code>自然就应该是<code>∂sigmoid/∂linear</code>。</p><p>我们继续，这个时候<code>self.outputs[0]</code>就是 loss,<code>loss.gradients[self]</code>那自然就应该是输出过来的<code>∂loss/∂sigmoid</code>，然后呢，我们需要将这两个部分乘起来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>    self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-string">&#x27;*&#x27;</span>.join([self.outputs[<span class="hljs-number">0</span>].gradients[self], <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">0</span>].name)])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[0]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">0</span>]]))<br></code></pre></td></tr></table></figure><p>接着，我们就需要来看看 Linear 了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>    self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-string">&#x27;*&#x27;</span>.join([self.outputs[<span class="hljs-number">0</span>].gradients[self], <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">0</span>].name)])<br>    self.gradients[self.inputs[<span class="hljs-number">1</span>]] = <span class="hljs-string">&#x27;*&#x27;</span>.join([self.outputs[<span class="hljs-number">0</span>].gradients[self], <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">1</span>].name)])<br>    self.gradients[self.inputs[<span class="hljs-number">2</span>]] = <span class="hljs-string">&#x27;*&#x27;</span>.join([self.outputs[<span class="hljs-number">0</span>].gradients[self], <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">2</span>].name)])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[0]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">0</span>]]))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[1]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">1</span>]]))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[2]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">2</span>]]))<br></code></pre></td></tr></table></figure><p>和上面的分析一样，我们先来看三个<code>inputs[n]</code>的部分，self在这里是 linear了，这里的<code>self.inputs[n]</code>分别应该是<code>x, k, b</code>对吧，那么它们就应该分别是<code>linear.gradients[x]</code>,<code>linear.gradients[k]</code>和<code>linear.gradients[b]</code>，也就是<code>∂linear/∂x</code>,<code>∂linear/∂k</code>,<code>∂linear/∂b</code>。</p><p>那反过来，<code>outputs</code>就应该反向来找，那么<code>self.outputs[0]</code>这会儿就应该是sigmoid。<code>sigmoid.gradients[self]</code>就是前一个输出过来的<code>∂loss/∂sigmoid * ∂sigmoid/∂linear</code>,那后面以此的[1]和[2]我们也就应该明白了。</p><p>然后后面分别是<code>∂linear/∂x</code>,<code>∂linear/∂k</code>,<code>∂linear/∂b</code>。一样，我们将它们用乘号连接起来。</p><p>公式就应该是：</p><p><span class="math display">\[\begin{align*}\frac{\partial loss}{\partial sigmoid} \cdot \frac{\partialsigmoid}{\partial linear} \cdot \frac{\partial linear}{\partial x} \\\frac{\partial loss}{\partial sigmoid} \cdot \frac{\partialsigmoid}{\partial linear} \cdot \frac{\partial linear}{\partial k} \\\frac{\partial loss}{\partial sigmoid} \cdot \frac{\partialsigmoid}{\partial linear} \cdot \frac{\partial linear}{\partial b} \\\end{align*}\]</span></p><p>那同理，我们还需要写一下<code>Placeholder</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Placeholder</span>(<span class="hljs-params">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;我获取了我自己的 gradients: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.outputs[<span class="hljs-number">0</span>].gradients[self]))<br>    ...<br></code></pre></td></tr></table></figure><p>好，我们来看下我们模拟的情况如何，看看它们是否都如期喊口号了，结合我们之前的前向传播的结果，我们一起来看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.forward()<br>    <br><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.name))<br>    node.backward()<br><br>---<br>Loss<br>[<span class="hljs-number">0</span>]: ∂Loss/∂y<br>[<span class="hljs-number">1</span>]: ∂Loss/∂Sigmoid<br><br>Sigmoid<br>[<span class="hljs-number">0</span>]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear<br><br>Linear<br>[<span class="hljs-number">0</span>]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂x<br>[<span class="hljs-number">1</span>]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂k<br>[<span class="hljs-number">2</span>]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂b<br><br>k<br>我获取了我自己的gradients: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂k<br><br>b<br>我获取了我自己的gradients: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂b<br><br>x<br>我获取了我自己的gradients: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂x<br><br>y<br>我获取了我自己的gradients: ∂Loss/∂y<br></code></pre></td></tr></table></figure><p>好，观察下来没问题，那我们现在还剩下最后一步。就是将这些口号替换成真正的计算的值，其实很简单，就是将我们之前学习过并写过的函数替换进去就可以了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        x, k, b = self.inputs[<span class="hljs-number">0</span>], self.inputs[<span class="hljs-number">1</span>], self.inputs[<span class="hljs-number">2</span>]<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = self.outputs[<span class="hljs-number">0</span>].gradients[self] * k.value<br>        self.gradients[self.inputs[<span class="hljs-number">1</span>]] = self.outputs[<span class="hljs-number">0</span>].gradients[self] * x.value<br>        self.gradients[self.inputs[<span class="hljs-number">2</span>]] = self.outputs[<span class="hljs-number">0</span>].gradients[self] * <span class="hljs-number">1</span><br>        ...<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sigmoid</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.value = self._sigmoid(self.x.value)<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = self.outputs[<span class="hljs-number">0</span>].gradients[self] * self.value * (<span class="hljs-number">1</span> - self.value)<br>        ...<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Loss</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        y_v = self.y.value<br>        yhat_v = self.y_hat.value<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-number">2</span>*np.mean(y_v - yhat_v)<br>        self.gradients[self.inputs[<span class="hljs-number">1</span>]] = -<span class="hljs-number">2</span>*np.mean(y_v - yhat_v)<br></code></pre></td></tr></table></figure><p>那我们来看下真正计算的结果是怎样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.name))<br>    node.backward()<br><br>---<br>Loss<br>∂Loss/∂y: -<span class="hljs-number">0.402796525409167</span><br>∂Loss/∂Sigmoid: <span class="hljs-number">0.402796525409167</span><br><br>Sigmoid<br>∂Sigmoid/∂Linear: <span class="hljs-number">0.06194395247945269</span><br><br>Linear<br>∂Linear/∂x: <span class="hljs-number">0.02224721841122111</span><br>∂Linear/∂k: <span class="hljs-number">0.18583185743835806</span><br>∂Linear/∂b: <span class="hljs-number">0.06194395247945269</span><br><br>y<br>gradients: -<span class="hljs-number">0.402796525409167</span><br><br>k<br>gradients: <span class="hljs-number">0.18583185743835806</span><br><br>b<br>gradients: <span class="hljs-number">0.06194395247945269</span><br><br>x<br>gradients: <span class="hljs-number">0.02224721841122111</span><br></code></pre></td></tr></table></figure><p>好，到这里，我们就实现了前向传播和反向传播，让程序自动计算出了它们的偏导值。</p><p>不过我们整个动作还没有结束，就是我们需要将 loss降低到最小才可以。</p><p>那我们下节课，就来完成这一步。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311012100649.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>21. 深度学习 - 拓朴排序的原理和实现</title>
    <link href="https://hivan.me/21.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E6%8B%93%E6%9C%B4%E6%8E%92%E5%BA%8F%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/"/>
    <id>https://hivan.me/21.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E6%8B%93%E6%9C%B4%E6%8E%92%E5%BA%8F%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/</id>
    <published>2023-11-15T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:25.438Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310311950753.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><p>上节课，我们讲了多层神经网络的原理，并且明白了，数据量是层级无法超过3 层的主要原因。</p><span id="more"></span><p>然后我们用一张图来解释了整个链式求导的过程：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310311950755.png"alt="Alt text" /></p><p>那么，我们如何将这张图里的节点关系来获得它的求导过程呢？</p><p>假如我们现在定义一个函数<code>get_output</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_output</span>(<span class="hljs-params">graph, node</span>):<br>    outputs = []<br>    <span class="hljs-keyword">for</span> n, links <span class="hljs-keyword">in</span> graph.items():<br>        <span class="hljs-keyword">if</span> node == n: outputs += links<br>    <span class="hljs-keyword">return</span> outputs<br>get_output(computing_graph, <span class="hljs-string">&#x27;k1&#x27;</span>)<br><br>---<br>[<span class="hljs-string">&#x27;L1&#x27;</span>]<br></code></pre></td></tr></table></figure><p>我们可以根据 k1 获得 l1。</p><p>来，让我们整理一下思路，问：如何获得 k1 的偏导：</p><ol type="1"><li>获得 k1 的输出节点</li><li>获得 k1 输出节点的输出节点</li><li>...直到我们找到最后一个节点</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_order = []<br><br>target = <span class="hljs-string">&#x27;k1&#x27;</span><br>out = get_output(computing_graph, target)[<span class="hljs-number">0</span>]<br>computing_order.append(target)<br><br><span class="hljs-keyword">while</span> out:<br>    computing_order.append(out)<br>    out = get_output(computing_graph, out)<br>    <span class="hljs-keyword">if</span> out: out = out[<span class="hljs-number">0</span>]<br><br>computing_order<br><br>---<br>[<span class="hljs-string">&#x27;k1&#x27;</span>, <span class="hljs-string">&#x27;L1&#x27;</span>, <span class="hljs-string">&#x27;sigmoid&#x27;</span>, <span class="hljs-string">&#x27;L2&#x27;</span>, <span class="hljs-string">&#x27;loss&#x27;</span>]<br></code></pre></td></tr></table></figure><p>我们从 k1 出发，它可以获得这么一套顺序。那么现在如果要计算 k1的偏导，我们的这个偏导顺序就等于从后到前给它求解一遍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">order = <span class="hljs-string">&#x27;&#x27;</span><br><br><span class="hljs-keyword">for</span> i, n <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(computing_order[:-<span class="hljs-number">1</span>]):<br>    order += <span class="hljs-string">&#x27;*∂&#123;&#125; / ∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(n, computing_order[i+<span class="hljs-number">1</span>])<br><br>order<br>---<br><span class="hljs-string">&#x27;*∂k1 / ∂L1*∂L1 / ∂sigmoid*∂sigmoid / ∂L2*∂L2 / ∂loss&#x27;</span><br></code></pre></td></tr></table></figure><p>现在 k1的求导顺序计算机就给它自动求解出来了，我们把它放到了一个图里面，然后它自动就求解出来了。只不过唯一的问题是现在这个order 是反着的，需要把它再反过来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i, n <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(computing_order[:-<span class="hljs-number">1</span>]):<br>    order.append((computing_order[i + <span class="hljs-number">1</span>], n))<br>    <span class="hljs-comment"># order += &#x27; * ∂&#123;&#125; / ∂&#123;&#125;&#x27;.format(n, computing_order[i+1])</span><br><br><span class="hljs-string">&#x27; * &#x27;</span>.join([<span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(a, b) <span class="hljs-keyword">for</span> a, b <span class="hljs-keyword">in</span> order[::-<span class="hljs-number">1</span>]])<br><br>---<br><span class="hljs-string">&#x27;∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂k1&#x27;</span><br></code></pre></td></tr></table></figure><p>这个过程用计算机实现之后，我们就可以拿它来看一下其他的参数，比如说<code>b1</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_order = []<br><br>target = <span class="hljs-string">&#x27;b1&#x27;</span><br>out = get_output(computing_graph, target)[<span class="hljs-number">0</span>]<br>computing_order.append(target)<br><br><span class="hljs-keyword">while</span> out:<br>    computing_order.append(out)<br>    out = get_output(computing_graph, out)<br>    <span class="hljs-keyword">if</span> out: out = out[<span class="hljs-number">0</span>]<br><br>order = []<br><br><span class="hljs-keyword">for</span> i, n <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(computing_order[:-<span class="hljs-number">1</span>]):<br>    order.append((computing_order[i + <span class="hljs-number">1</span>], n))<br>    <span class="hljs-comment"># order += &#x27; * ∂&#123;&#125; / ∂&#123;&#125;&#x27;.format(n, computing_order[i+1])</span><br><br><span class="hljs-string">&#x27; * &#x27;</span>.join([<span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(a, b) <span class="hljs-keyword">for</span> a, b <span class="hljs-keyword">in</span> order[::-<span class="hljs-number">1</span>]])<br><br>---<br><span class="hljs-string">&#x27;∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂b1&#x27;</span><br></code></pre></td></tr></table></figure><p>k2:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">...<br>target = <span class="hljs-string">&#x27;k2&#x27;</span><br>...<br><br>---<br><span class="hljs-string">&#x27;∂loss/∂L2 * ∂L2/∂k2&#x27;</span><br></code></pre></td></tr></table></figure><p>到这里，我们能够自动的求解各个参数的导数了。</p><p>然后我们将其封装一下，然后循环一下每一个参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_paramter_partial_order</span>(<span class="hljs-params">p</span>):<br>    ...<br>    target = p<br>    ...<br>    <span class="hljs-keyword">return</span> ...<br><br><span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;b1&#x27;</span>, <span class="hljs-string">&#x27;k1&#x27;</span>, <span class="hljs-string">&#x27;b2&#x27;</span>, <span class="hljs-string">&#x27;k2&#x27;</span>]:<br>    <span class="hljs-built_in">print</span>(get_paramter_partial_order(p))<br><br>---<br>∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂b1<br>∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂k1<br>∂loss/∂L2 * ∂L2/∂b2<br>∂loss/∂L2 * ∂L2/∂k2<br></code></pre></td></tr></table></figure><p>到这一步你就能够发现，每一个参数的导数的偏导我们都可以求解了。而且我们还发现一个问题，不管是<code>['b1', 'k1', 'b2', 'k2']</code>中的哪一个，我们都需要求求解<code>∂loss/∂L2</code>。</p><p>所以现在如果有一个内存能够记录结果，先把<code>∂loss/∂L2</code>的值求解下来，把这个值先存下来，只要算出来这一个值之后，再算<code>['b1', 'k1', 'b2', 'k2']</code>的时候直接拿过来就行了。</p><p>也就是说我们首先需要记录的就是这个值，其次，如果我们把 L2 和 sigmoid的值记下来，求解 b1 和 k1的时候直接拿过来用就行，不需要再去计算一遍，这个时候我们的效率就会提升很多。</p><p>首先把共有的一个基础<code>∂loss/∂L2</code>计算了，第二步，有了<code>∂loss/∂L2</code>，把<code>∂L2/∂sigmoid</code>再记录一遍，第三个是<code>∂sigmoid/∂L1</code>,然后后面以此就是<code>∂L1/∂b1</code>,<code>∂L1/∂k1</code>，<code>∂L2/∂b2</code>, <code>∂L2/∂k2</code>。</p><p>现在的问题就是就是怎么样让计算机自动得到这个顺序，计算机得到这个顺序的时候，把这些值都存在某个地方。</p><p>这个所谓的顺序就是我们非常重要的一个概念，在计算机科学，算法里面非常重要的一个概念：「拓朴排序」。</p><p>那拓朴排序该如何实现呢？来，我们一起来实现一下：</p><p>首先，我们定义一个方法，咱们输入的是一个图，这个图的定义方式是一个Dictionary，然后里面有一些节点，里面的很多个连接的点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">topologic</span>(<span class="hljs-params">graph</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    graph: dict</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">        node: [node1, node2, ..., noden]</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><p>因为我们要把它的结果存在一个变量里边，当我们不断的检查看这个图，看看它是否为空，然后我们来定义两个存储变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">topologic</span>(<span class="hljs-params">graph</span>):<br>    sorted_node = []<br><br>    <span class="hljs-keyword">while</span> graph:<br>        all_inputs = []<br>        all_outputs = <span class="hljs-built_in">list</span>(graph.keys())<br><br>    <span class="hljs-keyword">return</span> sorted_node<br></code></pre></td></tr></table></figure><p>这里的两个变量，<code>all_inputs</code>和<code>all_outputs</code>,一个是用来存储所有的输入节点，一个是存储所有的输出节点。</p><p>我们还记得我们那个图的格式是什么样的吗？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_graph = &#123;<br>    <span class="hljs-string">&#x27;k1&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;b1&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;x&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;L1&#x27;</span>:[<span class="hljs-string">&#x27;sigmoid&#x27;</span>],<br>    <span class="hljs-string">&#x27;sigmoid&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;k2&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;b2&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;L2&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>],<br>    <span class="hljs-string">&#x27;y&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>]<br>&#125;<br></code></pre></td></tr></table></figure><p>我们看这个数据，那所有的输出节点是不是就是其中的<code>key</code>啊？</p><p>打比方说，我们拿一个短小的数据来做示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">simple_graph = &#123;<br>    <span class="hljs-string">&#x27;a&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],<br>    <span class="hljs-string">&#x27;b&#x27;</span>: [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]<br>&#125;<br><br><span class="hljs-built_in">list</span>(simple_graph.keys())<br><br>---<br>[<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>]<br></code></pre></td></tr></table></figure><p>那我们这样就拿到了输出节点，并将其放在了一个列表内。</p><p>这里说点其他的，Python 3.9及以上的版本其实都实现了自带拓朴排序，但是如果你的 Python版本较低，那还是需要自己去实现。这个也是 Python 3.9里面一个比较重要的更新。</p><p>那为什么我们的 value 定义的是一个列表呢？这是因为这个key，也就是输出值可能会输出到好几个函数里面，因为我们现在拿的是一个比较简单的模型，但是在真实场景中，有可能会输出到更多的节点中。</p><p>这里，就获得了所有有输入的节点， <code>simple_graph</code>中，a输出给了[1,2], b 输出给了[2,3]。</p><p>那我们怎么获得所有输入的节点呢？那就应该是 value。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">list</span>(simple_graph.values())<br><br>---<br>[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br></code></pre></td></tr></table></figure><p>这样就获得所有有输入的节点。然后就是怎么样把这两个 list合并。可以有一个简单的方法，一个叫做 reduce 的方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">reduce(<span class="hljs-keyword">lambda</span> a, b: a+b, <span class="hljs-built_in">list</span>(simple_graph.values()))<br><br>---<br>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]<br></code></pre></td></tr></table></figure><p>这样就把它给它连起来了。</p><p>那我们还需要找一个，就是只有输出没有输入的节点，这些该怎么去找呢？其实也就是我们的<code>[k1, b1, k2, b2, y]</code>这些值。</p><p>来，我们还是拿刚才的<code>simple_graph</code>来举例，但是这次我们改一下里面的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">simple_graph = &#123;<br>    <span class="hljs-string">&#x27;a&#x27;</span>: [<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-number">2</span>],<br>    <span class="hljs-string">&#x27;b&#x27;</span>: [<span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-number">4</span>]<br>&#125;<br><br>a = <span class="hljs-built_in">list</span>(simple_graph.keys())<br>b = reduce(<span class="hljs-keyword">lambda</span> a, b: a+b, <span class="hljs-built_in">list</span>(simple_graph.values()))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(b) - <span class="hljs-built_in">set</span>(a)))<br><br>---<br>[<span class="hljs-number">2</span>, <span class="hljs-number">4</span>]<br></code></pre></td></tr></table></figure><p>我们没有用循环，而是将其变成了一个集合，然后利用集合的加减来做。</p><p>我们的实际代码就可以这样写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">topologic</span>(<span class="hljs-params">graph</span>):<br>    sorted_node = []<br><br>    <span class="hljs-keyword">while</span> graph:<br>        all_inputs = reduce(<span class="hljs-keyword">lambda</span> a, b: a+b, <span class="hljs-built_in">list</span>(graph.values()))<br>        all_outputs = <span class="hljs-built_in">list</span>(graph.keys())<br><br>        all_inputs = <span class="hljs-built_in">set</span>(all_inputs)<br>        all_outputs = <span class="hljs-built_in">set</span>(all_outputs)<br><br>        need_remove = all_outputs - all_inputs<br><br>    <span class="hljs-keyword">return</span> sorted_node<br></code></pre></td></tr></table></figure><p>那现在我们继续往后，如果找到了这些只有输出没有输入的节点之后，我们做一个判断，然后定义一个节点，用来保存随机选择的节点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(need_remove) &gt; <span class="hljs-number">0</span>:<br>    node = random.choice(<span class="hljs-built_in">list</span>(need_remove))<br></code></pre></td></tr></table></figure><p>这个时候 x, b, k, y都有可能，那么我们随机找一个就行。然后将这个找到的节点从 graph给它删除。并且将其插入到<code>sorted_node</code>中去，并且返回出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">    <span class="hljs-keyword">if</span> ...:<br>        node = random.choice(<span class="hljs-built_in">list</span>(need_remove))<br>        graph.pop(node)<br>        sorted_node.append(node)<br><br><span class="hljs-keyword">return</span> sorted_node<br></code></pre></td></tr></table></figure><p>然后这里还会出一个小问题，我们还是拿一个示例来说：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">simple_graph = &#123;<br>    <span class="hljs-string">&#x27;a&#x27;</span>: [<span class="hljs-string">&#x27;sigmoid&#x27;</span>],<br>    <span class="hljs-string">&#x27;b&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>],<br>    <span class="hljs-string">&#x27;c&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>]<br>&#125;<br><br>simple_graph.pop(<span class="hljs-string">&#x27;b&#x27;</span>)<br>simple_graph<br><br>---<br>&#123;<span class="hljs-string">&#x27;a&#x27;</span>: [<span class="hljs-string">&#x27;sigmoid&#x27;</span>], <span class="hljs-string">&#x27;c&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>]&#125;<br></code></pre></td></tr></table></figure><p>看，我们在删除 node 的时候，其所对应的 value也就一起删除了，那这个时候，我们最后的输出列表里会丢失最后一个node。所以，我们在判断为最后一个的时候，需要额外的将其加上，放在 pop方法执行之前。那我们整个代码需要调整一下先后顺序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">topologic</span>(<span class="hljs-params">graph</span>):<br>    sorted_node = []<br>    <span class="hljs-keyword">while</span> graph:<br>        ...<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(need_remove) &gt; <span class="hljs-number">0</span>:<br>            node = random.choice(<span class="hljs-built_in">list</span>(need_remove))<br>            sorted_node.append(node)<br>           <br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(graph) == <span class="hljs-number">1</span>: sorted_node += graph[node]         <br>               <br>            graph.pop(node)<br><br>    <span class="hljs-keyword">return</span> sorted_node<br></code></pre></td></tr></table></figure><p>现在其实这个代码就已经 OK 了，我们来再加几句话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">...<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(need_remove) &gt; <span class="hljs-number">0</span>:<br>    ...<br>    <span class="hljs-keyword">for</span> _, links <span class="hljs-keyword">in</span> graph.items():<br>        <span class="hljs-keyword">if</span> node <span class="hljs-keyword">in</span> links: links.remove(node)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-keyword">raise</span> TypeError(<span class="hljs-string">&#x27;This graph has circle, which cannot get topological order.&#x27;</span>)<br>...<br></code></pre></td></tr></table></figure><p>我们把它的连接关系，例如现在选择了 k1，我们要把 k1的连接关系从这些里边给它删掉。</p><p>遍历一下 graph，遍历的时候如果删除的 node在它的输出里边，我们就把它删除。</p><p>加上<code>else</code>判断，如果图不是空的，但是最终没有找到，也就是这两个集合作减法，但是得到一个空集，没有找到，那我们就来输出一个错误：<code>This graph has circle, which cannot get topological order.</code></p><p>现在我们可以来实验一下了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">x, k, b, linear, sigmoid, y, loss = <span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;k&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;linear&#x27;</span>, <span class="hljs-string">&#x27;sigmoid&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&#x27;loss&#x27;</span><br>test_graph = &#123;<br>    x: [linear],<br>    k: [linear],<br>    b: [linear],<br>    linear: [sigmoid],<br>    sigmoid: [loss],<br>    y: [loss]<br>&#125;<br><br>topologic(test_graph)<br><br>---<br>[<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;k&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&#x27;linear&#x27;</span>, <span class="hljs-string">&#x27;sigmoid&#x27;</span>, <span class="hljs-string">&#x27;loss&#x27;</span>]<br></code></pre></td></tr></table></figure><p>好，现在让我们来声明一个<code>class node</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure><p>然后我们先来抽象一下这些节点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## Our Simple Model Elements</span><br><br>node_x = Node(inputs=<span class="hljs-literal">None</span>, outputs=[node_linear])<br>node_y = Node(inputs=<span class="hljs-literal">None</span>, outputs=[node_loss])<br>node_k = Node(inputs=<span class="hljs-literal">None</span>, outputs=[node_linear])<br>node_b = Node(inputs=<span class="hljs-literal">None</span>, outputs=[node_linear])<br>node_linear = Node(inputs=[node_x, node_k, node_b], outputs=[node_sigmoid])<br>node_sigmoid = Node(inputs=[node_linear], outputs=[node_loss])<br>node_loss = Node(inputs=[node_sigmoid, node_y], outputs=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><p>现在咱们就把图中每个节点已经给它抽象好了，但是我们发现节点写成这个样子代码是比较冗余。打比方说：<code>node_linear = Node(input=[node_x, node_k, node_b], outputs=[node_sigmoid])</code>，既然我们已经告诉程序<code>node_linear</code>这个节点的输入是<code>[node_x, node_k, node_b]</code>，那其实也就是告诉程序这些节点的输出是<code>node_linear</code>。</p><p>好，我们接下来要在<code>class Node</code>里定义一个方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs, outputs</span>):<br>    self.inputs = inputs<br>    self.outputs = outputs<br></code></pre></td></tr></table></figure><p>现在我们根据上面对代码冗余的分析，可以加上这样简单的一句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs=[]</span>):<br>    self.inputs = inputs<br>    self.outputs = []<br><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> inputs:<br>        node.outputs.append(self)<br></code></pre></td></tr></table></figure><p>把这句加上之后，就可以只在里面输入 inputs 就行了，不用再输入outputs，代码就变得简单多了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## Our Simple Model Elements</span><br><span class="hljs-comment">### version - 02</span><br>node_x = Node()<br>node_y = Node()<br>node_k = Node()<br>node_b = Node()<br>node_linear = Node(inputs=[node_x, node_k, node_b])<br>node_sigmoid = Node(inputs=[node_linear])<br>node_loss = Node(inputs=[node_sigmoid, node_y])<br></code></pre></td></tr></table></figure><p>我们是把每个节点给它做出来了，那么怎么样能够把这个节点给它像串珠子一样串起来变成一张图呢？</p><p>其实我们只要去考察所有的边沿节点就可以了，把所有的 x，y，k 和 b这种外层的函数给个变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">need_expend = [node_x, node_y, node_k, node_b]<br></code></pre></td></tr></table></figure><p>咱们再生成一个变量，这个变量是用来通过外沿这些节点，把连接图给生成出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_graph = defaultdict(<span class="hljs-built_in">list</span>)<br><br><span class="hljs-keyword">while</span> need_expend:<br>    n = need_expend.pop(<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-keyword">if</span> n <span class="hljs-keyword">in</span> computing_graph: <span class="hljs-keyword">continue</span><br><br>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> n.outputs:<br>        computing_graph[n].append(m)<br>        need_expend.append(m)<br></code></pre></td></tr></table></figure><p><code>while</code>里面，当外沿节点的 list不为空的时候，我们就在里面来取一个点，我们就取第一个吧，取出来并删除。</p><p>然后如果这个点我们已经考察过了，那就<code>continue</code>，如果没有，我们对于所有的这个n 里边的<code>outputs</code>，插入到 computing_graph 的 n的位置。再插入到外沿节点的 list内。因为我们现在多了一个扩充节点，所以我们需要给插入进去。</p><p>比方说我们这次找出来了 linear，把 linear也加到这个需要扩充的点一行，然后就可以从 linear 再找到 sigmoid 了。</p><p>来，我们看下现在的这个<code>computing_graph</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_graph<br><br>---<br>defaultdict(<span class="hljs-built_in">list</span>,<br>            &#123;&lt;__main__.Node at <span class="hljs-number">0x12053e080</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x12053e9b0</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053ef50</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x12053d510</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x12053c280</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x1202860e0</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x1202860e0</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053ef50</span>&gt;]&#125;)<br></code></pre></td></tr></table></figure><p>这样就获得出来了，其实是把它变成了刚刚的那个图。这样呢，我们就可以应用<code>topologic</code>来进行拓朴排序了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">topologic(computing_graph)<br><br>---<br>[&lt;__main__.Node at <span class="hljs-number">0x12053c280</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053d510</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053e080</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053e9b0</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x1202860e0</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053ef50</span>&gt;]<br></code></pre></td></tr></table></figure><p>但是我们打出来的内容都是一些内存地址，我们还需要改一下这个程序。我们在我们的<code>class Node</code>里多增加一个方法，用于return 它的名字：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs=[], name=<span class="hljs-literal">None</span></span>):<br>    ...<br>    self.name = name<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;Node:&#123;&#125; &#x27;</span>.<span class="hljs-built_in">format</span>(self.name)<br></code></pre></td></tr></table></figure><p>这样之后，我们还需要改一下节点，在里面增加一个变量<code>name=''</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">node_x = Node(name=<span class="hljs-string">&#x27;x&#x27;</span>)<br>...<br>node_loss = Node(inputs=[node_sigmoid, node_y], name=<span class="hljs-string">&#x27;loss&#x27;</span>)<br></code></pre></td></tr></table></figure><p>每一个都需要加上，我用<code>...</code>简化了代码。</p><p>然后我们再来看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">topologic(computing_graph)<br><br>---<br>[Node:k , Node:x , Node:b , Node:linear , Node:sigmoid , Node:y , Node:loss ]<br></code></pre></td></tr></table></figure><p>然后我们来将这段封装起来，变成一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">feed_dict = &#123;<br>    node_x: <span class="hljs-number">3</span>, <br>    node_y: random.random(),<br>    node_k: random.random(),<br>    node_b: <span class="hljs-number">0.38</span><br>&#125;<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_feed_dict_to_graph</span>(<span class="hljs-params">feed_dict</span>):<br>    need_expend = [n <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> feed_dict]<br>    ...<br><br>    <span class="hljs-keyword">return</span> computing_graph<br></code></pre></td></tr></table></figure><p>一般来说，很多大厂在建立代码的时候，<code>x, y, k, b</code>这种东西会被称为<code>placeholder</code>，我们创建的<code>need_expend</code>会被称为是<code>feed_dict</code>。所以我们做了这样一个修改，将<code>need_expend</code>拿到方法里取重新获取。</p><p>这些节点刚开始的时候没有值，那我们给它一个初始值，我这里的值都是随意给的。</p><p>这样，就不仅把最外沿的节点给找出来了，而且还把值给他送进去了，相对来说就会更简单一些。所有定义出来的节点，我们都可以把它变成图关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">topologic(convert_feed_dict_to_graph(feed_dict))<br><br>---<br>[Node:k , Node:y , Node:b , Node:x , Node:linear , Node:sigmoid , Node:loss ]<br></code></pre></td></tr></table></figure><p>咱们现在再定一个点，我们用一个变量存起来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sorted_nodes = topologic(convert_feed_dict_to_graph(feed_dict))<br></code></pre></td></tr></table></figure><p>那么咱们现在来模拟一下它的计算过程，模拟神经网络的计算过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fowward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;I am &#123;&#125;, I calculate myself value!!!&#x27;</span>.<span class="hljs-built_in">format</span>(self.name))<br><br><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.forward()<br><br>---<br>I am y, I calculate myself value!!!<br>I am x, I calculate myself value!!!<br>I am b, I calculate myself value!!!<br>I am k, I calculate myself value!!!<br>I am linear, I calculate myself value!!!<br>I am sigmoid, I calculate myself value!!!<br>I am loss, I calculate myself value!!!<br></code></pre></td></tr></table></figure><p>我们在<code>Node</code>里定义了一个方法<code>forward</code>，从前往后运算，这个时候我们在每个里面加一个向前运算。</p><p>这个就是拓朴排序的作用，经过排序之后，那需要在后面计算的节点，就一定会放在后面再进行计算。</p><p>好，那我们现在需要区分两个内容，一个是被赋值的内容，一个是需要计算的内容。</p><p>刚才我们说过，在大厂的这些地方，<code>x,y,k,b</code>这种东西都被定义为占位符，那我们来修改一下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs=[], name=<span class="hljs-literal">None</span></span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;I am &#123;&#125;, 我需要自己计算自己的值。&#x27;</span>.<span class="hljs-built_in">format</span>(self.name))<br>    ...<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Placeholder</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, name=<span class="hljs-literal">None</span></span>):<br>        Node.__init__(self, name = name)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;I am &#123;&#125;, 我已经被人为赋值了。&#x27;</span>.<span class="hljs-built_in">format</span>(self.name))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;Node:&#123;&#125; &#x27;</span>.<span class="hljs-built_in">format</span>(self.name)<br><br><span class="hljs-comment">### version - 02</span><br>node_x = Placeholder(name=<span class="hljs-string">&#x27;x&#x27;</span>)<br>node_y = Placeholder(name=<span class="hljs-string">&#x27;y&#x27;</span>)<br>node_k = Placeholder(name=<span class="hljs-string">&#x27;k&#x27;</span>)<br>node_b = Placeholder(name=<span class="hljs-string">&#x27;b&#x27;</span>)<br>node_linear = Node(inputs=[node_x, node_k, node_b], name=<span class="hljs-string">&#x27;linear&#x27;</span>)<br>node_sigmoid = Node(inputs=[node_linear], name=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>node_loss = Node(inputs=[node_sigmoid, node_y], name=<span class="hljs-string">&#x27;loss&#x27;</span>)<br></code></pre></td></tr></table></figure><p>我们创建了一个 Placeholder 类，继承了 Node,然后我们取修改初始化方法，它是是没有 input 的，只有一个 name。</p><p>然后 forward 我们改一下，改成打印已经被赋值的语句。父类 Node 里的forward 也改一下，改成需要自己计算自己的值。</p><p>那我们这个时候将赋值的四个节点改成调用 Placeholder。</p><p>接下来，我们需要修改<code>convert_feed_dict_to_graph</code>方法了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_feed_dict_to_graph</span>(<span class="hljs-params">feed_dict</span>):<br>    ...<br>    <span class="hljs-keyword">while</span> need_expend:<br>        ...<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(n, Placeholder): n.value = feed_dict[n]<br>        ...<br>    ...<br></code></pre></td></tr></table></figure><p>我们来检查这个节点是否是 Placeholder，如果是的话，将当前的 feed_dict赋值给 n.value。来看下结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.forward()<br><br>---<br>I am b, 我已经被人为赋值了。<br>I am x, 我已经被人为赋值了。<br>I am k, 我已经被人为赋值了。<br>I am y, 我已经被人为赋值了。<br>I am linear, 我需要自己计算自己的值。<br>I am sigmoid, 我需要自己计算自己的值。<br>I am loss, 我需要自己计算自己的值。<br></code></pre></td></tr></table></figure><p>好，到现在为止，咱们只是打了一段文字，问题是对于<code>linear, sigmoid</code>和<code>loss</code>,到底是怎么计算的呢？</p><p>这个问题，咱们放到下一节课里面去讲，现在咱们这篇文章已经超标了，目测应该超过万字了吧。</p><p>好，下节课记得来看咱们具体如何在实现拓朴排序后将计算加进去。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202310311950753.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;上节课，我们讲了多层神经网络的原理，并且明白了，数据量是层级无法超过
3 层的主要原因。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>20. 深度学习 - 多层神经网络</title>
    <link href="https://hivan.me/20.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://hivan.me/20.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2023-11-12T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:28.619Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310694.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><p>之前两节课的内容，我们讲了一下相关性、显著特征、机器学习是什么，KNN模型以及随机迭代的方式取获取 K 和 B，然后定义了一个损失函数（loss函数），然后我们进行梯度下降。</p><span id="more"></span><p>可以说是又帮大家回顾了一下深度学习的相关知识，但是由于要保证整个内容的连续性，所以这也没办法。</p><p>那么接下来的课程里，咱们要来看一下神经网络，怎么样去拟合更加复杂的函数，什么是激活函数，什么是神经网络，什么是深度学习。</p><p>然后我们还要来学习一下反向传播，以及如何实现自动的反向传播，什么是错误排序以及怎么样自动的去计算元素的gradients。梯度怎么样自动求导。</p><p>从简单的线性回归函数到复杂的神经网络，从人工实现的求导到自动求导。那我们现在来跟大家一起来看一下。</p><p>上一节课结束的时候我们说过，现实生活中绝大多数事情的关系都不是线性的。</p><p>比方说，我工作的特别努力，然后就可以升职加薪了。但是其实有可能工作的努力程度和升职加薪程度之间的关系可能并不是一条直线的函数关系。</p><p>可能一开始不管怎么努力，薪水都没有什么大的变化，可是忽然有了一个机会，薪水涨的幅度很大，但是似乎没怎么努力。再之后，又趋于一条平行横轴的线，不管怎么努力都无法往上有提升。这是不是咱们这些社畜的真实写照？</p><p>在现实生活中有挺多这样的问题，这样的对应关系。比如艾宾浩斯曲线，再比如细菌生长曲线，很多很多。</p><p>经过刚刚的分析我们知道了除了线性函数(kx+b)，还有一种常见的函数关系式，是一种 s 型的一种函数，这种 s形的函数在我们整个计算机科学里我们称呼它为<code>sigmoid</code>函数：</p><p><span class="math display">\[\begin{align*}Sigmoid: f(x) = \sigma (x) = \frac{1}{1+e^{-x}}\end{align*}\]</span></p><p>这是一个非常常见的函数。我们可以节用 NumPy库来用代码将它实现出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br></code></pre></td></tr></table></figure><p>把它的图像描述出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">sub_x = np.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br>plt.plot(sub_x, sigmoid(sub_x))<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310696.png"alt="Alt text" /></p><p>然后我们来利用一下这个函数，我们定义一个随机线性函数，然后和 sigmoid函数一起应用画 5 根不同的线：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_linear</span>(<span class="hljs-params">x</span>):<br>    k, b = random.random(), random.random()<br>    <span class="hljs-keyword">return</span> k*x + b<br><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    plt.plot(sub_x, random_linear(sigmoid(random_linear(sub_x))))    <br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310697.png"alt="Alt text" /></p><p>这个里面起变化的就是 k 和 b 这两个参数，那我们来调节 k 和 b的画，就可以变化这条曲线的样式。</p><p>除了以上这些函数，我们生活中还会遇到更复杂的函数，甚至很有可能是一个复杂的三维图像。</p><p>那这个时候，我们该如何去拟合这么多复杂的函数呢？</p><p>一个比较直接的方法，当然就是我们人为的去观察，比如这个类似于sin(x):</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310698.png"alt="Alt text" /></p><p>还有这个 k*sin(kx+b)+b:</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310699.png"alt="Alt text" /></p><p>当然，这样理论上是可以的，每当我们遇到一个场景之后就自己去提出来一个函数模型，然后把函数模型让机器去拟合出来。</p><p>但是这样就会有一个问题，大家就会发现假如你是那个工作者，那你熊猫眼会很严重，因为我们要看到很多这样的场景。现实生活中的问题实在太多了，每一天我们都可能会遇到新的函数。</p><p>如果我们每观察一个情况就要去考察，去思考它的这个函数模型是什么，你就会发现你的工作量无穷无尽。而且你会发现一个问题：现在函数能够可视化的，但是如果在现实生活中有很多场景的函数是无法可视化的。</p><p>那么这个时候我们就需要其他的一些方法能够拟合更加复杂的函数，这个时候我们怎么样不通过去观察它就能够拟合出复杂的函数呢？其实很简单。</p><p>有一个老头子叫做 Hinton，他是 2018 年图灵奖的获得者。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310700.png"alt="Alt text" /></p><p>他在一九八几年的时候发了一篇文章，经过他多年的研究，发现人的脑能够做出非常复杂的一些行为，其实我们这个神经元的类型都是很有限的，并没有很多奇怪的东西，就是有很多不同的节点，其实就那么几种。人类就能够进行复杂行为，背后其实就是一些基本的神经元的一些组合。</p><p>只不过这些基本的在组合还有一种形式，就是输入进来的会经过一个叫做activateneurons，就是激活单元，去做一个非线性变化。然后经过不断的这种非线性变化，最后就拟合出来非常复杂的信号。</p><p>那非线性变化的这些函数其实都是一样的，就他们背后的逻辑都是一样。只不过有的时候非线性变化的多，有的时候非线性变化的少。</p><p>讲了这么多不直观的东西，我们来看点实际的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    i = random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sub_x)))<br>    output_1 = np.concatenate((random_linear(sub_x[:i]), random_linear(sub_x[i:])))<br>    plt.plot(sub_x, output_1)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310701.png"alt="Alt text" /></p><p>然后我们来做两件事，第一个是将 k,b随机方式改成<code>normalvariate()</code>，第二个在上面的基础上再做一次拆分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_linear</span>(<span class="hljs-params">x</span>):<br>    k, b = random.normalvariate(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>), random.normalvariate(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> k * x + b<br><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    i = random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sub_x)))<br>    linear_output = np.concatenate((random_linear(sub_x[:i]), random_linear(sub_x[i:])))<br>    i_2 = random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(linear_output)))<br>    output = np.concatenate((sigmoid(linear_output[:i_2]), sigmoid(linear_output[i_2:])))<br><br>    plt.plot(sub_x, output)   <br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310702.png"alt="Alt text" /></p><p>我们来看，这个时候你会发现他生成的这个图像比较奇怪。它生成了很多奇怪的函数，每一次根据不同的参数就形成了不同的函数图像。</p><p>迄今为止就两个函数，一个 sigmoid，一个 randomlinear，但是它生成了很多奇怪的函数。</p><p>面对这种函数，这么多层出不穷列举都列举不完的函数，我们怎么样能够每次遇到一个问题就得去提出它这个函数到底是什么样。而且关键是有可能函数维度高了之后都观察不到它是什么关系。</p><p>所以我们就会去考虑，怎么样能够让机器自动的去拟合出来更复杂的函数呢？</p><p>我们可以用基本模块经过组合拼接，然后就能够形成复杂函数。而在组合拼接的过程中，我们只需要让机器自动的去把K1、K2、B1、B2 等等这些参数给它拟合出来就行。</p><p>也就是说我们可以通过参数的变化来拟合出来各种各样的函数。</p><p>这其实就是深度神经网络的一个核心思想。就是用基本的模块像大家玩积木一样，并不会有很多积木类型给你，只有一些基本的东西，但是通过这些基本的可以造出来特别多复杂的东西。</p><p>这个就是背后的原理，通过函数的复合和叠加。这种变化的引起都是由一个线性函数加上一个非线性函数。</p><p>其实很大程度上由我们大脑里边这种简单东西可以构成复杂东西得到了启示。只不过人的大脑里边，在脑神经科学里面把这种非线性变化呢叫做activate neurons，叫做激活神经元。在程序里，我们把这种非线性函数叫activation function，激活函数。</p><p>激活函数的作用就是为了让我们的程序能够拟合非线性关系。如果没有激活函数，咱们的程序永远只能拟合最简单的线性关系，而现实生活中绝大多数关系并不是并不是线性关系。</p><p>让机器来拟合更加复杂函数的这种方法，和我们的神经网络很像，就是咱们现在做的这个事情，我们就把它命了个名叫做神经网络。</p><p>早些年的时候科学家们有一个理论，人们把一组线性和非线性变化叫做一层。在以前的时候科学家们发现这个层数不能多于三层，就是神经网络的层数不能多于三层。</p><p>为什么不能多于 3层？其实最主要的不是计算量太大的问题，最核心的原因是什么？</p><p>假设我们有一个 f(x) 和一个 x组成一个平面坐标系，在其中有无数的点，当我们在做拟合的时候，发现了一条直线可以拟合，但是实际上呢，当我们将数据量继续放大的时候，才发现我们的拟合的直线偏离的非常厉害。</p><p>我们之前在机器学习的课程里说过，我们要有高精度，就需要有足够的数据量。如果这个时候变成一个三维问题，就需要更多的数据量。没有更多的数据的话，就好比有一个平板在空中，它会摇来摇去，你以为拟合了一个正确的平板，但其实完全不对。</p><p>那这个时候，每当我们所需要拟合的参数多一个，多少数据量认为是足够的？这个不一定。这个和整个问题的复杂程度有关系。</p><p>后来科学家们发现一个规律，在相似的问题下，我们需要拟合的参数多一个，需要的数据就要多一个数量级。</p><p>当变成三层的时候，会发现参数就更多了，而参数变得特别多就会需要特别多的数据量。而早在一九八几年、一九九几年的时候并没有那么多的数据量，就会产生数据量不够的情况，所以模型在现实生活中没法用。</p><p>但是随着到二零零几年，再到二零一几年之后，产生了大量的数据。就给我们做函数拟合提供了数据资源，所以数据量是最重要的，数据量决定了这个东西能不能做。而其他的一些，比方说计算、GPU啊等等，它是加速这个过程的，是让它更方便。</p><p>那么后来我们把层数超过 3层的就叫深度神经网络，机器学习就简称深度学习。这是为什么深度学习在二零一几年的时候才开始火起来。</p><p>那现在我们把上一节课的这个问题再拿过来，现在来想想，如果我们把房价的函数关系也写成类似的，linear和 sigmoid 之间的关系，那会怎么样呢？</p><p>首先，我们的 k 和 b 就会多加一组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_times):<br>    k1 = k1 + (-<span class="hljs-number">1</span>) * loss对k1的偏导<br>    b1 = b1 + (-<span class="hljs-number">1</span>) * loss对b1的偏导<br>    k2 = k2 + (-<span class="hljs-number">1</span>) * loss对k2的偏导<br>    b2 = b2 + (-<span class="hljs-number">1</span>) * loss对b2的偏导<br><br>    loss_ = loss(y, model(X_rm, k1, b1, k2, b2))<br>    ...<br></code></pre></td></tr></table></figure><p>然后我们的 model也会多接受了一组参数，并且我们要将其内部函数关系做一个叠加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model</span>(<span class="hljs-params">x, k1, k2, b1, b2</span>):<br>    linear1_output = k1 * x + b1<br>    sigmoid_output = sigmoid(linear1_output)<br>    linear2_output = k2 * sigmoid_output + b2<br><br>    <span class="hljs-keyword">return</span> linear2_output<br></code></pre></td></tr></table></figure><p>在这个时候我们要求解的时候就会发现有个问题，我们现在的 loss是这样的：</p><p><span class="math display">\[\begin{align*}loss &amp; = \frac{1}{N}(y_i - \hat y)^2 \\&amp; = \frac{1}{N} \begin{bmatrix}y_1 - (k_2 \frac{1}{1+e^{-(k_1x+b_1)}}+b_2)\end{bmatrix} ^2\end{align*}\]</span></p><p>似乎变得有点过于复杂。当前情况下，我们是可以复杂的去求导，但是当函数继续复杂下去的时候，怎么把这个导数求出来呢？</p><p>函数还可以继续叠加，层数还可以写的越来越多。那么怎么样才能给它求出它的导数呢？</p><p>我们再将上面的式子做个变化：</p><p><span class="math display">\[\frac{1}{N}[l_2(\sigma(l_1(x))) -y_1]^2\]</span></p><p>这样我们就可以将问题进行简化，我们上面代码里<code>loss对k1的偏导</code>就可以写成：</p><p><span class="math display">\[\frac{\partial loss}{\partial  l_2} \cdot \frac{\partial l_2}{\partial\sigma} \cdot \frac{\partial \sigma}{\partial l_1} \cdot \frac{\partiall_1}{\partial k_1}\]</span></p><p>同理，<code>loss对b1的偏导</code>就是：</p><p><span class="math display">\[\frac{\partial loss}{\partial  l_2} \cdot \frac{\partial l_2}{\partial\sigma} \cdot \frac{\partial \sigma}{\partial l_1} \cdot \frac{\partiall_1}{\partial b_1}\]</span></p><p>这个时候，问题就变成一个可解决的了。</p><p><span class="math inline">\(\frac{\partial loss}{\partiall_2}\)</span>其实就等于<span class="math inline">\(\frac{2}{N}(l_2 -y_1)\)</span>。</p><p>我们继续往后看第二部分，那么这个时候我们可以得到<spanclass="math inline">\(l_2 = k_2 \cdot \sigma + b_2\)</span>，那<spanclass="math inline">\(\frac{\partial l_2}{\partial\sigma}\)</span>就等于<span class="math inline">\(k_2\)</span>。</p><p>再来看第三部分，<span class="math inline">\(\sigma&#39;(x) =\sigma(x) \cdot (1- \sigma(x)\)</span>, 所以<spanclass="math inline">\(\frac{\partial \sigma}{\partial l_1} = \sigma\cdot (1 - \sigma)\)</span>。</p><p>最后第四部分，<span class="math inline">\(\frac{\partiall_1}{\partial k_1} = x\)</span>。</p><p>这样，我们整个式子就应该变成这样：</p><p><span class="math display">\[\begin{align*}\frac{2}{N}(l_2 - y_1) \cdot k_2 \cdot \sigma \cdot (1 - \sigma) \cdot x\end{align*}\]</span></p><p>这样的话，我们就把 loss 对于 K1的偏导就求出来了，这里算是一个突破。本来看起来是很复杂的的一个问题，我们将其拆分成了这样的一种形式。那这种形式，我们把它称作「链式求导」。</p><p>但是现在其实还有个问题，这整个一串链式求导的东西是我们通过眼睛求出来的，但是现在怎么样让机器自动的把这一串东西写出来？就是机器怎么知道是这些数字乘到一起？</p><p>换句话说，我们现在把这个问题再形式化一下，定义一个问题。</p><p>给定一个模型定义，这个模型里边包含参数<code>&#123;k1, k2, b1, b2&#125;</code>，我们要构建一个程序，让它能够求解出k1,k2,b1,b2 的偏导是多少。</p><p>如果我们想解决这个问题，我们首先要思考一下，<spanclass="math inline">\(k_1, k_2, b_1, b_2, l_1, l_2, \sigma, y_{true},loss\)</span>, 它们之间是一种什么样的关系。</p><p>观察一下我们会发现它们之间的关系是这样的：</p><p><span class="math display">\[\begin{align*}&amp; \{k_1, b_1, x\} \to l_1 \to \sigma, \\&amp; \{\sigma, k_2, b_2\} \to l_2, \\&amp; \{l_2, y_{true}\} \to loss \\&amp; \to 表示的是&#39;输出到&#39;的关系。\end{align*}\]</span></p><p>要用计算机去表示这种关系，是典型的一个数据结构问题，怎么样让计算机合理的去存储它，你会发现这个是一个图案。</p><p>这种节点和节点之间通过关系连接起来的就把它叫做图，我们把它先表示成图的样子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_graph = &#123;<br>    <span class="hljs-string">&#x27;k1&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;b1&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;x&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;L1&#x27;</span>:[<span class="hljs-string">&#x27;sigmoid&#x27;</span>],<br>    <span class="hljs-string">&#x27;sigmoid&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;k2&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;b2&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;L2&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>],<br>    <span class="hljs-string">&#x27;y&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>]<br>&#125;<br><br>nx.draw(nx.DiGraph(computing_graph), with_labels = <span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310703.png"alt="Alt text" /></p><p>这个就是我们要表达的一个关系，我们把这个变成图。</p><p>现在我们将给定的一个model，这样一个函数变成了这样一张图。计算机里有现成的各种各样的图算法，我们就可以来计算这个图之间的关系了。</p><p>现在我们就要根据这个图的表示来思考我们如何求 loss 对 K1的偏导。那其实，我们可以发现 k1 在末尾出，一直在向前输入直到loss。换句话说，我们可以通过 k1一直往图的终点去寻找来找到它求导的这个过程。</p><p>也就是说，只要我们的能把模型变成一个图，然后我们就可以根据这些点去找到它们之间节点的对应关系，我们就可以通过这个节点关系来获得它的求导过程了。</p><p>那下一节课呢，我们就继续来看一下，如何将这个图的关系，变成一个自动求导的过程。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310694.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;之前两节课的内容，我们讲了一下相关性、显著特征、机器学习是什么，KNN
模型以及随机迭代的方式取获取 K 和 B，然后定义了一个损失函数（loss
函数），然后我们进行梯度下降。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>19. 深度学习 - 用函数解决问题</title>
    <link href="https://hivan.me/19.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E7%94%A8%E5%87%BD%E6%95%B0%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/"/>
    <id>https://hivan.me/19.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E7%94%A8%E5%87%BD%E6%95%B0%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/</id>
    <published>2023-11-09T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:32.376Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231030164335.png"alt="茶桁的 AI 秘籍 核心基础 19" /></p><p>Hi，你好。我是茶桁。</p><p>上一节课，我们从一个波士顿房价的预测开始写代码，写到了 KNN。</p><span id="more"></span><p>之前咱们机器学习课程中有讲到 KNN这个算法，分析过其优点和缺点，说起来，KNN这种方法比较低效，在数据量比较大的时候就比较明显。</p><p>那本节课，我们就来看一下更加有效的学习方法是什么，A more EfficientLearning Way.</p><p>接着我们上节课的代码我们继续啊，有不太了解的先回到上节课里去看一下。</p><p>我们<code>X_rm</code>和<code>y</code>如果能够找到这两者之间的函数关系，每次要计算的时候，输入给这个函数，就能直接获得预测值。</p><p>那这个函数关系怎么获得呢？我们需要先观察一下，这个时候就到了我们的拟合函数关系。</p><p>那既然要观察，当然最好就是将数据可视化之后进行观察：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.scatter(X_rm, y)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231030164157.png"alt="Alt text" /></p><p>可以看到，它们之间的关系大体应该这样一种关系：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231030164158.png"alt="Alt text" /></p><p>那这个样子的图我们熟悉不？是不是在线性回归那一张里我们见过？也就是用一根直线去拟合了这些点的一个趋势。</p><p>我们把它写出来：</p><p><span class="math display">\[f(x) = k \cdot rm + b\]</span></p><p>那我们现在就会把这个问题变成，假设现在的函数<code>k*rm+b</code>，那我们就需要找到一组k 和b，然后让它的拟合效果最好。这个时候我们就会遇到一个问题，拟合效果怎样算是好？</p><p>比方说我们现在有一组数据，一组实际的值，还有一组预测值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">real_y = &#123;<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>&#125;<br>y_hats = &#123;<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">7</span>&#125;<br>y_hats2 = &#123;<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>&#125;<br></code></pre></td></tr></table></figure><p>问哪个值更好。</p><p>我们会发现这两个预测都挺好的，那哪个更好？这个时候我们需要搬出我们的loss 函数了。</p><p>loss函数就是在我们进行预测的时候，它的信息损失了多少，所以我们称其为损失函数，loss函数。 <span class="math display">\[loss(y, \hat y) = \frac{1}{N}{\sum_{i \in N}}(y_i - \hat {y_i})^2\]</span> y_i - yhat_i 这个值越接近于 0。等于 0 的意思就是每一个预测的 y都和实际的 y 的值是一样的。那么如果这个值越大指的是预测的 y 和实际的 y之间差的越大。</p><p>那我们在这个地方就可以定义一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">y, yhat</span>):<br>    <span class="hljs-keyword">return</span> np.mean((np.array(y) - np.array(yhat))** <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p>然后我们直接将两组 yhat 和真实的 real_y 代入进去比对：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">loss(real_y, y_hats)<br>loss(real_y, y_hats2)<br><br>---<br><span class="hljs-number">1.3333333333333333</span><br><span class="hljs-number">0.3333333333333333</span><br></code></pre></td></tr></table></figure><p>所以它这个意思是说 yhats2 的效果更好一些。</p><p>那我们将上面这个 loss 函数就叫做 Mean SquaredError，就是均方误差，也简称 MSE。咱们现在有了loss，就有了是非判断的标准了，就可以找到最好的结果。</p><p>有了判断标准怎么样来获得最优的 k 和 b呢？早些年的时候有这么几种方法，第一种是直接用微积分的方法做计算。 <spanclass="math display">\[\begin{align*}loss &amp; = \frac{1}{N}\sum_{i\in N}(y_i - \hat y)^2 \\&amp; = \frac{1}{N}\sum_{i\in N}(y_i - (kx_i + b))^2 \\\end{align*}\]</span> 此时我们是知道 x_i 和 y_i 的值，N也是常数。那么其实求偏导之后它就可以变化成下面这组式子： <spanclass="math display">\[Ak^2 + Bk +C \\A&#39;b^2+B&#39;b+C&#39;\]</span> A、B、C 是根据我们所知道的 x_i 和 y_i 以及常数 N来计算出来的数。这个时候 loss 要取极值的时候，我们令其为 loss’，那loss’就等于-A/2B，或者-A’/2B’。那么这种方法我们就称之为最小二乘法，它是为了最小化MSE，对 MSE 求偏导数并令其等于零，来找到使 MSE 最小的参数值。</p><p>但是为什么后来人们没有用微积方的方法直接做呢？是因为这个函数会变得很复杂，当函数变得极其复杂的时候，学过微积分的同学就应该知道，你是不能直接求出来他的导数的。也就是说当函数变得极其复杂的时候，直接用微积分是求不出来极致点的，所以这种方法后来就没用。</p><p>第二种方法，后来人们想了可以用随机模拟的方法来做。</p><p>我们首先来在 -100 到 100 之间随机两个值：k 和 b</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">VAR_MAX, VAR_MIN = <span class="hljs-number">100</span>, -<span class="hljs-number">100</span><br>k, b = random.randint(VAR_MIN, VAR_MAX), random.randint(VAR_MIN, VAR_MAX)<br></code></pre></td></tr></table></figure><p>只拿到一组当然是无从比较的，所以我们决定拿个 100 组的随机值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">total_times = <span class="hljs-number">100</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_times):<br>    k, b = random.randint(VAR_MIN, VAR_MAX), random.randint(VAR_MIN, VAR_MAX)<br></code></pre></td></tr></table></figure><p>然后定义一个值，叫做最小的 loss。这个最小的 loss一开始取值为无穷大，并且再给两个值，最好的 k 和最好的b，先赋值为<code>None</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">min_loss = <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)<br>best_k, best_b = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><p>之后我们要拿预测值来赋值给新的loss，我们来定义一个函数，它要做的事情很简单，就是返回<code>k*x+b</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model</span>(<span class="hljs-params">x, k, b</span>):<br>    <span class="hljs-keyword">return</span> k*x + b<br><br>loss_ = loss(y, model(X_rm, k, b))<br></code></pre></td></tr></table></figure><p>接着我们就可以来进行对比了，就会找到那组最好的 k 和 b：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> loss_ &lt; min_loss:<br>    min_loss = loss_<br>    best_k, best_b = k, b<br></code></pre></td></tr></table></figure><p>完整的代码如下，当然我们是接着之前的代码写的，所以<code>loss</code>函数和<code>y</code>，还有<code>X_rm</code>都是在之前代码中有过定义的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">VAR_MAX, VAR_MIN = <span class="hljs-number">100</span>, -<span class="hljs-number">100</span><br>min_loss = <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)<br>best_k, best_b = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model</span>(<span class="hljs-params">x, k, b</span>):<br>    <span class="hljs-keyword">return</span> x * k +b<br><br>total_times = <span class="hljs-number">100</span><br><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_times):<br>    k, b = random.randint(VAR_MIN, VAR_MAX), random.randint(VAR_MIN,VAR_MAX)<br><br>    loss_ = loss(y, model(X_rm, k, b))<br><br>    <span class="hljs-keyword">if</span> loss_ &lt; min_loss:<br>        min_loss = loss_<br>        best_k, best_b = k, b<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;在&#123;&#125;时刻找到了更好的 k: &#123;&#125;, b: &#123;&#125;，这个 loss 是：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(t, k, b, loss_))<br><br>---<br>在<span class="hljs-number">0</span>时刻找到了更好的k: <span class="hljs-number">12</span>, b: <span class="hljs-number">89</span>， 这个loss是：<span class="hljs-number">20178.46882444269</span><br>在<span class="hljs-number">8</span>时刻找到了更好的k: <span class="hljs-number">2</span>, b: <span class="hljs-number">2</span>， 这个loss是：<span class="hljs-number">131.87000511462452</span><br>在<span class="hljs-number">21</span>时刻找到了更好的k: <span class="hljs-number">11</span>, b: -<span class="hljs-number">48</span>， 这个loss是：<span class="hljs-number">47.340357088932805</span><br></code></pre></td></tr></table></figure><p>如果我们将寻找的次数放大，改为 10**3,那我们会发现，开始找的很快，但是后面寻找的会越来越慢。</p><p>就类似于你现在在一个公司，假设你从刚进去的时候，要达到职位很高，薪水很高。小职员你想一直升职，你可以随机的去做很多你喜欢做的事情，没有人指导你。一开始的时候，你会发觉自己的升职加薪似乎并没有那么困难，但是随着自己越往上，升职的速度就降下来了，因为上面职位并没有那么多了。这个时候你所需要尝试和努力就会越来越多。到后面你每尝试一步，你所需要的努力就会越来越多。</p><p>那么这个时候我们就要想，我们怎么样能够让更新频率更快呢？而不要像这样到后面基本上不更新了。</p><p>不知道我们是否还记得大学时候的数学知识，假设现在这个 loss 和 k在一个二维平面上，我们对 loss 和 k 来求一个偏导：</p><p><span class="math display">\[\frac{\partial loss}{\partial k}\]</span></p><p>这个导数的取值范围就会导致两种情况，当其大于 0 的时候，k 越大，则loss 也越大，当其小于 0 的时候，k 越大，loss 则越小。</p><p>那我们在这里就可以总结出一个规律：</p><p><span class="math display">\[p&#39; = p + (-1)\frac{\partial loss}{\partial p} * \alpha\]</span></p><p><spanclass="math inline">\(\alpha\)</span>就是一个很小的数，因为我们每次要只能移动很小的一点，不能减小很多。</p><p>那有了这个，我们就可以将我们的 k 和 b 应用上去，也就可以得到：</p><p><span class="math display">\[\begin{align*}k&#39; = k + (-1)\frac{\partial loss}{\partial k} \cdot \alpha \\b&#39; = b + (-1)\frac{\partial loss}{\partial b} \cdot \alpha \\\end{align*}\]</span></p><p>那我们如何使用计算机来实现刚刚讲的这些内容呢？我们先把上面的式子再做一下变化：</p><p><span class="math display">\[k_{n+1} = k_n + -1 \cdot \frac{\partial loss(k, b)}{\partial k_n} \\b_{n+1} = b_n + -1 \cdot \frac{\partial loss(b, b)}{\partial b_n}\]</span></p><p>这个就是所谓的梯度下降。</p><p>那现在的问题就变成，如何使用计算机来实现梯度下降。我们就来定义两个求导函数，并且将之前的代码拿过来做一些修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">y, yhat</span>):<br>    <span class="hljs-keyword">return</span> np.mean((np.array(y) - np.array(yhat)) ** <span class="hljs-number">2</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partial_k</span>(<span class="hljs-params">x, y, k_n, b_n</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * np.mean((y - (k * x + b))*(-x))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partial_b</span>(<span class="hljs-params">x, y, k_n, b_n</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * np.mean((y - (k * x + b))*(-<span class="hljs-number">1</span>))<br><br>k,b = random.random(), random.random()<br><br>min_loss = <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)<br>best_k, best_b = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br>total_times = <span class="hljs-number">500</span><br>alpha = <span class="hljs-number">1e-3</span><br><br>k_b_history = []<br><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_times):<br>    k = k + (-<span class="hljs-number">1</span>) * partial_k(X_rm, y, k, b) * alpha <br>    b = b + (-<span class="hljs-number">1</span>) * partial_b(X_rm, y, k, b) * alpha<br><br>    loss_ = loss(y, model(X_rm, k, b))<br><br>    <span class="hljs-keyword">if</span> loss_ &lt; min_loss:<br>        min_loss = loss_<br>        best_k, best_b = k, b<br>        k_b_history.append([best_k, best_b])<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;在&#123;&#125;时刻找到了更好的 k: &#123;&#125;, b: &#123;&#125;，这个 loss 是：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(t, k, b, loss_))<br><br>---<br>在<span class="hljs-number">0</span>时刻找到了更好的k: <span class="hljs-number">0.8391888851738278</span>, b: <span class="hljs-number">0.44333100376779605</span>， 这个loss是：<span class="hljs-number">360.000103176194</span><br>在<span class="hljs-number">1</span>时刻找到了更好的k: <span class="hljs-number">1.0586893752129705</span>, b: <span class="hljs-number">0.474203003102507</span>， 这个loss是：<span class="hljs-number">312.7942150454931</span><br>...<br>在<span class="hljs-number">498</span>时刻找到了更好的k: <span class="hljs-number">3.587603582169745</span>, b: <span class="hljs-number">0.40777844839877003</span>， 这个loss是：<span class="hljs-number">58.761172062586965</span><br>在<span class="hljs-number">499</span>时刻找到了更好的k: <span class="hljs-number">3.587736446932306</span>, b: <span class="hljs-number">0.4069332804559017</span>， 这个loss是：<span class="hljs-number">58.760441520932375</span><br></code></pre></td></tr></table></figure><p>其实关于这个内容，我们在机器学习 -线性回归那一章就介绍过。看不懂这一段的小伙伴可以回过头取好好看一下那一章。</p><p>那这样，我们可以发现，之前是间隔很多次才作一词更新，而现在是每一次都会进行更新，一直在减小。这个是因为我们实现了一个「监督」。</p><p>在这样的情况下结果就变得更好了，比如我们再将次数调高一点，在全部运行完之后，我们来画个图看看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(X_rm, y)<br>plt.scatter(X_rm, best_k * X_rm + best_b, color=<span class="hljs-string">&#x27;orange&#x27;</span>)<br>plt.plot(X_rm, best_k * X_rm + best_b, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231030164159.png"alt="Alt text" /></p><p>我们可以看到它拟合出来的点和连接成的直线，和我们上面手动去画的似乎还是有很大差别的。</p><p>在刚才的代码里我还做了一件事情，定义了一个<code>k_b_history</code>,然后将所有的 best_k 和 best_b都存储到了里面。然后我们随机取几个点，第一个取第 10 个测试点，第二个取第50 次测试点，第三个我们取第 5000 次，第四个我们取最后一次：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_0, test_1, test_2, test_3, test_4 = <span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>, <span class="hljs-number">5000</span>, -<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>然后我们分别画一下这几个点的图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(X_rm, y)<br>plt.scatter(X_rm, k_b_history[test_0][<span class="hljs-number">0</span>] * X_rm + k_b_history[test_0][<span class="hljs-number">1</span>])<br>plt.scatter(X_rm, k_b_history[test_1][<span class="hljs-number">0</span>] * X_rm + k_b_history[test_1][<span class="hljs-number">1</span>])<br>plt.scatter(X_rm, k_b_history[test_2][<span class="hljs-number">0</span>] * X_rm + k_b_history[test_2][<span class="hljs-number">1</span>])<br>plt.scatter(X_rm, k_b_history[test_3][<span class="hljs-number">0</span>] * X_rm + k_b_history[test_3][<span class="hljs-number">1</span>])<br>plt.scatter(X_rm, k_b_history[test_4][<span class="hljs-number">0</span>] * X_rm + k_b_history[test_4][<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231030164200.png"alt="Alt text" /></p><p>我们就可以看到，刚开始的时候和最后的一次拟合的线的结果，还有中间一步步的拟合的变化。这条线在往上面一步一步的走。这样我们相当于是透视了它整个获得最优的k 和 b 的过程。</p><p>那这个时候我们来看一下，咱们怎么怎么预测呢？我们可以拿我们的<code>best_k</code>和<code>best_b</code>去输出最后的预测值了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">model(<span class="hljs-number">7</span>, best_k, best_b)<br><br>---<br><span class="hljs-number">28.718752244698216</span><br></code></pre></td></tr></table></figure><p>预测出来是 28.7 万。那房间数目为 7 的时候，我们预测出这个价格是 28.7万，还记得咱们上节课中用 KNN 预测出来的值么？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">find_price_by_simila(rm_to_price, <span class="hljs-number">7</span>)<br><br>---<br><span class="hljs-number">29.233333333333334</span><br></code></pre></td></tr></table></figure><p>是 29万对吧？现在我们就能看到了，这两种方式预测值基本很接近，都能预测。</p><p>那么我们使用函数来进行预测的原因还有一个，就是我们在使用函数在进行学习之后，然后拿模型去计算最后的值，这个计算过程速度会快很多。</p><p>好，咱们下节课将会学习怎样拟合更加复杂的函数，因为这个世界上的函数可不仅仅是最简单线性，还得拟合更加复杂的函数。</p><p>然后再后面的课程，我们会讲到激活函数，开始接触神经网络，什么是深度学习。</p><p>然后我们要来讲解一个很重要的概念，就是反向传播，会讲怎么样实现自动的反向传播。实现了自动的反向传播，我们会基于拓普排序的方法让计算机能够自动的计算它的梯度和偏导。</p><p>在讲完这些之后，基本上我们就有了构建一个深度学习神经网络框架的内容了。</p><p>好，希望小伙伴们在今天的课程中有所收获。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231030164335.png&quot;
alt=&quot;茶桁的 AI 秘籍 核心基础 19&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;上一节课，我们从一个波士顿房价的预测开始写代码，写到了 KNN。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>18. 深度学习 - 从零理解神经网络</title>
    <link href="https://hivan.me/18.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E4%BB%8E%E9%9B%B6%E7%90%86%E8%A7%A3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://hivan.me/18.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E4%BB%8E%E9%9B%B6%E7%90%86%E8%A7%A3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2023-11-07T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:36.061Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232439.png"alt="Alt text" /></p><p>Hi, 你好。我是茶桁。</p><p>我们终于又开启新的篇章了，从今天这节课开始，我们会花几节课来理解一下深度学习的相关知识，了解神经网络，多层神经网络相关知识。并且，我们会尝试着来打造一个自己的深度学习框架。</p><span id="more"></span><p>以前很多时候都会被人问到很多问题，其中比较多的就包括现在各种各样的框架应该用到哪一个，在学习人工智能的时候，对于深度学习框架有比较多的问题。那在这里我就希望能帮助各位小伙伴彻底的去理解一下什么是学习框架。</p><p>对于我们来说，就像小孩子去学一个东西，最好的就是从头到尾能把它拆了，然后再重建起来。</p><p>从今天开始往后的几节课里，我们都会去好好了解「如何从零构建一个深度学习框架」。</p><h2 id="本文目标">本文目标</h2><p>我们基本的核心目的就是来讲明白，什么是神经网络，以及神经网络的原理是什么。</p><p>我们要知道，人工智能有很多方法，但是神经网络是现代人工智能里面一个非常核心的内容。</p><p>咱们现在就是要先去了解神经网络的原理是怎么回事，然后在这个过程中我们来讲解清楚神经网络的框架到底是什么样的。</p><p>如我们之前学习过的几节机器学习课程，会发现它有很多的概念。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232440.png"alt="Alt text" /></p><p>比方说非监督学习、监督学习、强化学习，监督学习里面又分了回归和分类等等。</p><p>很多人看到这些，在初次接触、初次学习的时候就觉得人工智能很复杂，很难学会。除此之外，我们在学到人工智能目前比较核心的一个内容是关于深度学习神经网络。好多人不知道深度学习神经网络到底是什么原理。</p><p>在整个学习过程会发现有很多很多的问题，概念很多，变体也很多，学习很困难。</p><p>那这里要跟大家强调一点，就是千万别成为「马保国」，为什么这里会提到这个人呢？在我看来，这其实是一类人，他是一类人的代表。就是整很多的概念，假装子集很厉害。</p><p>就是我们脑子里不要总是去提很多概念，或者说很多很花哨的东西，最重要的还是基本功修炼好。我一直都强调一个观念，就是基础学科，基本功才是所有学科的基石。过多的概念其实并没有什么卵用。</p><p>早些时候，我上班的地方有一个叫「李雨晨」（匿名🙄）的产品经理，各种概念信手拈来，都是一些高大上的东西。也是将面试官唬的一愣一愣的。当时大家也是没多想，心想人家既然是个牛逼人物，那就多配合人家呗，结果是没过3 个月就原形毕露，当然是下面干事的人最先觉察出来的。</p><p>没办法，为了继续装下去只能是利用自己的职权和谎言去盗用别人的成果，比如设计稿啊，文档啊啥的，拿着当自己的东西向上汇报。</p><p>再然后，基本人人都开始防着他了，就开始恼羞成怒，一直打压那个最开始说他不行并防着他的产品。不过不行就是不行，其实最开始就能看出端倪，因为基本没有一家公司干活超过6个月，那肯定是有问题的。就这资质也能忽悠成高级产品经理，也能看出来那会儿产品这个行业的水份多大，门槛多低。不过终归潮水退了之后，裸泳的王八都要现行是吧。</p><p>好，说这么多吐槽的话其实也是想说一个道理，不要去搞花里胡哨的玩意，踏踏实实的把基本功练扎实，否则一时唬的了人，但是终归是走不远。</p><p>那这也是咱们这节课的目的，让大家去除掉背后这些繁杂的表象，那么它背后到底是什么，这就是咱们这三天的目的。</p><p>这些年，人工智能已经应用到我们各个地方了。先不说现在大火的AIGC，人工智能还应用到其他各个地方。</p><p>比方说在商场购物的时候，它的楼宇灯光，自动停车都是在做这些事情。买票的时候，机场，火车站都有人脸识别。每天给你推荐的各种商品，以及我们做物流配送等等这些东西，背后都有人工智能。</p><p>而这些人工智能背后有一个很重要的东西，就是用到了神经网络框架。</p><p>比方说众所周知的 TensorFlow,我们每次调用的时候，框架背后调用了很多东西。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Store layers weight &amp; bias</span><br><span class="hljs-comment"># A random value generator to initialize weights.</span><br>random_normal = tf.initializers.RandomNormal()<br><br>weights = &#123;<br>    <span class="hljs-string">&#x27;h1&#x27;</span>: tf.Variable(random_normal([num_features, n_hidden_1])),<br>    <span class="hljs-string">&#x27;h2&#x27;</span>: tf.Variable(random_normal([n_hidden_1, n_hidden_2])),<br>    <span class="hljs-string">&#x27;out&#x27;</span>: tf.Variable(random_normal([n_hidden_2, num_classes]))<br>&#125;<br><br>biases = &#123;<br>    <span class="hljs-string">&#x27;b1&#x27;</span>: tf.Variable(tf.zeros([n_hidden_1])),<br>    <span class="hljs-string">&#x27;b2&#x27;</span>: tf.Variable(tf.zeros([n_hidden_2])),<br>    <span class="hljs-string">&#x27;out&#x27;</span>: tf.Variable(tf.zeros([num_classes]))<br>&#125;<br>...<br><br><span class="hljs-comment"># Create model.</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">neural_net</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-comment"># Hidden fully connected layer with 128 neurons.</span><br>    layer_1 = tf.add(tf.matmul(x, weights[<span class="hljs-string">&#x27;h1&#x27;</span>]), biases[<span class="hljs-string">&#x27;b1&#x27;</span>])<br>    <span class="hljs-comment"># Apply sigmoid to layer_1 output for non_linerity.</span><br>    layer_1 = tf.nn.sigmoid(layer_1)<br><br>    <span class="hljs-comment"># Hidden fully connected layer with 256 neurons.</span><br>    layer_2 = tf.add(tf.matmul(layer_1, weights[<span class="hljs-string">&#x27;h2&#x27;</span>]), biases[<span class="hljs-string">&#x27;b2&#x27;</span>])<br>    <span class="hljs-comment"># Apply sigmoid to layer_1 output for non_linerity.</span><br>    layer_2 = tf.nn.sigmoid(layer_2)<br><br>    <span class="hljs-comment"># Output fully connected layer with a neuron for each class.</span><br>    out_layer = tf.matmul(layer_2, weights[<span class="hljs-string">&#x27;out&#x27;</span>]) + biases[<span class="hljs-string">&#x27;out&#x27;</span>]<br>    <span class="hljs-comment"># Apply softmax to normalize the logits to a probability distribution</span><br>    <span class="hljs-keyword">return</span> tf.nn.softmax(out_layer)<br></code></pre></td></tr></table></figure><p>我们现在想把这些框架搞清楚，就需要知道它背后这些东西到底是什么原理、什么原因。</p><p>那这几节课之后，就希望我们能从 0 到 1学会创建一个深度学习框架，从底层来理解这个神经网络的原理，理解现代人工智能的核心。</p><p>一开始的课程，内容也会稍微比较简单一些，越往后咱们就越难一点。最后，彻底理解深度学习神经网络原理。</p><h2 id="预测趋势与关系">预测趋势与关系</h2><p>我们以一个趋势预测的问题为引入。</p><p>如果对于自然哲学或者说科学研究这些，就是对科学研究方法论感兴趣的话，你会知道我们整个科学研究其实分为三个层面。</p><p>不管是牛顿、爱因斯坦，还是伽利略、图灵等等，所有的科学研究，所有的research，不管是关于数据还是别的，它都是三个层面。</p><p>第一个层面叫做描述性的，第二个叫做因果推理，第三个叫做未来的预测。</p><p>就说我们所有的科学活动，所有的研究活动都可以归为这三类。</p><p>描述性的东西，比方说你又长胖了多少，然后又增加了多少重量。今天的体重，明天的体重等等。</p><p>除此之外第二个层面是我们要看出来它们之间的相关性。比方吃的多和你长胖，它们之间是呈正相关的。还有其他的一些关系，比方说是呈负相关的等等。</p><p>那我们最重要也是最难的一个科学活动是要对它进行未来的预测，对于未来的预测。这个未来它不仅是predict。</p><p>比方说现在你知道的是几组数据，知道每个对应的结果。然后你看到了一组没有见过的数据，你去预测它。</p><p>就好比一个孩子做题，他见过的题都能做，没见过的题他也要会做。这个其实就是属于对未来的一种预测能力。</p><p>关于预测，我们最关心的预测是关于我们的身体健康，能活多久；还有就是关于挣钱的问题。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232441.png"alt="Alt text" /></p><p>我们看一下这个例子，你的性别和你的吸烟的频率，跟一种疾病（可能是肺癌），它会有一个相对应的概率。</p><p>性别不同，年龄不同，抽烟频率不同。我们会发现，得病概率随着年龄的增大并不会有多少增加，此时男性得病概率反而比女性还小。</p><p>但是随着抽烟频率越多，得病概率上升的非常快。其中呢，同样的年龄和抽烟频率下，男性得病的概率则会更高。</p><p>假设存在一个人 p，男性，年龄是 72 岁，他每天抽三根：P{age:72, sex:male, rate:3/day}。那他得这种病的概率大约是多少？那我们就先在图上随意画一个，假如说就如图的位置一样的概率：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232442.png"alt="Alt text" /></p><p>那么这个概率到底是多少？我们就需要用到数据去做预测，此时我们就得去做个拟合。</p><p>除此之外，我们再来看BMI，也就是身体指数。身体指数就是体重除以身高的平方：BMI =kg/h^2，越大就表示你越胖。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232443.png"alt="Alt text" /></p><p>当你到某一个值的时候，可以看到得病的概率。</p><p>我们假设有一个人 180 斤，身高一米73，我们来预测他得肾病的概率是多少。这个时候我们还是需要去做预测。</p><h2 id="波士顿房价预测">波士顿房价预测</h2><p>现在就来看一个非常经典的预测案例：波士顿房价案例。这个波士顿房价的数据，我们曾经在机器学习的线性回归里有用到，不知道小伙伴们有没有去看过。</p><p>波士顿地区是在美国东北部，房地产的价钱也比较稳定，那这个数据也是比较老的数据了，通过这些数据来考察，希望机器能够根据输入的内容来预测它的房价。</p><p>现在就以波士顿房价问题为例，来讲讲计算机怎么去预测。然后在预测的过程中我们来讲解实现深度学习的原理。最终把它封装成我们所需要的一个深度学习框架。</p><p>第一步自然是加载和分析数据。</p><p>之前的课程我提到过，这个数据由于一些原因，sklearn 的 datasets中已经删除了，那我们要想加载数据，就需要用到其中的 fetch_openml：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_openml<br><br>dataset = fetch_openml(name=<span class="hljs-string">&#x27;boston&#x27;</span>, version=<span class="hljs-number">1</span>, as_frame=<span class="hljs-literal">True</span>, return_X_y=<span class="hljs-literal">False</span>, parser=<span class="hljs-string">&#x27;pandas&#x27;</span>)<br></code></pre></td></tr></table></figure><p>在我们第一次获取到这个数据不知道怎么处理的时候，我们可以使用 dir来看看这个数据里面的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">dir</span>(dataset)<br><br>---<br>[<span class="hljs-string">&#x27;DESCR&#x27;</span>, <span class="hljs-string">&#x27;categories&#x27;</span>, <span class="hljs-string">&#x27;data&#x27;</span>, <span class="hljs-string">&#x27;details&#x27;</span>, <span class="hljs-string">&#x27;feature_names&#x27;</span>, <span class="hljs-string">&#x27;frame&#x27;</span>, <span class="hljs-string">&#x27;target&#x27;</span>, <span class="hljs-string">&#x27;target_names&#x27;</span>, <span class="hljs-string">&#x27;url&#x27;</span>]<br></code></pre></td></tr></table></figure><p>我们看到这个 dataset里有一个<code>feature_names</code>，直觉上这个应该是一些特征名称，我们来查看一下这个的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset[<span class="hljs-string">&#x27;feature_names&#x27;</span>]<br><br>---<br>[<span class="hljs-string">&#x27;CRIM&#x27;</span>, <span class="hljs-string">&#x27;ZN&#x27;</span>, <span class="hljs-string">&#x27;INDUS&#x27;</span>, <span class="hljs-string">&#x27;CHAS&#x27;</span>, <span class="hljs-string">&#x27;NOX&#x27;</span>, <span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;AGE&#x27;</span>, <span class="hljs-string">&#x27;DIS&#x27;</span>, <span class="hljs-string">&#x27;RAD&#x27;</span>, <span class="hljs-string">&#x27;TAX&#x27;</span>, <span class="hljs-string">&#x27;PTRATIO&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>]<br></code></pre></td></tr></table></figure><p>这里要说明一下，因为我是用的Jupyter，所以我可以这样直接打印出变量的具体内容，如果小伙伴们不是在Jupyter 里，而是在 Python文件中去编写代码，不要忘了使用<code>print</code>函数。</p><p>在拿到数据之后，我们先来定义一下问题。就是假设你现在要买一个房子，那么你就要根据他的这个房子的相关数据，来判断这个房子到底应该能卖到多少钱。所以我们的任务就是给定一组房屋的数据，然后要能够预测售价是多少。</p><p>定义完问题之后，我们来分析一下数据。</p><p>首先，要做数据，我们会先把它装载到一个表格里边。这里，我们使用Pandas。</p><p>Pandas 在 Python基础课里我有详细的讲过，它是做数据科学非常常用的一个东西。不要把它认为是熊猫啊，它是panel data set 的缩写，就是「面板数据集」，可以理解为一个Excel。可是它比 Excel 更方便编程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br>data = dataset[<span class="hljs-string">&#x27;data&#x27;</span>]<br>dataframe = pd.DataFrame(data)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataframe))<br>dataframe.head(<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><blockquote><p>为了节省篇幅，打印结果我就不贴出来了。</p></blockquote><p>有的小伙伴在处理这里<code>data</code>的时候，会发现头部没有特征名，会呈现1，2，3，4这样的数字。我们就需要将名称给它加上，之前我们说过，feature_name是特征名，于是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dataframe.columns = dataset[<span class="hljs-string">&#x27;feature_names]</span><br></code></pre></td></tr></table></figure><p>这个时候我们就能看出来每一个特征到底是什么。不过这组数据里因为只是特征数据，并没有相关的价格。价格原本是目标数据，也就是最初始数据里的<code>target</code>，所以我们这里给这组特征数据里加上一列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dataframe[<span class="hljs-string">&#x27;price&#x27;</span>] = dataset[<span class="hljs-string">&#x27;target&#x27;</span>]<br></code></pre></td></tr></table></figure><p>然后我们要想看看到底什么因素对房价的影响是最大的。「What's the mostsignificant（salient）feature of the house price」。</p><p>对于决定一个东西最重要的特征我们就叫做 significant，或者silence，显著特征。</p><p>在 pandas里边有一个很简单的东西，<code>correlation</code>。correlation就是两组变量的相关性。</p><p>关于特征相关性，我们在机器学习里面有详细的讲过，这里我们就粗略带过就行了，在使用<code>corr()</code>找到特征之间的相关性数据之后，可以使用seaborn 来将热图可视化出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br>sns.heatmap(dataframe.corr())<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232444.png"alt="Alt text" /></p><p>这里我们着重来看和价格相关的特征，除了它本身之外，正相关性最大的就是RM，负相关性最大的是 LSTAT。</p><p>我们来看一下这两个特征的说明：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(dataset[<span class="hljs-string">&#x27;DESCR&#x27;</span>])<br><br>---<br>...<br>RM       average number of rooms per dwelling<br>LSTAT    % lower status of the population<br>...<br></code></pre></td></tr></table></figure><p>RM是一套住宅的房间数量，一个是低收入人群的人口比例。也就是说，房间越多的房子越贵，小区内低收入人群的比例越低，小区内的房子越贵。那小区内低收入人群的比例居然比犯罪率的影响还要大一些，似乎有点让人难以接受，但是这个确实是事实。</p><p>基于以上分析，我们需要把房屋里边卧室的个数和房屋价格最成正相关。</p><p>把问题简单化：如何依据房屋里边卧室的数量来估计房子的面积？</p><p>在一九七几年的时候啊，当时有过这样一种想法，首先，我们将所有的 RM数据存下来，还有目标数据，也就是 price 也存下来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X_rm = dataframe[<span class="hljs-string">&#x27;RM&#x27;</span>].values<br>y = dataframe[<span class="hljs-string">&#x27;price&#x27;</span>]<br></code></pre></td></tr></table></figure><p>存下来之后我们把做一个字典映射：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">rm_to_price = &#123;r: y <span class="hljs-keyword">for</span> r, y <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(X_rm, y)&#125;<br><br>---<br>&#123;<span class="hljs-number">6.575</span>: <span class="hljs-number">24.0</span>,<br> <span class="hljs-number">6.421</span>: <span class="hljs-number">21.6</span>,<br> ...<br> <span class="hljs-number">6.976</span>: <span class="hljs-number">23.9</span>&#125;<br></code></pre></td></tr></table></figure><p>这样之后，问题也就相应的做了一个简化。假如有人在销售那里要求买房子，那销售就可以拿出一个字典，里面都是这样的对应关系，然后我们就可以去查一下就知道了。</p><p>这个时候假如有人告诉你有一个小区，他平均里边房屋平均是6.421。那一查就发现这个 6.421 的基本上卖 21 万。那如果小区里房屋数量是5.57 的时候我卖多少钱？卖 13 万。这都是一一对应的关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">rm_to_price[<span class="hljs-number">6.421</span>]<br><br>---<br><span class="hljs-number">21.6</span><br></code></pre></td></tr></table></figure><p>不过这个时候有一个人说我们那个小区里面平均是 7个房间，那是多少呢？我们发现，我们的字典里没有超过 7的数字，也就是没有这么一个对应关系。</p><p>那么找不到的时候怎么办呢？我们大部分时候解决问题都会找一个近似值，也就是最接近的数据来做参考。也可以根据以前的数据来做计算，其实也就是一句话的事：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">find_price_by_simila</span>(<span class="hljs-params">history_price, query_x, topn=<span class="hljs-number">3</span></span>):<br>    <span class="hljs-keyword">return</span> np.mean([p <span class="hljs-keyword">for</span> x, p <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(history_price.items(), key=<span class="hljs-keyword">lambda</span> x_y: (x_y[<span class="hljs-number">0</span>] - query_x) **<span class="hljs-number">2</span>)[:topn]])<br><br></code></pre></td></tr></table></figure><p>要根据以前的数据来做计算的话，我们定义了一个方法，传入了参数历史价格以及查询特征。然后我们返回的内容稍微有点复杂，首先给这个房屋进行排序，排序依据是按照x 和 query之间的距离来给他排序。排序的时候我们取最接近的这几个数字，这样就能够得到最接近的x 和 y。然后在 x 和 y 里面我们取它的 price，这就是最接近的 price。</p><p>然后我们来看看它给咱们算的如果房间数是 7 的情况是什么价格：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">find_price_by_simila(rm_to_price, <span class="hljs-number">7</span>)<br><br>---<br><span class="hljs-number">29.233333333333334</span><br></code></pre></td></tr></table></figure><p>关于排序那里看不懂的小伙伴，我们这里额外花点篇幅开个小灶。这样，假如说我们有下面一组数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">person_and_age = &#123;<br>    <span class="hljs-string">&#x27;A 张学友&#x27;</span>: <span class="hljs-number">62</span>,<br>    <span class="hljs-string">&#x27;C 周杰伦&#x27;</span>: <span class="hljs-number">44</span>,<br>    <span class="hljs-string">&#x27;B 毛不易&#x27;</span>: <span class="hljs-number">29</span><br>&#125;<br></code></pre></td></tr></table></figure><p>然后我们将这组数据改成列表并进行排序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">l = <span class="hljs-built_in">list</span>(person_and_age.items())<br><span class="hljs-built_in">sorted</span>(l)<br><br>---<br>[(<span class="hljs-string">&#x27;A 张学友&#x27;</span>, <span class="hljs-number">62</span>), (<span class="hljs-string">&#x27;B 毛不易&#x27;</span>, <span class="hljs-number">29</span>), (<span class="hljs-string">&#x27;C 周杰伦&#x27;</span>, <span class="hljs-number">44</span>)]<br></code></pre></td></tr></table></figure><p>我们可以看到它是按照数据的首字母进行排序的，可是这个时候我们不想以首字母来排序，而是想根据年龄大小进行排序该怎么办？这个时候我们就可以给排序方法的key 里面定规则，这个规则就是按照元素的第二个下标进行排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_first_items</span>(<span class="hljs-params">element</span>):<br>    <span class="hljs-keyword">return</span> element[<span class="hljs-number">1</span>]<br><br><span class="hljs-built_in">sorted</span>(l, key=get_first_items)<br><br>---<br>[(<span class="hljs-string">&#x27;B 毛不易&#x27;</span>, <span class="hljs-number">29</span>), (<span class="hljs-string">&#x27;C 周杰伦&#x27;</span>, <span class="hljs-number">44</span>), (<span class="hljs-string">&#x27;A 张学友&#x27;</span>, <span class="hljs-number">62</span>)]<br></code></pre></td></tr></table></figure><p>我们这里定义了一个函数<code>get_first_items</code>,其实做了一件很简单的事情，就是获得了<code>element</code>的第二个下标。</p><p>那么这里我们其实可以不用这样定义函数，而是直接用匿名函数。关于匿名函数，我在Python 基础课里也有详细的讲到，大家可以回头去翻看一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">sorted</span>(l, key=<span class="hljs-keyword">lambda</span> element: element[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>那其实，element是一个输入参数，是一个变量，所以我们完全可以就简写一下就行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">sorted</span>(l, key=<span class="hljs-keyword">lambda</span> e: e[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>然后我们再在后面多加一个切片操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">sorted</span>(l, key=<span class="hljs-keyword">lambda</span> e: e[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)[:<span class="hljs-number">2</span>]<br><br>---<br>[(<span class="hljs-string">&#x27;A 张学友&#x27;</span>, <span class="hljs-number">62</span>), (<span class="hljs-string">&#x27;C 周杰伦&#x27;</span>, <span class="hljs-number">44</span>)]<br></code></pre></td></tr></table></figure><p>不用在意那个<code>reverse=True</code>,只是打开了反向排序，因为个人情感上不想去掉<code>张学友</code>。</p><p>好，那这个时候呢我们在前面加一个<code>for</code>，就可以拿到名字和age，而我们只需要 age:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">[age <span class="hljs-keyword">for</span> name, age <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(l, key=<span class="hljs-keyword">lambda</span> e: e[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)[:<span class="hljs-number">2</span>]]<br><br>---<br>[<span class="hljs-number">62</span>, <span class="hljs-number">44</span>]<br></code></pre></td></tr></table></figure><p>这样我们就可以只取两个排序最靠前的年龄值，当然最后，就是<code>mean</code>，取平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">np.mean([age <span class="hljs-keyword">for</span> name, age <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(l, key=<span class="hljs-keyword">lambda</span> e: e[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)[:<span class="hljs-number">2</span>]])<br><br>---<br><span class="hljs-number">53.0</span><br></code></pre></td></tr></table></figure><p>那我们之前所写的函数内容就是这样一段话，拆解之后是不是就能明白了？</p><p>那么刚才讲到的这种方法，你会发现它是在找相似的东西，其实我们定义的这种方法，后来给它起个名字叫做：发现K 个最相近的邻居，<code>K-Neighbor-Nearest</code>,简称<code>KNN</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">knn</span>(<span class="hljs-params">history_price, query_x, topn=<span class="hljs-number">3</span></span>):<br>    <span class="hljs-keyword">return</span> np.mean([p <span class="hljs-keyword">for</span> x, p <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(history_price.items(), key=<span class="hljs-keyword">lambda</span> x_y: (x_y[<span class="hljs-number">0</span>] - query_x) **<span class="hljs-number">2</span>)[:topn]])<br></code></pre></td></tr></table></figure><p>这种算法之前机器学习的章节里咱们也详细讲过，这是一个非常经典的机器学习算法。关于KNN的有优点和缺点，我们之前也讲的很详细。那大家可以回过头取看我关于机器学习KNN 的部分来学习，这里就不再继续赘述 KNN的内容了，在这里，我们就了解之前我们所做的这么多内容，其实就是KNN，就可以了。</p><p>好，那这节课的内容就到这里，下一节课，咱们会继续写这一篇未完成的代码，来找到X_rm 和 y之间的函数关系。那么代码文件就依然还是<code>18.ipynb</code>。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232439.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi, 你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;我们终于又开启新的篇章了，从今天这节课开始，我们会花几节课来理解一下深度学习的相关知识，了解神经网络，多层神经网络相关知识。并且，我们会尝试着来打造一个自己的深度学习框架。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>17. 机器学习 - 随机森林</title>
    <link href="https://hivan.me/17.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <id>https://hivan.me/17.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</id>
    <published>2023-11-04T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:39.823Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224427.png"alt="茶桁的 AI 秘籍 核心基础 17" /></p><p>Hi，你好。我是茶桁。</p><p>我们之前那一节课讲了决策树，说了决策树的优点，也说了其缺点。</p><span id="more"></span><p>决策树实现起来比较简单，解释解释性也比较强。但是它唯一的问题就是不能拟合比较复杂的关系。</p><p>后来人们为了解决这个问题，让其能够拟合更加复杂的情况，提出来了一种模型，这种模型就叫做随机森林。</p><h2 id="随机森林">随机森林</h2><p>随机森林之所以叫随机森林，是因为它是由多棵树组成。它结合了决策树和随机性的概念，用于解决分类和回归问题，随机森林由多个决策树组成，每棵树都是随机构建的。</p><p>随机森林其核心组成部分是决策树，为了提高模型的性能和泛化能力，所以引入了两种主要形式的随机性。</p><p>第一种就是随机选择样本，对于每棵决策树的构建，随机森林从训练数据中随机抽取一部分样本（有放回地抽样），这称为自助采样（BootstrapSampling）。这就使得每棵树都在不同的样本子集上进行训练，增加了模型的多样性。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224333.png"alt="Alt text" /></p><p>第二种是随机选择特征，在每个节点上，随机森林只考虑特征的一个子集来进行分割决策，而不是考虑所有特征。这确保了每棵树的分裂过程是不同的，增加了多样性。</p><p>对于分类问题，随机森林中的每棵决策树都会对输入数据进行分类，那对于回归问题，就会变成是每棵决策树都会对输入数据进行预测了。最后的预测结果是通过对所有树的投票或平均值来获得的。这种集成方法可以减小过拟合奉先，提高模型的稳定性和泛化能力。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224334.png"alt="Alt text" /></p><ol type="1"><li>使用随机森林来预测。</li><li>在预测之前呢，我们使用 Out-Of-Bagging 样本来评估我们的模型。这个bagging 就是袋子，就是我们从袋子里随机取东西去衡量。</li><li>使用评估结果，我们可以选择合适的变量数。</li></ol><p>随机森林的原理其实很简单，是一个非常简单但是非常好用的一个方法。基本上，除了深度学习之外，也是企业用的最多的方法之一。咱们在这里就来演示一下随机森林的作用以及效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br>iris = load_iris()<br><br>x = iris.data<br>y = iris.target<br><span class="hljs-built_in">print</span>(x, y)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224335.png"alt="Alt text" /></p><p>这个是我们用 sklearn里面鸢尾花分类的数据做个简单例子，快速的展现一下它的效果。我们将数据拿到以后，x是鸢尾花的四个维度，四个维度对应了它的一个类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier<br><br>tree_clf = DecisionTreeClassifier()<br>tree_clf.fit(x, y)<br><br>tree_clf.feature_importances_<br><br>---<br>array([<span class="hljs-number">0.02666667</span>, <span class="hljs-number">0.</span>        , <span class="hljs-number">0.05072262</span>, <span class="hljs-number">0.92261071</span>])<br></code></pre></td></tr></table></figure><p>我们 fit 完之后就可以看到，这四个 feature 中，最终要的是第四个feature。然后是第三个，第二个根本就没用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><br>train_x, test_x,train_y, test_y = train_test_split(x, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)<br><br>tree_clf = DecisionTreeClassifier()<br>tree_clf.fit(train_x, train_y)<br><br><span class="hljs-built_in">print</span>(tree_clf.score(train_x, train_y))<br><span class="hljs-built_in">print</span>(tree_clf.score(test_x, test_y))<br><br>---<br><span class="hljs-number">1.0</span><br><span class="hljs-number">0.9777777777777777</span><br></code></pre></td></tr></table></figure><p>看结果我们其实可以看到，这个拟合度有点太高了。我们换个数据再来看，还是之前的课程中我们用到的Boston房价的数据，不过因为这个是一个回归问题，所以我们需要用回归预测的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeRegressor<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_openml<br>dataset = fetch_openml(name=<span class="hljs-string">&#x27;boston&#x27;</span>, version=<span class="hljs-number">1</span>, as_frame=<span class="hljs-literal">True</span>, return_X_y=<span class="hljs-literal">False</span>, parser=<span class="hljs-string">&#x27;pandas&#x27;</span>)<br><br>data = dataset[<span class="hljs-string">&#x27;data&#x27;</span>]<br>target = dataset[<span class="hljs-string">&#x27;target&#x27;</span>]<br><br>x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=<span class="hljs-number">0.2</span>)<br><br>tree_reg = DecisionTreeRegressor()<br>tree_reg.fit(x_train, y_train)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;whole dataset train acc: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(tree_reg.score(x_train, y_train)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;whole dataset test acc: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(tree_reg.score(x_test, y_test)))<br><br>---<br>whole dataset train acc: <span class="hljs-number">1.0</span><br>whole dataset test acc: <span class="hljs-number">0.6606392933985246</span><br></code></pre></td></tr></table></figure><p>现在我们来看，它的 train 上的 score 准确度是 1.0，在 test 上是0.81，这个是全数据量测试的情况。</p><p>然后我们来定义一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_forest</span>(<span class="hljs-params">x_train, y_train, x_test, y_test, drop_n=<span class="hljs-number">4</span></span>):<br>    <br>    features_random = np.random.choice(<span class="hljs-built_in">list</span>(x_train.columns), size=<span class="hljs-built_in">len</span>(x_train.columns)-drop_n)<br><br>    x_sample = x_train[features_random]<br>    y_sample = y_train<br><br>    reg = DecisionTreeRegressor()<br>    reg.fit(x_sample, y_sample)<br><br>    score_train = reg.score(x_sample, y_sample)<br>    score_test = reg.score(x_test[features_random], y_test)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;sub sample :: train score: &#123;&#125;, test score: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(score_train, score_test))<br><br>    y_predicated = reg.predict(x_test[features_random])<br><br>    <span class="hljs-keyword">return</span> y_predicated<br></code></pre></td></tr></table></figure><p>咱们随机的从<code>data</code>里面取一些数据，之后我们来看一下单个树的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">with_feature_names = pd.DataFrame(data)<br>with_feature_names.columns = dataset[<span class="hljs-string">&#x27;feature_names&#x27;</span>]<br><br>x_train, x_test, y_train, y_test = train_test_split(with_feature_names, target, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)<br><br>random_forest(x_train, y_train, x_test, y_test, <span class="hljs-number">4</span>)<br><br>---<br>sub sample :: train score: <span class="hljs-number">1.0</span>, test score: <span class="hljs-number">0.5171643497313849</span><br></code></pre></td></tr></table></figure><p>单个的结果显然是要比整个的数据量要差。那么咱们现在看一下最终的结果，把它变成一个森林：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">tree_num = <span class="hljs-number">4</span><br>predicates = []<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tree_num):<br>    predicated, score = random_forest(x_train, y_train, x_test, y_test)<br>    predicates.append((predicated))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the mean result is: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(np.mean(predicates), axis=<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the score of forest is: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(r2_score(y_test, np.mean(predicates, axis=<span class="hljs-number">0</span>))))<br><br>---<br>the mean result <span class="hljs-keyword">is</span>: <span class="hljs-number">21.614144736842107</span><br>the score of forest <span class="hljs-keyword">is</span>: <span class="hljs-number">0.7194989474162439</span><br></code></pre></td></tr></table></figure><p>从一开始到现在完整的打印结果为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">whole</span> dataset train acc: <span class="hljs-number">1</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">whole</span> dataset test acc: <span class="hljs-number">0</span>.<span class="hljs-number">6606392933985246</span><br><br><span class="hljs-attribute">ssub</span> sample :: train score: <span class="hljs-number">1</span>.<span class="hljs-number">0</span>, test score: <span class="hljs-number">0</span>.<span class="hljs-number">5885292814825753</span><br><span class="hljs-attribute">sub</span> sample :: train score: <span class="hljs-number">1</span>.<span class="hljs-number">0</span>, test score: <span class="hljs-number">0</span>.<span class="hljs-number">559086368163823</span><br><span class="hljs-attribute">sub</span> sample :: train score: <span class="hljs-number">1</span>.<span class="hljs-number">0</span>, test score: <span class="hljs-number">0</span>.<span class="hljs-number">6119989116140754</span><br><span class="hljs-attribute">sub</span> sample :: train score: <span class="hljs-number">1</span>.<span class="hljs-number">0</span>, test score: <span class="hljs-number">0</span>.<span class="hljs-number">21831688326567122</span><br><br><span class="hljs-attribute">the</span> mean result is: <span class="hljs-number">21</span>.<span class="hljs-number">614144736842107</span><br><span class="hljs-attribute">the</span> score of forest is: <span class="hljs-number">0</span>.<span class="hljs-number">7194989474162439</span><br></code></pre></td></tr></table></figure><p>这是个很典型的例子，使用全量的数据集，它的结果最终的在 test 集上是0.66，然后基本上每个的结都比它要差一些。但当我们用了森林的值做了平均之后，这个值就变得更好了。</p><p>当然其实这个值并不是每次都是如此，在我们进行计算的时候，因为数据什么的都是随机的，偶尔也会出现取均值之后变的更差的情况。不过大部分时候，都会更好一些。</p><p>我们现在再将结果稍微改一改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_forest</span>(<span class="hljs-params">x_train, y_train, x_test, y_test, drop_n=<span class="hljs-number">4</span></span>):<br>    ...<br><br>    <span class="hljs-keyword">return</span> y_predicated, score_test<br><br>tree_num = <span class="hljs-number">4</span><br>predicates = []<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tree_num):<br>    predicated, score = random_forest(x_train, y_train, x_test, y_test)<br>    predicates.append((predicated, score))<br><br>predicates_value = [v <span class="hljs-keyword">for</span> v, s <span class="hljs-keyword">in</span> predicates]<br>forest_scores = [s <span class="hljs-keyword">for</span> v, s <span class="hljs-keyword">in</span> predicates]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the score of forest is: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(r2_score(y_test, np.mean(predicates_value, axis=<span class="hljs-number">0</span>))))<br><br>weights = np.array(forest_scores) / np.<span class="hljs-built_in">sum</span>(forest_scores)<br>score_weights = np.zeros_like(np.mean(predicates_value, axis=<span class="hljs-number">0</span>))<br><br><span class="hljs-keyword">for</span> i, v <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(predicates_value):<br>    score_weights += v * weights[i]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the score of weighted forest is: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(r2_score(y_test, score_weights)))<br><br>---<br>the score of forest <span class="hljs-keyword">is</span>: <span class="hljs-number">0.7049603534192553</span><br>the score of weighted forest <span class="hljs-keyword">is</span>: <span class="hljs-number">0.7204901503020483</span><br></code></pre></td></tr></table></figure><p>后面这段代码呢，其实就是人们发现用了随机森林之后，效果明显要好了，那一些人就想如果在知道每一次的<code>test_score</code>之后，能不能给<code>test_score</code>比较高的值加一个比较大的权重。</p><p>也就是说，当我知道<code>test_score</code>比较好，那在最后做决策的时候给它加的权重大一些。</p><p>最后我们打印了常规状态下森林的结果和加权之后的结果。加权之后的结果又变得好了一些。</p><h2 id="adaboost">Adaboost</h2><p>然后人们沿着这个思路，就做了一件事情，就是 Adaboost(AdaptiveBoosting)：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224336.png"alt="Alt text" /></p><p>Adaboost 就是在随机森林的权重思路上做了一个优化，它的示意图也是有多个weak classifier, 然后最后有一个 Weighted Voter,这是一个权重的投票，这个就和我们上面加权的那部分代码非常的类似。只不过它在这里做了个细化：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224337.png"alt="Alt text" /></p><p>我们来注意看最后一个公式：</p><p><span class="math display">\[H(x) = sign(\sum_{t=1}^T\alpha_th_t(x))\]</span></p><p>公式里的<spanclass="math inline">\(\alpha_t\)</span>就是它的权重，最终的 H(x)就是很多<span class="math inline">\(\alpha_t \cdoth_t(x)\)</span>加在一起的结果。这里的这个<spanclass="math inline">\(\alpha_t\)</span>就是每一次小的数的权重：<code>v * weights[i]</code>。这个权重就不是像咱们刚才代码里那样根据<code>score</code>的大小简单的做个加权。</p><p>我们看上图中间又一个<spanclass="math inline">\(\alpha_t\)</span>的公式：</p><p><span class="math display">\[\begin{align*}\alpha_t = \frac{1}{2}ln(\frac{1-\varepsilon_t}{\varepsilon_t})\end{align*}\]</span></p><p>然后我们再往上倒腾，<spanclass="math inline">\(\varepsilon_t\)</span>是当你预测出来这个值和实际值错的越多，越趋近于1。如果完全没有错，一个错都没有的情况下，那么<spanclass="math inline">\(\varepsilon_t=0\)</span>。</p><p><span class="math display">\[\begin{align*}\varepsilon_t = Pr_{i\sim D_t}[h_t(x_i)\ne y_i]\end{align*}\]</span></p><p>如果<span class="math inline">\(\varepsilon_t =1\)</span>的话，那就是<spanclass="math inline">\(\frac{1-\varepsilon_t}{\varepsilon_t}\)</span>就是：<code>1-1/1=0</code>。ln0等于什么呢？它等于负的无穷大，那么<spanclass="math inline">\(\alpha_t\)</span>等于就没有。如果<spanclass="math inline">\(\varepsilon_t = 0\)</span>，<spanclass="math inline">\(ln(\frac{1-\varepsilon_t}{\varepsilon_t})\)</span>就是无穷大。</p><p>也就是说，随着<spanclass="math inline">\(\varepsilon_t\)</span>越大，那<spanclass="math inline">\(\alpha_t\)</span>会越大，随着<spanclass="math inline">\(\varepsilon_t\)</span>越小，<spanclass="math inline">\(\alpha_t\)</span>也会越小。而且在这个地方是呈指数变化的，就是误差会对<spanclass="math inline">\(\alpha_t\)</span>的变化影响的很大。</p><p>除了用指数的东西来做，它还有一个很重要的特性，这个特性才在我们整个Adaboost 里非常重要：</p><p><span class="math display">\[D_{t+1}(i) = \frac{D_t(i)exp(-\alpha_ty_ih_t(x_i))}{Z_t}\]</span></p><p>我们先来看<spanclass="math inline">\(y_ih_t(x_i)\)</span>这部分，假设 ht(xi)=1，yi预测对了等于 1，yi 预测错了等于 -1。那如果预测错了，这整个部分都等于-1，如果预测对了，这里就是 1。</p><p>前面有一个负号： <spanclass="math inline">\(-\alpha_ty_ih_t(x_i)\)</span>，那肯定是要变号的。也就是说，如果预测错了，那么这一串东西应该是正的，如果预测对了这一串东西应该是负的。</p><p>前面是什么，是<span class="math inline">\(D_{t+1}(i)\)</span>,这里其实就是第 i 个训练元素在<spanclass="math inline">\(D_{t+1}\)</span>被取到的概率。那么我们最前面有表示<spanclass="math inline">\(D_1(i) =\frac{1}{m}\)</span>，也就是说，所有元素被取到的概率都是一样的，是平均的。那第二次的概率就是：<spanclass="math inline">\(D_1(i)\cdot exp(...)\)</span>, exp 就是 e的多少次方。</p><p>那我们现在知道，如果预测对了，这里是 -1，预测错了这里是 1,都要再乘以<spanclass="math inline">\(\alpha_t\)</span>。那么如果预测对了，这里是<spanclass="math inline">\(-\alpha_t\)</span>，那 exp 这里就是小于 1的。那如果预测错了呢，exp 就是大于 1 的。</p><p>如果 exp 大于 1，那么<spanclass="math inline">\(D_{t+1}(i)\)</span>概率就会被<spanclass="math inline">\(D_t(i)\)</span>的概率要更大，反之就会更小。</p><p>这个就是我们 Ada 的含义，Ada 就是Adaptive，就是动态调整的意思。也就是通过这种方法实现的。</p><p>如果此时此刻<spanclass="math inline">\(x_i\)</span>算对了，那下一次就更不容易被取到，如果算错了，那下一次训练就会更有可能被取到。</p><p>觉得绕的小伙伴去理解这样一个例子：如果你是个学生去做卷子，那么你作对的题还会反复去做吗？肯定是不会的题才会反复刷，刷到自己会为止。</p><h2 id="gradient-boosting">Gradient Boosting</h2><p>除了 Adaboost 之外，后来人们又提出来了一个新的方法：GradientBoosting。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224338.png"alt="Alt text" /></p><p>Gradient Boosting 和 Adaboost 的核心原理很像：</p><p><span class="math display">\[loss(p, q) = -\sum_{i\in output classes}p(x)logq(x)\]</span></p><p>Gradient Boosting主要用于解决回归和分类问题。它基于决策树（通常是浅层决策树）构建模型，通过迭代改进预测的准确性。</p><p>其最核心的就是梯度提升，是一种集成学习方法。将多个弱预测模型，也就是决策树组合在一起，以提高整体性能。每个决策树在不同的数据子集上训练，然后进行组合以生成最终的预测。其核心原理就是通过迭代优化损失函数来构建模型。</p><p>在每一步中，模型的更新方向就是损失函数的负梯度。假设我们有一个损失函数L(y, f(x))，其中 y 是真实标签，f(x)是当前模型的预测，梯度提升的目标是找到一个新的模型 h(x), 使得损失函数L(y, f(x) + h(x)) 最小化。</p><p>梯度提升使用负梯度方向的决策树 h(x)来拟合当前模型的残差，因此可以通过以下方式迭代更新模型：</p><p><span class="math display">\[f(x) = f(x) + learning\_rate \cdot h(x) \\\]</span></p><p>也就是说，它其实要变成这样一个式子：</p><p><span class="math display">\[Boosted Ensemble = First Tree + \eta \cdot Second Tree \\loss(Boosted Ensemble) &lt; loss(First Tree)\]</span></p><p>也就是说，我们第二波的 h_t(x)，也就是 h_2(x)，前面乘以一个<spanclass="math inline">\(\eta\)</span>，这个<spanclass="math inline">\(\eta\)</span>是等于 Learning Rate 的，然后 h_2(x)加上 h_1(x) 最后得到的结果，要比 h_1(x) 的 loss 值更小。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224339.png"alt="Alt text" /></p><p>那么我们现在要做的就是改变<spanclass="math inline">\(\eta\)</span>的权重，这个东西的权重就是和之前我们在随机森林里调整权重不同。</p><p>一开始，梯度提升初始化一个简单的模型，通常是一个常数，用来拟合目标变量的平均值。</p><p>对于每一个训练样本，计算模型的梯度。这表示模型对于每个样本的预测误差。</p><p>使用新的决策树来拟合梯度的负梯度，也就是模型的残差。这意味着构建一个决策树，其目标是减小之前模型的误差。</p><p>将新构建的决策树与之前的模型相加，以形成一个新的模型。这个过程重复进行多次，每次都会减小误差。</p><p>重复 2 到 4步，直到满足某个停止条件，理入达到最大迭代次数或误差足够小。</p><p>最终模型是所有决策树的组合，可以用来进行预测。</p><p>那我们之前谈到的<span class="math inline">\(\eta\)</span>，也就是learning_rate，其实就是学习率，用于控制每次更新的幅度。</p><p>数学上，梯度提升通过迭代不断减小损失函数来逼近最优模型，这是一种梯度下降的优化方法，因此它的核心原理与梯度下降算法是密切相关的。</p><p>Grading Boost 和 AdaBoost 的整个区别不大，它们都是属于 EnsembleLearning，中文翻译是合唱团。</p><p>这个 Ensemble Learning我们可以取很多个分类、回归，然后我们把它做好之后给它求一个平均值。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224340.png"alt="Alt text" /></p><p>比如这样，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> VotingClassifier<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br><br>log_clf = LogisticRegression()<br>rnd_clf = RandomForestClassifier()<br>svm_clf = SVC()<br><br>voting_clf = VotingClassifier(<br>    estimators = [(<span class="hljs-string">&#x27;lr&#x27;</span>, log_clf),(<span class="hljs-string">&#x27;rf&#x27;</span>, rnd_clf),(<span class="hljs-string">&#x27;svc&#x27;</span>, svm_clf)], voting=<span class="hljs-string">&#x27;hard&#x27;</span>)<br><br>voting_clf.fit(x_train, y_train)<br>...<br><br></code></pre></td></tr></table></figure><p>在 sklearn 的 ensemble 中本身就有一个 VotingClassifier，也有RandomForestClassifier，我们可以直接用几个分类器可以实现。</p><p>AdaBoost 和 Gradient Boost 也是属于一个典型的 ensemble Learning。</p><p>那还有两个比较重要的东西，一个叫做 Xgboost，一个叫做LightBGM，这两个是 Grading Boost的升级版。它们被广泛的使用于机器挖掘，推荐系统等等。</p><p>当然这两块内容就不放在「核心基础」里讲了，将会在后面讲到 BI专业课的时候专门的去讲，这两个是很重要的点。</p><p>那本节课讲完之后呢，咱们核心基础的部分，关于机器学习就跨过一个小阶段了。下一节课开始，我们要讲「深度学习」了。属于向前要跨一大步。</p><p>好，那咱们经典机器学习模型到今天就讲完了。各位看文章的小伙伴，自己去把这个课程再好好巩固一下，咱们下节课开始，就进入深度学习了。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224427.png&quot;
alt=&quot;茶桁的 AI 秘籍 核心基础 17&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;我们之前那一节课讲了决策树，说了决策树的优点，也说了其缺点。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>16. 机器学习 - 决策树</title>
    <link href="https://hivan.me/16.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://hivan.me/16.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2023-11-01T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:43.122Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231027192720.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><p>在上一节课讲 SVM之后，再给大家将一个新的分类模型「决策树」。我们直接开始正题。</p><span id="more"></span><h2 id="决策树">决策树</h2><p>我们从一个例子开始，来看下面这张图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231027192721.png"alt="Alt text" /></p><p>假设我们的 x1 ~ x4 是特征，y是最终的决定，打比方说是买东西和不买东西，0 为不买，1 为买东西，假设现在y 是<code>[0,0,1,0,1]</code>。</p><p>那么，我们应该以哪个特征为准去判断到底 y 是 0 还是 1 呢？</p><p>如果关注 x3，那么 x3 为 A 的时候，即有 0 也有1，我们先放一边找找看有没有更合适的。</p><p>如果是 x4 的话，肉眼可见的，区分度是最准确的对吧？B 的都是都是 0，C的时候都是 1，那么 x4 也就是区分度最大的。</p><p>我们现在换成人的思维过程来说，肯定是期望先找到那个最能区分它的，就是最能识别的特征。这个最能识别的特征在数学里面有一个专门的定义：Salientfeature, 就是显著特征。</p><p>如果我们更改一下 x4 的值，变成<code>[B,C,C,B,B]</code>，那 x4也就不那么显著了。这个时候最能区分的就是 x2了，只在<code>x2[1]</code>的位置上判断错了一个。</p><p>这个时候，我们就需要客观的衡量一下，什么叫做最能区分，或者说是最能分割，最显著的区别。这就需要一个专门的数值来计算。</p><p>那我们就来看看，到底怎么样来做区分。我们现在根据一个值，将我们的数据分成了两堆，那咱们的期望就是这两堆数据尽可能是一样的。比如极端情况下，一堆全是0，一堆全是1，当然，中间混进去一个不一样的也行，但是尽可能的「纯」：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231027192722.png"alt="Alt text" /></p><p>好，那我们该怎么定义这个「纯」呢？</p><p>我们大家应该都知道物理里有一个定义：「熵」，那熵在物理里是衡量物体的混乱程度的。</p><p>比如说一个组织、一个单位、公司或者个人，其内部的熵都是在呈现越来越混乱，而且熵的混乱具有不可逆性。这个呢，就是熵增定律，也叫「热力学第二定律」。是德国人克劳修斯提出的理论，最初用于揭示事物总是向无序的方向的发展、以及“孤立系统下热量从高温物体流向低温不可逆”的热力学定律（要不说看我的文章涨学问呢是吧）。</p><h2 id="信息熵">信息熵</h2><p>好，说回咱们的机器学习。后来有一个叫「香农」的人要衡量一堆新息的混乱程度，就起了一个名字「信息熵」，也就成了衡量信息的复杂程度。</p><p>那么信息熵怎么求呢？</p><p><span class="math display">\[\begin{align*}Entropy = \sum_{i=1}^n-p(c_i)log_2(p(c_i))\end{align*}\]</span></p><p>这个值越大，就说明了这个新息越混乱，相反的，越小就说明越有秩序。来，我们看一下代码演示，定义一个熵的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> icecream <span class="hljs-keyword">import</span> ic<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pr</span>(<span class="hljs-params">e, elements</span>):<br>    counter = Counter(elements)<br>    <span class="hljs-keyword">return</span> counter[e] / <span class="hljs-built_in">len</span>(elements)<br><br><span class="hljs-comment"># 信息熵</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">entropy</span>(<span class="hljs-params">elements</span>):<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(pr(e, element) * np.log(pr(e, elements)) <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> elements)<br></code></pre></td></tr></table></figure><p>然后我们具体的来看几组数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">ic(entropy([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]))<br>ic(entropy([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]))<br>ic(entropy([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">8</span>]))<br>ic(entropy([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>]))<br>ic(entropy([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>]))<br>ic(entropy([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;d&#x27;</span>]))<br><br>---<br>ic| entropy([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]): <span class="hljs-number">1.05829973151282</span><br>ic| entropy([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]): -<span class="hljs-number">0.0</span><br>ic| entropy([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">8</span>]): <span class="hljs-number">1.7917594692280547</span><br>ic| entropy([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>]): <span class="hljs-number">1.7917594692280547</span><br>ic| entropy([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>]): <span class="hljs-number">1.7576608876629927</span><br>ic| entropy([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;d&#x27;</span>]): <span class="hljs-number">2.1130832934475294</span><br></code></pre></td></tr></table></figure><p>我们可以看到，最「纯」的是第二行数据，然后是第一行，第三行和第四行是一样的。5，6行就更混乱一些。</p><p>那接下来的知识点是只关于 Python的，我们看上面的代码是不是有点小问题？这个代码里有很多的冗余。一般情况下，会将counter改成全局变量，但是一般如果想要代码质量好一些，尽量不要轻易定义全局变量。我们来简单的修改一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">entropy</span>(<span class="hljs-params">elements</span>):<br>    <span class="hljs-comment"># 信息熵</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">pr</span>(<span class="hljs-params">es</span>):<br>        counter = Counter(es)<br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">_wrap</span>(<span class="hljs-params">e</span>):<br>            <span class="hljs-keyword">return</span> counter[e] / <span class="hljs-built_in">len</span>(elements)<br>        <br>        <span class="hljs-keyword">return</span> _wrap<br>    <br>    p = pr(elements)<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(p(e) * np.log(p(e)) <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> elements)<br></code></pre></td></tr></table></figure><p>这样写之后，我们再用刚才的数据来进行调用会看到结果完全一样。不过如果这样写之后，如果我们数据量很大的情况下，会发现会快很多。</p><h2 id="gini-系数">Gini 系数</h2><p>除了上面这个信息熵之外，还有一个叫 Gini 系数，和信息熵很类似：</p><p><span class="math display">\[\begin{align*}Gini = 1 - \sum_{i=1}^np^2(C_i) \\\end{align*}\]</span></p><p>假如说 probability 都是 1，也就是最纯的情况，那么 1 减去 1 就等于0。如果它特别长，特别混乱，都很分散，那 probability 就会越接近于 0，那么1 减去 0，那结果也就是越接近于 1。</p><p>那么代码实现一下就是这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">geni</span>(<span class="hljs-params">elements</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>-np.<span class="hljs-built_in">sum</span>(pr(e) ** <span class="hljs-number">2</span> <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">set</span>(elements))<br></code></pre></td></tr></table></figure><p>好吧，这个时候我发现一个问题，之前我们将 probability函数定义到熵函数内部了，为了让 Gini函数能够调用，我们还得拿出来。在我们之前修改的初衷不变的情况下，我们来这样进行修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pr</span>(<span class="hljs-params">es</span>):<br>    counter = Counter(es)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_wrap</span>(<span class="hljs-params">e</span>):<br>        <span class="hljs-keyword">return</span> counter[e] / <span class="hljs-built_in">len</span>(es)<br>    <span class="hljs-keyword">return</span> _wrap<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">entropy</span>(<span class="hljs-params">elements</span>):<br>    <span class="hljs-comment"># 信息熵</span><br>    p = pr(elements)<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(p(e) * np.log(p(e)) <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> elements)<br></code></pre></td></tr></table></figure><p>哎，这样就对了。优雅...</p><p>然后我们来修改 Gini 函数，让其调用 pr 函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">gini</span>(<span class="hljs-params">elements</span>):<br>    p = pr(elements)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> - np.<span class="hljs-built_in">sum</span>(p(e) ** <span class="hljs-number">2</span> <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">set</span>(elements))<br></code></pre></td></tr></table></figure><p>然后我们写一个衡量的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pure_func = gini<br></code></pre></td></tr></table></figure><p>然后我们将之前的调用都修改一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">ic(pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]))<br>ic(pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]))<br>ic(pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">8</span>]))<br>ic(pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>]))<br>ic(pure_func([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>]))<br>ic(pure_func([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;d&#x27;</span>]))<br><br>---<br>ic| pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]): <span class="hljs-number">0.2777777777777777</span><br>ic| pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]): <span class="hljs-number">0.0</span><br>ic| pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">8</span>]): <span class="hljs-number">0.8333333333333333</span><br>ic| pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>]): <span class="hljs-number">0.8333333333333333</span><br>ic| pure_func([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>]): <span class="hljs-number">0.44897959183673464</span><br>ic| pure_func([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;d&#x27;</span>]): <span class="hljs-number">0.6122448979591837</span><br></code></pre></td></tr></table></figure><p>我们可以看到，Gini 系数是把整个纯度压缩到了 0~1 之间，越接近于 1就是越混乱，越接近 0 呢就是越有秩序。</p><p>其实除了数组之外，字符串也是一样可以衡量的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">ic(pure_func(<span class="hljs-string">&quot;probability&quot;</span>))<br>ic(pure_func(<span class="hljs-string">&quot;apple&quot;</span>))<br>ic(pure_func(<span class="hljs-string">&quot;boom&quot;</span>))<br><br>---<br>ic| pure_func(<span class="hljs-string">&quot;probability&quot;</span>): <span class="hljs-number">0.8760330578512396</span><br>ic| pure_func(<span class="hljs-string">&quot;apple&quot;</span>): <span class="hljs-number">0.72</span><br>ic| pure_func(<span class="hljs-string">&quot;boom&quot;</span>): <span class="hljs-number">0.625</span><br></code></pre></td></tr></table></figure><p>在能够定义纯度之后，现在如果我们有很多数据，就比方说是我们最之前定义数据再增多一些：<code>[x1, x2, x3, ..., xn]</code>，那么我们决策树会做什么呢？</p><p>根据 x1 对 y 做了分类，根据 x2 对 y 做了分类，做了分类之后，通过 x1把 y 分成了两堆，一堆我们称呼其为<code>m_left</code>,另外一堆我们称呼其为<code>m_right</code>，然后我们来定义一个 loss函数：</p><p><span class="math display">\[\begin{align*}loss = \frac{m_{left}}{n} \cdot G_{left} + \frac{m_{right}}{m} \cdotG_{right}\end{align*}\]</span></p><p>现在要让这个 loss 函数的值最小。在整个式子中，G代表的是纯度函数，这个纯度函数可以是 Entropy，也可以是 Gini。</p><p>loss函数的值最小的时候，就可以实现左右两边分的很均匀。我们把这个算法就叫做CART。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231027192723.png"alt="Alt text" /></p><p>CART算法其实就是<code>classification and regression tree Algorithm</code>,也称为「分类和回归树算法」。</p><p>上面我们讲的所有内容可以实现分类问题，那么回归问题怎么解决呢？CART里可是包含了 Regression 的。</p><p>好，还是我们最之前给的数据，我们现在的 y不是<code>[0,0,1,0,1]</code>了，我们将其更改为<code>[1.3,1.4,0.5,0.8,1.9]</code>。我们人类大脑中的直觉会怎么分类？会将[1.3,1.4,1.9]分为一类，而[0.5,0.8]分为一类对吧？（我说的是大部分人，少部分「天才」忽略）。</p><p>那么为了完成这样的一个分类，我们将之前公式里的纯度函数替换成MSE，那么函数就会变成如下这个样子：</p><p><span class="math display">\[J(k, t_k) = \frac{m_{left}}{m} \cdot MSE_{left} +\frac{m_{right}}{m}MSE_{right}\]</span></p><p>MSE 是什么呢？其实就是均方误差。</p><p><span class="math display">\[\begin{align*}MSE_{node} &amp; = \sum_{i \in node}(\hat y_{node} - y^{(i)})^2 \\\hat y_{node} &amp; = \frac{1}{m_{node}}\sum_{i\in node}y^{(i)}\end{align*}\]</span></p><p>我们要取的，就是最小的那一个 MSE。</p><p>决策树最大的优点就是在决策上我们需要更大的解释性的时候很直观，决策树可以将其分析过程以树的形式展现出来。一般在商业、金融上进行决策的时候我们都需要很高的解释性。就比如下面这个例子：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231027192724.png"alt="Alt text" /></p><p>第二呢，它可以来提取重要特征。决策术可能某一类问题上效果假设最多只能做到85% 的准确度，我们期望换一种模型，希望用到的维度少一点。</p><p>比方说要做逻辑回归，我们期望的<code>w.x+b</code>乘以一个Sigmoid，原来的 x 是 10 维的，我们期望把它降到 5维。那这个时候决策树的构建过程就从最显著的特征开始逐渐构建，我们就可以把它前五个特征给他保留下来，前五个特征就是最salience 的feature。我们假如把它用到逻辑回归上，直接用这五个维度就可以了。</p><p>接着我们来个问题：本来是 10 维的我们期望把它变成 5维，为什么我们希望降维呢？</p><p>还记得我们「拟合」这一节么？我们说过，过拟合最主要的原因是数据量过少或者模型过于复杂，那为什么数据量过少呢？不知道是否还记得我讲过的维度灾难。</p><p>多个维度就需要多个数量级的数据。在仅有这么多数据的情况下，维度越多需要更多数据来拟合，但是大部分时候我们并没有那么多数据。</p><p>这个问题其实是一个很经典的问题，为什么我们做各种机器学习的时候期望降维。如果能把这个仔仔细细的想清楚，其实机器学习原理基本上已经能够掌握清楚了。</p><p>接下来我们再说说它的缺点，其实也很明显，它的分类规则太过于简单。所以它最大的缺点就是因为简单，所以好解释，但正是因为简单，所以解决不了复杂问题。</p><p>和 SVM一样，也可以给决策树加核函数，曾经有一段时间这也是很重要的一个研究领域。</p><p>好，在结束之前我们预告一下下一节课内容，我们还是讲决策树，讲讲决策树中的Adaboost 等，以及决策树的升级版：随机森林。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231027192720.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;在上一节课讲 SVM
之后，再给大家将一个新的分类模型「决策树」。我们直接开始正题。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>15. 机器学习 - 支持向量机</title>
    <link href="https://hivan.me/15.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>https://hivan.me/15.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</id>
    <published>2023-10-29T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:47.462Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026233247.png"alt="茶桁的 AI 秘籍 核心能力基础 15" /></p><p>Hi, 你好。我是茶桁。</p><span id="more"></span><h2 id="逻辑回归预测心脏病">逻辑回归预测心脏病</h2><p>在本节课开始呢，我给大家一份逻辑回归的练习，利用下面这个数据集做了一次逻辑回归预测心脏病的练习。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232831.png"alt="Alt text" /></p><p>本次练习的代码在「茶桁的 AI 秘籍」在 Github上的代码库内，数据集的获取在文末。这样做是因为我的数据集都是和百度盘同步的，很多数据集过大了，所以也就不传Github 了。而且，我直接获取盘内同步数据也更方便。</p><p><strong>还有一个原因，有些数据集可能以后会收费获取。</strong></p><p>好，让我们进入今天的正课。</p><p>因为未来几节课的内容比较多。「核心基础」的这部分内容已经超出我原本的预计，咱们「核心基础」的部分刚刚过半，可是已经写到15 节了，本来这部分内容我是想在 21节左右结束的，所以，我们还是要压缩一下内容了。</p><p>这节课咱们还是继续讲解经典的机器学习。</p><h2 id="支持向量机">支持向量机</h2><p>接下来，要讲解一个非常有趣的方法：支持向量机。</p><p>支持向量机的原理其实可以很复杂，但它是一个很经典的思想方法。咱们就把它的核心思想讲明白就行了。其实我们平时在工作中用的也比较少。但是面试中有一些老一代的面试官会比较喜欢问这个问题。</p><p>支持向量机的核心思想，假如我们有两堆数据，希望找一根线去把它做分类，那么咱们找哪一根线呢？</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232832.png"alt="Alt text" /></p><p>上图中，我们假设黑色的那根线定义为l，把离这根线最近的点，也就是直线距离最小的点，找到两个这样的点定义为P1、P2。</p><p>现在我们是希望离这个 l 最近的点，假如说是d1,d2，那么我们希望这两个距离加起来最大：max|d1+d2|。</p><p>现在再定义蓝色的线为直线 b，那直线 b 做分类就比直线 l要好。为什么直线 b 就比是直线 l 好呢？因为直线 b 离 d1,d2普遍都比较远。</p><p>现在这里的演示是一个二维平面中用一根线来分割，如果是在多维空间中，SVM的目标就是找到一个最佳的超平面来最大化间隔，同时确保正确分类样本。</p><p>假设我们有一组训练样本，每个样本用特征向量 x 表示，并且标记为正类别+1 或负类别 -1。</p><p>我们可以表示为以下凸优化问题：</p><p><span class="math display">\[\begin{align*}min_{w, b}\frac{1}{2}||w||^2\end{align*}\]</span></p><p>其中对所有样本</p><p><span class="math display">\[y_i(w \cdot x_i+b) \ge 1\]</span></p><p>w 是超平面的法向量，b 是截距项，yi 是样本 xi 的标签，也就是 +1 或者-1。</p><p>为了解决这个优化问题，我们引入拉格朗日乘子<spanclass="math inline">\(a_i\)</span>来得到拉格朗日函数：</p><p><span class="math display">\[L(w,b,a) = \frac{1}{2}||w||^2 - \sum_{i=1}^Na_i[y_i(w\cdot x_i +b) - 1]\]</span></p><p>然后我们要最小化拉格朗日函数，首先对 w 和 b 求偏导数，令它们等于0，然后代入拉格朗日乘子条件：</p><p><span class="math display">\[a_i[y_i(w\cdot x_i + b)-1] = 0\]</span></p><p>然后我们就可以得到如下这个式子</p><p><span class="math display">\[w = \sum_{i=1}^Na_iy_ix_i \\sum_{i=1}^N a_iy_i = 0\]</span></p><p>使用某种优化算法（例如，SMO 算法），求解拉格朗日乘子<spanclass="math inline">\(a_i\)</span>。我们就可以使用求解得到的<spanclass="math inline">\(a_i\)</span>计算超平面参数 w 和 b。</p><p>对于新样本 x，使用超平面<span class="math inline">\(w\cdot x +b\)</span>的符号来预测其类别。</p><p>那我们讲了这么半天，都是一个支持向量机的数学演示过程，下面我们来看看具体的代码实现。</p><p>我们先来生成两组数据，这两组数据咱们让他距离更大：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>label_a = np.random.normal(<span class="hljs-number">6</span>, <span class="hljs-number">2</span>, size=(<span class="hljs-number">50</span>, <span class="hljs-number">2</span>))<br>label_b = np.random.normal(-<span class="hljs-number">6</span>, <span class="hljs-number">2</span>, size=(<span class="hljs-number">50</span>, <span class="hljs-number">2</span>))<br></code></pre></td></tr></table></figure><p>我们现在来观察以下生成的这些点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.scatter(*<span class="hljs-built_in">zip</span>(*label_a))<br>plt.scatter(*<span class="hljs-built_in">zip</span>(*label_b))<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232833.png"alt="Alt text" /></p><p>然后我们继续：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">label_a_x = label_a[:, <span class="hljs-number">0</span>]<br>label_b_x = label_b[:, <span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p>我们就将这两组数据的第一列分别取出来了。</p><p>接着我们随机的定义一些 w 和 b</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    w, b = (np.random.random(size=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)) * <span class="hljs-number">10</span> - <span class="hljs-number">5</span>)[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p>然后我们按照之前讲的数学演示来定义一个函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> w*x+b<br></code></pre></td></tr></table></figure><p>然后我们之前从数学演示里已经知道，<spanclass="math inline">\(y_i(w\cdot x+b) \ge 1\)</span>,而我们也知道这个说的是距离，也就是说，同样的$y_i(wx+b) $。</p><p>也就是说，我们要让函数 f 小于等于 -1，并且大于等于1。当然，为了保证其被分到两边，我们将函数的最大值定义为小于等于-1，将函数的最小值定义为大于等于 1。这样就保证 (-1,1)之间是不存在任何函数值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.<span class="hljs-built_in">max</span>(f(label_a_x, w, b)) &lt;= -<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> np.<span class="hljs-built_in">min</span>(f(label_b_x, w, b)) &gt;= <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>只有同时满足这两个条件的值，我们才会留下来进行保存。我们可以定义一个变量将其保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">w_and_b = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    w, b = (np.random.random(size=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)) * <span class="hljs-number">10</span> - <span class="hljs-number">5</span>)[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">if</span> np.<span class="hljs-built_in">min</span>(f(label_a_x, w, b)) &gt;= -<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> np.<span class="hljs-built_in">min</span>(f(label_b_x, w, b)) &gt;= <span class="hljs-number">1</span>:<br>        w_and_b.append((w, b))<br></code></pre></td></tr></table></figure><p>在得到这些 w,b 之后，我们将这些 w,b 连起来进行画图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> w, b <span class="hljs-keyword">in</span> w_and_b:<br>    x = np.concatenate((label_a_x, label_b_x))<br>    plt.plot(x, f(x, w, b))<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232834.png"alt="Alt text" /></p><p>这样，我们就拟合出来了很多的曲线。这些个曲线到底哪一个是最好的那一个呢？</p><p>现在根据刚刚得到的那个结论，现在所有的<spanclass="math inline">\(y_i(w\cdot x_i + b)\)</span>,那么现在其实就是<span class="math inline">\(margin =\frac{2}{||w||}\)</span>。</p><p>那我们现在就找这个 w 最小的这个值就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">w, b = <span class="hljs-built_in">min</span>(w_and_b, key = <span class="hljs-keyword">lambda</span> w_b: w_b[<span class="hljs-number">0</span>])<br>all_x = np.concatenate((label_a_x, label_b_x))<br>plt.plot(all_x, f(all_x, w, b), <span class="hljs-string">&#x27;r-o&#x27;</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232835.png"alt="Alt text" /></p><p>现在我们就可以看到那个最优的直线了，就是众多红色的点连接起来的那根线。</p><p>当然，最后代码执行顺序和讲解顺序有一些不一样，为了避免数据每次重新生成造成的差别，所以最开始是生成数据，之后是定义函数、过滤参数以及生成图像。</p><p>这个就是支持向量机的原理，我们找到离它所有的点的一个距离，让它这个边距最大，最后得到一个简化结果。</p><h2 id="核函数">核函数</h2><p>然后我们再来看另外一个点：「核函数」：</p><p>核函数是支持向量机里面非常重要的一个东西。</p><p>如果支持向量机只要数据是线性可分的，那么我们一定能够找到它的分割线。但是在实际的现实生活中有很多点并不是线性可分的。</p><p>举个例子，我们来画一张图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232836.jpeg"alt="Alt text" /></p><p>就比如图中的这种数据，是无论如何用一条直线无法分割的，不管怎么画，都无法把蓝色和红色的点分割开。</p><p>就像我们下面这张图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232837.png"alt="Alt text" /></p><p>但是，我们我们可以做这样一件事情，假设我们在一个坐标轴上拥有 8个点，A、B、C、D 为一组，a,b,c,d 为一组。如下图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232838.jpeg"alt="Alt text" /></p><p>分别为 A(-1,1), B(1,1), C(1, -1), D(-1,-1)；a(-0.5, 0.5), b(0.5,0.5), c(0.5, -0.5), d(-0.5, -0.5)。</p><p>现在我们 ABCD 和 abcd 是无法用一根直线来分割的，然后我们令：</p><p><span class="math display">\[\begin{align*}f(x) =&gt; \begin{Bmatrix} x^2 \\ y^2 \end{Bmatrix}\end{align*}\]</span></p><p>那在这种情况下，八个点分别就变成了 A(1, 1),B(1, 1),C(1, 1),D(1,1)，a(0.25, 0.25),b(0.25, 0.25),c(0.25, 0.25),d(0.25, 0.25)。</p><p>那这样的情况下，我们就完全可以用一根直线去分割了：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232839.jpeg"alt="Alt text" /></p><p>那现在找到这根线是 w2 =wx+b，那我们遇到新数据应用到这个函数里边，再应用到这个线里面做分割就可以了。我们把原本线性不可分的东西，变成线性可分的。那么这个就是核函数神奇的地方。</p><p>支持向量机通过某非线性变换 φ(x)，将输入空间映射到高维特征空间。特征空间的维数可能非常高。如果支持向量机的求解只用到内积运算，而在低维输入空间又存在某个函数K(x, x′) ，它恰好等于在高维空间中这个内积，即 K(x, x′) =φ(x)⋅φ(x') ;。那么支持向量机就不用计算复杂的非线性变换，而由这个函数 K(x, x′)直接得到非线性变换的内积，使大大简化了计算。我们就将这种函数函数 K(x,x′) 称为核函数。</p><p><span class="math display">\[\varphi (x) = \begin{bmatrix} x \\ x^2 \\ x^3 \end{bmatrix}\]</span></p><p>那其实，就类似的事情，已经有人总结了一些相应的公式来使用：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232840.png"alt="Alt text" /></p><p>这些是一些常见的核函数。</p><p>一般在使用的时候调用它就可以，如果在用 SVM的时候，它会有一个参数。可以自己定义一个核函数，但一般不自己定义，调用现有的就够了。</p><p>SVM其实也有弊端，当数据量很复杂的时候，现有的核函数就没有作用了。因为它会失效，所以我们需要很多的人工分析，整个效率很低。</p><p>但是在整个机器学习的发展史上，它曾经有非常重要的一段历史。有一段时间它的论文量非常的多，做科研的非常爱做SVM，不是因为快速，是因为可以提出来各种各样的 Kerno 函数。</p><p>假如有一组数据不好分割，但是你提出了一种新的核函数，这个函数量可以比较复杂啊然后提升了分割率，提高了效果。</p><p>但是这种方法其实曾经一度让机器学习非常不受人待见，在学术圈非常不受人待见。搞机器学习的人就是每天就是发论文，说我的曲线比你的曲线强，这就是他们干的事。</p><p>所以 10年左右，做机器学习、做人工智能的人都不说自己是做机器学习，做人工智能的。都换个名字，说做文本挖掘等等。</p><p>SVM因为要做各种升维，当数据量比较大的时候，计算量非常的复杂，计算需求量非常的大。</p><p>但是 SVM 它有个好处，就是它比较直观，还有就是 SVM对于不平衡的数据比较有用。</p><p>好，这节课我们就讲到这里，下一节课我们来看「决策树」。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231026233247.png&quot;
alt=&quot;茶桁的 AI 秘籍 核心能力基础 15&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi, 你好。我是茶桁。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>14. 机器学习 - KNN &amp; 贝叶斯</title>
    <link href="https://hivan.me/14.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20KNN%20-%20%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    <id>https://hivan.me/14.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20KNN%20-%20%E8%B4%9D%E5%8F%B6%E6%96%AF/</id>
    <published>2023-10-27T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:50.457Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231024012141.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><p>咱们之前几节课的内容，从线性回归开始到最后讲到了数据集的处理。还有最后补充了SOFTMAX。</p><span id="more"></span><p>这些东西，都挺零碎的，但是又有着相互之间的关系，并且也都蛮重要的。并且是在学习机器学习过程当中比较容易忽视的一些内容。</p><p>从这节课开始呢，我要跟大家将一些其他的内容。</p><p>虽然最近几年用到的方法主要都是深度学习的方法，但是机器学习并不代表就只有深度学习这一种方法。</p><p>当然现在的深度学习其实是从线性回归演化来的，都是用一种梯度下降的方式来做。但是呢其实有很多机器学习方法用的不是这种思想。</p><p>那接下来就给大家要讲的，就是曾经非常有名，也非常有用的一些方法。这些方法的思想和用法和线性回归的机器学习不太一样。</p><p>为什么咱们现在主要用深度学习呢？之所以深度学习很火，原因就是我们的整个机器学习的模型可以像搭乐高积木一样。</p><p>比方有一个线性变化，是 sigmoid，然后有一个Softmax，还有之后大家要学到什么 LSTM，RNCN，还有 Linearregression。他可以互相去连接，可以像玩乐高积木或者说像做电路一样，可以做出来非常复杂的模型。</p><p>那么深度学习的模型变得极其复杂之后，上节课我讲过，如果模型特别复杂，可以表证比较复杂的情况，但是需要比较多的数据去拟合它。</p><p>如果模型很复杂，就需要比较多的数据。而在现在，最近十几年互联网产生的数据量啊大了很多，所以深度学习这种复杂模型的特点就可以被释放出来了。</p><p>而在以前数据量特别小，类似深度学习这种方法，参数也特别多，效果不太好。</p><p>那我们为什么要去学习这些老的经典的学习方法的原因，就是咱们有些时候经常会遇到一个情况就是训练数据其实没有那么多，可能也就一两千个，或者说三五千个。就这几千几万个数据，用深度学习模型其实它是很复杂的，效果也不好。因为模型太复杂了，需要的数据比较多。</p><p>所以，当数据量比较小的时候，问题比较简单的时候，其实用一些比较经典的方法是比较好的。</p><p>第二个原因，像贝叶斯、KNN、还有决策树，这些方法现在虽然慢慢不是主流了，但是他背后的思想其实可以帮助我们做很多事情。</p><p>假如说要判断物体是不是相似等等类似的这些情况，这些方法可以很好的启发你。可以去用在其他场景下去解决问题。</p><p>第三个原因，传统的机器学习有比较好的可解释性。函数 f(x)到底是怎么样求得的 y，里边的每一步可以解释的很清楚。</p><p>举个例子，现在我们给任何一个人一台计算机，只要给足够的时间，也不用多就一个月，用深度学习模型去做股市的预测，都可以拟合一个函数拟合这个股市的预测程度非常高。</p><p>按照这个做法来做的话，我们去股市投一年可以赚200%。但是你用这个模型去预测未来的时候就不行了。</p><p>就是你去预测过去的事情，做训练可以用，不代表未来也可以用。</p><p>所以巴菲特如果问你，为啥这个东西可以？你说因为我在收集到的历史数据上做的效果比较好。你觉得巴菲特会信你吗？</p><p>但是老一代的机械学习模型，尤其是KNN、贝叶斯，可以把它的决策逻辑，为什么决策，为什么得到这个结果的过程给大家讲清楚。</p><p>除此之外要说的是，如果你想成为一个技术很厉害的技术达人，或者一个技术专家，那你要注意一件事，心态一定要开放。</p><p>深度学习不能从逻辑上解释，只能凭感觉去解释，就是只能人去解释。</p><p>就好比一个占卜的人去解释，只能靠人去去阐述。就好比牧师去解读圣经一样。科学的尽头都是玄学是吧？</p><p>之前给大家说过监督学习，监督学习有一个比较数学，比较形式化的定义：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231024012142.png"alt="Alt text" /></p><p>对于一些数据，如上图，数据 D={D1, D2, ..., Dn}。对于这些数据有 x 和y，y 就是它 label，是 desire 的 output，是期望的输出。</p><p>然后我们希望能够学习到一种映射，f:x -&gt; y, 从 x 能够到y，只要能够实现从 x 到 y 这样的一种映射，那么它就是一种监督学习。</p><p>如果这个 y 输出是连续的，那么它就是回归。如果它是discrete，那么我们就说它是分类。</p><p>所以这个mapping，这种映射关系可以是各种各样的一种映射关系，可以是很多种。</p><h2 id="knn">KNN</h2><p>我们现在要来讲的，就是第一种。除了深度学习，线性回归和逻辑回归之外，咱们要讲的第一种，就是KNN，又称 K 近临。</p><p>KNN 几乎可以说是最简单、最直接、最古老的一种机器学习方法了。</p><p>他的原理很简单。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231024012143.png"alt="KNN" /></p><p>比方说我们现在有这么些点，然后问 X轴红色五角星的位置向上对应的点在什么位置？</p><p>那 KNN无法告诉你准确的这个点的位置是多少，但是我们可以利用周围确定的点，也就是圆圈圈定的范围内的这些点，用它们来求一个平均值。</p><p>也就是说，离的最近的 k个值是多少，然后求个平均值就行了。这个好像很有道理的样子。</p><p>好，那我们看到，这个其实是解决了回归问题。那现在我们来看看分类问题：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231024012144.png"alt="Alt text" /></p><p>比方说上面这张图，有一些红色的点和一些蓝色的点。那么现在问题就是，<code>?</code>号所在的这些点是什么呢？</p><p>那左边的圈里，离的最近的是一个蓝色，四个红色，那<code>？</code>的这个点就是红色。在右边的这个圈里，红色比蓝色更多，那这个点也是红色。</p><p>这样的话，你会发现整个求解起来就很简单。</p><p>假如咱们给一组数据 x，</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231024023020.png"alt="Alt text" /></p><p>那么我们有一组<span class="math inline">\(x_i\)</span>, y 有可能是Numerical, 也有可能是一个类别。</p><p>现在来了一个新的<span class="math inline">\(x_i\)</span>,假如要知道他的值，我们要找到离他最近的。</p><p>现在我们有 n 个 x，打比方说有 n=100，在这里 k 假设先定义成 30000, k也是一个参数可以自己改。</p><p>那对应到表格内，如果这个问题是一个回归问题，我们就那最近的点求一个平均值，如果是分类问题，我们就看附近的所有点哪一个分类出现的最多就可以了。</p><p>那按照表格内的数据，假如新出现的这个<spanclass="math inline">\(x_i\)</span>附近是 x1, x2, x3, x4，回归问题就是(0.38+1.27+3.56+3.19)/4, 分类问题就是 (0, 1, 0)。</p><p>这个就是 KNN 的原理，如果要手动去写的话，按照大家水平不到 5分钟就能把这个分类和回归的全部写完。</p><p>它实现起来真的特别简单，而且解释起来也很好解释。比方说它预测出来是红色，为什么是红色呢？因为离我最近的3 个或者 5 个占大多数的是红色。</p><p>它也有一些缺点，比较大的缺点是什么呢？</p><p>一个显著的缺点就是当我们要求的这个点附近完全没有值，离他最近的那个值都离的特别远，那这个时候我们要去求解，它的值就会跑到很远但是离它最近的那些点之间。</p><p>KNN找的是和自己最类似的，但是如果他找不到和自己最类似的，他就傻了。</p><p>总结一下 KNN 的优缺点：</p><table><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><thead><tr class="header"><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>容易实现、容易理解</td><td>运行时间长</td></tr><tr class="even"><td>模型调整容易，可以方便的改变 k 的数量，或者给不同距离的 k进行加权</td><td>容易被异常值影响</td></tr><tr class="odd"><td>适合解决各种复杂问题（分类、回归、高纬、低纬、复杂关系、简单关系）</td><td>所需空间大</td></tr><tr class="even"><td></td><td>高纬空间的距离区分度不大</td></tr></tbody></table><p>关于这个时间久，我们来看一个问题：KNN 的时间复杂度是多少？</p><p>如果是 2.7Ghz 英特尔 i7 处理器，训练数据有 1 千万哥数据是 300维，定义的 k=11，则预测 100 个实例需要多久？</p><p>几毫秒、几秒、几分钟、几个小时还是几天？</p><p>2.7Ghz，Ghz 就是一秒钟可以运行一个 G，一个 G 就是 2 的 11 次方。</p><p>那么估算下来其实应该是几个小时的操作。</p><p>那在一些大厂，阿里，蚂蚁，微信等等，那随随便便都是几千万的数据。</p><p>所以 KNN 更合适数据量小的情况。</p><p>还有就是我们说一个数学上的概念，当一个向量的维度很高的时候，比如说几百几千维，那个时候的向量就会有一个特点，基本上任意两个向量之间的距离都差不多，区别不是很大。</p><p>所以对于 KNN 这种在高纬向量里面做检索，整个距离差的也不大。</p><p>我们需要了解一点，机器学习里面分了两种方式，第一种叫lazy-learning，第二种叫 eager learning。</p><p>lazy-learning 就是懒惰的学习，KNN 就是这种方式。KNN 是典型的一种lazy-learning。</p><p>KNN 只是简单的内容记下来，然后去找了一个最接近的东西。lazy-learning最大的问题就是所观测到的是它周围的这些结果。</p><p>Data site 是比较少的维度，其实效果倒也可以。</p><p>为什么要用 Lazy呢，这个其实和东西方的教育观念的一个差别，在我们传统观念中的那种勤奋，就是死记硬背的埋头苦读其实就是一种Lazy，其含义是思维上比较 Lazy。</p><p>然后比较擅长总结归纳，然后分析预测这种我们叫做的 eager。</p><p>这种方法看到的更加全面，要看到更加广阔的问题，然后抽象出更高层次的函数。我们把这种叫做eager learning。</p><p>基本上在咱们整个课程里面除了 KNN 算法，别的全部都是 eager。</p><h2 id="贝叶斯">贝叶斯</h2><p>接着我们来讲一下贝叶斯，首先我们来看一段文本：</p><blockquote><p>对于某种商品，根据以往购买的数据，在任意投放广告，未进行特点渠道优化的时候，点击广告到购买商品的比率为7%，自然形成的用户中，本科及以上学历的用户占 15%，本科以下学历占85%；现在有一笔广告预算，本科及以上学历渠道的获客成本市场 100元每人，本科以下人群投放广告的成本是 70元每人。问，该广告投放到本科及以上学历专门的人群还是本科以下人群？（本科及以上学历占总国人的比例约为5%，2016 年国家统计局数据）</p></blockquote><p>假如遇到这种问题的时候，一般都会有两拨人互相PK，互相撕扯。一波人认为顾客里面买东西的有 15%的是本科生，本科生比例还挺高，应该大力发展本科生这部分用户，应该把广告主要往本科生这边投。另外一波人会说，有85% 的人是本科以下学历的人，而且国家有 95%的人是本科以下学历，所以这个市场更大，应该去投本科以下的学历。</p><p>这是非常实际的一个问题，假如你以后在公司里的遇到这个问题的时候怎么样去估计呢？我们可以做一个比较简单的数学式子，其实现在我们要估算两个概率，第一个概率是本科及以上的人看见了广告买东西的概率是多少，另外一个就是本科以下的人看见广告买东西的概率是多少。</p><p>但是我们现在没有这个数据，只有购买的人里边有多少个人是本科以上，有多少是本科以下。</p><p>也就是说，我们现在不知道一个本科生看见广告之后有多少概率会买，但是可以通过一个方法来解决。就是本科及以上的人购买的概率，其实等于如下这个方程：P(A|B)= P(AB) / P(B) = P(B|A)P(A) / P(B)。</p><p>对应我们现在面对的这个案例，那其实就等于是：</p><p>Pr(购买 | 本科及以上) = Pr(本科及以上 | 购买) * Pr(购买) /Pr(本科及以上) = 15% * 7% / 5% = 21%</p><p>Pr(购买 | 本科以下) = Pr(本科以下 | 购买) * Pr(购买) / Pr(本科以下) =85% * 7% / 95% = 6.26%</p><p>贝叶斯其实也就是就是这个方法。</p><p>我们根据得出的结论，本科及以上每个人的广告成本为 100 块钱，有 21%的人会买，所以平均成交一个人需要花 476，100/21 = 476。</p><p>同理本科以下的人每个广告成本是 70%, 但是最终会购买东西的概率只有 6%,所以说最终成本是 1,180 块。70 / 6% = 1167。</p><p>那假如咱们的产品卖 2,000，花 100万广告费只投放给本科以上的人群，那公司就可以收入 420万，投给本科以下的人，就只能卖 116.7 万。</p><p>这个例子是一个非常典型的商业决策例子，这种决策都有个特点，未来的事情谁都说不上。过去的事情板上钉钉的已经发生了。</p><p>我们再说一个很典型的例子，一个骰子连续丢出 4 次 6，那第 5 次出现 6的概率到底是大于 1/6 还是小于 1/6，还是等于 1/6？</p><p>等于 1/6 的说法，因为骰子出不出 1/6每一次事件之间是独立的，所以说第五次也是 1/6。小于 1/6的说法，已经出了那么多 6 了，接下来不应该出 6了。而对于有些人来说，这色子现在明显就是有问题，一个骰子一般来说很少会出现这样的情况，所以这个骰子容易出6，概率肯定是比 1/6 大。</p><p>所以一般来说，比较聪明的决策是把未来看成是过去的再次发生。贝叶斯分类其实就是做这件事情的。</p><p>贝叶斯定理表示了在给定先验概率和条件概率的情况下，如何计算后验概率。</p><p><span class="math display">\[\begin{align*}P(A|B) = \frac{P(B|A)*P(A)}{P(B)}\end{align*}\]</span></p><ul><li>P(A|B) 是后验概率，表示在给定观测数据 B 后事件 A 发生的概率。</li><li>P(B|A) 是条件概率，表示事件 A 发生的情况下事件 B 发生的概率。</li><li>P(A) 是先验概率，表示事件 A 在没有观测数据 B 的情况下的概率。</li><li>P(B) 是边际概率，表示事件 B 发生的总概率。</li></ul><p>在朴素贝叶斯分类中，我们使用特征向量 X=(x1,x2,...,xn)来表示一个样本，其中 x_i 是第 i个特征的取值。我们希望根据这些特征来分类样本为不同的类别 C。</p><p><span class="math display">\[\begin{align*}p(C_k,x_1,...,x_n) &amp; = p(x_1,...,x_n, C_k) \\&amp; = p(x_1|x_2,...,x_n,C_k)p(x_2,...,x_n,C_k) \\&amp; = p(x_1|x_2,...,x_n,C_k)p(x_2|x_3,...,x_n,C_k)p(x_3,...,x_n,C_k)\\&amp; = ... \\&amp; =p(x_1|x_2,...,x_n,C_k)p(x_2|x_3,...,x_n,C_k)...p(x_{n-1}|x_n,C_k)p(x_n|C_k)p(C_k)\end{align*}\]</span></p><p>我们要做这个决策，在以上的式子基础上做了一个很重要的假设，假设什所有特征之间的条件是独立的，即给定类别C下，特征之间的关系是独立的。这个假设使得计算变得简单，但通常并不成立，尤其实在自然语言处理任务中。</p><p>因为这个原因，所以它被称为 NaiveBayes，就是我们通常所称的「朴素贝叶斯」。其实 Naive真正的翻译应该是「幼稚的」。</p><p>简化后，朴素贝叶斯分类器的数学公式：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231024012145.png"alt="Alt text" /></p><ul><li>P(C|X) 是后验概率，表示在给定特征向量 X 的情况下，样本属于类别 C的概率。</li><li>P(X|C): 是似然度，表示在类别 C 下观测到特征向量 X的概率。基于朴素独立性假设，可以将它分解为各个特征的条件概率的乘积：<spanclass="math inline">\(P(X|C) =P(x_1|C)*P(x_2|C)*...*P(x_n|C)\)</span></li><li>P(C) 是先验概率，表示样本属于类别 C 的概率。</li><li>P(X) 是归一化常数，用于确保后验概率的总和为 1。</li></ul><p>为了进行分类决策，朴素贝叶斯分类器计算每个类别的后验概率，然后选择具有最高后验概率的类别作为分类结果。数学上，这可以表示为：</p><p><span class="math display">\[\begin{align*}C_{MAP} = \arg max_cP(C|X)\end{align*}\]</span></p><p>其中<span class="math inline">\(C_{MAP}\)</span>是最可能的类别。</p><p>比方下面这个列表：</p><table><thead><tr class="header"><th>Example No.</th><th>Color</th><th>Type</th><th>Origin</th><th>Stolen?</th></tr></thead><tbody><tr class="odd"><td>1</td><td>Red</td><td>Sports</td><td>Domestic</td><td>YES</td></tr><tr class="even"><td>2</td><td>Red</td><td>Sports</td><td>Domestic</td><td>NO</td></tr><tr class="odd"><td>3</td><td>Red</td><td>Sports</td><td>Domestic</td><td>YES</td></tr><tr class="even"><td>4</td><td>Yellow</td><td>Sports</td><td>Domestic</td><td>NO</td></tr><tr class="odd"><td>5</td><td>Yellow</td><td>Sports</td><td>Imported</td><td>YES</td></tr><tr class="even"><td>6</td><td>Yellow</td><td>SUV</td><td>Imported</td><td>NO</td></tr><tr class="odd"><td>7</td><td>Yellow</td><td>SUV</td><td>Imported</td><td>YES</td></tr><tr class="even"><td>8</td><td>Yellow</td><td>SUV</td><td>Domestic</td><td>NO</td></tr><tr class="odd"><td>9</td><td>Red</td><td>SUV</td><td>Imported</td><td>NO</td></tr><tr class="even"><td>10</td><td>Red</td><td>Sports</td><td>Imported</td><td>YES</td></tr></tbody></table><p>列表中分别有 Color，车的颜色，Type，车的型号，Origin，车的产地，以及Stolen，是否被偷。</p><p>我们令<span class="math inline">\(x_1\)</span> = color, <spanclass="math inline">\(x_2\)</span> = type, <spanclass="math inline">\(x_3\)</span> = origin，那么<spanclass="math inline">\(x_1,x_2,x_3\)</span>现在就是特征。</p><p>假如基于上面朴素贝叶斯的数学公式，我们得到：</p><p><span class="math display">\[\begin{align*}P(C_1 |x) = \frac{分子}{P(x)} \\P(C_2 |x) = \frac{分子}{P(x)} \\\end{align*}\]</span></p><p>我们可以发觉，它们的分母其实都是一样的，都是<spanclass="math inline">\(P(x)\)</span>。虽然我们并不知道<spanclass="math inline">\(P(x)\)</span>是多少，但是我们要比较的是<spanclass="math inline">\(P(C_1|x)\)</span>和<spanclass="math inline">\(P(C_2|x)\)</span>的大小，所以我们完全可以不考虑一样大小的分母，就只比较分子大小就可以了。</p><p>那我们就将式子变为：</p><p><span class="math display">\[\begin{align*}P(C_1|x) = P(x_1|C_1)P(x_2|C_1)P(x_3|C_1)P(C_1) \\P(C_2|x) = P(x_1|C_2)P(x_2|C_2)P(x_3|C_2)P(C_2)\end{align*}\]</span></p><p>我们拿一个来分析，其中的<spanclass="math inline">\(P(x_1|C_1)\)</span>, 假设现在<spanclass="math inline">\(x_1 = Red, C_1 = Stolen(YES)\)</span>,那其实就是<spanclass="math inline">\(P(Red|Stolen(YES))\)</span>，也就是所有被偷的里面，Red占多少。那么现在就简单的数数就行了。同理，各个特征的值我们都可以直接数数就能数出来。</p><p><span class="math inline">\(P(C_1)\)</span>和<spanclass="math inline">\(P(C_2)\)</span>的概率是多少？<spanclass="math inline">\(P(C_1)\)</span>和<spanclass="math inline">\(P(C_2)\)</span>是我们所有的汽车里面，被偷的占比多少，没偷的占比多少，也是数数就可以数出来。</p><p>贝叶斯方法就是我们只需要有一张表格，然后通过数数，通过简单的计算就能够把概率给求出来。黄色的车被偷的概率大还是红色车被偷的概率大，直接就可以数出来。</p><p>我们现在看到的贝叶斯都是一个一个类别，但是有的时候，假如说我们所面对的不是0，1 这种类别，而是：3，2，2.5, 1.6这种实数该怎么办呢？我们就要用到「高斯贝叶斯分布」。</p><p>高斯贝叶斯分布是在处理连续值的时候一个非常典型的做法，就是把连续值做一个分布，做一个离散化处理：</p><p><span class="math display">\[\begin{align*}p(x = v|C_k) = \frac{1}{\sqrt{2\pi\sigma^2_k}}e^{-\frac{(v-\muk)^2}{2\sigma^2_k}}\end{align*}\]</span></p><p>我们只需要知道这一点就行了，贝叶斯解决连续值的问题用了高斯分布。</p><p>那么贝叶斯公式的优点是什么呢？首先它非常容易被实现，基本上写代码都能写到，就是不断的数数。</p><p>而且它的预测其实很快，它其实做出了一个概率函数，不像 KNN那样还要把所有的数据都存下来，就是存下来了一个判别的数字，直接一乘就可以，特别快。而且在数据量很大的时候效果也比较好。</p><p>但是它也有缺点，因为做了一个 if 的假设，就是 X1 和 X2、X3之间都没有关系，但是其实很多时候 x之间是有关系的，这其实是很错误的。</p><p>所以当问题变得复杂的时候，贝叶斯往往就不行了。比方说要解决复杂的自然语言处理问题、图像识别问题就不行了。</p><h2 id="贝叶斯案例---预测广告">贝叶斯案例 - 预测广告</h2><p>咱们来看一个贝叶斯的题目。</p><p>假设我们现在有三段短信内容：第一段是一段广告："快来抢购，这是最大的优惠"；第二段也是一段广告："今天抢购最优惠"；第三段不是广告，就是一个正常的短信内容："今天什么时候回家"</p><ul><li>Ad: 快来抢购，这是最大的优惠</li><li>Ad: 今天抢购最优惠</li><li>Text: 今天什么时候回家</li></ul><p>现在我们有一个问题：“今天回家抢购”，我们现在要做的是一个垃圾短信拦截，那么这一段内容是属于广告还是不属于广告？</p><p>那这个问题，我们实际上就可以用贝叶斯来进行解决。</p><p>我们假设 S = 今天回家抢购</p><p>那么我们要做的事情就是比较这两个概率的大小:Pr(Ad|S) ~Pr(Text|S)。</p><p>根据贝叶斯的公式，我们就可以得到下面这一步：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Pr(Ad | S) = Pr(Ad | w1 w2 w3)<br>Pr(Text | S) = Pr(Ad | w1 w2 w3)<br></code></pre></td></tr></table></figure><p>然后我们可以将其转化为：</p><p><span class="math display">\[\begin{align*}Pr(Ad|w_1 w_2 w_3) =\frac{Pr(w_1|Ad)Pr(w_2|Ad)Pr(w_3|Ad)Pr(Ad)}{Pr(w_1w_2w_3)}\end{align*}\]</span></p><p>同理呢，Text 和 Ad 一样可以进行转化：</p><p><span class="math display">\[\begin{align*}Pr(Text|w_1 w_2 w_3) =\frac{Pr(w_1|Text)Pr(w_2|Text)Pr(w_3|Text)Pr(Text)}{Pr(w_1w_2w_3)}\end{align*}\]</span></p><p>其中这些 w1, w2, w3 就是相关的特征，我们将 S 分词成 w1 = 今天，w2 =回家，w3 = 抢购。</p><p>现在我们来看一下，Pr(Ad) 等于多少？等于2/3，也就是我们这三句话中，已知的广告概率是多少。相对的，Pr(Text) 就是1/3。</p><p>接着我们可以知道。Pr(w1|Ad) 就是“今天”在所有广告里出现了几次，广告一共是 2 次，“今天”出现了 1 次，所以应该是 1/2。</p><p>那 Pr(w2|Ad)呢，“回家"没有出现过，不过这里我们要注意，虽然它没有出现过，但是我们不能让它的概率为0，我们要给它一个估计值，这个叫做 OOV，out of vocabulary。因为直接为 0的话，这个词有比较奇怪就没法去做了，判断不了。</p><p>经过简单的分词，我们一共有 13 个单词，13 个单词里边 w2出现了一次，所以咱们可以给他一个估计值 1/13。</p><p>如果没有这个估计值的话，可能只要在内容里面随机加一些生僻字，就能够躲过系统的检测。</p><p>w3 是“抢购”，Pr(w3|Ad) 就是 1。那么 Pr(Ad|S) 的分子部分就是 Pr(Ad) =2/3, Pr(w1|Ad) = 1/2, Pr(w2|Ad)=1/13, Pr(w3|Ad) = 1。</p><p>那我们上节课说过，分母我们其实不用管，也就是 Pr(w1 w2 w3)在过程中其实是不用计算的。</p><p>我们同理再来看以下 Pr(Text|S) 的分子，Pr(Text)=1/3, Pr(w1|Text)=1,Pr(w2|Text)=1, Pr(w3|Text)=2/13。</p><p>那我们最后可以得到简单的一个小学式子：</p><p><span class="math display">\[\begin{align*}Pr(Ad|S) &amp; = 1/2 * 1/13 * 2/3 = 1/13 * 1/3  \\Pr(Text|S) &amp; = 2/13*1/3\end{align*}\]</span></p><p>明显 Pr(Text|S)更大一点，所以“今天回家抢购”这句话更大的概率下不是广告。</p><p>这个就是贝叶斯的一个应用案例，也是非常常见的一个面实题。</p><p>好，那这节课的内容就到这里了，要记得复习。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231024012141.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;咱们之前几节课的内容，从线性回归开始到最后讲到了数据集的处理。还有最后补充了
SOFTMAX。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Mac上挂载APFS移动硬盘</title>
    <link href="https://hivan.me/mount-apfs-on-mac/"/>
    <id>https://hivan.me/mount-apfs-on-mac/</id>
    <published>2023-10-26T10:10:41.000Z</published>
    <updated>2023-10-26T10:52:24.631Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>自用文，有需要的自取。</p></blockquote><p>百度网盘同步会认为移动硬盘是系统盘，所以无法进行同步。当然，也有例外的，之前我也是不知怎么同步的。</p><p>这次设置的时候被警告了，不允许设置。</p><span id="more"></span><p>好吧，那就只能将移动硬盘挂载到我的用户目录里了，我的移动硬盘是APFS类型，执行下面命令：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># 查看当前硬盘IDENTIFIER</span><br>diskutil apfs list<br><br><span class="hljs-comment"># 或者下面这段命令</span><br>diskutil list<br><br><span class="hljs-comment"># 然后需要进行解锁，恢复键值(recovery_key):</span><br>diskutil apfs unlockVolume <span class="hljs-regexp">/dev/</span>apfs_volume_id -passphrase recovery_key<br><br><span class="hljs-comment"># 接着进行装载到自己期望的目录</span><br>diskutil mount -mountPoint Path apfs_volume_id<br></code></pre></td></tr></table></figure><p>假定我的硬盘<code>apfs_volume_id</code>为disk5s1,希望挂载到<code>~/mount</code>则：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">diskutil apfs unlockVolume /dev/disk5s1 -passphrase recovery_key<br>diskutil mount -mountPoint ~/mount /dev/disk5s1<br></code></pre></td></tr></table></figure><p>本是留待自用的，有需要的有缘人自行取走。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;自用文，有需要的自取。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;百度网盘同步会认为移动硬盘是系统盘，所以无法进行同步。当然，也有例外的，之前我也是不知怎么同步的。&lt;/p&gt;
&lt;p&gt;这次设置的时候被警告了，不允许设置。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>13. 机器学习 - 数据集的处理</title>
    <link href="https://hivan.me/13.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%84%E7%90%86/"/>
    <id>https://hivan.me/13.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%84%E7%90%86/</id>
    <published>2023-10-24T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:54.093Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195926.png"alt="茶桁的 AI 秘籍 核心基础 13" /></p><p>Hi，你好。我是茶桁。</p><p>上一节课，咱们讲解了『拟合』，了解了什么是过拟合，什么是欠拟合。也说过，如果大家以后在工作中做的就是机器学习的相关事情，那么欠拟合和过拟合就会一直陪伴着你，这两者是相互冲突的。</p><span id="more"></span><p>现在，让我们一起来思考一个问题：<code>overfitting</code>，过拟合产生的原因是什么？</p><p>如果这是在模型层面的话，参数过多还是过少？如果从数据层面来看，是过多还是过少呢？</p><p>好，我们来揭晓答案。如果模型层面思考，那是就是参数过多。如果从数据层面来看，那是数据过少。</p><p>现在我们需要理解一件事情，这两个事情其实是一回事，数据量多和模型复杂其实是一回事。它背后的原因就是因为任何一个f(x)如果有很多的参数，拟合的时候随着这个参数数量越多，那么我们所需要的训练数据集也要增多。也就是说当模型非常复杂，参数特别多，只要数据量特别大，那就不算多。就说现有的数据量对于参数不够，训练力度不够。</p><p>这就好比是有一个天才的孩子，脑子极其聪明，就跟茶桁一样。哎，这个孩子呢智商极其高，但是他想事情想的特别的复杂，结果他现在见到的事情都是太过于简单的东西。那么就不能把他的这个潜力发挥出来。</p><p>好，我们接着下一个问题：如何判断一件事情有没有发生过拟合或者欠拟合呢？</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195927.jpeg"alt="Alt text" /></p><p>我们看这张图，假如这是一个 2 分类问题，咱们训练时候结果的准确度是 0.7左右。那么大家想一下，这个是过拟合还是欠拟合呢？</p><p>如果模型训练的时候效果还不错，快接近于 1了，达到了百分之九十几。但是实际上用 validation数据集去测的时候发现准确度下到百分之八十几，或者百分之七十几，总之就是比在训练的时候那个效果要差。这个就叫作过拟合。</p><p>咱们上节课给大家说的就是这个问题，机器学习的整个流程最终的目的不是为了把loss 函数降到最低，我们要关心的是像recall，precition，这种信息才是最关键的。</p><h2 id="training-data-split">Training data split</h2><p>接下来，咱么要再讲几个机器学习里面极其重要的几个概念，第一个是数据集的切分(Training data split)。第二个是Normalization。第三个，Standardized。</p><p>其实上节课，咱们已经说过了数据集的切分问题。数据集切分最主要的原因是因为我们经常会遇见过拟合的情况，为了避免我们把所有的数据拿来不断的做training, 然后在使用的时候效果变得不好，那我们不如自己找一些数据出来做test sets，为了可以反复多次的去检验效果好不好，就增加了一个 validationsets。</p><p>在真实环境下我们是怎么去做这样一件事呢？我们来简单的演示下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>sample_data = np.random.random(size(<span class="hljs-number">100</span>, <span class="hljs-number">5</span>))<br><br>train, test = train_test_split(sample_data, train_size=<span class="hljs-number">0.8</span>)<br>train<br><br>---<br>array([[<span class="hljs-number">1.55582066e-01</span>, <span class="hljs-number">8.19437761e-01</span>, <span class="hljs-number">3.54628257e-02</span>, <span class="hljs-number">5.53248385e-01</span>,<br>        <span class="hljs-number">4.23785508e-01</span>],<br>...<br>       [<span class="hljs-number">7.24889349e-01</span>, <span class="hljs-number">1.23458057e-01</span>, <span class="hljs-number">9.74101303e-01</span>, <span class="hljs-number">1.72605427e-01</span>,<br>        <span class="hljs-number">6.59164912e-01</span>]])<br></code></pre></td></tr></table></figure><p>非常的简单，我们来看，<code>sklearn</code>里自带了这种分割方法。我们随机了100 行 5列的数据，然后使用<code>train_test_split</code>将其分割成<code>train</code>和<code>test</code>两份，在后面的参数内设置了百分位。</p><p>这样，这个数据就做了一个拆分。值得注意的是，给大家教一个小技巧，这是第一种方法：split。其实不只是sklearn，pytorch 和 keras 也都有 split 方法。</p><p>但是我们去看一下源码会发现，这个 split 方法是没有validation，它的输出只有 train 和 test 两部分。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195928.png"alt="Alt text" /></p><p>为了解决这个问题，我们可以用一个简单的方法。这次我们使用 Numpy。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">indices = np.random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sample_data)), size=<span class="hljs-built_in">int</span>(<span class="hljs-number">0.8</span>*(<span class="hljs-built_in">len</span>(sample_data))), replace=<span class="hljs-literal">True</span>)<br><br>indices<br><br>---<br>array([<span class="hljs-number">39</span>, <span class="hljs-number">65</span>,  <span class="hljs-number">5</span>, <span class="hljs-number">13</span>, <span class="hljs-number">69</span>,  <span class="hljs-number">8</span>, <span class="hljs-number">49</span>,  <span class="hljs-number">2</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">99</span>, <span class="hljs-number">13</span>, <span class="hljs-number">64</span>, <span class="hljs-number">76</span>, <span class="hljs-number">55</span>,<br>       <span class="hljs-number">96</span>, <span class="hljs-number">12</span>, <span class="hljs-number">87</span>, <span class="hljs-number">81</span>, <span class="hljs-number">55</span>, <span class="hljs-number">96</span>, <span class="hljs-number">54</span>, <span class="hljs-number">94</span>, <span class="hljs-number">15</span>, <span class="hljs-number">44</span>, <span class="hljs-number">23</span>, <span class="hljs-number">17</span>, <span class="hljs-number">76</span>, <span class="hljs-number">98</span>, <span class="hljs-number">84</span>, <span class="hljs-number">21</span>, <span class="hljs-number">50</span>,<br>       <span class="hljs-number">62</span>, <span class="hljs-number">58</span>, <span class="hljs-number">21</span>, <span class="hljs-number">95</span>, <span class="hljs-number">22</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">6</span>, <span class="hljs-number">35</span>, <span class="hljs-number">93</span>, <span class="hljs-number">34</span>, <span class="hljs-number">68</span>, <span class="hljs-number">49</span>, <span class="hljs-number">29</span>, <span class="hljs-number">81</span>, <span class="hljs-number">58</span>, <span class="hljs-number">45</span>, <span class="hljs-number">95</span>,<br>       <span class="hljs-number">26</span>, <span class="hljs-number">21</span>, <span class="hljs-number">97</span>, <span class="hljs-number">43</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span>, <span class="hljs-number">52</span>, <span class="hljs-number">93</span>, <span class="hljs-number">34</span>, <span class="hljs-number">17</span>, <span class="hljs-number">71</span>, <span class="hljs-number">76</span>, <span class="hljs-number">38</span>, <span class="hljs-number">92</span>, <span class="hljs-number">62</span>, <span class="hljs-number">21</span>, <span class="hljs-number">98</span>,<br>       <span class="hljs-number">56</span>, <span class="hljs-number">28</span>, <span class="hljs-number">54</span>, <span class="hljs-number">39</span>, <span class="hljs-number">15</span>, <span class="hljs-number">17</span>, <span class="hljs-number">62</span>, <span class="hljs-number">81</span>, <span class="hljs-number">61</span>,  <span class="hljs-number">4</span>, <span class="hljs-number">51</span>, <span class="hljs-number">71</span>])<br></code></pre></td></tr></table></figure><p>这里我们等于是把它的整体的顺序打乱，后面的 replace就是可以重复的去取。这样我们就随机的取了一些下标。</p><p>这是一个比较简单的方法，那么我们为什么要设置<code>replace=True</code>呢？当数量特别大的时候，多取几个少取几个其实不是很影响，另外replace的话，他内部的那个随机的算法其实是不一样的，速度会快的多。以后如果遇到类似的事情，你也可以去用这个方法去做它。</p><h2 id="normalization">Normalization</h2><p>除了这个以外，做机器学习的时候，要做数值的归一化 (Normalization)和标准化 (Standardized) 这样一个动作。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195929.png"alt="Alt text" /></p><p>我们这么做的目的是什么呢？假设我们现在有多个特征的数据集，不过我们注意到一点，就是这些特征值跨越的范围是无法进行比较的。</p><p>比如，一个特征在 1 和 10 之间变化，但是另外一个实在 1 和 1000之间变化。如果我们忽略了这一点而直接进行建模，模型分配给这些特征的权重将会受到严重影响，模型最终会为较大的变量分配较高的权重。</p><p>现在要解决这个问题，将这些特征置于相同或者至少是可比较的范围内，那就需要对数据做一个数据归一化。</p><p>归一化的目标是讲数据缩放到特定范围内，一般来说是[0,1]或者[-1,1]之间。这有助于消除不同特征之间的尺度差异，确保它们对模型的权重贡献大致相等。</p><p>数据归一化对于每个特征x，归一化后的值<code>Xnormalized</code>计算如下：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195930.png"alt="Alt text" /></p><p>其中 min 是特征的最小值，max是特征的最大值。这个操作确保了数据的最小值映射到 0，最大值映射到 1.</p><p>在数据预处理过程中，首先计算每个特征的最小值和最大值，然后使用上述公式对数据进行归一化。这通常通过一次遍历数据来实现。</p><p>在进行归一化的时候，我们所使用的那个公式会有一个缺点，就是它并不能很好的去处理异常值。比方说，如果有0 到 40 之间的 99 个值，其中一个值为 100，则这 99 个值讲全部转换为 0 到0.4 之间的值。这些数据和以前一样被压缩！下图就是个示例：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195931.png"alt="Alt text" /></p><p>这些数据在进行归一化之后，解决的是 y 轴上堆集的问题，但是 x轴上的问题依然存在，就像途中橙色点那个异常值：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195932.png"alt="Alt text" /></p><p>关于这个知识点，我们来看一个极其简单的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">some_large_number = [<span class="hljs-number">23421421</span>,<span class="hljs-number">42155151</span>,<span class="hljs-number">25531238</span>,<span class="hljs-number">21826139</span>, <span class="hljs-number">32189732</span>, <span class="hljs-number">32103721</span>]<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">normalize</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> (x - np.<span class="hljs-built_in">min</span>(x)) / (np.<span class="hljs-built_in">max</span>(x) - np.<span class="hljs-built_in">min</span>(x))<br>ic(normalize(np.array(some_large_number)))<br><br>---<br>array([<span class="hljs-number">0.07847317</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.18225672</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.50979325</span>, <span class="hljs-number">0.5055623</span> ])<br></code></pre></td></tr></table></figure><p>我手动定义了 6个比较大的数字，在进行处理之后我们看到了，都变成了一些特别小的数字。</p><p>同样的，对于特别小的数字，它一样可以进行处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">some_small_number = [<span class="hljs-number">0.00000231213</span>,  <span class="hljs-number">0.0005600321</span>, <span class="hljs-number">0.0000041412892</span>, <span class="hljs-number">0.000987890576</span>, <span class="hljs-number">0.0000578921764</span>]<br>ic(normalize(np.array(some_small_number)))<br><br>--- <br>array([<span class="hljs-number">0.</span>, <span class="hljs-number">0.56588085</span>, <span class="hljs-number">0.00185592</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.05639333</span>])<br></code></pre></td></tr></table></figure><h2 id="standardized">Standardized</h2><p>那么还有就是标准化，对于标准化，其目标是讲数据转化为均值为0，标准差为 1的分布，也就是标准正态分布。这有助于处理偏斜分布的数据，并确保数据的均值和方差在模型中起到合适的作用。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195933.png"alt="Alt text" /></p><p>那对于每一个特征 x，标准化的值<code>z</code>计算如下：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195934.png"alt="Alt text" /></p><p><span class="math inline">\(\mu\)</span>是特征的均值，<spanclass="math inline">\(\sigma\)</span>是特征的标准差。这个操作使数据的均值为0，标准差为 1。</p><p>在数据预处理的过程中，首先计算每个特征的均值和标准差，然后使用上述公式对数据进行标准化处理。标准化后的数据具有均值0 和标准差 1，这有助于模型更好的理解和捕捉数据之间的关系。</p><p>无论是归一化还是标准化，其实依据来源都是基于线性代数的变化理论，这确保了归一化和标准化后的数据分布具有特定的属性，这些属性对于机器学习算法的表现非常有帮助。</p><p>我们来看一个标准化的例子，为了让大家更为明显的了解其意义，我做了一些非常大的数据，但是每一个都不相同。这些数据有一个特点，就是相对于数值本身的大小来说，几个数值之间的差距可以说是非常微小的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">some_dense_number = [<span class="hljs-number">47238941</span>, <span class="hljs-number">47238946</span>, <span class="hljs-number">47238951</span>, <span class="hljs-number">47238931</span>, <span class="hljs-number">47238949</span>, <span class="hljs-number">47238936</span>]<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">standarlize</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> (x - np.mean(x))/ np.std(x)<br><br>ic(standarlize(np.array(some_dense_number)))<br><br>---<br>array([-<span class="hljs-number">0.18752289</span>,  <span class="hljs-number">0.51568795</span>,  <span class="hljs-number">1.2188988</span> , -<span class="hljs-number">1.59394459</span>,  <span class="hljs-number">0.93761446</span>, -<span class="hljs-number">0.89073374</span>])<br></code></pre></td></tr></table></figure><p>我们定义的数据实际上是非常密集，但是使用 standarlize公式之后，就变得比较的分散，比较的均匀了。这个情况还是很多的。</p><h2 id="one-hot">ONE-HOT</h2><p>在讲完 training data split, normalization, Standardized之后，我们来看下面一点：ONE-HOT。</p><p>为什么要用ONE-HOT？我们都直到，咱们计算机里其实都是数字，包括视频，图片，声音，文字等其实都是数字。</p><p>数字和数字其实是不一样的。比如，有一群人分成了<code>4</code>组：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195935.png"alt="Alt text" /></p><p>然后有一个女生的 GPA 是<code>4</code>:</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195936.png"alt="Alt text" /></p><p>那么分组的<code>4</code>和 GPA的<code>4</code>有什么区别？最明显的一个区别就是，分组的<code>4</code>只是一个组名，那么假如和<code>1</code>组交换组名并没有太大的关系，但是GPA 的这个<code>4</code>如何和<code>1</code>交换一下，那就从 4 分变成 1分了，那这两个是不能相互变换的。本质上，其区别就是一个可比一个不可比。</p><p>我们也就发现了，数字其实是有区别的。这个世界中，数字其实可以分成两类：</p><p>第一类叫作Categorical，叫作分类数据，也被称为离散数据或名义数据。它们之间不能被比较，也不能被排序，这些数字也仅仅是表示一个和另外一个不一样。就我们刚才讲人群分为1、2、3、4 组，其实分成 A、B、C、D 组也是一样的，只是表示区别。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195937.png"alt="Alt text" /></p><p>第二类是Numerical，数值数据，也被称为连续数据。这个是可以比较的，也可以进行排序。这种数据包括可以用来进行数学运算的实数值。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195938.png"alt="Alt text" /></p><p>Numerical 还可以进一步分为整数和浮点数。</p><p>知道了这一点之后，那我们以后遇到类似的情况不要随便的做加减乘除。</p><p>那我们有了 Categorical 和 Numerical这两种类型之后，会对我们有一些什么比较重要的影响？</p><p>如果现在有一个函数，这个函数输入一个 x 向量，它输出就是分为一个Categorical 和 numerical。</p><p>输出是 0-1 这样一个数字，是一个典型的逻辑回归。</p><p>假如有一个人在北京，年龄27，性别男，月入一万二。然后还有一个人，生活在安徽，年龄28，性别女，月入 8,000。第三个住在上海，年龄28，性别男，月入一万三。</p><ol type="1"><li>北京，27, 12000</li><li>安徽，28, 8000</li><li>上海，28, 13000</li></ol><p>我们注意这三组数据，如果现在做一个向量表证。</p><p>关于地域，我们常常使用的方法包括邮编排序，或者使用拼音排序。假如这里我们就使用拼音首字母来进行排序，安徽假如是1，北京是 2，上海是 27。</p><p>我们的数据进行向量化可能就会变成下面这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. 北京</span><br>vec(<span class="hljs-number">2</span>, <span class="hljs-number">27</span>, <span class="hljs-number">12000</span>)<br><span class="hljs-comment"># 2. 安徽</span><br>vec(<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">8000</span>)<br><span class="hljs-comment"># 3. 上海</span><br>vec(<span class="hljs-number">27</span>, <span class="hljs-number">28</span>, <span class="hljs-number">13000</span>)<br></code></pre></td></tr></table></figure><p>然后我们定义一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> (<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>非常简单一个函数，返回表示对某一样东西买还是不买。</p><p>函数的实现过程就是类似于<code>wi * xi + b</code>这种形式。</p><p>我们观察向量发现，就向量值而言，北京这个人和安徽这个人之间的向量差比北京和上海这两人之间的向量差还要小。</p><p><span class="math display">\[|v_1 - v_2| &lt; |v_1 - v_3|\]</span></p><p>我们假如说经过函数<code>f(x)</code>之后，输出的结果分别为 Y1, Y2,Y3。因为 v1 和 v2 离的更近，就会有一个结果，Y1 和 Y2的结果其实会更相似。但是其实呢，这种结果完全不对。这样乱比其实会出问题，会让程序出错。</p><p>我们现在知道，这其实是一个 Categorical 的问题。为了解决 Categorical的这种问题，我把 Categorical 改成这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">北京: [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]<br>安徽: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]<br>上海: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><p>改成这样之后这个向量就变成了这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. 北京</span><br>vec(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">27</span>, <span class="hljs-number">12000</span>)<br><span class="hljs-comment"># 2. 安徽</span><br>vec(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">28</span>, <span class="hljs-number">8000</span>)<br><span class="hljs-comment"># 3. 上海</span><br>vec(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">13000</span>)<br></code></pre></td></tr></table></figure><p>向量变成这样之后，就解决了我们刚刚说的那个问题。不会导致因为分类过于相似让北京和安徽向量相似度大于北京和上海的相似度。</p><p>对于这样一个向量，三组数据中改变的那个值向量值就都为<spanclass="math inline">\(\sqrt 2\)</span>，这一种方式就被称为 ONE-HOT。</p><p>那这种方式也是存在问题的，目前我们只去考虑三个城市。可是当存在成百上千个城市的时候，比如说Google 地图等等这些应用。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195939.png"alt="Alt text" /></p><p>当城市越来越多的时候，那它的维度就会变得很高：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">Beijing     = [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>]<br>Shanghai    = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>]<br>Chengdu     = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>]<br>Shenzhen    = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>]<br>...<br></code></pre></td></tr></table></figure><p>我们想想一下，这样得有多少个地址？可能空间会极其的大，你这样的话数字光存起来得上亿个存储单元。</p><p>ONE-HOT 就有这样的问题：</p><ol type="1"><li>耗费空间</li><li>数据量大，更新起来，效率极低</li><li>遗漏了很多重要新息</li></ol><p>就比如，我们再增加几个人如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">- 重庆 <span class="hljs-number">27</span> <span class="hljs-number">9000</span><br>- 成都 <span class="hljs-number">26</span> <span class="hljs-number">8500</span><br>- 呼和浩特 <span class="hljs-number">26</span> <span class="hljs-number">8500</span><br></code></pre></td></tr></table></figure><p>在这三个城市中，我们脑子里其实就直到，重庆和成都是非常接近的。但是在ONE-HOT 里是体现不出来，其向量值依然是根号 2。</p><p>为了解决这些问题，人们就用到了更先进的一种方法：embedding，叫作嵌入。</p><p>嵌入就是把东西放在固定的位置，这个就是嵌入的意思。在这里，就我们空间中如果有几个实体NTT1 NTT2 NTT3，我们把这些实体放到这个空间中，要达到一个结果就是如果实体1 和实体 2 的相似度小于实体 1 和实体 3的相似度，这个相似度我们可以自己来定义，比如成都和重庆的生活方式，再比如重庆和北京都是直辖市。</p><p>在这个问题场景下，我们期望达到的结果是如果这两个实体相似那么他们在空间中的距离也接近。</p><p>如何实现 Embedding,这本身是一个研究领域，是现在非监督学习，表证学习里面非常重要的一个研究领域，属于比较高级的一个知识点。</p><p>第二就是如果之后咱们学NLP，那么一定会讲到这个，因为要把文本单词进行嵌入，到时候会学到。如果是学推荐系统的，大家也会学什么Graph embedding，基于图的用户行为。</p><p>那之后咱们学习 NLP，其基础就是Embedding。关于这个问题，我们其实目前了解到这里就行了。再往下延展下去，又是一个专门的研究话题。延展后的这个问题解决方案，在我们后面的课程中会等着大家去学习。</p><p>我们再来看看 ONE-HOT 的实际展示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">array = [<span class="hljs-string">&#x27;北京&#x27;</span>,<span class="hljs-string">&#x27;上海&#x27;</span>,<span class="hljs-string">&#x27;广州&#x27;</span>,<span class="hljs-string">&#x27;宁夏&#x27;</span>,<span class="hljs-string">&#x27;成都&#x27;</span>,<span class="hljs-string">&#x27;上海&#x27;</span>,<span class="hljs-string">&#x27;北京&#x27;</span>]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">one_hot</span>(<span class="hljs-params">elements</span>):<br>    pure = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(elements))<br>    <br>    vectors = []<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> elements:<br>        vec = [<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(pure)<br>        vec[pure.index(i)] = <span class="hljs-number">1</span><br>        vectors.append(vec)<br><br>    <span class="hljs-keyword">return</span> vectors<br>ic(one_hot(array))<br><br>---<br>one_hot(array): [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>                [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>                [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>                [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>                [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>                [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>                [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]]<br></code></pre></td></tr></table></figure><p>其实 ONE-HOT非常简单，但是基本上很多面试官都喜欢问这个问题。这个问题主要就是一个可以考察一下你的Python 编程能力，其次他可以去问一下你 one hot的作用是什么，再者还可以往后问你 one hot有什么缺点，怎么解决等等。一个问题就可以问你半个小时。</p><h2 id="补充softmax-和-cross-entropy">补充：SOFTMAX 和CROSS-ENTROPY</h2><p>好，在本节课最后，我们来做一个前面课程的补充，在今天才想起来，有一个相关的点遗漏了没有讲到。</p><p>之前我们讲过逻辑回归的 loss 函数：</p><p>假如 y=1，loss 可以等于-log(yhat), 如果 y 等于 0，loss就可以写成-log(1-yhat)。两个合并后就组成了最终的 loss 函数：</p><p><span class="math display">\[loss = -(ylog\hat y + (1-y)log(1- \hat y))\]</span></p><p>那么，这个是解决二分类的，结果才不是 0 就是1。现在的问题就是如果我们要解决多分类的问题怎么办。</p><p>如果要解决多分类的话，需要把 x 变成一种能预测多分类的东西。那最终yhat 可以表示成 <span class="math inline">\(\hat y = (0.25, 0.20,0.75)\)</span>。</p><p>也就是，现在要表示三个类别，那我们可以用三个小数来表示。这个向量经过各种计算，如果能够变成一个三维的向量，然后再去优化里边的参数就可以做到。</p><p>那这也就代表的是类别 1、类别 2、类别 3 的概率。ytrue 就可以写成 yhat的形式，就变成 (1, 0, 0)。</p><p>就是我们给定一个<span class="math inline">\(\vec x\)</span>, 它实际的y 是 (1, 0, 0)，那么 yhat 就是估计值等于 0.25、0.20 和0.75。然后对比一下两组数据之间的差别，这样我们就可以优化其中的形成参数(w, b)。</p><p>通过不断优化，就可以计算到更接近于 (1, 0, 0) 这样的值。</p><p>首先就是怎么样把 x 向量变成 3 维的。</p><p>这个其实不难，如果 x 是 10 维的，1<em>10。那么给他再乘以一个 10</em>3的矩阵，它最后就会变成一个 1 行乘 3 列的矩阵。</p><p>那么现在假如说现在有这样一个 x:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = [<span class="hljs-number">1231</span>, <span class="hljs-number">12314</span>, <span class="hljs-number">4341</span>, <span class="hljs-number">1542</span>, <span class="hljs-number">4123</span>, <span class="hljs-number">4512</span>, <span class="hljs-number">3213</span>, <span class="hljs-number">1241</span>, <span class="hljs-number">1231</span>, <span class="hljs-number">6842</span>]<br></code></pre></td></tr></table></figure><p>然后我们来做这样一件事：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">x = np.array(normalize(x))<br>weights = np.random.random(size=(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>))<br>np.dot(x, weights)<br><br>---<br>array([<span class="hljs-number">0.86907231</span>, <span class="hljs-number">1.32234548</span>, <span class="hljs-number">0.88170994</span>])<br></code></pre></td></tr></table></figure><p>这样，我们就生成了一个维度是 3的一串数字。在机器学习里面，我们把这个叫做算子：logits。</p><p>现在我们将一个 10 维的 x 变成了一个 3 维的logit，下一步我们就要考虑，怎么将这个 logit 变成一个概率分布呢？</p><p>我们就要用到一个和逻辑函数特别像的一个函数，Softmax：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195940.png"alt="Alt text" /></p><p>我把它写出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">logits = np.dot(x, weights)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> np.exp(x) / np.<span class="hljs-built_in">sum</span>(np.exp(x))<br><br>ic(softmax(logits))<br><br>---<br>array([<span class="hljs-number">0.27884889</span>, <span class="hljs-number">0.43875588</span>, <span class="hljs-number">0.28239524</span>])<br></code></pre></td></tr></table></figure><p>这样，我们输入的是 logits，输入到 Softmax，输出的就是概率了。</p><p>输出成概率之后，我们定义一个依然和逻辑函数很像的一个函数，叫做Cross-entropy。</p><p>我们刚才使用 softmax 输出的数组就是概率，也就是估算的 yhat。这个Cross-entropy 的 loss 就是：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195941.png"alt="Alt text" /></p><p>求得 loss，然后再对 x 求偏导，就可以通过梯度下降让输入的 x得到和真正的 y 相近的 yhat。</p><p>那我们将 cross-entropy 也写一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy</span>(<span class="hljs-params">yhat, y</span>):<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(y_i * np.log(yhat_i) <span class="hljs-keyword">for</span> y_i, yhat_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(y, yhat))<br></code></pre></td></tr></table></figure><p>现在我们需要一组真正的y，也就是真实值，和我们预测房价时所使用的真实值是一样的东西，只是现在我们的y 的维度不太一样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">y = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p>接着我们使用<code>cross_entropy</code>将我们之前使用 softmax计算的概率分布和真实的 y 放进去：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">ic(cross_entropy(softmax(logits),y))<br><br>---<br><span class="hljs-number">1.040664959870481</span><br></code></pre></td></tr></table></figure><p>这个时候我们就得到了一个 loss 值。</p><p>我们现在去给 weights 求偏导。然后通过不断的迭代，就能找到一组 wi，和x 进行点乘就能够生成和 y 接近的值。</p><p>以上这些就是 softmax 和 cross-entropy 的作用。</p><p>cross-centropy 就是用来衡量产生的 yhat 和 y之间的相似程度差距的。Softmax是把任意的一组数字变成概率分布，然后这个概率分布就可以送到 loss函数里面和实际上的 y 进行对比。</p><p>Softmax 有这么几个特性，它的结果是一个典型的概率分布。还有就是Softmax 中有 e 的 n 次方，可以把 Max 变得更大。除了把 Max变得更大，还保留原来小的数字。</p><p>理论上完全可以找别的函数代替，计算机里边很多东西，只要好用就行。这就是放大特征，正是面对多分类任务的一个做法。</p><p>Softmax在实现的时候有个坑稍微要注意一下，在实现的时候我们多加一句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">x</span>):<br>    x = np.array(x)<br>    x -= np.<span class="hljs-built_in">max</span>(x) <span class="hljs-comment"># 多加这么两句</span><br>    <span class="hljs-keyword">return</span> np.exp(x) / np.<span class="hljs-built_in">sum</span>(np.exp(x))<br><br>ic(softmax(logits))<br></code></pre></td></tr></table></figure><p>首先，如果 x 的输入是一个 array就不用管了，但是如果不是，我们就要强制转换一下。</p><p>下一句代码是因为 e 的 x次方可能非常的大，但是我们计算机的存储是有限的，最大只能表示 2^63的数字，再大就表示不了了。所以我们就需要这样一段代码来处理一下，让最后结果的数字不要那么大。</p><p>好，那这一节课的内容到这里也就结束了。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195926.png&quot;
alt=&quot;茶桁的 AI 秘籍 核心基础 13&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;上一节课，咱们讲解了『拟合』，了解了什么是过拟合，什么是欠拟合。也说过，如果大家以后在工作中做的就是机器学习的相关事情，那么欠拟合和过拟合就会一直陪伴着你，这两者是相互冲突的。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>12. 机器学习 - 拟合</title>
    <link href="https://hivan.me/12.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%8B%9F%E5%90%88/"/>
    <id>https://hivan.me/12.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%8B%9F%E5%90%88/</id>
    <published>2023-10-22T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:57.697Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021148.png"alt="茶桁的 AI 秘籍 核心基础 12" /></p><p>Hi, 你好。我是茶桁。</p><p>这一节课一开始我们要说一个非常重要的概念：拟合。</p><span id="more"></span><h2 id="拟合">拟合</h2><p>相信只要你关注机器学习，那么多少在某些场合下都会听到拟合这个概念。</p><p>什么叫做拟合，什么叫做过拟合或者欠拟合呢？</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021056.png"alt="Alt text" /></p><p>假如有一个模型，这个模型在训练数据的时候效果很好，体现在 loss很小，或者说 precision 很高，accuracy也比较好，但是在实际情况下，用到没有见过的数据的时候，效果就很差，那么这个就过拟合了。</p><p>在这个过程中，要主一的是仅当数据 label 比较均衡的时候，才有必要使用acc.</p><p>我们来看三条曲线：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021057.png"alt="Alt text" /></p><p>第一个图是比较合理的参数模型，第二个图就是过拟合的参数模型。</p><p>因为对数据过度拟合，当有新的点出现的时候，函数的趋势和新的点并不匹配。那过度拟合就会对于未来的点就预测不对了。为了在训练的时候效果很好，在见过的数据里效果特别好，结果在新的未见过的数据里，效果就很差。</p><p>第三个图就是欠拟合的状态。训练的时候这个效果就不好，整个接近程度就不高。</p><p>比较好的场景就是第一张图的拟合状态，其形成了一个合理的参数模型。在训练的时候拟合也没有那么高，实际中的结果会发现结果也没那么差。这其实也就暗合了我们前几节课里所讲的[奥卡姆剃刀原理].</p><p>过拟合和欠拟合这两个概念，在我们平时的工作中会是每天都要一直取解决的问题。遇到一个问题，训练的时候效果很差这个欠拟合，经过了很多调试结果发现效果还不错，结果在实际问题中发现效果很差，这个就是过拟合。</p><p>这两件事情其实它是互相冲突的，这个可以通过 loss 来判断，也可以通过percision 来判断，只不过在计算新问题的时候，不存在 lose函数这回事儿。就是当你把模型已经训练完了，去用真实数据做测试了，那个时候是不存在loss 函数的。</p><p>在整个机器学习的发展历程中，我们一直在不断的做的事情就是怎么样提高欠拟合的准确度，同时降低过拟合。</p><p>影响过拟合和欠拟合原因有很多，既和数据有关系，也和模型有关系。但是在这个过程中有一点大家需要注意。所有的机器学习任务里边，在我们收集数据的时候，有一个很重要的问题就是异常值对过拟合和欠拟合影响会很大。</p><h2 id="outliner">OUTLINER</h2><p>有一本书就叫《Outliner》(异类), 大家有空可以去看一下。</p><p>outliner 为什么会对我们整个值影响很大呢？</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021058.png"alt="Alt text" /></p><p>咱们来看这个图，本来正常的线性走向应该是右边这张图。可是因为存在异常值的情况，所以导致线性偏向左边这张图的情况。可是在我们的数据中，这种极端的异常值属于少数，并且因为数值偏差过大，就导致整体趋势的偏斜。</p><p>那么我们怎么样去判断异常值呢？为了把模型做好，从一开始收集数据以及清洗数据的时候就要把那些异常值给它去掉。</p><p>所谓异常值是没有一个标准定义的，但是在数学上会有一个比较常见的去除方法，就是利用百分位，常见的方法就是按百分位来解决数值型问题。</p><p>numpy 里有一个<code>persontile</code>，它接受一个array，和一个浮点值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.percentile(np.array([]), number)<br></code></pre></td></tr></table></figure><p>那这个 percentile 是干嘛的呢？比如下面这张图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021059.png"alt="Alt text" /></p><p>把所人从左到右排排队，比方说第 80 分位，就是第 80%的是那一个。而我们如果是要处理数据的话，会让这些人按数值大小来排队，比如按身高，那就是最矮的在最左边。</p><p>一般统计学上常见的几个数字，一个是 0.5，还有 0.25 和 0.75。</p><p>比如我们之前的 lstat 数据，我们来找一下其中的异常值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>np.where(np.array(lstat) &lt; np.percentile(np.array(lstat), <span class="hljs-number">0.25</span>) / <span class="hljs-number">1.5</span>)<br><br>---<br>(array([], dtype=int64),)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">np.where(np.array(lstat) &gt; np.percentile(np.array(lstat), <span class="hljs-number">0.75</span>) * <span class="hljs-number">2.5</span>)<br><br>---<br>(array([  <span class="hljs-number">1</span>,   <span class="hljs-number">6</span>,   <span class="hljs-number">7</span>,   <span class="hljs-number">8</span>,   <span class="hljs-number">9</span>,  <span class="hljs-number">10</span>,  <span class="hljs-number">11</span>,  <span class="hljs-number">12</span>,  <span class="hljs-number">13</span>,  <span class="hljs-number">14</span>,  ...<br><span class="hljs-number">500</span>, <span class="hljs-number">501</span>, <span class="hljs-number">502</span>, <span class="hljs-number">505</span>]))<br></code></pre></td></tr></table></figure><p>在寻找极大值的时候，我们找到了一堆的数字。如果当异常值比较多的时候，很难把它们定义成异常值。我们总结异常值规律的时候，其实它的和周围的信状它很不一样。</p><h2 id="bias-and-variance">BIAS AND VARIANCE</h2><p>好，再接下来我们来一起看看 BIAS 和 VARIANCE。</p><p>在整个机器学习过程中，我们持续的有一个问题就是它的过拟合和欠拟合一直在互相PK。那么不管是欠拟合比较严重还是过拟合比较严重，这都是问题。但这两种问题在统计学里有两个名字。</p><blockquote><p>The bias is an error from erroneous assumptions in the learningalgorithm. High bias can cause an algorithm to miss the relevantrelations between feature and target outputs. (underfitting);</p></blockquote><p>我们把欠拟合这种问题叫做偏见，BIAS 叫做偏见。</p><p>假如说对于一个人来说，你对一件事情的判断判断错了，有BIAS，就是有偏见。这个就是你的脑子对这件事情的抽象程度不够，脑子的判断模型就错了。所以效果就不好。</p><p>就比方说分明是一个二次函数，你硬是要拿直线去怼出来，那你怎么怼？这就叫BIAS。</p><p>而 VARIANCE 是什么呢？它指的是你的那个变化太大。</p><blockquote><p>The variance is an error from sensitivity to small fluctuations inthe training set. High variance can cause an algorithm to model therandom noise in the training data, rather than the intended outputs(overfitting).</p></blockquote><p>也就是说，这个训练集对未来比较敏感，实际的值稍微有一点不一样就会产生很差的结果。高VARIANCE 最后会导致模型产生的结果都很随机，效果很差。</p><p>产生 VARIANCE 的背后有一个很重要的特性，就是模型复杂度。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021100.png"alt="Alt text" /></p><p>随着模型越来越复杂，它的 BIAS 会越来越低，就是模型越来越复杂。</p><p>就比方下面这个图中的模型：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021101.png"alt="Alt text" /></p><p>右边的这个曲线就是用了一个很复杂的模型，它的 BIAS 就会很低，BIAS低的时候它的 VARIANCE 就会变大。</p><p>因为模型复杂，所以之后问题稍微变化了，这个模型就会产生出来变化比较大的输出值。而模型越简单，BIAS就会越高，对于未来也会变化没有那么大。</p><p>这是一个非常重要的一个 Dilemma，是一个两难问题，进退两难的一个问题叫Dilemma。</p><p>这两类错误背后其实都是和我们的模型复杂程度有关。</p><p>那么讲到这里我们就可以来谈谈，BIAS 和VARIANCE，过拟合和欠拟合背后的原因有哪些。</p><table><thead><tr class="header"><th>过拟合 overfitting</th><th>欠拟合 Underfitting</th></tr></thead><tbody><tr class="odd"><td>训练数据占总体数据过少</td><td>训练数据占总体数据过多</td></tr><tr class="even"><td>模型过于复杂</td><td>模型过于简单</td></tr><tr class="odd"><td>采样过于不均衡</td><td></td></tr><tr class="even"><td>没有正则化...</td><td></td></tr></tbody></table><p>模型过于简单可能会产生欠拟合情况，模型过于复杂呢有可能会产生一个过拟合的情况。如果产生了过拟合，还有一个很重要的特点就是有可能模型采样非常不均衡。</p><p>模型复杂不复杂，单不简单，采样均衡不均衡。其实背后都有一个重点叫做『训练数据占总体数据的比例』，就是训练数据是不是够多。</p><p>所谓的采样均衡不均衡，异常值的出现最终都指向的是一个问题，就是我们的训练数据不够多。</p><p>为什么训练数据不够多会引起模型过于复杂之类的情况呢？在整个机器学习里是个非常重要的概念，叫做维度灾难。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021102.png"alt="Alt text" /></p><p>我们举例来说明下，就比如，在一个平面坐标轴上有几百个点，我们用两点去确定了一根直线，但是这根直线并不能代表这几百个点确定的走向，我们需要去确定更多的点来调整这根线。</p><p>但是如果是三维轴上，我们为了去确定一个平面，就需要更多的点，比二维轴上确定直线的点多的多。为了能更加精确地确定一个平面，需要更多的数据才行。</p><p>在机器学习中算是一个经验，当机器学习的维度每增加一个，那么所需要的样本量基本上要增加一个数量级。</p><p>举个例子，假如班上有一个老师，这个老师要预测同学能不能考上重点大学。假如现在是高一，还没有开始考试，他预测这个同学有这么几个情况：</p><ul><li>第一个情况是这个同学做作业的情况；</li><li>第二个情况是这个孩子每天上课时回答问题的活跃程度</li></ul><p>假如是这两个点，如果他靠这两个 features来预测这个孩子能不能考上大学，准确度能够做到不错，假如到百分之八九十，他需要50 个学生能够预测。</p><p>那么现在又加一个 features，这个 features还是和前两个值不相关的，又加了一个孩子的家庭收入情况。那么它得变成上百个学生数据才能预测对。因为每增加一个features，在这个世界中就会增加很多不确定的情况。</p><p>所以模型之所以过于复杂，其实背后本质上还是数据量太少。</p><p>就在我们今天大数据的情况下，模型其实进步的并没有非常大，但是数据量变大之后，整个的效果就好多了。在机器学习里有一个点就是算法再好，模型再好，抵不过不过数据量大。</p><p>真正工作、学习的时候，一定要想办法提前检测出来过拟合、欠拟合的情况。为了提前检测出这些，有一个很简单的方法。</p><p>假如现在给了许多的训练数据，不要把这训练数据全部拿上做完。而是选一部分，拿其中的一部分数据做训练，比方80%。然后，剩下 20% 不给模型去看，然后把模型拿到这 20%上去看一下结果。这样，我们就可以测试出结果。这就叫做训练集和数据集。</p><p>为什么要有训练集和测试集呢？有一个非常极端的情况，如果不做训练集和数据集想获得好的结果，直接把所有label 对应的值记下来，也就是小时候我们背诵古诗散文。</p><p>你的模型只会阅读并背诵全文，那想想训练的时候，acc、precision、recall等评测指标就非常高，但是效果就很差，过拟合的情况就会很严重。</p><p>在实际的工作中，除了<code>train_set</code>和<code>test_set</code>，还有一个值叫<code>validation_set</code>。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021103.png"alt="Alt text" /></p><p>早些年的时候，做测试数据集只会分成训练集和测试集。然后在训练集上去训练，完事在测试集上去测试。但是人们发现一个情况，比方说在测试集上训练完了之后，在测试集上发现效果不太好，过拟合有点严重，于是分析test 数据哪里做错了，找到错误之后修改代码或者修改数据。</p><p>就好比一个同学做题的时候有十套卷子，他做完 8套，留了两套。再做这两套之后发现哪里不太对，然后反反复复去观察后面这两套，也就是我们的test数据。这个时候其实是在做针对性的调整，在有针对性的解决问题，整体的能力并没有提升。</p><p>为了解决这个问题，在实际的工作中我们会把数据集分成三个数据集。</p><p>训练数据集不断的去训练，然后结果给到 validation set这样一个小数据集里。我们去观察 validation 的结果，再去调整，当validation的结果很不错的时候我们拿一套完全没做过的题来检验。这个完全没见过的题就是我们test set 了。</p><p>当 test set用过之后，如果考试成绩太差，那只能把数据集打乱，再重新取一份新的 test数据了。因为这个时候如果再去有针对性的调整模型结果，那其实是在手动过拟合了。</p><p>在这个过程中，假设一共有 100 个数据，test 里有 20 个，validation 有10 个，train 里有 70个。为了尽可能多的把所有的数据都用上，把它的效率都发挥上，有一个很简单的操作：crossvalidation，也叫做交叉验证。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021104.png"alt="Alt text" /></p><p>如图，假如我把数据分成很多份，我让其中一部分做 validation数据集，下一次训练的时候，我再让另外一部分做 validation 数据集。</p><p>再回过头来看我们之前见过的机器学习的通用框架，这几节课学习了评价指标之后，我们就应该知道，在这个背后多了一个acc 和 precision，我们要持续的去观测它的结果。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021105.png"alt="Alt text" /></p><p>在这个过程中有了数据，然后定义一个𝜃，这个𝜃就是我们的参数。然后根据loss function，gradient descent 不断优化这个参数。结果并不是要一个低的loss 参数，是期望有一个好的 acc 和precision。这个是在之前的基础上完善的整个学习过程。</p><p>好，下节课呢，咱们来看看 FEATURE SCALING，特征缩放。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021148.png&quot;
alt=&quot;茶桁的 AI 秘籍 核心基础 12&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi, 你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;这一节课一开始我们要说一个非常重要的概念：拟合。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>11. 机器学习 - 评价指标 2</title>
    <link href="https://hivan.me/11.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%872/"/>
    <id>https://hivan.me/11.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%872/</id>
    <published>2023-10-19T23:30:00.000Z</published>
    <updated>2023-11-25T04:41:28.659Z</updated>
    
    <content type="html"><![CDATA[<p>Hi, 你好。我是茶桁。</p><p>上一节课，咱们讲到了评测指标，并且在文章的最后提到了一个矩阵，我们就从这里开始。</p><span id="more"></span><h2 id="混淆矩阵">混淆矩阵</h2><p>在我们实际的工作中，会有一个矩阵，这个矩阵是分析结果常用的。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231019125248.png"alt="Image 2023-10-18 192838.png" /></p><p>我们来看看具体是什么意思。</p><p>所谓的<code>True condition</code>,指的是真实值，<code>Predicted condition</code>，指的是预测值。</p><p>其中行表示，<code>Predicted condition positive</code>表示预测值是1，<code>Predicted condition negative</code>表示预测值是 0。</p><p>列表示则为：<code>Condition positive</code>表示真实值是 1，<code>Condition negative</code>表示真实值是 0。</p><p>这样行列交叉就组成了这样一个矩阵。这个矩阵叫做混淆矩阵，英文名字叫做Confusion Matrix.</p><p>这个混淆矩阵是什么意思呢？</p><p><code>True Positive</code> 意思就是预测值是1，预测对了，<code>True negative</code>意思是预测值是0，预测对了。那相对的， <code>False positive</code>意思就是预测值是1，预测错了， <code>False negative</code>意思就是预测值是0，预测错了。</p><p>混淆矩阵在常见的机器学习里边是一个很重要的分析工具：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix<br>confusion_matrix(true_labels, predicated_labels)<br><br>—<br>array([[<span class="hljs-number">59</span>,  <span class="hljs-number">6</span>],<br>       [ <span class="hljs-number">6</span>, <span class="hljs-number">29</span>]])<br></code></pre></td></tr></table></figure><p>我们可以直接看看这个方法的源码里有相关说明：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">??confusion_matrix<br><br>---<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">confusion_matrix</span>(<span class="hljs-params"></span><br><span class="hljs-params">    ...</span><br><span class="hljs-params">    the count of true negatives <span class="hljs-keyword">is</span> :math:`C_&#123;<span class="hljs-number">0</span>,<span class="hljs-number">0</span>&#125;`, </span><br><span class="hljs-params">    false negatives <span class="hljs-keyword">is</span> :math:`C_&#123;<span class="hljs-number">1</span>,<span class="hljs-number">0</span>&#125;`, </span><br><span class="hljs-params">    true positives <span class="hljs-keyword">is</span> :math:`C_&#123;<span class="hljs-number">1</span>,<span class="hljs-number">1</span>&#125;` </span><br><span class="hljs-params">    false positives <span class="hljs-keyword">is</span> :math:`C_&#123;<span class="hljs-number">0</span>,<span class="hljs-number">1</span>&#125;`.</span><br><span class="hljs-params">    ...</span><br></code></pre></td></tr></table></figure><p>tp 实际上是<code>1</code>预测值是<code>1</code>，tn实际是<code>0</code>预测是<code>0</code>, fp实际是<code>0</code>预测是<code>1</code> fn实际是<code>1</code>预测是<code>0</code>。</p><p>這個時候我們再回頭來看上节课结尾处的那个公式：</p><p><span class="math display">\[\begin{align*}Precision &amp; = \frac{tp}{ tp + fp} \\Recall &amp; = \frac{tp}{tp + fn}\end{align*}\]</span></p><p>很多人看到这个就有点晕，其实很简单。切换成我们刚才查看源码时查询到的就就成了这样：</p><p><span class="math display">\[\begin{align*}Precision &amp; = \frac{C(0, 0)}{ C(0, 0) + C(1, 0)} \\Recall &amp; = \frac{C(0, 0)}{C(0, 0)+ C(0, 1)}\end{align*}\]</span></p><p>tp 是实际上是 positive, 预测也是 positive. fp 就是实际上并不是positive，但是预测的值是 positive. 那么 tp+fp 就是所有预测为 positive的值。所以 precision 就是预测对的 positive 比上所有预测的 positive.</p><p>fn 指的是实际上是 positive, 但是预测值并不是 positive 的值。所以tp+fn 就是所有实际的 positive 值，recall 就是预测对的 positive比上所有实际的 positive 值。</p><p>我们这样对比着矩阵和公式来理解 Precision 和 Recall是不是就清晰了很多？这就是 position 和 recall根据混淆矩阵的一种定义方式。</p><p>刚刚讲了 baseline, baseline是在做评估的时候要知道结果一定要比什么好才行。如果是个二分类问题，基本上是一半一半，准确度是50%, 那基本上就没用。</p><p>Precision 和 recall这两个是针对于分类问题进行评价，那我们怎么解决回归问题的评价呢？</p><p>回归问题，它也有一个 accuracy 如下：</p><p><span class="math display">\[acc(y, \hat y) = \sum_{i \in N}|y_i - \hat y_i|  \\acc(y, \hat y) = \sum_{i \in N}|y_i - \hat y_i|^2 \\acc(y, \hat y) = \sum_{i \in N} \frac{|y_i - \hat{y_i}|}{|y_i|}\]</span></p><p>除此之外，regression问题里面有一个比较重要的评价方式叫做<code>R2-scoree</code>:</p><p><span class="math display">\[R^2(y, \hat y) = 1 - \frac{\sum_{i=1}^n(y_i - \haty_i)^2}{\sum_{i=1}^n(y_i - \bar y)^2}\]</span></p><ul><li>第一种情况：如果所有的 y_i 和 yhat_i 的值都相等，那么 R2(y, yhat) =1</li><li>第二种情况：如果所有的 yhat_i 是 y_i 的平均值，那么 R2(y, yhat) =0</li><li>第三种情况：如果 R2 的值比 0还小，就意味着它还不如我们做统计求平均值，瞎猜的结果。也就是连 baseline都没达到。</li></ul><p>R2-scoree之所以常常会被用于进行回归问题的评测，主要的原因就是它防止了机器作弊。</p><p>比方说我们现在有一组数据，这组数据实际都是 0.99, 0.97, 0.98...,这些数字都很小，而且都很密集。那么给机器使用的时候随便做一个平均值，感觉到准确度还挺高，那就被骗了。</p><h2 id="f-score">F-score</h2><p>在 precision 和 recall 之外，还有一个比较重要的内容，叫做F-score.</p><p>首先我们要知道，precision 和 recall这两个值在实际工作中往往是相互冲突的。为了做个均衡，就有了 F-score.</p><p><span class="math display">\[\begin{align*}F-score &amp; = \frac{(1+\beta^2) * precision \times recall}{\beta^2 *precision + recall}\end{align*}\]</span></p><p><spanclass="math inline">\(\beta\)</span>是自行定义的参数，由这个式子可见F-score 能同时考虑 precision 和 recall 这两种数值。分子为 precision 和recall 相乘，根据式子，只要 precision 或 recall 趋近于 0，F-score就会趋近于 0，代表着这个算法的精确度非常低。一个好的算法，最好能够平衡recall 和precision，且尽量让两种指标都很高。所以有一套判断方式可以同时考虑 recall和 precision。当<span class="math inline">\(\beta \to 0\)</span>,F-score 就会退化为 precision, 反之，当<span class="math inline">\(\beta\to \infty\)</span>, F-socre 就会退化为 recall.</p><p>我们一般说起来，F-score 没有特别定义的话，就是说<spanclass="math inline">\(\beta\)</span>为 1, 一般我们写成 F1-score.</p><p><span class="math display">\[\begin{align*}F1-score &amp; = 2 \times \frac{precision \times recall}{precision +recall}\end{align*}\]</span></p><p>F1-score 是仅当 precision 和 recall 都为 1 的时候，其值才等于 1.而如果这两个值中任意一个不为 1 时，其值都不能等于 1. 也就是说，当 2*1/2= 1 时，F1-score=100%, 代表该算法有着最佳的精确度。</p><h2 id="auc-roc">AUC-ROC</h2><p>除了 F-score 之外，还有比较重要的一个概念：AUC-ROC.这个也是为了解决样本不均衡提出来的一个解决方案。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231019125249.png"alt="Alt text" /></p><p>首先我们要先了解 ROC 曲线 (receiveroperating characteristic), ROC曲线上的每一个点反映着对同一信号刺激的感受。AOC(Area under Curve), 是ROC 曲线下的面积，取值是在 0.1 ~ 1 之间。</p><p>我们直接来看看，它在实际场景下是怎么用的。</p><p>还记得咱们在之前设定的阈值<code>decision_boundary = 0.5</code>,我们就拿这个阈值来看。<code>threshold:0.5</code>.在我们二分类问题中，当预测值大于 0.5 的时候，也就等于 1了。也就是说，只要超过 0.5, 我们就判定为 positive 值。</p><p>好，现在还是的请我们劳烦了无数次的警察 a 同志来帮帮我们。当警察 a去抓罪犯的时候，盘但一个人是不是犯了罪，他的决策很重要。在事实清晰之前，警察a 的决策只有超过 0.5 的时候，才能判定这个人是positive，也就是罪犯。这个时候呢，我们假设 precision 是 0.7.</p><p>现在又需要警察 b 出场了，这个警察 b 的 threshold 为 0.1 的时候，其precision 就为 0.7. 也就是说，他预计出的值，只要大于 0.1, 就判定为positive, 这种情况下，警察 b 判定的 precision 为 0.7.</p><p>别急，这次需要的演员有点多，所以，警察 c 登场了。那么警察 c 的threshold 为 0.9. 也就是说，警察 c比较谨慎，只有非常确定的时候，才能判定 positive. 警察 c 的情况，判定的precision 也是 0.7.</p><p>好，现在我们来用脑子思考下，这三个警察哪个警察能力最强？</p><p>必须是警察 b 最厉害。</p><p>就如我们上面的那四个坐标轴，X 轴代表 threshold, Y 轴表实 positive, 当threshold 轴上的取值还很小的时候，positive 已经很大了。那明显紫色线条和threshold 轴圈住的区域面积越大，这个面积就是越大越好。</p><p>这就是 AUC for ROC curves,这个主要就是为了解决那些样本及其不均衡的问题。因为样本非常不均衡的时候，position和 recall 你有可能都会很低，这个时候就不好对比。AUC曲线对于这种情况就比较好用一些。</p><p>其实在真实情况下，绝大多数问题都不是很均衡的问题。比方说预测病，找消费者，找高潜力用户。换句话说，如果高潜用户多就不用找了。</p><p>我们在研究 ROC 曲线实际应用的时候，依然会用到上面给大家所讲的 tp, fp,fn, tn. 这里会引出另外两个东西，TPR 和 FPR, 如下：</p><p><span class="math display">\[\begin{align*}TPR &amp; = \frac{tp}{tp+fn} \\FPR &amp; = \frac{fp}{fp+tn}\end{align*}\]</span></p><p>我们来看看咱们之前的这组数据的 AUC 值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_curve, auc<br><br>fpr, tpr, thresholds = roc_curve(true_labels, losses)<br><br>roc_auc = auc(fpr, tpr)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;AUC: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(roc_auc))<br><br>---<br>AUC: <span class="hljs-number">0.9300356506238858</span><br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231019140016.png"alt="Alt text" /></p><p>下一节课，咱们来说一个非常重要的概念：拟合和欠拟合。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hi, 你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;上一节课，咱们讲到了评测指标，并且在文章的最后提到了一个矩阵，我们就从这里开始。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
</feed>
