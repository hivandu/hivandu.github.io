<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>茶桁.MAMT</title>
  
  <subtitle>ChaHeng Notes，codding and writting ~</subtitle>
  <link href="https://hivan.me/atom.xml" rel="self"/>
  
  <link href="https://hivan.me/"/>
  <updated>2023-11-25T04:40:17.365Z</updated>
  <id>https://hivan.me/</id>
  
  <author>
    <name>Hivan Du</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>23. 深度学习 - 多维向量自动求导</title>
    <link href="https://hivan.me/23.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%AE%8C%E6%88%90%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A1%86%E6%9E%B6/"/>
    <id>https://hivan.me/23.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%AE%8C%E6%88%90%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A1%86%E6%9E%B6/</id>
    <published>2023-11-21T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:17.365Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509025.png"alt="茶桁的 AI 秘籍 核心能力 23" /></p><span id="more"></span><p>Hi, 你好。我是茶桁。</p><p>前面几节课中，我们从最初的理解神经网络，到讲解函数，多层神经网络，拓朴排序以及自动求导。可以说，最难的部分已经过去了，这节课到了我们来收尾的阶段，没错，生长了这么久，终于到迎接成果的时候了。</p><p>好，让我们开始。</p><blockquote><p>我们还是用上一节课的代码：<code>21.ipynb</code>。</p></blockquote><p>我们上一节课中，实现了自动计算的部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.name))<br>    node.backward()<br></code></pre></td></tr></table></figure><p>结果我就不打印了，节省篇幅。</p><p>那我们到这一步之后，咱们就已经获得了偏导，现在要考虑的问题就是去更新它，去优化它的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">learning_rate = <span class="hljs-number">1e-5</span><br><br><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.value = node.value + -<span class="hljs-number">1</span> * node.gradients[node] * learning_rate<br></code></pre></td></tr></table></figure><p>node 的值去更新，就应该等于它本身的值加上一个 -1乘以它的偏导在乘以一个<code>learning_rate</code>,我们对这个是不是已经很熟悉了？我们从第 8节线性回归的时候就一直在接触这个公式。</p><p>只不过在这个地方，x, y的值也要更新吗？它们的值是不应该去更新的，那要更新的应该是 k, b的值。</p><p>那么在这个地方该怎么办呢？其实很简单，我们添加一个判断就可以了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    <span class="hljs-keyword">if</span> node.is_trainable:<br>        node.value = node.value + -<span class="hljs-number">1</span> * node.gradients[node] * learning_rate<br></code></pre></td></tr></table></figure><p>然后我们给之前定义的类上加一个变量用于判断。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">..., is_trainable=<span class="hljs-literal">False</span></span>):<br>        ...<br>        self.is_trainable = is_trainable<br><br></code></pre></td></tr></table></figure><p>在这里我们默认是不可以训练的，只有少数的一些是需要训练的。</p><p>然后我们在初始化的部分把这个定义的值加上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">node_k = Placeholder(name=<span class="hljs-string">&#x27;k&#x27;</span>, is_trainable=<span class="hljs-literal">True</span>)<br>node_b = Placeholder(name=<span class="hljs-string">&#x27;b&#x27;</span>, is_trainable=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>对了，我们还需要将 Placeholder 做些改变：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Placeholder</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">..., is_trainable=<span class="hljs-literal">False</span></span>):<br>        Node.__init__(.., is_trainable=is_trainable)<br>        ...<br>    ...<br></code></pre></td></tr></table></figure><p>这就意味着，运行 for 循环的时候只有 k 和 b的值会更新，我们再加几句话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    <span class="hljs-keyword">if</span> node.is_trainable:<br>        ...<br>        cmp = <span class="hljs-string">&#x27;large&#x27;</span> <span class="hljs-keyword">if</span> node.gradients[node] &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;small&#x27;</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#123;&#125;的值&#123;&#125;，需要更新。&#x27;</span>.<span class="hljs-built_in">format</span>(node.name, cmp))<br><br>---<br>k的值small，需要更新。<br>b的值small，需要更新。<br></code></pre></td></tr></table></figure><p>我们现在将 forward, backward 和 optimize的三个循环封装乘三个方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">graph_sorted_nodes</span>):<br>    <span class="hljs-comment"># Forward</span><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>        node.forward()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">graph_sorted_nodes</span>):<br>    <span class="hljs-comment"># Backward</span><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.name))<br>        node.backward()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">optimize</span>(<span class="hljs-params">graph_sorted_nodes, learning_rate=<span class="hljs-number">1e-3</span></span>):<br>    <span class="hljs-comment"># optimize</span><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>        <span class="hljs-keyword">if</span> node.is_trainable:<br>            node.value = node.value + -<span class="hljs-number">1</span> * node.gradients[node] * learning_rate<br>            cmp = <span class="hljs-string">&#x27;large&#x27;</span> <span class="hljs-keyword">if</span> node.gradients[node] &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;small&#x27;</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#123;&#125;的值&#123;&#125;，需要更新。&#x27;</span>.<span class="hljs-built_in">format</span>(node.name, cmp))<br><br></code></pre></td></tr></table></figure><p>然后我们再来定义一个 epoch 方法，将 forward 和 backward放进去一起执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">run_one_epoch</span>(<span class="hljs-params">graph_sorted_nodes</span>):<br>    forward(graph_sorted_nodes)<br>    backward(graph_sorted_nodes)<br></code></pre></td></tr></table></figure><p>这样，我们完成一次完整的求值 - 求导 - 更新，就可以写成这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">run_one_epoch(sorted_nodes)<br>optimize(sorted_nodes)<br></code></pre></td></tr></table></figure><p>为了更好的观察，我们将所有的 print 都删掉，然后在 backward方法中写一个观察 loss 的打印函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">graph_sorted_nodes</span>):<br>    <span class="hljs-comment"># Backward</span><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(node, Loss):<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;loss value: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.value))<br>        node.backward()<br></code></pre></td></tr></table></figure><p>然后我们来对刚才完整的过程做个循环：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 完整的一次求值 - 求导 - 更新：</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    run_one_epoch(sorted_nodes)<br>    optimize(sorted_nodes, learning_rate=<span class="hljs-number">1e-1</span>)<br><br>---<br>loss value: <span class="hljs-number">0.12023025149136042</span><br>loss value: <span class="hljs-number">0.11090709486917472</span><br>loss value: <span class="hljs-number">0.10118818479676453</span><br>loss value: <span class="hljs-number">0.09120180962480523</span><br>loss value: <span class="hljs-number">0.08111466190584131</span><br>loss value: <span class="hljs-number">0.0711246044819575</span><br>loss value: <span class="hljs-number">0.061446239826641165</span><br>loss value: <span class="hljs-number">0.05229053883349982</span><br>loss value: <span class="hljs-number">0.043842158831920566</span><br>loss value: <span class="hljs-number">0.036239620745126</span><br></code></pre></td></tr></table></figure><p>可以看到 loss 在一点点的下降。当然，这样循环 10次我们还能观察出来，但是我们如果要成百上千次的去计算它，这样可就不行了，那我们需要将history 存下来，然后用图来显示出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_history = []<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    ...<br>    _loss_node = sorted_nodes[-<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">isinstance</span>(_loss_node, Loss)<br>    loss_history.append(_loss_node.value)<br>    optimize(sorted_nodes, learning_rate=<span class="hljs-number">1e-1</span>)<br><br>plt.plot(loss_history)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509027.png"alt="Alt text" /></p><p>我们现在可以验证一下，我们拟合的 yhat 和真实的 y之间差距有多大，首先我们当然是要获取到每个值的下标，然后用 sigmoid函数来算一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">sorted_nodes<br><br>---<br>[k, y, x, b, Linear, Sigmoid, Loss]<br></code></pre></td></tr></table></figure><p>通过下标来进行计算，k 是 0，x 是 2，b 是 3，y 是 1：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br><br><span class="hljs-comment"># k*x+b</span><br>sigmoid_x = sorted_nodes[<span class="hljs-number">0</span>].value * sorted_nodes[<span class="hljs-number">2</span>].value + sorted_nodes[<span class="hljs-number">3</span>].value<br><span class="hljs-built_in">print</span>(sigmoid(sigmoid_x))<br><br><span class="hljs-comment"># y</span><br><span class="hljs-built_in">print</span>(sorted_nodes[<span class="hljs-number">1</span>].value)<br><br>---<br><span class="hljs-number">0.891165479601981</span><br><span class="hljs-number">0.8988713384533658</span><br></code></pre></td></tr></table></figure><p>可以看到，非常的接近。那说明我们拟合的情况还是不错的。</p><p>好，这里总结一下，就是我们有了拓朴排序，就能向前去计算它的值，通过向前计算的值就可以向后计算它的值。那现在其实我们已经完成了一个mini的深度学习框架的核心内容，咱们能够定义节点，能够前向传播运算，能够反向传播运算，能更新梯度了。</p><p>那接下来是不是就结束了呢？很遗憾，并没有，接着咱们还要考虑如何处理多维数据。咱们现在看到的数据都是x、k、b 的输入，也就是都是一维的。</p><p>然而咱们真实世界中大多数场景下其实都是多维度的，其实都是多维数组。那么多维数组的还需要更新些什么，和现在有什么区别呢？</p><p>我们来接着往后看，因为基本上写法和现在这些几乎完全一样，那我也就不这么细致的讲了。</p><p>为了和之前代码做一个区分，所以我将多维向量计算的代码从新开了个文件，放在了<code>23.ipynb</code>里，小伙伴可以去下载到本地研习。</p><p>那么多维和现在最大的区别在哪里呢？就在于计算的时候，我们就要用到矩阵运算了。只是值变成了矩阵，运算变成的了矩阵运算。好，我们从Node开始来改动它，没什么变化的地方我就直接用<code>...</code>来省略了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>=[]</span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">raise</span> <span class="hljs-literal">NotImplemented</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">raise</span> <span class="hljs-literal">NotImplemented</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Placeholder</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        Node.__init__(self)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, value=<span class="hljs-literal">None</span></span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients = &#123;self:<span class="hljs-number">0</span>&#125;<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.outputs:<br>            grad_cost = n.gradients[self]<br>            self.gradients[self] = grad_cost * <span class="hljs-number">1</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, k, b</span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients = &#123;n: np.zeros_like(n.value) <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.inputs&#125;<br><br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.outputs:<br>            grad_cost = n.gradients[self]<br><br>            self.gradients[self.inputs[<span class="hljs-number">0</span>]] = np.dot(grad_cost, self.inputs[<span class="hljs-number">1</span>].value.T)<br>            self.gradients[self.inputs[<span class="hljs-number">1</span>]] = np.dot(self.inputs[<span class="hljs-number">0</span>].value.T, grad_cost)<br>            self.gradients[self.inputs[<span class="hljs-number">2</span>]] = np.<span class="hljs-built_in">sum</span>(grad_cost, axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-literal">False</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sigmoid</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, node</span>):<br>        Node.__init__(self, [node])<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_sigmoid</span>(<span class="hljs-params">self, x</span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.partial = self._sigmoid(self.x) * (<span class="hljs-number">1</span> - self._sigmoid(self.x))<br>        self.gradients = &#123;n: np.zeros_like(n.value) <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.inputs&#125;<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.outputs:<br>            grad_cost = n.gradients[self]  <br>            self.gradients[self.inputs[<span class="hljs-number">0</span>]] = grad_cost * self.partial<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MSE</span>(<span class="hljs-title class_ inherited__">Node</span>): <span class="hljs-comment"># 也就是之前的 Loss 类</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, y, a</span>):<br>        Node.__init__(self, [y, a])<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        y = self.inputs[<span class="hljs-number">0</span>].value.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        a = self.inputs[<span class="hljs-number">1</span>].value.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">assert</span>(y.shape == a.shape)<br>        self.m = self.inputs[<span class="hljs-number">0</span>].value.shape[<span class="hljs-number">0</span>]<br>        self.diff = y - a<br>        self.value = np.mean(self.diff**<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = (<span class="hljs-number">2</span> / self.m) * self.diff<br>        self.gradients[self.inputs[<span class="hljs-number">1</span>]] = (-<span class="hljs-number">2</span> / self.m) * self.diff<br></code></pre></td></tr></table></figure><p>类完成之后，我们还有一些其他的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_and_backward</span>(<span class="hljs-params">graph</span>): <span class="hljs-comment"># run_one_epoch</span><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> graph:<br>        n.forward()<br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span>  graph[::-<span class="hljs-number">1</span>]:<br>        n.backward()<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">toplogic</span>(<span class="hljs-params">graph</span>):<br>    ...<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_feed_dict_to_graph</span>(<span class="hljs-params">feed_dict</span>):<br>    ...<br><span class="hljs-comment"># 将 sorted_nodes 赋值从新定义了一个方法</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">topological_sort_feed_dict</span>(<span class="hljs-params">feed_dict</span>):<br>    graph = convert_feed_dict_to_graph(feed_dict)<br>    <span class="hljs-keyword">return</span> toplogic(graph)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">optimize</span>(<span class="hljs-params">trainables, learning_rate=<span class="hljs-number">1e-2</span></span>):<br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> trainables:<br>        node.value += -<span class="hljs-number">1</span> * learning_rate * node.gradients[node]<br></code></pre></td></tr></table></figure><p>这样就完成了。可以发现基本上代码没有什么变动，变化比较大的都是各个类中的backward 方法，因为要将其变成使用矩阵运算。</p><p>我们来尝试着用一下这个多维算法，我们还是用波士顿房价的那个数据来做一下尝试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python">X_ = data[<span class="hljs-string">&#x27;data&#x27;</span>]<br>y_ = data[<span class="hljs-string">&#x27;target&#x27;</span>]<br><br><span class="hljs-comment"># Normalize data</span><br>X_ = (X_ - np.mean(X_, axis=<span class="hljs-number">0</span>)) / np.std(X_, axis=<span class="hljs-number">0</span>)<br><br>n_features = X_.shape[<span class="hljs-number">1</span>]<br>n_hidden = <span class="hljs-number">10</span><br>W1_ = np.random.randn(n_features, n_hidden)<br>b1_ = np.zeros(n_hidden)<br>W2_ = np.random.randn(n_hidden, <span class="hljs-number">1</span>)<br>b2_ = np.zeros(<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># Neural network</span><br>X, y = Placeholder(), Placeholder()<br>W1, b1 = Placeholder(), Placeholder()<br>W2, b2 = Placeholder(), Placeholder()<br><br>l1 = Linear(X, W1, b1)<br>s1 = Sigmoid(l1)<br>l2 = Linear(s1, W2, b2)<br>cost = MSE(y, l2)<br><br>feed_dict = &#123;<br>    X: X_,<br>    y: y_,<br>    W1: W1_,<br>    b1: b1_,<br>    W2: W2_,<br>    b2: b2_<br>&#125;<br><br>epochs = <span class="hljs-number">5000</span><br><span class="hljs-comment"># Total number of examples</span><br>m = X_.shape[<span class="hljs-number">0</span>]<br>batch_size = <span class="hljs-number">16</span><br>steps_per_epoch = m // batch_size<br><br>graph = topological_sort_feed_dict(feed_dict)<br>trainables = [W1, b1, W2, b2]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Total number of examples = &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(m))<br></code></pre></td></tr></table></figure><p>我们在中间定义了 l1, s1, l2, cost,分别来实例化四个类。然后我们就需要根据数据来进行迭代计算了，定义一个losses 来保存历史数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">losses = []<br><br>epochs = <span class="hljs-number">100</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    loss = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(steps_per_epoch):<br>        <span class="hljs-comment"># Step 1</span><br>        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)<br><br>        X.value = X_batch<br>        y.value = y_batch<br><br>        <span class="hljs-comment"># Step 2</span><br>        forward_and_backward(graph) <span class="hljs-comment"># set output node not important.</span><br><br>        <span class="hljs-comment"># Step 3</span><br>        rate = <span class="hljs-number">1e-2</span><br>    <br>        optimize(trainables, rate)<br><br>        loss += graph[-<span class="hljs-number">1</span>].value<br>    <br>    <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>: <br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Epoch: &#123;&#125;, Loss: &#123;:.3f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(i+<span class="hljs-number">1</span>, loss/steps_per_epoch))<br>        losses.append(loss/steps_per_epoch)<br><br>---<br>Epoch: <span class="hljs-number">1</span>, Loss: <span class="hljs-number">194.170</span><br>...<br>Epoch: <span class="hljs-number">4901</span>, Loss: <span class="hljs-number">3.137</span><br></code></pre></td></tr></table></figure><p>可以看到它 loss下降的非常快，还记得咱们刚开始的时候在训练波士顿房价数据的时候，那个loss 下降到多少？最低是不是就下降到在第一节课的时候我们的 lose最多下降到了多少 47.34 对吧？那现在呢？直接下降到了3，这是为什么？因为我们的维度多了，维度多了它就准确了。这说明什么？说明大家去谈恋爱的时候，不要盯着对象的一个方面，多方面考察，才能知道这个人是否合适。</p><p>好，现在看起来效果是很好，但是我们想知道到底拟合出来的什么函数，那怎么办？咱们把这个维度降低成三维空间就可以看了。</p><p>现在咱们这个波士顿的所有数据实际上是一个 15 维的数据，15维的数据你根本看不了，咱们现在只要把 x这个里边取一点值，在这个里边稍微把值给它变一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X_ = dataframe[[<span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>]]<br>y_ = data[<span class="hljs-string">&#x27;target&#x27;</span>]<br></code></pre></td></tr></table></figure><p>在咱们之前的课程中对其进行计算的时候就分析过，RM 和 LSTAT是影响最大的两个特征，我们还是来用这个。然后我们将刚才的代码从新运行一遍：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">losses = []<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm_notebook(<span class="hljs-built_in">range</span>(epochs)):<br>    ...<br><br>---<br>Epoch: <span class="hljs-number">1</span>, Loss: <span class="hljs-number">150.122</span><br>...<br>Epoch: <span class="hljs-number">4901</span>, Loss: <span class="hljs-number">16.181</span><br></code></pre></td></tr></table></figure><p>这次下降的就没上次好了。</p><p>现在我们可视化一下这个三维空间来看看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mpl_toolkits.mplot3d <span class="hljs-keyword">import</span> Axes3D<br><br>predicate_results = []<br><span class="hljs-keyword">for</span> rm, ls <span class="hljs-keyword">in</span> X_.values:<br>    X.value = np.array([[rm, ls]])<br>    forward_and_backward(graph)<br>    predicate_results.append(graph[-<span class="hljs-number">2</span>].value[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br><br>%matplotlib widget<br><br>fig = plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>))<br>ax = fig.add_subplot(<span class="hljs-number">111</span>, projection=<span class="hljs-string">&#x27;3d&#x27;</span>)<br><br>X_ = dataframe[[<span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>]].values[:, <span class="hljs-number">0</span>]<br>Y_ = dataframe[[<span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>]].values[:, <span class="hljs-number">1</span>]<br><br>Z = predicate_results<br><br>rm_and_lstp_price = ax.plot_trisurf(X_, Y_, Z, color=<span class="hljs-string">&#x27;green&#x27;</span>)<br><br>ax.set_xlabel(<span class="hljs-string">&#x27;RM&#x27;</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;% of lower state&#x27;</span>)<br>ax.set_zlabel(<span class="hljs-string">&#x27;Predicated-Price&#x27;</span>)<br></code></pre></td></tr></table></figure><p>然后我们就能看到一个数据的三维图形，因为我们开启了widget，所以可以进行拖动。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509029.png"alt="Alt text" /></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509030.png"alt="Alt text" /></p><p>从图形上看，确实符合房间越多，低收入人群越少，房价越高的特性。</p><p>那现在计算机确实帮我们自动的去找到了一个函数，这个函数到底怎么设置咱们都不用关心，它自动就给你求解出来，这个就是深度学习的意义。咱们经过这一系列写出来的东西其实就已经能够做到。</p><p>我觉得这个真的有一种数学之美，它从最简单的东西出发，最后做成了这样一个复杂的东西。确实很深其，并且还都在我们的掌握之中。</p><p>好，大家下来以后记得要多多自己敲代码，多分析其中的一些过程和原理。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509025.png&quot;
alt=&quot;茶桁的 AI 秘籍 核心能力 23&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>22. 深度学习 - 自动求导</title>
    <link href="https://hivan.me/22.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <id>https://hivan.me/22.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%9E%E7%8E%B0/</id>
    <published>2023-11-18T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:21.030Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311012100649.png"alt="Alt text" /></p><span id="more"></span><p>Hi，你好。我是茶桁。</p><p>咱们接着上节课内容继续讲，我们上节课已经了解了拓朴排序的原理，并且简单的模拟实现了。我们这节课就来开始将其中的内容变成具体的计算过程。</p><p><code>linear, sigmoid</code>和<code>loss</code>这三个函数的值具体该如何计算呢？</p><p>我们现在似乎大脑已经有了一个起比较模糊的印象，可以通过它的输入来计算它的点。</p><p>让我们先把最初的父类 Node 改造一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs=[], name=<span class="hljs-literal">None</span></span>):<br>        ...<br>        self.value = <span class="hljs-literal">None</span><br>    <br>    ...<br></code></pre></td></tr></table></figure><p>然后再复制出一个，和<code>Placeholder</code>一样，我们需要继承Node，并且改写这个方法自己独有的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, k, b, name=<span class="hljs-literal">None</span></span>):<br>        Node.__init__(self, inputs=[x, k, b], name=name)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        x, k, b = self.inputs[<span class="hljs-number">0</span>], self.inputs[<span class="hljs-number">1</span>], self.inputs[<span class="hljs-number">2</span>]<br>        self.value = k.value * x.value + b.value<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;我是&#123;&#125;, 我没有人类爸爸，需要自己计算结果&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.value))<br>    ...<br></code></pre></td></tr></table></figure><p>我们新定义的这个类叫<code>Linear</code>, 它会接收 x, k, b。它继承了Node。这个里面的 forward该如何计算呢？我们需要每一个节点都需要一个值，一个变量，因为我们初始化的时候接收的x,k,b 都赋值到了 inputs里，这里我们将其取出来就行了，然后就是线性方程的公式<code>k*x+b</code>，赋值到它自己的value 上。</p><p>然后接着呢，就轮到 Sigmoid 了，一样的，我们定义一个子类来继承Node:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sigmoid</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, name=<span class="hljs-literal">None</span></span>):<br>        Node.__init__(self, inputs=[x], name=name)<br>        self.x = self.inputs[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_sigmoid</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        self.value = self._sigmoid(self.x.value)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;我是&#123;&#125;, 我自己计算了结果&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.value))<br>    ...<br></code></pre></td></tr></table></figure><p>Sigmoid 函数只接收一个参数，就是 x，其公式为1/(1+e^{-x})，我们在这里定义一个新的方法来计算，然后在 forward里把传入的 x取出来，再将其送到这个方法里进行计算，最后将结果返回给它自己的value。</p><p>那下面自然是 Loss 函数了，方式也是一模一样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Loss</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, y, yhat, name=<span class="hljs-literal">None</span></span>):<br>        Node.__init__(self, inputs = [y, yhat], name=name)<br>        self.y = self.inputs[<span class="hljs-number">0</span>]<br>        self.yhat = self.inputs[<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        y_v = np.array(self.y.value)<br>        yhat_v = np.array(self.y_hat.value)<br>        self.value = np.mean((y.value - yhat.value) ** <span class="hljs-number">2</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;我是&#123;&#125;, 我自己计算了结果&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.value))<br><br>    ...<br></code></pre></td></tr></table></figure><p>那我们这里定义成 Loss其实并不确切，因为我们虽然喊它是损失函数，但是其实损失函数的种类也非常多。而这里，我们用的MSE。所以我们应该定义为<code>MSE</code>，不过为了避免歧义，这里还是沿用Loss 好了。</p><p>定义完类之后，我们参数调用的类名也就需要改一下了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">...<br>node_linear = Linear(x=node_x, k=node_k, b=node_b, name=<span class="hljs-string">&#x27;linear&#x27;</span>)<br>node_sigmoid = Sigmoid(x=node_linear, name=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>node_loss = Loss(y=node_y, yhat=node_sigmoid, name=<span class="hljs-string">&#x27;loss&#x27;</span>)<br></code></pre></td></tr></table></figure><p>好，这个时候我们基本完成了，计算之前让我们先看一下<code>sorted_node</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">sorted_node<br><br>---<br>[Placeholder: y,<br> Placeholder: k,<br> Placeholder: x,<br> Placeholder: b,<br> Linear: Linear,<br> Sigmoid: Sigmoid,<br> MSE: Loss]<br></code></pre></td></tr></table></figure><p>没有问题，我们现在可以模拟神经网络的计算过程了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.forward()<br><br>---<br>我是x, 我已经被人类爸爸赋值为<span class="hljs-number">3</span><br>我是b, 我已经被人类爸爸赋值为<span class="hljs-number">0.3737660632429008</span><br>我是k, 我已经被人类爸爸赋值为<span class="hljs-number">0.35915077292816744</span><br>我是y, 我已经被人类爸爸赋值为<span class="hljs-number">0.6087876106387002</span><br>我是Linear, 我没有人类爸爸，需要自己计算结果<span class="hljs-number">1.4512183820274032</span><br>我是Sigmoid, 我没有人类爸爸，需要自己计算结果<span class="hljs-number">0.8101858733432837</span><br>我是Loss, 我没有人类爸爸，需要自己计算结果<span class="hljs-number">0.04056126022042443</span><br></code></pre></td></tr></table></figure><p>咱们这个整个过程就像是数学老师推公式一样，因为这个比较复杂。你不了解这个过程就求解不出来。</p><p>这就是为什么我一直坚持要手写代码的原因。<code>c+v</code>大法确实好，但是肯定是学的不够深刻。表面的东西懂了，但是更具体的为什么不清楚。</p><p>我们可以看到，我们现在已经将 Linear、Sigmoid 和 Loss都将值计算出来了。那我们现在已经实现了从 x 到 loss 的前向传播</p><p>现在我们有了loss，那就又要回到我们之前机器学习要做的事情了，就是将损失函数 loss的值降低。</p><p>之前咱们讲过，要将 loss的值减小，那我们就需要求它的偏导，我们前面课程的求导公式这个时候就需要拿过来了。</p><p>然后我们需要做的事情并不是完成求导就好了，而是要实现「链式求导」。</p><p>那从 Loss 开始反向传播的时候该做些什么？先让我们把“口号”喊出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">...</span>):<br>        ...<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.inputs:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;获取∂&#123;&#125; / ∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, n.name))<br></code></pre></td></tr></table></figure><p>这样修改一下Node，然后在其中假如一个反向传播的方法，将口号喊出来。</p><p>然后我们来看一下口号喊的如何，用<code>[::-1]</code>来实现反向获取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    node.backward()<br><br>---<br>获取∂Loss / ∂y<br>获取∂Loss / ∂Sigmoid<br>获取∂Sigmoid / ∂Linear<br>获取∂Linear / ∂x<br>获取∂Linear / ∂k<br>获取∂Linear / ∂b<br></code></pre></td></tr></table></figure><p>这样看着似乎不是太直观，我们再将 node的名称加上去来看就明白很多：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    <span class="hljs-built_in">print</span>(node.name)<br>    node.backward()<br>---<br>Loss<br>获取∂Loss / ∂y<br>获取∂Loss / ∂Sigmoid<br>Sigmoid<br>获取∂Sigmoid / ∂Linear<br>Linear<br>获取∂Linear / ∂x<br>获取∂Linear / ∂k<br>获取∂Linear / ∂b<br>...<br></code></pre></td></tr></table></figure><p>最后的<code>k, y, x, b</code>我就用...代替了，主要是函数。</p><p>那我们就清楚的看到，Loss 获取了两个偏导，然后传到了 Sigmoid，Sigmoid获取到一个，再传到Linear，获取了三个。那现在其实我们只要把这些值能乘起来就可以了。我们要计算步骤都有了，只需要把它乘起来就行了。</p><p>我们先是需要一个变量，用于存储 Loss 对某个值的偏导</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">...</span>):<br>        ...<br>        self.gradients = <span class="hljs-built_in">dict</span>()<br>    ...<br></code></pre></td></tr></table></figure><p>然后我们倒着来看，先来看 Loss:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Loss</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">0</span>].name)<br>        self.gradients[self.inputs[<span class="hljs-number">1</span>]] = <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">1</span>].name)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[0]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">0</span>]]))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[1]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">1</span>]]))<br></code></pre></td></tr></table></figure><p>眼尖的小伙伴应该看出来了，我现在依然还是现在里面进行「喊口号」的动作。主要是先来看一下过程。</p><p>刚才每个 node 都有一个 gradients，它代表的是对某个节点的偏导。</p><p>现在这个节点 self 就是 loss，然后我们<code>self.inputs[0]</code>就是y, <code>self.inputs[1]</code>就是 yhat,也就是<code>node_sigmoid</code>。那么我们现在这个<code>self.gradients[self.inputs[n]]</code>其实就分别是<code>∂loss/∂y</code>和<code>∂loss/∂yhat</code>，我们把对的值分别赋值给它们。</p><p>然后我们再来看 Sigmoid：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sigmoid</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">0</span>].name)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[0]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">0</span>]]))<br></code></pre></td></tr></table></figure><p>我们依次来看哈，这个时候的 self 就是 Sigmoid了，这个时候的<code>sigmoid.inputs[0]</code>应该是 Linear对吧，然后我们整个<code>self.gradients[self.inputs[0]]</code>自然就应该是<code>∂sigmoid/∂linear</code>。</p><p>我们继续，这个时候<code>self.outputs[0]</code>就是 loss,<code>loss.gradients[self]</code>那自然就应该是输出过来的<code>∂loss/∂sigmoid</code>，然后呢，我们需要将这两个部分乘起来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>    self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-string">&#x27;*&#x27;</span>.join([self.outputs[<span class="hljs-number">0</span>].gradients[self], <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">0</span>].name)])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[0]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">0</span>]]))<br></code></pre></td></tr></table></figure><p>接着，我们就需要来看看 Linear 了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>    self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-string">&#x27;*&#x27;</span>.join([self.outputs[<span class="hljs-number">0</span>].gradients[self], <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">0</span>].name)])<br>    self.gradients[self.inputs[<span class="hljs-number">1</span>]] = <span class="hljs-string">&#x27;*&#x27;</span>.join([self.outputs[<span class="hljs-number">0</span>].gradients[self], <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">1</span>].name)])<br>    self.gradients[self.inputs[<span class="hljs-number">2</span>]] = <span class="hljs-string">&#x27;*&#x27;</span>.join([self.outputs[<span class="hljs-number">0</span>].gradients[self], <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">2</span>].name)])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[0]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">0</span>]]))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[1]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">1</span>]]))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[2]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">2</span>]]))<br></code></pre></td></tr></table></figure><p>和上面的分析一样，我们先来看三个<code>inputs[n]</code>的部分，self在这里是 linear了，这里的<code>self.inputs[n]</code>分别应该是<code>x, k, b</code>对吧，那么它们就应该分别是<code>linear.gradients[x]</code>,<code>linear.gradients[k]</code>和<code>linear.gradients[b]</code>，也就是<code>∂linear/∂x</code>,<code>∂linear/∂k</code>,<code>∂linear/∂b</code>。</p><p>那反过来，<code>outputs</code>就应该反向来找，那么<code>self.outputs[0]</code>这会儿就应该是sigmoid。<code>sigmoid.gradients[self]</code>就是前一个输出过来的<code>∂loss/∂sigmoid * ∂sigmoid/∂linear</code>,那后面以此的[1]和[2]我们也就应该明白了。</p><p>然后后面分别是<code>∂linear/∂x</code>,<code>∂linear/∂k</code>,<code>∂linear/∂b</code>。一样，我们将它们用乘号连接起来。</p><p>公式就应该是：</p><p><span class="math display">\[\begin{align*}\frac{\partial loss}{\partial sigmoid} \cdot \frac{\partialsigmoid}{\partial linear} \cdot \frac{\partial linear}{\partial x} \\\frac{\partial loss}{\partial sigmoid} \cdot \frac{\partialsigmoid}{\partial linear} \cdot \frac{\partial linear}{\partial k} \\\frac{\partial loss}{\partial sigmoid} \cdot \frac{\partialsigmoid}{\partial linear} \cdot \frac{\partial linear}{\partial b} \\\end{align*}\]</span></p><p>那同理，我们还需要写一下<code>Placeholder</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Placeholder</span>(<span class="hljs-params">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;我获取了我自己的 gradients: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.outputs[<span class="hljs-number">0</span>].gradients[self]))<br>    ...<br></code></pre></td></tr></table></figure><p>好，我们来看下我们模拟的情况如何，看看它们是否都如期喊口号了，结合我们之前的前向传播的结果，我们一起来看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.forward()<br>    <br><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.name))<br>    node.backward()<br><br>---<br>Loss<br>[<span class="hljs-number">0</span>]: ∂Loss/∂y<br>[<span class="hljs-number">1</span>]: ∂Loss/∂Sigmoid<br><br>Sigmoid<br>[<span class="hljs-number">0</span>]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear<br><br>Linear<br>[<span class="hljs-number">0</span>]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂x<br>[<span class="hljs-number">1</span>]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂k<br>[<span class="hljs-number">2</span>]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂b<br><br>k<br>我获取了我自己的gradients: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂k<br><br>b<br>我获取了我自己的gradients: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂b<br><br>x<br>我获取了我自己的gradients: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂x<br><br>y<br>我获取了我自己的gradients: ∂Loss/∂y<br></code></pre></td></tr></table></figure><p>好，观察下来没问题，那我们现在还剩下最后一步。就是将这些口号替换成真正的计算的值，其实很简单，就是将我们之前学习过并写过的函数替换进去就可以了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        x, k, b = self.inputs[<span class="hljs-number">0</span>], self.inputs[<span class="hljs-number">1</span>], self.inputs[<span class="hljs-number">2</span>]<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = self.outputs[<span class="hljs-number">0</span>].gradients[self] * k.value<br>        self.gradients[self.inputs[<span class="hljs-number">1</span>]] = self.outputs[<span class="hljs-number">0</span>].gradients[self] * x.value<br>        self.gradients[self.inputs[<span class="hljs-number">2</span>]] = self.outputs[<span class="hljs-number">0</span>].gradients[self] * <span class="hljs-number">1</span><br>        ...<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sigmoid</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.value = self._sigmoid(self.x.value)<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = self.outputs[<span class="hljs-number">0</span>].gradients[self] * self.value * (<span class="hljs-number">1</span> - self.value)<br>        ...<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Loss</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        y_v = self.y.value<br>        yhat_v = self.y_hat.value<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-number">2</span>*np.mean(y_v - yhat_v)<br>        self.gradients[self.inputs[<span class="hljs-number">1</span>]] = -<span class="hljs-number">2</span>*np.mean(y_v - yhat_v)<br></code></pre></td></tr></table></figure><p>那我们来看下真正计算的结果是怎样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.name))<br>    node.backward()<br><br>---<br>Loss<br>∂Loss/∂y: -<span class="hljs-number">0.402796525409167</span><br>∂Loss/∂Sigmoid: <span class="hljs-number">0.402796525409167</span><br><br>Sigmoid<br>∂Sigmoid/∂Linear: <span class="hljs-number">0.06194395247945269</span><br><br>Linear<br>∂Linear/∂x: <span class="hljs-number">0.02224721841122111</span><br>∂Linear/∂k: <span class="hljs-number">0.18583185743835806</span><br>∂Linear/∂b: <span class="hljs-number">0.06194395247945269</span><br><br>y<br>gradients: -<span class="hljs-number">0.402796525409167</span><br><br>k<br>gradients: <span class="hljs-number">0.18583185743835806</span><br><br>b<br>gradients: <span class="hljs-number">0.06194395247945269</span><br><br>x<br>gradients: <span class="hljs-number">0.02224721841122111</span><br></code></pre></td></tr></table></figure><p>好，到这里，我们就实现了前向传播和反向传播，让程序自动计算出了它们的偏导值。</p><p>不过我们整个动作还没有结束，就是我们需要将 loss降低到最小才可以。</p><p>那我们下节课，就来完成这一步。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311012100649.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>21. 深度学习 - 拓朴排序的原理和实现</title>
    <link href="https://hivan.me/21.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E6%8B%93%E6%9C%B4%E6%8E%92%E5%BA%8F%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/"/>
    <id>https://hivan.me/21.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E6%8B%93%E6%9C%B4%E6%8E%92%E5%BA%8F%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/</id>
    <published>2023-11-15T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:25.438Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310311950753.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><p>上节课，我们讲了多层神经网络的原理，并且明白了，数据量是层级无法超过3 层的主要原因。</p><span id="more"></span><p>然后我们用一张图来解释了整个链式求导的过程：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310311950755.png"alt="Alt text" /></p><p>那么，我们如何将这张图里的节点关系来获得它的求导过程呢？</p><p>假如我们现在定义一个函数<code>get_output</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_output</span>(<span class="hljs-params">graph, node</span>):<br>    outputs = []<br>    <span class="hljs-keyword">for</span> n, links <span class="hljs-keyword">in</span> graph.items():<br>        <span class="hljs-keyword">if</span> node == n: outputs += links<br>    <span class="hljs-keyword">return</span> outputs<br>get_output(computing_graph, <span class="hljs-string">&#x27;k1&#x27;</span>)<br><br>---<br>[<span class="hljs-string">&#x27;L1&#x27;</span>]<br></code></pre></td></tr></table></figure><p>我们可以根据 k1 获得 l1。</p><p>来，让我们整理一下思路，问：如何获得 k1 的偏导：</p><ol type="1"><li>获得 k1 的输出节点</li><li>获得 k1 输出节点的输出节点</li><li>...直到我们找到最后一个节点</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_order = []<br><br>target = <span class="hljs-string">&#x27;k1&#x27;</span><br>out = get_output(computing_graph, target)[<span class="hljs-number">0</span>]<br>computing_order.append(target)<br><br><span class="hljs-keyword">while</span> out:<br>    computing_order.append(out)<br>    out = get_output(computing_graph, out)<br>    <span class="hljs-keyword">if</span> out: out = out[<span class="hljs-number">0</span>]<br><br>computing_order<br><br>---<br>[<span class="hljs-string">&#x27;k1&#x27;</span>, <span class="hljs-string">&#x27;L1&#x27;</span>, <span class="hljs-string">&#x27;sigmoid&#x27;</span>, <span class="hljs-string">&#x27;L2&#x27;</span>, <span class="hljs-string">&#x27;loss&#x27;</span>]<br></code></pre></td></tr></table></figure><p>我们从 k1 出发，它可以获得这么一套顺序。那么现在如果要计算 k1的偏导，我们的这个偏导顺序就等于从后到前给它求解一遍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">order = <span class="hljs-string">&#x27;&#x27;</span><br><br><span class="hljs-keyword">for</span> i, n <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(computing_order[:-<span class="hljs-number">1</span>]):<br>    order += <span class="hljs-string">&#x27;*∂&#123;&#125; / ∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(n, computing_order[i+<span class="hljs-number">1</span>])<br><br>order<br>---<br><span class="hljs-string">&#x27;*∂k1 / ∂L1*∂L1 / ∂sigmoid*∂sigmoid / ∂L2*∂L2 / ∂loss&#x27;</span><br></code></pre></td></tr></table></figure><p>现在 k1的求导顺序计算机就给它自动求解出来了，我们把它放到了一个图里面，然后它自动就求解出来了。只不过唯一的问题是现在这个order 是反着的，需要把它再反过来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i, n <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(computing_order[:-<span class="hljs-number">1</span>]):<br>    order.append((computing_order[i + <span class="hljs-number">1</span>], n))<br>    <span class="hljs-comment"># order += &#x27; * ∂&#123;&#125; / ∂&#123;&#125;&#x27;.format(n, computing_order[i+1])</span><br><br><span class="hljs-string">&#x27; * &#x27;</span>.join([<span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(a, b) <span class="hljs-keyword">for</span> a, b <span class="hljs-keyword">in</span> order[::-<span class="hljs-number">1</span>]])<br><br>---<br><span class="hljs-string">&#x27;∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂k1&#x27;</span><br></code></pre></td></tr></table></figure><p>这个过程用计算机实现之后，我们就可以拿它来看一下其他的参数，比如说<code>b1</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_order = []<br><br>target = <span class="hljs-string">&#x27;b1&#x27;</span><br>out = get_output(computing_graph, target)[<span class="hljs-number">0</span>]<br>computing_order.append(target)<br><br><span class="hljs-keyword">while</span> out:<br>    computing_order.append(out)<br>    out = get_output(computing_graph, out)<br>    <span class="hljs-keyword">if</span> out: out = out[<span class="hljs-number">0</span>]<br><br>order = []<br><br><span class="hljs-keyword">for</span> i, n <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(computing_order[:-<span class="hljs-number">1</span>]):<br>    order.append((computing_order[i + <span class="hljs-number">1</span>], n))<br>    <span class="hljs-comment"># order += &#x27; * ∂&#123;&#125; / ∂&#123;&#125;&#x27;.format(n, computing_order[i+1])</span><br><br><span class="hljs-string">&#x27; * &#x27;</span>.join([<span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(a, b) <span class="hljs-keyword">for</span> a, b <span class="hljs-keyword">in</span> order[::-<span class="hljs-number">1</span>]])<br><br>---<br><span class="hljs-string">&#x27;∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂b1&#x27;</span><br></code></pre></td></tr></table></figure><p>k2:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">...<br>target = <span class="hljs-string">&#x27;k2&#x27;</span><br>...<br><br>---<br><span class="hljs-string">&#x27;∂loss/∂L2 * ∂L2/∂k2&#x27;</span><br></code></pre></td></tr></table></figure><p>到这里，我们能够自动的求解各个参数的导数了。</p><p>然后我们将其封装一下，然后循环一下每一个参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_paramter_partial_order</span>(<span class="hljs-params">p</span>):<br>    ...<br>    target = p<br>    ...<br>    <span class="hljs-keyword">return</span> ...<br><br><span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;b1&#x27;</span>, <span class="hljs-string">&#x27;k1&#x27;</span>, <span class="hljs-string">&#x27;b2&#x27;</span>, <span class="hljs-string">&#x27;k2&#x27;</span>]:<br>    <span class="hljs-built_in">print</span>(get_paramter_partial_order(p))<br><br>---<br>∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂b1<br>∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂k1<br>∂loss/∂L2 * ∂L2/∂b2<br>∂loss/∂L2 * ∂L2/∂k2<br></code></pre></td></tr></table></figure><p>到这一步你就能够发现，每一个参数的导数的偏导我们都可以求解了。而且我们还发现一个问题，不管是<code>['b1', 'k1', 'b2', 'k2']</code>中的哪一个，我们都需要求求解<code>∂loss/∂L2</code>。</p><p>所以现在如果有一个内存能够记录结果，先把<code>∂loss/∂L2</code>的值求解下来，把这个值先存下来，只要算出来这一个值之后，再算<code>['b1', 'k1', 'b2', 'k2']</code>的时候直接拿过来就行了。</p><p>也就是说我们首先需要记录的就是这个值，其次，如果我们把 L2 和 sigmoid的值记下来，求解 b1 和 k1的时候直接拿过来用就行，不需要再去计算一遍，这个时候我们的效率就会提升很多。</p><p>首先把共有的一个基础<code>∂loss/∂L2</code>计算了，第二步，有了<code>∂loss/∂L2</code>，把<code>∂L2/∂sigmoid</code>再记录一遍，第三个是<code>∂sigmoid/∂L1</code>,然后后面以此就是<code>∂L1/∂b1</code>,<code>∂L1/∂k1</code>，<code>∂L2/∂b2</code>, <code>∂L2/∂k2</code>。</p><p>现在的问题就是就是怎么样让计算机自动得到这个顺序，计算机得到这个顺序的时候，把这些值都存在某个地方。</p><p>这个所谓的顺序就是我们非常重要的一个概念，在计算机科学，算法里面非常重要的一个概念：「拓朴排序」。</p><p>那拓朴排序该如何实现呢？来，我们一起来实现一下：</p><p>首先，我们定义一个方法，咱们输入的是一个图，这个图的定义方式是一个Dictionary，然后里面有一些节点，里面的很多个连接的点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">topologic</span>(<span class="hljs-params">graph</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    graph: dict</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">        node: [node1, node2, ..., noden]</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><p>因为我们要把它的结果存在一个变量里边，当我们不断的检查看这个图，看看它是否为空，然后我们来定义两个存储变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">topologic</span>(<span class="hljs-params">graph</span>):<br>    sorted_node = []<br><br>    <span class="hljs-keyword">while</span> graph:<br>        all_inputs = []<br>        all_outputs = <span class="hljs-built_in">list</span>(graph.keys())<br><br>    <span class="hljs-keyword">return</span> sorted_node<br></code></pre></td></tr></table></figure><p>这里的两个变量，<code>all_inputs</code>和<code>all_outputs</code>,一个是用来存储所有的输入节点，一个是存储所有的输出节点。</p><p>我们还记得我们那个图的格式是什么样的吗？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_graph = &#123;<br>    <span class="hljs-string">&#x27;k1&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;b1&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;x&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;L1&#x27;</span>:[<span class="hljs-string">&#x27;sigmoid&#x27;</span>],<br>    <span class="hljs-string">&#x27;sigmoid&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;k2&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;b2&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;L2&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>],<br>    <span class="hljs-string">&#x27;y&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>]<br>&#125;<br></code></pre></td></tr></table></figure><p>我们看这个数据，那所有的输出节点是不是就是其中的<code>key</code>啊？</p><p>打比方说，我们拿一个短小的数据来做示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">simple_graph = &#123;<br>    <span class="hljs-string">&#x27;a&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],<br>    <span class="hljs-string">&#x27;b&#x27;</span>: [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]<br>&#125;<br><br><span class="hljs-built_in">list</span>(simple_graph.keys())<br><br>---<br>[<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>]<br></code></pre></td></tr></table></figure><p>那我们这样就拿到了输出节点，并将其放在了一个列表内。</p><p>这里说点其他的，Python 3.9及以上的版本其实都实现了自带拓朴排序，但是如果你的 Python版本较低，那还是需要自己去实现。这个也是 Python 3.9里面一个比较重要的更新。</p><p>那为什么我们的 value 定义的是一个列表呢？这是因为这个key，也就是输出值可能会输出到好几个函数里面，因为我们现在拿的是一个比较简单的模型，但是在真实场景中，有可能会输出到更多的节点中。</p><p>这里，就获得了所有有输入的节点， <code>simple_graph</code>中，a输出给了[1,2], b 输出给了[2,3]。</p><p>那我们怎么获得所有输入的节点呢？那就应该是 value。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">list</span>(simple_graph.values())<br><br>---<br>[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br></code></pre></td></tr></table></figure><p>这样就获得所有有输入的节点。然后就是怎么样把这两个 list合并。可以有一个简单的方法，一个叫做 reduce 的方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">reduce(<span class="hljs-keyword">lambda</span> a, b: a+b, <span class="hljs-built_in">list</span>(simple_graph.values()))<br><br>---<br>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]<br></code></pre></td></tr></table></figure><p>这样就把它给它连起来了。</p><p>那我们还需要找一个，就是只有输出没有输入的节点，这些该怎么去找呢？其实也就是我们的<code>[k1, b1, k2, b2, y]</code>这些值。</p><p>来，我们还是拿刚才的<code>simple_graph</code>来举例，但是这次我们改一下里面的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">simple_graph = &#123;<br>    <span class="hljs-string">&#x27;a&#x27;</span>: [<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-number">2</span>],<br>    <span class="hljs-string">&#x27;b&#x27;</span>: [<span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-number">4</span>]<br>&#125;<br><br>a = <span class="hljs-built_in">list</span>(simple_graph.keys())<br>b = reduce(<span class="hljs-keyword">lambda</span> a, b: a+b, <span class="hljs-built_in">list</span>(simple_graph.values()))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(b) - <span class="hljs-built_in">set</span>(a)))<br><br>---<br>[<span class="hljs-number">2</span>, <span class="hljs-number">4</span>]<br></code></pre></td></tr></table></figure><p>我们没有用循环，而是将其变成了一个集合，然后利用集合的加减来做。</p><p>我们的实际代码就可以这样写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">topologic</span>(<span class="hljs-params">graph</span>):<br>    sorted_node = []<br><br>    <span class="hljs-keyword">while</span> graph:<br>        all_inputs = reduce(<span class="hljs-keyword">lambda</span> a, b: a+b, <span class="hljs-built_in">list</span>(graph.values()))<br>        all_outputs = <span class="hljs-built_in">list</span>(graph.keys())<br><br>        all_inputs = <span class="hljs-built_in">set</span>(all_inputs)<br>        all_outputs = <span class="hljs-built_in">set</span>(all_outputs)<br><br>        need_remove = all_outputs - all_inputs<br><br>    <span class="hljs-keyword">return</span> sorted_node<br></code></pre></td></tr></table></figure><p>那现在我们继续往后，如果找到了这些只有输出没有输入的节点之后，我们做一个判断，然后定义一个节点，用来保存随机选择的节点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(need_remove) &gt; <span class="hljs-number">0</span>:<br>    node = random.choice(<span class="hljs-built_in">list</span>(need_remove))<br></code></pre></td></tr></table></figure><p>这个时候 x, b, k, y都有可能，那么我们随机找一个就行。然后将这个找到的节点从 graph给它删除。并且将其插入到<code>sorted_node</code>中去，并且返回出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">    <span class="hljs-keyword">if</span> ...:<br>        node = random.choice(<span class="hljs-built_in">list</span>(need_remove))<br>        graph.pop(node)<br>        sorted_node.append(node)<br><br><span class="hljs-keyword">return</span> sorted_node<br></code></pre></td></tr></table></figure><p>然后这里还会出一个小问题，我们还是拿一个示例来说：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">simple_graph = &#123;<br>    <span class="hljs-string">&#x27;a&#x27;</span>: [<span class="hljs-string">&#x27;sigmoid&#x27;</span>],<br>    <span class="hljs-string">&#x27;b&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>],<br>    <span class="hljs-string">&#x27;c&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>]<br>&#125;<br><br>simple_graph.pop(<span class="hljs-string">&#x27;b&#x27;</span>)<br>simple_graph<br><br>---<br>&#123;<span class="hljs-string">&#x27;a&#x27;</span>: [<span class="hljs-string">&#x27;sigmoid&#x27;</span>], <span class="hljs-string">&#x27;c&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>]&#125;<br></code></pre></td></tr></table></figure><p>看，我们在删除 node 的时候，其所对应的 value也就一起删除了，那这个时候，我们最后的输出列表里会丢失最后一个node。所以，我们在判断为最后一个的时候，需要额外的将其加上，放在 pop方法执行之前。那我们整个代码需要调整一下先后顺序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">topologic</span>(<span class="hljs-params">graph</span>):<br>    sorted_node = []<br>    <span class="hljs-keyword">while</span> graph:<br>        ...<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(need_remove) &gt; <span class="hljs-number">0</span>:<br>            node = random.choice(<span class="hljs-built_in">list</span>(need_remove))<br>            sorted_node.append(node)<br>           <br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(graph) == <span class="hljs-number">1</span>: sorted_node += graph[node]         <br>               <br>            graph.pop(node)<br><br>    <span class="hljs-keyword">return</span> sorted_node<br></code></pre></td></tr></table></figure><p>现在其实这个代码就已经 OK 了，我们来再加几句话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">...<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(need_remove) &gt; <span class="hljs-number">0</span>:<br>    ...<br>    <span class="hljs-keyword">for</span> _, links <span class="hljs-keyword">in</span> graph.items():<br>        <span class="hljs-keyword">if</span> node <span class="hljs-keyword">in</span> links: links.remove(node)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-keyword">raise</span> TypeError(<span class="hljs-string">&#x27;This graph has circle, which cannot get topological order.&#x27;</span>)<br>...<br></code></pre></td></tr></table></figure><p>我们把它的连接关系，例如现在选择了 k1，我们要把 k1的连接关系从这些里边给它删掉。</p><p>遍历一下 graph，遍历的时候如果删除的 node在它的输出里边，我们就把它删除。</p><p>加上<code>else</code>判断，如果图不是空的，但是最终没有找到，也就是这两个集合作减法，但是得到一个空集，没有找到，那我们就来输出一个错误：<code>This graph has circle, which cannot get topological order.</code></p><p>现在我们可以来实验一下了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">x, k, b, linear, sigmoid, y, loss = <span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;k&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;linear&#x27;</span>, <span class="hljs-string">&#x27;sigmoid&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&#x27;loss&#x27;</span><br>test_graph = &#123;<br>    x: [linear],<br>    k: [linear],<br>    b: [linear],<br>    linear: [sigmoid],<br>    sigmoid: [loss],<br>    y: [loss]<br>&#125;<br><br>topologic(test_graph)<br><br>---<br>[<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;k&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&#x27;linear&#x27;</span>, <span class="hljs-string">&#x27;sigmoid&#x27;</span>, <span class="hljs-string">&#x27;loss&#x27;</span>]<br></code></pre></td></tr></table></figure><p>好，现在让我们来声明一个<code>class node</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure><p>然后我们先来抽象一下这些节点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## Our Simple Model Elements</span><br><br>node_x = Node(inputs=<span class="hljs-literal">None</span>, outputs=[node_linear])<br>node_y = Node(inputs=<span class="hljs-literal">None</span>, outputs=[node_loss])<br>node_k = Node(inputs=<span class="hljs-literal">None</span>, outputs=[node_linear])<br>node_b = Node(inputs=<span class="hljs-literal">None</span>, outputs=[node_linear])<br>node_linear = Node(inputs=[node_x, node_k, node_b], outputs=[node_sigmoid])<br>node_sigmoid = Node(inputs=[node_linear], outputs=[node_loss])<br>node_loss = Node(inputs=[node_sigmoid, node_y], outputs=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><p>现在咱们就把图中每个节点已经给它抽象好了，但是我们发现节点写成这个样子代码是比较冗余。打比方说：<code>node_linear = Node(input=[node_x, node_k, node_b], outputs=[node_sigmoid])</code>，既然我们已经告诉程序<code>node_linear</code>这个节点的输入是<code>[node_x, node_k, node_b]</code>，那其实也就是告诉程序这些节点的输出是<code>node_linear</code>。</p><p>好，我们接下来要在<code>class Node</code>里定义一个方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs, outputs</span>):<br>    self.inputs = inputs<br>    self.outputs = outputs<br></code></pre></td></tr></table></figure><p>现在我们根据上面对代码冗余的分析，可以加上这样简单的一句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs=[]</span>):<br>    self.inputs = inputs<br>    self.outputs = []<br><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> inputs:<br>        node.outputs.append(self)<br></code></pre></td></tr></table></figure><p>把这句加上之后，就可以只在里面输入 inputs 就行了，不用再输入outputs，代码就变得简单多了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## Our Simple Model Elements</span><br><span class="hljs-comment">### version - 02</span><br>node_x = Node()<br>node_y = Node()<br>node_k = Node()<br>node_b = Node()<br>node_linear = Node(inputs=[node_x, node_k, node_b])<br>node_sigmoid = Node(inputs=[node_linear])<br>node_loss = Node(inputs=[node_sigmoid, node_y])<br></code></pre></td></tr></table></figure><p>我们是把每个节点给它做出来了，那么怎么样能够把这个节点给它像串珠子一样串起来变成一张图呢？</p><p>其实我们只要去考察所有的边沿节点就可以了，把所有的 x，y，k 和 b这种外层的函数给个变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">need_expend = [node_x, node_y, node_k, node_b]<br></code></pre></td></tr></table></figure><p>咱们再生成一个变量，这个变量是用来通过外沿这些节点，把连接图给生成出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_graph = defaultdict(<span class="hljs-built_in">list</span>)<br><br><span class="hljs-keyword">while</span> need_expend:<br>    n = need_expend.pop(<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-keyword">if</span> n <span class="hljs-keyword">in</span> computing_graph: <span class="hljs-keyword">continue</span><br><br>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> n.outputs:<br>        computing_graph[n].append(m)<br>        need_expend.append(m)<br></code></pre></td></tr></table></figure><p><code>while</code>里面，当外沿节点的 list不为空的时候，我们就在里面来取一个点，我们就取第一个吧，取出来并删除。</p><p>然后如果这个点我们已经考察过了，那就<code>continue</code>，如果没有，我们对于所有的这个n 里边的<code>outputs</code>，插入到 computing_graph 的 n的位置。再插入到外沿节点的 list内。因为我们现在多了一个扩充节点，所以我们需要给插入进去。</p><p>比方说我们这次找出来了 linear，把 linear也加到这个需要扩充的点一行，然后就可以从 linear 再找到 sigmoid 了。</p><p>来，我们看下现在的这个<code>computing_graph</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_graph<br><br>---<br>defaultdict(<span class="hljs-built_in">list</span>,<br>            &#123;&lt;__main__.Node at <span class="hljs-number">0x12053e080</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x12053e9b0</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053ef50</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x12053d510</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x12053c280</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x1202860e0</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x1202860e0</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053ef50</span>&gt;]&#125;)<br></code></pre></td></tr></table></figure><p>这样就获得出来了，其实是把它变成了刚刚的那个图。这样呢，我们就可以应用<code>topologic</code>来进行拓朴排序了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">topologic(computing_graph)<br><br>---<br>[&lt;__main__.Node at <span class="hljs-number">0x12053c280</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053d510</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053e080</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053e9b0</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x1202860e0</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053ef50</span>&gt;]<br></code></pre></td></tr></table></figure><p>但是我们打出来的内容都是一些内存地址，我们还需要改一下这个程序。我们在我们的<code>class Node</code>里多增加一个方法，用于return 它的名字：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs=[], name=<span class="hljs-literal">None</span></span>):<br>    ...<br>    self.name = name<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;Node:&#123;&#125; &#x27;</span>.<span class="hljs-built_in">format</span>(self.name)<br></code></pre></td></tr></table></figure><p>这样之后，我们还需要改一下节点，在里面增加一个变量<code>name=''</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">node_x = Node(name=<span class="hljs-string">&#x27;x&#x27;</span>)<br>...<br>node_loss = Node(inputs=[node_sigmoid, node_y], name=<span class="hljs-string">&#x27;loss&#x27;</span>)<br></code></pre></td></tr></table></figure><p>每一个都需要加上，我用<code>...</code>简化了代码。</p><p>然后我们再来看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">topologic(computing_graph)<br><br>---<br>[Node:k , Node:x , Node:b , Node:linear , Node:sigmoid , Node:y , Node:loss ]<br></code></pre></td></tr></table></figure><p>然后我们来将这段封装起来，变成一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">feed_dict = &#123;<br>    node_x: <span class="hljs-number">3</span>, <br>    node_y: random.random(),<br>    node_k: random.random(),<br>    node_b: <span class="hljs-number">0.38</span><br>&#125;<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_feed_dict_to_graph</span>(<span class="hljs-params">feed_dict</span>):<br>    need_expend = [n <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> feed_dict]<br>    ...<br><br>    <span class="hljs-keyword">return</span> computing_graph<br></code></pre></td></tr></table></figure><p>一般来说，很多大厂在建立代码的时候，<code>x, y, k, b</code>这种东西会被称为<code>placeholder</code>，我们创建的<code>need_expend</code>会被称为是<code>feed_dict</code>。所以我们做了这样一个修改，将<code>need_expend</code>拿到方法里取重新获取。</p><p>这些节点刚开始的时候没有值，那我们给它一个初始值，我这里的值都是随意给的。</p><p>这样，就不仅把最外沿的节点给找出来了，而且还把值给他送进去了，相对来说就会更简单一些。所有定义出来的节点，我们都可以把它变成图关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">topologic(convert_feed_dict_to_graph(feed_dict))<br><br>---<br>[Node:k , Node:y , Node:b , Node:x , Node:linear , Node:sigmoid , Node:loss ]<br></code></pre></td></tr></table></figure><p>咱们现在再定一个点，我们用一个变量存起来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sorted_nodes = topologic(convert_feed_dict_to_graph(feed_dict))<br></code></pre></td></tr></table></figure><p>那么咱们现在来模拟一下它的计算过程，模拟神经网络的计算过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fowward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;I am &#123;&#125;, I calculate myself value!!!&#x27;</span>.<span class="hljs-built_in">format</span>(self.name))<br><br><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.forward()<br><br>---<br>I am y, I calculate myself value!!!<br>I am x, I calculate myself value!!!<br>I am b, I calculate myself value!!!<br>I am k, I calculate myself value!!!<br>I am linear, I calculate myself value!!!<br>I am sigmoid, I calculate myself value!!!<br>I am loss, I calculate myself value!!!<br></code></pre></td></tr></table></figure><p>我们在<code>Node</code>里定义了一个方法<code>forward</code>，从前往后运算，这个时候我们在每个里面加一个向前运算。</p><p>这个就是拓朴排序的作用，经过排序之后，那需要在后面计算的节点，就一定会放在后面再进行计算。</p><p>好，那我们现在需要区分两个内容，一个是被赋值的内容，一个是需要计算的内容。</p><p>刚才我们说过，在大厂的这些地方，<code>x,y,k,b</code>这种东西都被定义为占位符，那我们来修改一下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs=[], name=<span class="hljs-literal">None</span></span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;I am &#123;&#125;, 我需要自己计算自己的值。&#x27;</span>.<span class="hljs-built_in">format</span>(self.name))<br>    ...<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Placeholder</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, name=<span class="hljs-literal">None</span></span>):<br>        Node.__init__(self, name = name)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;I am &#123;&#125;, 我已经被人为赋值了。&#x27;</span>.<span class="hljs-built_in">format</span>(self.name))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;Node:&#123;&#125; &#x27;</span>.<span class="hljs-built_in">format</span>(self.name)<br><br><span class="hljs-comment">### version - 02</span><br>node_x = Placeholder(name=<span class="hljs-string">&#x27;x&#x27;</span>)<br>node_y = Placeholder(name=<span class="hljs-string">&#x27;y&#x27;</span>)<br>node_k = Placeholder(name=<span class="hljs-string">&#x27;k&#x27;</span>)<br>node_b = Placeholder(name=<span class="hljs-string">&#x27;b&#x27;</span>)<br>node_linear = Node(inputs=[node_x, node_k, node_b], name=<span class="hljs-string">&#x27;linear&#x27;</span>)<br>node_sigmoid = Node(inputs=[node_linear], name=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>node_loss = Node(inputs=[node_sigmoid, node_y], name=<span class="hljs-string">&#x27;loss&#x27;</span>)<br></code></pre></td></tr></table></figure><p>我们创建了一个 Placeholder 类，继承了 Node,然后我们取修改初始化方法，它是是没有 input 的，只有一个 name。</p><p>然后 forward 我们改一下，改成打印已经被赋值的语句。父类 Node 里的forward 也改一下，改成需要自己计算自己的值。</p><p>那我们这个时候将赋值的四个节点改成调用 Placeholder。</p><p>接下来，我们需要修改<code>convert_feed_dict_to_graph</code>方法了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_feed_dict_to_graph</span>(<span class="hljs-params">feed_dict</span>):<br>    ...<br>    <span class="hljs-keyword">while</span> need_expend:<br>        ...<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(n, Placeholder): n.value = feed_dict[n]<br>        ...<br>    ...<br></code></pre></td></tr></table></figure><p>我们来检查这个节点是否是 Placeholder，如果是的话，将当前的 feed_dict赋值给 n.value。来看下结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.forward()<br><br>---<br>I am b, 我已经被人为赋值了。<br>I am x, 我已经被人为赋值了。<br>I am k, 我已经被人为赋值了。<br>I am y, 我已经被人为赋值了。<br>I am linear, 我需要自己计算自己的值。<br>I am sigmoid, 我需要自己计算自己的值。<br>I am loss, 我需要自己计算自己的值。<br></code></pre></td></tr></table></figure><p>好，到现在为止，咱们只是打了一段文字，问题是对于<code>linear, sigmoid</code>和<code>loss</code>,到底是怎么计算的呢？</p><p>这个问题，咱们放到下一节课里面去讲，现在咱们这篇文章已经超标了，目测应该超过万字了吧。</p><p>好，下节课记得来看咱们具体如何在实现拓朴排序后将计算加进去。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202310311950753.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;上节课，我们讲了多层神经网络的原理，并且明白了，数据量是层级无法超过
3 层的主要原因。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>20. 深度学习 - 多层神经网络</title>
    <link href="https://hivan.me/20.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://hivan.me/20.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2023-11-12T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:28.619Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310694.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><p>之前两节课的内容，我们讲了一下相关性、显著特征、机器学习是什么，KNN模型以及随机迭代的方式取获取 K 和 B，然后定义了一个损失函数（loss函数），然后我们进行梯度下降。</p><span id="more"></span><p>可以说是又帮大家回顾了一下深度学习的相关知识，但是由于要保证整个内容的连续性，所以这也没办法。</p><p>那么接下来的课程里，咱们要来看一下神经网络，怎么样去拟合更加复杂的函数，什么是激活函数，什么是神经网络，什么是深度学习。</p><p>然后我们还要来学习一下反向传播，以及如何实现自动的反向传播，什么是错误排序以及怎么样自动的去计算元素的gradients。梯度怎么样自动求导。</p><p>从简单的线性回归函数到复杂的神经网络，从人工实现的求导到自动求导。那我们现在来跟大家一起来看一下。</p><p>上一节课结束的时候我们说过，现实生活中绝大多数事情的关系都不是线性的。</p><p>比方说，我工作的特别努力，然后就可以升职加薪了。但是其实有可能工作的努力程度和升职加薪程度之间的关系可能并不是一条直线的函数关系。</p><p>可能一开始不管怎么努力，薪水都没有什么大的变化，可是忽然有了一个机会，薪水涨的幅度很大，但是似乎没怎么努力。再之后，又趋于一条平行横轴的线，不管怎么努力都无法往上有提升。这是不是咱们这些社畜的真实写照？</p><p>在现实生活中有挺多这样的问题，这样的对应关系。比如艾宾浩斯曲线，再比如细菌生长曲线，很多很多。</p><p>经过刚刚的分析我们知道了除了线性函数(kx+b)，还有一种常见的函数关系式，是一种 s 型的一种函数，这种 s形的函数在我们整个计算机科学里我们称呼它为<code>sigmoid</code>函数：</p><p><span class="math display">\[\begin{align*}Sigmoid: f(x) = \sigma (x) = \frac{1}{1+e^{-x}}\end{align*}\]</span></p><p>这是一个非常常见的函数。我们可以节用 NumPy库来用代码将它实现出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br></code></pre></td></tr></table></figure><p>把它的图像描述出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">sub_x = np.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br>plt.plot(sub_x, sigmoid(sub_x))<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310696.png"alt="Alt text" /></p><p>然后我们来利用一下这个函数，我们定义一个随机线性函数，然后和 sigmoid函数一起应用画 5 根不同的线：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_linear</span>(<span class="hljs-params">x</span>):<br>    k, b = random.random(), random.random()<br>    <span class="hljs-keyword">return</span> k*x + b<br><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    plt.plot(sub_x, random_linear(sigmoid(random_linear(sub_x))))    <br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310697.png"alt="Alt text" /></p><p>这个里面起变化的就是 k 和 b 这两个参数，那我们来调节 k 和 b的画，就可以变化这条曲线的样式。</p><p>除了以上这些函数，我们生活中还会遇到更复杂的函数，甚至很有可能是一个复杂的三维图像。</p><p>那这个时候，我们该如何去拟合这么多复杂的函数呢？</p><p>一个比较直接的方法，当然就是我们人为的去观察，比如这个类似于sin(x):</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310698.png"alt="Alt text" /></p><p>还有这个 k*sin(kx+b)+b:</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310699.png"alt="Alt text" /></p><p>当然，这样理论上是可以的，每当我们遇到一个场景之后就自己去提出来一个函数模型，然后把函数模型让机器去拟合出来。</p><p>但是这样就会有一个问题，大家就会发现假如你是那个工作者，那你熊猫眼会很严重，因为我们要看到很多这样的场景。现实生活中的问题实在太多了，每一天我们都可能会遇到新的函数。</p><p>如果我们每观察一个情况就要去考察，去思考它的这个函数模型是什么，你就会发现你的工作量无穷无尽。而且你会发现一个问题：现在函数能够可视化的，但是如果在现实生活中有很多场景的函数是无法可视化的。</p><p>那么这个时候我们就需要其他的一些方法能够拟合更加复杂的函数，这个时候我们怎么样不通过去观察它就能够拟合出复杂的函数呢？其实很简单。</p><p>有一个老头子叫做 Hinton，他是 2018 年图灵奖的获得者。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310700.png"alt="Alt text" /></p><p>他在一九八几年的时候发了一篇文章，经过他多年的研究，发现人的脑能够做出非常复杂的一些行为，其实我们这个神经元的类型都是很有限的，并没有很多奇怪的东西，就是有很多不同的节点，其实就那么几种。人类就能够进行复杂行为，背后其实就是一些基本的神经元的一些组合。</p><p>只不过这些基本的在组合还有一种形式，就是输入进来的会经过一个叫做activateneurons，就是激活单元，去做一个非线性变化。然后经过不断的这种非线性变化，最后就拟合出来非常复杂的信号。</p><p>那非线性变化的这些函数其实都是一样的，就他们背后的逻辑都是一样。只不过有的时候非线性变化的多，有的时候非线性变化的少。</p><p>讲了这么多不直观的东西，我们来看点实际的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    i = random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sub_x)))<br>    output_1 = np.concatenate((random_linear(sub_x[:i]), random_linear(sub_x[i:])))<br>    plt.plot(sub_x, output_1)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310701.png"alt="Alt text" /></p><p>然后我们来做两件事，第一个是将 k,b随机方式改成<code>normalvariate()</code>，第二个在上面的基础上再做一次拆分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_linear</span>(<span class="hljs-params">x</span>):<br>    k, b = random.normalvariate(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>), random.normalvariate(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> k * x + b<br><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    i = random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sub_x)))<br>    linear_output = np.concatenate((random_linear(sub_x[:i]), random_linear(sub_x[i:])))<br>    i_2 = random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(linear_output)))<br>    output = np.concatenate((sigmoid(linear_output[:i_2]), sigmoid(linear_output[i_2:])))<br><br>    plt.plot(sub_x, output)   <br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310702.png"alt="Alt text" /></p><p>我们来看，这个时候你会发现他生成的这个图像比较奇怪。它生成了很多奇怪的函数，每一次根据不同的参数就形成了不同的函数图像。</p><p>迄今为止就两个函数，一个 sigmoid，一个 randomlinear，但是它生成了很多奇怪的函数。</p><p>面对这种函数，这么多层出不穷列举都列举不完的函数，我们怎么样能够每次遇到一个问题就得去提出它这个函数到底是什么样。而且关键是有可能函数维度高了之后都观察不到它是什么关系。</p><p>所以我们就会去考虑，怎么样能够让机器自动的去拟合出来更复杂的函数呢？</p><p>我们可以用基本模块经过组合拼接，然后就能够形成复杂函数。而在组合拼接的过程中，我们只需要让机器自动的去把K1、K2、B1、B2 等等这些参数给它拟合出来就行。</p><p>也就是说我们可以通过参数的变化来拟合出来各种各样的函数。</p><p>这其实就是深度神经网络的一个核心思想。就是用基本的模块像大家玩积木一样，并不会有很多积木类型给你，只有一些基本的东西，但是通过这些基本的可以造出来特别多复杂的东西。</p><p>这个就是背后的原理，通过函数的复合和叠加。这种变化的引起都是由一个线性函数加上一个非线性函数。</p><p>其实很大程度上由我们大脑里边这种简单东西可以构成复杂东西得到了启示。只不过人的大脑里边，在脑神经科学里面把这种非线性变化呢叫做activate neurons，叫做激活神经元。在程序里，我们把这种非线性函数叫activation function，激活函数。</p><p>激活函数的作用就是为了让我们的程序能够拟合非线性关系。如果没有激活函数，咱们的程序永远只能拟合最简单的线性关系，而现实生活中绝大多数关系并不是并不是线性关系。</p><p>让机器来拟合更加复杂函数的这种方法，和我们的神经网络很像，就是咱们现在做的这个事情，我们就把它命了个名叫做神经网络。</p><p>早些年的时候科学家们有一个理论，人们把一组线性和非线性变化叫做一层。在以前的时候科学家们发现这个层数不能多于三层，就是神经网络的层数不能多于三层。</p><p>为什么不能多于 3层？其实最主要的不是计算量太大的问题，最核心的原因是什么？</p><p>假设我们有一个 f(x) 和一个 x组成一个平面坐标系，在其中有无数的点，当我们在做拟合的时候，发现了一条直线可以拟合，但是实际上呢，当我们将数据量继续放大的时候，才发现我们的拟合的直线偏离的非常厉害。</p><p>我们之前在机器学习的课程里说过，我们要有高精度，就需要有足够的数据量。如果这个时候变成一个三维问题，就需要更多的数据量。没有更多的数据的话，就好比有一个平板在空中，它会摇来摇去，你以为拟合了一个正确的平板，但其实完全不对。</p><p>那这个时候，每当我们所需要拟合的参数多一个，多少数据量认为是足够的？这个不一定。这个和整个问题的复杂程度有关系。</p><p>后来科学家们发现一个规律，在相似的问题下，我们需要拟合的参数多一个，需要的数据就要多一个数量级。</p><p>当变成三层的时候，会发现参数就更多了，而参数变得特别多就会需要特别多的数据量。而早在一九八几年、一九九几年的时候并没有那么多的数据量，就会产生数据量不够的情况，所以模型在现实生活中没法用。</p><p>但是随着到二零零几年，再到二零一几年之后，产生了大量的数据。就给我们做函数拟合提供了数据资源，所以数据量是最重要的，数据量决定了这个东西能不能做。而其他的一些，比方说计算、GPU啊等等，它是加速这个过程的，是让它更方便。</p><p>那么后来我们把层数超过 3层的就叫深度神经网络，机器学习就简称深度学习。这是为什么深度学习在二零一几年的时候才开始火起来。</p><p>那现在我们把上一节课的这个问题再拿过来，现在来想想，如果我们把房价的函数关系也写成类似的，linear和 sigmoid 之间的关系，那会怎么样呢？</p><p>首先，我们的 k 和 b 就会多加一组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_times):<br>    k1 = k1 + (-<span class="hljs-number">1</span>) * loss对k1的偏导<br>    b1 = b1 + (-<span class="hljs-number">1</span>) * loss对b1的偏导<br>    k2 = k2 + (-<span class="hljs-number">1</span>) * loss对k2的偏导<br>    b2 = b2 + (-<span class="hljs-number">1</span>) * loss对b2的偏导<br><br>    loss_ = loss(y, model(X_rm, k1, b1, k2, b2))<br>    ...<br></code></pre></td></tr></table></figure><p>然后我们的 model也会多接受了一组参数，并且我们要将其内部函数关系做一个叠加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model</span>(<span class="hljs-params">x, k1, k2, b1, b2</span>):<br>    linear1_output = k1 * x + b1<br>    sigmoid_output = sigmoid(linear1_output)<br>    linear2_output = k2 * sigmoid_output + b2<br><br>    <span class="hljs-keyword">return</span> linear2_output<br></code></pre></td></tr></table></figure><p>在这个时候我们要求解的时候就会发现有个问题，我们现在的 loss是这样的：</p><p><span class="math display">\[\begin{align*}loss &amp; = \frac{1}{N}(y_i - \hat y)^2 \\&amp; = \frac{1}{N} \begin{bmatrix}y_1 - (k_2 \frac{1}{1+e^{-(k_1x+b_1)}}+b_2)\end{bmatrix} ^2\end{align*}\]</span></p><p>似乎变得有点过于复杂。当前情况下，我们是可以复杂的去求导，但是当函数继续复杂下去的时候，怎么把这个导数求出来呢？</p><p>函数还可以继续叠加，层数还可以写的越来越多。那么怎么样才能给它求出它的导数呢？</p><p>我们再将上面的式子做个变化：</p><p><span class="math display">\[\frac{1}{N}[l_2(\sigma(l_1(x))) -y_1]^2\]</span></p><p>这样我们就可以将问题进行简化，我们上面代码里<code>loss对k1的偏导</code>就可以写成：</p><p><span class="math display">\[\frac{\partial loss}{\partial  l_2} \cdot \frac{\partial l_2}{\partial\sigma} \cdot \frac{\partial \sigma}{\partial l_1} \cdot \frac{\partiall_1}{\partial k_1}\]</span></p><p>同理，<code>loss对b1的偏导</code>就是：</p><p><span class="math display">\[\frac{\partial loss}{\partial  l_2} \cdot \frac{\partial l_2}{\partial\sigma} \cdot \frac{\partial \sigma}{\partial l_1} \cdot \frac{\partiall_1}{\partial b_1}\]</span></p><p>这个时候，问题就变成一个可解决的了。</p><p><span class="math inline">\(\frac{\partial loss}{\partiall_2}\)</span>其实就等于<span class="math inline">\(\frac{2}{N}(l_2 -y_1)\)</span>。</p><p>我们继续往后看第二部分，那么这个时候我们可以得到<spanclass="math inline">\(l_2 = k_2 \cdot \sigma + b_2\)</span>，那<spanclass="math inline">\(\frac{\partial l_2}{\partial\sigma}\)</span>就等于<span class="math inline">\(k_2\)</span>。</p><p>再来看第三部分，<span class="math inline">\(\sigma&#39;(x) =\sigma(x) \cdot (1- \sigma(x)\)</span>, 所以<spanclass="math inline">\(\frac{\partial \sigma}{\partial l_1} = \sigma\cdot (1 - \sigma)\)</span>。</p><p>最后第四部分，<span class="math inline">\(\frac{\partiall_1}{\partial k_1} = x\)</span>。</p><p>这样，我们整个式子就应该变成这样：</p><p><span class="math display">\[\begin{align*}\frac{2}{N}(l_2 - y_1) \cdot k_2 \cdot \sigma \cdot (1 - \sigma) \cdot x\end{align*}\]</span></p><p>这样的话，我们就把 loss 对于 K1的偏导就求出来了，这里算是一个突破。本来看起来是很复杂的的一个问题，我们将其拆分成了这样的一种形式。那这种形式，我们把它称作「链式求导」。</p><p>但是现在其实还有个问题，这整个一串链式求导的东西是我们通过眼睛求出来的，但是现在怎么样让机器自动的把这一串东西写出来？就是机器怎么知道是这些数字乘到一起？</p><p>换句话说，我们现在把这个问题再形式化一下，定义一个问题。</p><p>给定一个模型定义，这个模型里边包含参数<code>&#123;k1, k2, b1, b2&#125;</code>，我们要构建一个程序，让它能够求解出k1,k2,b1,b2 的偏导是多少。</p><p>如果我们想解决这个问题，我们首先要思考一下，<spanclass="math inline">\(k_1, k_2, b_1, b_2, l_1, l_2, \sigma, y_{true},loss\)</span>, 它们之间是一种什么样的关系。</p><p>观察一下我们会发现它们之间的关系是这样的：</p><p><span class="math display">\[\begin{align*}&amp; \{k_1, b_1, x\} \to l_1 \to \sigma, \\&amp; \{\sigma, k_2, b_2\} \to l_2, \\&amp; \{l_2, y_{true}\} \to loss \\&amp; \to 表示的是&#39;输出到&#39;的关系。\end{align*}\]</span></p><p>要用计算机去表示这种关系，是典型的一个数据结构问题，怎么样让计算机合理的去存储它，你会发现这个是一个图案。</p><p>这种节点和节点之间通过关系连接起来的就把它叫做图，我们把它先表示成图的样子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_graph = &#123;<br>    <span class="hljs-string">&#x27;k1&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;b1&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;x&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;L1&#x27;</span>:[<span class="hljs-string">&#x27;sigmoid&#x27;</span>],<br>    <span class="hljs-string">&#x27;sigmoid&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;k2&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;b2&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;L2&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>],<br>    <span class="hljs-string">&#x27;y&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>]<br>&#125;<br><br>nx.draw(nx.DiGraph(computing_graph), with_labels = <span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310703.png"alt="Alt text" /></p><p>这个就是我们要表达的一个关系，我们把这个变成图。</p><p>现在我们将给定的一个model，这样一个函数变成了这样一张图。计算机里有现成的各种各样的图算法，我们就可以来计算这个图之间的关系了。</p><p>现在我们就要根据这个图的表示来思考我们如何求 loss 对 K1的偏导。那其实，我们可以发现 k1 在末尾出，一直在向前输入直到loss。换句话说，我们可以通过 k1一直往图的终点去寻找来找到它求导的这个过程。</p><p>也就是说，只要我们的能把模型变成一个图，然后我们就可以根据这些点去找到它们之间节点的对应关系，我们就可以通过这个节点关系来获得它的求导过程了。</p><p>那下一节课呢，我们就继续来看一下，如何将这个图的关系，变成一个自动求导的过程。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310694.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;之前两节课的内容，我们讲了一下相关性、显著特征、机器学习是什么，KNN
模型以及随机迭代的方式取获取 K 和 B，然后定义了一个损失函数（loss
函数），然后我们进行梯度下降。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>19. 深度学习 - 用函数解决问题</title>
    <link href="https://hivan.me/19.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E7%94%A8%E5%87%BD%E6%95%B0%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/"/>
    <id>https://hivan.me/19.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E7%94%A8%E5%87%BD%E6%95%B0%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/</id>
    <published>2023-11-09T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:32.376Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231030164335.png"alt="茶桁的 AI 秘籍 核心基础 19" /></p><p>Hi，你好。我是茶桁。</p><p>上一节课，我们从一个波士顿房价的预测开始写代码，写到了 KNN。</p><span id="more"></span><p>之前咱们机器学习课程中有讲到 KNN这个算法，分析过其优点和缺点，说起来，KNN这种方法比较低效，在数据量比较大的时候就比较明显。</p><p>那本节课，我们就来看一下更加有效的学习方法是什么，A more EfficientLearning Way.</p><p>接着我们上节课的代码我们继续啊，有不太了解的先回到上节课里去看一下。</p><p>我们<code>X_rm</code>和<code>y</code>如果能够找到这两者之间的函数关系，每次要计算的时候，输入给这个函数，就能直接获得预测值。</p><p>那这个函数关系怎么获得呢？我们需要先观察一下，这个时候就到了我们的拟合函数关系。</p><p>那既然要观察，当然最好就是将数据可视化之后进行观察：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.scatter(X_rm, y)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231030164157.png"alt="Alt text" /></p><p>可以看到，它们之间的关系大体应该这样一种关系：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231030164158.png"alt="Alt text" /></p><p>那这个样子的图我们熟悉不？是不是在线性回归那一张里我们见过？也就是用一根直线去拟合了这些点的一个趋势。</p><p>我们把它写出来：</p><p><span class="math display">\[f(x) = k \cdot rm + b\]</span></p><p>那我们现在就会把这个问题变成，假设现在的函数<code>k*rm+b</code>，那我们就需要找到一组k 和b，然后让它的拟合效果最好。这个时候我们就会遇到一个问题，拟合效果怎样算是好？</p><p>比方说我们现在有一组数据，一组实际的值，还有一组预测值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">real_y = &#123;<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>&#125;<br>y_hats = &#123;<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">7</span>&#125;<br>y_hats2 = &#123;<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>&#125;<br></code></pre></td></tr></table></figure><p>问哪个值更好。</p><p>我们会发现这两个预测都挺好的，那哪个更好？这个时候我们需要搬出我们的loss 函数了。</p><p>loss函数就是在我们进行预测的时候，它的信息损失了多少，所以我们称其为损失函数，loss函数。 <span class="math display">\[loss(y, \hat y) = \frac{1}{N}{\sum_{i \in N}}(y_i - \hat {y_i})^2\]</span> y_i - yhat_i 这个值越接近于 0。等于 0 的意思就是每一个预测的 y都和实际的 y 的值是一样的。那么如果这个值越大指的是预测的 y 和实际的 y之间差的越大。</p><p>那我们在这个地方就可以定义一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">y, yhat</span>):<br>    <span class="hljs-keyword">return</span> np.mean((np.array(y) - np.array(yhat))** <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p>然后我们直接将两组 yhat 和真实的 real_y 代入进去比对：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">loss(real_y, y_hats)<br>loss(real_y, y_hats2)<br><br>---<br><span class="hljs-number">1.3333333333333333</span><br><span class="hljs-number">0.3333333333333333</span><br></code></pre></td></tr></table></figure><p>所以它这个意思是说 yhats2 的效果更好一些。</p><p>那我们将上面这个 loss 函数就叫做 Mean SquaredError，就是均方误差，也简称 MSE。咱们现在有了loss，就有了是非判断的标准了，就可以找到最好的结果。</p><p>有了判断标准怎么样来获得最优的 k 和 b呢？早些年的时候有这么几种方法，第一种是直接用微积分的方法做计算。 <spanclass="math display">\[\begin{align*}loss &amp; = \frac{1}{N}\sum_{i\in N}(y_i - \hat y)^2 \\&amp; = \frac{1}{N}\sum_{i\in N}(y_i - (kx_i + b))^2 \\\end{align*}\]</span> 此时我们是知道 x_i 和 y_i 的值，N也是常数。那么其实求偏导之后它就可以变化成下面这组式子： <spanclass="math display">\[Ak^2 + Bk +C \\A&#39;b^2+B&#39;b+C&#39;\]</span> A、B、C 是根据我们所知道的 x_i 和 y_i 以及常数 N来计算出来的数。这个时候 loss 要取极值的时候，我们令其为 loss’，那loss’就等于-A/2B，或者-A’/2B’。那么这种方法我们就称之为最小二乘法，它是为了最小化MSE，对 MSE 求偏导数并令其等于零，来找到使 MSE 最小的参数值。</p><p>但是为什么后来人们没有用微积方的方法直接做呢？是因为这个函数会变得很复杂，当函数变得极其复杂的时候，学过微积分的同学就应该知道，你是不能直接求出来他的导数的。也就是说当函数变得极其复杂的时候，直接用微积分是求不出来极致点的，所以这种方法后来就没用。</p><p>第二种方法，后来人们想了可以用随机模拟的方法来做。</p><p>我们首先来在 -100 到 100 之间随机两个值：k 和 b</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">VAR_MAX, VAR_MIN = <span class="hljs-number">100</span>, -<span class="hljs-number">100</span><br>k, b = random.randint(VAR_MIN, VAR_MAX), random.randint(VAR_MIN, VAR_MAX)<br></code></pre></td></tr></table></figure><p>只拿到一组当然是无从比较的，所以我们决定拿个 100 组的随机值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">total_times = <span class="hljs-number">100</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_times):<br>    k, b = random.randint(VAR_MIN, VAR_MAX), random.randint(VAR_MIN, VAR_MAX)<br></code></pre></td></tr></table></figure><p>然后定义一个值，叫做最小的 loss。这个最小的 loss一开始取值为无穷大，并且再给两个值，最好的 k 和最好的b，先赋值为<code>None</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">min_loss = <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)<br>best_k, best_b = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><p>之后我们要拿预测值来赋值给新的loss，我们来定义一个函数，它要做的事情很简单，就是返回<code>k*x+b</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model</span>(<span class="hljs-params">x, k, b</span>):<br>    <span class="hljs-keyword">return</span> k*x + b<br><br>loss_ = loss(y, model(X_rm, k, b))<br></code></pre></td></tr></table></figure><p>接着我们就可以来进行对比了，就会找到那组最好的 k 和 b：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> loss_ &lt; min_loss:<br>    min_loss = loss_<br>    best_k, best_b = k, b<br></code></pre></td></tr></table></figure><p>完整的代码如下，当然我们是接着之前的代码写的，所以<code>loss</code>函数和<code>y</code>，还有<code>X_rm</code>都是在之前代码中有过定义的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">VAR_MAX, VAR_MIN = <span class="hljs-number">100</span>, -<span class="hljs-number">100</span><br>min_loss = <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)<br>best_k, best_b = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model</span>(<span class="hljs-params">x, k, b</span>):<br>    <span class="hljs-keyword">return</span> x * k +b<br><br>total_times = <span class="hljs-number">100</span><br><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_times):<br>    k, b = random.randint(VAR_MIN, VAR_MAX), random.randint(VAR_MIN,VAR_MAX)<br><br>    loss_ = loss(y, model(X_rm, k, b))<br><br>    <span class="hljs-keyword">if</span> loss_ &lt; min_loss:<br>        min_loss = loss_<br>        best_k, best_b = k, b<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;在&#123;&#125;时刻找到了更好的 k: &#123;&#125;, b: &#123;&#125;，这个 loss 是：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(t, k, b, loss_))<br><br>---<br>在<span class="hljs-number">0</span>时刻找到了更好的k: <span class="hljs-number">12</span>, b: <span class="hljs-number">89</span>， 这个loss是：<span class="hljs-number">20178.46882444269</span><br>在<span class="hljs-number">8</span>时刻找到了更好的k: <span class="hljs-number">2</span>, b: <span class="hljs-number">2</span>， 这个loss是：<span class="hljs-number">131.87000511462452</span><br>在<span class="hljs-number">21</span>时刻找到了更好的k: <span class="hljs-number">11</span>, b: -<span class="hljs-number">48</span>， 这个loss是：<span class="hljs-number">47.340357088932805</span><br></code></pre></td></tr></table></figure><p>如果我们将寻找的次数放大，改为 10**3,那我们会发现，开始找的很快，但是后面寻找的会越来越慢。</p><p>就类似于你现在在一个公司，假设你从刚进去的时候，要达到职位很高，薪水很高。小职员你想一直升职，你可以随机的去做很多你喜欢做的事情，没有人指导你。一开始的时候，你会发觉自己的升职加薪似乎并没有那么困难，但是随着自己越往上，升职的速度就降下来了，因为上面职位并没有那么多了。这个时候你所需要尝试和努力就会越来越多。到后面你每尝试一步，你所需要的努力就会越来越多。</p><p>那么这个时候我们就要想，我们怎么样能够让更新频率更快呢？而不要像这样到后面基本上不更新了。</p><p>不知道我们是否还记得大学时候的数学知识，假设现在这个 loss 和 k在一个二维平面上，我们对 loss 和 k 来求一个偏导：</p><p><span class="math display">\[\frac{\partial loss}{\partial k}\]</span></p><p>这个导数的取值范围就会导致两种情况，当其大于 0 的时候，k 越大，则loss 也越大，当其小于 0 的时候，k 越大，loss 则越小。</p><p>那我们在这里就可以总结出一个规律：</p><p><span class="math display">\[p&#39; = p + (-1)\frac{\partial loss}{\partial p} * \alpha\]</span></p><p><spanclass="math inline">\(\alpha\)</span>就是一个很小的数，因为我们每次要只能移动很小的一点，不能减小很多。</p><p>那有了这个，我们就可以将我们的 k 和 b 应用上去，也就可以得到：</p><p><span class="math display">\[\begin{align*}k&#39; = k + (-1)\frac{\partial loss}{\partial k} \cdot \alpha \\b&#39; = b + (-1)\frac{\partial loss}{\partial b} \cdot \alpha \\\end{align*}\]</span></p><p>那我们如何使用计算机来实现刚刚讲的这些内容呢？我们先把上面的式子再做一下变化：</p><p><span class="math display">\[k_{n+1} = k_n + -1 \cdot \frac{\partial loss(k, b)}{\partial k_n} \\b_{n+1} = b_n + -1 \cdot \frac{\partial loss(b, b)}{\partial b_n}\]</span></p><p>这个就是所谓的梯度下降。</p><p>那现在的问题就变成，如何使用计算机来实现梯度下降。我们就来定义两个求导函数，并且将之前的代码拿过来做一些修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">y, yhat</span>):<br>    <span class="hljs-keyword">return</span> np.mean((np.array(y) - np.array(yhat)) ** <span class="hljs-number">2</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partial_k</span>(<span class="hljs-params">x, y, k_n, b_n</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * np.mean((y - (k * x + b))*(-x))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partial_b</span>(<span class="hljs-params">x, y, k_n, b_n</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * np.mean((y - (k * x + b))*(-<span class="hljs-number">1</span>))<br><br>k,b = random.random(), random.random()<br><br>min_loss = <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)<br>best_k, best_b = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br>total_times = <span class="hljs-number">500</span><br>alpha = <span class="hljs-number">1e-3</span><br><br>k_b_history = []<br><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_times):<br>    k = k + (-<span class="hljs-number">1</span>) * partial_k(X_rm, y, k, b) * alpha <br>    b = b + (-<span class="hljs-number">1</span>) * partial_b(X_rm, y, k, b) * alpha<br><br>    loss_ = loss(y, model(X_rm, k, b))<br><br>    <span class="hljs-keyword">if</span> loss_ &lt; min_loss:<br>        min_loss = loss_<br>        best_k, best_b = k, b<br>        k_b_history.append([best_k, best_b])<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;在&#123;&#125;时刻找到了更好的 k: &#123;&#125;, b: &#123;&#125;，这个 loss 是：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(t, k, b, loss_))<br><br>---<br>在<span class="hljs-number">0</span>时刻找到了更好的k: <span class="hljs-number">0.8391888851738278</span>, b: <span class="hljs-number">0.44333100376779605</span>， 这个loss是：<span class="hljs-number">360.000103176194</span><br>在<span class="hljs-number">1</span>时刻找到了更好的k: <span class="hljs-number">1.0586893752129705</span>, b: <span class="hljs-number">0.474203003102507</span>， 这个loss是：<span class="hljs-number">312.7942150454931</span><br>...<br>在<span class="hljs-number">498</span>时刻找到了更好的k: <span class="hljs-number">3.587603582169745</span>, b: <span class="hljs-number">0.40777844839877003</span>， 这个loss是：<span class="hljs-number">58.761172062586965</span><br>在<span class="hljs-number">499</span>时刻找到了更好的k: <span class="hljs-number">3.587736446932306</span>, b: <span class="hljs-number">0.4069332804559017</span>， 这个loss是：<span class="hljs-number">58.760441520932375</span><br></code></pre></td></tr></table></figure><p>其实关于这个内容，我们在机器学习 -线性回归那一章就介绍过。看不懂这一段的小伙伴可以回过头取好好看一下那一章。</p><p>那这样，我们可以发现，之前是间隔很多次才作一词更新，而现在是每一次都会进行更新，一直在减小。这个是因为我们实现了一个「监督」。</p><p>在这样的情况下结果就变得更好了，比如我们再将次数调高一点，在全部运行完之后，我们来画个图看看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(X_rm, y)<br>plt.scatter(X_rm, best_k * X_rm + best_b, color=<span class="hljs-string">&#x27;orange&#x27;</span>)<br>plt.plot(X_rm, best_k * X_rm + best_b, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231030164159.png"alt="Alt text" /></p><p>我们可以看到它拟合出来的点和连接成的直线，和我们上面手动去画的似乎还是有很大差别的。</p><p>在刚才的代码里我还做了一件事情，定义了一个<code>k_b_history</code>,然后将所有的 best_k 和 best_b都存储到了里面。然后我们随机取几个点，第一个取第 10 个测试点，第二个取第50 次测试点，第三个我们取第 5000 次，第四个我们取最后一次：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_0, test_1, test_2, test_3, test_4 = <span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>, <span class="hljs-number">5000</span>, -<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>然后我们分别画一下这几个点的图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(X_rm, y)<br>plt.scatter(X_rm, k_b_history[test_0][<span class="hljs-number">0</span>] * X_rm + k_b_history[test_0][<span class="hljs-number">1</span>])<br>plt.scatter(X_rm, k_b_history[test_1][<span class="hljs-number">0</span>] * X_rm + k_b_history[test_1][<span class="hljs-number">1</span>])<br>plt.scatter(X_rm, k_b_history[test_2][<span class="hljs-number">0</span>] * X_rm + k_b_history[test_2][<span class="hljs-number">1</span>])<br>plt.scatter(X_rm, k_b_history[test_3][<span class="hljs-number">0</span>] * X_rm + k_b_history[test_3][<span class="hljs-number">1</span>])<br>plt.scatter(X_rm, k_b_history[test_4][<span class="hljs-number">0</span>] * X_rm + k_b_history[test_4][<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231030164200.png"alt="Alt text" /></p><p>我们就可以看到，刚开始的时候和最后的一次拟合的线的结果，还有中间一步步的拟合的变化。这条线在往上面一步一步的走。这样我们相当于是透视了它整个获得最优的k 和 b 的过程。</p><p>那这个时候我们来看一下，咱们怎么怎么预测呢？我们可以拿我们的<code>best_k</code>和<code>best_b</code>去输出最后的预测值了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">model(<span class="hljs-number">7</span>, best_k, best_b)<br><br>---<br><span class="hljs-number">28.718752244698216</span><br></code></pre></td></tr></table></figure><p>预测出来是 28.7 万。那房间数目为 7 的时候，我们预测出这个价格是 28.7万，还记得咱们上节课中用 KNN 预测出来的值么？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">find_price_by_simila(rm_to_price, <span class="hljs-number">7</span>)<br><br>---<br><span class="hljs-number">29.233333333333334</span><br></code></pre></td></tr></table></figure><p>是 29万对吧？现在我们就能看到了，这两种方式预测值基本很接近，都能预测。</p><p>那么我们使用函数来进行预测的原因还有一个，就是我们在使用函数在进行学习之后，然后拿模型去计算最后的值，这个计算过程速度会快很多。</p><p>好，咱们下节课将会学习怎样拟合更加复杂的函数，因为这个世界上的函数可不仅仅是最简单线性，还得拟合更加复杂的函数。</p><p>然后再后面的课程，我们会讲到激活函数，开始接触神经网络，什么是深度学习。</p><p>然后我们要来讲解一个很重要的概念，就是反向传播，会讲怎么样实现自动的反向传播。实现了自动的反向传播，我们会基于拓普排序的方法让计算机能够自动的计算它的梯度和偏导。</p><p>在讲完这些之后，基本上我们就有了构建一个深度学习神经网络框架的内容了。</p><p>好，希望小伙伴们在今天的课程中有所收获。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231030164335.png&quot;
alt=&quot;茶桁的 AI 秘籍 核心基础 19&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;上一节课，我们从一个波士顿房价的预测开始写代码，写到了 KNN。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>18. 深度学习 - 从零理解神经网络</title>
    <link href="https://hivan.me/18.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E4%BB%8E%E9%9B%B6%E7%90%86%E8%A7%A3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://hivan.me/18.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E4%BB%8E%E9%9B%B6%E7%90%86%E8%A7%A3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2023-11-07T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:36.061Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232439.png"alt="Alt text" /></p><p>Hi, 你好。我是茶桁。</p><p>我们终于又开启新的篇章了，从今天这节课开始，我们会花几节课来理解一下深度学习的相关知识，了解神经网络，多层神经网络相关知识。并且，我们会尝试着来打造一个自己的深度学习框架。</p><span id="more"></span><p>以前很多时候都会被人问到很多问题，其中比较多的就包括现在各种各样的框架应该用到哪一个，在学习人工智能的时候，对于深度学习框架有比较多的问题。那在这里我就希望能帮助各位小伙伴彻底的去理解一下什么是学习框架。</p><p>对于我们来说，就像小孩子去学一个东西，最好的就是从头到尾能把它拆了，然后再重建起来。</p><p>从今天开始往后的几节课里，我们都会去好好了解「如何从零构建一个深度学习框架」。</p><h2 id="本文目标">本文目标</h2><p>我们基本的核心目的就是来讲明白，什么是神经网络，以及神经网络的原理是什么。</p><p>我们要知道，人工智能有很多方法，但是神经网络是现代人工智能里面一个非常核心的内容。</p><p>咱们现在就是要先去了解神经网络的原理是怎么回事，然后在这个过程中我们来讲解清楚神经网络的框架到底是什么样的。</p><p>如我们之前学习过的几节机器学习课程，会发现它有很多的概念。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232440.png"alt="Alt text" /></p><p>比方说非监督学习、监督学习、强化学习，监督学习里面又分了回归和分类等等。</p><p>很多人看到这些，在初次接触、初次学习的时候就觉得人工智能很复杂，很难学会。除此之外，我们在学到人工智能目前比较核心的一个内容是关于深度学习神经网络。好多人不知道深度学习神经网络到底是什么原理。</p><p>在整个学习过程会发现有很多很多的问题，概念很多，变体也很多，学习很困难。</p><p>那这里要跟大家强调一点，就是千万别成为「马保国」，为什么这里会提到这个人呢？在我看来，这其实是一类人，他是一类人的代表。就是整很多的概念，假装子集很厉害。</p><p>就是我们脑子里不要总是去提很多概念，或者说很多很花哨的东西，最重要的还是基本功修炼好。我一直都强调一个观念，就是基础学科，基本功才是所有学科的基石。过多的概念其实并没有什么卵用。</p><p>早些时候，我上班的地方有一个叫「李雨晨」（匿名🙄）的产品经理，各种概念信手拈来，都是一些高大上的东西。也是将面试官唬的一愣一愣的。当时大家也是没多想，心想人家既然是个牛逼人物，那就多配合人家呗，结果是没过3 个月就原形毕露，当然是下面干事的人最先觉察出来的。</p><p>没办法，为了继续装下去只能是利用自己的职权和谎言去盗用别人的成果，比如设计稿啊，文档啊啥的，拿着当自己的东西向上汇报。</p><p>再然后，基本人人都开始防着他了，就开始恼羞成怒，一直打压那个最开始说他不行并防着他的产品。不过不行就是不行，其实最开始就能看出端倪，因为基本没有一家公司干活超过6个月，那肯定是有问题的。就这资质也能忽悠成高级产品经理，也能看出来那会儿产品这个行业的水份多大，门槛多低。不过终归潮水退了之后，裸泳的王八都要现行是吧。</p><p>好，说这么多吐槽的话其实也是想说一个道理，不要去搞花里胡哨的玩意，踏踏实实的把基本功练扎实，否则一时唬的了人，但是终归是走不远。</p><p>那这也是咱们这节课的目的，让大家去除掉背后这些繁杂的表象，那么它背后到底是什么，这就是咱们这三天的目的。</p><p>这些年，人工智能已经应用到我们各个地方了。先不说现在大火的AIGC，人工智能还应用到其他各个地方。</p><p>比方说在商场购物的时候，它的楼宇灯光，自动停车都是在做这些事情。买票的时候，机场，火车站都有人脸识别。每天给你推荐的各种商品，以及我们做物流配送等等这些东西，背后都有人工智能。</p><p>而这些人工智能背后有一个很重要的东西，就是用到了神经网络框架。</p><p>比方说众所周知的 TensorFlow,我们每次调用的时候，框架背后调用了很多东西。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Store layers weight &amp; bias</span><br><span class="hljs-comment"># A random value generator to initialize weights.</span><br>random_normal = tf.initializers.RandomNormal()<br><br>weights = &#123;<br>    <span class="hljs-string">&#x27;h1&#x27;</span>: tf.Variable(random_normal([num_features, n_hidden_1])),<br>    <span class="hljs-string">&#x27;h2&#x27;</span>: tf.Variable(random_normal([n_hidden_1, n_hidden_2])),<br>    <span class="hljs-string">&#x27;out&#x27;</span>: tf.Variable(random_normal([n_hidden_2, num_classes]))<br>&#125;<br><br>biases = &#123;<br>    <span class="hljs-string">&#x27;b1&#x27;</span>: tf.Variable(tf.zeros([n_hidden_1])),<br>    <span class="hljs-string">&#x27;b2&#x27;</span>: tf.Variable(tf.zeros([n_hidden_2])),<br>    <span class="hljs-string">&#x27;out&#x27;</span>: tf.Variable(tf.zeros([num_classes]))<br>&#125;<br>...<br><br><span class="hljs-comment"># Create model.</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">neural_net</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-comment"># Hidden fully connected layer with 128 neurons.</span><br>    layer_1 = tf.add(tf.matmul(x, weights[<span class="hljs-string">&#x27;h1&#x27;</span>]), biases[<span class="hljs-string">&#x27;b1&#x27;</span>])<br>    <span class="hljs-comment"># Apply sigmoid to layer_1 output for non_linerity.</span><br>    layer_1 = tf.nn.sigmoid(layer_1)<br><br>    <span class="hljs-comment"># Hidden fully connected layer with 256 neurons.</span><br>    layer_2 = tf.add(tf.matmul(layer_1, weights[<span class="hljs-string">&#x27;h2&#x27;</span>]), biases[<span class="hljs-string">&#x27;b2&#x27;</span>])<br>    <span class="hljs-comment"># Apply sigmoid to layer_1 output for non_linerity.</span><br>    layer_2 = tf.nn.sigmoid(layer_2)<br><br>    <span class="hljs-comment"># Output fully connected layer with a neuron for each class.</span><br>    out_layer = tf.matmul(layer_2, weights[<span class="hljs-string">&#x27;out&#x27;</span>]) + biases[<span class="hljs-string">&#x27;out&#x27;</span>]<br>    <span class="hljs-comment"># Apply softmax to normalize the logits to a probability distribution</span><br>    <span class="hljs-keyword">return</span> tf.nn.softmax(out_layer)<br></code></pre></td></tr></table></figure><p>我们现在想把这些框架搞清楚，就需要知道它背后这些东西到底是什么原理、什么原因。</p><p>那这几节课之后，就希望我们能从 0 到 1学会创建一个深度学习框架，从底层来理解这个神经网络的原理，理解现代人工智能的核心。</p><p>一开始的课程，内容也会稍微比较简单一些，越往后咱们就越难一点。最后，彻底理解深度学习神经网络原理。</p><h2 id="预测趋势与关系">预测趋势与关系</h2><p>我们以一个趋势预测的问题为引入。</p><p>如果对于自然哲学或者说科学研究这些，就是对科学研究方法论感兴趣的话，你会知道我们整个科学研究其实分为三个层面。</p><p>不管是牛顿、爱因斯坦，还是伽利略、图灵等等，所有的科学研究，所有的research，不管是关于数据还是别的，它都是三个层面。</p><p>第一个层面叫做描述性的，第二个叫做因果推理，第三个叫做未来的预测。</p><p>就说我们所有的科学活动，所有的研究活动都可以归为这三类。</p><p>描述性的东西，比方说你又长胖了多少，然后又增加了多少重量。今天的体重，明天的体重等等。</p><p>除此之外第二个层面是我们要看出来它们之间的相关性。比方吃的多和你长胖，它们之间是呈正相关的。还有其他的一些关系，比方说是呈负相关的等等。</p><p>那我们最重要也是最难的一个科学活动是要对它进行未来的预测，对于未来的预测。这个未来它不仅是predict。</p><p>比方说现在你知道的是几组数据，知道每个对应的结果。然后你看到了一组没有见过的数据，你去预测它。</p><p>就好比一个孩子做题，他见过的题都能做，没见过的题他也要会做。这个其实就是属于对未来的一种预测能力。</p><p>关于预测，我们最关心的预测是关于我们的身体健康，能活多久；还有就是关于挣钱的问题。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232441.png"alt="Alt text" /></p><p>我们看一下这个例子，你的性别和你的吸烟的频率，跟一种疾病（可能是肺癌），它会有一个相对应的概率。</p><p>性别不同，年龄不同，抽烟频率不同。我们会发现，得病概率随着年龄的增大并不会有多少增加，此时男性得病概率反而比女性还小。</p><p>但是随着抽烟频率越多，得病概率上升的非常快。其中呢，同样的年龄和抽烟频率下，男性得病的概率则会更高。</p><p>假设存在一个人 p，男性，年龄是 72 岁，他每天抽三根：P{age:72, sex:male, rate:3/day}。那他得这种病的概率大约是多少？那我们就先在图上随意画一个，假如说就如图的位置一样的概率：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232442.png"alt="Alt text" /></p><p>那么这个概率到底是多少？我们就需要用到数据去做预测，此时我们就得去做个拟合。</p><p>除此之外，我们再来看BMI，也就是身体指数。身体指数就是体重除以身高的平方：BMI =kg/h^2，越大就表示你越胖。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232443.png"alt="Alt text" /></p><p>当你到某一个值的时候，可以看到得病的概率。</p><p>我们假设有一个人 180 斤，身高一米73，我们来预测他得肾病的概率是多少。这个时候我们还是需要去做预测。</p><h2 id="波士顿房价预测">波士顿房价预测</h2><p>现在就来看一个非常经典的预测案例：波士顿房价案例。这个波士顿房价的数据，我们曾经在机器学习的线性回归里有用到，不知道小伙伴们有没有去看过。</p><p>波士顿地区是在美国东北部，房地产的价钱也比较稳定，那这个数据也是比较老的数据了，通过这些数据来考察，希望机器能够根据输入的内容来预测它的房价。</p><p>现在就以波士顿房价问题为例，来讲讲计算机怎么去预测。然后在预测的过程中我们来讲解实现深度学习的原理。最终把它封装成我们所需要的一个深度学习框架。</p><p>第一步自然是加载和分析数据。</p><p>之前的课程我提到过，这个数据由于一些原因，sklearn 的 datasets中已经删除了，那我们要想加载数据，就需要用到其中的 fetch_openml：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_openml<br><br>dataset = fetch_openml(name=<span class="hljs-string">&#x27;boston&#x27;</span>, version=<span class="hljs-number">1</span>, as_frame=<span class="hljs-literal">True</span>, return_X_y=<span class="hljs-literal">False</span>, parser=<span class="hljs-string">&#x27;pandas&#x27;</span>)<br></code></pre></td></tr></table></figure><p>在我们第一次获取到这个数据不知道怎么处理的时候，我们可以使用 dir来看看这个数据里面的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">dir</span>(dataset)<br><br>---<br>[<span class="hljs-string">&#x27;DESCR&#x27;</span>, <span class="hljs-string">&#x27;categories&#x27;</span>, <span class="hljs-string">&#x27;data&#x27;</span>, <span class="hljs-string">&#x27;details&#x27;</span>, <span class="hljs-string">&#x27;feature_names&#x27;</span>, <span class="hljs-string">&#x27;frame&#x27;</span>, <span class="hljs-string">&#x27;target&#x27;</span>, <span class="hljs-string">&#x27;target_names&#x27;</span>, <span class="hljs-string">&#x27;url&#x27;</span>]<br></code></pre></td></tr></table></figure><p>我们看到这个 dataset里有一个<code>feature_names</code>，直觉上这个应该是一些特征名称，我们来查看一下这个的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset[<span class="hljs-string">&#x27;feature_names&#x27;</span>]<br><br>---<br>[<span class="hljs-string">&#x27;CRIM&#x27;</span>, <span class="hljs-string">&#x27;ZN&#x27;</span>, <span class="hljs-string">&#x27;INDUS&#x27;</span>, <span class="hljs-string">&#x27;CHAS&#x27;</span>, <span class="hljs-string">&#x27;NOX&#x27;</span>, <span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;AGE&#x27;</span>, <span class="hljs-string">&#x27;DIS&#x27;</span>, <span class="hljs-string">&#x27;RAD&#x27;</span>, <span class="hljs-string">&#x27;TAX&#x27;</span>, <span class="hljs-string">&#x27;PTRATIO&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>]<br></code></pre></td></tr></table></figure><p>这里要说明一下，因为我是用的Jupyter，所以我可以这样直接打印出变量的具体内容，如果小伙伴们不是在Jupyter 里，而是在 Python文件中去编写代码，不要忘了使用<code>print</code>函数。</p><p>在拿到数据之后，我们先来定义一下问题。就是假设你现在要买一个房子，那么你就要根据他的这个房子的相关数据，来判断这个房子到底应该能卖到多少钱。所以我们的任务就是给定一组房屋的数据，然后要能够预测售价是多少。</p><p>定义完问题之后，我们来分析一下数据。</p><p>首先，要做数据，我们会先把它装载到一个表格里边。这里，我们使用Pandas。</p><p>Pandas 在 Python基础课里我有详细的讲过，它是做数据科学非常常用的一个东西。不要把它认为是熊猫啊，它是panel data set 的缩写，就是「面板数据集」，可以理解为一个Excel。可是它比 Excel 更方便编程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br>data = dataset[<span class="hljs-string">&#x27;data&#x27;</span>]<br>dataframe = pd.DataFrame(data)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataframe))<br>dataframe.head(<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><blockquote><p>为了节省篇幅，打印结果我就不贴出来了。</p></blockquote><p>有的小伙伴在处理这里<code>data</code>的时候，会发现头部没有特征名，会呈现1，2，3，4这样的数字。我们就需要将名称给它加上，之前我们说过，feature_name是特征名，于是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dataframe.columns = dataset[<span class="hljs-string">&#x27;feature_names]</span><br></code></pre></td></tr></table></figure><p>这个时候我们就能看出来每一个特征到底是什么。不过这组数据里因为只是特征数据，并没有相关的价格。价格原本是目标数据，也就是最初始数据里的<code>target</code>，所以我们这里给这组特征数据里加上一列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dataframe[<span class="hljs-string">&#x27;price&#x27;</span>] = dataset[<span class="hljs-string">&#x27;target&#x27;</span>]<br></code></pre></td></tr></table></figure><p>然后我们要想看看到底什么因素对房价的影响是最大的。「What's the mostsignificant（salient）feature of the house price」。</p><p>对于决定一个东西最重要的特征我们就叫做 significant，或者silence，显著特征。</p><p>在 pandas里边有一个很简单的东西，<code>correlation</code>。correlation就是两组变量的相关性。</p><p>关于特征相关性，我们在机器学习里面有详细的讲过，这里我们就粗略带过就行了，在使用<code>corr()</code>找到特征之间的相关性数据之后，可以使用seaborn 来将热图可视化出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br>sns.heatmap(dataframe.corr())<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232444.png"alt="Alt text" /></p><p>这里我们着重来看和价格相关的特征，除了它本身之外，正相关性最大的就是RM，负相关性最大的是 LSTAT。</p><p>我们来看一下这两个特征的说明：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(dataset[<span class="hljs-string">&#x27;DESCR&#x27;</span>])<br><br>---<br>...<br>RM       average number of rooms per dwelling<br>LSTAT    % lower status of the population<br>...<br></code></pre></td></tr></table></figure><p>RM是一套住宅的房间数量，一个是低收入人群的人口比例。也就是说，房间越多的房子越贵，小区内低收入人群的比例越低，小区内的房子越贵。那小区内低收入人群的比例居然比犯罪率的影响还要大一些，似乎有点让人难以接受，但是这个确实是事实。</p><p>基于以上分析，我们需要把房屋里边卧室的个数和房屋价格最成正相关。</p><p>把问题简单化：如何依据房屋里边卧室的数量来估计房子的面积？</p><p>在一九七几年的时候啊，当时有过这样一种想法，首先，我们将所有的 RM数据存下来，还有目标数据，也就是 price 也存下来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X_rm = dataframe[<span class="hljs-string">&#x27;RM&#x27;</span>].values<br>y = dataframe[<span class="hljs-string">&#x27;price&#x27;</span>]<br></code></pre></td></tr></table></figure><p>存下来之后我们把做一个字典映射：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">rm_to_price = &#123;r: y <span class="hljs-keyword">for</span> r, y <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(X_rm, y)&#125;<br><br>---<br>&#123;<span class="hljs-number">6.575</span>: <span class="hljs-number">24.0</span>,<br> <span class="hljs-number">6.421</span>: <span class="hljs-number">21.6</span>,<br> ...<br> <span class="hljs-number">6.976</span>: <span class="hljs-number">23.9</span>&#125;<br></code></pre></td></tr></table></figure><p>这样之后，问题也就相应的做了一个简化。假如有人在销售那里要求买房子，那销售就可以拿出一个字典，里面都是这样的对应关系，然后我们就可以去查一下就知道了。</p><p>这个时候假如有人告诉你有一个小区，他平均里边房屋平均是6.421。那一查就发现这个 6.421 的基本上卖 21 万。那如果小区里房屋数量是5.57 的时候我卖多少钱？卖 13 万。这都是一一对应的关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">rm_to_price[<span class="hljs-number">6.421</span>]<br><br>---<br><span class="hljs-number">21.6</span><br></code></pre></td></tr></table></figure><p>不过这个时候有一个人说我们那个小区里面平均是 7个房间，那是多少呢？我们发现，我们的字典里没有超过 7的数字，也就是没有这么一个对应关系。</p><p>那么找不到的时候怎么办呢？我们大部分时候解决问题都会找一个近似值，也就是最接近的数据来做参考。也可以根据以前的数据来做计算，其实也就是一句话的事：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">find_price_by_simila</span>(<span class="hljs-params">history_price, query_x, topn=<span class="hljs-number">3</span></span>):<br>    <span class="hljs-keyword">return</span> np.mean([p <span class="hljs-keyword">for</span> x, p <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(history_price.items(), key=<span class="hljs-keyword">lambda</span> x_y: (x_y[<span class="hljs-number">0</span>] - query_x) **<span class="hljs-number">2</span>)[:topn]])<br><br></code></pre></td></tr></table></figure><p>要根据以前的数据来做计算的话，我们定义了一个方法，传入了参数历史价格以及查询特征。然后我们返回的内容稍微有点复杂，首先给这个房屋进行排序，排序依据是按照x 和 query之间的距离来给他排序。排序的时候我们取最接近的这几个数字，这样就能够得到最接近的x 和 y。然后在 x 和 y 里面我们取它的 price，这就是最接近的 price。</p><p>然后我们来看看它给咱们算的如果房间数是 7 的情况是什么价格：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">find_price_by_simila(rm_to_price, <span class="hljs-number">7</span>)<br><br>---<br><span class="hljs-number">29.233333333333334</span><br></code></pre></td></tr></table></figure><p>关于排序那里看不懂的小伙伴，我们这里额外花点篇幅开个小灶。这样，假如说我们有下面一组数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">person_and_age = &#123;<br>    <span class="hljs-string">&#x27;A 张学友&#x27;</span>: <span class="hljs-number">62</span>,<br>    <span class="hljs-string">&#x27;C 周杰伦&#x27;</span>: <span class="hljs-number">44</span>,<br>    <span class="hljs-string">&#x27;B 毛不易&#x27;</span>: <span class="hljs-number">29</span><br>&#125;<br></code></pre></td></tr></table></figure><p>然后我们将这组数据改成列表并进行排序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">l = <span class="hljs-built_in">list</span>(person_and_age.items())<br><span class="hljs-built_in">sorted</span>(l)<br><br>---<br>[(<span class="hljs-string">&#x27;A 张学友&#x27;</span>, <span class="hljs-number">62</span>), (<span class="hljs-string">&#x27;B 毛不易&#x27;</span>, <span class="hljs-number">29</span>), (<span class="hljs-string">&#x27;C 周杰伦&#x27;</span>, <span class="hljs-number">44</span>)]<br></code></pre></td></tr></table></figure><p>我们可以看到它是按照数据的首字母进行排序的，可是这个时候我们不想以首字母来排序，而是想根据年龄大小进行排序该怎么办？这个时候我们就可以给排序方法的key 里面定规则，这个规则就是按照元素的第二个下标进行排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_first_items</span>(<span class="hljs-params">element</span>):<br>    <span class="hljs-keyword">return</span> element[<span class="hljs-number">1</span>]<br><br><span class="hljs-built_in">sorted</span>(l, key=get_first_items)<br><br>---<br>[(<span class="hljs-string">&#x27;B 毛不易&#x27;</span>, <span class="hljs-number">29</span>), (<span class="hljs-string">&#x27;C 周杰伦&#x27;</span>, <span class="hljs-number">44</span>), (<span class="hljs-string">&#x27;A 张学友&#x27;</span>, <span class="hljs-number">62</span>)]<br></code></pre></td></tr></table></figure><p>我们这里定义了一个函数<code>get_first_items</code>,其实做了一件很简单的事情，就是获得了<code>element</code>的第二个下标。</p><p>那么这里我们其实可以不用这样定义函数，而是直接用匿名函数。关于匿名函数，我在Python 基础课里也有详细的讲到，大家可以回头去翻看一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">sorted</span>(l, key=<span class="hljs-keyword">lambda</span> element: element[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>那其实，element是一个输入参数，是一个变量，所以我们完全可以就简写一下就行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">sorted</span>(l, key=<span class="hljs-keyword">lambda</span> e: e[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>然后我们再在后面多加一个切片操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">sorted</span>(l, key=<span class="hljs-keyword">lambda</span> e: e[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)[:<span class="hljs-number">2</span>]<br><br>---<br>[(<span class="hljs-string">&#x27;A 张学友&#x27;</span>, <span class="hljs-number">62</span>), (<span class="hljs-string">&#x27;C 周杰伦&#x27;</span>, <span class="hljs-number">44</span>)]<br></code></pre></td></tr></table></figure><p>不用在意那个<code>reverse=True</code>,只是打开了反向排序，因为个人情感上不想去掉<code>张学友</code>。</p><p>好，那这个时候呢我们在前面加一个<code>for</code>，就可以拿到名字和age，而我们只需要 age:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">[age <span class="hljs-keyword">for</span> name, age <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(l, key=<span class="hljs-keyword">lambda</span> e: e[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)[:<span class="hljs-number">2</span>]]<br><br>---<br>[<span class="hljs-number">62</span>, <span class="hljs-number">44</span>]<br></code></pre></td></tr></table></figure><p>这样我们就可以只取两个排序最靠前的年龄值，当然最后，就是<code>mean</code>，取平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">np.mean([age <span class="hljs-keyword">for</span> name, age <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(l, key=<span class="hljs-keyword">lambda</span> e: e[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)[:<span class="hljs-number">2</span>]])<br><br>---<br><span class="hljs-number">53.0</span><br></code></pre></td></tr></table></figure><p>那我们之前所写的函数内容就是这样一段话，拆解之后是不是就能明白了？</p><p>那么刚才讲到的这种方法，你会发现它是在找相似的东西，其实我们定义的这种方法，后来给它起个名字叫做：发现K 个最相近的邻居，<code>K-Neighbor-Nearest</code>,简称<code>KNN</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">knn</span>(<span class="hljs-params">history_price, query_x, topn=<span class="hljs-number">3</span></span>):<br>    <span class="hljs-keyword">return</span> np.mean([p <span class="hljs-keyword">for</span> x, p <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(history_price.items(), key=<span class="hljs-keyword">lambda</span> x_y: (x_y[<span class="hljs-number">0</span>] - query_x) **<span class="hljs-number">2</span>)[:topn]])<br></code></pre></td></tr></table></figure><p>这种算法之前机器学习的章节里咱们也详细讲过，这是一个非常经典的机器学习算法。关于KNN的有优点和缺点，我们之前也讲的很详细。那大家可以回过头取看我关于机器学习KNN 的部分来学习，这里就不再继续赘述 KNN的内容了，在这里，我们就了解之前我们所做的这么多内容，其实就是KNN，就可以了。</p><p>好，那这节课的内容就到这里，下一节课，咱们会继续写这一篇未完成的代码，来找到X_rm 和 y之间的函数关系。那么代码文件就依然还是<code>18.ipynb</code>。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231029232439.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi, 你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;我们终于又开启新的篇章了，从今天这节课开始，我们会花几节课来理解一下深度学习的相关知识，了解神经网络，多层神经网络相关知识。并且，我们会尝试着来打造一个自己的深度学习框架。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>17. 机器学习 - 随机森林</title>
    <link href="https://hivan.me/17.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <id>https://hivan.me/17.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</id>
    <published>2023-11-04T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:39.823Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224427.png"alt="茶桁的 AI 秘籍 核心基础 17" /></p><p>Hi，你好。我是茶桁。</p><p>我们之前那一节课讲了决策树，说了决策树的优点，也说了其缺点。</p><span id="more"></span><p>决策树实现起来比较简单，解释解释性也比较强。但是它唯一的问题就是不能拟合比较复杂的关系。</p><p>后来人们为了解决这个问题，让其能够拟合更加复杂的情况，提出来了一种模型，这种模型就叫做随机森林。</p><h2 id="随机森林">随机森林</h2><p>随机森林之所以叫随机森林，是因为它是由多棵树组成。它结合了决策树和随机性的概念，用于解决分类和回归问题，随机森林由多个决策树组成，每棵树都是随机构建的。</p><p>随机森林其核心组成部分是决策树，为了提高模型的性能和泛化能力，所以引入了两种主要形式的随机性。</p><p>第一种就是随机选择样本，对于每棵决策树的构建，随机森林从训练数据中随机抽取一部分样本（有放回地抽样），这称为自助采样（BootstrapSampling）。这就使得每棵树都在不同的样本子集上进行训练，增加了模型的多样性。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224333.png"alt="Alt text" /></p><p>第二种是随机选择特征，在每个节点上，随机森林只考虑特征的一个子集来进行分割决策，而不是考虑所有特征。这确保了每棵树的分裂过程是不同的，增加了多样性。</p><p>对于分类问题，随机森林中的每棵决策树都会对输入数据进行分类，那对于回归问题，就会变成是每棵决策树都会对输入数据进行预测了。最后的预测结果是通过对所有树的投票或平均值来获得的。这种集成方法可以减小过拟合奉先，提高模型的稳定性和泛化能力。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224334.png"alt="Alt text" /></p><ol type="1"><li>使用随机森林来预测。</li><li>在预测之前呢，我们使用 Out-Of-Bagging 样本来评估我们的模型。这个bagging 就是袋子，就是我们从袋子里随机取东西去衡量。</li><li>使用评估结果，我们可以选择合适的变量数。</li></ol><p>随机森林的原理其实很简单，是一个非常简单但是非常好用的一个方法。基本上，除了深度学习之外，也是企业用的最多的方法之一。咱们在这里就来演示一下随机森林的作用以及效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br>iris = load_iris()<br><br>x = iris.data<br>y = iris.target<br><span class="hljs-built_in">print</span>(x, y)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224335.png"alt="Alt text" /></p><p>这个是我们用 sklearn里面鸢尾花分类的数据做个简单例子，快速的展现一下它的效果。我们将数据拿到以后，x是鸢尾花的四个维度，四个维度对应了它的一个类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier<br><br>tree_clf = DecisionTreeClassifier()<br>tree_clf.fit(x, y)<br><br>tree_clf.feature_importances_<br><br>---<br>array([<span class="hljs-number">0.02666667</span>, <span class="hljs-number">0.</span>        , <span class="hljs-number">0.05072262</span>, <span class="hljs-number">0.92261071</span>])<br></code></pre></td></tr></table></figure><p>我们 fit 完之后就可以看到，这四个 feature 中，最终要的是第四个feature。然后是第三个，第二个根本就没用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><br>train_x, test_x,train_y, test_y = train_test_split(x, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)<br><br>tree_clf = DecisionTreeClassifier()<br>tree_clf.fit(train_x, train_y)<br><br><span class="hljs-built_in">print</span>(tree_clf.score(train_x, train_y))<br><span class="hljs-built_in">print</span>(tree_clf.score(test_x, test_y))<br><br>---<br><span class="hljs-number">1.0</span><br><span class="hljs-number">0.9777777777777777</span><br></code></pre></td></tr></table></figure><p>看结果我们其实可以看到，这个拟合度有点太高了。我们换个数据再来看，还是之前的课程中我们用到的Boston房价的数据，不过因为这个是一个回归问题，所以我们需要用回归预测的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeRegressor<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_openml<br>dataset = fetch_openml(name=<span class="hljs-string">&#x27;boston&#x27;</span>, version=<span class="hljs-number">1</span>, as_frame=<span class="hljs-literal">True</span>, return_X_y=<span class="hljs-literal">False</span>, parser=<span class="hljs-string">&#x27;pandas&#x27;</span>)<br><br>data = dataset[<span class="hljs-string">&#x27;data&#x27;</span>]<br>target = dataset[<span class="hljs-string">&#x27;target&#x27;</span>]<br><br>x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=<span class="hljs-number">0.2</span>)<br><br>tree_reg = DecisionTreeRegressor()<br>tree_reg.fit(x_train, y_train)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;whole dataset train acc: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(tree_reg.score(x_train, y_train)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;whole dataset test acc: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(tree_reg.score(x_test, y_test)))<br><br>---<br>whole dataset train acc: <span class="hljs-number">1.0</span><br>whole dataset test acc: <span class="hljs-number">0.6606392933985246</span><br></code></pre></td></tr></table></figure><p>现在我们来看，它的 train 上的 score 准确度是 1.0，在 test 上是0.81，这个是全数据量测试的情况。</p><p>然后我们来定义一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_forest</span>(<span class="hljs-params">x_train, y_train, x_test, y_test, drop_n=<span class="hljs-number">4</span></span>):<br>    <br>    features_random = np.random.choice(<span class="hljs-built_in">list</span>(x_train.columns), size=<span class="hljs-built_in">len</span>(x_train.columns)-drop_n)<br><br>    x_sample = x_train[features_random]<br>    y_sample = y_train<br><br>    reg = DecisionTreeRegressor()<br>    reg.fit(x_sample, y_sample)<br><br>    score_train = reg.score(x_sample, y_sample)<br>    score_test = reg.score(x_test[features_random], y_test)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;sub sample :: train score: &#123;&#125;, test score: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(score_train, score_test))<br><br>    y_predicated = reg.predict(x_test[features_random])<br><br>    <span class="hljs-keyword">return</span> y_predicated<br></code></pre></td></tr></table></figure><p>咱们随机的从<code>data</code>里面取一些数据，之后我们来看一下单个树的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">with_feature_names = pd.DataFrame(data)<br>with_feature_names.columns = dataset[<span class="hljs-string">&#x27;feature_names&#x27;</span>]<br><br>x_train, x_test, y_train, y_test = train_test_split(with_feature_names, target, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)<br><br>random_forest(x_train, y_train, x_test, y_test, <span class="hljs-number">4</span>)<br><br>---<br>sub sample :: train score: <span class="hljs-number">1.0</span>, test score: <span class="hljs-number">0.5171643497313849</span><br></code></pre></td></tr></table></figure><p>单个的结果显然是要比整个的数据量要差。那么咱们现在看一下最终的结果，把它变成一个森林：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">tree_num = <span class="hljs-number">4</span><br>predicates = []<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tree_num):<br>    predicated, score = random_forest(x_train, y_train, x_test, y_test)<br>    predicates.append((predicated))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the mean result is: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(np.mean(predicates), axis=<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the score of forest is: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(r2_score(y_test, np.mean(predicates, axis=<span class="hljs-number">0</span>))))<br><br>---<br>the mean result <span class="hljs-keyword">is</span>: <span class="hljs-number">21.614144736842107</span><br>the score of forest <span class="hljs-keyword">is</span>: <span class="hljs-number">0.7194989474162439</span><br></code></pre></td></tr></table></figure><p>从一开始到现在完整的打印结果为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">whole</span> dataset train acc: <span class="hljs-number">1</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">whole</span> dataset test acc: <span class="hljs-number">0</span>.<span class="hljs-number">6606392933985246</span><br><br><span class="hljs-attribute">ssub</span> sample :: train score: <span class="hljs-number">1</span>.<span class="hljs-number">0</span>, test score: <span class="hljs-number">0</span>.<span class="hljs-number">5885292814825753</span><br><span class="hljs-attribute">sub</span> sample :: train score: <span class="hljs-number">1</span>.<span class="hljs-number">0</span>, test score: <span class="hljs-number">0</span>.<span class="hljs-number">559086368163823</span><br><span class="hljs-attribute">sub</span> sample :: train score: <span class="hljs-number">1</span>.<span class="hljs-number">0</span>, test score: <span class="hljs-number">0</span>.<span class="hljs-number">6119989116140754</span><br><span class="hljs-attribute">sub</span> sample :: train score: <span class="hljs-number">1</span>.<span class="hljs-number">0</span>, test score: <span class="hljs-number">0</span>.<span class="hljs-number">21831688326567122</span><br><br><span class="hljs-attribute">the</span> mean result is: <span class="hljs-number">21</span>.<span class="hljs-number">614144736842107</span><br><span class="hljs-attribute">the</span> score of forest is: <span class="hljs-number">0</span>.<span class="hljs-number">7194989474162439</span><br></code></pre></td></tr></table></figure><p>这是个很典型的例子，使用全量的数据集，它的结果最终的在 test 集上是0.66，然后基本上每个的结都比它要差一些。但当我们用了森林的值做了平均之后，这个值就变得更好了。</p><p>当然其实这个值并不是每次都是如此，在我们进行计算的时候，因为数据什么的都是随机的，偶尔也会出现取均值之后变的更差的情况。不过大部分时候，都会更好一些。</p><p>我们现在再将结果稍微改一改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_forest</span>(<span class="hljs-params">x_train, y_train, x_test, y_test, drop_n=<span class="hljs-number">4</span></span>):<br>    ...<br><br>    <span class="hljs-keyword">return</span> y_predicated, score_test<br><br>tree_num = <span class="hljs-number">4</span><br>predicates = []<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tree_num):<br>    predicated, score = random_forest(x_train, y_train, x_test, y_test)<br>    predicates.append((predicated, score))<br><br>predicates_value = [v <span class="hljs-keyword">for</span> v, s <span class="hljs-keyword">in</span> predicates]<br>forest_scores = [s <span class="hljs-keyword">for</span> v, s <span class="hljs-keyword">in</span> predicates]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the score of forest is: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(r2_score(y_test, np.mean(predicates_value, axis=<span class="hljs-number">0</span>))))<br><br>weights = np.array(forest_scores) / np.<span class="hljs-built_in">sum</span>(forest_scores)<br>score_weights = np.zeros_like(np.mean(predicates_value, axis=<span class="hljs-number">0</span>))<br><br><span class="hljs-keyword">for</span> i, v <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(predicates_value):<br>    score_weights += v * weights[i]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;the score of weighted forest is: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(r2_score(y_test, score_weights)))<br><br>---<br>the score of forest <span class="hljs-keyword">is</span>: <span class="hljs-number">0.7049603534192553</span><br>the score of weighted forest <span class="hljs-keyword">is</span>: <span class="hljs-number">0.7204901503020483</span><br></code></pre></td></tr></table></figure><p>后面这段代码呢，其实就是人们发现用了随机森林之后，效果明显要好了，那一些人就想如果在知道每一次的<code>test_score</code>之后，能不能给<code>test_score</code>比较高的值加一个比较大的权重。</p><p>也就是说，当我知道<code>test_score</code>比较好，那在最后做决策的时候给它加的权重大一些。</p><p>最后我们打印了常规状态下森林的结果和加权之后的结果。加权之后的结果又变得好了一些。</p><h2 id="adaboost">Adaboost</h2><p>然后人们沿着这个思路，就做了一件事情，就是 Adaboost(AdaptiveBoosting)：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224336.png"alt="Alt text" /></p><p>Adaboost 就是在随机森林的权重思路上做了一个优化，它的示意图也是有多个weak classifier, 然后最后有一个 Weighted Voter,这是一个权重的投票，这个就和我们上面加权的那部分代码非常的类似。只不过它在这里做了个细化：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224337.png"alt="Alt text" /></p><p>我们来注意看最后一个公式：</p><p><span class="math display">\[H(x) = sign(\sum_{t=1}^T\alpha_th_t(x))\]</span></p><p>公式里的<spanclass="math inline">\(\alpha_t\)</span>就是它的权重，最终的 H(x)就是很多<span class="math inline">\(\alpha_t \cdoth_t(x)\)</span>加在一起的结果。这里的这个<spanclass="math inline">\(\alpha_t\)</span>就是每一次小的数的权重：<code>v * weights[i]</code>。这个权重就不是像咱们刚才代码里那样根据<code>score</code>的大小简单的做个加权。</p><p>我们看上图中间又一个<spanclass="math inline">\(\alpha_t\)</span>的公式：</p><p><span class="math display">\[\begin{align*}\alpha_t = \frac{1}{2}ln(\frac{1-\varepsilon_t}{\varepsilon_t})\end{align*}\]</span></p><p>然后我们再往上倒腾，<spanclass="math inline">\(\varepsilon_t\)</span>是当你预测出来这个值和实际值错的越多，越趋近于1。如果完全没有错，一个错都没有的情况下，那么<spanclass="math inline">\(\varepsilon_t=0\)</span>。</p><p><span class="math display">\[\begin{align*}\varepsilon_t = Pr_{i\sim D_t}[h_t(x_i)\ne y_i]\end{align*}\]</span></p><p>如果<span class="math inline">\(\varepsilon_t =1\)</span>的话，那就是<spanclass="math inline">\(\frac{1-\varepsilon_t}{\varepsilon_t}\)</span>就是：<code>1-1/1=0</code>。ln0等于什么呢？它等于负的无穷大，那么<spanclass="math inline">\(\alpha_t\)</span>等于就没有。如果<spanclass="math inline">\(\varepsilon_t = 0\)</span>，<spanclass="math inline">\(ln(\frac{1-\varepsilon_t}{\varepsilon_t})\)</span>就是无穷大。</p><p>也就是说，随着<spanclass="math inline">\(\varepsilon_t\)</span>越大，那<spanclass="math inline">\(\alpha_t\)</span>会越大，随着<spanclass="math inline">\(\varepsilon_t\)</span>越小，<spanclass="math inline">\(\alpha_t\)</span>也会越小。而且在这个地方是呈指数变化的，就是误差会对<spanclass="math inline">\(\alpha_t\)</span>的变化影响的很大。</p><p>除了用指数的东西来做，它还有一个很重要的特性，这个特性才在我们整个Adaboost 里非常重要：</p><p><span class="math display">\[D_{t+1}(i) = \frac{D_t(i)exp(-\alpha_ty_ih_t(x_i))}{Z_t}\]</span></p><p>我们先来看<spanclass="math inline">\(y_ih_t(x_i)\)</span>这部分，假设 ht(xi)=1，yi预测对了等于 1，yi 预测错了等于 -1。那如果预测错了，这整个部分都等于-1，如果预测对了，这里就是 1。</p><p>前面有一个负号： <spanclass="math inline">\(-\alpha_ty_ih_t(x_i)\)</span>，那肯定是要变号的。也就是说，如果预测错了，那么这一串东西应该是正的，如果预测对了这一串东西应该是负的。</p><p>前面是什么，是<span class="math inline">\(D_{t+1}(i)\)</span>,这里其实就是第 i 个训练元素在<spanclass="math inline">\(D_{t+1}\)</span>被取到的概率。那么我们最前面有表示<spanclass="math inline">\(D_1(i) =\frac{1}{m}\)</span>，也就是说，所有元素被取到的概率都是一样的，是平均的。那第二次的概率就是：<spanclass="math inline">\(D_1(i)\cdot exp(...)\)</span>, exp 就是 e的多少次方。</p><p>那我们现在知道，如果预测对了，这里是 -1，预测错了这里是 1,都要再乘以<spanclass="math inline">\(\alpha_t\)</span>。那么如果预测对了，这里是<spanclass="math inline">\(-\alpha_t\)</span>，那 exp 这里就是小于 1的。那如果预测错了呢，exp 就是大于 1 的。</p><p>如果 exp 大于 1，那么<spanclass="math inline">\(D_{t+1}(i)\)</span>概率就会被<spanclass="math inline">\(D_t(i)\)</span>的概率要更大，反之就会更小。</p><p>这个就是我们 Ada 的含义，Ada 就是Adaptive，就是动态调整的意思。也就是通过这种方法实现的。</p><p>如果此时此刻<spanclass="math inline">\(x_i\)</span>算对了，那下一次就更不容易被取到，如果算错了，那下一次训练就会更有可能被取到。</p><p>觉得绕的小伙伴去理解这样一个例子：如果你是个学生去做卷子，那么你作对的题还会反复去做吗？肯定是不会的题才会反复刷，刷到自己会为止。</p><h2 id="gradient-boosting">Gradient Boosting</h2><p>除了 Adaboost 之外，后来人们又提出来了一个新的方法：GradientBoosting。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224338.png"alt="Alt text" /></p><p>Gradient Boosting 和 Adaboost 的核心原理很像：</p><p><span class="math display">\[loss(p, q) = -\sum_{i\in output classes}p(x)logq(x)\]</span></p><p>Gradient Boosting主要用于解决回归和分类问题。它基于决策树（通常是浅层决策树）构建模型，通过迭代改进预测的准确性。</p><p>其最核心的就是梯度提升，是一种集成学习方法。将多个弱预测模型，也就是决策树组合在一起，以提高整体性能。每个决策树在不同的数据子集上训练，然后进行组合以生成最终的预测。其核心原理就是通过迭代优化损失函数来构建模型。</p><p>在每一步中，模型的更新方向就是损失函数的负梯度。假设我们有一个损失函数L(y, f(x))，其中 y 是真实标签，f(x)是当前模型的预测，梯度提升的目标是找到一个新的模型 h(x), 使得损失函数L(y, f(x) + h(x)) 最小化。</p><p>梯度提升使用负梯度方向的决策树 h(x)来拟合当前模型的残差，因此可以通过以下方式迭代更新模型：</p><p><span class="math display">\[f(x) = f(x) + learning\_rate \cdot h(x) \\\]</span></p><p>也就是说，它其实要变成这样一个式子：</p><p><span class="math display">\[Boosted Ensemble = First Tree + \eta \cdot Second Tree \\loss(Boosted Ensemble) &lt; loss(First Tree)\]</span></p><p>也就是说，我们第二波的 h_t(x)，也就是 h_2(x)，前面乘以一个<spanclass="math inline">\(\eta\)</span>，这个<spanclass="math inline">\(\eta\)</span>是等于 Learning Rate 的，然后 h_2(x)加上 h_1(x) 最后得到的结果，要比 h_1(x) 的 loss 值更小。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224339.png"alt="Alt text" /></p><p>那么我们现在要做的就是改变<spanclass="math inline">\(\eta\)</span>的权重，这个东西的权重就是和之前我们在随机森林里调整权重不同。</p><p>一开始，梯度提升初始化一个简单的模型，通常是一个常数，用来拟合目标变量的平均值。</p><p>对于每一个训练样本，计算模型的梯度。这表示模型对于每个样本的预测误差。</p><p>使用新的决策树来拟合梯度的负梯度，也就是模型的残差。这意味着构建一个决策树，其目标是减小之前模型的误差。</p><p>将新构建的决策树与之前的模型相加，以形成一个新的模型。这个过程重复进行多次，每次都会减小误差。</p><p>重复 2 到 4步，直到满足某个停止条件，理入达到最大迭代次数或误差足够小。</p><p>最终模型是所有决策树的组合，可以用来进行预测。</p><p>那我们之前谈到的<span class="math inline">\(\eta\)</span>，也就是learning_rate，其实就是学习率，用于控制每次更新的幅度。</p><p>数学上，梯度提升通过迭代不断减小损失函数来逼近最优模型，这是一种梯度下降的优化方法，因此它的核心原理与梯度下降算法是密切相关的。</p><p>Grading Boost 和 AdaBoost 的整个区别不大，它们都是属于 EnsembleLearning，中文翻译是合唱团。</p><p>这个 Ensemble Learning我们可以取很多个分类、回归，然后我们把它做好之后给它求一个平均值。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224340.png"alt="Alt text" /></p><p>比如这样，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> VotingClassifier<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br><br>log_clf = LogisticRegression()<br>rnd_clf = RandomForestClassifier()<br>svm_clf = SVC()<br><br>voting_clf = VotingClassifier(<br>    estimators = [(<span class="hljs-string">&#x27;lr&#x27;</span>, log_clf),(<span class="hljs-string">&#x27;rf&#x27;</span>, rnd_clf),(<span class="hljs-string">&#x27;svc&#x27;</span>, svm_clf)], voting=<span class="hljs-string">&#x27;hard&#x27;</span>)<br><br>voting_clf.fit(x_train, y_train)<br>...<br><br></code></pre></td></tr></table></figure><p>在 sklearn 的 ensemble 中本身就有一个 VotingClassifier，也有RandomForestClassifier，我们可以直接用几个分类器可以实现。</p><p>AdaBoost 和 Gradient Boost 也是属于一个典型的 ensemble Learning。</p><p>那还有两个比较重要的东西，一个叫做 Xgboost，一个叫做LightBGM，这两个是 Grading Boost的升级版。它们被广泛的使用于机器挖掘，推荐系统等等。</p><p>当然这两块内容就不放在「核心基础」里讲了，将会在后面讲到 BI专业课的时候专门的去讲，这两个是很重要的点。</p><p>那本节课讲完之后呢，咱们核心基础的部分，关于机器学习就跨过一个小阶段了。下一节课开始，我们要讲「深度学习」了。属于向前要跨一大步。</p><p>好，那咱们经典机器学习模型到今天就讲完了。各位看文章的小伙伴，自己去把这个课程再好好巩固一下，咱们下节课开始，就进入深度学习了。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231028224427.png&quot;
alt=&quot;茶桁的 AI 秘籍 核心基础 17&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;我们之前那一节课讲了决策树，说了决策树的优点，也说了其缺点。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>16. 机器学习 - 决策树</title>
    <link href="https://hivan.me/16.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://hivan.me/16.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2023-11-01T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:43.122Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231027192720.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><p>在上一节课讲 SVM之后，再给大家将一个新的分类模型「决策树」。我们直接开始正题。</p><span id="more"></span><h2 id="决策树">决策树</h2><p>我们从一个例子开始，来看下面这张图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231027192721.png"alt="Alt text" /></p><p>假设我们的 x1 ~ x4 是特征，y是最终的决定，打比方说是买东西和不买东西，0 为不买，1 为买东西，假设现在y 是<code>[0,0,1,0,1]</code>。</p><p>那么，我们应该以哪个特征为准去判断到底 y 是 0 还是 1 呢？</p><p>如果关注 x3，那么 x3 为 A 的时候，即有 0 也有1，我们先放一边找找看有没有更合适的。</p><p>如果是 x4 的话，肉眼可见的，区分度是最准确的对吧？B 的都是都是 0，C的时候都是 1，那么 x4 也就是区分度最大的。</p><p>我们现在换成人的思维过程来说，肯定是期望先找到那个最能区分它的，就是最能识别的特征。这个最能识别的特征在数学里面有一个专门的定义：Salientfeature, 就是显著特征。</p><p>如果我们更改一下 x4 的值，变成<code>[B,C,C,B,B]</code>，那 x4也就不那么显著了。这个时候最能区分的就是 x2了，只在<code>x2[1]</code>的位置上判断错了一个。</p><p>这个时候，我们就需要客观的衡量一下，什么叫做最能区分，或者说是最能分割，最显著的区别。这就需要一个专门的数值来计算。</p><p>那我们就来看看，到底怎么样来做区分。我们现在根据一个值，将我们的数据分成了两堆，那咱们的期望就是这两堆数据尽可能是一样的。比如极端情况下，一堆全是0，一堆全是1，当然，中间混进去一个不一样的也行，但是尽可能的「纯」：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231027192722.png"alt="Alt text" /></p><p>好，那我们该怎么定义这个「纯」呢？</p><p>我们大家应该都知道物理里有一个定义：「熵」，那熵在物理里是衡量物体的混乱程度的。</p><p>比如说一个组织、一个单位、公司或者个人，其内部的熵都是在呈现越来越混乱，而且熵的混乱具有不可逆性。这个呢，就是熵增定律，也叫「热力学第二定律」。是德国人克劳修斯提出的理论，最初用于揭示事物总是向无序的方向的发展、以及“孤立系统下热量从高温物体流向低温不可逆”的热力学定律（要不说看我的文章涨学问呢是吧）。</p><h2 id="信息熵">信息熵</h2><p>好，说回咱们的机器学习。后来有一个叫「香农」的人要衡量一堆新息的混乱程度，就起了一个名字「信息熵」，也就成了衡量信息的复杂程度。</p><p>那么信息熵怎么求呢？</p><p><span class="math display">\[\begin{align*}Entropy = \sum_{i=1}^n-p(c_i)log_2(p(c_i))\end{align*}\]</span></p><p>这个值越大，就说明了这个新息越混乱，相反的，越小就说明越有秩序。来，我们看一下代码演示，定义一个熵的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> icecream <span class="hljs-keyword">import</span> ic<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pr</span>(<span class="hljs-params">e, elements</span>):<br>    counter = Counter(elements)<br>    <span class="hljs-keyword">return</span> counter[e] / <span class="hljs-built_in">len</span>(elements)<br><br><span class="hljs-comment"># 信息熵</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">entropy</span>(<span class="hljs-params">elements</span>):<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(pr(e, element) * np.log(pr(e, elements)) <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> elements)<br></code></pre></td></tr></table></figure><p>然后我们具体的来看几组数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">ic(entropy([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]))<br>ic(entropy([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]))<br>ic(entropy([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">8</span>]))<br>ic(entropy([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>]))<br>ic(entropy([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>]))<br>ic(entropy([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;d&#x27;</span>]))<br><br>---<br>ic| entropy([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]): <span class="hljs-number">1.05829973151282</span><br>ic| entropy([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]): -<span class="hljs-number">0.0</span><br>ic| entropy([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">8</span>]): <span class="hljs-number">1.7917594692280547</span><br>ic| entropy([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>]): <span class="hljs-number">1.7917594692280547</span><br>ic| entropy([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>]): <span class="hljs-number">1.7576608876629927</span><br>ic| entropy([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;d&#x27;</span>]): <span class="hljs-number">2.1130832934475294</span><br></code></pre></td></tr></table></figure><p>我们可以看到，最「纯」的是第二行数据，然后是第一行，第三行和第四行是一样的。5，6行就更混乱一些。</p><p>那接下来的知识点是只关于 Python的，我们看上面的代码是不是有点小问题？这个代码里有很多的冗余。一般情况下，会将counter改成全局变量，但是一般如果想要代码质量好一些，尽量不要轻易定义全局变量。我们来简单的修改一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">entropy</span>(<span class="hljs-params">elements</span>):<br>    <span class="hljs-comment"># 信息熵</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">pr</span>(<span class="hljs-params">es</span>):<br>        counter = Counter(es)<br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">_wrap</span>(<span class="hljs-params">e</span>):<br>            <span class="hljs-keyword">return</span> counter[e] / <span class="hljs-built_in">len</span>(elements)<br>        <br>        <span class="hljs-keyword">return</span> _wrap<br>    <br>    p = pr(elements)<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(p(e) * np.log(p(e)) <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> elements)<br></code></pre></td></tr></table></figure><p>这样写之后，我们再用刚才的数据来进行调用会看到结果完全一样。不过如果这样写之后，如果我们数据量很大的情况下，会发现会快很多。</p><h2 id="gini-系数">Gini 系数</h2><p>除了上面这个信息熵之外，还有一个叫 Gini 系数，和信息熵很类似：</p><p><span class="math display">\[\begin{align*}Gini = 1 - \sum_{i=1}^np^2(C_i) \\\end{align*}\]</span></p><p>假如说 probability 都是 1，也就是最纯的情况，那么 1 减去 1 就等于0。如果它特别长，特别混乱，都很分散，那 probability 就会越接近于 0，那么1 减去 0，那结果也就是越接近于 1。</p><p>那么代码实现一下就是这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">geni</span>(<span class="hljs-params">elements</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>-np.<span class="hljs-built_in">sum</span>(pr(e) ** <span class="hljs-number">2</span> <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">set</span>(elements))<br></code></pre></td></tr></table></figure><p>好吧，这个时候我发现一个问题，之前我们将 probability函数定义到熵函数内部了，为了让 Gini函数能够调用，我们还得拿出来。在我们之前修改的初衷不变的情况下，我们来这样进行修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pr</span>(<span class="hljs-params">es</span>):<br>    counter = Counter(es)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_wrap</span>(<span class="hljs-params">e</span>):<br>        <span class="hljs-keyword">return</span> counter[e] / <span class="hljs-built_in">len</span>(es)<br>    <span class="hljs-keyword">return</span> _wrap<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">entropy</span>(<span class="hljs-params">elements</span>):<br>    <span class="hljs-comment"># 信息熵</span><br>    p = pr(elements)<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(p(e) * np.log(p(e)) <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> elements)<br></code></pre></td></tr></table></figure><p>哎，这样就对了。优雅...</p><p>然后我们来修改 Gini 函数，让其调用 pr 函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">gini</span>(<span class="hljs-params">elements</span>):<br>    p = pr(elements)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> - np.<span class="hljs-built_in">sum</span>(p(e) ** <span class="hljs-number">2</span> <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">set</span>(elements))<br></code></pre></td></tr></table></figure><p>然后我们写一个衡量的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pure_func = gini<br></code></pre></td></tr></table></figure><p>然后我们将之前的调用都修改一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">ic(pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]))<br>ic(pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]))<br>ic(pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">8</span>]))<br>ic(pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>]))<br>ic(pure_func([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>]))<br>ic(pure_func([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;d&#x27;</span>]))<br><br>---<br>ic| pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]): <span class="hljs-number">0.2777777777777777</span><br>ic| pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]): <span class="hljs-number">0.0</span><br>ic| pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">8</span>]): <span class="hljs-number">0.8333333333333333</span><br>ic| pure_func([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>]): <span class="hljs-number">0.8333333333333333</span><br>ic| pure_func([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>]): <span class="hljs-number">0.44897959183673464</span><br>ic| pure_func([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-string">&#x27;d&#x27;</span>]): <span class="hljs-number">0.6122448979591837</span><br></code></pre></td></tr></table></figure><p>我们可以看到，Gini 系数是把整个纯度压缩到了 0~1 之间，越接近于 1就是越混乱，越接近 0 呢就是越有秩序。</p><p>其实除了数组之外，字符串也是一样可以衡量的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">ic(pure_func(<span class="hljs-string">&quot;probability&quot;</span>))<br>ic(pure_func(<span class="hljs-string">&quot;apple&quot;</span>))<br>ic(pure_func(<span class="hljs-string">&quot;boom&quot;</span>))<br><br>---<br>ic| pure_func(<span class="hljs-string">&quot;probability&quot;</span>): <span class="hljs-number">0.8760330578512396</span><br>ic| pure_func(<span class="hljs-string">&quot;apple&quot;</span>): <span class="hljs-number">0.72</span><br>ic| pure_func(<span class="hljs-string">&quot;boom&quot;</span>): <span class="hljs-number">0.625</span><br></code></pre></td></tr></table></figure><p>在能够定义纯度之后，现在如果我们有很多数据，就比方说是我们最之前定义数据再增多一些：<code>[x1, x2, x3, ..., xn]</code>，那么我们决策树会做什么呢？</p><p>根据 x1 对 y 做了分类，根据 x2 对 y 做了分类，做了分类之后，通过 x1把 y 分成了两堆，一堆我们称呼其为<code>m_left</code>,另外一堆我们称呼其为<code>m_right</code>，然后我们来定义一个 loss函数：</p><p><span class="math display">\[\begin{align*}loss = \frac{m_{left}}{n} \cdot G_{left} + \frac{m_{right}}{m} \cdotG_{right}\end{align*}\]</span></p><p>现在要让这个 loss 函数的值最小。在整个式子中，G代表的是纯度函数，这个纯度函数可以是 Entropy，也可以是 Gini。</p><p>loss函数的值最小的时候，就可以实现左右两边分的很均匀。我们把这个算法就叫做CART。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231027192723.png"alt="Alt text" /></p><p>CART算法其实就是<code>classification and regression tree Algorithm</code>,也称为「分类和回归树算法」。</p><p>上面我们讲的所有内容可以实现分类问题，那么回归问题怎么解决呢？CART里可是包含了 Regression 的。</p><p>好，还是我们最之前给的数据，我们现在的 y不是<code>[0,0,1,0,1]</code>了，我们将其更改为<code>[1.3,1.4,0.5,0.8,1.9]</code>。我们人类大脑中的直觉会怎么分类？会将[1.3,1.4,1.9]分为一类，而[0.5,0.8]分为一类对吧？（我说的是大部分人，少部分「天才」忽略）。</p><p>那么为了完成这样的一个分类，我们将之前公式里的纯度函数替换成MSE，那么函数就会变成如下这个样子：</p><p><span class="math display">\[J(k, t_k) = \frac{m_{left}}{m} \cdot MSE_{left} +\frac{m_{right}}{m}MSE_{right}\]</span></p><p>MSE 是什么呢？其实就是均方误差。</p><p><span class="math display">\[\begin{align*}MSE_{node} &amp; = \sum_{i \in node}(\hat y_{node} - y^{(i)})^2 \\\hat y_{node} &amp; = \frac{1}{m_{node}}\sum_{i\in node}y^{(i)}\end{align*}\]</span></p><p>我们要取的，就是最小的那一个 MSE。</p><p>决策树最大的优点就是在决策上我们需要更大的解释性的时候很直观，决策树可以将其分析过程以树的形式展现出来。一般在商业、金融上进行决策的时候我们都需要很高的解释性。就比如下面这个例子：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231027192724.png"alt="Alt text" /></p><p>第二呢，它可以来提取重要特征。决策术可能某一类问题上效果假设最多只能做到85% 的准确度，我们期望换一种模型，希望用到的维度少一点。</p><p>比方说要做逻辑回归，我们期望的<code>w.x+b</code>乘以一个Sigmoid，原来的 x 是 10 维的，我们期望把它降到 5维。那这个时候决策树的构建过程就从最显著的特征开始逐渐构建，我们就可以把它前五个特征给他保留下来，前五个特征就是最salience 的feature。我们假如把它用到逻辑回归上，直接用这五个维度就可以了。</p><p>接着我们来个问题：本来是 10 维的我们期望把它变成 5维，为什么我们希望降维呢？</p><p>还记得我们「拟合」这一节么？我们说过，过拟合最主要的原因是数据量过少或者模型过于复杂，那为什么数据量过少呢？不知道是否还记得我讲过的维度灾难。</p><p>多个维度就需要多个数量级的数据。在仅有这么多数据的情况下，维度越多需要更多数据来拟合，但是大部分时候我们并没有那么多数据。</p><p>这个问题其实是一个很经典的问题，为什么我们做各种机器学习的时候期望降维。如果能把这个仔仔细细的想清楚，其实机器学习原理基本上已经能够掌握清楚了。</p><p>接下来我们再说说它的缺点，其实也很明显，它的分类规则太过于简单。所以它最大的缺点就是因为简单，所以好解释，但正是因为简单，所以解决不了复杂问题。</p><p>和 SVM一样，也可以给决策树加核函数，曾经有一段时间这也是很重要的一个研究领域。</p><p>好，在结束之前我们预告一下下一节课内容，我们还是讲决策树，讲讲决策树中的Adaboost 等，以及决策树的升级版：随机森林。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231027192720.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;在上一节课讲 SVM
之后，再给大家将一个新的分类模型「决策树」。我们直接开始正题。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>15. 机器学习 - 支持向量机</title>
    <link href="https://hivan.me/15.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>https://hivan.me/15.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</id>
    <published>2023-10-29T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:47.462Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026233247.png"alt="茶桁的 AI 秘籍 核心能力基础 15" /></p><p>Hi, 你好。我是茶桁。</p><span id="more"></span><h2 id="逻辑回归预测心脏病">逻辑回归预测心脏病</h2><p>在本节课开始呢，我给大家一份逻辑回归的练习，利用下面这个数据集做了一次逻辑回归预测心脏病的练习。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232831.png"alt="Alt text" /></p><p>本次练习的代码在「茶桁的 AI 秘籍」在 Github上的代码库内，数据集的获取在文末。这样做是因为我的数据集都是和百度盘同步的，很多数据集过大了，所以也就不传Github 了。而且，我直接获取盘内同步数据也更方便。</p><p><strong>还有一个原因，有些数据集可能以后会收费获取。</strong></p><p>好，让我们进入今天的正课。</p><p>因为未来几节课的内容比较多。「核心基础」的这部分内容已经超出我原本的预计，咱们「核心基础」的部分刚刚过半，可是已经写到15 节了，本来这部分内容我是想在 21节左右结束的，所以，我们还是要压缩一下内容了。</p><p>这节课咱们还是继续讲解经典的机器学习。</p><h2 id="支持向量机">支持向量机</h2><p>接下来，要讲解一个非常有趣的方法：支持向量机。</p><p>支持向量机的原理其实可以很复杂，但它是一个很经典的思想方法。咱们就把它的核心思想讲明白就行了。其实我们平时在工作中用的也比较少。但是面试中有一些老一代的面试官会比较喜欢问这个问题。</p><p>支持向量机的核心思想，假如我们有两堆数据，希望找一根线去把它做分类，那么咱们找哪一根线呢？</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232832.png"alt="Alt text" /></p><p>上图中，我们假设黑色的那根线定义为l，把离这根线最近的点，也就是直线距离最小的点，找到两个这样的点定义为P1、P2。</p><p>现在我们是希望离这个 l 最近的点，假如说是d1,d2，那么我们希望这两个距离加起来最大：max|d1+d2|。</p><p>现在再定义蓝色的线为直线 b，那直线 b 做分类就比直线 l要好。为什么直线 b 就比是直线 l 好呢？因为直线 b 离 d1,d2普遍都比较远。</p><p>现在这里的演示是一个二维平面中用一根线来分割，如果是在多维空间中，SVM的目标就是找到一个最佳的超平面来最大化间隔，同时确保正确分类样本。</p><p>假设我们有一组训练样本，每个样本用特征向量 x 表示，并且标记为正类别+1 或负类别 -1。</p><p>我们可以表示为以下凸优化问题：</p><p><span class="math display">\[\begin{align*}min_{w, b}\frac{1}{2}||w||^2\end{align*}\]</span></p><p>其中对所有样本</p><p><span class="math display">\[y_i(w \cdot x_i+b) \ge 1\]</span></p><p>w 是超平面的法向量，b 是截距项，yi 是样本 xi 的标签，也就是 +1 或者-1。</p><p>为了解决这个优化问题，我们引入拉格朗日乘子<spanclass="math inline">\(a_i\)</span>来得到拉格朗日函数：</p><p><span class="math display">\[L(w,b,a) = \frac{1}{2}||w||^2 - \sum_{i=1}^Na_i[y_i(w\cdot x_i +b) - 1]\]</span></p><p>然后我们要最小化拉格朗日函数，首先对 w 和 b 求偏导数，令它们等于0，然后代入拉格朗日乘子条件：</p><p><span class="math display">\[a_i[y_i(w\cdot x_i + b)-1] = 0\]</span></p><p>然后我们就可以得到如下这个式子</p><p><span class="math display">\[w = \sum_{i=1}^Na_iy_ix_i \\sum_{i=1}^N a_iy_i = 0\]</span></p><p>使用某种优化算法（例如，SMO 算法），求解拉格朗日乘子<spanclass="math inline">\(a_i\)</span>。我们就可以使用求解得到的<spanclass="math inline">\(a_i\)</span>计算超平面参数 w 和 b。</p><p>对于新样本 x，使用超平面<span class="math inline">\(w\cdot x +b\)</span>的符号来预测其类别。</p><p>那我们讲了这么半天，都是一个支持向量机的数学演示过程，下面我们来看看具体的代码实现。</p><p>我们先来生成两组数据，这两组数据咱们让他距离更大：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>label_a = np.random.normal(<span class="hljs-number">6</span>, <span class="hljs-number">2</span>, size=(<span class="hljs-number">50</span>, <span class="hljs-number">2</span>))<br>label_b = np.random.normal(-<span class="hljs-number">6</span>, <span class="hljs-number">2</span>, size=(<span class="hljs-number">50</span>, <span class="hljs-number">2</span>))<br></code></pre></td></tr></table></figure><p>我们现在来观察以下生成的这些点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.scatter(*<span class="hljs-built_in">zip</span>(*label_a))<br>plt.scatter(*<span class="hljs-built_in">zip</span>(*label_b))<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232833.png"alt="Alt text" /></p><p>然后我们继续：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">label_a_x = label_a[:, <span class="hljs-number">0</span>]<br>label_b_x = label_b[:, <span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p>我们就将这两组数据的第一列分别取出来了。</p><p>接着我们随机的定义一些 w 和 b</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    w, b = (np.random.random(size=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)) * <span class="hljs-number">10</span> - <span class="hljs-number">5</span>)[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p>然后我们按照之前讲的数学演示来定义一个函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> w*x+b<br></code></pre></td></tr></table></figure><p>然后我们之前从数学演示里已经知道，<spanclass="math inline">\(y_i(w\cdot x+b) \ge 1\)</span>,而我们也知道这个说的是距离，也就是说，同样的$y_i(wx+b) $。</p><p>也就是说，我们要让函数 f 小于等于 -1，并且大于等于1。当然，为了保证其被分到两边，我们将函数的最大值定义为小于等于-1，将函数的最小值定义为大于等于 1。这样就保证 (-1,1)之间是不存在任何函数值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.<span class="hljs-built_in">max</span>(f(label_a_x, w, b)) &lt;= -<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> np.<span class="hljs-built_in">min</span>(f(label_b_x, w, b)) &gt;= <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>只有同时满足这两个条件的值，我们才会留下来进行保存。我们可以定义一个变量将其保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">w_and_b = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    w, b = (np.random.random(size=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)) * <span class="hljs-number">10</span> - <span class="hljs-number">5</span>)[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">if</span> np.<span class="hljs-built_in">min</span>(f(label_a_x, w, b)) &gt;= -<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> np.<span class="hljs-built_in">min</span>(f(label_b_x, w, b)) &gt;= <span class="hljs-number">1</span>:<br>        w_and_b.append((w, b))<br></code></pre></td></tr></table></figure><p>在得到这些 w,b 之后，我们将这些 w,b 连起来进行画图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> w, b <span class="hljs-keyword">in</span> w_and_b:<br>    x = np.concatenate((label_a_x, label_b_x))<br>    plt.plot(x, f(x, w, b))<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232834.png"alt="Alt text" /></p><p>这样，我们就拟合出来了很多的曲线。这些个曲线到底哪一个是最好的那一个呢？</p><p>现在根据刚刚得到的那个结论，现在所有的<spanclass="math inline">\(y_i(w\cdot x_i + b)\)</span>,那么现在其实就是<span class="math inline">\(margin =\frac{2}{||w||}\)</span>。</p><p>那我们现在就找这个 w 最小的这个值就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">w, b = <span class="hljs-built_in">min</span>(w_and_b, key = <span class="hljs-keyword">lambda</span> w_b: w_b[<span class="hljs-number">0</span>])<br>all_x = np.concatenate((label_a_x, label_b_x))<br>plt.plot(all_x, f(all_x, w, b), <span class="hljs-string">&#x27;r-o&#x27;</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232835.png"alt="Alt text" /></p><p>现在我们就可以看到那个最优的直线了，就是众多红色的点连接起来的那根线。</p><p>当然，最后代码执行顺序和讲解顺序有一些不一样，为了避免数据每次重新生成造成的差别，所以最开始是生成数据，之后是定义函数、过滤参数以及生成图像。</p><p>这个就是支持向量机的原理，我们找到离它所有的点的一个距离，让它这个边距最大，最后得到一个简化结果。</p><h2 id="核函数">核函数</h2><p>然后我们再来看另外一个点：「核函数」：</p><p>核函数是支持向量机里面非常重要的一个东西。</p><p>如果支持向量机只要数据是线性可分的，那么我们一定能够找到它的分割线。但是在实际的现实生活中有很多点并不是线性可分的。</p><p>举个例子，我们来画一张图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232836.jpeg"alt="Alt text" /></p><p>就比如图中的这种数据，是无论如何用一条直线无法分割的，不管怎么画，都无法把蓝色和红色的点分割开。</p><p>就像我们下面这张图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232837.png"alt="Alt text" /></p><p>但是，我们我们可以做这样一件事情，假设我们在一个坐标轴上拥有 8个点，A、B、C、D 为一组，a,b,c,d 为一组。如下图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232838.jpeg"alt="Alt text" /></p><p>分别为 A(-1,1), B(1,1), C(1, -1), D(-1,-1)；a(-0.5, 0.5), b(0.5,0.5), c(0.5, -0.5), d(-0.5, -0.5)。</p><p>现在我们 ABCD 和 abcd 是无法用一根直线来分割的，然后我们令：</p><p><span class="math display">\[\begin{align*}f(x) =&gt; \begin{Bmatrix} x^2 \\ y^2 \end{Bmatrix}\end{align*}\]</span></p><p>那在这种情况下，八个点分别就变成了 A(1, 1),B(1, 1),C(1, 1),D(1,1)，a(0.25, 0.25),b(0.25, 0.25),c(0.25, 0.25),d(0.25, 0.25)。</p><p>那这样的情况下，我们就完全可以用一根直线去分割了：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232839.jpeg"alt="Alt text" /></p><p>那现在找到这根线是 w2 =wx+b，那我们遇到新数据应用到这个函数里边，再应用到这个线里面做分割就可以了。我们把原本线性不可分的东西，变成线性可分的。那么这个就是核函数神奇的地方。</p><p>支持向量机通过某非线性变换 φ(x)，将输入空间映射到高维特征空间。特征空间的维数可能非常高。如果支持向量机的求解只用到内积运算，而在低维输入空间又存在某个函数K(x, x′) ，它恰好等于在高维空间中这个内积，即 K(x, x′) =φ(x)⋅φ(x') ;。那么支持向量机就不用计算复杂的非线性变换，而由这个函数 K(x, x′)直接得到非线性变换的内积，使大大简化了计算。我们就将这种函数函数 K(x,x′) 称为核函数。</p><p><span class="math display">\[\varphi (x) = \begin{bmatrix} x \\ x^2 \\ x^3 \end{bmatrix}\]</span></p><p>那其实，就类似的事情，已经有人总结了一些相应的公式来使用：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231026232840.png"alt="Alt text" /></p><p>这些是一些常见的核函数。</p><p>一般在使用的时候调用它就可以，如果在用 SVM的时候，它会有一个参数。可以自己定义一个核函数，但一般不自己定义，调用现有的就够了。</p><p>SVM其实也有弊端，当数据量很复杂的时候，现有的核函数就没有作用了。因为它会失效，所以我们需要很多的人工分析，整个效率很低。</p><p>但是在整个机器学习的发展史上，它曾经有非常重要的一段历史。有一段时间它的论文量非常的多，做科研的非常爱做SVM，不是因为快速，是因为可以提出来各种各样的 Kerno 函数。</p><p>假如有一组数据不好分割，但是你提出了一种新的核函数，这个函数量可以比较复杂啊然后提升了分割率，提高了效果。</p><p>但是这种方法其实曾经一度让机器学习非常不受人待见，在学术圈非常不受人待见。搞机器学习的人就是每天就是发论文，说我的曲线比你的曲线强，这就是他们干的事。</p><p>所以 10年左右，做机器学习、做人工智能的人都不说自己是做机器学习，做人工智能的。都换个名字，说做文本挖掘等等。</p><p>SVM因为要做各种升维，当数据量比较大的时候，计算量非常的复杂，计算需求量非常的大。</p><p>但是 SVM 它有个好处，就是它比较直观，还有就是 SVM对于不平衡的数据比较有用。</p><p>好，这节课我们就讲到这里，下一节课我们来看「决策树」。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231026233247.png&quot;
alt=&quot;茶桁的 AI 秘籍 核心能力基础 15&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi, 你好。我是茶桁。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>14. 机器学习 - KNN &amp; 贝叶斯</title>
    <link href="https://hivan.me/14.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20KNN%20-%20%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    <id>https://hivan.me/14.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20KNN%20-%20%E8%B4%9D%E5%8F%B6%E6%96%AF/</id>
    <published>2023-10-27T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:50.457Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231024012141.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><p>咱们之前几节课的内容，从线性回归开始到最后讲到了数据集的处理。还有最后补充了SOFTMAX。</p><span id="more"></span><p>这些东西，都挺零碎的，但是又有着相互之间的关系，并且也都蛮重要的。并且是在学习机器学习过程当中比较容易忽视的一些内容。</p><p>从这节课开始呢，我要跟大家将一些其他的内容。</p><p>虽然最近几年用到的方法主要都是深度学习的方法，但是机器学习并不代表就只有深度学习这一种方法。</p><p>当然现在的深度学习其实是从线性回归演化来的，都是用一种梯度下降的方式来做。但是呢其实有很多机器学习方法用的不是这种思想。</p><p>那接下来就给大家要讲的，就是曾经非常有名，也非常有用的一些方法。这些方法的思想和用法和线性回归的机器学习不太一样。</p><p>为什么咱们现在主要用深度学习呢？之所以深度学习很火，原因就是我们的整个机器学习的模型可以像搭乐高积木一样。</p><p>比方有一个线性变化，是 sigmoid，然后有一个Softmax，还有之后大家要学到什么 LSTM，RNCN，还有 Linearregression。他可以互相去连接，可以像玩乐高积木或者说像做电路一样，可以做出来非常复杂的模型。</p><p>那么深度学习的模型变得极其复杂之后，上节课我讲过，如果模型特别复杂，可以表证比较复杂的情况，但是需要比较多的数据去拟合它。</p><p>如果模型很复杂，就需要比较多的数据。而在现在，最近十几年互联网产生的数据量啊大了很多，所以深度学习这种复杂模型的特点就可以被释放出来了。</p><p>而在以前数据量特别小，类似深度学习这种方法，参数也特别多，效果不太好。</p><p>那我们为什么要去学习这些老的经典的学习方法的原因，就是咱们有些时候经常会遇到一个情况就是训练数据其实没有那么多，可能也就一两千个，或者说三五千个。就这几千几万个数据，用深度学习模型其实它是很复杂的，效果也不好。因为模型太复杂了，需要的数据比较多。</p><p>所以，当数据量比较小的时候，问题比较简单的时候，其实用一些比较经典的方法是比较好的。</p><p>第二个原因，像贝叶斯、KNN、还有决策树，这些方法现在虽然慢慢不是主流了，但是他背后的思想其实可以帮助我们做很多事情。</p><p>假如说要判断物体是不是相似等等类似的这些情况，这些方法可以很好的启发你。可以去用在其他场景下去解决问题。</p><p>第三个原因，传统的机器学习有比较好的可解释性。函数 f(x)到底是怎么样求得的 y，里边的每一步可以解释的很清楚。</p><p>举个例子，现在我们给任何一个人一台计算机，只要给足够的时间，也不用多就一个月，用深度学习模型去做股市的预测，都可以拟合一个函数拟合这个股市的预测程度非常高。</p><p>按照这个做法来做的话，我们去股市投一年可以赚200%。但是你用这个模型去预测未来的时候就不行了。</p><p>就是你去预测过去的事情，做训练可以用，不代表未来也可以用。</p><p>所以巴菲特如果问你，为啥这个东西可以？你说因为我在收集到的历史数据上做的效果比较好。你觉得巴菲特会信你吗？</p><p>但是老一代的机械学习模型，尤其是KNN、贝叶斯，可以把它的决策逻辑，为什么决策，为什么得到这个结果的过程给大家讲清楚。</p><p>除此之外要说的是，如果你想成为一个技术很厉害的技术达人，或者一个技术专家，那你要注意一件事，心态一定要开放。</p><p>深度学习不能从逻辑上解释，只能凭感觉去解释，就是只能人去解释。</p><p>就好比一个占卜的人去解释，只能靠人去去阐述。就好比牧师去解读圣经一样。科学的尽头都是玄学是吧？</p><p>之前给大家说过监督学习，监督学习有一个比较数学，比较形式化的定义：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231024012142.png"alt="Alt text" /></p><p>对于一些数据，如上图，数据 D={D1, D2, ..., Dn}。对于这些数据有 x 和y，y 就是它 label，是 desire 的 output，是期望的输出。</p><p>然后我们希望能够学习到一种映射，f:x -&gt; y, 从 x 能够到y，只要能够实现从 x 到 y 这样的一种映射，那么它就是一种监督学习。</p><p>如果这个 y 输出是连续的，那么它就是回归。如果它是discrete，那么我们就说它是分类。</p><p>所以这个mapping，这种映射关系可以是各种各样的一种映射关系，可以是很多种。</p><h2 id="knn">KNN</h2><p>我们现在要来讲的，就是第一种。除了深度学习，线性回归和逻辑回归之外，咱们要讲的第一种，就是KNN，又称 K 近临。</p><p>KNN 几乎可以说是最简单、最直接、最古老的一种机器学习方法了。</p><p>他的原理很简单。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231024012143.png"alt="KNN" /></p><p>比方说我们现在有这么些点，然后问 X轴红色五角星的位置向上对应的点在什么位置？</p><p>那 KNN无法告诉你准确的这个点的位置是多少，但是我们可以利用周围确定的点，也就是圆圈圈定的范围内的这些点，用它们来求一个平均值。</p><p>也就是说，离的最近的 k个值是多少，然后求个平均值就行了。这个好像很有道理的样子。</p><p>好，那我们看到，这个其实是解决了回归问题。那现在我们来看看分类问题：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231024012144.png"alt="Alt text" /></p><p>比方说上面这张图，有一些红色的点和一些蓝色的点。那么现在问题就是，<code>?</code>号所在的这些点是什么呢？</p><p>那左边的圈里，离的最近的是一个蓝色，四个红色，那<code>？</code>的这个点就是红色。在右边的这个圈里，红色比蓝色更多，那这个点也是红色。</p><p>这样的话，你会发现整个求解起来就很简单。</p><p>假如咱们给一组数据 x，</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231024023020.png"alt="Alt text" /></p><p>那么我们有一组<span class="math inline">\(x_i\)</span>, y 有可能是Numerical, 也有可能是一个类别。</p><p>现在来了一个新的<span class="math inline">\(x_i\)</span>,假如要知道他的值，我们要找到离他最近的。</p><p>现在我们有 n 个 x，打比方说有 n=100，在这里 k 假设先定义成 30000, k也是一个参数可以自己改。</p><p>那对应到表格内，如果这个问题是一个回归问题，我们就那最近的点求一个平均值，如果是分类问题，我们就看附近的所有点哪一个分类出现的最多就可以了。</p><p>那按照表格内的数据，假如新出现的这个<spanclass="math inline">\(x_i\)</span>附近是 x1, x2, x3, x4，回归问题就是(0.38+1.27+3.56+3.19)/4, 分类问题就是 (0, 1, 0)。</p><p>这个就是 KNN 的原理，如果要手动去写的话，按照大家水平不到 5分钟就能把这个分类和回归的全部写完。</p><p>它实现起来真的特别简单，而且解释起来也很好解释。比方说它预测出来是红色，为什么是红色呢？因为离我最近的3 个或者 5 个占大多数的是红色。</p><p>它也有一些缺点，比较大的缺点是什么呢？</p><p>一个显著的缺点就是当我们要求的这个点附近完全没有值，离他最近的那个值都离的特别远，那这个时候我们要去求解，它的值就会跑到很远但是离它最近的那些点之间。</p><p>KNN找的是和自己最类似的，但是如果他找不到和自己最类似的，他就傻了。</p><p>总结一下 KNN 的优缺点：</p><table><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><thead><tr class="header"><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>容易实现、容易理解</td><td>运行时间长</td></tr><tr class="even"><td>模型调整容易，可以方便的改变 k 的数量，或者给不同距离的 k进行加权</td><td>容易被异常值影响</td></tr><tr class="odd"><td>适合解决各种复杂问题（分类、回归、高纬、低纬、复杂关系、简单关系）</td><td>所需空间大</td></tr><tr class="even"><td></td><td>高纬空间的距离区分度不大</td></tr></tbody></table><p>关于这个时间久，我们来看一个问题：KNN 的时间复杂度是多少？</p><p>如果是 2.7Ghz 英特尔 i7 处理器，训练数据有 1 千万哥数据是 300维，定义的 k=11，则预测 100 个实例需要多久？</p><p>几毫秒、几秒、几分钟、几个小时还是几天？</p><p>2.7Ghz，Ghz 就是一秒钟可以运行一个 G，一个 G 就是 2 的 11 次方。</p><p>那么估算下来其实应该是几个小时的操作。</p><p>那在一些大厂，阿里，蚂蚁，微信等等，那随随便便都是几千万的数据。</p><p>所以 KNN 更合适数据量小的情况。</p><p>还有就是我们说一个数学上的概念，当一个向量的维度很高的时候，比如说几百几千维，那个时候的向量就会有一个特点，基本上任意两个向量之间的距离都差不多，区别不是很大。</p><p>所以对于 KNN 这种在高纬向量里面做检索，整个距离差的也不大。</p><p>我们需要了解一点，机器学习里面分了两种方式，第一种叫lazy-learning，第二种叫 eager learning。</p><p>lazy-learning 就是懒惰的学习，KNN 就是这种方式。KNN 是典型的一种lazy-learning。</p><p>KNN 只是简单的内容记下来，然后去找了一个最接近的东西。lazy-learning最大的问题就是所观测到的是它周围的这些结果。</p><p>Data site 是比较少的维度，其实效果倒也可以。</p><p>为什么要用 Lazy呢，这个其实和东西方的教育观念的一个差别，在我们传统观念中的那种勤奋，就是死记硬背的埋头苦读其实就是一种Lazy，其含义是思维上比较 Lazy。</p><p>然后比较擅长总结归纳，然后分析预测这种我们叫做的 eager。</p><p>这种方法看到的更加全面，要看到更加广阔的问题，然后抽象出更高层次的函数。我们把这种叫做eager learning。</p><p>基本上在咱们整个课程里面除了 KNN 算法，别的全部都是 eager。</p><h2 id="贝叶斯">贝叶斯</h2><p>接着我们来讲一下贝叶斯，首先我们来看一段文本：</p><blockquote><p>对于某种商品，根据以往购买的数据，在任意投放广告，未进行特点渠道优化的时候，点击广告到购买商品的比率为7%，自然形成的用户中，本科及以上学历的用户占 15%，本科以下学历占85%；现在有一笔广告预算，本科及以上学历渠道的获客成本市场 100元每人，本科以下人群投放广告的成本是 70元每人。问，该广告投放到本科及以上学历专门的人群还是本科以下人群？（本科及以上学历占总国人的比例约为5%，2016 年国家统计局数据）</p></blockquote><p>假如遇到这种问题的时候，一般都会有两拨人互相PK，互相撕扯。一波人认为顾客里面买东西的有 15%的是本科生，本科生比例还挺高，应该大力发展本科生这部分用户，应该把广告主要往本科生这边投。另外一波人会说，有85% 的人是本科以下学历的人，而且国家有 95%的人是本科以下学历，所以这个市场更大，应该去投本科以下的学历。</p><p>这是非常实际的一个问题，假如你以后在公司里的遇到这个问题的时候怎么样去估计呢？我们可以做一个比较简单的数学式子，其实现在我们要估算两个概率，第一个概率是本科及以上的人看见了广告买东西的概率是多少，另外一个就是本科以下的人看见广告买东西的概率是多少。</p><p>但是我们现在没有这个数据，只有购买的人里边有多少个人是本科以上，有多少是本科以下。</p><p>也就是说，我们现在不知道一个本科生看见广告之后有多少概率会买，但是可以通过一个方法来解决。就是本科及以上的人购买的概率，其实等于如下这个方程：P(A|B)= P(AB) / P(B) = P(B|A)P(A) / P(B)。</p><p>对应我们现在面对的这个案例，那其实就等于是：</p><p>Pr(购买 | 本科及以上) = Pr(本科及以上 | 购买) * Pr(购买) /Pr(本科及以上) = 15% * 7% / 5% = 21%</p><p>Pr(购买 | 本科以下) = Pr(本科以下 | 购买) * Pr(购买) / Pr(本科以下) =85% * 7% / 95% = 6.26%</p><p>贝叶斯其实也就是就是这个方法。</p><p>我们根据得出的结论，本科及以上每个人的广告成本为 100 块钱，有 21%的人会买，所以平均成交一个人需要花 476，100/21 = 476。</p><p>同理本科以下的人每个广告成本是 70%, 但是最终会购买东西的概率只有 6%,所以说最终成本是 1,180 块。70 / 6% = 1167。</p><p>那假如咱们的产品卖 2,000，花 100万广告费只投放给本科以上的人群，那公司就可以收入 420万，投给本科以下的人，就只能卖 116.7 万。</p><p>这个例子是一个非常典型的商业决策例子，这种决策都有个特点，未来的事情谁都说不上。过去的事情板上钉钉的已经发生了。</p><p>我们再说一个很典型的例子，一个骰子连续丢出 4 次 6，那第 5 次出现 6的概率到底是大于 1/6 还是小于 1/6，还是等于 1/6？</p><p>等于 1/6 的说法，因为骰子出不出 1/6每一次事件之间是独立的，所以说第五次也是 1/6。小于 1/6的说法，已经出了那么多 6 了，接下来不应该出 6了。而对于有些人来说，这色子现在明显就是有问题，一个骰子一般来说很少会出现这样的情况，所以这个骰子容易出6，概率肯定是比 1/6 大。</p><p>所以一般来说，比较聪明的决策是把未来看成是过去的再次发生。贝叶斯分类其实就是做这件事情的。</p><p>贝叶斯定理表示了在给定先验概率和条件概率的情况下，如何计算后验概率。</p><p><span class="math display">\[\begin{align*}P(A|B) = \frac{P(B|A)*P(A)}{P(B)}\end{align*}\]</span></p><ul><li>P(A|B) 是后验概率，表示在给定观测数据 B 后事件 A 发生的概率。</li><li>P(B|A) 是条件概率，表示事件 A 发生的情况下事件 B 发生的概率。</li><li>P(A) 是先验概率，表示事件 A 在没有观测数据 B 的情况下的概率。</li><li>P(B) 是边际概率，表示事件 B 发生的总概率。</li></ul><p>在朴素贝叶斯分类中，我们使用特征向量 X=(x1,x2,...,xn)来表示一个样本，其中 x_i 是第 i个特征的取值。我们希望根据这些特征来分类样本为不同的类别 C。</p><p><span class="math display">\[\begin{align*}p(C_k,x_1,...,x_n) &amp; = p(x_1,...,x_n, C_k) \\&amp; = p(x_1|x_2,...,x_n,C_k)p(x_2,...,x_n,C_k) \\&amp; = p(x_1|x_2,...,x_n,C_k)p(x_2|x_3,...,x_n,C_k)p(x_3,...,x_n,C_k)\\&amp; = ... \\&amp; =p(x_1|x_2,...,x_n,C_k)p(x_2|x_3,...,x_n,C_k)...p(x_{n-1}|x_n,C_k)p(x_n|C_k)p(C_k)\end{align*}\]</span></p><p>我们要做这个决策，在以上的式子基础上做了一个很重要的假设，假设什所有特征之间的条件是独立的，即给定类别C下，特征之间的关系是独立的。这个假设使得计算变得简单，但通常并不成立，尤其实在自然语言处理任务中。</p><p>因为这个原因，所以它被称为 NaiveBayes，就是我们通常所称的「朴素贝叶斯」。其实 Naive真正的翻译应该是「幼稚的」。</p><p>简化后，朴素贝叶斯分类器的数学公式：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231024012145.png"alt="Alt text" /></p><ul><li>P(C|X) 是后验概率，表示在给定特征向量 X 的情况下，样本属于类别 C的概率。</li><li>P(X|C): 是似然度，表示在类别 C 下观测到特征向量 X的概率。基于朴素独立性假设，可以将它分解为各个特征的条件概率的乘积：<spanclass="math inline">\(P(X|C) =P(x_1|C)*P(x_2|C)*...*P(x_n|C)\)</span></li><li>P(C) 是先验概率，表示样本属于类别 C 的概率。</li><li>P(X) 是归一化常数，用于确保后验概率的总和为 1。</li></ul><p>为了进行分类决策，朴素贝叶斯分类器计算每个类别的后验概率，然后选择具有最高后验概率的类别作为分类结果。数学上，这可以表示为：</p><p><span class="math display">\[\begin{align*}C_{MAP} = \arg max_cP(C|X)\end{align*}\]</span></p><p>其中<span class="math inline">\(C_{MAP}\)</span>是最可能的类别。</p><p>比方下面这个列表：</p><table><thead><tr class="header"><th>Example No.</th><th>Color</th><th>Type</th><th>Origin</th><th>Stolen?</th></tr></thead><tbody><tr class="odd"><td>1</td><td>Red</td><td>Sports</td><td>Domestic</td><td>YES</td></tr><tr class="even"><td>2</td><td>Red</td><td>Sports</td><td>Domestic</td><td>NO</td></tr><tr class="odd"><td>3</td><td>Red</td><td>Sports</td><td>Domestic</td><td>YES</td></tr><tr class="even"><td>4</td><td>Yellow</td><td>Sports</td><td>Domestic</td><td>NO</td></tr><tr class="odd"><td>5</td><td>Yellow</td><td>Sports</td><td>Imported</td><td>YES</td></tr><tr class="even"><td>6</td><td>Yellow</td><td>SUV</td><td>Imported</td><td>NO</td></tr><tr class="odd"><td>7</td><td>Yellow</td><td>SUV</td><td>Imported</td><td>YES</td></tr><tr class="even"><td>8</td><td>Yellow</td><td>SUV</td><td>Domestic</td><td>NO</td></tr><tr class="odd"><td>9</td><td>Red</td><td>SUV</td><td>Imported</td><td>NO</td></tr><tr class="even"><td>10</td><td>Red</td><td>Sports</td><td>Imported</td><td>YES</td></tr></tbody></table><p>列表中分别有 Color，车的颜色，Type，车的型号，Origin，车的产地，以及Stolen，是否被偷。</p><p>我们令<span class="math inline">\(x_1\)</span> = color, <spanclass="math inline">\(x_2\)</span> = type, <spanclass="math inline">\(x_3\)</span> = origin，那么<spanclass="math inline">\(x_1,x_2,x_3\)</span>现在就是特征。</p><p>假如基于上面朴素贝叶斯的数学公式，我们得到：</p><p><span class="math display">\[\begin{align*}P(C_1 |x) = \frac{分子}{P(x)} \\P(C_2 |x) = \frac{分子}{P(x)} \\\end{align*}\]</span></p><p>我们可以发觉，它们的分母其实都是一样的，都是<spanclass="math inline">\(P(x)\)</span>。虽然我们并不知道<spanclass="math inline">\(P(x)\)</span>是多少，但是我们要比较的是<spanclass="math inline">\(P(C_1|x)\)</span>和<spanclass="math inline">\(P(C_2|x)\)</span>的大小，所以我们完全可以不考虑一样大小的分母，就只比较分子大小就可以了。</p><p>那我们就将式子变为：</p><p><span class="math display">\[\begin{align*}P(C_1|x) = P(x_1|C_1)P(x_2|C_1)P(x_3|C_1)P(C_1) \\P(C_2|x) = P(x_1|C_2)P(x_2|C_2)P(x_3|C_2)P(C_2)\end{align*}\]</span></p><p>我们拿一个来分析，其中的<spanclass="math inline">\(P(x_1|C_1)\)</span>, 假设现在<spanclass="math inline">\(x_1 = Red, C_1 = Stolen(YES)\)</span>,那其实就是<spanclass="math inline">\(P(Red|Stolen(YES))\)</span>，也就是所有被偷的里面，Red占多少。那么现在就简单的数数就行了。同理，各个特征的值我们都可以直接数数就能数出来。</p><p><span class="math inline">\(P(C_1)\)</span>和<spanclass="math inline">\(P(C_2)\)</span>的概率是多少？<spanclass="math inline">\(P(C_1)\)</span>和<spanclass="math inline">\(P(C_2)\)</span>是我们所有的汽车里面，被偷的占比多少，没偷的占比多少，也是数数就可以数出来。</p><p>贝叶斯方法就是我们只需要有一张表格，然后通过数数，通过简单的计算就能够把概率给求出来。黄色的车被偷的概率大还是红色车被偷的概率大，直接就可以数出来。</p><p>我们现在看到的贝叶斯都是一个一个类别，但是有的时候，假如说我们所面对的不是0，1 这种类别，而是：3，2，2.5, 1.6这种实数该怎么办呢？我们就要用到「高斯贝叶斯分布」。</p><p>高斯贝叶斯分布是在处理连续值的时候一个非常典型的做法，就是把连续值做一个分布，做一个离散化处理：</p><p><span class="math display">\[\begin{align*}p(x = v|C_k) = \frac{1}{\sqrt{2\pi\sigma^2_k}}e^{-\frac{(v-\muk)^2}{2\sigma^2_k}}\end{align*}\]</span></p><p>我们只需要知道这一点就行了，贝叶斯解决连续值的问题用了高斯分布。</p><p>那么贝叶斯公式的优点是什么呢？首先它非常容易被实现，基本上写代码都能写到，就是不断的数数。</p><p>而且它的预测其实很快，它其实做出了一个概率函数，不像 KNN那样还要把所有的数据都存下来，就是存下来了一个判别的数字，直接一乘就可以，特别快。而且在数据量很大的时候效果也比较好。</p><p>但是它也有缺点，因为做了一个 if 的假设，就是 X1 和 X2、X3之间都没有关系，但是其实很多时候 x之间是有关系的，这其实是很错误的。</p><p>所以当问题变得复杂的时候，贝叶斯往往就不行了。比方说要解决复杂的自然语言处理问题、图像识别问题就不行了。</p><h2 id="贝叶斯案例---预测广告">贝叶斯案例 - 预测广告</h2><p>咱们来看一个贝叶斯的题目。</p><p>假设我们现在有三段短信内容：第一段是一段广告："快来抢购，这是最大的优惠"；第二段也是一段广告："今天抢购最优惠"；第三段不是广告，就是一个正常的短信内容："今天什么时候回家"</p><ul><li>Ad: 快来抢购，这是最大的优惠</li><li>Ad: 今天抢购最优惠</li><li>Text: 今天什么时候回家</li></ul><p>现在我们有一个问题：“今天回家抢购”，我们现在要做的是一个垃圾短信拦截，那么这一段内容是属于广告还是不属于广告？</p><p>那这个问题，我们实际上就可以用贝叶斯来进行解决。</p><p>我们假设 S = 今天回家抢购</p><p>那么我们要做的事情就是比较这两个概率的大小:Pr(Ad|S) ~Pr(Text|S)。</p><p>根据贝叶斯的公式，我们就可以得到下面这一步：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Pr(Ad | S) = Pr(Ad | w1 w2 w3)<br>Pr(Text | S) = Pr(Ad | w1 w2 w3)<br></code></pre></td></tr></table></figure><p>然后我们可以将其转化为：</p><p><span class="math display">\[\begin{align*}Pr(Ad|w_1 w_2 w_3) =\frac{Pr(w_1|Ad)Pr(w_2|Ad)Pr(w_3|Ad)Pr(Ad)}{Pr(w_1w_2w_3)}\end{align*}\]</span></p><p>同理呢，Text 和 Ad 一样可以进行转化：</p><p><span class="math display">\[\begin{align*}Pr(Text|w_1 w_2 w_3) =\frac{Pr(w_1|Text)Pr(w_2|Text)Pr(w_3|Text)Pr(Text)}{Pr(w_1w_2w_3)}\end{align*}\]</span></p><p>其中这些 w1, w2, w3 就是相关的特征，我们将 S 分词成 w1 = 今天，w2 =回家，w3 = 抢购。</p><p>现在我们来看一下，Pr(Ad) 等于多少？等于2/3，也就是我们这三句话中，已知的广告概率是多少。相对的，Pr(Text) 就是1/3。</p><p>接着我们可以知道。Pr(w1|Ad) 就是“今天”在所有广告里出现了几次，广告一共是 2 次，“今天”出现了 1 次，所以应该是 1/2。</p><p>那 Pr(w2|Ad)呢，“回家"没有出现过，不过这里我们要注意，虽然它没有出现过，但是我们不能让它的概率为0，我们要给它一个估计值，这个叫做 OOV，out of vocabulary。因为直接为 0的话，这个词有比较奇怪就没法去做了，判断不了。</p><p>经过简单的分词，我们一共有 13 个单词，13 个单词里边 w2出现了一次，所以咱们可以给他一个估计值 1/13。</p><p>如果没有这个估计值的话，可能只要在内容里面随机加一些生僻字，就能够躲过系统的检测。</p><p>w3 是“抢购”，Pr(w3|Ad) 就是 1。那么 Pr(Ad|S) 的分子部分就是 Pr(Ad) =2/3, Pr(w1|Ad) = 1/2, Pr(w2|Ad)=1/13, Pr(w3|Ad) = 1。</p><p>那我们上节课说过，分母我们其实不用管，也就是 Pr(w1 w2 w3)在过程中其实是不用计算的。</p><p>我们同理再来看以下 Pr(Text|S) 的分子，Pr(Text)=1/3, Pr(w1|Text)=1,Pr(w2|Text)=1, Pr(w3|Text)=2/13。</p><p>那我们最后可以得到简单的一个小学式子：</p><p><span class="math display">\[\begin{align*}Pr(Ad|S) &amp; = 1/2 * 1/13 * 2/3 = 1/13 * 1/3  \\Pr(Text|S) &amp; = 2/13*1/3\end{align*}\]</span></p><p>明显 Pr(Text|S)更大一点，所以“今天回家抢购”这句话更大的概率下不是广告。</p><p>这个就是贝叶斯的一个应用案例，也是非常常见的一个面实题。</p><p>好，那这节课的内容就到这里了，要记得复习。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231024012141.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;咱们之前几节课的内容，从线性回归开始到最后讲到了数据集的处理。还有最后补充了
SOFTMAX。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Mac上挂载APFS移动硬盘</title>
    <link href="https://hivan.me/mount-apfs-on-mac/"/>
    <id>https://hivan.me/mount-apfs-on-mac/</id>
    <published>2023-10-26T10:10:41.000Z</published>
    <updated>2023-10-26T10:52:24.631Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>自用文，有需要的自取。</p></blockquote><p>百度网盘同步会认为移动硬盘是系统盘，所以无法进行同步。当然，也有例外的，之前我也是不知怎么同步的。</p><p>这次设置的时候被警告了，不允许设置。</p><span id="more"></span><p>好吧，那就只能将移动硬盘挂载到我的用户目录里了，我的移动硬盘是APFS类型，执行下面命令：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># 查看当前硬盘IDENTIFIER</span><br>diskutil apfs list<br><br><span class="hljs-comment"># 或者下面这段命令</span><br>diskutil list<br><br><span class="hljs-comment"># 然后需要进行解锁，恢复键值(recovery_key):</span><br>diskutil apfs unlockVolume <span class="hljs-regexp">/dev/</span>apfs_volume_id -passphrase recovery_key<br><br><span class="hljs-comment"># 接着进行装载到自己期望的目录</span><br>diskutil mount -mountPoint Path apfs_volume_id<br></code></pre></td></tr></table></figure><p>假定我的硬盘<code>apfs_volume_id</code>为disk5s1,希望挂载到<code>~/mount</code>则：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">diskutil apfs unlockVolume /dev/disk5s1 -passphrase recovery_key<br>diskutil mount -mountPoint ~/mount /dev/disk5s1<br></code></pre></td></tr></table></figure><p>本是留待自用的，有需要的有缘人自行取走。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;自用文，有需要的自取。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;百度网盘同步会认为移动硬盘是系统盘，所以无法进行同步。当然，也有例外的，之前我也是不知怎么同步的。&lt;/p&gt;
&lt;p&gt;这次设置的时候被警告了，不允许设置。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>13. 机器学习 - 数据集的处理</title>
    <link href="https://hivan.me/13.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%84%E7%90%86/"/>
    <id>https://hivan.me/13.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%84%E7%90%86/</id>
    <published>2023-10-24T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:54.093Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195926.png"alt="茶桁的 AI 秘籍 核心基础 13" /></p><p>Hi，你好。我是茶桁。</p><p>上一节课，咱们讲解了『拟合』，了解了什么是过拟合，什么是欠拟合。也说过，如果大家以后在工作中做的就是机器学习的相关事情，那么欠拟合和过拟合就会一直陪伴着你，这两者是相互冲突的。</p><span id="more"></span><p>现在，让我们一起来思考一个问题：<code>overfitting</code>，过拟合产生的原因是什么？</p><p>如果这是在模型层面的话，参数过多还是过少？如果从数据层面来看，是过多还是过少呢？</p><p>好，我们来揭晓答案。如果模型层面思考，那是就是参数过多。如果从数据层面来看，那是数据过少。</p><p>现在我们需要理解一件事情，这两个事情其实是一回事，数据量多和模型复杂其实是一回事。它背后的原因就是因为任何一个f(x)如果有很多的参数，拟合的时候随着这个参数数量越多，那么我们所需要的训练数据集也要增多。也就是说当模型非常复杂，参数特别多，只要数据量特别大，那就不算多。就说现有的数据量对于参数不够，训练力度不够。</p><p>这就好比是有一个天才的孩子，脑子极其聪明，就跟茶桁一样。哎，这个孩子呢智商极其高，但是他想事情想的特别的复杂，结果他现在见到的事情都是太过于简单的东西。那么就不能把他的这个潜力发挥出来。</p><p>好，我们接着下一个问题：如何判断一件事情有没有发生过拟合或者欠拟合呢？</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195927.jpeg"alt="Alt text" /></p><p>我们看这张图，假如这是一个 2 分类问题，咱们训练时候结果的准确度是 0.7左右。那么大家想一下，这个是过拟合还是欠拟合呢？</p><p>如果模型训练的时候效果还不错，快接近于 1了，达到了百分之九十几。但是实际上用 validation数据集去测的时候发现准确度下到百分之八十几，或者百分之七十几，总之就是比在训练的时候那个效果要差。这个就叫作过拟合。</p><p>咱们上节课给大家说的就是这个问题，机器学习的整个流程最终的目的不是为了把loss 函数降到最低，我们要关心的是像recall，precition，这种信息才是最关键的。</p><h2 id="training-data-split">Training data split</h2><p>接下来，咱么要再讲几个机器学习里面极其重要的几个概念，第一个是数据集的切分(Training data split)。第二个是Normalization。第三个，Standardized。</p><p>其实上节课，咱们已经说过了数据集的切分问题。数据集切分最主要的原因是因为我们经常会遇见过拟合的情况，为了避免我们把所有的数据拿来不断的做training, 然后在使用的时候效果变得不好，那我们不如自己找一些数据出来做test sets，为了可以反复多次的去检验效果好不好，就增加了一个 validationsets。</p><p>在真实环境下我们是怎么去做这样一件事呢？我们来简单的演示下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>sample_data = np.random.random(size(<span class="hljs-number">100</span>, <span class="hljs-number">5</span>))<br><br>train, test = train_test_split(sample_data, train_size=<span class="hljs-number">0.8</span>)<br>train<br><br>---<br>array([[<span class="hljs-number">1.55582066e-01</span>, <span class="hljs-number">8.19437761e-01</span>, <span class="hljs-number">3.54628257e-02</span>, <span class="hljs-number">5.53248385e-01</span>,<br>        <span class="hljs-number">4.23785508e-01</span>],<br>...<br>       [<span class="hljs-number">7.24889349e-01</span>, <span class="hljs-number">1.23458057e-01</span>, <span class="hljs-number">9.74101303e-01</span>, <span class="hljs-number">1.72605427e-01</span>,<br>        <span class="hljs-number">6.59164912e-01</span>]])<br></code></pre></td></tr></table></figure><p>非常的简单，我们来看，<code>sklearn</code>里自带了这种分割方法。我们随机了100 行 5列的数据，然后使用<code>train_test_split</code>将其分割成<code>train</code>和<code>test</code>两份，在后面的参数内设置了百分位。</p><p>这样，这个数据就做了一个拆分。值得注意的是，给大家教一个小技巧，这是第一种方法：split。其实不只是sklearn，pytorch 和 keras 也都有 split 方法。</p><p>但是我们去看一下源码会发现，这个 split 方法是没有validation，它的输出只有 train 和 test 两部分。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195928.png"alt="Alt text" /></p><p>为了解决这个问题，我们可以用一个简单的方法。这次我们使用 Numpy。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">indices = np.random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sample_data)), size=<span class="hljs-built_in">int</span>(<span class="hljs-number">0.8</span>*(<span class="hljs-built_in">len</span>(sample_data))), replace=<span class="hljs-literal">True</span>)<br><br>indices<br><br>---<br>array([<span class="hljs-number">39</span>, <span class="hljs-number">65</span>,  <span class="hljs-number">5</span>, <span class="hljs-number">13</span>, <span class="hljs-number">69</span>,  <span class="hljs-number">8</span>, <span class="hljs-number">49</span>,  <span class="hljs-number">2</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">99</span>, <span class="hljs-number">13</span>, <span class="hljs-number">64</span>, <span class="hljs-number">76</span>, <span class="hljs-number">55</span>,<br>       <span class="hljs-number">96</span>, <span class="hljs-number">12</span>, <span class="hljs-number">87</span>, <span class="hljs-number">81</span>, <span class="hljs-number">55</span>, <span class="hljs-number">96</span>, <span class="hljs-number">54</span>, <span class="hljs-number">94</span>, <span class="hljs-number">15</span>, <span class="hljs-number">44</span>, <span class="hljs-number">23</span>, <span class="hljs-number">17</span>, <span class="hljs-number">76</span>, <span class="hljs-number">98</span>, <span class="hljs-number">84</span>, <span class="hljs-number">21</span>, <span class="hljs-number">50</span>,<br>       <span class="hljs-number">62</span>, <span class="hljs-number">58</span>, <span class="hljs-number">21</span>, <span class="hljs-number">95</span>, <span class="hljs-number">22</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">6</span>, <span class="hljs-number">35</span>, <span class="hljs-number">93</span>, <span class="hljs-number">34</span>, <span class="hljs-number">68</span>, <span class="hljs-number">49</span>, <span class="hljs-number">29</span>, <span class="hljs-number">81</span>, <span class="hljs-number">58</span>, <span class="hljs-number">45</span>, <span class="hljs-number">95</span>,<br>       <span class="hljs-number">26</span>, <span class="hljs-number">21</span>, <span class="hljs-number">97</span>, <span class="hljs-number">43</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span>, <span class="hljs-number">52</span>, <span class="hljs-number">93</span>, <span class="hljs-number">34</span>, <span class="hljs-number">17</span>, <span class="hljs-number">71</span>, <span class="hljs-number">76</span>, <span class="hljs-number">38</span>, <span class="hljs-number">92</span>, <span class="hljs-number">62</span>, <span class="hljs-number">21</span>, <span class="hljs-number">98</span>,<br>       <span class="hljs-number">56</span>, <span class="hljs-number">28</span>, <span class="hljs-number">54</span>, <span class="hljs-number">39</span>, <span class="hljs-number">15</span>, <span class="hljs-number">17</span>, <span class="hljs-number">62</span>, <span class="hljs-number">81</span>, <span class="hljs-number">61</span>,  <span class="hljs-number">4</span>, <span class="hljs-number">51</span>, <span class="hljs-number">71</span>])<br></code></pre></td></tr></table></figure><p>这里我们等于是把它的整体的顺序打乱，后面的 replace就是可以重复的去取。这样我们就随机的取了一些下标。</p><p>这是一个比较简单的方法，那么我们为什么要设置<code>replace=True</code>呢？当数量特别大的时候，多取几个少取几个其实不是很影响，另外replace的话，他内部的那个随机的算法其实是不一样的，速度会快的多。以后如果遇到类似的事情，你也可以去用这个方法去做它。</p><h2 id="normalization">Normalization</h2><p>除了这个以外，做机器学习的时候，要做数值的归一化 (Normalization)和标准化 (Standardized) 这样一个动作。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195929.png"alt="Alt text" /></p><p>我们这么做的目的是什么呢？假设我们现在有多个特征的数据集，不过我们注意到一点，就是这些特征值跨越的范围是无法进行比较的。</p><p>比如，一个特征在 1 和 10 之间变化，但是另外一个实在 1 和 1000之间变化。如果我们忽略了这一点而直接进行建模，模型分配给这些特征的权重将会受到严重影响，模型最终会为较大的变量分配较高的权重。</p><p>现在要解决这个问题，将这些特征置于相同或者至少是可比较的范围内，那就需要对数据做一个数据归一化。</p><p>归一化的目标是讲数据缩放到特定范围内，一般来说是[0,1]或者[-1,1]之间。这有助于消除不同特征之间的尺度差异，确保它们对模型的权重贡献大致相等。</p><p>数据归一化对于每个特征x，归一化后的值<code>Xnormalized</code>计算如下：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195930.png"alt="Alt text" /></p><p>其中 min 是特征的最小值，max是特征的最大值。这个操作确保了数据的最小值映射到 0，最大值映射到 1.</p><p>在数据预处理过程中，首先计算每个特征的最小值和最大值，然后使用上述公式对数据进行归一化。这通常通过一次遍历数据来实现。</p><p>在进行归一化的时候，我们所使用的那个公式会有一个缺点，就是它并不能很好的去处理异常值。比方说，如果有0 到 40 之间的 99 个值，其中一个值为 100，则这 99 个值讲全部转换为 0 到0.4 之间的值。这些数据和以前一样被压缩！下图就是个示例：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195931.png"alt="Alt text" /></p><p>这些数据在进行归一化之后，解决的是 y 轴上堆集的问题，但是 x轴上的问题依然存在，就像途中橙色点那个异常值：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195932.png"alt="Alt text" /></p><p>关于这个知识点，我们来看一个极其简单的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">some_large_number = [<span class="hljs-number">23421421</span>,<span class="hljs-number">42155151</span>,<span class="hljs-number">25531238</span>,<span class="hljs-number">21826139</span>, <span class="hljs-number">32189732</span>, <span class="hljs-number">32103721</span>]<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">normalize</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> (x - np.<span class="hljs-built_in">min</span>(x)) / (np.<span class="hljs-built_in">max</span>(x) - np.<span class="hljs-built_in">min</span>(x))<br>ic(normalize(np.array(some_large_number)))<br><br>---<br>array([<span class="hljs-number">0.07847317</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.18225672</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.50979325</span>, <span class="hljs-number">0.5055623</span> ])<br></code></pre></td></tr></table></figure><p>我手动定义了 6个比较大的数字，在进行处理之后我们看到了，都变成了一些特别小的数字。</p><p>同样的，对于特别小的数字，它一样可以进行处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">some_small_number = [<span class="hljs-number">0.00000231213</span>,  <span class="hljs-number">0.0005600321</span>, <span class="hljs-number">0.0000041412892</span>, <span class="hljs-number">0.000987890576</span>, <span class="hljs-number">0.0000578921764</span>]<br>ic(normalize(np.array(some_small_number)))<br><br>--- <br>array([<span class="hljs-number">0.</span>, <span class="hljs-number">0.56588085</span>, <span class="hljs-number">0.00185592</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.05639333</span>])<br></code></pre></td></tr></table></figure><h2 id="standardized">Standardized</h2><p>那么还有就是标准化，对于标准化，其目标是讲数据转化为均值为0，标准差为 1的分布，也就是标准正态分布。这有助于处理偏斜分布的数据，并确保数据的均值和方差在模型中起到合适的作用。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195933.png"alt="Alt text" /></p><p>那对于每一个特征 x，标准化的值<code>z</code>计算如下：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195934.png"alt="Alt text" /></p><p><span class="math inline">\(\mu\)</span>是特征的均值，<spanclass="math inline">\(\sigma\)</span>是特征的标准差。这个操作使数据的均值为0，标准差为 1。</p><p>在数据预处理的过程中，首先计算每个特征的均值和标准差，然后使用上述公式对数据进行标准化处理。标准化后的数据具有均值0 和标准差 1，这有助于模型更好的理解和捕捉数据之间的关系。</p><p>无论是归一化还是标准化，其实依据来源都是基于线性代数的变化理论，这确保了归一化和标准化后的数据分布具有特定的属性，这些属性对于机器学习算法的表现非常有帮助。</p><p>我们来看一个标准化的例子，为了让大家更为明显的了解其意义，我做了一些非常大的数据，但是每一个都不相同。这些数据有一个特点，就是相对于数值本身的大小来说，几个数值之间的差距可以说是非常微小的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">some_dense_number = [<span class="hljs-number">47238941</span>, <span class="hljs-number">47238946</span>, <span class="hljs-number">47238951</span>, <span class="hljs-number">47238931</span>, <span class="hljs-number">47238949</span>, <span class="hljs-number">47238936</span>]<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">standarlize</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> (x - np.mean(x))/ np.std(x)<br><br>ic(standarlize(np.array(some_dense_number)))<br><br>---<br>array([-<span class="hljs-number">0.18752289</span>,  <span class="hljs-number">0.51568795</span>,  <span class="hljs-number">1.2188988</span> , -<span class="hljs-number">1.59394459</span>,  <span class="hljs-number">0.93761446</span>, -<span class="hljs-number">0.89073374</span>])<br></code></pre></td></tr></table></figure><p>我们定义的数据实际上是非常密集，但是使用 standarlize公式之后，就变得比较的分散，比较的均匀了。这个情况还是很多的。</p><h2 id="one-hot">ONE-HOT</h2><p>在讲完 training data split, normalization, Standardized之后，我们来看下面一点：ONE-HOT。</p><p>为什么要用ONE-HOT？我们都直到，咱们计算机里其实都是数字，包括视频，图片，声音，文字等其实都是数字。</p><p>数字和数字其实是不一样的。比如，有一群人分成了<code>4</code>组：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195935.png"alt="Alt text" /></p><p>然后有一个女生的 GPA 是<code>4</code>:</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195936.png"alt="Alt text" /></p><p>那么分组的<code>4</code>和 GPA的<code>4</code>有什么区别？最明显的一个区别就是，分组的<code>4</code>只是一个组名，那么假如和<code>1</code>组交换组名并没有太大的关系，但是GPA 的这个<code>4</code>如何和<code>1</code>交换一下，那就从 4 分变成 1分了，那这两个是不能相互变换的。本质上，其区别就是一个可比一个不可比。</p><p>我们也就发现了，数字其实是有区别的。这个世界中，数字其实可以分成两类：</p><p>第一类叫作Categorical，叫作分类数据，也被称为离散数据或名义数据。它们之间不能被比较，也不能被排序，这些数字也仅仅是表示一个和另外一个不一样。就我们刚才讲人群分为1、2、3、4 组，其实分成 A、B、C、D 组也是一样的，只是表示区别。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195937.png"alt="Alt text" /></p><p>第二类是Numerical，数值数据，也被称为连续数据。这个是可以比较的，也可以进行排序。这种数据包括可以用来进行数学运算的实数值。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195938.png"alt="Alt text" /></p><p>Numerical 还可以进一步分为整数和浮点数。</p><p>知道了这一点之后，那我们以后遇到类似的情况不要随便的做加减乘除。</p><p>那我们有了 Categorical 和 Numerical这两种类型之后，会对我们有一些什么比较重要的影响？</p><p>如果现在有一个函数，这个函数输入一个 x 向量，它输出就是分为一个Categorical 和 numerical。</p><p>输出是 0-1 这样一个数字，是一个典型的逻辑回归。</p><p>假如有一个人在北京，年龄27，性别男，月入一万二。然后还有一个人，生活在安徽，年龄28，性别女，月入 8,000。第三个住在上海，年龄28，性别男，月入一万三。</p><ol type="1"><li>北京，27, 12000</li><li>安徽，28, 8000</li><li>上海，28, 13000</li></ol><p>我们注意这三组数据，如果现在做一个向量表证。</p><p>关于地域，我们常常使用的方法包括邮编排序，或者使用拼音排序。假如这里我们就使用拼音首字母来进行排序，安徽假如是1，北京是 2，上海是 27。</p><p>我们的数据进行向量化可能就会变成下面这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. 北京</span><br>vec(<span class="hljs-number">2</span>, <span class="hljs-number">27</span>, <span class="hljs-number">12000</span>)<br><span class="hljs-comment"># 2. 安徽</span><br>vec(<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">8000</span>)<br><span class="hljs-comment"># 3. 上海</span><br>vec(<span class="hljs-number">27</span>, <span class="hljs-number">28</span>, <span class="hljs-number">13000</span>)<br></code></pre></td></tr></table></figure><p>然后我们定义一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> (<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>非常简单一个函数，返回表示对某一样东西买还是不买。</p><p>函数的实现过程就是类似于<code>wi * xi + b</code>这种形式。</p><p>我们观察向量发现，就向量值而言，北京这个人和安徽这个人之间的向量差比北京和上海这两人之间的向量差还要小。</p><p><span class="math display">\[|v_1 - v_2| &lt; |v_1 - v_3|\]</span></p><p>我们假如说经过函数<code>f(x)</code>之后，输出的结果分别为 Y1, Y2,Y3。因为 v1 和 v2 离的更近，就会有一个结果，Y1 和 Y2的结果其实会更相似。但是其实呢，这种结果完全不对。这样乱比其实会出问题，会让程序出错。</p><p>我们现在知道，这其实是一个 Categorical 的问题。为了解决 Categorical的这种问题，我把 Categorical 改成这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">北京: [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]<br>安徽: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]<br>上海: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><p>改成这样之后这个向量就变成了这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. 北京</span><br>vec(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">27</span>, <span class="hljs-number">12000</span>)<br><span class="hljs-comment"># 2. 安徽</span><br>vec(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">28</span>, <span class="hljs-number">8000</span>)<br><span class="hljs-comment"># 3. 上海</span><br>vec(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">13000</span>)<br></code></pre></td></tr></table></figure><p>向量变成这样之后，就解决了我们刚刚说的那个问题。不会导致因为分类过于相似让北京和安徽向量相似度大于北京和上海的相似度。</p><p>对于这样一个向量，三组数据中改变的那个值向量值就都为<spanclass="math inline">\(\sqrt 2\)</span>，这一种方式就被称为 ONE-HOT。</p><p>那这种方式也是存在问题的，目前我们只去考虑三个城市。可是当存在成百上千个城市的时候，比如说Google 地图等等这些应用。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195939.png"alt="Alt text" /></p><p>当城市越来越多的时候，那它的维度就会变得很高：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">Beijing     = [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>]<br>Shanghai    = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>]<br>Chengdu     = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>]<br>Shenzhen    = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>]<br>...<br></code></pre></td></tr></table></figure><p>我们想想一下，这样得有多少个地址？可能空间会极其的大，你这样的话数字光存起来得上亿个存储单元。</p><p>ONE-HOT 就有这样的问题：</p><ol type="1"><li>耗费空间</li><li>数据量大，更新起来，效率极低</li><li>遗漏了很多重要新息</li></ol><p>就比如，我们再增加几个人如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">- 重庆 <span class="hljs-number">27</span> <span class="hljs-number">9000</span><br>- 成都 <span class="hljs-number">26</span> <span class="hljs-number">8500</span><br>- 呼和浩特 <span class="hljs-number">26</span> <span class="hljs-number">8500</span><br></code></pre></td></tr></table></figure><p>在这三个城市中，我们脑子里其实就直到，重庆和成都是非常接近的。但是在ONE-HOT 里是体现不出来，其向量值依然是根号 2。</p><p>为了解决这些问题，人们就用到了更先进的一种方法：embedding，叫作嵌入。</p><p>嵌入就是把东西放在固定的位置，这个就是嵌入的意思。在这里，就我们空间中如果有几个实体NTT1 NTT2 NTT3，我们把这些实体放到这个空间中，要达到一个结果就是如果实体1 和实体 2 的相似度小于实体 1 和实体 3的相似度，这个相似度我们可以自己来定义，比如成都和重庆的生活方式，再比如重庆和北京都是直辖市。</p><p>在这个问题场景下，我们期望达到的结果是如果这两个实体相似那么他们在空间中的距离也接近。</p><p>如何实现 Embedding,这本身是一个研究领域，是现在非监督学习，表证学习里面非常重要的一个研究领域，属于比较高级的一个知识点。</p><p>第二就是如果之后咱们学NLP，那么一定会讲到这个，因为要把文本单词进行嵌入，到时候会学到。如果是学推荐系统的，大家也会学什么Graph embedding，基于图的用户行为。</p><p>那之后咱们学习 NLP，其基础就是Embedding。关于这个问题，我们其实目前了解到这里就行了。再往下延展下去，又是一个专门的研究话题。延展后的这个问题解决方案，在我们后面的课程中会等着大家去学习。</p><p>我们再来看看 ONE-HOT 的实际展示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">array = [<span class="hljs-string">&#x27;北京&#x27;</span>,<span class="hljs-string">&#x27;上海&#x27;</span>,<span class="hljs-string">&#x27;广州&#x27;</span>,<span class="hljs-string">&#x27;宁夏&#x27;</span>,<span class="hljs-string">&#x27;成都&#x27;</span>,<span class="hljs-string">&#x27;上海&#x27;</span>,<span class="hljs-string">&#x27;北京&#x27;</span>]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">one_hot</span>(<span class="hljs-params">elements</span>):<br>    pure = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(elements))<br>    <br>    vectors = []<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> elements:<br>        vec = [<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(pure)<br>        vec[pure.index(i)] = <span class="hljs-number">1</span><br>        vectors.append(vec)<br><br>    <span class="hljs-keyword">return</span> vectors<br>ic(one_hot(array))<br><br>---<br>one_hot(array): [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>                [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>                [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>                [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>                [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>                [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>                [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]]<br></code></pre></td></tr></table></figure><p>其实 ONE-HOT非常简单，但是基本上很多面试官都喜欢问这个问题。这个问题主要就是一个可以考察一下你的Python 编程能力，其次他可以去问一下你 one hot的作用是什么，再者还可以往后问你 one hot有什么缺点，怎么解决等等。一个问题就可以问你半个小时。</p><h2 id="补充softmax-和-cross-entropy">补充：SOFTMAX 和CROSS-ENTROPY</h2><p>好，在本节课最后，我们来做一个前面课程的补充，在今天才想起来，有一个相关的点遗漏了没有讲到。</p><p>之前我们讲过逻辑回归的 loss 函数：</p><p>假如 y=1，loss 可以等于-log(yhat), 如果 y 等于 0，loss就可以写成-log(1-yhat)。两个合并后就组成了最终的 loss 函数：</p><p><span class="math display">\[loss = -(ylog\hat y + (1-y)log(1- \hat y))\]</span></p><p>那么，这个是解决二分类的，结果才不是 0 就是1。现在的问题就是如果我们要解决多分类的问题怎么办。</p><p>如果要解决多分类的话，需要把 x 变成一种能预测多分类的东西。那最终yhat 可以表示成 <span class="math inline">\(\hat y = (0.25, 0.20,0.75)\)</span>。</p><p>也就是，现在要表示三个类别，那我们可以用三个小数来表示。这个向量经过各种计算，如果能够变成一个三维的向量，然后再去优化里边的参数就可以做到。</p><p>那这也就代表的是类别 1、类别 2、类别 3 的概率。ytrue 就可以写成 yhat的形式，就变成 (1, 0, 0)。</p><p>就是我们给定一个<span class="math inline">\(\vec x\)</span>, 它实际的y 是 (1, 0, 0)，那么 yhat 就是估计值等于 0.25、0.20 和0.75。然后对比一下两组数据之间的差别，这样我们就可以优化其中的形成参数(w, b)。</p><p>通过不断优化，就可以计算到更接近于 (1, 0, 0) 这样的值。</p><p>首先就是怎么样把 x 向量变成 3 维的。</p><p>这个其实不难，如果 x 是 10 维的，1<em>10。那么给他再乘以一个 10</em>3的矩阵，它最后就会变成一个 1 行乘 3 列的矩阵。</p><p>那么现在假如说现在有这样一个 x:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = [<span class="hljs-number">1231</span>, <span class="hljs-number">12314</span>, <span class="hljs-number">4341</span>, <span class="hljs-number">1542</span>, <span class="hljs-number">4123</span>, <span class="hljs-number">4512</span>, <span class="hljs-number">3213</span>, <span class="hljs-number">1241</span>, <span class="hljs-number">1231</span>, <span class="hljs-number">6842</span>]<br></code></pre></td></tr></table></figure><p>然后我们来做这样一件事：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">x = np.array(normalize(x))<br>weights = np.random.random(size=(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>))<br>np.dot(x, weights)<br><br>---<br>array([<span class="hljs-number">0.86907231</span>, <span class="hljs-number">1.32234548</span>, <span class="hljs-number">0.88170994</span>])<br></code></pre></td></tr></table></figure><p>这样，我们就生成了一个维度是 3的一串数字。在机器学习里面，我们把这个叫做算子：logits。</p><p>现在我们将一个 10 维的 x 变成了一个 3 维的logit，下一步我们就要考虑，怎么将这个 logit 变成一个概率分布呢？</p><p>我们就要用到一个和逻辑函数特别像的一个函数，Softmax：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195940.png"alt="Alt text" /></p><p>我把它写出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">logits = np.dot(x, weights)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> np.exp(x) / np.<span class="hljs-built_in">sum</span>(np.exp(x))<br><br>ic(softmax(logits))<br><br>---<br>array([<span class="hljs-number">0.27884889</span>, <span class="hljs-number">0.43875588</span>, <span class="hljs-number">0.28239524</span>])<br></code></pre></td></tr></table></figure><p>这样，我们输入的是 logits，输入到 Softmax，输出的就是概率了。</p><p>输出成概率之后，我们定义一个依然和逻辑函数很像的一个函数，叫做Cross-entropy。</p><p>我们刚才使用 softmax 输出的数组就是概率，也就是估算的 yhat。这个Cross-entropy 的 loss 就是：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195941.png"alt="Alt text" /></p><p>求得 loss，然后再对 x 求偏导，就可以通过梯度下降让输入的 x得到和真正的 y 相近的 yhat。</p><p>那我们将 cross-entropy 也写一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy</span>(<span class="hljs-params">yhat, y</span>):<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(y_i * np.log(yhat_i) <span class="hljs-keyword">for</span> y_i, yhat_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(y, yhat))<br></code></pre></td></tr></table></figure><p>现在我们需要一组真正的y，也就是真实值，和我们预测房价时所使用的真实值是一样的东西，只是现在我们的y 的维度不太一样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">y = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p>接着我们使用<code>cross_entropy</code>将我们之前使用 softmax计算的概率分布和真实的 y 放进去：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">ic(cross_entropy(softmax(logits),y))<br><br>---<br><span class="hljs-number">1.040664959870481</span><br></code></pre></td></tr></table></figure><p>这个时候我们就得到了一个 loss 值。</p><p>我们现在去给 weights 求偏导。然后通过不断的迭代，就能找到一组 wi，和x 进行点乘就能够生成和 y 接近的值。</p><p>以上这些就是 softmax 和 cross-entropy 的作用。</p><p>cross-centropy 就是用来衡量产生的 yhat 和 y之间的相似程度差距的。Softmax是把任意的一组数字变成概率分布，然后这个概率分布就可以送到 loss函数里面和实际上的 y 进行对比。</p><p>Softmax 有这么几个特性，它的结果是一个典型的概率分布。还有就是Softmax 中有 e 的 n 次方，可以把 Max 变得更大。除了把 Max变得更大，还保留原来小的数字。</p><p>理论上完全可以找别的函数代替，计算机里边很多东西，只要好用就行。这就是放大特征，正是面对多分类任务的一个做法。</p><p>Softmax在实现的时候有个坑稍微要注意一下，在实现的时候我们多加一句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">x</span>):<br>    x = np.array(x)<br>    x -= np.<span class="hljs-built_in">max</span>(x) <span class="hljs-comment"># 多加这么两句</span><br>    <span class="hljs-keyword">return</span> np.exp(x) / np.<span class="hljs-built_in">sum</span>(np.exp(x))<br><br>ic(softmax(logits))<br></code></pre></td></tr></table></figure><p>首先，如果 x 的输入是一个 array就不用管了，但是如果不是，我们就要强制转换一下。</p><p>下一句代码是因为 e 的 x次方可能非常的大，但是我们计算机的存储是有限的，最大只能表示 2^63的数字，再大就表示不了了。所以我们就需要这样一段代码来处理一下，让最后结果的数字不要那么大。</p><p>好，那这一节课的内容到这里也就结束了。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231021195926.png&quot;
alt=&quot;茶桁的 AI 秘籍 核心基础 13&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;上一节课，咱们讲解了『拟合』，了解了什么是过拟合，什么是欠拟合。也说过，如果大家以后在工作中做的就是机器学习的相关事情，那么欠拟合和过拟合就会一直陪伴着你，这两者是相互冲突的。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>12. 机器学习 - 拟合</title>
    <link href="https://hivan.me/12.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%8B%9F%E5%90%88/"/>
    <id>https://hivan.me/12.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%8B%9F%E5%90%88/</id>
    <published>2023-10-22T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:57.697Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021148.png"alt="茶桁的 AI 秘籍 核心基础 12" /></p><p>Hi, 你好。我是茶桁。</p><p>这一节课一开始我们要说一个非常重要的概念：拟合。</p><span id="more"></span><h2 id="拟合">拟合</h2><p>相信只要你关注机器学习，那么多少在某些场合下都会听到拟合这个概念。</p><p>什么叫做拟合，什么叫做过拟合或者欠拟合呢？</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021056.png"alt="Alt text" /></p><p>假如有一个模型，这个模型在训练数据的时候效果很好，体现在 loss很小，或者说 precision 很高，accuracy也比较好，但是在实际情况下，用到没有见过的数据的时候，效果就很差，那么这个就过拟合了。</p><p>在这个过程中，要主一的是仅当数据 label 比较均衡的时候，才有必要使用acc.</p><p>我们来看三条曲线：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021057.png"alt="Alt text" /></p><p>第一个图是比较合理的参数模型，第二个图就是过拟合的参数模型。</p><p>因为对数据过度拟合，当有新的点出现的时候，函数的趋势和新的点并不匹配。那过度拟合就会对于未来的点就预测不对了。为了在训练的时候效果很好，在见过的数据里效果特别好，结果在新的未见过的数据里，效果就很差。</p><p>第三个图就是欠拟合的状态。训练的时候这个效果就不好，整个接近程度就不高。</p><p>比较好的场景就是第一张图的拟合状态，其形成了一个合理的参数模型。在训练的时候拟合也没有那么高，实际中的结果会发现结果也没那么差。这其实也就暗合了我们前几节课里所讲的[奥卡姆剃刀原理].</p><p>过拟合和欠拟合这两个概念，在我们平时的工作中会是每天都要一直取解决的问题。遇到一个问题，训练的时候效果很差这个欠拟合，经过了很多调试结果发现效果还不错，结果在实际问题中发现效果很差，这个就是过拟合。</p><p>这两件事情其实它是互相冲突的，这个可以通过 loss 来判断，也可以通过percision 来判断，只不过在计算新问题的时候，不存在 lose函数这回事儿。就是当你把模型已经训练完了，去用真实数据做测试了，那个时候是不存在loss 函数的。</p><p>在整个机器学习的发展历程中，我们一直在不断的做的事情就是怎么样提高欠拟合的准确度，同时降低过拟合。</p><p>影响过拟合和欠拟合原因有很多，既和数据有关系，也和模型有关系。但是在这个过程中有一点大家需要注意。所有的机器学习任务里边，在我们收集数据的时候，有一个很重要的问题就是异常值对过拟合和欠拟合影响会很大。</p><h2 id="outliner">OUTLINER</h2><p>有一本书就叫《Outliner》(异类), 大家有空可以去看一下。</p><p>outliner 为什么会对我们整个值影响很大呢？</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021058.png"alt="Alt text" /></p><p>咱们来看这个图，本来正常的线性走向应该是右边这张图。可是因为存在异常值的情况，所以导致线性偏向左边这张图的情况。可是在我们的数据中，这种极端的异常值属于少数，并且因为数值偏差过大，就导致整体趋势的偏斜。</p><p>那么我们怎么样去判断异常值呢？为了把模型做好，从一开始收集数据以及清洗数据的时候就要把那些异常值给它去掉。</p><p>所谓异常值是没有一个标准定义的，但是在数学上会有一个比较常见的去除方法，就是利用百分位，常见的方法就是按百分位来解决数值型问题。</p><p>numpy 里有一个<code>persontile</code>，它接受一个array，和一个浮点值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.percentile(np.array([]), number)<br></code></pre></td></tr></table></figure><p>那这个 percentile 是干嘛的呢？比如下面这张图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021059.png"alt="Alt text" /></p><p>把所人从左到右排排队，比方说第 80 分位，就是第 80%的是那一个。而我们如果是要处理数据的话，会让这些人按数值大小来排队，比如按身高，那就是最矮的在最左边。</p><p>一般统计学上常见的几个数字，一个是 0.5，还有 0.25 和 0.75。</p><p>比如我们之前的 lstat 数据，我们来找一下其中的异常值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>np.where(np.array(lstat) &lt; np.percentile(np.array(lstat), <span class="hljs-number">0.25</span>) / <span class="hljs-number">1.5</span>)<br><br>---<br>(array([], dtype=int64),)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">np.where(np.array(lstat) &gt; np.percentile(np.array(lstat), <span class="hljs-number">0.75</span>) * <span class="hljs-number">2.5</span>)<br><br>---<br>(array([  <span class="hljs-number">1</span>,   <span class="hljs-number">6</span>,   <span class="hljs-number">7</span>,   <span class="hljs-number">8</span>,   <span class="hljs-number">9</span>,  <span class="hljs-number">10</span>,  <span class="hljs-number">11</span>,  <span class="hljs-number">12</span>,  <span class="hljs-number">13</span>,  <span class="hljs-number">14</span>,  ...<br><span class="hljs-number">500</span>, <span class="hljs-number">501</span>, <span class="hljs-number">502</span>, <span class="hljs-number">505</span>]))<br></code></pre></td></tr></table></figure><p>在寻找极大值的时候，我们找到了一堆的数字。如果当异常值比较多的时候，很难把它们定义成异常值。我们总结异常值规律的时候，其实它的和周围的信状它很不一样。</p><h2 id="bias-and-variance">BIAS AND VARIANCE</h2><p>好，再接下来我们来一起看看 BIAS 和 VARIANCE。</p><p>在整个机器学习过程中，我们持续的有一个问题就是它的过拟合和欠拟合一直在互相PK。那么不管是欠拟合比较严重还是过拟合比较严重，这都是问题。但这两种问题在统计学里有两个名字。</p><blockquote><p>The bias is an error from erroneous assumptions in the learningalgorithm. High bias can cause an algorithm to miss the relevantrelations between feature and target outputs. (underfitting);</p></blockquote><p>我们把欠拟合这种问题叫做偏见，BIAS 叫做偏见。</p><p>假如说对于一个人来说，你对一件事情的判断判断错了，有BIAS，就是有偏见。这个就是你的脑子对这件事情的抽象程度不够，脑子的判断模型就错了。所以效果就不好。</p><p>就比方说分明是一个二次函数，你硬是要拿直线去怼出来，那你怎么怼？这就叫BIAS。</p><p>而 VARIANCE 是什么呢？它指的是你的那个变化太大。</p><blockquote><p>The variance is an error from sensitivity to small fluctuations inthe training set. High variance can cause an algorithm to model therandom noise in the training data, rather than the intended outputs(overfitting).</p></blockquote><p>也就是说，这个训练集对未来比较敏感，实际的值稍微有一点不一样就会产生很差的结果。高VARIANCE 最后会导致模型产生的结果都很随机，效果很差。</p><p>产生 VARIANCE 的背后有一个很重要的特性，就是模型复杂度。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021100.png"alt="Alt text" /></p><p>随着模型越来越复杂，它的 BIAS 会越来越低，就是模型越来越复杂。</p><p>就比方下面这个图中的模型：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021101.png"alt="Alt text" /></p><p>右边的这个曲线就是用了一个很复杂的模型，它的 BIAS 就会很低，BIAS低的时候它的 VARIANCE 就会变大。</p><p>因为模型复杂，所以之后问题稍微变化了，这个模型就会产生出来变化比较大的输出值。而模型越简单，BIAS就会越高，对于未来也会变化没有那么大。</p><p>这是一个非常重要的一个 Dilemma，是一个两难问题，进退两难的一个问题叫Dilemma。</p><p>这两类错误背后其实都是和我们的模型复杂程度有关。</p><p>那么讲到这里我们就可以来谈谈，BIAS 和VARIANCE，过拟合和欠拟合背后的原因有哪些。</p><table><thead><tr class="header"><th>过拟合 overfitting</th><th>欠拟合 Underfitting</th></tr></thead><tbody><tr class="odd"><td>训练数据占总体数据过少</td><td>训练数据占总体数据过多</td></tr><tr class="even"><td>模型过于复杂</td><td>模型过于简单</td></tr><tr class="odd"><td>采样过于不均衡</td><td></td></tr><tr class="even"><td>没有正则化...</td><td></td></tr></tbody></table><p>模型过于简单可能会产生欠拟合情况，模型过于复杂呢有可能会产生一个过拟合的情况。如果产生了过拟合，还有一个很重要的特点就是有可能模型采样非常不均衡。</p><p>模型复杂不复杂，单不简单，采样均衡不均衡。其实背后都有一个重点叫做『训练数据占总体数据的比例』，就是训练数据是不是够多。</p><p>所谓的采样均衡不均衡，异常值的出现最终都指向的是一个问题，就是我们的训练数据不够多。</p><p>为什么训练数据不够多会引起模型过于复杂之类的情况呢？在整个机器学习里是个非常重要的概念，叫做维度灾难。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021102.png"alt="Alt text" /></p><p>我们举例来说明下，就比如，在一个平面坐标轴上有几百个点，我们用两点去确定了一根直线，但是这根直线并不能代表这几百个点确定的走向，我们需要去确定更多的点来调整这根线。</p><p>但是如果是三维轴上，我们为了去确定一个平面，就需要更多的点，比二维轴上确定直线的点多的多。为了能更加精确地确定一个平面，需要更多的数据才行。</p><p>在机器学习中算是一个经验，当机器学习的维度每增加一个，那么所需要的样本量基本上要增加一个数量级。</p><p>举个例子，假如班上有一个老师，这个老师要预测同学能不能考上重点大学。假如现在是高一，还没有开始考试，他预测这个同学有这么几个情况：</p><ul><li>第一个情况是这个同学做作业的情况；</li><li>第二个情况是这个孩子每天上课时回答问题的活跃程度</li></ul><p>假如是这两个点，如果他靠这两个 features来预测这个孩子能不能考上大学，准确度能够做到不错，假如到百分之八九十，他需要50 个学生能够预测。</p><p>那么现在又加一个 features，这个 features还是和前两个值不相关的，又加了一个孩子的家庭收入情况。那么它得变成上百个学生数据才能预测对。因为每增加一个features，在这个世界中就会增加很多不确定的情况。</p><p>所以模型之所以过于复杂，其实背后本质上还是数据量太少。</p><p>就在我们今天大数据的情况下，模型其实进步的并没有非常大，但是数据量变大之后，整个的效果就好多了。在机器学习里有一个点就是算法再好，模型再好，抵不过不过数据量大。</p><p>真正工作、学习的时候，一定要想办法提前检测出来过拟合、欠拟合的情况。为了提前检测出这些，有一个很简单的方法。</p><p>假如现在给了许多的训练数据，不要把这训练数据全部拿上做完。而是选一部分，拿其中的一部分数据做训练，比方80%。然后，剩下 20% 不给模型去看，然后把模型拿到这 20%上去看一下结果。这样，我们就可以测试出结果。这就叫做训练集和数据集。</p><p>为什么要有训练集和测试集呢？有一个非常极端的情况，如果不做训练集和数据集想获得好的结果，直接把所有label 对应的值记下来，也就是小时候我们背诵古诗散文。</p><p>你的模型只会阅读并背诵全文，那想想训练的时候，acc、precision、recall等评测指标就非常高，但是效果就很差，过拟合的情况就会很严重。</p><p>在实际的工作中，除了<code>train_set</code>和<code>test_set</code>，还有一个值叫<code>validation_set</code>。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021103.png"alt="Alt text" /></p><p>早些年的时候，做测试数据集只会分成训练集和测试集。然后在训练集上去训练，完事在测试集上去测试。但是人们发现一个情况，比方说在测试集上训练完了之后，在测试集上发现效果不太好，过拟合有点严重，于是分析test 数据哪里做错了，找到错误之后修改代码或者修改数据。</p><p>就好比一个同学做题的时候有十套卷子，他做完 8套，留了两套。再做这两套之后发现哪里不太对，然后反反复复去观察后面这两套，也就是我们的test数据。这个时候其实是在做针对性的调整，在有针对性的解决问题，整体的能力并没有提升。</p><p>为了解决这个问题，在实际的工作中我们会把数据集分成三个数据集。</p><p>训练数据集不断的去训练，然后结果给到 validation set这样一个小数据集里。我们去观察 validation 的结果，再去调整，当validation的结果很不错的时候我们拿一套完全没做过的题来检验。这个完全没见过的题就是我们test set 了。</p><p>当 test set用过之后，如果考试成绩太差，那只能把数据集打乱，再重新取一份新的 test数据了。因为这个时候如果再去有针对性的调整模型结果，那其实是在手动过拟合了。</p><p>在这个过程中，假设一共有 100 个数据，test 里有 20 个，validation 有10 个，train 里有 70个。为了尽可能多的把所有的数据都用上，把它的效率都发挥上，有一个很简单的操作：crossvalidation，也叫做交叉验证。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021104.png"alt="Alt text" /></p><p>如图，假如我把数据分成很多份，我让其中一部分做 validation数据集，下一次训练的时候，我再让另外一部分做 validation 数据集。</p><p>再回过头来看我们之前见过的机器学习的通用框架，这几节课学习了评价指标之后，我们就应该知道，在这个背后多了一个acc 和 precision，我们要持续的去观测它的结果。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021105.png"alt="Alt text" /></p><p>在这个过程中有了数据，然后定义一个𝜃，这个𝜃就是我们的参数。然后根据loss function，gradient descent 不断优化这个参数。结果并不是要一个低的loss 参数，是期望有一个好的 acc 和precision。这个是在之前的基础上完善的整个学习过程。</p><p>好，下节课呢，咱们来看看 FEATURE SCALING，特征缩放。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231020021148.png&quot;
alt=&quot;茶桁的 AI 秘籍 核心基础 12&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi, 你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;这一节课一开始我们要说一个非常重要的概念：拟合。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>11. 机器学习 - 评价指标 2</title>
    <link href="https://hivan.me/11.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%872/"/>
    <id>https://hivan.me/11.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%872/</id>
    <published>2023-10-19T23:30:00.000Z</published>
    <updated>2023-11-25T04:41:28.659Z</updated>
    
    <content type="html"><![CDATA[<p>Hi, 你好。我是茶桁。</p><p>上一节课，咱们讲到了评测指标，并且在文章的最后提到了一个矩阵，我们就从这里开始。</p><span id="more"></span><h2 id="混淆矩阵">混淆矩阵</h2><p>在我们实际的工作中，会有一个矩阵，这个矩阵是分析结果常用的。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231019125248.png"alt="Image 2023-10-18 192838.png" /></p><p>我们来看看具体是什么意思。</p><p>所谓的<code>True condition</code>,指的是真实值，<code>Predicted condition</code>，指的是预测值。</p><p>其中行表示，<code>Predicted condition positive</code>表示预测值是1，<code>Predicted condition negative</code>表示预测值是 0。</p><p>列表示则为：<code>Condition positive</code>表示真实值是 1，<code>Condition negative</code>表示真实值是 0。</p><p>这样行列交叉就组成了这样一个矩阵。这个矩阵叫做混淆矩阵，英文名字叫做Confusion Matrix.</p><p>这个混淆矩阵是什么意思呢？</p><p><code>True Positive</code> 意思就是预测值是1，预测对了，<code>True negative</code>意思是预测值是0，预测对了。那相对的， <code>False positive</code>意思就是预测值是1，预测错了， <code>False negative</code>意思就是预测值是0，预测错了。</p><p>混淆矩阵在常见的机器学习里边是一个很重要的分析工具：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix<br>confusion_matrix(true_labels, predicated_labels)<br><br>—<br>array([[<span class="hljs-number">59</span>,  <span class="hljs-number">6</span>],<br>       [ <span class="hljs-number">6</span>, <span class="hljs-number">29</span>]])<br></code></pre></td></tr></table></figure><p>我们可以直接看看这个方法的源码里有相关说明：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">??confusion_matrix<br><br>---<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">confusion_matrix</span>(<span class="hljs-params"></span><br><span class="hljs-params">    ...</span><br><span class="hljs-params">    the count of true negatives <span class="hljs-keyword">is</span> :math:`C_&#123;<span class="hljs-number">0</span>,<span class="hljs-number">0</span>&#125;`, </span><br><span class="hljs-params">    false negatives <span class="hljs-keyword">is</span> :math:`C_&#123;<span class="hljs-number">1</span>,<span class="hljs-number">0</span>&#125;`, </span><br><span class="hljs-params">    true positives <span class="hljs-keyword">is</span> :math:`C_&#123;<span class="hljs-number">1</span>,<span class="hljs-number">1</span>&#125;` </span><br><span class="hljs-params">    false positives <span class="hljs-keyword">is</span> :math:`C_&#123;<span class="hljs-number">0</span>,<span class="hljs-number">1</span>&#125;`.</span><br><span class="hljs-params">    ...</span><br></code></pre></td></tr></table></figure><p>tp 实际上是<code>1</code>预测值是<code>1</code>，tn实际是<code>0</code>预测是<code>0</code>, fp实际是<code>0</code>预测是<code>1</code> fn实际是<code>1</code>预测是<code>0</code>。</p><p>這個時候我們再回頭來看上节课结尾处的那个公式：</p><p><span class="math display">\[\begin{align*}Precision &amp; = \frac{tp}{ tp + fp} \\Recall &amp; = \frac{tp}{tp + fn}\end{align*}\]</span></p><p>很多人看到这个就有点晕，其实很简单。切换成我们刚才查看源码时查询到的就就成了这样：</p><p><span class="math display">\[\begin{align*}Precision &amp; = \frac{C(0, 0)}{ C(0, 0) + C(1, 0)} \\Recall &amp; = \frac{C(0, 0)}{C(0, 0)+ C(0, 1)}\end{align*}\]</span></p><p>tp 是实际上是 positive, 预测也是 positive. fp 就是实际上并不是positive，但是预测的值是 positive. 那么 tp+fp 就是所有预测为 positive的值。所以 precision 就是预测对的 positive 比上所有预测的 positive.</p><p>fn 指的是实际上是 positive, 但是预测值并不是 positive 的值。所以tp+fn 就是所有实际的 positive 值，recall 就是预测对的 positive比上所有实际的 positive 值。</p><p>我们这样对比着矩阵和公式来理解 Precision 和 Recall是不是就清晰了很多？这就是 position 和 recall根据混淆矩阵的一种定义方式。</p><p>刚刚讲了 baseline, baseline是在做评估的时候要知道结果一定要比什么好才行。如果是个二分类问题，基本上是一半一半，准确度是50%, 那基本上就没用。</p><p>Precision 和 recall这两个是针对于分类问题进行评价，那我们怎么解决回归问题的评价呢？</p><p>回归问题，它也有一个 accuracy 如下：</p><p><span class="math display">\[acc(y, \hat y) = \sum_{i \in N}|y_i - \hat y_i|  \\acc(y, \hat y) = \sum_{i \in N}|y_i - \hat y_i|^2 \\acc(y, \hat y) = \sum_{i \in N} \frac{|y_i - \hat{y_i}|}{|y_i|}\]</span></p><p>除此之外，regression问题里面有一个比较重要的评价方式叫做<code>R2-scoree</code>:</p><p><span class="math display">\[R^2(y, \hat y) = 1 - \frac{\sum_{i=1}^n(y_i - \haty_i)^2}{\sum_{i=1}^n(y_i - \bar y)^2}\]</span></p><ul><li>第一种情况：如果所有的 y_i 和 yhat_i 的值都相等，那么 R2(y, yhat) =1</li><li>第二种情况：如果所有的 yhat_i 是 y_i 的平均值，那么 R2(y, yhat) =0</li><li>第三种情况：如果 R2 的值比 0还小，就意味着它还不如我们做统计求平均值，瞎猜的结果。也就是连 baseline都没达到。</li></ul><p>R2-scoree之所以常常会被用于进行回归问题的评测，主要的原因就是它防止了机器作弊。</p><p>比方说我们现在有一组数据，这组数据实际都是 0.99, 0.97, 0.98...,这些数字都很小，而且都很密集。那么给机器使用的时候随便做一个平均值，感觉到准确度还挺高，那就被骗了。</p><h2 id="f-score">F-score</h2><p>在 precision 和 recall 之外，还有一个比较重要的内容，叫做F-score.</p><p>首先我们要知道，precision 和 recall这两个值在实际工作中往往是相互冲突的。为了做个均衡，就有了 F-score.</p><p><span class="math display">\[\begin{align*}F-score &amp; = \frac{(1+\beta^2) * precision \times recall}{\beta^2 *precision + recall}\end{align*}\]</span></p><p><spanclass="math inline">\(\beta\)</span>是自行定义的参数，由这个式子可见F-score 能同时考虑 precision 和 recall 这两种数值。分子为 precision 和recall 相乘，根据式子，只要 precision 或 recall 趋近于 0，F-score就会趋近于 0，代表着这个算法的精确度非常低。一个好的算法，最好能够平衡recall 和precision，且尽量让两种指标都很高。所以有一套判断方式可以同时考虑 recall和 precision。当<span class="math inline">\(\beta \to 0\)</span>,F-score 就会退化为 precision, 反之，当<span class="math inline">\(\beta\to \infty\)</span>, F-socre 就会退化为 recall.</p><p>我们一般说起来，F-score 没有特别定义的话，就是说<spanclass="math inline">\(\beta\)</span>为 1, 一般我们写成 F1-score.</p><p><span class="math display">\[\begin{align*}F1-score &amp; = 2 \times \frac{precision \times recall}{precision +recall}\end{align*}\]</span></p><p>F1-score 是仅当 precision 和 recall 都为 1 的时候，其值才等于 1.而如果这两个值中任意一个不为 1 时，其值都不能等于 1. 也就是说，当 2*1/2= 1 时，F1-score=100%, 代表该算法有着最佳的精确度。</p><h2 id="auc-roc">AUC-ROC</h2><p>除了 F-score 之外，还有比较重要的一个概念：AUC-ROC.这个也是为了解决样本不均衡提出来的一个解决方案。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231019125249.png"alt="Alt text" /></p><p>首先我们要先了解 ROC 曲线 (receiveroperating characteristic), ROC曲线上的每一个点反映着对同一信号刺激的感受。AOC(Area under Curve), 是ROC 曲线下的面积，取值是在 0.1 ~ 1 之间。</p><p>我们直接来看看，它在实际场景下是怎么用的。</p><p>还记得咱们在之前设定的阈值<code>decision_boundary = 0.5</code>,我们就拿这个阈值来看。<code>threshold:0.5</code>.在我们二分类问题中，当预测值大于 0.5 的时候，也就等于 1了。也就是说，只要超过 0.5, 我们就判定为 positive 值。</p><p>好，现在还是的请我们劳烦了无数次的警察 a 同志来帮帮我们。当警察 a去抓罪犯的时候，盘但一个人是不是犯了罪，他的决策很重要。在事实清晰之前，警察a 的决策只有超过 0.5 的时候，才能判定这个人是positive，也就是罪犯。这个时候呢，我们假设 precision 是 0.7.</p><p>现在又需要警察 b 出场了，这个警察 b 的 threshold 为 0.1 的时候，其precision 就为 0.7. 也就是说，他预计出的值，只要大于 0.1, 就判定为positive, 这种情况下，警察 b 判定的 precision 为 0.7.</p><p>别急，这次需要的演员有点多，所以，警察 c 登场了。那么警察 c 的threshold 为 0.9. 也就是说，警察 c比较谨慎，只有非常确定的时候，才能判定 positive. 警察 c 的情况，判定的precision 也是 0.7.</p><p>好，现在我们来用脑子思考下，这三个警察哪个警察能力最强？</p><p>必须是警察 b 最厉害。</p><p>就如我们上面的那四个坐标轴，X 轴代表 threshold, Y 轴表实 positive, 当threshold 轴上的取值还很小的时候，positive 已经很大了。那明显紫色线条和threshold 轴圈住的区域面积越大，这个面积就是越大越好。</p><p>这就是 AUC for ROC curves,这个主要就是为了解决那些样本及其不均衡的问题。因为样本非常不均衡的时候，position和 recall 你有可能都会很低，这个时候就不好对比。AUC曲线对于这种情况就比较好用一些。</p><p>其实在真实情况下，绝大多数问题都不是很均衡的问题。比方说预测病，找消费者，找高潜力用户。换句话说，如果高潜用户多就不用找了。</p><p>我们在研究 ROC 曲线实际应用的时候，依然会用到上面给大家所讲的 tp, fp,fn, tn. 这里会引出另外两个东西，TPR 和 FPR, 如下：</p><p><span class="math display">\[\begin{align*}TPR &amp; = \frac{tp}{tp+fn} \\FPR &amp; = \frac{fp}{fp+tn}\end{align*}\]</span></p><p>我们来看看咱们之前的这组数据的 AUC 值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_curve, auc<br><br>fpr, tpr, thresholds = roc_curve(true_labels, losses)<br><br>roc_auc = auc(fpr, tpr)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;AUC: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(roc_auc))<br><br>---<br>AUC: <span class="hljs-number">0.9300356506238858</span><br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231019140016.png"alt="Alt text" /></p><p>下一节课，咱们来说一个非常重要的概念：拟合和欠拟合。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hi, 你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;上一节课，咱们讲到了评测指标，并且在文章的最后提到了一个矩阵，我们就从这里开始。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>10. 机器学习 - 评测指标</title>
    <link href="https://hivan.me/10.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/"/>
    <id>https://hivan.me/10.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/</id>
    <published>2023-10-17T23:30:00.000Z</published>
    <updated>2023-11-25T04:41:24.500Z</updated>
    
    <content type="html"><![CDATA[<p>Hi，你好。我是茶桁。</p><p>之前的课程中，我们学习了两个最重要的回归方法，一个线性回归，一个逻辑回归。也讲解了为什么学习机器学习要从逻辑回归和线性回归讲起。因为我们在解决问题的时候，有限选择简单的假设，越复杂的模型出错的概率也就越高。</p><span id="more"></span><p>本节课中，我们要继续我们未完成的内容。</p><p>还记得，咱们上一节课中最后所说的吗？在完成了基本回归之后，该如何去判断一个模型的好坏，以及如何调整和优化。</p><p>好，我们开始本节课程。</p><h2 id="pickle">PICKLE</h2><p>本节课中，会重点的给大家做一件事，叫「评测指标」。</p><p>在这之前，我们发现了一个麻烦事。就是我们现在需要去观测我们的分类结果，我们不得不再去执行一遍我们之前的训练程序，拿到最后的分类结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">RM:<span class="hljs-number">6.38</span>, LSTAT:<span class="hljs-number">24.08</span>, EXPENSIVE:<span class="hljs-number">0</span>, Predicated:<span class="hljs-number">0</span><br>...<br>RM:<span class="hljs-number">6.319</span>, LSTAT:<span class="hljs-number">11.1</span>, EXPENSIVE:<span class="hljs-number">1</span>, Predicated:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>这很麻烦，训练结果每次要使用的时候都需要运行一次，这样非常的麻烦。现在我想要把这个model不要每一次都训练一下，而是要把它做一个保存，下次用的时候不需要从头到尾再训练一次。</p><p>现在现在，可以给他做一个persistence，做一个留存。现在就是要做这么一件事情。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pickle<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;logistic_regression.model&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    pickle.dump(model, f)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;w.model&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    pickle.dump(w, f)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;b.model&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    pickle.dump(b, f)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;pickle finished&#x27;</span>)<br><br>---<br>pickle finished<br></code></pre></td></tr></table></figure><p>并且最后我得到了三个文件，分别是<code>logistic_regression.model</code>,<code>w.model</code>以及<code>b.model</code>。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231017042848.png"alt="image-20231016020658120" /></p><p>现在就可以把训练完成的 model 做保存了。之后我们用 Pytorch, tenserflow之类的做，它都有这样的功能。</p><p>到这一步之后，我们上一节上所写的代码就可以暂时不用了。不过为了整个代码的完整性，我仍然将其又在本节课的<code>10.ipynb</code>内些了一遍。</p><p>那么，我们要用的时候怎么办呢？如果要用这个对象的时候，将我们之前对文件操作的代码拿过来，然后将其中的<code>wb</code>参数改成<code>rb</code>，然后再将二进制文件读取一遍：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;logistic_regression.model&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    model_r = pickle.load(f)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;w.model&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    w_r = pickle.load(f)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;b.model&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    b_r = pickle.load(f)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;pickle read finished&#x27;</span>)<br></code></pre></td></tr></table></figure><p><code>rb</code>的意思是<code>read binary</code>，也就是读取二进制文件。然后，为了在测试的时候避免混乱，让我接下来所使用的文件使用的是我重新读取的模型而不是之前训练时生成的的，我将重新读取的这几个文件命名为<code>model_r</code>，<code>w_r</code>,<code>b_r</code>。</p><p>那再之后，虽然不用重新训练了，但是数据还是要读取一遍的，并且，按照训练数据的规则重新整理好，都完善了之后，就可以开搞进行分类了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_openml<br><br>dataset = fetch_openml(name=<span class="hljs-string">&#x27;boston&#x27;</span>, version=<span class="hljs-number">1</span>, as_frame=<span class="hljs-literal">True</span>, return_X_y=<span class="hljs-literal">False</span>, parser=<span class="hljs-string">&#x27;pandas&#x27;</span>)<br><br>data = dataset[<span class="hljs-string">&#x27;data&#x27;</span>]<br>target = dataset[<span class="hljs-string">&#x27;target&#x27;</span>]<br><br>dataframe = pd.DataFrame(data)<br><br>rm = dataframe[<span class="hljs-string">&#x27;RM&#x27;</span>]<br>lstat = dataframe[<span class="hljs-string">&#x27;LSTAT&#x27;</span>]<br>dataframe[<span class="hljs-string">&#x27;price&#x27;</span>] = dataset[<span class="hljs-string">&#x27;target&#x27;</span>]<br><br>greater_then_most = np.percentile(dataframe[<span class="hljs-string">&#x27;price&#x27;</span>], <span class="hljs-number">66</span>)<br>dataframe[<span class="hljs-string">&#x27;expensive&#x27;</span>] = dataframe[<span class="hljs-string">&#x27;price&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> p: <span class="hljs-built_in">int</span>(p &gt; greater_then_most))<br><br><br>expensive = dataframe[<span class="hljs-string">&#x27;expensive&#x27;</span>]<br>random_test_indices = np.random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm)), size=<span class="hljs-number">100</span>)<br>decision_boundary = <span class="hljs-number">0.5</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> random_test_indices:<br>    x1, x2, y = rm[i], lstat[i], expensive[i]<br>    predicate = model_r(np.array([x1, x2]), w_r, b_r)<br>    predicate_label = <span class="hljs-built_in">int</span>(predicate &gt; decision_boundary)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;RM:&#123;&#125;, LSTAT:&#123;&#125;, EXPENSIVE:&#123;&#125;, Predicated:&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(x1, x2, y, predicate_label))<br></code></pre></td></tr></table></figure><h2 id="评测指标">评测指标</h2><p>好，解决了模型的重复使用之后，我们再回到课程中继续。</p><p>很多人在学习过程中，会觉得「评测指标」是一个没有那么有趣的事情。比方说，咱们学模型，学算法，就可以去写程序，可以运行，写出来的时候会感觉还蛮酷的。但是评测指标呢，很多同学就觉得不是那么有趣。</p><p>其实，我想告诉大家，评测指标是一个非常重要的东西。好比完成任何一个任务，不管你现在是完成普通的编程任务，还是要完成一个公司的市场行为、运营行为。一般来说，越复杂的任务，只要把评价指标，评价方式做对，这个任务基本上就已经完成了一半了。</p><p>对于我们来说，工作的时候要知道，对于一个机器学习任务，能找到正确的评测指标，这个机器学习任务就已经成功一半了。</p><p>首先，来看一个问题：Losses 持续下降，到底是意味着什么呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>%matplotlib inline<br>plt.plot(losses)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231017042849.png"alt="image-20231016230405058" /></p><p>loss持续下降意味着误差越来越小？方向是对的？测试值更加接近真实值？更精确的说法是，它在逼近最优解，但是效果是不是特别好，还不知道。</p><p>接下来这个问题是一个比较复杂的问题，是一个难点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">-np.<span class="hljs-built_in">sum</span>(y * np.log(yhat) + (<span class="hljs-number">1</span> - y) * np.log(<span class="hljs-number">1</span> - yhat))<br></code></pre></td></tr></table></figure><p>这段代码是我们写的 loss 函数，我们现在来假设有一组数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">true_label = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]) <span class="hljs-comment"># 二分类</span><br></code></pre></td></tr></table></figure><p>再假设有一个模型，在执行的时候，它会知道咱们做的是一个二分类问题，那么结果就是不是1，就是0。这个时候模型有可能偷懒，那给到的数据就会是随机的，好吧，开个玩笑，其实就只是因为数据不足造成给到的数据过于随机：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">predicate_1 = np.array([<span class="hljs-number">0.8</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.8</span>])<br></code></pre></td></tr></table></figure><p>然后我们执行算法来拿到结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_lose</span>(<span class="hljs-params">y, yhat</span>):<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(y * np.log(yhat) + (<span class="hljs-number">1</span> - y) * np.log(<span class="hljs-number">1</span> - yhat))<br>test_lose(true_label, predicate_1)<br><br>---<br><span class="hljs-number">2.2300784022072975</span><br></code></pre></td></tr></table></figure><p>现在我们拿到的值为2.23，不过要记得，咱们这只是一个假设值。那这个时候引入我们刚才谈到的loss 的曲线，loss 是持续下降的，当它下降到最低的值的时候依然比这个 2.23还要高，那就说明这个模型都还没有随机猜测的准确度高。</p><p>这个情况其实是经常会遇到的一个问题，你会看到你的的模型一直在下降，下降的非常好，但是一做实际测试的时候效果就特别差。</p><p>再换个说法就是，这个模型跑的时候，瞎猜的值都有 2.23 的准确，但是 loss虽然一只在下降，一只下降到了 3。虽然 loss看起来在下降，但是这整个结果都不是太好。</p><p>瞎猜的时候的准确度，loss 值，我们称为这个模型的Baseline。你的值最起码要比这个好。</p><p>所以就如之前所的，loss持续下降意味着模型在向着最优的方向在寻找，但并不意味着结果就会很好，因为有可能连瞎猜都不如。</p><p>好，以上是第一点，我们接着来看第二点。</p><p>loss 一直在下降，但是我们现在想知道的是有多少个 label预测对了。先建立两个变量来分别存储数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">true_labels, predicated_labels = [], []<br><br>...<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> random_test_indices:<br>    ...<br><br>    true_labels.append(y)<br>    predicated_labels.append(predicate_label)<br></code></pre></td></tr></table></figure><p>然后分别获得了两组数据，一个是<code>true_labels</code>，一个是<code>predicated_labels</code>。有了这两组数据之后，我们来定义一个<code>accuracy</code>，这个是预测的值和相似的值一共有多少个是一样的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">accuracy</span>(<span class="hljs-params">ytrues, ylabels</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> yt, y1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(ytrues, ylabels) <span class="hljs-keyword">if</span> yt == y1) / <span class="hljs-built_in">len</span>(ytrues)<br><br>accuracy(true_labels, precicated_labels)<br><br>---<br><span class="hljs-number">0.89</span><br></code></pre></td></tr></table></figure><p>0.89, 就是说有 89% 的 label 都是猜对了。</p><p>最早的时候其实只有这一个标记，但这个标记很容易出错。</p><p>假设有一个警察局，要在 100 个人里边判断谁是犯罪分子。现在我们知道有 3个是犯罪分子，然后警察说这 100个人全部都是犯罪分子。那么现在准确度有多少？</p><p>然后又有一个警察站出来说，这 100个人都不是犯罪分子，那他的准确度又是多少？</p><p>我们现在让第一个警察是 a，第二个警察是 b。</p><p>警察 b 有 97个标签都说对了，这会给人一种错觉，好像他预测的很准确的。但是其实，a 和 b两个人都判断的不准确。那我们这个时候就需要引出一个定义：Precision。</p><p>precision 也是准确度的意思，和 accuracy 不同点是，accuracy的对比是对比目标和现有值是否匹配，匹配的就算正确。而 precision除了看是否匹配之外，还要目标值，也就是 positive。</p><p>这里举个例子说明一下，比如我们去检测是否有新冠病毒，那么目标是为了检测出有新馆病毒的人，那么呈阳性的人就是我们的positive，那么我们 precision除了预测出有新冠和没有新冠的人之外，有新冠的人也需要一一对应上，也就是positive 要正确。</p><p>如果是写代码的话，也就是将之前的 accuracy拿过来改改就可以直接用了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">precision</span>(<span class="hljs-params">ytrues, yhats</span>):<br>    <span class="hljs-comment"># 预测标签是 1 的里面，正确的比例是多少</span><br><br>    positives_pred = [y <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> yhats <span class="hljs-keyword">if</span> y == <span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> yt, y <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(ytrues, yhats) <span class="hljs-keyword">if</span> yt == y <span class="hljs-keyword">and</span> y == <span class="hljs-number">1</span>) / <span class="hljs-built_in">len</span>(positives_pred)<br><br>precision(true_labels, predicated_labels)<br><br>---<br><span class="hljs-number">0.8333333333333334</span><br></code></pre></td></tr></table></figure><p>先将预测为1，也就是预测呈阳性的目标放到<code>positives_pred</code>中，再来检测一下在这些预测出来的目标中，预测对的有多少。</p><p>除此之外之外，还有一个值叫做<code>recall</code>，它的意思是在实际的<code>positive</code>里，有多少比例被找到了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">recall</span>(<span class="hljs-params">ytrues, yhats</span>):<br>    <br>    true_positive = [y <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> ytrues <span class="hljs-keyword">if</span> y == <span class="hljs-number">1</span>]     <br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> yt, y <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(ytrues, yhats) <span class="hljs-keyword">if</span> yt == y <span class="hljs-keyword">and</span> yt == <span class="hljs-number">1</span>) / <span class="hljs-built_in">len</span>(true_positive)<br><br>recall(true_labels, predicated_labels)<br><br>---<br><span class="hljs-number">0.8064516129032258</span><br></code></pre></td></tr></table></figure><p>好，我们再来复盘一下这三个值，一个是<code>accuracy</code>,一个是<code>precision</code>，一个是<code>recall</code>。</p><p><code>accuracy</code>就是预测值和实际值有多少是一样的。但是有可能会在实际场景都不是很均衡。</p><p><code>precision</code>是拿到预测后的目标值，然后拿这些目标的实际值去比较看有多大比例是一样的。</p><p><code>recall</code>是先拿到实际的目标值，然后拿目标预测值比较看有多大比例是一样的。</p><p>根据我们之前说的警察抓坏人的那个假设，我们现在来做一个测试，假设我们现在好人有90 个，坏人有 10 个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">people = [<span class="hljs-number">0</span>] * <span class="hljs-number">90</span> + [<span class="hljs-number">1</span>] * <span class="hljs-number">10</span><br><span class="hljs-keyword">import</span> random<br>random.shuffle(people)<br></code></pre></td></tr></table></figure><p>现在警察 a来了，就判断说：全部都是好人，把他们全部都放了吧。这样的话，它的accuracy 是多少呢？accuracy 就是预测的，只要是实际值的那个 label就行。我们来看看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">a = [<span class="hljs-number">0</span>] * <span class="hljs-number">100</span><br>accuracy(people, a)<br><br>---<br><span class="hljs-number">0.9</span><br></code></pre></td></tr></table></figure><p>我们看这个准确度就会很高，这个也能理解，因为警察 a 将这 100 个人中的90 个好人全部判断准确了对吧？</p><p>让我们来看看其他两个：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">precision(people, a)<br><br>---<br>ZeroDivisionError: division by zero<br><br>======<br>recall(people, a)<br><br>---<br><span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>precision 警告我们分母为 0，报错了。那分母为什么为 0 呢？因为 a说了，所有都是好人，那么预测的目标值，也就是分母上的坏人就为 0。</p><p>而 recall 呢，结果为 0。这是因为分母上的坏人实际值虽然为10，但是预测的目标值，也就是分子上为 0。那结果肯定是为 0。</p><p>本来 a 的 accuracy 是0.9，别人还以为准确度很高，结果一个坏人都没抓住。这肯定不行。</p><p>那 b 的情况又如何呢？之前说过，b 说所有的都是坏人，统统抓起来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">b = [<span class="hljs-number">1</span>] * <span class="hljs-number">100</span><br>accuracy(people, b)<br><br>---<br><span class="hljs-number">0.1</span><br><br>========<br>precision(people, b)<br><br>---<br><span class="hljs-number">0.1</span><br><br>=========<br>recall(people, b)<br><br>---<br><span class="hljs-number">1.0</span><br></code></pre></td></tr></table></figure><p>虽然<code>accuracy</code>和<code>precision</code>都不高，但是似乎目标都被找出来了。颇有一种「宁可错杀1000，不可放过一个」的感觉。</p><p>那以上这些，就是为什么要有这 3 个非常重要的指标的原因。</p><p>好，那下一节课中，我们要来看看关于<code>precition</code>和<code>recall</code>的一个矩阵，这个矩阵呢，将会是我们工作中分析结果常用的。</p><p><span class="math display">\[\begin{align*}Precision &amp; = \frac{tp}{ tp + fp} \\Recall &amp; = \frac{tp}{tp + fn}\end{align*}\]</span></p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;之前的课程中，我们学习了两个最重要的回归方法，一个线性回归，一个逻辑回归。也讲解了为什么学习机器学习要从逻辑回归和线性回归讲起。因为我们在解决问题的时候，有限选择简单的假设，越复杂的模型出错的概率也就越高。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Python 批量修改文件名</title>
    <link href="https://hivan.me/Python-change-file-names-in-batches/"/>
    <id>https://hivan.me/Python-change-file-names-in-batches/</id>
    <published>2023-10-17T09:44:13.000Z</published>
    <updated>2023-10-17T09:45:14.997Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>仅个人需求，有需要的可以自取。</p></blockquote><p>前段时间为家里孩子下载了一批课程，但是文件命名就很奇怪也很乱，就想着将文件名修改掉便于查看。</p><span id="more"></span><p>这批视频下载下来后前边都给了诸如<code>001</code>之类的编号，当然是序列。可是这批序列又非常的乱，比如，「数列」和「导数」给的是考前的编号，而课本上要先学习的「集合」，「逻辑」，「不等式」等又编号又很靠后。不仅如此，就算是同一部分，其中的编号也是混乱的。</p><p><img src="https://raw.githubusercontent.com/hivandu/notes/main/img/20231017173523.png" alt="image-20231017172008515" /></p><p>那么就有了批量修改文件名的需求，当然我第一时间想到的是<code>Better rename</code>，已经是一个很古老的版本了：</p><p><img src="https://files.mdnice.com/user/43981/baf01bcb-fdf3-4150-97df-b99ed3b4a375.png" /></p><p>可是当我使用的时候才发现并不能满足我的个人需求，也许是我不太会用吧。起码，我是想删掉开头的那些序列以及其中重复不必要的内容。但是这玩意并不能支持正则或者相关的功能。没办法，眼见有几种方式去做，一种是Mac自带的「自动操作」，一种是「捷径」，还有就是干脆用Python写个脚本。</p><p>所以，我使用了自己觉得最简便的方式，写了这样一个脚本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> tkinter <span class="hljs-keyword">as</span> tk<br><span class="hljs-keyword">from</span> tkinter <span class="hljs-keyword">import</span> filedialog<br><span class="hljs-keyword">import</span> re<br><br>root = tk.Tk()<br>root.withdraw()<br><br>folderPath = filedialog.askdirectory() <span class="hljs-comment"># 获得选择好的文件夹</span><br><span class="hljs-comment"># filePath = filedialog.askopenfilename() # 获得选择好的文件</span><br><br><span class="hljs-built_in">print</span>(folderPath)<br><span class="hljs-comment"># print(filePath)</span><br><br>files = os.listdir(folderPath)<br><br>fileList = []<br>newFileList = []<br><br>oldStr = <span class="hljs-built_in">input</span>(<span class="hljs-string">&#x27;请输入要修改的内容或正则:&#x27;</span>)<br>newStr = <span class="hljs-built_in">input</span>(<span class="hljs-string">&#x27;请输入要替换的内容，不修改只删除可留空:&#x27;</span>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> files:<br>    <span class="hljs-keyword">if</span> i[<span class="hljs-number">0</span>] != <span class="hljs-string">&#x27;.&#x27;</span>:<br>        portion = os.path.splitext(i)<br>        newname = re.sub(<span class="hljs-string">r&#x27;&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(oldStr), <span class="hljs-string">r&#x27;&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(newStr), portion[<span class="hljs-number">0</span>])<br>        <span class="hljs-comment"># os.chdir(folderPath) # 测试完毕后要正式修改文件名取消这里注释</span><br>        <span class="hljs-comment"># os.rename(portion[0]+portion[1], newname+portion[1]) # 测试完毕后要正式修改文件名取消这里注释</span><br>        <br>        fileList.append(portion[<span class="hljs-number">0</span>]+portion[<span class="hljs-number">1</span>])<br>        newFileList.append(newname+portion[<span class="hljs-number">1</span>])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;修改前，一共有&#123;&#125;个文件\n  &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(fileList), fileList))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;修改后，一共有&#123;&#125;个文件\n  &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(newFileList), newFileList))<br></code></pre></td></tr></table></figure><p>有需要的小伙伴可以自取了。代码执行后会让你选取你要修改的文件的目录，然后会让你输入你要修改的内容，可以是正则，然后输入你要修改成的内容。</p><p><img src="https://raw.githubusercontent.com/hivandu/notes/main/img/20231017173524.png" alt="image-20231017173302128" /></p><p>比如，我需要修改标题：</p><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-comment"># 输入需要替换的内容</span><br><span class="hljs-string">\d&#123;3&#125;</span> - <br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/hivandu/notes/main/img/20231017173525.png" alt="image-20231017173338085" /></p><p>要替换的内容我直接留空回车，打印结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">修改前，一共有<span class="hljs-number">16</span>个文件<br>  [<span class="hljs-string">&#x27;001 - 不等式 1.1 不等式的基本性质 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;016 - 不等式 4.4 恒成立与存在性问题 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;007 - 不等式 3.3 高次不等式 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;008 - 不等式 3.4 含参讨论 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;003 - 不等式 1.3 比大小思路 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;002 - 不等式 1.2 不等式证明思路 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;006 - 不等式 3.1 二次不等式 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;015 - 不等式 1.4 复杂证明题 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;005 - 不等式 2.3 基本不等式变式 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;011 - 不等式 2.4 不要过度放缩三元不等式 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;010 - 不等式 2.2 使用条件误区 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;009 - 不等式 4.1 对勾型函数最值 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;013 - 不等式 4.2 对称与均值 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;004 - 不等式 2.1 基本不等式均值不等式 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;014 - 不等式 4.3 齐次与均值 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;012 - 不等式 3.2 穿针引线法 高中数学.mp4&#x27;</span>]<br>修改后，一共有<span class="hljs-number">16</span>个文件<br>  [<span class="hljs-string">&#x27;不等式 1.1 不等式的基本性质 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 4.4 恒成立与存在性问题 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 3.3 高次不等式 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 3.4 含参讨论 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 1.3 比大小思路 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 1.2 不等式证明思路 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 3.1 二次不等式 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 1.4 复杂证明题 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 2.3 基本不等式变式 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 2.4 不要过度放缩三元不等式 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 2.2 使用条件误区 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 4.1 对勾型函数最值 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 4.2 对称与均值 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 2.1 基本不等式均值不等式 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 4.3 齐次与均值 高中数学.mp4&#x27;</span>, <span class="hljs-string">&#x27;不等式 3.2 穿针引线法 高中数学.mp4&#x27;</span>]<br></code></pre></td></tr></table></figure><p>打印内容中查看自己修改前和修改后的文件对比，感觉没问题了，把其中注释的两行代码打开注释，就可以完成文件修改了。</p><p><img src="https://raw.githubusercontent.com/hivandu/notes/main/img/20231017173526.png" alt="image-20231017173440797" /></p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;仅个人需求，有需要的可以自取。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;前段时间为家里孩子下载了一批课程，但是文件命名就很奇怪也很乱，就想着将文件名修改掉便于查看。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="https://hivan.me/tags/Python/"/>
    
    <category term="办公自动化" scheme="https://hivan.me/tags/%E5%8A%9E%E5%85%AC%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>09. 机器学习 - 逻辑回归</title>
    <link href="https://hivan.me/09.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>https://hivan.me/09.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</id>
    <published>2023-10-14T23:30:00.000Z</published>
    <updated>2023-11-25T04:41:20.546Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231013145642.png"alt="茶桁的 AI 秘籍 09" /></p><p>Hi，你好。我是茶桁。</p><p>上一节课，在结尾的时候咱们预约了这节课一开始对上一节课的内容进行一个回顾，并且预告了这节课内容主要是「逻辑回归」，那我们现在就开始吧。</p><span id="more"></span><h2 id="线性回归回顾">线性回归回顾</h2><p>在上一节课中，我们定义了 model，loss 函数以及求导函数。最后我们用 for循环来完成了求导过程。本节课一开始，咱们先来对上一节课的代码做一次优化，优化后的代码也会上传到课程代码仓库内。</p><blockquote><p>此部分代码依然在 08.ipynb 中。</p></blockquote><p>首先，我们将之前的 model重新更名为<code>linear</code>，以便知道我们这个函数是要做什么的。接着，我们把for 循环内对 w 和 b 的偏导封装为一个函数，便于我们之后调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">optimize</span>(<span class="hljs-params">w, b, x, y, yhat, pw, pb, learning_rate</span>):<br>    w = w + -<span class="hljs-number">1</span> * pw(x, y, yhat) * learning_rate<br>    b = b + -<span class="hljs-number">1</span> * pb(x, y, yhat) * learning_rate<br><br>    <span class="hljs-keyword">return</span> w, b<br></code></pre></td></tr></table></figure><p>然后我们将整个 for 循环封装一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model_to_be_train, target, loss, pw, pb</span>):<br>    w = np.random.random_sample(size = (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<span class="hljs-comment"># w normal</span><br>    b = np.random.random()<br>    learning_rate = <span class="hljs-number">1e-5</span><br>    epoch = <span class="hljs-number">200</span><br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm)):<br>            <span class="hljs-comment"># batch trainning</span><br>            index = random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm)))<br>            rm_x, lstat_x = rm[index], lstat[index]<br>            x = np.array([rm_x, lstat_x])<br>            y = target[index]<br><br>            yhat = model_to_be_train(x, w, b)<br>            loss_v = loss(yhat, y)<br><br>            batch_loss.append(loss_v)<br><br>            w, b = optimize(w, b, x, y, yhat, pw, pb, learning_rate)<br><br>            <span class="hljs-keyword">if</span> batch % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Epoch: &#123;&#125; Batch: &#123;&#125;, loss: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i, batch, loss_v))<br><br><br>    <span class="hljs-keyword">return</span> model_to_be_train, w, b<br></code></pre></td></tr></table></figure><p>在最后呢，我们可以在调用函数之前，导入所需第三方库，然后将之前的数据处理在执行函数前获取并处理一遍：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_openml<br>dataset = fetch_openml(name=<span class="hljs-string">&#x27;boston&#x27;</span>, version=<span class="hljs-number">1</span>, as_frame=<span class="hljs-literal">True</span>, return_X_y=<span class="hljs-literal">False</span>, parser=<span class="hljs-string">&#x27;pandas&#x27;</span>)<br><br>data = dataset[<span class="hljs-string">&#x27;data&#x27;</span>]<br>target = dataset[<span class="hljs-string">&#x27;target&#x27;</span>]<br>columns = dataset[<span class="hljs-string">&#x27;feature_names&#x27;</span>]<br>dataframe = pd.DataFrame(data)<br>dataframe[<span class="hljs-string">&#x27;price&#x27;</span>]  = target<br><br>rm = dataframe[<span class="hljs-string">&#x27;RM&#x27;</span>]<br>lstat = dataframe[<span class="hljs-string">&#x27;LSTAT&#x27;</span>]<br><br>model, w, b = train(linear, target, loss, partial_w, partial_b)<br></code></pre></td></tr></table></figure><p>为什么我们每次都要随机取一个数字呢？</p><p>你也可以把所有的 x 全部输入进去，所有的 y全部输入进去。但是实际上在整个场景下，比方说我们有很多的训练数据，每个训练数据都有一个x，有一个 y。</p><p>loss 函数本来写的是 i 属于所有的 N，y_i 减去 yhat_i的平方。但是现在如果把所有的 x 和所有的 y在真实的场景下输入进去的话，假设现在 x 有 100 万个或者 200万个，输入进去之后整个求解过程可能 loss函数这个程序加载都加载不出来，会非常非常慢。</p><p>所以在实际的工作中，假如说 i 属于 D:<spanclass="math inline">\(\sum_{i \in D}\)</span>，D 就是 distribution的意思，就是随机取一些数据，然后再把随机取的一些数据求解。这样的话每一次就可以保证它可以运行。</p><p>但是这样的一个区别是什么？</p><p>每次把所有的 x 和 y 都输入进去，这种梯度下降方式中 loss下降是一个很顺滑的样子，这个叫做 BGD。</p><p>还有一种情况就是咱们课上用的这种剃度下降方式，每次随机取了一个随机值，叫随机剃度下降，就随机取一个数字做梯度下降，简称SGD。这个 loss 下降就会上下波动很厉害。如我们上面展示的图。</p><p>再下来呢还有一种，它是取这两者之间，每次不是取一个，是取了多个。我们把这个叫做MBGD。</p><p>这是三种梯度下降方式。在实际的工作中 SGD用的最多，因为可以快速的进行梯度下降学习。</p><p>我们可以将代码修改一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>    ...<br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm) // batch_size):<br>        indices = np.random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm)))<br>        rm_x, lstat_x = rm[indices], lstat[indices]<br>        x = np.array([rm_x, lstat_x])<br>        y = target[indices]<br>        ...<br></code></pre></td></tr></table></figure><p>关于这一部分内容，这里仅仅是提一下，在后面的课程中，我们还会更详细的来讲解。</p><p>我们在循环中，将原来的次数 50 替换成了<code>epoch</code>,<code>epoch</code>在机械学习里边指的是运行了整整一遍。</p><p>在第二个循环内，里面是 rm个东西，每次都是随机取，我们随机取了多少次呢？取了 rm个。也就是说平均每个样本会被取样一次。</p><p>这就是数据量大的好处，当数据量很大的时候，有个别的点没有取到或者说有个别的点取了多次其实对最终的效果是不影响的。</p><p>也就是说因为数量很大，所以一两次的变化，一两个数值取的少了或者取的多了，其实不是非常影响。</p><p>我们把每次<code>epoch</code>的 batch 打出来，我们来看一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">...</span>):<br>    ...<br>    losses = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>        batch_loss = []<br>        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm)):<br>            ...<br>            batch_loss.append(loss_v)<br>            ...<br>        <br>        losses.append(np.mean(batch_loss))<br>    <br>    <span class="hljs-keyword">return</span> model_to_be_train, w, b, losses<br><br>model, w, b, losses = train(linear, target, loss, partial_w, partial_b)<br>plt.plot(losses)<br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231013145643.png"alt="image-20231012120642561" /></p><p>那如果是上面我们更改的代码，使用了<code>batch_size</code>控制之后，图形就完全不一样。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231013145644.png"alt="image-20231012141443971" /></p><p>可以看到，这个 loss 下降还是挺明显的。</p><p>这个时候，我们假设知道一组的 rm 等于 19,lstat 等于7。而此时其实已经有了 w 和 b，求到最终的 w 和b，就能够有一个预测值了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">predicate = model(np.array([<span class="hljs-number">19</span>, <span class="hljs-number">7</span>]), w, b)<br><span class="hljs-built_in">print</span>(predicate)<br><br>---<br>Epoch: <span class="hljs-number">0</span> Batch: <span class="hljs-number">0</span>, loss: <span class="hljs-number">46.17245060319155</span><br>...<br>Epoch: <span class="hljs-number">199</span> Batch: <span class="hljs-number">0</span>, loss: <span class="hljs-number">0.2053457975383563</span><br></code></pre></td></tr></table></figure><p>我们在这个实例中，只用了两个最显著的特征，如果把 x的维度变多一些，其实就会更加接近了。</p><p>好，这个线性回归的过程，其中包括线性函数的定义，为什么要用线性函数，loss函数的意义，梯度下降的意义就都讲完了。</p><p>这个内容是我从斯坦福大学的参考书上弄过来的。</p><p>除了定义一个这样一个平方值的 loss，可以定一个绝对值loss，都是一样的，都可以实现找到最优值。</p><p>只不过这个二次方的这个 loss 对于结果，它的惩罚会更大一些。</p><p>经过这一段代码的洗礼，对于之前的那个数学式子应该能看的更明白一些了。</p><h2 id="逻辑回归">逻辑回归</h2><p>我们讲完了线性回归，下面再跟大家来讲一下逻辑回归。</p><p>逻辑回归是什么？假如还是如上那个问题，前面代码都没变。当然，库需要再导入一遍：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_openml<br><br>dataset = fetch_openml(name=<span class="hljs-string">&#x27;boston&#x27;</span>, version=<span class="hljs-number">1</span>, as_frame=<span class="hljs-literal">True</span>, return_X_y=<span class="hljs-literal">False</span>, parser=<span class="hljs-string">&#x27;pandas&#x27;</span>)<br></code></pre></td></tr></table></figure><p>现在咱们要变一个问题场景，我们先打印一下<code>np.percentile()</code>，这是要求百分位，比方说我们填入一下<code>target</code>，其实我们数据预处理的时候知道，就是<code>dataframe['price']</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(np.percentile(target, <span class="hljs-number">66</span>))<br></code></pre></td></tr></table></figure><p>我们写入一个<code>target</code>,其实就是<code>price</code>，然后我们在后面写了一个66，也就是说，我们将这里所有的price，也就是房价，做了一次排序，然后，我取从 0 到 100 中的第 66%个位置的数值，就是大于 2/3 的房价。同样的，如果我这里填了一个50，那么就是取最中间的那个值。</p><p>输出的结果为 23.53, 是 23 万美金。还是比较便宜，23 万美金折合 100多万。</p><p>好，现在我们来做一个判断：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">greater_then_most = np.percentile(target, <span class="hljs-number">66</span>)<br>dataframe[<span class="hljs-string">&#x27;expensive&#x27;</span>] = dataframe[<span class="hljs-string">&#x27;price&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> p: <span class="hljs-built_in">int</span>(p &gt; greater_then_most))<br><br><span class="hljs-built_in">print</span>(dataframe[:<span class="hljs-number">20</span>])<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231013145645.png"alt="image-20231012161144568" /></p><p>我们定义了一个<code>expensive</code>，在 dataframe中加入了这个特征。这个特征在房价大于 2/3 房价的时候 int 为 1，否则为0。</p><p>做了这样一件事之后，就是问这个房子是不是贵房子，如果是1，就是贵房子，0 就不是贵房子。根据我们添加的特征来进行判断。</p><p>那接着呢，问题发生了改变。我们不知道这个房子的<code>price</code>，现在需要进行预测这个房子是不是属于一个高档小区。在预测中，假如是1，就表示是高档小区，0就表示不是高档小区。现在要根据它的一些特征来猜测它是不是高档小区。</p><p>我们刚刚其实已经知道，所谓的高档小区其实是和价格有一定关系的。</p><p>假如说现在咱们有一个问题要求解，现在要有一个模型能够预测它到底是 1还是0，或者我们要预测是开心还是难过，咱们现在只要做一件事情就可以，就是把我们期望目标标成1，把另外那个相对的目标标成 0。</p><p>如果我们能够拟合一个函数，这个函数的输出要么是 1，要么是0，我们让这个模型的值越接近于实际的值就可以了。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231013145646.png"alt="image-20231012163700062" /></p><p>比方说刚刚回顾完的线性回归，给定的 (x, y) 里边，y这个值它是一个实数。如果现在变成了 0、1。比方说 1 就是 happy，0 就是sad。或者还是用咱们之前定义的：1 就是 expensive，0 就是 notexpensive。</p><p>把 1 和 0 认为是概率，如果是概率的话，1 就是 100% 是，0 就 100%不是。</p><p>那么咱们之前的 model 输出的是实数R, 这次需要的 model 就是输出的是0~1。这个模型的任务就变成了如果 x 给定的是 1，那么 model输出最后要尽可能的接近 1。</p><p>怎么样才能让我们的 model 输出是 0 到 1之间呢？有一个方法，一个函数叫做 logistic 函数，logistic function: <spanclass="math display">\[\begin{align*}J(\theta)=-\sum_i(y^{(i)}log(h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta}(x^{(i)})))\end{align*}\]</span></p><p>这个函数其实在复杂系统里面是一个很重要的函数，人们其实是期望获得一种导数，数学家们研究的是这个：y’= y(1-y), 就是 y 的导数等于 y 乘以1-y。研究完了之后发现有一种函数就满足这个特征： <spanclass="math display">\[\begin{align*}f(x) = \frac{1}{1+e^{-x}}\end{align*}\]</span></p><p>这个函数的值画出来，就是这个样子：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231013145647.png"alt="image-20231012173054474" /></p><p>值全部是从 0 到 1 之间，中间与 y 轴交点为 0.5。</p><p>给大家讲一下这个原理，逻辑函数原本是想研究y’=y(1-y)，求解出来有这样一个函数满足这样的特征：f(x) =1/1+e^{-x}。那我们这里需要注意一下，这个特征以后会有大用。把它的图形画出来呢，就是如上图这样的一种函数，这个函数值就是在0～1 之间。</p><p>为什么我们要用逻辑函数来做概率预测呢？首先第一个原因就是因为它的值本身输出就是0～1之间，天然的适合做概率这块，第二，他还处处可导，逻辑函数它是处处可导的。</p><p>所以我们就可以用这个函数来进行分类，可以把原来的模型 f(x)=wx+b,这整个模型写成： <span class="math display">\[\begin{align*}f(x) = \frac{1}{1+e^{-(wx+b)}}\end{align*}\]</span></p><p>原来的 f(x) 拟合的是等于 wx+b, 现在把这个 f(x)变成如上式的样子。这个输出的就变成 0-1 了，就能够让它的值在 0 到 1之间变换。</p><p>这就是为什么我们把这种方法叫做逻辑回归的原因。就是它是在回归曲线上加了一个逻辑函数，所以我们称其为逻辑回归。</p><p>加上逻辑函数虽然输出的值是 0～1 之间，但其实是在做分类。越接近于 1就越近于一类，越近于 0 就越近于另一类。逻辑回归本质上就是在做分类。</p><p>接着，咱们来上代码给大家详细的讲解一遍。</p><p>其实我们整个代码和之前实现的线性回归非常的像，唯一的区别是我们需要一个叫做sigmoid 的函数，也就是逻辑函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br></code></pre></td></tr></table></figure><p>我们来把这个函数画出来看看是什么样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.plot(sigmoid(np.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)))<br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231013145648.png"alt="image-20231012184251402" /></p><p>其实，机器学习是个很简单的问题。机器学习其实是计算机里面最简单的几个部分，哪些比这更复杂呢？第一个、编译器原理，还有程序设计语言与自动机，还有计算机图形学，复杂系统，还有计算复杂性，操作系统。其实这些都比深度学习复杂的多。</p><p>为什么我们现在深度学习用的多，就是因为深度学习简单。所以我说这些题外话是想告诉大家，在学习这个的时候不要有什么顾虑和负担，放松一点，放开膀子撸起袖子干就完了。</p><p>好，我们现在把 model 写出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-keyword">return</span> sigmoid(np.dot(x, w.T) + b)<br></code></pre></td></tr></table></figure><p>那么来看，我们现在如果要预测，给一个 rm 和 lstat 输入进去，一个 RM 和LSTAT 的值输入进去。</p><p>我们先来看一下真实的值是怎样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">rm = dataframe[<span class="hljs-string">&#x27;RM&#x27;</span>]<br>lstat = dataframe[<span class="hljs-string">&#x27;LSTAT&#x27;</span>]<br>target = dataframe[<span class="hljs-string">&#x27;expensive&#x27;</span>]<br>epoch = <span class="hljs-number">200</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>    <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm)):<br>        index = random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm)))<br><br>        x = np.array([rm[index], lstat[index]])<br>        y = target[index]<br><br>        <span class="hljs-built_in">print</span>(x, y)<br></code></pre></td></tr></table></figure><p>这个呢就是我们在训练时候每个给的数据，每次给他给一组数据，然后给它的这个值到底是0 还是 1。我们期望的是求解一组 (w,b)，能够让它输入 x 的时候也能得到 0或者 1。就它真实的时候是 0，期望的是这一组输入进去之后，根据 (w,b)运行完了之后也是 0。这个就是我们的目标。</p><p>假如已经获得线性回归了，然后要通过线性回归加一些东西想实现 0 和 1的分类。之前我们在线性回归那里是不是先定义了一个 loss 函数？把 loss函数定义清楚之后再对 loss 求偏导就可以了。那这里也是一样，需要定义loss，只要把 loss 定义出来之后给 loss求偏导就可以了，和之前一模一样。</p><p>现在的问题就转变成，咱们怎么求 loss 呢？</p><p>我们的目标给定如果是 0 那么 yhat 也要是 0。y 是 1 的时候 yhat 也得是1。如果 y 等于 1 的情况下，yhat 等于 0，就意味着错的很厉害啊。相对的，y等于 1，yhat 等于 0 也同样是错的很厉害。</p><p>那么，如果 y 等于 1 的时候，yhat 等于 0.9, 错的就比较少。yhat 等于 1的时候，错误就是 0，也就是没错误。</p><p>那么我们就可以写-log(yhat),把这个写出来就是这样一个函数：当它越接近于 0 的时候，loss值会接近于无穷大，当它接近于 1 的时候，loss 会接近于 0。</p><p>当 y 等于 1 的时候，loss 可以等于-log(yhat)。如果 y 等于 0，lose值就越接近于无穷大。这个时候 loss 就可以写成-log(1-yhat)。</p><p>那么现在这里就出现一个问题，也是通常面试时候的一个高频题：为什么在逻辑回归里，loss函数不直接写成 1-yhat？</p><p>就是，如果 y 等于 1 的情况下，loss 函数不直接写成 1-y, 当 y 等于 0的时候，loss 不直接写成 y。</p><p>因为这样会导致这条线呈现出一个直线，所有的偏导结果都是一致没有发生变化。</p><p>就好比有一个孩子考试成绩特别差，假如现在的目标是等于1，他的成绩特别特别差，0.001。现在的这个梯度还是比较小，他考特别好的时候这个梯度还是一样。</p><p>但是我们知道，梯度代表了接下来的变化方向和力度。这个就是我们为什么要用这个的原因。</p><p>当然，其实还有一些概率上的解释，这里就不继续延展着讲了。</p><p>对于上面讲的，当 y=1 和 y=0 的两个不同的 loss函数，可以做一个归纳，写成一个 loss 函数：</p><p><span class="math display">\[loss = - (ylog\hat y + (1-y) log(1-\hat y))\]</span></p><p>为什么能够变成这样呢？我们来分析一下，如果 y=0 的时候，那 ylog(yhat)就等于 0，也就是说仅剩下 (1-y)log(1-yhat)，反过来，当 y=1的时候，等式后面部分就等于 0，仅剩下 ylog(yhat)。</p><p>那下面，我们就来完成代码来实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">yhat, y</span>):<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(y*np.log(yhat) + (<span class="hljs-number">1</span>-y)*np.log(<span class="hljs-number">1</span>-yhat))<br></code></pre></td></tr></table></figure><p>lose 函数求解出来之后，对于 (w,b) 怎么求偏导呢？</p><p>那么其实式子就可以变成： <span class="math display">\[\begin{align*}&amp; -(ylog \sigma (wx+b) + (1-y)log \sigma (1-(wx+b))) \\&amp; -(ylog \sigma (w_1x_1+w_2x_2+b) + (1-y)log \sigma(1-(w_1x_1+w_2x_2+b)))\end{align*}\]</span> 那么对其求偏导，一系列推导完成后就可以变成： <spanclass="math display">\[\begin{align*}\frac{\partial loss}{\partial w_i} &amp; = \sum(\hat y - y) x_i\end{align*}\]</span> 我们来完成其函数代码，和线性部分一样，包含对 w 和 b求导两部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">partial_w</span>(<span class="hljs-params">x, y, yhat</span>):<br>    <span class="hljs-keyword">return</span> np.array([np.<span class="hljs-built_in">sum</span>((yhat-y) * x[<span class="hljs-number">0</span>]), np.<span class="hljs-built_in">sum</span>((yhat-y) * x[<span class="hljs-number">1</span>])])<br>  <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partial_b</span>(<span class="hljs-params">x, y, yhat</span>):<br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>((yhat - y))<br></code></pre></td></tr></table></figure><p>那接下来我们干嘛？上节课的内容还有印象吗？接下来我们要给 w,b随机值对吧？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = w = np.random.random_sample((<span class="hljs-number">1</span>,<span class="hljs-number">2</span>))<br>b = np.random.random()<br></code></pre></td></tr></table></figure><p>接着我们修改上面实现过的对真实值的实现代码，删掉我们曾经打印的(x,y)，然后利用我们实现的 loss 函数和偏导函数来计算预测值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">rm = dataframe[<span class="hljs-string">&#x27;RM&#x27;</span>]<br>lstat = dataframe[<span class="hljs-string">&#x27;LSTAT&#x27;</span>]<br>target = dataframe[<span class="hljs-string">&#x27;expensive&#x27;</span>]<br><br>learning_rate = <span class="hljs-number">1e-5</span><br>epoch = <span class="hljs-number">200</span><br>losses = []<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>    batch_loss = []<br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm)):<br>        index = random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm)))<br><br>        x = np.array([rm[index], lstat[index]])<br>        y = target[index]<br>        <br>        <span class="hljs-comment"># print(x, y)</span><br><br>        yhat = model(x, w, b)<br>        loss_v = loss(yhat, y)<br><br>        w = w + -<span class="hljs-number">1</span> * partial_w(x, y, yhat) * learning_rate<br>        b = b + -<span class="hljs-number">1</span> * partial_b(x, y, yhat) * learning_rate<br><br>        <span class="hljs-keyword">if</span> batch % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Epoch: &#123;&#125;, Batch: &#123;&#125;, loss:&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i, batch, loss_v))<br>    losses.append(np.mean(batch_loss))<br></code></pre></td></tr></table></figure><p>执行完之后，我们可以看到 loss 在慢慢的变小。</p><p>现在我们在数据中随机取一些数据，比如说我们去 100个吧，用于去预测，检验我们的模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">random_test_indices = np.random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm)), size=<span class="hljs-number">100</span>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> random_test_indices:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;RM:&#123;&#125;, STAT:&#123;&#125;, TARGET:&#123;&#125;, PRE:&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(rm[i], lstat[i], target[i], model(np.array([rm[i], lstat[i]]), w, b)))<br>    <br>---<br>RM:<span class="hljs-number">6.425</span>, STAT:<span class="hljs-number">12.03</span>, TARGET:<span class="hljs-number">0</span>, PRE:[<span class="hljs-number">0.15662289</span>]<br>...<br>RM:<span class="hljs-number">5.0</span>, STAT:<span class="hljs-number">31.99</span>, TARGET:<span class="hljs-number">0</span>, PRE:[<span class="hljs-number">4.87033539e-06</span>]<br>...<br>RM:<span class="hljs-number">8.247</span>, STAT:<span class="hljs-number">3.95</span>, TARGET:<span class="hljs-number">1</span>, PRE:[<span class="hljs-number">0.9623407</span>]<br>...<br>RM:<span class="hljs-number">7.686</span>, STAT:<span class="hljs-number">3.92</span>, TARGET:<span class="hljs-number">1</span>, PRE:[<span class="hljs-number">0.95077171</span>]<br><br></code></pre></td></tr></table></figure><p>我随机展示了一些数据，我们从这里能看到，预测值内有的值偏向0，有的值甚至比 1 还要大。对比前面的 TARGET真实值来看，预测的大部分还是准确的。</p><p>不过这个时候还是有问题，我们做这个预测的初衷是为了要做分类，也就是到底是0 还是 1，那 PRE值到底是什么，怎么分类呢？咱们就要牵扯到一个东西：<code>dicision boundary</code>。</p><p>也就是决策的边界，咱们假定为0.5，让我们拿到的预测值去和这个边界值做对比，大于它的就是 1，小于的就是0:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">dicision_boundary = <span class="hljs-number">0.5</span><br>predicate_label = <span class="hljs-built_in">int</span>(predicate &gt; decision_boundary)<br></code></pre></td></tr></table></figure><p>有了这个之后，我们需要更改下我们之前的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">random_test_indices = np.random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm)), size=<span class="hljs-number">100</span>)<br>decision_boundary = <span class="hljs-number">0.5</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> random_test_indices:<br>    x1, x2, y = rm[i], lstat[i], target[i]<br>    predicate = model(np.array([x1, x2]), w, b)<br>    predicate_label = <span class="hljs-built_in">int</span>(predicate &gt; decision_boundary)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;RM:&#123;&#125;, LSTAT:&#123;&#125;, EXPENSIVE:&#123;&#125;, Predicated:&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(x1, x2, y, predicate_label))<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231013145649.png"alt="image-20231013142817740" /></p><p>更改完之后我们执行，和真实值进行对比，我们发现整个预测的还算事准确。当然也有部分预测错误的。</p><p>现在这个模型能够预测出来了，根据两个值能够预测出来它到底属于一个高档房子，还是不属于一个高档房子。但是我们会发现其实还有算错的地方。那么现在要问，如何衡量模型的好坏？以下就是我们要继续研究的问题：</p><ol type="1"><li>accuracy 准确度</li><li>precision 精确度</li><li>recall 召回率</li><li>f1, f2 score</li><li>AUC-ROC 曲线</li></ol><p>这些就是我们用于衡量模型的一些指标，通过这个，我们要引出一个非常重要的概念，就是过拟合和欠拟合(over-fitting andunder-fitting)。我们可以说，整个机器学习的过程，就是在不断的进行过拟合和欠拟合的调整。<strong>那么这些呢，就是我们下面课程的内容了。</strong></p><p>目前来讲，我们学习了监督学习里面最重要的线性回归和逻辑回归，接下来什么我们要去学的LSTM，CNN 等等，其实都是为了提高这个准确度所要做的事情。</p><p>也就是，现在我们发现虽然模型还是稍微有一些错误，这个时候就需要一起来再研究一下如何衡量模型的好坏。只有知道了如何衡量模型的好坏，才知道怎么样去调整它，怎么去优化它。</p><p>好，那下节课记得不见不散。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231013145642.png&quot;
alt=&quot;茶桁的 AI 秘籍 09&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;上一节课，在结尾的时候咱们预约了这节课一开始对上一节课的内容进行一个回顾，并且预告了这节课内容主要是「逻辑回归」，那我们现在就开始吧。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>08. 机器学习 - 线性回归</title>
    <link href="https://hivan.me/08.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>https://hivan.me/08.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</id>
    <published>2023-10-11T23:30:00.000Z</published>
    <updated>2023-11-25T04:41:16.932Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231011224909.png"alt="茶桁的 AI 秘籍 08" /></p><span id="more"></span><blockquote><p>从本次课程开始，大部分时候我将不再将打印结果贴出来了，因为太占用篇幅。小伙伴可以根据我的输出执行敲一遍代码来进行学习和验证。同样是为了节省篇幅，我也不会再一行行那么仔细的解释代码了，一般只会告诉你我代码做了什么。其中的逻辑关系和关键词，小伙伴们自行好好的琢磨一下。</p></blockquote><p>Hi, 你好。我是茶桁。</p><p>前几节课咱们主要是了解到了什么是人工智能，整个机器学习的工作路径等等。不过可以说，前面的几节课，从机器学习导论到上一节课介绍K-means。</p><p>咱们快速回顾一下，前几节课里的内容都有什么。之前咱们学习了什么是优化问题，然后还有什么是动态规划，什么是机器学习问题，以及监督学习和非监督学习的区别，我们还学习了一个非常著名的非监督学习方法：K-means。</p><p>这些都还是一个铺垫。真正的内容，从这节课才算事正式开始。</p><p>从这一节课开始，我会给大家开始系统的学习监督学习，监督学习其实内容比较多，我们可能需要多花多一点时间。</p><p>在最开始，我还是要更大家强调一下上一节课上更大家讲的问题：要学习算法，不仅仅是要学习很多很多的这个算法模型，更重要的是什么要能够把问题抽象成一个一个的算法。工作场景中的问题并不能使用一个单独的算法模块能够解决。</p><p>机器学习算法只是人工智能其中的一个部分，或者说人工智能的某个部分，比方说取它的特征等等。它的某一个部分用在真正的工作中，用在项目中是很杂揉、很混合的一个状态。</p><p>咱们篇幅短内容多，密度比较大，短短几节课，可能是大家研究生课程一个月时间的内容。可以说，咱们课程还是有点难，信息量也比较大。</p><p>OK，开始第一个问题。</p><h2 id="线性回归-linear-regression">线性回归 LINEAR REGRESSION</h2><p>咱们第一个要讲的，也是非常重要的一个方法，就是线性回归。</p><p>线性回归非常的简单，也非常的基础。但是它作为我们整个人工智能，整个深度学习中要讲的第一课，里面蕴含了非常多的机器学习的基本思想。所以大家一定要把它学清楚。如果能把它学好，其实对于咱们以后学习帮助非常大。</p><p>咱们来看一下，什么是线性回归。</p><p>在生活中有很多这样的问题，比方说咱们的血糖，往往在吃饭的时候，吃的糖分、碳水化合物等比较多，饭后的半个小时、一个小时内血糖会更高。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231010204910.png"alt="image-20231010201907259" /></p><p>还有一种情况，抽烟抽的越多的人，得肺病的概率往往会越高。并不是一定说抽烟抽的多的人就一定会生病，但是抽烟抽的多的人得肺病的概率往往会越高。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231010204944.png"alt="image-20231010204944417" /></p><p>那么还有一种情况，比方在咱们工作的时候，随着工作时间的增加，收入往往也会越来越多。尤其是在日本，是一个非常典型的情况。在日本基本上一个人的收入和他的工作年限是最相关的。他们在一个固定的时间内的薪资变化不会非常大。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231010205124.png"alt="image-20231010205124864" /></p><p>比方说都是 29 岁、都是 32 岁、都是 35岁，假如都在同一家公司，那么薪资待遇也会很接近。</p><p>往往这个其实反映的是我们现实生活中一个非常基本的一个关系，随着有一些值的增大另外一些值也随着变化。</p><p>假如说我们把它变成自变量和因变量，所谓的自变量就是它的变化会引起因变量的变化。也就是说在现实生活中，我们最基本的关系是随着一个变量的变化另外一个变量要么增加，要么减小。当然不变可以算是一种特殊情况，就是变化为0。</p><p>那么吃糖的多少、抽烟和工作年限就是整个自变量和因变量的一种关系。现在希望让机器来找到这个关系。</p><p>如果把这种关系画出来的话，我们用一个图表画出来就会发现其相关性。</p><p>就比如下方这三张图表：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231010213659.png"alt="image-20231010213659846" /></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231010213718.png"alt="image-20231010213718109" /></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231011110537.jpg" /></p><p>这三个图是非常非常典型的，随着一个量的变化另外的一个量要么减少，要么增加。</p><p>这个时候工程师就希望我们能够对现实生活中这种随着一个变量的增加或减少导致另外一个量增加或减小，能够找到一种关系来刻画。</p><p>当然之间的这种关系可能会很多，可以是一种线性结构，y = kx+b, 也可以是其他的什么结构。最简单的一种其实就是线性关系。 <spanclass="math display">\[\begin{align*}\vec {x} = [x_0, x_1, x_2, ..., x_n] \\f(x) = \sum_{i \in N} w_i \times x_i +b\end{align*}\]</span> 我们把这种关系称为是线性关系。</p><p>为什么称为线性关系呢？假设 x 现在是一维空间，<spanclass="math inline">\(x \in R^1\)</span>, 那么 f(x) 就是一条直线。如果 x是二维空间的话，<span class="math inline">\(x\in R^2\)</span>, f(x)就是一个平面。这种关系其实是自然界中最简单的一种关系。</p><p>除了这种关系之外，你还可以想象一下，如果我们要刻画 x 和 f(x)之间的关系，你还能想到哪些函数呢？</p><p>比如咱们可以有：</p><ul><li>二次函数 <span class="math inline">\(f(x) = ax^2 + bx +c\)</span>，</li><li>三角函数<span class="math inline">\(f(x) = sin(x)\)</span>,</li><li>幂函数<span class="math inline">\(f(x) = a^x\)</span>,</li><li>反函数<span class="math inline">\(f(x) = tanh(x)\)</span>,</li><li>对数函数<span class="math inline">\(f(x) = log(x)\)</span>，</li></ul><p>咱们整个来了个数学回顾。这个时候你会发现好像有非常多种函数，甚至我们还可以在这个基础之上做一些复杂的函数，比如说$f(x)= x<sup>{bx</sup>3+cx<sup>2+dlog</sup>x}+elog^x_m $。</p><p>理论上，我们可以做有无数种可能的关系。</p><p>其实就是如何找到因变量和自变量之间的关系，长久以来，这其实是我们整个自然科学界一直在思考探索的一个问题。</p><p>像牛顿，笛卡尔、爱因斯坦、波尔这些人其实都是在研究这件事情。当观察到了很多事情，然后期望用一种函数关系能够把它来表证出来。</p><p>后来，在 14世纪一个非常著名的哲学家就提出来了这样的一个理论，叫做奥卡姆剃刀原理。奥卡姆剃刀原理说的是对于一件事情，你要解释它的关系的话，最简单的：</p><blockquote><p>The explanation requiring the fewest assumptions is most likely to becorrect.</p></blockquote><p>这个 fewest assumptions指的是什么意思？就是最少的假设，你可以把它理解成是几个假设，假如对应到函数上，就有很多变量。</p><p>举个例子，你们单位上有一个同事经常迟到，有三个人对这个同事为什么迟到有不同的说法。</p><p>第一个人说这个同事迟到大概率是因为他前一天晚上吃坏了肚子，导致一晚上没睡好，一大早还因为拉肚子迟到了。</p><p>第二个人说同事昨天和一个男生出去了，可能玩太晚导致没起来。</p><p>第三个说这个同事可能对昨天法的工资有抱怨，去找领导议论没有得到结果，昨天找男朋友安慰了。今天也是因为赌气故意迟到。</p><p>那么对于一个女孩子第二天早上迟到，人们就有 3种不同的说法。那么大家仔细思考一下，对于你来说，你要相信一个的话，在我们日常生活中你会发现把一件事情想的越复杂往往就错了。</p><p>因为比方说第三个条件的，首先昨天发没发工资是一种可能性，然后发了工资她有没有去找老板？找了老板老板有没有怼她？就算怼了她，那她有没有找男朋友？这一系列下来你会发现这个事情如果把它变得因素很多，你对这个事情的估计可能会更错误。</p><p>奥卡姆就提出：若无必要，勿增实体。如果对于同一现象有几种不同的假说，我们应该采取最简单的哪一种。那么在我们抽象的函数上，拟合也是这样，我们要拟合一种关系，一种最简单的关系其实往往是最有用的。</p><p>比方说在这种关系上：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231011224923.jpg" /></p><p>在这种关系上，你可以说他是一条直线。</p><p>比方说我们看到绿色这条线，比较弯，可以说这条线就是这样的一个函数。但是最简单的一种方法，假设它就是一条直线，就像红色这条线。</p><p>我们如果把绿色这条定义为 a，把红色这条定义为 b。a好像拟合的程度更高一些，b其实是做了一个特别简单的假设，它假设就是一个简单的线性关系。线性关系就是随着一个变量的增多，另外一个也成比例的增多或者减小。</p><p>a 看起来更复杂，但 b 在整个状态上看更稳定。a这个函数在没有看到的地方，其实按照趋势就有可能差的特别大。而 b虽然在观察到的地方有一些差别，但是因为它这个模型很简单，假设很简单，所以在这些没有观测到的地方你会发现还是和我们整体的趋势会比较接近。</p><p>a复杂，在做函数拟合的时候，不管这个关系看起来有多复杂，先假设它是最简单的一种线性关系。当线性关系实在不好的时候，再把它变复杂。</p><p>这就是为什么学习机器学习监督式学习要先学习线性拟合的原因。对简单的假设不一定对，但是除非这个简单的假设不行，否则我们就不要给他更复杂的假设。</p><p>比方说下面这个图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231011224924.jpg" /></p><p>这四张图中，都可以用一个直线去拟合。当然有的时候，比如说 (x2,y2),当它数据量很多的时候用一个直线效果就会不太好，包括 (x4, y4)效果可能也不会太好。</p><p>在这个时候，当我们发现它效果很差的时候，再去给他换一个模型。那像(x1,y1)，还有 (x3, y3)，还有如下图这种：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231011224925.jpg" /></p><p>这些其实都可以用一种线性关系来拟合。所谓的线性拟合，就是把函数写成自变量x 和它的权重相乘相加，然后再加上一个 b。就是我们刚才所描述的：<spanclass="math inline">\(f(x) = \sum_{i\in N} w_i \times x_i +b\)</span>；这种形式。这个就是我们的线性模型。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231011224926.jpg" /></p><p>如果 Regression输出是一个实数，利用一种线性关系，就是我们图中下方的公式： <spanclass="math display">\[\begin{align*}y_i = \beta_0+\beta_1x_{i1}+...+\beta_px_{ip}+\varepsilon_i = x_i^T\beta+ \varepsilon_i, \qquad i = 1, ..., n,\end{align*}\]</span>我们把它的关系假设成是一种线性关系，输出是一系列的实数，这个是一种回归现象，我们就把这个叫做线性回归。</p><p>图下方的式子是线性关系，Regression 是回归现象。我们就把要拟定的 f(x)叫做线性回归。</p><p>假如 y 是一个向量，x 是一个矩阵，y 等于 x 矩阵和<spanclass="math inline">\(\beta\)</span>矩阵做相乘运算。 <spanclass="math display">\[\begin{align*}y &amp; = \begin{bmatrix} y_1 \\ y_2 \\ \vdots y_n\end{bmatrix}, y =X\beta + \varepsilon; \\ \\X &amp; = \begin{bmatrix} x_1^T \\ x_2^T \\ vdots \\ x_n^T \end{bmatrix}= \begin{bmatrix} 1 &amp; x_{11} &amp; \cdots &amp; x_{1p} \\1 &amp; x_{21} &amp; \cdots &amp; x_{2p} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\1 &amp; x_{n1} &amp; \cdots &amp; x_{np} \\\end{bmatrix}, \\ \\\beta &amp; = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\\beta_p \end{bmatrix},\varepsilon =  \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots\\ \varepsilon_n \\  \end{bmatrix}\end{align*}\]</span></p><p>这个也是咱们之后做深度学习的时候之所以会经常接受矩阵的一个原因。</p><p>线性回归，刚给大家把原理讲了。现在咱们来演示一个非常基本的例子。</p><p>我给大家演示这个线性回归，用了一个非常经典的数据集。这个数据集叫做波士顿房价问题，很久前我在学习大数据的时候也成用过这个数据集。</p><p>其实我还曾经做过一个一线城市房价的研究，包括北京、上海等地区。但是为什么我没有用这些数据集，而是使用了一个很古老的波士顿地区房价数据集？</p><p>因为波士顿这个房价，和 roomsize，地点，地铁，高速路，周围的犯罪率等等有一个比较明显的关系。所以观察关系比较容易。但是北京的房价有个特点，它和远近没有关系，就是五环六环，也有些房子会很贵，三环也有房子会比较便宜。</p><p>和房屋的状况关系也不大，就是有的很老但是也是很贵。他唯一一个有关系的就是学区，这是最重要的一个决定因素。基本上周围有学区，这个房子就会非常贵，尤其是在海淀区。</p><p>波士顿数据集虽然很老，但是我们主要是为了学习他背后的这个线性回归原理。</p><p>北京这个房价要预测其实很简单，就是你用关键字来预测一下，看一下它里边包不包含学区两个字，然后再看一下那个学区排名就可以了。也并不是说说简单用学区就可以，而是学区对于北京房价的影响是最大的，别的因素都没有那么明显。</p><p>不过这个数据集在 scikit-learn 的 1.0 版本中被弃用，更甚的是在 1.2版本中已经删除，所以我们要想使用这个数据集还需要费一番功夫。</p><p>我们可以使用公开库 openml 来进行下载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_openml<br>dataset = fetch_openml(name=<span class="hljs-string">&#x27;boston&#x27;</span>, version=<span class="hljs-number">1</span>, as_frame=<span class="hljs-literal">True</span>, return_X_y=<span class="hljs-literal">False</span>, parser=<span class="hljs-string">&#x27;pandas&#x27;</span>)<br></code></pre></td></tr></table></figure><p>这其中，<code>name</code>就是数据集的名称，<code>version</code>为版本，<code>return_X_y</code>是下载拆分的特征和标签还是字典，False是默认值，下载的会是字典，如果设定为True，这需要两个变量分别接收特征和标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">data_x, data_y = fetch_openml(name=<span class="hljs-string">&quot;boston&quot;</span>, version=<span class="hljs-number">1</span>, as_frame=<span class="hljs-literal">True</span>, return_X_y=<span class="hljs-literal">True</span>, parser=<span class="hljs-string">&quot;pandas&quot;</span>)<br></code></pre></td></tr></table></figure><p>然后我们来处理一下数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">data = dataset[<span class="hljs-string">&#x27;data&#x27;</span>]<br>target = dataset[<span class="hljs-string">&#x27;target&#x27;</span>]<br>columns = dataset[<span class="hljs-string">&#x27;feature_names&#x27;</span>]<br>dataframe = pd.DataFrame(data)<br>dataframe[<span class="hljs-string">&#x27;price&#x27;</span>]  = target<br>dataframe.head()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231011224927.jpg" /></p><p>在我们获取的数据<code>dataframe</code>中，我们可以使用一个<code>corr()</code>，<code>show the correlation of dataframe variables</code>，<code>correlation</code>是相关系数。</p><p>那么相关系数的关系就是，如果一个值的增大，会引起另外一个值一定增大，而且是定比例增大，相关系数就越接近于1。如果是 0，就是两者之间没有任何关系。那如果是 -1呢，就是一个值增大，另外一个值就一定见效，而且减小是成相等比例的。</p><p>那我们来看一下<code>dataframe</code>的<code>correlation</code>之间的关系：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dataframe.corr()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231011224928.jpg" /></p><p>当然，我截图不全。小伙伴们下去自己去执行看看。</p><p>现在我们得到了一个 14 乘 14的一个矩阵，我们可以通过<code>seaborn</code>的<code>heatmap</code>来图形化，方便我们更直接的看到其相关性。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231011224929.jpg" /></p><p>在这张图中，越接近于黑色就越呈负相关，越接近于这个浅色就越是正相关，越接近于1.</p><p>比方说 prime 和 prime 是 1, 也就说把 price 当成因变量，再把 price也当成自变量的时候这两个值是一个增加另外一个一定增加。</p><p>price 除了自己本身之外，最亮的是 RM，这是和 price相关性最大的一个。我们去查询数据源，RM 就是小区平均的卧室个数。</p><p>换句话说这个小区如果卧室越多，就意味着房子可能越大，就越是有钱人住的。</p><p>再接着找一下影响最负相关的是什么？就是哪一个值的增大会引起房价的降低。</p><p>最下面的 LSTAT 是最负面影响的，只要 LSTAT增大，房价就会明显的随之下降。LSTAT 是什么呢？LSTAT就是一个小区中的低收入人群在周围的额比例，比例越高，那么房价就会越低。</p><p>咱们现在把这两个数值给它全部拿出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">rm = dataframe[<span class="hljs-string">&#x27;RM&#x27;</span>]<br>lstat = dataframe[<span class="hljs-string">&#x27;LSTAT&#x27;</span>]<br></code></pre></td></tr></table></figure><p>现在我们发现，RM 是最房价正向影响最多的，LSTAT是对房价负面影响最多的。现在我们要通过这两个值，因为这两个是影响房价最明显的特征，所以我们现在要建立一个模型，要根据我们已知的RM 和 LSTAT 来预测房价是多少。</p><p>我们要假设一个关系，要建立一个模型。所谓模型其实就是假设关系。很多模型其实都是现实世界中的一种抽象和简化。</p><p>高等数学概率统计，第一册的后半部分专门有一个地方就讲相关系数的。有兴趣的回过头再去看看咱们「AI秘籍」的数学篇。</p><p>这里，大家要知道相关系数的意义是什么就行。</p><p>我们现在假设和房价之间是一种最简单的线性关系。先从最简单的线性关系开始，假设它是线性关系的话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model1</span>(<span class="hljs-params">rm, lstat, w1, w2, b</span>):<br>    <span class="hljs-keyword">return</span> w1 * w2 * lstat + b<br></code></pre></td></tr></table></figure><p>这样，我们就用代码简单的实现了一个典型的线性关系。</p><p>这个时候，我们通过前面所讲的内容：</p><p><span class="math display">\[\begin{align*}\vec {x} = [x_0, x_1, x_2, ..., x_n] \\f(x) = \sum_{i \in N} w_i \times x_i +b\end{align*}\]</span></p><p>我们知道 x 是一个向量，wi也是一个向量。我们来重新定义一下这个<code>model</code>，你会发现更简单一些：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model2</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    if x = (rm, lstat)</span><br><span class="hljs-string">    w = (w1, w2)</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">return</span> np.dot(x, w.T) + b<br></code></pre></td></tr></table></figure><p>我们把模型重新写一下，如果每一个 x 就等于 (rm, lstat), 然后 w 就等于(w1,w2)，就是 x 和 w是两个向量，那么<code>model1()</code>就可以简写<code>model2()</code>的形式。</p><p>就是 x1 乘以 w1 加 x2 乘以 w2，再加上 b。</p><p>那我们写成向量形式，和<code>model1</code>有什么区别，或者说有什么好处呢？它的好处其实就是在后续需要添加数据的时候函数是不需要改动的。</p><p>有了这个 model，我们的目标是要获得一组 w 和b，要能够使得对于我们的值的预测最好。</p><p>怎么预测呢？</p><p>$$ <span class="math display">\[\begin{align*}loss(\theta) &amp; = \frac{1}{2} \sum(f_{\theta}(x^i)-y^i)^2 =\frac{1}{2}\sum(\theta ^T - y^i)^2 \\loss(\theta) &amp; = \frac{1}{2} \sum|f_{\theta}(x^i)-y^i| =\frac{1}{2}\sum|\theta ^T - y^i|\end{align*}\]</span> $$</p><p>做一个 loss 函数，这个 loss 函数里，<spanclass="math inline">\(\theta\)</span>指的是我们所有的参数。就是在这一组参数下，xi送到 f(x) 里面，它产生的估计的值，然后再计算和 y 之间的差别。</p><p>也就是为了获得最优的参数集合，比方说是 (w, b)，我们定义了一个 loss函数，这个 loss 函数在<spanclass="math inline">\(\theta\)</span>下，我们输入一组 x: <spanclass="math inline">\(loss(\theta;\vec{x})\)</span>，然后它就等于求和，i 属于所有的 i:<spanclass="math inline">\(\sum_{i \in N}\)</span>， <spanclass="math inline">\(f_{\theta}(x_i)\)</span>减去<spanclass="math inline">\(y_i\)</span>之后的平方： <spanclass="math display">\[\begin{align*}loss(\theta; \vec {x}) = \sum_{i \in N}(f_{\theta}(x_i) - y_i)^2\end{align*}\]</span> 如果这个<span class="math inline">\(f_{\theta}\)</span>对 x的预测值越好，就说给的 x都能非常准确的预测出来值是多少，预测值和实际值完全一样，那么这个时候loss 就等于 0。因为我们的<spanclass="math inline">\(f_\theta(x_i)\)</span>和<spanclass="math inline">\(y_i\)</span>完全相等。两者相减必定等于 0。</p><p>当 loss 特别大的时候，其实是意味着预测值就和真实值差得很远。</p><p>在统计学里预估值往往会写成<span class="math inline">\(\haty\)</span>，那我们就可以将式子变成如下这种形式： <spanclass="math display">\[\begin{align*}loss(x) = \frac{1}{n}\sum_{i \in N}(\hat y_i - y_i)^2\end{align*}\]</span> 之前的课程里咱们讲过，为了找出变量让 loss能够取得最小值，我们可以使用梯度下降的方法。那么我们上面的式子就也可以是如下这种形式：<span class="math display">\[\begin{align*}loss(x) = \frac{1}{n}\sum(w_1 \times x_1 + w_2 \times x_2 +b - y_i)^2\end{align*}\]</span> 现在为了获得一组 (w,b), 使得 loss 最小。那写出 loss 对 w1,w2的偏导，对 b 的偏导，就能求解出来了。</p><p>那么 loss 对于 W1 的偏导等于多少呢？ <span class="math display">\[\begin{align*}\frac{\partial{loss}}{\partial{w_1}}\end{align*}\]</span></p><p>这个都不用手算，眼睛都能看出来。我们将 2 放下来， 把后边的指数 2放下来，然后再把 X1提出去。如果你还不会算这个，可以去复习一下导数怎么求。可以找一本高数去好好看看，也可以去我之前写的《数学篇》里去好好看一下。<span class="math display">\[\begin{align*}\frac{\partial{loss}}{\partial{w_1}} = \frac{2}{n}\sum_{i \in N}(w_1\times x_{i1} + w_2 \times x_{i2} + b - y_i) \times x_{i1}\end{align*}\]</span></p><p>与此类似，loss 对于 W2 的偏导就等于： <span class="math display">\[\begin{align*}\frac{\partial{loss}}{\partial{w_2}} = \frac{2}{n}\sum_{i \in N}(w_1\times x_{i1} + w_2 \times x_{i2} + b - y_i) \times x_{i2}\end{align*}\]</span> 对于 b 的偏导，直接就乘以 1 了： <span class="math display">\[\begin{align*}\frac{\partial{loss}}{\partial{b}} = \frac{2}{n}\sum_{i \in N}(w_1\times x_{i1} + w_2 \times x_{i2} + b - y_i)\end{align*}\]</span></p><p>我们之前是要求什么？求 rm 和 lstat 对吧？那我们现在就可以将其中的 x1和 x2 替换掉就可以了：</p><p><span class="math display">\[\begin{align*}\frac{\partial{loss}}{\partial{w_1}} &amp; = \frac{2}{n}\sum_{i \inN}(w_1 \times rm_i + w_2 \times lstat_i + b - y_i) \times rm_i \\\frac{\partial{loss}}{\partial{w_2}} &amp; = \frac{2}{n}\sum_{i \inN}(w_1 \times rm_i + w_2 \times lstat_i + b - y_i) \times lstat_i \\\frac{\partial{loss}}{\partial{b}} &amp; = \frac{2}{n}\sum_{i \in N}(w_1\times rm_i + w_2 \times lstat_i + b - y_i)\end{align*}\]</span></p><p>写成这样之后，接下来只要在编程的时候实现出来就行了。也就是，把这一段数学翻译成代码。</p><p>我们现在来翻译一下 loss 函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">yhat, y</span>):<br>    loss_ = <span class="hljs-number">0</span><br>    <br>    <span class="hljs-keyword">for</span> y_i, yhat_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(y, yhat):<br>        loss_ += (y_i - yhat_i) ** <span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">return</span> loss_ / <span class="hljs-built_in">len</span>(yhat)<br></code></pre></td></tr></table></figure><p>我们有一个 loss, loss 的值先等于 0，我们循环一下 (y, yhat)，然后 loss就加等于 y_i - yhat_i 结果的平方。然后 loss 再除以 len(yhat)。</p><p>这样就是最简单的一种翻译，之前的 loss 函数就可以翻译成这样。</p><p>不过我们要知道另外一种方法，就是 NumPy里提供了一个方法<code>mean()</code>，意思是求平均值。</p><p>假如说里面有 12345：<code>mean(1,2,3,4,5)</code>，就是把 12345这些数字全部加起来，再求它的平均值。</p><p>那我们之前的内容就可以写成<code>mean((yhat-y)**2)</code>,其实就是<code>y_i</code>和<code>yhat_i</code>加起来求个平均值。</p><p>所以，我们就可以将代码写成如下这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">yhat, y</span>):<br>    <span class="hljs-keyword">return</span> np.mean((yhat - y) ** <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p>这个写法就是 NumPy 的广播方法。咱们在之前的 Python篇中有讲到这部分内容，不记得小伙伴可以回头去翻看一下，应该是第 26章，大家可以去看一下，为了顺利进行下去，我们这里再提一下。</p><p>比如说，我们有两个 list：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">vec1</span> = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br><span class="hljs-attr">vec2</span> = [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]<br></code></pre></td></tr></table></figure><p>那么我要进行计算，你会发现会报错：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">vec1 - vec2<br><br>---<br>unsupported operand <span class="hljs-built_in">type</span>(s) <span class="hljs-keyword">for</span> -: <span class="hljs-string">&#x27;list&#x27;</span> <span class="hljs-keyword">and</span> <span class="hljs-string">&#x27;list&#x27;</span><br></code></pre></td></tr></table></figure><p>当然，我们可以使用<code>for</code>进行循环，但是这样的方法未免太过笨重。而NumPy 中就提供了一种广播的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">vec1 = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>vec2 = np.array([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br>vec1 - vec2<br><br>---<br>array([-<span class="hljs-number">3.</span> -<span class="hljs-number">3.</span> -<span class="hljs-number">3</span>])<br></code></pre></td></tr></table></figure><p>将数据改变成 NumPy 的array，其实就是把它进行向量法，这样去做减法就直接可以减了，这就是咱们代码这样改的一个原因。</p><p>接着，咱们要对 w 求偏导：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">partial_w</span>(<span class="hljs-params">x, y, yhat</span>):<br>    <span class="hljs-keyword">return</span> np.array([<span class="hljs-number">2</span> * np.mean((yhat - y) * x[<span class="hljs-number">0</span>]), <span class="hljs-number">2</span> * np.mean((yhat - y) * x[<span class="hljs-number">1</span>])])<br></code></pre></td></tr></table></figure><p>这里，我们的<code>yhat</code>也就是<span class="math inline">\(\haty\)</span>，其实是相当于<span class="math inline">\(w_1 \times x_{i1} +w_2 \times x_{i2} + b\)</span>这一部分，也就是我们预计的值。</p><p>预计的值减去实际的值，再乘上 xi。假如把 rm 和 lstat一起输入进来的话，x 是一个向量，第一个是 rm, 第二个是 x0。</p><p>接下来，对 b 求导也就很好写了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">partial_b</span>(<span class="hljs-params">x, y, yhat</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * np.mean((yhat - y))<br></code></pre></td></tr></table></figure><p>现在已经定义好了线性模型，定义好了loss，定义好了偏导。我们现在就初始化一个 w，一个一行两列的数组，b默认可以是 0，也可以是一个随机值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = np.random.random_sample(size = (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<br>b = np.random.random()<br></code></pre></td></tr></table></figure><p>不过一般来说，w 初始化成一个 normal, 但是 b 一般要初始化成0。至于为什么，咱们大概讲到深度学习的时候详细的来讲。</p><p>然后现在来得到 yhat，这个时候我们就要得到一个 x，w 是有的，b是有的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">yhat = model(x, w, b)<br></code></pre></td></tr></table></figure><p>x怎么求解呢？就需要带一个知识点了，这个知识点就叫做<code>batch training</code>。就是在做机器学习的时候每次取一个或者少数几个数字来进行学习。</p><p>这个是为什么呢？其实理论上是可以把所有的数据一起放进去的，但是在真正的工作中，比方说阿里云里面那个数据那么多，在做模型的时候一下放进去，既存不下，速度还会很慢。所以随机的找几个。那这样，你就可以得到不同的<code>yhat</code>了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">50</span>):<br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm)):<br>        <span class="hljs-comment"># batch training</span><br>        index = random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rm)))<br>        rm_x = rm[index]<br>        lstat_x = lstat[index]<br>        x = np.array([rm_x, lstat_x])<br>        yhat = model(x, w, b)<br>        <span class="hljs-built_in">print</span>(yhat)<br></code></pre></td></tr></table></figure><p>可以得到不同的<code>yhats</code>，因为每次随机取的值不一样。因为他的x 不一样，所以估计出来的 y 也不一样。</p><p>可以加一个 loss(yhat, y)，我们来看一下它的 loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">y = target[index]<br>loss_ = loss(yhat, y)<br><br><span class="hljs-built_in">print</span>(loss_)<br><br>---<br><span class="hljs-number">976.1638310150673</span><br>...<br><span class="hljs-number">1.7070546224396366</span><br>...<br><span class="hljs-number">121.30523219624953</span><br></code></pre></td></tr></table></figure><p>loss 一直比较大，偶尔会出现一个比较小的值，也是昙花一现，因为 w是随机的，就是 loss 一直在随机波动。</p><p>现在咱们要做一件事：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">learning_rate = <span class="hljs-number">1e-5</span><br><br>w = w + -<span class="hljs-number">1</span> * partial_w(x, y, yhat) * learning_rate<br>b = b + -<span class="hljs-number">1</span> * partial_b(x, y, yhat) * learning_rate<br></code></pre></td></tr></table></figure><p>然后我们每 100 下来打印一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> batch % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Epoch: &#123;&#125; Batch: &#123;&#125;, loss: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i, batch, loss_))<br></code></pre></td></tr></table></figure><p>讲到这，线性回归基本上原理就已经讲完了，咱们下节课再见。下节课，咱们先来总结一下本节课内容，然后咱们开讲「逻辑回归」。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231011224909.png&quot;
alt=&quot;茶桁的 AI 秘籍 08&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>07. 机器学习入门 3 - 了解 K-means</title>
    <link href="https://hivan.me/07.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A83%20-%20%E4%BA%86%E8%A7%A3K-means/"/>
    <id>https://hivan.me/07.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A83%20-%20%E4%BA%86%E8%A7%A3K-means/</id>
    <published>2023-10-08T23:30:00.000Z</published>
    <updated>2023-11-25T04:41:13.706Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231008152724.png" /></p><p>Hi，你好。我是茶桁。</p><span id="more"></span><p>我们在机器学习入门已经学习了两节课，分别接触了动态规划，机器学习的背景，特征向量以及梯度下降。</p><p>本节课，我们在深入的学习一点其他的知识，我们来看看 K-means.</p><p>当然，在本节课我们也只是浅尝即止，关于这些内容，后面我们还有更详细的内容等着我们去深入学习。</p><h2 id="淘宝的商品问题">淘宝的商品问题</h2><p>上节课的最后，我们学习的内容是梯度下降，在这里不得不再次强调一下，梯度下降是咱们以后非常非常重要的一个内容。</p><p>那关于今天要讲的 K-means呢，我们还是按照惯例，用一个问题来引入。这个问题也是一个实际问题，并非乱举例。</p><p>在淘宝国际上经常会有一些人，基本是境外人员，会从国外出售违禁商品，国家是不许卖的，这些东西会被要求全部下架。</p><p>这些人就有一个很聪明的方法，他会更换物品的名字。</p><p>比方枪支又叫狗子，或者叫什么野狗。赌博账号叫米科，毒品账号可以叫野狼等等。</p><p>我们把这些话称为黑话、黑词。我们在明处、他们在暗处。整个淘宝假设现在就只知道十几个、二十几个人黑话，想屏蔽他们就很难屏蔽。</p><p>有人就说，我们可以像拦截垃圾邮件一样，我们可以去做记忆训练。</p><p>但是这个时候就有一个情况，你做那个垃圾邮件的时候会发现有很多很多的垃圾邮件可以供你训练，但是在互联网上，这种暗语可能有几千上万，但是在整个淘宝里边，他特别的少，比例特别低，其次经常还会变。也就是我们经常会说到的时效性。</p><p>后来人们想了一个这样的方法：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007230945.png"alt="未命名 -1" /></p><p>第一步选取部分淘宝的商品描述，第二步将文本向量化，也就是把一个一个的文本给它变成一个一个向量。接下来对文本进行聚类。</p><p>之前我们说过，聚类其实没有标准答案。我们就让它聚三类、聚四类、聚五类，可以规定聚多少类。把非常非常多的向量，距离接近的这些向量归为一类。所以对应出来把文本向量化之后，就是把文本接近的词归为同样一类。</p><p>然后我们按照已知的暗语定位商品的类别，获得该商品下词汇频率远高于正常词汇分布的单词。</p><p>比如我们知道“野狗”这个单词是一个黑词，首先让在淘宝里的这些商品进行自由分类，让文本相似的聚集到一块，然后我们看一下包含了”野狗“这个单词这个商品的那一大类里边哪些单词出现的频率比正常情况下出现的高。</p><p>就是说，如果这个人爱说黑话，那么他所说的别的话也更有可能是黑话。</p><p>假如说有一些街头帮派的人，他们爱说一些你听不懂的话，那么你听到了一句这样的话就找到了这样的这个人。把这一类人找到，再每天去监听一下他们说了什么话，他们说的有些话的频率其实比正常情况人群说的高，大概率就是黑话。</p><p>看这个过程，其实对应了我们上一节说说的，机器学习的基本框架。有观察的部分，有提取特征的部分，还有让机器进行聚类，让他去学习的过程。</p><p>这个时候我们又发现，让机器自动去聚类之后我们并没有用那个最终求解出来的f。也就是我们并没有让它去预测新数据，是用老数据，然后让老数据去给它聚类，并没有让这个东西去预测新数据。</p><p>其实我说两个非常重要的工程经验。</p><p>第一个工程经验是，所谓算法工程师并不是说他记住了多少算法，而是他要能把一个实际问题抽象成一个算法问题。</p><p>就比方说淘宝的这个问题，如果没人告诉你的话，你并不知道他是一个聚类问题。另外一些人，就能想到可以用聚类这个方法去解决。这个其实才是最重要的。</p><p>我有很多同学，记住了很多但是不会应用，这就不行。</p><p>第二个重要的点是在真正的工作中，我们的机器学习方法很多时候是作为整个项目的一部分，单靠机器学习很难解决完整项目。</p><p>整个项目它可能很长，有十多步，其中有几步用到了机器学习。而且用到的还是机器学习的中间的一些部分。</p><p>假如我们现在选 10万个淘宝的商品，对他进行了聚类。其实按照标准的机器学习过程，聚类完成之后得再学些新的商品，让现在这个模型对这些新的去进行分类。</p><p>但是我们没有进行这一步，我们把这 10万给它分完类，自动聚完类之后就停了，就进行下面的工作了。这就说明机器学习这个完整的步骤完全可以只使用其中的某几个部分，而不是一定要把这个全程跑完。</p><p>这就是要做一个算法工程师很重要的点。</p><h2 id="k-means">K-means</h2><p>接下来，来看第一个机器学习算法问题。我们把第一个机器学习问题叫做K-means。</p><p>如果给出了很多个点，给出了很多个向量。我们拿比较方便看的二维向量举例。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007232522.png"alt="image-20231007232522070" /></p><p>K-means原理很简单，假如有这么些数据。虽然我不知道最终的分类的结果是什么样的，但是可以告诉你我想把这些数据分成几类。</p><p>假如现在我要把这个数据分成三类，那么就随机产生 3 个点。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007232701.png"alt="image-20231007232701026" /></p><p>第一步是随机产生 k 个点。</p><p>第二步，判断其余待分类的点，离我们随机的 K点哪一个更近一点。每一个待分类点一定能找到一个离的更近的 k 点。</p><p>然后把每一个分类的里面点再重新求一下它的中间值。</p><p>求得了新的中间值之后，把这个新的中间值再做为刚才的 k点，让里边的所有的点去选择到底离哪个 k 最近。</p><p>然后再产生一轮新的 k，当一轮新的 k 和上一轮的 k的距离很接近的时候，我们就说找到了，这个中心点基本上就不变了。我们把这个方法叫做K-means。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007233608.png"alt="image-20231007233608095" /></p><p>这个 means 就是平均值的意思，首先有 k个点，然后把这个图像里面的所有的点分配到这 k 个点，找到离 k个点里面最近的。然后把这些隶属于每个 k点最近的点求平均值，也就是求他的中心点。</p><p>求完中心点，这个中心点又变成了新的 k 值。同样的我们再执行，找离 k最近的那些点，再求平均值。</p><p>这样一轮一轮的，把 k 点这样求出来之后，当上一次的 k 和这一次的 k不怎么变的时候，我们就找到了它的中心值，就把它聚类起来。我们就把它叫做K-means。</p><p>以上是 K-means的原理，接下来，咱们来看一个实际的案例。比如说，咱们想这样一个场景，我们给中国建几个能源中心，给咱们中国的省会设置能源中心。</p><p>OK，我们来看一下，那首先呢，我们需要一组数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">coordination_source = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">&#123;name:&#x27;兰州&#x27;, geoCoord:[103.73, 36.03]&#125;,</span><br><span class="hljs-string">...</span><br><span class="hljs-string">&#123;name:&#x27;澳门&#x27;, geoCoord:[113.54, 22.19]&#125;</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure><blockquote><p>完整数据请查看我源代码<code>Core foundations/07.ipynb</code>，为了篇幅我这里就不放完整的了。</p></blockquote><p>这组数据就是每个省的城市和坐标，我们用正则表达式把这个城市里的名字和数字地址全部找出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-comment"># 创建变量保存</span><br>city_location = &#123;<br>    <span class="hljs-string">&#x27;香港&#x27;</span>: (<span class="hljs-number">114.17</span>, <span class="hljs-number">22.28</span>)<br>&#125;<br><br>test_string = <span class="hljs-string">&quot;&#123;name:&#x27;兰州&#x27;, geoCoord:[103.73, 36.03]&#125;,&quot;</span><br><br>pattern = re.<span class="hljs-built_in">compile</span>(<span class="hljs-string">r&quot;name:&#x27;(\w+)&#x27;,\s+geoCoord:\[(\d+.\d+),\s(\d+.\d+)\]&quot;</span>)<br><br><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> test_string.split(<span class="hljs-string">&#x27;\n&#x27;</span>):<br>    city_info = pattern.findall(line)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> city_info: <span class="hljs-keyword">continue</span><br>    <br>    <span class="hljs-comment"># following: we find the city info</span><br>    <br>    city, long, lat = city_info[<span class="hljs-number">0</span>]<br>    <br>    long, lat = <span class="hljs-built_in">float</span>(long), <span class="hljs-built_in">float</span>(lat)<br>    <br>    city_location[city] = (long, lat)<br><br>city_location<br><br>---<br>&#123;<span class="hljs-string">&#x27;香港&#x27;</span>: (<span class="hljs-number">114.17</span>, <span class="hljs-number">22.28</span>), <span class="hljs-string">&#x27;兰州&#x27;</span>: (<span class="hljs-number">103.73</span>, <span class="hljs-number">36.03</span>)&#125;<br></code></pre></td></tr></table></figure><p>测试字段证明我们这一段正则生效了，现在可以将内容替换成我们之前写的数据变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> test_string.split(<span class="hljs-string">&#x27;\n&#x27;</span>):<br>    ...<br><br>city_location<br><br>---<br>&#123;<span class="hljs-string">&#x27;香港&#x27;</span>: (<span class="hljs-number">114.17</span>, <span class="hljs-number">22.28</span>),<br>...<br> <span class="hljs-string">&#x27;澳门&#x27;</span>: (<span class="hljs-number">113.54</span>, <span class="hljs-number">22.19</span>)&#125;<br></code></pre></td></tr></table></figure><p>找出来之后，就可以求距离了。但是我们需要注意一下，就是城市之间是有一个球面距离，就是在球面同样都是那个纬度，东经60 度和东经 45度，在北纬的这个位置不一样的时候距离差的是挺大的。最极限的时候在两极是0，最宽的是在赤道上，就会很大。所以有一个专门求经纬度距离的式子，就是经纬度在实际中球面距离的一个式子。</p><p>除此之外，还有一些距离公式，比方说余弦距离 (CosineDistance)、欧几里德距离 (Euclidean Distance)、曼哈顿距离 (Manhattandistance or Manhattan length):</p><p><strong>余弦距离</strong> <span class="math display">\[cos(p,q) = \frac{x\times y}{|x|\times|y|} =\frac{\sum_{i=1}^{n}x_iy_i}{\sqrt{\sum_{i=1}^nx_i^2}\sqrt{\sum_{i=1}^ny_i^2}},\]</span></p><p><strong>欧几里德距离</strong> <span class="math display">\[\begin{align*}2 维空间中：&amp; \\d(p,q) &amp; = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2} \\ \\n 维空间中：&amp; \\d(p,q) &amp; = \sqrt{\sum_{i-1}^n(p_i-q_i)^2}  \\ &amp;  = \sqrt{(p_1 -q_1)^2 + (p_2 - q_2)^2+...+(p_n - q_n)^2}\end{align*}\]</span></p><p><strong>曼哈顿距离</strong> <span class="math display">\[\begin{align*}d(p,q) = |p_1-q_1| + |p_2 - q_2|\end{align*}\]</span></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231008133954.png"alt="image-20231008133949057" /></p><p>上图中红线代表曼哈顿距离，绿色代表欧氏距离，也就是直线距离，而蓝色和黄色代表等价的曼哈顿距离。曼哈顿距离——两点在南北方向上的距离加上在东西方向上的距离。</p><p>这些都是不同的求距离的方法。就之前给大家说过，我们可以把对象变成向量，向量要求解距离方法不仅仅只有一种。就像这里，咱们将会使用地理上的球面距离：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">geo_distance</span>(<span class="hljs-params">origin, destination</span>):<br>    lon1, lat1 = origin<br>    lon2, lat2 = destination<br>    radius = <span class="hljs-number">6371</span>  <span class="hljs-comment"># km</span><br><br>    dlat = math.radians(lat2 - lat1)<br>    dlon = math.radians(lon2 - lon1)<br>    a = (math.sin(dlat / <span class="hljs-number">2</span>) * math.sin(dlat / <span class="hljs-number">2</span>) +<br>         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *<br>         math.sin(dlon / <span class="hljs-number">2</span>) * math.sin(dlon / <span class="hljs-number">2</span>))<br>    c = <span class="hljs-number">2</span> * math.atan2(math.sqrt(a), math.sqrt(<span class="hljs-number">1</span> - a))<br>    d = radius * c<br><br>    <span class="hljs-keyword">return</span> d<br></code></pre></td></tr></table></figure><p>这里，咱们涉及到了几个参数，一个是<code>origin</code>,一个是<code>destination</code>，我们是要求这两个参数的距离。都是经度、纬度，类型为<code>tuple of float</code>。然后我们返回一个<code>float</code>。</p><p>比如慕尼黑到柏林的例子：</p><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">origin = (<span class="hljs-number">48.1372</span>, <span class="hljs-number">11.5756</span>)  <span class="hljs-comment"># Munich</span></span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">destination = (<span class="hljs-number">52.5186</span>, <span class="hljs-number">13.4083</span>)  <span class="hljs-comment"># Berlin</span></span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-built_in">round</span>(distance(origin, destination), <span class="hljs-number">1</span>)</span><br>504.2<br></code></pre></td></tr></table></figure><p><code>city_location</code>就是每一个城市的位置，我们就可以通过第三方库<code>networkx</code>画出来。</p><p>不过这里我们需要注意一点，就是如果是现实中文，我们还需要做一项工作，否则会出现乱码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 正常显示中文</span><br><span class="hljs-keyword">from</span> pylab <span class="hljs-keyword">import</span> mpl<br><br>mpl.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>] <span class="hljs-comment"># 默认字体</span><br>mpl.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span> <span class="hljs-comment"># 解决保存图像负号&#x27;-&#x27;显示为方块的问题</span><br></code></pre></td></tr></table></figure><p>然后我们来将图画出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> networkx <span class="hljs-keyword">as</span> nx<br><br>%matplotlib inline<br><br>city_graph = nx.Graph()<br>city_graph.add_nodes_from(<span class="hljs-built_in">list</span>(city_location.keys()))<br>nx.draw(city_graph, city_location, with_labels=<span class="hljs-literal">True</span>, node_size=<span class="hljs-number">30</span>)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231008141458.png"alt="image-20231008141458372" /></p><p>看，一个“公鸡”的骨架是不是就出现在图上了？</p><p>接下来，咱们就需要实现 K-means 的部分了。</p><p>我们需要将经纬度分别放在 x,y 里面：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">all_x = []<br>all_y = []<br><br><span class="hljs-keyword">for</span> _, location <span class="hljs-keyword">in</span> city_location.items():<br>    x, y = location<br>    all_x.append(x)<br>    all_y.append(y)<br></code></pre></td></tr></table></figure><p>然后我们随机找了 k 个 center:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_random_center</span>(<span class="hljs-params">all_x, all_y</span>):<br>    r_x = random.uniform(<span class="hljs-built_in">min</span>(all_x), <span class="hljs-built_in">max</span>(all_x))<br>    r_y = random.uniform(<span class="hljs-built_in">min</span>(all_y), <span class="hljs-built_in">max</span>(all_y))<br>    <br>    <span class="hljs-keyword">return</span> r_x, r_y<br><br>get_random_center(all_x, all_y)<br><br>K = <span class="hljs-number">5</span><br>centers = &#123;<span class="hljs-string">&#x27;&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i+<span class="hljs-number">1</span>): get_random_center(all_x, all_y) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K)&#125;<br></code></pre></td></tr></table></figure><p><code>all_x</code>和<code>all_y</code>是刚刚所有的这些城市的经纬度，让这些城市的经纬度来和每个k 去求距离，现在是让所有的 x,y 去一个一个和 k 去做对比，找到离它最近的k。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">closet_points = defaultdict(<span class="hljs-built_in">list</span>)<br><br><span class="hljs-keyword">for</span> x, y, <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(all_x, all_y):<br>    closet_c, closet_dis = <span class="hljs-built_in">min</span>([(k, geo_distance((x, y), centers[k])) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> centers], key=<span class="hljs-keyword">lambda</span> t: t[<span class="hljs-number">1</span>])    <br>    <br>    closet_points[closet_c].append([x, y])<br></code></pre></td></tr></table></figure><p>找到了 k 之后，我们在所有的图上找到了离 k 最近的这些点，然后再求一个means，在离 k 最近的这些点中求出新的平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">iterate_once</span>(<span class="hljs-params">centers, closet_points, threshold=<span class="hljs-number">5</span></span>):<br>    have_changed = <span class="hljs-literal">False</span><br>    <br>    <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> closet_points:<br>        former_center = centers[c]<br><br>        neighbors = closet_points[c]<br><br>        neighbors_center = np.mean(neighbors, axis=<span class="hljs-number">0</span>)<br><br>        <span class="hljs-keyword">if</span> geo_distance(neighbors_center, former_center) &gt; threshold:<br>            centers[c] = neighbors_center<br>            have_changed = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">pass</span> <span class="hljs-comment">## keep former center</span><br>        <br>    <span class="hljs-keyword">return</span> centers, have_changed<br></code></pre></td></tr></table></figure><p>如果新的中心点和原有的中心点距离大于一个阈值，就把这个 central改成新的值，否则就不改变它。</p><p>然后不断的去监测它，不断的去根据已知的这个 k 来求解。离每个 k最近的这些点，找到这些最近的点，就求解出来新的 means 的center，就是平均的 center。当我们发现这个 center没有改变的时候就停了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">kmeans</span>(<span class="hljs-params">Xs, k, threshold=<span class="hljs-number">5</span></span>):<br>    all_x = Xs[:, <span class="hljs-number">0</span>]<br>    all_y = Xs[:, <span class="hljs-number">1</span>]<br>    <br>    K = k<br>    <br>    centers = &#123;<span class="hljs-string">&#x27;&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i+<span class="hljs-number">1</span>): get_random_center(all_x, all_y) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K)&#125;<br>    <br>    changed = <span class="hljs-literal">True</span><br>    <br>    <span class="hljs-keyword">while</span> changed:<br>        closet_points = defaultdict(<span class="hljs-built_in">list</span>)<br><br>        <span class="hljs-keyword">for</span> x, y, <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(all_x, all_y):<br>            closet_c, closet_dis = <span class="hljs-built_in">min</span>([(k, geo_distance((x, y), centers[k])) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> centers], key=<span class="hljs-keyword">lambda</span> t: t[<span class="hljs-number">1</span>])    <br>            closet_points[closet_c].append([x, y])   <br>            <br>        centers, changed = iterate_once(centers, closet_points, threshold)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;iteration&#x27;</span>)<br><br>    <span class="hljs-keyword">return</span> centers<br></code></pre></td></tr></table></figure><p>然后我们就可以得出来我们的 K-means 迭代出来的情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">kmeans(np.array(<span class="hljs-built_in">list</span>(city_location.values())), k=<span class="hljs-number">5</span>, threshold=<span class="hljs-number">5</span>)<br><br>---<br>iteration<br>iteration<br>iteration<br>iteration<br>iteration<br>iteration<br><br>&#123;<span class="hljs-string">&#x27;1&#x27;</span>: array([<span class="hljs-number">118.14307692</span>,  <span class="hljs-number">37.97923077</span>]),<br> <span class="hljs-string">&#x27;2&#x27;</span>: array([<span class="hljs-number">115.528</span>,  <span class="hljs-number">25.643</span>]),<br> <span class="hljs-string">&#x27;3&#x27;</span>: array([<span class="hljs-number">106.22</span>      ,  <span class="hljs-number">28.16333333</span>]),<br> <span class="hljs-string">&#x27;4&#x27;</span>: array([<span class="hljs-number">99.518</span>, <span class="hljs-number">38.86</span> ]),<br> <span class="hljs-string">&#x27;5&#x27;</span>: array([<span class="hljs-number">91.11</span>, <span class="hljs-number">29.97</span>])&#125;<br></code></pre></td></tr></table></figure><p>我们可以将这些点画出图来，得到五个能源中心的地点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(all_x, all_y)<br>plt.scatter(*<span class="hljs-built_in">zip</span>(*centers.values()))<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231008143742.png"alt="image-20231008143742811" /></p><p>在内蒙有一个，东南沿海有一个，重庆有一个，西藏有一个。</p><p>感觉咱们这个迭代的次数应该是还不够，如果迭代次数再多一些，能源站应该会放到珠三角这个地方。</p><p>这段代码再运行的时间长一点，几个能源中心基本上它会自然而然的变到京津冀江浙沪，还有珠三角和成都重庆这些地方，可能还会有一个在嘉峪关和乌鲁木齐这个地方。</p><p>咱们会发现国家的经济中心的形成，其实是有很强的地理关系，非常的神奇。</p><p>我们这段代码是咱们从头到尾的把原理实现了一遍，其实是我们手写的一个K-means。这样的话，大家就对这个原理就了解多了。当你以后要用到它或者要用它一部分的时候才能知道它的用途。</p><p>其实 K-means也是有一个最大的缺点的，就是最终计算出来的结果会受到初始选取中心的影响。当然，除此之外还有一个缺点，就是它的计算时间复杂度比较高。</p><p>尤其是当每次图形越变化越大的时候，就是越斜越细长就会有越受影响，可以设定一个同样的random seed。</p><p>这个代码，大家之后一定要自己去敲，去运行。这样就会更加彻底，比绝大多数人更加理解。本节课的相关代码可以去课程仓库中去找，课程的相关问题可以留言提问（不负责免费解答其他无关问题）。</p><p>好，那我们机器学习的入门部分，也就随着本节课结束了。下节课开始的很长一段时间之内，咱们可以慢慢的来消化机器学习的相关知识点。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231008152724.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>06. 机器学习入门 2 - 理解特征和向量</title>
    <link href="https://hivan.me/06.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A82%20-%20%E7%90%86%E8%A7%A3%E7%89%B9%E5%BE%81%E5%92%8C%E5%90%91%E9%87%8F/"/>
    <id>https://hivan.me/06.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A82%20-%20%E7%90%86%E8%A7%A3%E7%89%B9%E5%BE%81%E5%92%8C%E5%90%91%E9%87%8F/</id>
    <published>2023-10-06T23:30:00.000Z</published>
    <updated>2023-11-25T04:41:10.881Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007182127.png"alt="茶桁的 AI 秘籍 06" /></p><p>Hi, 你好。我是茶桁。</p><span id="more"></span><p>上一节课，咱们用一个案例引入了机器学习的话题，并跟大家讲了一下「动态规划」。</p><p>那这节课，我们要真正进入机器学习。</p><h2 id="机器学习初探">机器学习初探</h2><p>在正式开始之前，我们来想这样一个问题：我为什么要先讲解「动态规划」，然后再引入机器学习呢？</p><p>原因其实是这样：曾经有一度时间，差不多一九七几年开始，大概有三十四年，动态规划其实可以变成图和树的问题。计算机科学里，图和树其实是占主流的，人们去解决图像的分割，图像的分类，文本的识别，文本的分类问题，有很大一部分都会将其优化为图和树的问题去解决。包括上两节课中我们提到的李开复去解决语音识别的问题，也是拆分成语言树。</p><p>为什么要优化成图和树来解决呢？因为这个技术在当时非常的成熟。但是，因为有了图和树，那么依照我们上一节的分析，树会继续向下细分成更小的树，也就是会形成子图。</p><p>所以，人们就会发现，当我们将问题优化成图和树之后，再使用动态规划，就能让问题加速解决。也就是像我们上节课中所讲的一样。有时候，甚至不用动态规划都无法解决。</p><p>可是动态规划是有局限的，当问题过度复杂的时候，使用动态规划也开始解决不了了。</p><p>这个问题的一个非常经典的案例是一九九几年的时候，当时电子邮件开始兴起，也就催生了一个非常重要的产业，就是垃圾邮件。因为垃圾邮件成本很低，只要有一个服务器，然后不断的发送就可以了。</p><p>比如邮件内容可以写：</p><blockquote><p>因为你经常上网，我获得了你的一些账户密码，我已经将你的一些见不得人的浏览记录都记录下来了，需要你在三天之内，向某个账户转多少多少钱，否则我将公布你的所有记录。</p></blockquote><p>我知道，你们肯定会有人觉得：这么弱智的诈骗邮件都能得逞啊？但其实是能得逞的，垃圾邮件不像诈骗电话，还需要人拨。当然，我知道现在诈骗电话都不需要人值守了，只要有一个电脑连上电话服务，然后AI 会自动打电话，通过 AI合成语音就可以。但是当时那个年代可没有这个，发邮件相对就简单很多，只需要有一个服务器在那不断的发送就行了。</p><p>家在之前的数学课中应该都学过概率了。这里也就涉及到了一个概率问题，我发10000个邮件，哪怕只有一个人会上钩，那我也会挣钱。所以当时发送垃圾邮件是一个很大的产业。</p><p>既然有人为这个东西所困扰，就会有人想着用通过正当的方式去挣钱。当时网易的163，还有美国最早的各种邮箱比如 Hotmail等等，都提供了一个功能是付费提供拦截垃圾邮件的功能。</p><p>当时垃圾邮件可以多到整个互联网上收到的 99.8%的邮件都是垃圾邮件。如果不花钱，基本上都用不了邮箱，因为邮箱地址也是可以随机生成的。</p><p>结果像什么 163，还有 Hotmail等等，要去攻克垃圾邮件，而垃圾邮件要去绕开他们的防锁，要能诈骗到钱。就进行了这样的反复的斗争。</p><p>那这个时候的程序员是怎么做的呢？很简单，他们用的想法也是一样，就是要分析文字，分析语法，把它变成文字树，语法树，变成文本关系。</p><p>我们想想，垃圾邮件规律是不是基本上找不到？只要找到一种规律它马上可以变。就算找到了一种规律，把它写成代码了，但是做垃圾邮件的人很快就可以攻克。</p><p>当时人们就很头疼，完全没有办法。用这种分析的方法，用类似于动态规划等的分析方法解决不了。</p><p>当时哈佛大学有一个老师用了一种方法，叫做基于统计的文本：贝叶斯分析方法，来判断一个邮件是不是垃圾邮件。</p><p>他说，不要人工去定规则，不要人工去分析，去找规则。假如在这里找到 2万个垃圾邮件，然后现在来了一条新的邮件，我不知道内容，但是可以根据以前这2万个垃圾邮件，根据它里边的这个文本的内容，文字，看一下之前的垃圾邮件里出现的次数到底是多少，就可以进行贝叶斯分类。</p><p>也就是说，里面的单词分别在垃圾邮件里出现了多少次，出现次数多不多等等。这个时候就可以给他一个概率，比方说是垃圾邮件的概率是0.7，非垃圾邮件的概率是 0.3，那就可以判定是垃圾邮件。</p><p>在以前，人们都是写一个方法来判断是或不是，而现在则是变成了一种概率。</p><p>结果人们就发现这样非常好做，这样做其实也做不到 100% 正确。虽然做不到100% 正确，但是可以做一个比较高的准确度，可以拦截大部分垃圾邮件。</p><p>而且它可以自动更新，只要把这个程序放这，不断的有垃圾邮件进来，样本库越来越多，接下来再收到新的邮件，就能够知道这个是不是垃圾邮件了。</p><p>当时大家还会融入统计分析方法，后来人们就发现根据原来的这些信息提炼出一些数据，让机器自动或半自动的提炼出一些信息，然后去预测新问题。这个过程就特别像小孩学习的时候，你给他很多知识他自己去学，学完之后去解决没有见过的问题。</p><p>这种解决问题的方法，后来就叫做机器学习。我们就把解决这种问题的整个方法就叫做机器学习。</p><p>之前的这一些内容，也就是咱们机器学习产生的背景。</p><h2 id="特征和向量">特征和向量</h2><p>对于整个世界上的所有东西来说，都是可以被量化的。在管理学上有一个东西叫做<code>if one thing cannot be measure， it cannot be managed</code>。就是一个事情如果不能被量化，它就不能被管理。</p><p>在科学上其实也有一个，笛卡尔当年就说过，如果一个东西不能被量化，那么它就不能被分析。</p><p>比方说一个人，要衡量这个人，要刻画这个人的特点，你可以给出特点。例如说身高一米73，月收入 18000。假如我们用 0 和 1 来表示，到底是男还是女，200910表示的可能是住址编码，28 可能是年龄。</p><p>这个时候我们就会得到一个东西，如果我们把一位男士的信息抽象成这样一个向量。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007181836.jpg" /></p><p>假如有另外一个向量，这个向量我们把叫做 man2。如果 man2 和 man的向量的距离是接近的，我们就知道其实它里边的数值是接近的。因为它的向量的计算值比较小，意味着每一个对应的两个数字之间比较小。</p><p>所以当我们把一个一个的对象能够量化，变成一个向量之后，我们就能够知道哪些向量之间是相似的，哪些对象之间是相似的。</p><p>除此之外，不仅是人，还可以把邮件也处理一下。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007181837.jpg" /></p><p>假如说一封邮件里面包含了 213 个字符，包含了 1 个关键字，标题长度为27，0 个抄送地址。那么邮件也是可以向量化，各种东西都可以被向量化。</p><p>向量化之后就可以有一个什么样的结果呢？假如存在一种函数： <spanclass="math display">\[\begin{align*}f_1 \begin{pmatrix}\overrightarrow {man}  = [\\ \\ 1.75 \\ \\ 18000 \\ \\ 1 \\ \\ 200910 \\ \\ 28 \\ \\]\end{pmatrix}\end{align*}\]</span></p><p>我们来看上面这个式子，这一串数字代表的是我们现实生活中的一个对象。这个函数现在我们虽然不知道它是啥，但是我们知道输入一个向量给到一个函数，这个函数可以产生出不同的东西。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007142100.png"alt="image-20231007142100435" /></p><p>假如这个函数返回 0.75，或者是1.38，负一点几等等。只要是个连续的数字，属于R。这个函数就是一个回归函数，Regression。</p><p>一个函数，它返回出来的只有 +1 和 -1，或者只有 0 和 1两种结果。我们就把它叫做二分类。</p><p>如果返回 3 个值，这三个值加起来等于1，每一个表示的是某个东西的概率，那我们把这个叫做多分类。</p><p>分类函数和回归函数是机器学习里边最典型的两个函数。</p><p>分类函数大家好理解，它给出来的结果表示的是类别的概率。例如说是 1就表示可能是 a 类别，-1 可能是 b 类别。</p><p>0.2、0.7、0.1 表示三类，第二类的概率最大。</p><p>Regression，回归是什么意思呢？</p><p>回归这个词当年其实是一个生物学概念，一个遗传学概念。指的是生物的下一代的特征会更偏向于群体的平均值。</p><p>比方说姚明两米多，一个正常人的身高是 1 米75，那么姚明的儿子的身高大概率会向着 1 米 75这个方向变，而不会变得更高。也就是说姚明的儿子大概率会比姚明低。一个人个子特别矮，他儿子大概率呢会比爸爸高。这个就叫做回归现象。</p><p>与此同时，其实在我们的整个职业发展中也有这样的情况。假如说一个人特别优秀，大概率他儿子不会像他那么优秀，生物学上把这个遗传线叫做回归。</p><p>后来呢生物学家、包括心理学家就发现回归其实本质上是我们的平均值，整体趋势的平均值。所以当时统计学家也用了这个词，他们把群体趋势就叫做Regression，就叫做回归。</p><p>后来在机器学习里，所谓的群体趋势其实就是，假设我们现在有这么多点：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007125558.svg" /></p><p>现在这个群体的趋势假设是这样，我们拟合了一个函数f(x)（红色直线）。那么我们输入一个 f(x)输入到这条直线里面，就可以得到一个实数的输出，这样的过程就叫做Regression。</p><p>当我们输入一个数字的时候，不仅可能会输出概率，可能还会输出一连串东西。</p><p>例如我们现在是一个决策问题，输入了一个情况到一个函数里面，要预测接下来我们该怎么办。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007142408.png"alt="image-20231007142408435" /></p><p>我们输入了行动 1、行动 2、行动 3、行动 4...，我们把这种学习问题叫做sequence，就是序列问题。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007142918.png"alt="image-20231007142917934" /></p><p>比如输入的是 1、2、3、4 这四个人的信息，输出是3、2、4、1，给这四个人排了个序。这种排了个序的事情我们就把它叫做rank，尤其是在推荐系统，在搜索引擎里面用的非常非常多。同理，输入一个Email 其实也可以做这样的事情。</p><p>那我们一起来想一下，假设我们有一个 f(x)，f(x)具体怎么实现先不管。假设存在一个 f(x), 如果我们要让 f(x) 执行一个Regression 的任务，可以想一下，这个 f(x) 可以是在什么场景下。 <spanclass="math display">\[\begin{align*}f(x) \begin{pmatrix}\overrightarrow {email}  = [\\ \\ 213 \\ \\  1 \\ \\ 27 \\ \\ 0 \\ \\]\end{pmatrix}\end{align*}\]</span></p><p>举个例子，比方说要让 f(x) 执行一个 rank任务，它的场景是这里有十封未读邮件，要排个序。要输出哪些邮件最紧急，然后去回复。这就是一个rank 的场景。</p><p>再举一个例子，如果这个 f(x)要执行的任务是一个多分类任务，可以是在什么场景下。就是邮件分组对吧？所以我们可以看到，只要我们可以把一个一个对象表示成向量，当我们有一个函数的时候，就可以执行各种各样的任务了。</p><p>那我现在问，如果这个 f(x) 要做 Regression，可能是哪个场景呢？</p><p>机器学习其实就是反反复复的在做这么一件事情，就是让机器半自动的得到这个f。注意是半自动，它并不能全自动。</p><p>在求解 f 的过程中，我们需要输入一个x，真正的值是多少是有标准答案的。f 算的对还是错，是有标准答案的。</p><p>比方说贝叶斯、SBM、决策树、神经网络，这些其实都是一种f。里边这些关键参数是机器自动获得的，但是到底这个函数类型是什么，是概率式、还是if else 的，还是神经网络，这种形式得人来定。</p><p>f有标准答案，是有对错的。我们把这种有对错的求解函数的方法叫做监督学习。</p><p>为什么有对错就要监督学习呢？就是在整个学习过程中，也就是整个获得 f的过程中，我们会不断的监督他，看他学对了还是学错了。如果学对了就给他沿着正确的方向继续走，如果学错了就要换一个方向。这就叫监督学习。</p><p>除了监督学习之外，还有一种机器学习的方法，是让机器自动去归类。 <spanclass="math display">\[\begin{align*}\overrightarrow {man01} \qquad \overrightarrow {man02} \qquad\overrightarrow {man03} \qquad \overrightarrow {man04} \\\overrightarrow {man05} \qquad \overrightarrow {man06} \qquad\overrightarrow {man07} \qquad \overrightarrow {man08} \\\overrightarrow {man09} \qquad \overrightarrow {man10} \qquad\overrightarrow {man11} \qquad \overrightarrow {man12} \\\overrightarrow {man13} \qquad \overrightarrow {man14} \qquad\overrightarrow {man15} \qquad \overrightarrow {man16} \\\end{align*}\]</span></p><p>假如有很多人，我们希望机器能自动的把这些人根据某些特征自动的进行一个分类。在这个过程中，其实是没有标准答案的，是机器根据这些向量自动分类的。我们把这种学习方式叫做非监督学习，也叫做聚类。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007151705.png"alt="image-20231007151705074" /></p><p>非监督学习的难点就是我们不太好衡量，到底是不是对的，还是错的。万一这个分类不是我们需要的分法，就只能改变参数让它再分一遍了。</p><p>所以，非监督学习不好量化，它的结果只能作为参考，没有标准答案。</p><h2 id="机器学习的通用框架">机器学习的通用框架</h2><p>不管是做什么机器学习，不管是在小公司还是大公司，还是在航空航天局。不管是在哪里，我们都有一个通用的方法。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007155314.png"alt="未命名" /></p><p>首先，observed data,会有一些观察到的数据。观察到的数据之后我们就要进行一件叫做特征提取的事情。特征提取就是把我们观察到的这些数据变成一个一个向量。</p><p>观察到的是路边上的一个一个的人，我们要把这一个一个的人变成一条一条的向量。变成向量之后就要进行所谓的学习。这个学习就是需要根据原来的向量，在人的指导下去优化一些函数，得到一些函数的参数。</p><p>然后这个函数要能预测新的没有见过的，也就是 New Data部分，是没有见过的一些新数据，要能得到结果，图中就是得到y。这是我们整个学习的过程，整个机器学习基本上都是这样的一套流程。</p><p>那什么叫做监督学习呢？监督学习就是在学习的这个过程中，每次为了获得f，会输入一对一对的 x 和 y。y就指的是我们在训练的时候，我们已知的这些数据的 x 以及对应的值。</p><p>通过这些大量对应的值，机器自动去总结规律，抽象规律得到 f。</p><p>监督学习就是在学习的时候我们会给到机器 x 和 y, y 就指的是这个 x对应的值。而非监督学习就不提供这些东西。</p><p>非监督学习只提供 x，经过 x 之间的向量的距离近不近等等，自动的去获得 x的分类。</p><h2 id="梯度下降">梯度下降</h2><p>在这个求解 f的过程中，监督学习的时候有一个非常非常重要的方法叫做「梯度下降」。</p><p>梯度下降是一个非常重要的点，之后的课程中咱们会讲到。</p><p>假如我们有一组 k 和 b，输入一个 x 可以得到一个 y。假如就是 kx+b =y，我们现在其实是想求一组 k 和一组 b，能够使得我们输入任意的 x的时候得到的值都任意的和 y 接近。</p><p>还是拿上边这个函数来讲，比如 f(x) = kx + b, 我们现在想求一组 k 和一组b，让它和 y 的值越接近越好。我们怎么来评价它越接近越好呢？写一个函数<span class="math inline">\(\sum [(kx+b)-y]^2\)</span>，这个我们把它叫做loss 函数：<span class="math inline">\(loss =\sum [(kx+b)-y]^2\)</span>，表示这个值如果越大我们信息差的越多，这个值越小就表示我们信息保留的越好，丢失的越少。</p><p>其实原理就是要获得一组 k 和 b，然后使得 loss 取最小值。为了求得一组 k和 b，让这组 k 和 b能够使我们的函数最接近于我们真实的值，可以给他一个随机值，然后让 loss去给 k 求偏导。</p><p>如果此时此刻求出来的偏导是大于 0 的，就是随着 k 的减小，loss值要减小。如果 loss 对 k 的偏导小于 0，意味着随着 k 的增大，loss要减小。</p><p>那新获得的 k 就等于原来的 k 加上 loss 给 k 求偏导的相反数。 <spanclass="math display">\[k_{2} = k_{1} + (-1) \frac{\partial loss}{\partial k} \times \propto\]</span></p><p>当然我们最后乘上了一个系数，这个系数必须是一个很小的数字，比如说是0.001。这个系数的作用是什么遇到的一些函数，偏导特别大，但是此时我们其实已经很接近那个最优点了，可是偏导特别的垂直。那在这里就要加一个很小的系数控制一下。</p><p>这里要说一下，这个部分不能死记公式，没什么所谓的公式，都是一些比较基础的数学知识。这也就是为什么我之前花那么久来写数学基础的原因。</p><p>另外就是，在数学基础之上，要拿出你的笔和纸，当然平板也可以，要多画画，然后你就懂了。如果这个东西不多动笔，觉得要背下来，劝你趁早别干这行了，也别学了，可以去做个文职的工作，就天天背书就可以了。现在学的这些东西一定是要内化的，一定要拿着笔多练，多敲代码。</p><p>与此类似的，b 也可以做这样的运算，<span class="math inline">\(b_{2} =b_{1} + (-1) \frac{\partial loss}{\partial b} \times\propto\)</span>。这样，经过我们不断地输入 x 和y，就能够慢慢地找到一组最优的 k 和 b了。这个，就是梯度下降所做的事情。</p><p>接下来，咱们就演示一下梯度下降的意义。</p><p>我们现在有一个 loss 函数，这个函数会返回一个运算结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">k</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">3</span> * (k ** <span class="hljs-number">2</span>) + <span class="hljs-number">7</span> * k - <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><p>现在对于 k 的偏导，我们把 2 放下来，那就是 6*k，再加上 7:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">partial</span>(<span class="hljs-params">k</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">6</span> * k + <span class="hljs-number">7</span><br></code></pre></td></tr></table></figure><p>这个就是它的偏导。</p><p>现在给他随机出一个值，为了让数据更明显，我们将范围定在 (-10, 10)之间。顺便给一个很小的系数<code>alpha</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br>k = random.randint(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br>alpha = 1e - <span class="hljs-number">3</span> <span class="hljs-comment"># 0.001</span><br></code></pre></td></tr></table></figure><p>接着，我们来做循环。之前咱们分析过整个式子，直接将其写出来就可以了，最后是打印出k 和 loss(k)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">k = k + (-<span class="hljs-number">1</span>) * partial(k) * alpha<br><span class="hljs-built_in">print</span>(k, loss(k))<br></code></pre></td></tr></table></figure><p>将这一段代码扔到循环里，为了更明显，我们让它循环 100次，完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">k</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">3</span> * (k ** <span class="hljs-number">2</span>) + <span class="hljs-number">7</span> * k - <span class="hljs-number">10</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partial</span>(<span class="hljs-params">k</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">6</span> * k + <span class="hljs-number">7</span><br><br>k = random.randint(-<span class="hljs-number">10</span>,<span class="hljs-number">10</span>)<br>alpha = <span class="hljs-number">1e-3</span> <span class="hljs-comment"># 0.001</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    k = k + (-<span class="hljs-number">1</span>) * partial(k) * alpha<br>    <span class="hljs-built_in">print</span>(k, loss(k))<br><br>---<br>-<span class="hljs-number">9.947</span> <span class="hljs-number">217.19942699999993</span><br>-<span class="hljs-number">9.894317999999998</span> <span class="hljs-number">214.43236005537193</span><br>-<span class="hljs-number">9.841952091999998</span> <span class="hljs-number">211.69839829966944</span><br>-<span class="hljs-number">9.789900379447998</span> <span class="hljs-number">208.99714566241215</span><br>...<br>-<span class="hljs-number">6.064345358065952</span> <span class="hljs-number">57.87843635922653</span><br>-<span class="hljs-number">6.034959285917557</span> <span class="hljs-number">57.01748574662477</span><br>-<span class="hljs-number">6.005749530202052</span> <span class="hljs-number">56.16683554715212</span><br></code></pre></td></tr></table></figure><p>我们可以看到它的值一直在下降，虽然不能直接求解出最好的那个 k是什么，但是通过梯度下降这样的方法，一步一步的慢慢的就找到了这个函数的最小值。</p><p>当我们把循环次数再次提升到 100000 的时候，我们来看看最后的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100000</span>):<br>    k = k + (-<span class="hljs-number">1</span>) * partial(k) * alpha<br>    <span class="hljs-built_in">print</span>(k, loss(k))<br><br>---<br>...<br>-<span class="hljs-number">1.1666666666666852</span> -<span class="hljs-number">14.083333333333332</span><br>-<span class="hljs-number">1.1666666666666852</span> -<span class="hljs-number">14.083333333333332</span><br>-<span class="hljs-number">1.1666666666666852</span> -<span class="hljs-number">14.083333333333332</span><br></code></pre></td></tr></table></figure><p>最后几次打印出的结果基本趋于一致了，k 的值就是-1.1666，那数学里边我们学过，这个二次函数最优值应该是<code>-b/2a</code>，应该是<code>-7/6</code>，我们计算一下看看：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/20231007174350.png"alt="image-20231007174350309" /></p><p>可以看到，和我们梯度下降所求的值很接近，几乎一致。</p><p>这个例子说明通过靠梯度下降，是能够找到一个变量让这个函数取得最小值。</p><p>既然咱们刚才面对这个问题能直接能计算出来它的值是-b/2a = -1.16666...,为什么要用梯度下降的方法来得到这个不精确的值呢？</p><p>我们的这个例子是一个简单函数，可是当函数很复杂的时候，很多复杂的函数我们是求解不出来的。</p><p>好，到这里就是我们这节课的内容。下节课就是我们机器学习入门的最后一节课，我们来谈谈K-means。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/20231007182127.png&quot;
alt=&quot;茶桁的 AI 秘籍 06&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi, 你好。我是茶桁。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
</feed>
