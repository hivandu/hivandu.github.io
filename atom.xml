<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>茶桁.MAMT</title>
  
  <subtitle>ChaHeng Notes，codding and writting ~</subtitle>
  <link href="https://hivan.me/atom.xml" rel="self"/>
  
  <link href="https://hivan.me/"/>
  <updated>2024-01-10T18:23:22.328Z</updated>
  <id>https://hivan.me/</id>
  
  <author>
    <name>Hivan Du</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>04. BI - LightGBM vs CatBoost，具体实现分析</title>
    <link href="https://hivan.me/04.%20BI%20-%20LightGBM%20vs%20CatBoost%EF%BC%8C%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/"/>
    <id>https://hivan.me/04.%20BI%20-%20LightGBM%20vs%20CatBoost%EF%BC%8C%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/</id>
    <published>2024-01-09T23:30:00.000Z</published>
    <updated>2024-01-10T18:23:22.328Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/202401101541800.png"alt="茶桁的AI秘籍 核心BI 04" /></p><span id="more"></span><p>[TOC]</p><p>Hi，你好。我是茶桁。</p><p>那今天我们是来讲解另外两个Boosting的工具，首先是微软出品的LightGBM。</p><h2 id="lightgbm">LightGBM</h2><p>LightGBM 是微软提出来的,是属于XGBoost的升级版，也曾经是Kaggle里面使用模型最多的机器学习的神器。当然，目前LightGBM之外，BERT以及GPT都越来越受关注，但是LightGBM这么久了，依然还是占据一席之地，依然还是某些性质及任务要求下的首选。</p><p>Light的概念就是轻和快，GBM 全称为 Gradient BoostingMachine，这个GBM就把它理解成就是GBDT，所以它其实就是轻量级的GBDT，而且是升级版本。所以我们看一看，它到底做了哪些轻量级的一些操作。</p><p>常用的机器学习算法，例如神经网络等算法，都可以以<code>mini-batch</code>的方式训练，训练数据的大小不会受到内存限制。</p><p>GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小，如果不装进内存，反复地读写训练数据又会消耗非常大的时间。对于工业级海量的数据，普通的GBDT 算法是不能满足其需求的。</p><p>LightGBM 的提出是为了解决 GBDT 在海量数据遇到的问题，让 GBDT可以更好更快地用于工业场景。</p><p>我们看整个的例子，先让大家有个直观的感受。</p><p>我找了四个数据集，然后用 XGBoost, XGBoost_approx 以及 LightGBM来做一个比较. 其中 XGBoost_approx 是2016年左右提出来的 XGBoost的近似版.</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231118000026.png"alt="20231118000026" /></p><p>一共做了两种对比，一种对比是它的内存消耗，看谁的内存占用更小。可以看到LightGBM 明显比 XGBoost 的内存会更小一点。同样的数据集只有大约 1/6左右。</p><p>指标除了内存以外还是要关注一下评价结果，结果上LightGBM和XGBoost差别并不大，甚至有些情况下还会更好。所以在结果差别不大的情况下，内存只有原来的1/6.</p><p>除此之外，训练速度上还做了一个对比</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231118000552.png"alt="20231118000552" /></p><p>这里<code>seconds</code>代表的是时长，这四种训练集里面，XGBoost的2016版比原来2014版速度要快，因为它是近似方法，还记得上节课说的直方图吧？LightGBM明显还比它所有的这些版本都要快，大概快了有1/10。</p><p>所以我们用了 1/10 的速度，用了 1/6的内存，得到了一个还不错的结果，这两个模型也比较相当。这两个模型还可以帮我们来自动处理一些特征的确认值，XGBoost是不支持类别特征的，而LightGBM支持类别特征。</p><p>那么问题来了, 为什么 LightGBM 会更快呢？</p><p>让我们稍微拆解了一下模型复杂度的流程</p><p><span class="math display">\[模型复杂度 = 树的棵数 \times 每颗树的叶子数量 \times 每片叶子生成复杂度\]</span></p><p>树的个数越多就会越复杂，每棵树的叶子的数量越多应该也越复杂。然后再乘上每个叶子节点的生成的复杂度，这个生成的复杂度又会等于特征数量乘上候选的分裂点的数量以及样本的数量。</p><p>LightGBM 看到的这样的一个特点，就想要从这三个维度做一些简化。</p><p>第一个简化，减少分裂点的数量。采用 Histogram 算法。</p><p>这个其实跟 2016年的版本是完全一致的。采用了直方图的方式先减少分类节点数量。</p><p>然后第二个，GOSS 算法。</p><p>这个方法基于梯度的单边采样算法，减少了样本的数量。</p><p>还记得上节课咱们使用 XGBoost 时给大家讲的 subsample 吗？XGBoost里面用的是 0.5，比如原来是有 1 万个样本就用 50%，也就是说只用了 5,000个样本来进行训练。如果是随机的选择 5,000个样本来做训练，对于精度来说是有损失的。目的是希望更快，但是会损失一定的进度。</p><p>第三， EFB 算法。</p><p>它使用互斥特征捆绑算法，减少特征的数量。原来有100个特征，现在随机抽取80个特征，对精度也是有一定损失。</p><p>XGBoost的预排序（ pre-sorted）算法是将样本按照特征取值排序，然后从全部特征取值中找到最优的分裂点位。预排序算法的侯选分裂点数量= 样本特征不同取值个数减1。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231118133706.png"alt="20231118133706" /></p><p>这是一个排序的方法，我们可以按照切分的方式来进行一个顺序的切分。原来的切分的方式每一个地方都可以进行切分，而现在分成了三个桶，就只有两种切分的方法,如下：</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231118134058.png"alt="20231118134058" /></p><p>这样每一个桶就当成了一个整体和一个集合，相对来说，分裂节点的数量就减少了。这种方式其实就是Histogram算法，2016年的XGBoost就是采用的这种算法。这种方式替代了XGBoost原先的pre-sorted 算法。</p><p>因为做了合并，原来是三个样本的<code>gi</code>,现在变成了一个<code>Gi</code>。<code>Gi</code>是求和，这里的<code>Gi</code>就把三个里面一阶导数的梯度做为个累加，就等于<code>0.1</code>。<code>Hi</code>二阶梯度相加就等于<code>0.29</code>。这样就是第一个桶的一个特征，合并以后Histogram变成了三个样本。整体的一阶导数是<code>0.1</code>，二阶导数是<code>0.29</code>，以它来完成运算。</p><p>第二个桶也有三个样本，一阶的导数之和是<code>0.79</code>，二阶导数之和是<code>0.12</code>。第三个桶是两个样本，一阶和二阶导数分别是<code>0.67</code>和<code>0.06</code>。</p><p>那么未来做分割的时候，我们就只要在这个基础上来做分割就好了。因为它前面已经合并成了一个整体，这种方式候选的节点的数量变成了<code>桶的个数-1</code> ，也就是<code>k-1</code>。</p><p>它的思想就是连续的浮点特征值离散化成<code>k</code>个整数，同时构造一个宽度为<code>k</code>的直方图，即将连续特征值离散化到<code>k</code>个<code>bins</code>上。当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</p><p>XGBoost是需要遍历所有离散化的值，LightGBM就只需要遍历k个直方图的值。其侯选分裂点数量就等于<code>k-1</code>。</p><p>除了这种方法以外，LightGBM 还有两种优化策略。GOSS算法的全称是<code>Gradient-based One-Side Sampling</code>,基于梯度的单边采样算法。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231118135829.png"alt="20231118135829" /></p><p>刚才是用 subsample，XGBoost里面专门有一个参数，可以把它设成<code>0.5</code>，也就是50%，它会有精度的下降，会有损失。</p><p>那我们现在来思考一个问题，样本的梯度是大好还是小好呢？我们在机器学习过程中是通过什么来去更新我们的参数？</p><p>机器学习有一种方式叫做梯度下降，梯度下降证明了你学习的方向。如果方向梯度越大，就代表我学习方向越明确。如果梯度已经变成了<code>0.00001</code>，就不好学习到内容。所以对于样本的梯度来说，其实希望它大一点好，可以持续降低。梯度越大就证明学习方向是非常明确的，更容易学到内容。</p><p>GOSS 方法它想到，之前 50% 随机采样，舍弃了50%，如果舍弃的是那些梯度比较大的样本在精度上更容易有损失。所以GOSS就希望先保留那些梯度大的样本，给它设了一个阈值。比如说图例中阈值设为<code>0.1</code>，梯度大于<code>0.1</code>我们就全部保留，因为这些样本它是属于好的样本。</p><p>好样本的梯度怎么理解？</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231118145053.png"alt="20231118145053" /></p><p>我们来看这张图，红色的点是一个样本点。我们的样本在这里，预测出来的值是<span class="math inline">\(y&#39;\)</span>，实际值是 <spanclass="math inline">\(y\)</span>。所以它们之间是有一个方向，<spanclass="math inline">\(y\)</span> 和 <spanclass="math inline">\(y&#39;\)</span> 之间有个方向。用 MSE的话，我们都知道 <span class="math inline">\(MES = (y&#39; -y)^2\)</span>，所以它的梯度就代表了是我要学习下降的一个方向。如果方向越明显其实就更容易去前进，我们再看绿色的点<code>2</code>，其实在后面已经不容易学到内容了。点 <code>1</code>明显梯度会更大一点。</p><p>回到上面那张数据的图上，<code>6</code> 和 <code>7</code>的的<code>gi</code>一个是<code>0.7</code>，一个是<code>0.6</code>，都大于<code>0.1</code>，所以这两个是必须要留下来，因为它还没有学好。剩下的小于<code>0.1</code>的样本，我们就保留了1/3。这是一个随机性的，假设现在选中的是<code>2</code>，<code>4</code>这两个。因为<code>8</code>减去<code>6</code>，<code>7</code>之后还剩下六个样本，六个样本里面的1/3，就是保留两个，我们随机保留了 <code>2</code> 和 <code>4</code>。</p><p>最后结果是我们也保留了 50% 的样本进行采样，但是这 50% 是使用了 GOSS算法计算之后的结果。</p><p>前面的直方图我们是已经计算好了的，所以有三个<code>bin</code>。对于第一个<code>bin</code>的计算,做了<code>1/3</code>的采样,选中的<code>2</code>。可以想成是人大代表，代表3个人。因为是从3个人里面选举出来的这一个人作为代表，这也是3个人的情况。</p><p>那么对于第一个桶来说，里面只抽出来了样本<code>2</code>，<code>2</code> 是一个代表，所以它相当于是 <spanclass="math inline">\(1 \times 3\)</span>，3个样本。样本 <code>2</code>的<code>gi</code>原来是 <spanclass="math inline">\(0.03\)</span>，它的代表相当于是 <spanclass="math inline">\(0.03 \times 3\)</span>，<code>hi</code> 是<code>0.04</code>，同理就是 <span class="math inline">\(0.04 \times 3 =0.12\)</span>。</p><p>那对于第二个桶，是一样的计算方法。<code>6</code>是全部保留，<code>4</code> 代表了3个，所以是 <spanclass="math inline">\(1 \times 3\)</span> 个，那就是 <spanclass="math inline">\(1+1\times 3\)</span>。<code>Gi</code> 就是<code>6</code> 加上 <code>4</code> 乘 <code>3</code>， 那就是 <spanclass="math inline">\(0.7 + 0.05 \times 3 =0.85\)</span>。<code>Hi</code> 也是一样的算法，<spanclass="math inline">\(0.02+0.02 \times 3 = 0.08\)</span>。</p><p>那最后一个桶，<code>bin3</code> 里也是这么计算得来的。</p><p>GOSS 算法的思想是通过样本采样，减少目标函数增益 <code>Gain</code>的计算复杂度。单边采样，只对梯度绝对值较小的样本按照一定比例进行采样，而保留了梯度绝对值较大的样本。因为目标函数增益主要来自于梯度绝对值较大的样本=&gt; GOSS 算法在性能和精度之间进行了很好的权衡。</p><p>那最后，LightGBM 内还包含了一个 EFB算法。刚才咱们的采样可以理解为行采样，就是从10,000个样本减到5,000个采样，EFB 是列采样。 EFB其实是互斥特征绑定法，<code>Exclusive Feature Bunding</code>。</p><p>机器学习过程中，有的时候会用 one-hot 编码把类别特征转化成为<code>0-1</code> 特征。 one-hot 就是将你要的特征变为<code>1</code>，其它变为<code>0</code>。比如说，在一个星期中，我要星期三，那么这组特征就会变成<code>0010000</code>。</p><p>如果用了 one-hot 编码会出现大量稀疏特征。什么叫稀疏，<code>0</code>代表没有，大量为 <code>0</code> 的叫稀疏。这个过程放眼望去肯定是<code>0</code> 多，<code>0</code> 代表空，<code>1</code>代表有价值有数据，所以它是大量的稀疏特征。</p><p>EFB 就发现了这样的一个逻辑：XGBoost有很多人提前做了 one-hot编码，就会有大量稀疏特征。那能不能把这个大量稀疏特征给它合并到一起？这也就是EFB的思想。特征中包含大量稀疏特征的时候，减少构建直方图的特征数量，从而降低计算复杂度。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231120160537.png"alt="20231120160537" /></p><p>比如说第一个 <code>feature</code>，<code>0</code>这个代表为空没有含义，<code>1</code> 和 <code>2</code>有10个值，这是有价值的。</p><p>第二个特征也是一个稀疏特征，有95个是没有的意义，只有 <code>1</code>和 <code>2</code> 是有价值的。</p><p>那作者就想到，能不能把 <code>1</code> 和 <code>2</code>这两个特征合并成一个新的特征，叫 <code>feature 1'</code>。怎么合并？首先<code>feature 1</code> 里的 <code>1</code> 和 <code>2</code>依然把它作为 <code>1</code> 和 <code>2</code> ， <code>feature 2</code>里的 <code>1</code> 和 <code>2</code> 跟 <code>feature 1</code> 里的<code>1</code> 和 <code>2</code>不是一个概念，所以我们要把它做一个新的编码，把它称为 <code>3</code> 和<code>4</code> 。这里的 <code>3</code> 对应出来是原来<code>feature 2</code> 里的 <code>1</code> ， <code>4</code> 对应的是<code>feature 2</code> 里的 <code>2</code> 。</p><p>原来 <code>feature 1</code> 里面是有100个特征的，现在增加了<code>3</code>和<code>4</code>，一个是4个，一个是1个，所以匀了5个特征给后面补进来的部分，整个特征数是不能变的，所以最前面的<code>0</code> 特征，就从90个里面减去了5个，变成了 <spanclass="math inline">\(90-5 =85\)</span>个。这样就把两个特征列合二为一变成一个新的特征列，而且精度没有损失，因为把原来的做成一个等价的还原。</p><p>如果用 EFB方法把两个特征捆绑到一起合成一个新的特征，这个特征没有损失，是可以完全唯一的还原。因为它能发现有大量one-hot稀疏特征很容易进行合并，合并之后就可以让特征的数量大大减少，就不需要用刚才说的设置100 自动的抽 80 列，那样其实信息是有损失的，但是 EFB 不会有损失。</p><h2 id="lightgbm的使用">LightGBM的使用</h2><p>LightGBM的用法也是从引入包开始<code>import lightgbm as lgb</code>，其参数也基本上差不多，我们来看下：</p><ul><li><code>boosting_type</code>，训练方式，gbdt</li><li><code>objective</code>，目标函数，可以是binary，regression</li><li><code>metric</code>，评估指标，可以选择auc,mae，mse，binary_logloss, multi_logloss</li><li><code>max_depth</code>，树的最大深度，当模型过拟合时，可以降低max_depth</li><li><code>min_data_in_leaf</code>，叶子节点最小记录数，默认20</li><li><code>lambda</code>，正则化项，范围为0～1</li><li><code>min_gain_to_split</code>，描述分裂的最小gain，控制树的有用的分裂</li><li><code>max_cat_group</code>，在 group边界上找到分割点，当类别数量很多时，找分割点很容易过拟合时</li><li><code>num_boost_round</code>，迭代次数，通常 100+</li><li><code>num_leaves</code>，默认 31</li><li><code>device</code>，指定cpu 或者 gpu</li><li><code>max_bin</code>，表示 feature 将存入的 bin 的最大数量</li><li><code>categorical_feature</code>，如果 categorical_features =0,1,2， 则列 0，1，2是 categorical 变量</li><li><code>ignore_column</code>，与 categorical_features类似，只不过不是将特定的列视为categorical，而是完全忽略</li></ul><p>Bagging参数：bagging_fraction + bagging_freq（需要同时设置）</p><ul><li><code>bagging_fraction</code>，每次迭代时用的数据比例，用于加快训练速度和减小过拟合</li><li><code>bagging_freq</code>：bagging的次数。默认为0，表示禁用bagging，非零值表示执行k次bagging，可以设置为3-5</li><li><code>feature_fraction</code>，设置在每次迭代中使用特征的比例，例如为0.8时，意味着在每次迭代中随机选择80％的参数来建树</li><li><code>early_stopping_round</code>，如果一次验证数据的一个度量在最近的round中没有提高，模型将停止训练</li></ul><p>那我们比较常见的参数配置如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">params = &#123;<br>    <span class="hljs-string">&#x27;boosting_type&#x27;</span>: <span class="hljs-string">&#x27;gbdt&#x27;</span>,<br>    <span class="hljs-string">&#x27;objective&#x27;</span>: <span class="hljs-string">&#x27;binary&#x27;</span>, <br>    <span class="hljs-string">&#x27;eta&#x27;</span>: <span class="hljs-number">0.01</span>,<br>    <span class="hljs-string">&#x27;max_depth&#x27;</span>: <span class="hljs-number">15</span>,<br>    <span class="hljs-string">&#x27;num_leaves&#x27;</span>: <span class="hljs-number">31</span>,  <span class="hljs-comment"># 根据需要调整</span><br>    <span class="hljs-string">&#x27;colsample_bytree&#x27;</span>: <span class="hljs-number">0.8</span>,<br>    <span class="hljs-string">&#x27;subsample&#x27;</span>: <span class="hljs-number">0.9</span>,<br>    <span class="hljs-string">&#x27;subsample_freq&#x27;</span>: <span class="hljs-number">8</span>,<br>    <span class="hljs-string">&#x27;alpha&#x27;</span>: <span class="hljs-number">0.6</span>,<br>    <span class="hljs-string">&#x27;lambda&#x27;</span>: <span class="hljs-number">0</span>,<br>    <span class="hljs-string">&#x27;device_type&#x27;</span>: <span class="hljs-string">&#x27;cpu&#x27;</span>, <span class="hljs-comment"># 我是M1电脑，所以使用的是CPU</span><br>&#125;<br></code></pre></td></tr></table></figure><p>现在咱们还是用之前员工离职预测的来做一个代码示例，在进行模型训练的时候，这里有一个和XGBoost 类似的地方，之前 XGBoost用的是自己的数据结构<code>DMatrix</code>，在 LightGBM 里也有一个Dataset，也是几乎一样的用法，除了用官方的Dataset方式进行封装之外，训练的时候要用 <code>train</code> 来进行训练,那这是一个官方的版本，不过我们这里用 sklearn提供的版本来使用。为什么要用它而不是官方版本，这是因为sklearn的参数名称都比较统一，比如说我们的机器学习里面有个参数都叫<code>n_estimators</code>, 而官方的 XGBoost 和LightGBM 都称作<code>num_boost_round</code>。为了和其他机器学习方式统一避免麻烦，所以我建议大家还是使用sklearn 里的方式来使用。</p><p>那我们这次为了看 LightGBM 的实际效果，还是使用官方的方法:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">trn_data = lgb.Dataset(X_train, label=y_train)<br>val_data = lgb.Dataset(X_valid, label=y_valid)<br><br>model = lgb.train(params, trn_data, num_boost_round=<span class="hljs-number">100</span>, valid_sets=[val_data], feature_name=<span class="hljs-string">&#x27;auto&#x27;</span>, categorical_feature=<span class="hljs-string">&#x27;auto&#x27;</span>, keep_training_booster=<span class="hljs-literal">False</span>)<br><br>---<br>[LightGBM] [Info] Number of positive: <span class="hljs-number">153</span>, number of negative: <span class="hljs-number">787</span><br>[LightGBM] [Info] Total Bins <span class="hljs-number">1128</span><br>[LightGBM] [Info] Number of data points <span class="hljs-keyword">in</span> the train <span class="hljs-built_in">set</span>: <span class="hljs-number">940</span>, number of used features: <span class="hljs-number">30</span><br>[LightGBM] [Info] [binary:BoostFromScore]: pavg=<span class="hljs-number">0.162766</span> -&gt; initscore=-<span class="hljs-number">1.637790</span><br>[LightGBM] [Info] Start training <span class="hljs-keyword">from</span> score -<span class="hljs-number">1.637790</span><br></code></pre></td></tr></table></figure><p>训练好之后，来看看结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用训练好的模型进行预测</span><br>y_pred = model.predict(X_valid, num_iteration=model.best_iteration)<br><br><span class="hljs-comment"># 将概率值转换为类别标签</span><br>threshold = <span class="hljs-number">0.5</span>  <span class="hljs-comment"># 设置阈值</span><br>y_pred_binary = np.where(y_pred &gt; threshold, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 计算准确率</span><br>accuracy = accuracy_score(y_valid, y_pred_binary)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;准确率：<span class="hljs-subst">&#123;accuracy&#125;</span>&quot;</span>)<br><br>---<br>准确率：<span class="hljs-number">0.864406779661017</span><br></code></pre></td></tr></table></figure><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231120221520.png"alt="20231120221520" /></p><h2 id="catboost">CatBoost</h2><p>后面还有一种方法叫 CatBoost。https://arxiv.org/pdf/1706.09516.pdf</p><p>这个方法只要知道它的一个大概使用情况就好。cat 不是猫，应该叫做catgorical，就是分类的概念。所以它是专门针对分类特征多的情况下提出来的boosting的算法。这个方法不一定效果好，但是它有可能对于分类特征多的数据集有奇效，所以你可以把这个方法作为一个备选，也可以尝试着去用一用，尤其是分类特征比较多的情况。</p><p>这里有一个数据集，是 kaggle 上的一个数据集。2015年航班延误数据，包含分类和数值变量：https://www.kaggle.com/usdot/fight-delays/data。这个数据集大约有500 万条记录，使用 10% 的数据，即 50 万条记录。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231120222008.png"alt="20231120222008" /></p><p>这里有 XGBoost，LightGBM 和 CatBoost,我们可以看一下大家可以看一看，XGBoost 和 LightGBM训练集非常好，但是测试集差很多，这种我们都知道，就是过拟合了。</p><p>但是 CatBoost 对于这个航班延误的数据集的训练结果是84%、88%，测试结果跟它相差不是很大，相比于 XGBoost 和 LightGBM来说，CatBoost 的过拟合程度是最小的，这是它的特点。此外时间最快是LightGBM，其次是 CatBoost，最后是 XGBoost。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231120230530.png"alt="20231120230530" /></p><p>这三种模型我们还对比了一下精度，还是来先看训练时间，最快的依然是Light， Cat 次之，最慢的依然是 XGBoost。那从训练的精度上来看，XGBoost 和LightGBM 差不多，但是 Cat 稍微差一点。不过不要认为 CatBoost就不行，就如之前说的，在一些特殊的分类特征更多的情况下，CatBoost的表现反而是最好的那一个，当然，80%以上的情况下，它的效果都会较差一点。</p><h2 id="catboost-的使用">CatBoost 的使用</h2><p>CatBoost 工具的 Github 地址：https://github.com/catboost/catboost，还有https://catboost.ai/en/docs/</p><p>我们还是可以直接调包去使用，它的模型包跟前面的模型包基本上差别也不是很大。</p><p>构造函数:</p><ul><li><code>learning_rate</code>，学习率</li><li><code>depth</code>， 树的深度</li><li><code>l2_leaf_reg</code>，L2正则化系数</li><li><code>n_estimators</code>，树的最大数量，即迭代次数</li><li><code>one_hot_max_size</code>，one-hot编码最大规模，默认值根据数据和训练环境的不同而不同</li><li><code>loss_function</code>，损失函数，包括Logloss，RMSE，MAE，CrossEntropy，回归任务默认RMSE，分类任务默认Logloss</li><li><code>eval_metric</code>，优化目标，包括RMSE，Logloss，MAE，CrossEntropy，Recall，Precision，F1，Accuracy，AUC，R2</li></ul><p>fit函数参数：</p><ul><li><code>X</code>，输入数据数据类型可以是：list; pandas.DataFrame;pandas.Series</li><li><code>y=None</code></li><li><code>cat_features=None</code>，用于处理分类特征</li><li><code>sample_weight=None</code>，输入数据的样本权重</li><li><code>logging_level=None</code>，控制是否输出日志信息，或者其他信息</li><li><code>plot=False</code>，训练过程中，绘制，度量值，所用时间等</li><li><code>eval_set=None</code>，验证集合，数据类型list(X, y)tuples</li><li><code>baseline=None</code></li><li><code>use_best_model=None</code></li><li><code>verbose=None</code></li></ul><p>那对于员工离职预测这个问题，我做了一版，大家可以看我下面的代码自己进行尝试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">model = cb.CatBoostClassifier(<br>    iterations=<span class="hljs-number">1000</span>, <br>    depth=<span class="hljs-number">7</span>, <br>    learning_rate=<span class="hljs-number">0.01</span>, <br>    loss_function=<span class="hljs-string">&#x27;Logloss&#x27;</span>, <br>    eval_metric=<span class="hljs-string">&#x27;AUC&#x27;</span>,<br>    logging_level=<span class="hljs-string">&#x27;Verbose&#x27;</span>, <br>    metric_period=<span class="hljs-number">50</span><br>)<br><br><span class="hljs-comment"># 得到分类特征的列号</span><br>categorical_features_indices = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(X_train.columns)):<br>    <span class="hljs-keyword">if</span> X_train.columns.values[i] <span class="hljs-keyword">in</span> attr:<br>        categorical_features_indices.append(i)<br><span class="hljs-built_in">print</span>(categorical_features_indices)<br><br>---<br>[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>]<br></code></pre></td></tr></table></figure><p>然后：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=categorical_features_indices)<br><br>---<br><span class="hljs-number">0</span>:test: <span class="hljs-number">0.6390374</span>best: <span class="hljs-number">0.6390374</span> (<span class="hljs-number">0</span>)total: <span class="hljs-number">58.3</span>msremaining: <span class="hljs-number">58.2</span>s<br>...<br><span class="hljs-number">999</span>:test: <span class="hljs-number">0.8101059</span>best: <span class="hljs-number">0.8123977</span> (<span class="hljs-number">700</span>)total: <span class="hljs-number">2.9</span>sremaining: 0us<br><br>bestTest = <span class="hljs-number">0.8123976863</span><br>bestIteration = <span class="hljs-number">700</span><br><br>Shrink model to first <span class="hljs-number">701</span> iterations.<br></code></pre></td></tr></table></figure><p>好，那到这里，关于 Boosting的几种工具就都给大家介绍完了，来简单总结一下，这三种工具，LightGBM效率是最高的，在 Kaggle 比赛中应用多， CatBoost对于分类特征多的数据，可以高效的处理，过拟合程度小，效果好。XGBoost，LightGBM和 CatBoost 的参数都比较多，调参需要花大量时间。 Boosting集成学习包括了AdaBoosting 和 Gradient Boosting， 那 Boosting就只是集成学习中的一种，还有 Bagging 和 Stacking。</p><p>最后留一些问题啊给大家去思考一下，那这些问题大家最好自己去梳理一下，然后写上自己的答案，把你的答案写到本文留言框里，我们来看看谁梳理的最好。</p><ul><li>Thinking 1: XGBoost 与 GBDT 的区别是什么</li><li>Thinking 2: XGBoost 与 LightGBM 的区别是什么</li></ul><p>那除此之外，还有一个问题</p><ul><li>Thinking3：举一个你之前做过的预测例子（用的什么模型，解决什么问题，比如我用LR模型，对员工离职进行了预测，效果如何...你可以在下面留言来说一下。）</li></ul><p>那我们要做的题目：Action1，用我之前给大家使用过的：男女声音识别的数据集：voice.csv</p><p>链接: https://pan.baidu.com/s/1UgXmDZLOpVeXz21-Ebddog?pwd=5t4e提取码: 5t4e --来自百度网盘超级会员v7的分享</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231120233241.png"alt="20231120233241" /></p><p>这个数据集中有 3168 个录制的声音样本，采集的频率范围是 0hz-280hz，已经对数据进行了预处理。</p><p>一共有 21 个属性值，请判断该声音是男还是女。</p><p>最后，使用 Accuracy 作为评价标准。</p><p>那我们之前在 BI 第二课的时候用的是 SVM 的方法进行预测，在这个 Action1中，大家试试用 XGBoost 和 LightGBM 的方式来进行。</p><p>那这个 Action1就是留给大家的一个实操作业，可以去我的代码库中找相关的示例，但是我还是希望大家能自己去做一下，不要一上来就去看参考。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://cdn.jsdelivr.net/gh/hivandu/notes/img/202401101541800.png&quot;
alt=&quot;茶桁的AI秘籍 核心BI 04&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="BI" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/BI/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
    <category term="BI" scheme="https://hivan.me/tags/BI/"/>
    
  </entry>
  
  <entry>
    <title>03. BI - XGBoost</title>
    <link href="https://hivan.me/03.%20BI%20-%20XGBoost/"/>
    <id>https://hivan.me/03.%20BI%20-%20XGBoost/</id>
    <published>2024-01-06T23:30:00.000Z</published>
    <updated>2024-01-07T06:23:01.739Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231117184644.png"alt="20231117184644" /></p><span id="more"></span><p>[TOC]</p><p>Hi，你好。我是茶桁。</p><p>学习总是一个循序渐进的过程，之前两节课的内容中，咱们去了解了LR和SVM在实际项目中是如何使用的，我给大家看了两个项目都是跟分类相关，一个是员工离职预测，一个是男女声音识别。</p><p>其实也能看到，男女声音识别也不一定都要用神经网络，能找到一些关键特征把它转化为结构化的数据你也可以用机器学习来完成预测，而且机器学习的效果还是非常好，基本上都有百分之97，98的准确性。</p><p>那今天这节课主要给大家讲解的是「机器学习的神器」，也是今天最主要的内容。</p><p>这个内容希望大家多去仔细阅读，如果你遇到哪些问题可以给我留言，文章下或者私信都可以，基本上，一些容易解答的问题我都会给予回复，大家保持一个良好的学习的方法。</p><h2 id="集成学习">集成学习</h2><p>这些机器学习的神器都跟集成学习相关，先给大家看一个概念叫集成学习。集成学习就是把多个分类器合到一起，可以把它理解成叫三个臭<strong>裨将</strong>顶个诸葛亮。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231116162328.png"alt="20231116162328" /></p><p>集中学习里面有些策略，Bagging是一种，它像一个袋子一样，数据是放到袋子里面去，叫有放回的抽样方式。这个袋子里面如果你要做一个分类的模型会按照少数服从多数。最简单的就是一个陪审团，看一看大家投票的情况，这是分类问题。回归问题我们要用的是大家的平均值，你预测一下薪酬，他预测一下薪酬，把大家预测结果相加以后除上个数就是求平均值。这些都是一个banging的策略，集中学习把这些大家的结果给合并到一起。</p><p>Stacking叫做堆，什么叫Stacking？上图中下面的部分就是Stacking，我们把它分成两类分类器，分类器1，也就是前面的<code>Classifier</code>做了特征的提取，分类器2,<code>Meta Classifier</code>做了分类的过程。它是属于先后两阶段，先做第一种再做第二种，这是有先后逻辑顺序关系。如果是Bagging是没有先后逻辑关系。它是一个并行方法。你做你的，我做我的，最后我们可以综合起来，这个结果没有先后逻辑关系。而Stacking的话是有一个先后逻辑关系的，这是集成学习的不同种的学习的方式。</p><p>还有一种学习方式的话叫Boosting，Boosting中文可以把它称为叫提升，它也有先后的顺序。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231116163911.png"alt="20231116163911" /></p><p>我们看这张图，原始的数据给了模型，第一个分类器模型做了以后得到一些新的一些数据，再喂给第二个模型，然后再生成一些数据再喂给第三个模型，这三个模型之间是有顺序的。先计算第一个，再计算后面的第二个，再计算第三个，所以这种Boosting的方法是有一些顺序的关系。</p><p>通过Boosting的方式可以把弱分类器结合到一起形成一个强的分类器，这是它的一个Boosting的关系。Boosting有两个比较重要的算法，一个AdaBoost（自适应提升），一个是GradientBoosting（梯度提升）。这两种方法在咱们之前的机器学习课程中都有详细的讲解。</p><p>AdaBoost是使用前面的学习器用简单的模型去适配数据，然后分析错误。然后会给予错误预测的数据更高权重，然后用后面的学习器去修复。</p><p>所以集成学习是有三种模式，Bagging是一种，Stacking是一种，还有就是Boosting。总的来说都是把多个分类器组合起来，会胜过一个分类器。这几中模型之间比较常见的模型是Boosting和Bagging。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231116164758.png"alt="Bagging学习方式" /></p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231116164849.png"alt="Boosting 学习方式" /></p><p>我们对这两个做个对比。</p><ul><li>在结构上，Bagging是基分类器并行处理，而Boosting是串行处理。</li><li>训练集上，Bagging的基分类器训练是独立的，而Boosting的训练集是依赖于之前的模型</li><li>在作用上，Bagging的作用是减少variance，而Boosting在于减少bias。</li></ul><p>并行的方式和串形的方法没有什么特别的好坏之分，如果要去判断也是跟数据相关。我今天讲解的神器是属于最后一种，就是Boosting的方式，所以它应该是一个串形的方法。</p><p>这种分类器里面有很多种，上面我介绍了两个算法，一个是AdaBoost，一个是GradientBoosting，那我们主要看看后面这种算法。这个算法中包含了几个比较重要的工具，有XGBoost、LightGBM、CatBoost以及NGBoost，实际上是对GBDT方法的不同实现，针对同一目标做了不同的优化处理。基本上出现的年限如下：</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231116170231.png"alt="20231116170231" /></p><p>Boosting这种方式典型代表是XGBoost、LightGBM和CatBoost，NGBoost采用的boosting的方法跟前三种boosting不太一样，通常我们机器学习的神器还是指的前面三种。当然，近些年还有一些新的工具，比如H2OGBM，以及TensorFlow BoostedTrees（TFBT），咱们我们不去探讨它们，以后有机会写进阶课程的时候再说。</p><p>XGBoost最早提出来的是2014年，它是由陈天奇提出来的，提出来以后在Kaggle的比赛中是大火，基本上在2014年那个阶段只要你参加机器学习的比赛必用XGBoost，而且第一名基本上都是XGBoost，效果是最好的。</p><p>三年之后在2017年，微软提出来了一个lightGBM的版本，它是站在原来的XGBoost基础上做了一些简化，让它的版本更轻，轻的一个优势就是快。所以LightGBM占用内存更少，速度更快。</p><p>三个月之后俄罗斯的一家公司叫Yandex又做了一个新的版本，叫CatBoost，这家公司你可以把它理解成是俄罗斯的Google，是个科技巨头，也做测速引擎，同时也开源很多的机器学习的工具箱，那我们现在用的CatBoost就是Yandex提出来的一个模型。</p><h2 id="xgboost">XGBoost</h2><p>https://arxiv.org/abs/1603.02754</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231116172913.png"alt="20231116172913" /></p><p>XGBoost是2014年提出来的模型，它本身是基于树的。一般来说用的是CART回归树。这个是一个决策树，这是它的机器学习的模型。</p><p>我们现在要去完成一个预测y值，<code>一个人是否喜欢电子游戏</code>。就是电子游戏的市场跟哪些特征相关，年龄、性别、职业这些特征。前面是我们的X，有很多X，最后是那个y。</p><p>现在如果要建一棵树，用一棵角色树可能会建出来如图的一个过程。先判断他的age是不是小于15岁，如果小于15岁就走左边，再判断他的性别是不是男性，如果是男性我们就认为他会玩两个小时，如果不是男性就是0.1个小时。如果他是大于15岁我们就认为他是-1。</p><p>这个是其中一棵树的一个结果，他的预测是在叶子节点里面会有一个数值，这等于他的输出，所以输出都是在叶子节点里，中间那颗分支都是按照不同的逻辑来做个判断。</p><p>XGBoost它本身是集中学习，其实它背后的那个过程原理叫GBDT，大家先知道就好了，我们今天没有详细展开GBDT，这个是属于它的理论。就是说我有多少棵树一起来学习。就是之前看到那张图上的模型，依照数据流，Model1先去做，做完以后Model2去做，再做完以后Model3去做。它本身的原理就是多棵树相加。</p><p>那GBDT的理论版本是这样，XGBoost是它的工程版本。工程版本的目的是要更加的泛化，所以它主要是在原来GBDT的基础上又加了一个叫做正则化项：</p><p><span class="math display">\[\begin{align*}目标函数 = 损失函数 + 正则化项 \\Obj(\varTheta) = L(\varTheta) + \Omega(\varTheta)\end{align*}\]</span></p><p>这里，<spanclass="math inline">\(L(\varTheta)\)</span>是损失函数，拟合数据。 <spanclass="math inline">\(\Omega(\varTheta)\)</span>是正则化项，惩罚复杂模型。</p><p>我们的目标函数是由损失函数加正则化项。一般我们要判断的是想让它的预测结果和实际值更小，这个叫lossfunctio，之前课程中，我们一直跟loss打交道。多出来的结果叫y',和实际值的y之间, 我们会计算一个损失函数。</p><p>比如说我们要用用MSE做回归值，(y' - y)^2，这等于它loss function。</p><p>所以，正则化项意义就是对我们的叶子节点做了一惩罚项。</p><p><span class="math display">\[\begin{align*}\Omega(f_t) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2\end{align*}\]</span></p><p>整个<span class="math inline">\(\Omega(f_t)\)</span>用于控制树的复杂度，防止过拟合，使得模型更简化，也使得最终的模型的预测结果更稳定。</p><p>这个复杂的公式里，T代表的就是叶子数量，你想，如果你的决策数叶子数量很多，这个数模型就会很复杂。</p><p>w_j 是叶子分数的L2正则项，如果它的叶子的分数也是很大的话，也比较复杂，所以我们希望这棵树简单一点，没有这么多的叶子节点，而且叶子节点的数值也比较小一点。这样就是一个稍微小巧一点的模型。</p><p><spanclass="math inline">\(\gamma\)</span>是加入新叶子节点引入的复杂度代价。</p><p>那为什么要加正则化项呢？我给大家举个场景，你自己体会一下。我们的目标是希望损失函数最小化，比如说我们目标是想要挣更多的钱，有两种人a和b。a月薪是2万块钱，他每天就是朝九晚五，办公室的白领。b是网约车司机，每天早上6点出门，晚上12点回家，他也是月薪2万块钱。</p><p>你想办公室的白领他的模型相对来说比较简单一点，后面我们的系数就是大家不需要太多去努力，大概读出来结果-1，-0.1，+1，+0.1就好了。</p><p>网约车司机他会非常的奔波，很累。可能这个系数抖动比较大，最后得出结果+10，+20， -10， -20等等。</p><p>现在想一想，同样月薪2万块钱，你们希望是做a还是做b呢？我们同样可以得到这样一个结果，是希望是像办公室白领一样轻轻松松可以达到你的lossfunction这样的一个目标，还是希望像网约车司机一样特别的辛苦，很复杂。早上6点出门，晚上是24点回家。那大部分人应该都是a，这逻辑是一样的。</p><p>我们希望我们的那棵树没有那么的复杂，也能达到比较好的效果。所以在我们的目标函数过程中统计了两个代价，一个代价叫做lossfunction，损失代价，还有一个就是模型的代价。模型代价跟谁相关呢？跟模型的叶子数和叶子的分数相关。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231116185017.png"alt="20231116185017" /></p><p>以上就把目标函数的两个过程，损失函数和正则化项给大家讲完了。</p><p>接下来我们就详细的看一看它是怎么去做的。</p><p>预测函数，样本的预测结果=每棵树预测分数之和。</p><p><span class="math display">\[\begin{align*}\hat y_i = \sum^k_{k=1}f_k(x_i)\end{align*}\]</span></p><p>我们对目标函数进行优化</p><p><span class="math display">\[\begin{align*}Obj(\varTheta) &amp; = \sum_il(y_i, \hat y_i)+\sum_k\Omega(f_k) \\\Omega(f) &amp; = \gamma T + \frac{1}{2}\lambda ||w||^2\end{align*}\]</span></p><p>我们在原来的lossfunction里面加了一个正则化项，下面的那个是正则化项的公式，前面是叶子节点的数量，后面是叶子节点的分数。我们希望目标函数最小化，把这个目标函数写成以下的一个过程：</p><p><span class="math display">\[\begin{align*}Obj^t = \sum_{i=1}^n l(y_i, \hat y_i^{t-1} + f_t(x_i))+\Omega(f_t) +constant\end{align*}\]</span></p><p>集成学习的树是由多棵树来完成的，如果你现在做的是t棵树，前面那个结果就是t-1棵树。t-1棵树的结果加上<spanclass="math inline">\(\varDelta\)</span>，也就是<spanclass="math inline">\(f_t(x_i)\)</span>，就说第t棵树的结果。之前咱们说的model1,model2, model3这是三棵树,如果t等于3的话前面两棵树是t-1，预测结果加上第三棵树的预测结果。</p><p>这两个过程我们都是拿它做一个lossfunction的一个组合，再加上正则化项，再加上一个常数项，这等它的目标函数。</p><p>对这个函数改进，进行二阶泰勒展开：</p><p><span class="math display">\[\begin{align*}f(x+\varDelta x) \approx f(x) + f&#39;(x)\varDelta x + \frac{1}{2}f&#39;&#39;(x)\varDelta x^2\end{align*}\]</span></p><p>那关于泰勒展开，我在数学基础篇里有一篇专门来讲这个。现在我们只要知道它是一个定理，这个定理就是说你的变量<spanclass="math inline">\(x+\varDeltax\)</span>可以近似的把它展开出来这样。</p><p><span class="math display">\[\begin{align*}\hat y_i^{(0)} &amp; = 0 \\\hat y_i^{(1)} &amp; = f_1(x_i) = \hat y^{(0)} + f_1(x_i) \\\hat y_i^{(2)} &amp; = f_1(x_i) + f_2(x_i) = \hat y^{(1)} + f_2(x_i) \\\cdots &amp; \\\hat y_i^{(t)} &amp; = \sum_{k=1}^tf_k(x_i) = \hat y_i^{(t-1)} +f_t(x_i)\end{align*}\]</span></p><p>那这个式子就可以这样推理得到。其中<span class="math inline">\(\haty_i^{(t)}\)</span>是第t轮的模型预测，<span class="math inline">\(\haty_i^{(t-1)}\)</span>是保留前t-1轮的模型预测， 而<spanclass="math inline">\(f_t(x_i)\)</span>是加入新的预测函数。</p><p>我们可以做多阶泰勒展开，二阶泰勒展开呢相对简单一点。现在只要知道有这么一个概念，这个概念是做一个近似的过程即可。今天就不去讲这个数学的推导了，关于如何利用数学进行推导，大家回到我数学篇里专门有一篇讲泰勒展开的一节去好好补一下基础。</p><p>那这个过程就还是一个lossfunction，这里就是一个任何的function都是一样的。后面这个f'(x)是一个导数，f'是一阶导数，f''是二阶导数，就是做完一阶以后再去做一阶。</p><p>一阶导数乘上<span class="math inline">\(\varDeltax\)</span>，再加上二阶导数乘上<span class="math inline">\(\varDeltax^2\)</span>，这等于二阶泰勒展开，这是一个定理。那这个定理代入的就是刚才这套过程。</p><p>我们来看定义：</p><p>$$ <span class="math display">\[\begin{align*}g_i &amp; = \partial_{\hat y^{(t-1)}}l(y_i, \hat y^{(t-1)}) \\h_i &amp; = \partial^2_{\hat y^{(t-1)}}l(y_i, \hat y^{(t-1)}) \\Obj^t &amp; \approx \sum_{i=1}^n \left [ l(y_i, \hat y^{(t-1)}) +g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i) \right ] + \Omega(f_t) + constant\end{align*}\]</span> $$</p><p>这里，f(x)就是等于<span class="math inline">\(l(y_i, \haty_i^{(t-1)})\)</span>，后面这个<spanclass="math inline">\(f_t(x_i)\)</span>不就是<spanclass="math inline">\(\varDeltax\)</span>吗，然后f'(x)是定义成了一阶导数，用g来代表，再之后是<spanclass="math inline">\(\varDelta x^2\)</span>，它就是<spanclass="math inline">\(f_t^2(x_i)\)</span>。那个二阶导数用h来代表，前面再把1/2拿过来。</p><p>这样目标函数我们就把它做了个改写，我们把它用二阶泰勒展开做了个改写，中间的一阶导数项用g，二阶导数项用h，所以它是个约等于。</p><p>有了这个流程以后，刚才这是个约等于，是用二阶泰勒展开。还可以再去详细的去看一看,f_t(x_i)，这是第7棵树的结果，因为咱们用的是个决策树，它的结果是在叶子节点，那么叶子节点可以作为定义。它叶子节点假设是w，那它的叶子节点的平方也是w的平方，我们再加上后面的正则化项，正则化项是刚刚我们定义好的<spanclass="math inline">\(\gamma T+\lambda\frac{1}{2}\sum_{i=1}^Tw_j^2\)</span>,这是陈天奇定义好的一个公式。这样一个推导我们还可以再把它去做一个合并的过程,这个过程就不完全展开了，可以自己看一下，我们来看一个完整的推导：</p><p><span class="math display">\[\begin{align*}Obj^t &amp; = \sum_{i=1}^n \left [ g_if_t(x_i) -\frac{1}{2}h_if_t^2(x_i) \right ] + \Omega(f_t) \\&amp; = \sum_{i=1}^n \left [ g_iw_{q(x_i)} + \frac{1}{2} h_iw^2_{q(x_i)}\right ] + \gamma T + \lambda\frac{1}{2}\sum_{i=1}^T w_j^2 \\&amp; = \sum_{j=1}^T \left [\left( \sum_{i\in I_j} g_i \right) w_j +\frac{1}{2} \left ( \sum_{i\in I_j} h_i + \lambda \right ) w_j^2 \right]+ \gamma T\end{align*}\]</span></p><p>T为叶子节点数量， <spanclass="math inline">\(I_j\)</span>定义为每个叶子节点里面的样本集合<spanclass="math inline">\(I_j = \{ i | q(x_i) = j \}\)</span>，<spanclass="math inline">\(f_t(x_i) =w_{q(x_i)}\)</span>即每个样本所在叶子节点索引的分数（叶子权重w）。</p><p>那么我们就可以看到，g是做了一个求和项,h也做了一个求和项。所以我们就把一阶导数的求和用一个大G去表达，<spanclass="math inline">\(G_j = \sum_{i\in I_j}g_i\)</span>，二阶的求和用个大H来做表达<span class="math inline">\(H_j =\sum_{i\in I_j}h_i\)</span>，就是把这个过程用大G和大H来去做一个表达，那我们上面最后那一步的那个复杂公式就可以写成：</p><p><span class="math display">\[\begin{align*}Obj^t = \sum_{j=1}^T \left[ G_jw_j + \frac{1}{2}(H_j + \lambda) w_j^2\right] + \gamma T\end{align*}\]</span></p><p>以上就把它的目标函数做了一个改写,那现在我们是希望这个目标函数是越大越好，还是越小越好？自然是希望它越小越好。那什么时候得到最小值？导数为0的时候，就是对<spanclass="math inline">\(\frac{\partial Obj}{\partialw_j}\)</span>求偏导，，那求偏导就得到：</p><p><span class="math display">\[\begin{align*}\frac{\partial Obj}{\partial w_j} = G_j + (H_j + \lambda)w_j = 0\end{align*}\]</span></p><p>导数等于0的时候，我们就可以求到极值，它等于0的时候我们可以求解得：</p><p><span class="math display">\[\begin{align*}w_j &amp; = - \frac{G_j}{H_j + \lambda} \\Obj &amp; = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j + \lambda} +\gamma T\end{align*}\]</span></p><p>先求得<spanclass="math inline">\(w_j\)</span>之后再将它代入到前面那个公式，我们就可以得到Obj。</p><p>所以要想让目标函数最小，我们可以直接求出来w_j的极值以及最小化的那个Obj。</p><p>有了这个过程之后我们一起看一看，我们的XGBoost是怎么去进行运算的。</p><p>我们的Obj的目标函数也是称为一个叫结构分数（打分函数），我们希望这个结构分数越小越好。越小就代表它这个结构越稳定。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231116233928.png"alt="20231116233928" /></p><p>我们看图，第一个部分，判断ismale为yes的时候的叶子是一个样本，为no的时候是一个样本，那判断age &lt;15为no的时候是三个样本。如果三个样本输出的结果的话，我们的的大G就是三个样本的之和，大H也是这三个样本的h，二阶导数之和。</p><p>Obj是衡量模型好坏的标准，我们希望这个分数越小越好，就是这个数会更加的稳定一些。</p><p>那怎么样去求解这个Obj让它更小？刚才我们已经找到了这个机制，也就是</p><p><span class="math display">\[\begin{align*}Obj &amp; = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j + \lambda} +\gamma T\end{align*}\]</span></p><p>这样Obj会比较好一点。那我们的树要去做分割，大家知道这个学习过程中的树是一点点长出来的，长出来的话叶子节点做分割就会成为一个父亲和孩子的一个结构。那要不要做分割的依据是啥？孩子的Obj应该要更小一点才会更好。所以你要去做的事情我们把它称为叫做一个Gain，Gain就是你分割的一个条件。</p><p><span class="math display">\[\begin{align*}Gain = \frac{1}{2}\left[ \frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R+ \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda}\right ] - \gamma\end{align*}\]</span></p><p>这个式子中的几个部分如下：</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231116235512.png"alt="20231116235512" /></p><p>Gain等于父亲啊减去孩子，也就是分割前的Obj减去分割后的左右Obj。如果说，父亲的Obj减去孩子的Obj等于Gain，那么Gain如果小于0，还要不要做分割？那么要记得，Gain&lt;0,那说明孩子比父亲还不稳定，那这个节点就不做分割，我们要找Gain&gt;0的点。那Gain&gt;0也有很多，我们要找其中最大的来做分割。这是XGBoost的一个过程。</p><p>那这里的可能性多不多我们怎么做？分裂节点的分裂，我们以这五个样本为例：</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231117000741.png"alt="20231117000741" /></p><p>这是一个叶子节点，这叶子节点里面要去给它做分裂，先按照原来的g_i,就是一阶的导数从小到大来做个排序，按照一定的顺序。</p><p>那g1,g4的顺序实际上就是g1比g4要小，后面也是。按照这个顺序来做排序，排序以后，我们现在切分有几种切分的方法？如果是5个样本的话，从最前面和最后面分割毫无意义，我们要做的是从中间将它们一分为二，那无非就是<code>[[1,4], [2, 3], [3, 2], [4, 1]]</code>。所以应该是四种结构。</p><p>我们有四种分裂的可能性，我们要找这种分裂的Obj最小的,或者叫Gain最大的。四种结构我们要求4个Gain,在四个里面去找到一种最大的来去做判断。</p><p>我们知道，我们的样本数有可能很多，一般机器学习有可能有上万个样本。一个节点，最开始原来样本假设有1万个，想想，1万个这样的样本要把它做划分的话，现在还是用从小到大给它规范好，这样的顺序来做划分有多少种划分方式呢？要计算<code>1w-1</code>次，接近1万次，9,999次。</p><p>这只是划分一次，决策树的划分不仅仅分裂一次，分裂完一次以后下个节点还可以再做分裂。所以每次来计算的话，这个计算量相当于是个for循环一样，计算量其实是蛮大的。</p><p>这是我们最开始的XGBoost的版本，对于它的节点划分来说我们要计算<code>1w - 1</code>次,如果它的这个节点的样本是1w的话。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231117151029.png"alt="20231117151029" /></p><p>原始的XGBoost的计算量会比较大，这是在2014年的版本。XGBoost的原理在2014年提出来用的是一种贪心算法。这个贪心是从小到大的顺序来做了一个规范化，其实整个的顺序是有多种可能性的，我们是按照从小到大的顺序。然后去切的过程中，我们也只是看当下自有解，这是贪心计算方法。</p><p>但即使这种计算方法的计算量级也很多，在2016年作者就提出来一种改进的方式叫做histogram。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231117151147.png"alt="20231117151147" /></p><p>它用直方图，其目的就是把多个样本给它捆绑到一起。我们还是要做一个分裂的事情，再看刚才的结果，如果你在叶子节点上有1万个样本，原来是要切分出来9,999刀，现在把这1万个样本用绳子给它捆绑出来128个桶。桶就是一个最小的单位，把前面这些样本都拿绳子捆到一起，后面这个捆到一起，一共有多少桶？128个桶。</p><p>我们如果再去做切分的时候只能在桶与桶之间来做切分，那它的划分的样式有多少种？原来的1万要做9,999次的切分，现在128个桶，在做计算的时候就变成了127次。这种方式是种降维处理，有点类似于像聚类的方式，这样我们的计算量就大大缩减，所以他的计算的时间就会快很多。</p><p>这是XBGoost的一种近似的方法，近似的方法它不代表好，但是它是属于近似最优解，可以用更快的时间提升，基本上快几十倍还是有可能的。</p><p>以上就是XGBoost的原理，我们简单的再总结一下。</p><p>XGBoost是在GBDT多棵集成学习树上面做的优化。多棵学习树可以把它理解成model1+model2+...+modeln,这是原来的集成学习的概念。XGBoost在原有基础上加了正则化项，正则化项的目的是防止过拟合。同时这个正则化项构造的很精巧，它用了一个公式，这个公式带进去以后经过一系列的转化，它的二阶项跟前面的1/2就消掉了。转化以后通过求偏导的方式可以把极值给求出来。前后相减的分裂过程是希望孩子的Obj更小。也就是说我们的父亲的Obj减去孩子的Obj等于Gain，每一项的话都可以进行一个求解，我们希望它的Gain变得更大一点。</p><p>那么怎么做分裂呢？就会有尝试多种分裂的方法，找到一种更最大的分裂方式。在这么多种分裂方法过程中采用的是贪心算法，1万个样本就要切1万减1刀。作者在2016年提出来了更快的方法，就是直方图的方法，这方法可以按照桶的个数来进行划分，所以它是一种近似的方式。</p><p>XGBoost算法的一些特点呢，就是讲树模型的复杂度加入到正则项中，从而避免过拟合，泛化性能好。其损失函数是用泰勒展开去完成的，用到了一阶和二阶导数，可以加快优化速度。它在寻找最佳分割点的时候，采用的是近似贪心算法，用来加速计算。那直方图还可以使用GPU来进行计算，GPU就可以采用并性化的方式来进行计算，所以速度就会比较快。XGBoost不仅支持CART作为基分类器，还支持线性分类器，在使用线性分类器的时候可以使用L1，L2正则化。</p><p>XGBoost有点是速度快、效果好、能处理大规模数据、支持自定义损失函数等，缺点就是算法参数过多，调参复杂，不适合处理超高维度特征数据。</p><p>XGBoost的通用参数：</p><ul><li><code>booster[default=gbtree]</code>，模型选择，gbtree或者gblinear。gbtree使用基于树的模型进行提升计算，gblinear使用线性模型进行提升计算。。</li><li><code>silent[default=0]</code>，缄默方式，0表示打印运行时信息，1表示以缄默方式运行，不打印运行时信息。</li><li><code>nthread[default=缺省值是当前系统可以获得的最大线程数]</code>，XGBoost运行时的线程数。</li><li><code>num_feature</code>，boosting过程中用到的特征个数，XGBoost会自动设置。</li><li><code>eta[default=0.3]</code>，为了防止过拟合，更新过程中用到的收缩步长。在每次提升计算之后，算法会直接获取新特征的权重。eta通过缩减特征的权重使提升计算过程更加保守，取值范围为<code>[0, 1]</code>。</li><li><code>gamma[default=0]</code>,分裂节点时，损失函数减小值只有大于等于gamma节点才分裂，gamma值越大，算法越保守，越不容易过拟合，但性能就不一定能保证，需要tradeoff， 取值范围<code>[0, ∞]</code>。</li><li><code>max_depth[default=6]</code>，树的最大深度，取值范围为<code>[1, ∞]</code>, 典型值为3-10。</li><li><code>min_child_weight[default=1]</code>，一个自己的所有观察值的最小权重和。如果新分裂的节点的样本权重和小于<code>min_child_weight</code>则停止分裂。这个可以用来减少过拟合，但是也不能太高，会导致欠拟合，取值范围为<code>[0, ∞]</code>。</li><li><code>subsample[default=1]</code>,构建每颗树对样本的采样率，如果设置成0.5，XGBoost会随机选择50%的样本作为训练集。</li><li><code>colsample_bytree[default=1]</code>，列采样率，也就是特征采样率。</li><li><code>lambda[default=1, alias:reg_lambda]</code>,L2正则化，用来控制XGBoost的正则化部分</li><li><code>alpha[default=0, alias:reg_alpha]</code>，L2正则化，增加该值会让模型更加收敛。</li><li><code>scale_pos_weight[default=1]</code>,在类别高度不平衡的情况下，将参数设置大于0，可以加快收敛。</li></ul><p>学习目标参数：</p><ul><li><code>objective[default=reg:linear]</code>，定义学习目标，reg:linear，reg:logistic，binary:logistic，binary:logitraw，count:poisson，multi:softmax，multi:softprob，rank:pairwise</li><li><code>eval_metric</code>，评价指标，包括rmse，logloss，error，merror，mlogloss，auc，ndcg，map等</li><li><code>seed[default=0]</code>，随机数的种子</li><li><code>dtrain</code>，训练的数据</li><li><code>num_boost_round</code>，提升迭代的次数，也就是生成多少基模型</li><li><code>early_stopping_rounds</code>，早停法迭代次数</li><li><code>evals</code>：这是一个列表，用于对训练过程中进行评估列表中的元素。形式是evals= [(dtrain,'train'),(dval,'val')]或者是evals = [(dtrain,'train')]，对于第一种情况，它使得我们可以在训练过程中观察验证集的效果</li><li><code>verbose_eval</code>，如果为True，则对evals中元素的评估输出在结果中；如果输入数字，比如5，则每隔5个迭代输出一次nm - <code>learning_rates</code>：每一次提升的学习率的列表</li></ul><p>我们看这个参数量还挺多的，XGBoost里面参数量确实还是比较多的，如果你用到的话可以回头再来看看我这篇文章，当作一个手册来看。默认情况下了，我会教给大家一些比较常见的参数设置，你直接用它就可以。</p><p>我这里还是给大家看一个示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 天猫用户复购预测（XGBoost使用示意）</span><br>X_train, X_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=<span class="hljs-number">.2</span>)<br><br><span class="hljs-comment"># 使用XGBoost</span><br>model = xgb.XGBClassifier(<br>    max_depth = <span class="hljs-number">8</span>, <span class="hljs-comment"># 树的最大深度</span><br>    n_estimators = <span class="hljs-number">1000</span>, <span class="hljs-comment"># 提升迭代的次数，也就是生成多少基模型</span><br>    min_child_weight = <span class="hljs-number">300</span>, <span class="hljs-comment"># 一个子集的所有观察值的最小权重和</span><br>    colsample_bytree = <span class="hljs-number">0.8</span>, <span class="hljs-comment"># 列采样率，也就是特征采样率</span><br>    subsample = <span class="hljs-number">0.8</span>,  <span class="hljs-comment"># 构建每颗树对样本的采样率</span><br>    eta = <span class="hljs-number">0.3</span>, <span class="hljs-comment"># eta通过缩减特征的权重使提升计算过程更加保守，防止过拟合</span><br>    seed = <span class="hljs-number">42</span> <span class="hljs-comment"># 随机数种子</span><br>)<br><br>model.fit(X_train, y_train,<br>          eval_metric=<span class="hljs-string">&#x27;auc&#x27;</span>, <br>          eval_set=[(X_train, y_train), (X_valid, y_valid)],<br>          verbose=<span class="hljs-literal">True</span>,<br>          <span class="hljs-comment"># 早停法，如果auc在10epoch没有进步就stop</span><br>          early_stopping_rounds = <span class="hljs-number">10</span><br>          )<br>model.fit(X_train, y_train)<br>prob = model.predict_proba(test_data)<br></code></pre></td></tr></table></figure><p>比如我们现在创建好了一个model，<code>XGBClassifier</code>，创建好之后我们可以设置参数，比如一些树的深度等:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">param = &#123;<br>    <span class="hljs-string">&#x27;boosting_type&#x27;</span>:<span class="hljs-string">&#x27;gbdt&#x27;</span>, <br>    <span class="hljs-string">&#x27;objective&#x27;</span>:<span class="hljs-string">&#x27;binary:logistic&#x27;</span>, <span class="hljs-comment"># 任务目标</span><br>    <span class="hljs-string">&#x27;eval_metric&#x27;</span>:<span class="hljs-string">&#x27;auc&#x27;</span>, <span class="hljs-comment"># 评估指标</span><br>    <span class="hljs-string">&#x27;eta&#x27;</span>:<span class="hljs-number">0.01</span>, <span class="hljs-comment"># 学习率</span><br>    <span class="hljs-string">&#x27;max_depth&#x27;</span>:<span class="hljs-number">15</span>, <span class="hljs-comment">#树最大深度</span><br>    <span class="hljs-string">&#x27;colsample_bytree&#x27;</span>:<span class="hljs-number">0.8</span>, <span class="hljs-comment">#设置在每次迭代中使用特征的比例</span><br>    <span class="hljs-string">&#x27;subsample&#x27;</span>: <span class="hljs-number">0.9</span>, <span class="hljs-comment">#样本采样比例</span><br>    <span class="hljs-string">&#x27;subsample_freq&#x27;</span>: <span class="hljs-number">8</span>, <span class="hljs-comment">#bagging的次数</span><br>    <span class="hljs-string">&#x27;alpha&#x27;</span>: <span class="hljs-number">0.6</span>, <span class="hljs-comment">#L1正则</span><br>    <span class="hljs-string">&#x27;lambda&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-comment">#L2正则</span><br>&#125;<br></code></pre></td></tr></table></figure><p><code>colsample</code>和<code>subsample</code>,这个分别代表我们的列采样和行采样。设置行采样和列采样是让我们每次训练的时候更加的快一点，更加的轻量一点。这两个参数和树的深度参数，这三个参数都是比较常见的需要设置的参数。此外我们还需要针对你的任务来去做设置任务目标。</p><p>我们以attraction这个题目为例可以看一看怎么用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data = xgb.DMatrix(X_train, label=y_train) <br>valid_data = xgb.DMatrix(X_valid, label=y_valid) <br>test_data = xgb.DMatrix(test) <br><br>model = xgb.train(param, train_data, evals=[(train_data, <span class="hljs-string">&#x27;train&#x27;</span>), (valid_data, <span class="hljs-string">&#x27;valid&#x27;</span>)], num_boost_round = <span class="hljs-number">10000</span>, early_stopping_rounds=<span class="hljs-number">200</span>, verbose_eval=<span class="hljs-number">25</span>) <br><br>predict = model.predict(test_data) <br>test[<span class="hljs-string">&#x27;Attrition&#x27;</span>]=predict <span class="hljs-comment"># 转化为二分类输出 </span><br>test[<span class="hljs-string">&#x27;Attrition&#x27;</span>]=test[<span class="hljs-string">&#x27;Attrition&#x27;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x&gt;=<span class="hljs-number">0.5</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>) <br>test[[<span class="hljs-string">&#x27;Attrition&#x27;</span>]].to_csv(<span class="hljs-string">&#x27;dataset/submit_lgb.csv&#x27;</span>)<br></code></pre></td></tr></table></figure><p>原来的XGBoost还有两种版本,一种版本的话是用它的<code>DMatrix</code>，这属于官方封装好的一个结构。把原来切分好的数据集用DMatrix来做的一个封装，封装好以后再进行训练。所以它是属于一个自己的一个训练的一个数据结构，叫DMatrix。我们以前用训练的话一般用<code>fit</code>，如果你用XGBoost官方版本的话，它写的是<code>train</code>，这是它的一个写法会稍微有一些区别。</p><p>带进去之后，其实后面都是调包的过程，train完以后predict，得到一个结果，最后把这个结果进行输出。</p><p>那我们来去用XGBoost来完成一下上节课我们完成的项目，首先还是数据的一些处理，这个和我们前几节课没有什么不同。主要就是我们要对一个参数进行设置；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">param = &#123;<br>    <span class="hljs-string">&#x27;boosting_type&#x27;</span>: <span class="hljs-string">&#x27;gbdt&#x27;</span>,<br>    <span class="hljs-string">&#x27;objective&#x27;</span>: <span class="hljs-string">&#x27;binary:logistic&#x27;</span>,<br>    <span class="hljs-string">&#x27;eval_metric&#x27;</span>: <span class="hljs-string">&#x27;auc&#x27;</span>,<br>    <span class="hljs-string">&#x27;eta&#x27;</span>: <span class="hljs-number">0.01</span>,<br>    <span class="hljs-string">&#x27;max_depth&#x27;</span>: <span class="hljs-number">15</span>,<br>    <span class="hljs-string">&#x27;colsample_bytree&#x27;</span>: <span class="hljs-number">0.8</span>,<br>    <span class="hljs-string">&#x27;subsample&#x27;</span>: <span class="hljs-number">0.9</span>,<br>    <span class="hljs-string">&#x27;subsample_freq&#x27;</span>: <span class="hljs-number">8</span>,<br>    <span class="hljs-string">&#x27;alpha&#x27;</span>: <span class="hljs-number">0.6</span>,<br>    <span class="hljs-string">&#x27;lambda&#x27;</span>:<span class="hljs-number">0</span><br>&#125;<br></code></pre></td></tr></table></figure><p>这个就比我们之前调用其他模型来进行计算的参数量多了很多。然后我们用它官方的结构DMatrix：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data = xgb.DMatrix(X_train, label=y_train)<br>valid_data = xgb.DMatrix(X_valid, label=y_valid)<br>test_data = xgb.DMatrix(test)<br></code></pre></td></tr></table></figure><p>这个套用就是把<code>X_train</code>，<code>y_train</code>给它放进去，它会封装一个自己的数据结构。所有样本都是一样，放进去训练的话就用自己的数据结构来去做训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model = xgb.train(param, train_data, evals=[(train_data, <span class="hljs-string">&#x27;train&#x27;</span>), (valid_data, <span class="hljs-string">&#x27;valid&#x27;</span>)], num_boost_round=<span class="hljs-number">10000</span>, early_stopping_rounds=<span class="hljs-number">200</span>, verbose_eval=<span class="hljs-number">25</span>)<br></code></pre></td></tr></table></figure><p><code>param</code>是前面设置好的，我们的训练的一些参数设置成一个字典，这是常见的一些配置。训练以后就可以拿这个模型去做预测得到一个预测结果，再把这个结果进行输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">predict = model.predict(test_data)<br>test[<span class="hljs-string">&#x27;Attrition&#x27;</span>] = predict<br><span class="hljs-built_in">print</span>(predict)<br><br>---<br>[<span class="hljs-number">25</span>]train-auc:<span class="hljs-number">0.98897</span>valid-auc:<span class="hljs-number">0.75885</span><br>...<br>[<span class="hljs-number">675</span>]train-auc:<span class="hljs-number">1.00000</span>valid-auc:<span class="hljs-number">0.77299</span><br>[<span class="hljs-number">0.11253858</span> <span class="hljs-number">0.07342984</span> <span class="hljs-number">0.19541897</span> <span class="hljs-number">0.11211961</span> <span class="hljs-number">0.8137899</span>  <span class="hljs-number">0.19079192</span><br>...<br> <span class="hljs-number">0.07080463</span> <span class="hljs-number">0.07864323</span> <span class="hljs-number">0.09115468</span> <span class="hljs-number">0.21122025</span> <span class="hljs-number">0.06211422</span> <span class="hljs-number">0.06264106</span>]<br></code></pre></td></tr></table></figure><p>我们打印的结果来看，发生了过拟合的情况。在做训练过程中，我们加了一个validation，现在train-auc和valid-auc都有一个评分。现在呢，训练集基本满分，但是验证集和它差别很大。</p><p>这种情况下我们就可以调整参数，来防止过拟合状况。那我们首当其冲应该想到的就是eta以及max_depth，深度过大会造成过拟合，eta本来就是为了防止过拟合而在更新过程中用到的收缩步长。</p><p>在进行调整之后，过拟合状况就好多了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">290</span>]train-auc:<span class="hljs-number">0.91738</span>valid-auc:<span class="hljs-number">0.83852</span><br></code></pre></td></tr></table></figure><p>下一节课，我们来看看Boosting的另外一个版本，微软出的LightBGM.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231117184644.png&quot;
alt=&quot;20231117184644&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="BI" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/BI/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
    <category term="BI" scheme="https://hivan.me/tags/BI/"/>
    
  </entry>
  
  <entry>
    <title>02. BI - Project Two, 男女声音识别</title>
    <link href="https://hivan.me/02.%20BI%20-%20%E7%94%B7%E5%A5%B3%E5%A3%B0%E9%9F%B3%E8%AF%86%E5%88%AB/"/>
    <id>https://hivan.me/02.%20BI%20-%20%E7%94%B7%E5%A5%B3%E5%A3%B0%E9%9F%B3%E8%AF%86%E5%88%AB/</id>
    <published>2024-01-02T23:30:00.000Z</published>
    <updated>2024-01-03T08:32:45.879Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231116155948.png"alt="茶桁的AI秘籍 核心BI 02" /></p><span id="more"></span><p>[TOC]</p><p>Hi, 你好。我是茶桁。</p><p>上一节课，咱们用一个员工离职预测的案例来学习了LR和SVM。</p><p>那今天咱们还是来看案例，从案例来入手。那今天的例子会带着大家一起来做一个练习，是一个男男女声音识别的例子。数据集来自于3,168个录音的样本，有些男性和女性，采集了一些特征，特征都是跟频谱相关的，一共有21个属性，去基于这个属性来预测声音是男还是女。指标是以Accuracy为评价指标。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231115184648.png"alt="20231115184648" /></p><p>我们看一看，这个例子我们该怎么去用刚才的模型来解答？可以先看一看要预测是哪一个字段，就是<code>label</code>字段。除了label字段以外，其他的类型都属于我们的特征类型。</p><p>想基于这个特征来预测label思路是啥？先梳理一下思路。我们想想，跟上一节课的流程是一样的，如果对之前的那个离职预测问题能清楚它的结构的话。那这里我们的结构也是先去加载，加在以后去预处理。预处理环节先看看数据长什么样，尤其是那个target，就是这个label标签，平均还是不平均等等。</p><p>如果它是一个非数值类型需要给它做个映射，要采用这个SVM或者是LR这两种模型，跟距离有没有关系？就这个模型的运算流程跟距离有关系吗？是有关系的。</p><p>SVM可以把它理解成是跟平面的距离，就是这个坐标跟超平面的那个距离是有关系的。LR是个分类器，它本身是跟线性回归相关的，它也是一条线，所以它跟距离也有关系。</p><p>这两个模型跟距离计算是有联系的，所以我们需要先做一个归一化的处理，归一化处理以后去调包，调包以后就可以完成预测。就是这样一个任务。</p><p>这个任务我们一起来写写代码，大家可以熟练一下，看看这个流程。</p><p>这是一个csv数据集，<code>voice.csv</code>。老样子，文末有数据集地址。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">dataframe = pd.read_csv(<span class="hljs-string">&#x27;data/voice.csv&#x27;</span>)<br>dataframe.head()<br><br>---<br>meanfreqsdmedianQ25Q75IQRskewkurtsp.entsfm...centroidmeanfunminfunmaxfunmeandommindommaxdomdfrangemodindxlabel<br>...<br><span class="hljs-number">5</span> rows × <span class="hljs-number">21</span> columns<br></code></pre></td></tr></table></figure><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231115185052.png"alt="20231115185052" /></p><p>查看一下数据集的头部，来大概了解一下数据。一共大概有21个词段,最后是label。label现在是male和female。除此之外，还有哪些比较常见的数据探索呢？比如说缺失值个数，是通过isnall加sum来做判断的:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 缺失值个数</span><br><span class="hljs-built_in">print</span>(dataframe.isnull().<span class="hljs-built_in">sum</span>())<br><span class="hljs-comment"># 矩阵的大小</span><br><span class="hljs-built_in">print</span>(dataframe.shape)<br><br>---<br>meanfreq    <span class="hljs-number">0</span><br>            ...<br>label       <span class="hljs-number">0</span><br>dtype: int64<br>(<span class="hljs-number">3168</span>, <span class="hljs-number">21</span>)<br></code></pre></td></tr></table></figure><p>打印出来查看的结果，所有数据没有缺失值。大小是3168个，21个指标，没有问题。也可以只使用<code>shape[0]</code>来查看样本个数。</p><p>然后我们还要查看一下样本个数分别男女各是多少，使用<code>label</code>来做一个判断进行筛选：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;男性个数:&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(dataframe[dataframe.label==<span class="hljs-string">&#x27;male&#x27;</span>].shape[<span class="hljs-number">0</span>]))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;女性个数:&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(dataframe[dataframe.label==<span class="hljs-string">&#x27;female&#x27;</span>].shape[<span class="hljs-number">0</span>]))<br><br>---<br>男性个数:<span class="hljs-number">1584</span><br>女性个数:<span class="hljs-number">1584</span><br></code></pre></td></tr></table></figure><p>男性1,584，女性1,584，所以这个数据是不是比较规整，它属于一个均衡的一个样本，而且它没有缺失值。</p><p>那下面要去建模之前先要把它分割成为特征，就是提取特征列和目标列。目标列可以把它称为叫label列或叫target</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">X = dataframe.iloc[:, :-<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><p>那咱们在<ahref="http://mp.weixin.qq.com/mp/homepage?__biz=MzA4NzE4MDQzMg==&amp;hid=1&amp;sn=4662a6b4305960a2e30a70c26fcefa53&amp;scene=18#wechat_redirect">Python基础课</a>里有教授过Python相关的切片操作，这里面<code>[:, :-]</code>前面一个冒号代表是的取所有行，后面<code>:-1</code>是除了最后一列之外。那为什么要去掉最后一列呢？因为最后一列是label，本来就是我们的目标列，在特征数据集内不应该存在目标。所以我们新的数据集不应该存在这一列。</p><p>那相对的，如果我们是要单独提取一个目标集，那就该反过来写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">y = dataframe.iloc[:, -<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><p>现在X和y就分别是我们的特征列和目标列。在调包之前一个很关键的过程就是把特征和目标提取出来，那么我们就用了iloc的方式，通过-1的方式给他做了个提取。</p><p>这些特征刚才说了，我们在用模型，如果你用LR模型的话跟距离相关，我们还要做什么操作呢？还要给它做一个归一化操作。label现在是male和female，还要给它做一个标签编码方式，还是使用sklearn里面的LabelEncoder，定义一个<code>gender_encoder</code>，用它来做一个fit和transform。fit是先指定我们的标签关系，然后transform来做一个应用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用标签编码</span><br>gender_encoder = LabelEncoder()<br><span class="hljs-built_in">print</span>(y)<br>y = gender_encoder.fit_transform(y)<br><span class="hljs-built_in">print</span>(y)<br><br>---<br><span class="hljs-number">0</span>         male<br>         ...  <br><span class="hljs-number">3167</span>    female<br>Name: label, Length: <span class="hljs-number">3168</span>, dtype: <span class="hljs-built_in">object</span><br>[<span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> ... <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p>我们把前后的<code>y</code>都打印出来查看一下区别。可以看到之前打印出来的是<code>male</code>和<code>female</code>，字母的形式，在操作之后就变成了1和0.所以1代表的是male，0代表的是female。这是我们编码的一个映射。需要把所有的这个类别特征转化成为数值。</p><p>然后在运行机器学习模型之前，尤其是跟距离相关的，我们还需要给它做归一化操作。数据归一化。我们这里用另一种归一化方式:<code>StandardScaler</code>,也是一样的，叫正在分布归一化。都是给它做了一个标准化操作.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 数据归一化</span><br>scaler = StandardScaler()<br><span class="hljs-comment"># 对原时特征进行归一化</span><br>scaler.fit_transform(X)<br><span class="hljs-built_in">print</span>(X)<br><br>---<br>[[-<span class="hljs-number">4.04924806</span>  <span class="hljs-number">0.4273553</span>  -<span class="hljs-number">4.22490077</span> ... -<span class="hljs-number">1.43142165</span> -<span class="hljs-number">1.41913712</span><br>  -<span class="hljs-number">1.45477229</span>]<br>...<br> [-<span class="hljs-number">0.51474626</span>  <span class="hljs-number">2.14765111</span> -<span class="hljs-number">0.07087873</span> ... -<span class="hljs-number">1.27608595</span> -<span class="hljs-number">1.2637521</span><br>   <span class="hljs-number">1.47567886</span>]]<br></code></pre></td></tr></table></figure><p>一样，先定义一个<code>scaler</code>，然后用它去fit和transform我们的X，这个是对原始特征进行归一化。</p><p>归一化以后再把它喂回来，打一下我们的X看一下归一化之后的结果是什么样。正态分布归一化之后，均值就变成了0，所以它有可能小于0，也可能大于0。</p><p>这个数据归一化是因为我们用了一个叫做正态分布归一化，正态分布的话，它的归一化是以0为中心点，下图这样的曲线：</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231115201019.png"alt="20231115201019" /></p><p>这个中心点<spanclass="math inline">\(\mu\)</span>是0，方差为1。所以我们就把它变成了这样的正态分布了，所以有没有小于0的？一定要有的。</p><p>正态分布有一个叫3Sigma原则，正1和-1之间的这个范围大概是68%，这叫1Sigma，2Sigma的话是95%，3Sigma是99.7%。所以它不是一个-1到1的结果，正态分布它是有可能小于-3的，也可能大于3，只是概率比较小。</p><p>如果我不用它，我用<code>MinMaxScaler</code>会有小于0的吗？我们来设置一下，这里scaler改为<code>MinMaxScaler</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 测试归一化</span><br>scaler = MinMaxScaler()<br>X = scaler.fit_transform(X)<br><span class="hljs-built_in">print</span>(X)<br><br>---<br>[[<span class="hljs-number">9.64185977e-02</span> <span class="hljs-number">4.73408557e-01</span> <span class="hljs-number">8.41252523e-02</span> ... <span class="hljs-number">0.00000000e+00</span><br>  <span class="hljs-number">0.00000000e+00</span> <span class="hljs-number">0.00000000e+00</span>]<br> ...<br> [<span class="hljs-number">5.95699639e-01</span> <span class="hljs-number">7.68963896e-01</span> <span class="hljs-number">6.87590032e-01</span> ... <span class="hljs-number">2.50178699e-02</span><br>  <span class="hljs-number">2.50357654e-02</span> <span class="hljs-number">3.75385802e-01</span>]]<br></code></pre></td></tr></table></figure><p>再看一看这个结果, 这个结果有可能小于0吗？不会。</p><p>后面就是用数据集切分。还是一样，20%测试集，给一个随机种子数,我还是使用今年年份2023.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 数据集切分</span><br>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">2023</span>)<br></code></pre></td></tr></table></figure><p>数据切完之后现在要做数据建模了，这里建模你可以用逻辑回归也可以用我们的SVC。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 数据建模</span><br>svc = SVC()<br><span class="hljs-comment"># 模型训练</span><br>svc.fit(X_train, y_train)<br><span class="hljs-comment"># 用训练好的模型进行预测</span><br>y_pred = svc.predict(X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;SVM 预测结果：&#123;&#125;, \n 准确率: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(y_pred, accuracy_score(y_test, y_pred)))<br><br>---<br>SVM 预测结果：[<span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">1</span> ... <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span>], <br> 准确率: <span class="hljs-number">0.9715976331360947</span><br></code></pre></td></tr></table></figure><p>这里用的是个非线性的SVC，创建模型SVC之后就fit，这个就是模型训练。</p><p>之后模型预测是把刚才训练好的这个结果去做一个predict，用训练好的模型进行预测得到我们预测的结果，得到预测结果还要判断一下预测结果的准确性。我们先把结果打印出来，SVM的预测结果。再看一看它的准确率。</p><p>准确率我们用了<code>accuracy_score</code>，帮你计算它的准确性，把测试集的数据和预测的结果<code>y_pred</code>来对比判断一下。可以看到预测的结果，1为男性，0为女性。然后准确率达到了百分之97以上，这个结果还是比较好的。当然，这个数据集也比较的简单。</p><p>好，我们再回过头来说说归一化的问题，和上一节课不同，我们这次使用的是一个正态分布的方式去做归一化。这两个哪个好哪个不好，没有统一的标准。没有说正态分布或者是0-1分布的归一化哪个更好，都可以尝试。找适合的数据集。只不过是让它变得更加标准化，看看是不是方便你去找到它的规律。没有一个特别的规范还说明说该用哪一个不该用哪一个，这两个其实都可以。</p><p>如果真是要说一下区别的话，我个人感觉正态分布更关注于人的一些属性。比如说人的身高、体重这种就比较偏向于正态分布，它更有可能找到好的结果。</p><p>那作为归一化处理，也仅仅是处理特征。我们称呼其为weight,也就是权重。整个流程中，y是不需要进行归一化的。回到刚才的例子里，一共有21个特征，除了最后一个以外的话应该就是20个特征。20个特征里面如果用它原始的数值，比如说它是0到1,000，另外一个是0到10，那它就自带的weight会很高，第一个是第二个的100倍。所以对于X来说，如果不给它做归一化，它的量纲就不统一。那我们就让它的weight都一样，就每个特征它的权重大小都是一致的。然后放到模型里面跟y来做对比就可以。</p><p>我们再换个场景，如果我们做的是一个树模型。大家知道最经典的数模型是CART，如果我们用CART角色树来做分类的话，请问需要提前做归一化操作吗？就是对我们的X都要转化，比如说转化成0-1之间区间范围吗？</p><p>因为数模型的计算原理与距离无关，它的原理是跟距离没有关系的。它跟顺序有关系跟你的大小没有关系，所以对树模型来说的话你做不做对它的结果没有影响。但是对于LR、SVM来说做不做会有影响，因为它的权重不一样。</p><p>好这是刚才我们整个的流程，刚刚就把整个的流程给大家梳理清楚了，现在这道题目跟上节课里那个离职预测的题目基本上是一致的过程。</p><p>这两个例子如果你能看明白，下来自己也能把它跑通，基本上机器学习应该就算入门了。比如说你至少能会调包去使用了，而且对它的流程，过程原理还是清楚的。这个是希望大家能明白它的整个过程原理。</p><p>那现在我们再给大家对比一下刚才我们两个项目讲解的两种分类器。一种叫LR，它的这个速度比较快，比较简单，通常用于我们的工业问题上。因为它速度快、资源少，而且方便调整，这是它的优点。</p><p>缺点是啥，刚才说了有20个特征，男女声音识别有20个特征。那请问LR里面要学的参数量有多少？他要学习的一共就是<code>20+1</code>个。针对这样的模型速度比较快，同样的代价就是容易欠拟合，准确性不高。有可能学的不好。</p><p>我们来看一下LR的准确性如何</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">model = LogisticRegression()<br><br>model.fit(X_train, y_train)<br>y_pred = model.predict(X_test)<br>accuracy_score(y_test, y_pred)<br><br>---<br><span class="hljs-number">0.9692307692307692</span><br></code></pre></td></tr></table></figure><p>可以看到，看到我们这两个模型里面，LR准确性是稍微差一点。虽然也有97%了，但它的准确性是比较差的。</p><p>对于非线性模型为什么它差，是因为它不好发现非线性的特征。那谁可以发现？SVM可以发现。因为SVM的原理就是把低维映射到高维，更容易找到非线性的特征。</p><p>处理非线性的特征同样要做数据归一化处理，因为它跟距离相关，刚才给大家讲过。</p><p>SVM的缺点是啥？效率低，刚才速度快的原因是因为样本数不多，如果样本数变成了10万个再去看一看速度，它可能需要十几秒几十秒。那对于LR来说照样速度会很快。</p><p>缺点二，你要做的是个非线性的映射，但不代表每次都能找到这样的一个映射。好的关系如果没有找到就得到不了很好的结果，所以非线性映射没有统一方案，可能很难找到合适的核函数。</p><p>我们有了四种kernel,这四种kernel都可以尝试。但这四种有可能都不属于最终的解。所以这个kernel没有统一的方案。</p><p>另外我们选择kernel还是有一点小的技巧的。比如说我们的样本数量比较小的情况下用简单的线性核，多的情况下就要用复杂的非线性核。</p><p>每种模型都有自己的适用场景，建议大家未来在工作过程中可以先用简单的模型跑一遍，比如说LR模型。它作为我们预测模型的baseline，baseline我们也把它称为叫做基线。基线就是速度快、简单、效果还可以。不能说好，它的目的不在于好而是在于快，可以拿到一个60分的结果。有了baseline以后再去做复杂的模型，可以知道复杂模型到底好还是不好。</p><p>比如说LR刚才那个模型，97%这是个baseline。用SVM得到98%就可以知道它比baseline要高。如果你直接上了一个复杂模型，我们也无法对比。所以可以先用基线来做一个参考。</p><p>常见预测模型除了刚才说的分类模型以外其实还有树模型。树模型之后会详细给大家介绍，这个模型的模块是主要的内容，因为在未来的比赛过程中或项目过程中想要得到好的结果，还是要用到一些复杂的模型。</p><p>好，基本上，我们利用两个项目就基本介绍完了咱们最基本的LR和SVM。和之前讲解机器学习基础原理不同，我们现在主要是基于案例来看具体我们该怎么应用。</p><p>下一节课，咱们来看看几个机器学习神器。</p><p>链接: https://pan.baidu.com/s/1UgXmDZLOpVeXz21-Ebddog?pwd=5t4e提取码: 5t4e --来自百度网盘超级会员v7的分享</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231120233241.png"alt="20231120233241" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://cdn.jsdelivr.net/gh/hivandu/notes/img/20231116155948.png&quot;
alt=&quot;茶桁的AI秘籍 核心BI 02&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="BI" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/BI/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
    <category term="BI" scheme="https://hivan.me/tags/BI/"/>
    
  </entry>
  
  <entry>
    <title>茶桁的AI秘籍 - 数学篇 PDF发布下载</title>
    <link href="https://hivan.me/Tea%20Truss&#39;s%20AI%20cheats%20math%20PDF%20release%20download/"/>
    <id>https://hivan.me/Tea%20Truss&#39;s%20AI%20cheats%20math%20PDF%20release%20download/</id>
    <published>2024-01-02T11:00:00.000Z</published>
    <updated>2024-01-03T08:30:03.688Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/茶桁的AI秘籍_数学篇.png"alt="茶桁的AI秘籍_数学篇" /></p><span id="more"></span><p>Hi, 大家好。我是茶桁。</p><p>在<ahref="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4NzE4MDQzMg==&amp;action=getalbum&amp;album_id=3035995870421073928&amp;scene=173&amp;subscene=&amp;sessionid=undefined&amp;enterid=0&amp;from_msgid=2648748542&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect">Python篇</a><ahref="https://mp.weixin.qq.com/s/mbnag65xDP-1Ct_81Byn5g">PDF发布</a>后,我又制作了<ahref="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4NzE4MDQzMg==&amp;action=getalbum&amp;album_id=3074770001140400130&amp;from_itemidx=1&amp;from_msgid=2648748768#wechat_redirect">数学篇</a>。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20240103160818.png"alt="20240103160818" /></p><p>在整个连载的过程中, 后台有小伙伴留言说想要电子书,在公众号内进行阅读还是有些不太方便。在平时,茶桁其实很多的知识和内容都是来自于公众号, 当然, 更多的是来自于书本。</p><p>咱们整个数学篇基本上都属于基础理论知识, 概念、公式、推导比较多,但是为了让大家将数学概念和程序关联上, 在其中也涉及到了一些编码的部分,也讲解了人工智能上数学是怎么应用的, 比如在Chapter6中,我就给大家讲解了数学如何运用于AI.</p><p>所有在数学篇内涉及到的程序代码, 也都在代码仓库中可以找到:可以选择一个自己访问顺畅的进行拉取。</p><ul><li><a href="https://github.com/hivandu/AI_Cheats">Github</a></li><li><a href="https://gitee.com/hivandu/ai_cheats">Gitee</a></li></ul><p>这个仓库内包含了目前为止整个《AI秘籍》的全部代码以及数据集,方便小伙伴下来之后进行练习。那仓库中的代码截至运行时间是2023年12月26日,为什么要写这个, 因为有些包里的一些参数会做一些变动,到目前为止这些代码都可正常运行, 假如过若干时间之后大家发现代码运行不了,可以去看看包的官方文档, 也可以后台留言给我进行说明,我会对电子书进行更新。</p><p>由于「数学篇」是一个付费专辑, 所以这本电子书并不是免费发放的,仅针对已经付费的用户发送, 其余小伙伴想要阅读,只能是进行购买了。不过本电子书是长期修正更新的, 在购买之后,如果电子书内容进行了刊正, 则会对其进行更新,并再次发送给已经购买的用户。还望小伙伴们在观看的过程中能够指出我的错误,以便我对本电子书进行维护。</p><p>和制作Python篇的电子书的时候不同, 为了对数学篇进行付费的小伙伴们负责,这次整个数学篇的电子书都一篇一篇进行了修正, 有些篇章更是重新写了一遍.然后用Latex来重新设计和制作排版, 等于是整本书基本上都重构了一遍.</p><p>在修正的过程中才发现,之前很多地方都有错误,有些错误甚至都是有误导性的.在此要跟已经购买过数学篇的小伙伴们说声抱歉.</p><p>当然, 虽然已经尽力的去寻找并且修正,但也许不可避免的仍然还会有些错漏的地方, 在整个重新制作的过程中,主要将经历放在了公式和概念上, 首先是争取对同学们不造成误导,其次是有些地方的推导重新推导了, 争取能更容易进行理解. 在阅读过程中,如果有小伙伴发现了错误或者遗漏, 还望给我留言, 我会尽快修正.</p><p>本电子书加入了版本号, 在每一章节的右下角部分, 如果有更新,版本号都会有所变化. 并且在首页二维码下方的日期也会变化.</p><p>本电子书在<ahref="https://github.com/hivandu/Ebook_AI_Math">Github上公开了源码</a>,不过由于内容还是付费内容, 所其中章节并未上传,仅上传了主文件和一些设置内容.不过对于想研究电子书制作的的小伙伴已经足够了.</p><p>详细电子书内容还请移步<ahref="https://mp.weixin.qq.com/s/kcpPmUpIXBWTJQLVkAuXhQ">公众号原文</a>进行获取.</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20240103160751.png"alt="20240103160751" /></p><p>最后, 感谢大家的支持,特别感谢付费支持的小伙伴们。还望关注「坍缩的奇点」,日后咱们会有一些干货分享给大家, 也会和大家一起来完成一些企业项目,帮助小伙伴们更快的步入到实际工作中去。</p><p>敬请关注「坍缩的奇点」，获取更多人工智能相关教程及资料。</p><p><imgsrc="https://github.com/hivandu/notes/blob/main/img/Capture-2023-11-02-164446.png?raw=true"alt="Capture-2023-11-02-164446" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://cdn.jsdelivr.net/gh/hivandu/notes/img/茶桁的AI秘籍_数学篇.png&quot;
alt=&quot;茶桁的AI秘籍_数学篇&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="Math" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/Math/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>01. BI - Project one, 员工离职预测</title>
    <link href="https://hivan.me/01.%20BI%20-%20%E5%91%98%E5%B7%A5%E7%A6%BB%E8%81%8C%E9%A2%84%E6%B5%8B/"/>
    <id>https://hivan.me/01.%20BI%20-%20%E5%91%98%E5%B7%A5%E7%A6%BB%E8%81%8C%E9%A2%84%E6%B5%8B/</id>
    <published>2023-12-30T23:30:00.000Z</published>
    <updated>2024-01-03T08:32:51.046Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311142107687.png"alt="Alt text" /></p><span id="more"></span><p>[TOC]</p><p>Hi，你好。我是茶桁。</p><p>又是开始了一个新的阶段。我不建议一些没基础的同学直接从这里开始，还是要先去之前的课程里补补基础。有的时候即便依葫芦画瓢的把代码写出来了，但是基本原理不清楚。而有的时候，则可能听都听不懂。</p><p>好了，接下来的课程里，我给大家讲解一下机器学习的预测神器。就是是XGBoost，LightGBM以及CatBoost这样的一些方法。这个方法在后续的机器学习工作中会经常使用到。不光是在工作中使用，如果你打一些比赛，也是一个必知必会的模型，基本上属于TOP3的级别。</p><p>在讲解机器学习神器之前要给大家看一看比较常见的预测的全家桶。</p><p>首先先看一个题目，这个数据是一个「员工离职的预测」，第二个项目是关于「男女声音的识别」。这两个项目里面我们都是要做一些分类任务，所以会有一些分类算法的讲解，包括LR，SVM和KNN。矩阵分解我们会用到，FunkSVD，BiasSVD以及SVD++，还有就是树模型，除了上面我提到的神器之外，还有GBDT以及NGBoost。</p><p>那三种机器学习的神器应该是属于必知必会的内容，未来在工作中基本上大家都掉包就可以。那很多人都知道，我们的工作会被戏称为掉包侠，这个是你入职之后可以调包，但是入职之前面试官经常会考一些问题，考察一下你对原理是不是了解。我建议大家第一次学习的时候还是要了解一下它背后的一些原理，这样使用的时候你会更有感觉。</p><p>我在这第一节课上先做一个调查，就是小伙伴们对这3种方法有了解的同学可以在下面进行留言。那我们还是先要去了解原理再去理解工具的使用。在项目中我们会使用这些工具去解决我们的问题。</p><p>接下来，咱们先看一张图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311142107688.png"alt="Alt text" /></p><p>最后如果要找工作的话，要竞争，背后的这个基础就是算法的原理、工具的使用、简历中的项目经验。有了这样一些基础以后去找工作才会更加有竞争性，尤其现在咱们这个大环境下，属于岗位少，但是竞争的人数多。一个职位可能会有多个人一起去竞争，那面试官一定会考核各个方面的部分，尤其在面试的时候也可能会问到一些算法的原理。</p><p>那首先咱们还是先来看看咱们的预测全家桶。</p><h2 id="项目概要">项目概要</h2><p>先从第一个项目开始</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311142107689.png"alt="Alt text" /></p><p>这个项目是来自于Kaggle开过的一场比赛，大家可以去这里去查看项目本身，也可以在这里下载相关数据集：https://www.kaggle.com/competitions/bi-attrition-predict/</p><p>题目要做的是员工离职预测，有一些员工的属性，这些属性包括了公司、出差信息、工作满意度、投入度、加班等等。大概有31个feature，这31个feature要去预测那个y,target，一会儿可以看一看咱们这个target是什么，大家可以跟我一起来看一下。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311142107691.png"alt="Alt text" /></p><p>这些数据的字段，Age, Attrition,businessTravel等等，那想从这里面的词段里面找到一下我们要预测的那个字段，看一看是哪一个字段。</p><p>我们要做的任务是员工离职预测任务，首先拿到这个题目要思考的问题是我们要去选用怎样的模型。我们当前的任务应该是典型的分类任务，而且是一个2分类问题。我们需要预测它是yes还是no。所以，我们要预测Attrition字段。除此之外，所有此段都属于它的特征，大概是在31个特征维度。</p><p>这些特征一般来说把它分成两种，一种叫做类别，还有一种叫做数值。关于类别和数值有什么区别，我们曾经在机器学习的篇章里给大家有详细的解释。</p><p>来看一看哪些是类别特征，第3个字段businessTravel，应该属于类别特征，是一个三分类的特征。那第四个字段DailyRate，这个可能就是一个数值类型。所以变量可以把它分成类别特征和数值类型特征。这两种之间还是有比较明显的区别的。</p><p>有些时候我们可以看一看，比如WorkLifeBalance, 工作与生活平衡程度,1-4。那咱们分析一下，这样的一个词段是类别特征还是数值类型？有些情况我们看到它也是个数字啊，1-4，从这个个数去看的话我们可以把它看成分类，更接近于类别。但是这种类别特征又有区别于我们以往的类别特征。因为1-4的话，它是有顺序关系的，所以它又具有数值的一些属性、数值特征，就是一个明显的数顺序关系。</p><p>所以可以把它理解成是有顺序关系的类别特征。在处理的过程中可以把它看成四个分类，这是没有问题的。但是在运算逻辑上我们要取它的数值大小，不能把它再去做一遍。所以它是属于一个特殊的一个维度。</p><p>针对这个项目怎么做呢？</p><blockquote><p>常用预测（分类、回归）模型： 1. 分类算法: LR, SVM, KNN 2. 矩阵分解：FunkSVD, BiasSVD, SVD++ 3. FM模型：FM，FFM，DeepFM，NFM，AFM 4. 树模型：GBDT, XGBoost, LightGBM, CatBoost, NGBoost 5. Attention模型：DIN，DIEN，DSIN</p></blockquote><p>我们要看一看预测模型有哪些，典型的机器学习分类器有LR，这个LR应该把它称为叫做logisticregression，逻辑回归。还有SVM和KNN，这些是属于传统机器学习的方法。除了这个以外我们还有一些，像CART决策树，这些也都属于机器学习的方法。</p><p>矩阵分解它也是一种预测模型，在推荐系统里面我会给大家进行讲解，后面会有详细的推荐系统的章节会使用到。</p><p>还有推荐系统里面会使用到的FM模型，矩阵分解和FM模型都是在推荐算法里面。</p><p>树模型呢，跟分类器是一样的，这两个部分都可以做分类任务，也可以做回归任务。</p><p>Attention是属于一种技术，它也是在推荐系统里面。</p><p>整个处理这道问题的流程，脑海中要先想到我们可以采用哪些模型。就是第1个类别分类任务，还有第4个树模型，这些是可以使用的。</p><p>模型要使用的话基本上掉包就可以了，所以大家可以思考一下区别，如果你要得到一个很好的成绩区别在哪？区别是在于模型的使用还是在于特征工程的一个拆解？</p><p>模型使用基本上如果参加项目的时候都掉包，而且参数可能也不会有太大的变化，所以区别应该是在于前面，就是特征工程，好的特征工程是拿分的关键。</p><p>在工作里面也是一样，这31个特征能不能再做一些组合，看看哪些特征之间给你更多的一些启发。所以特征工程在工作中、在项目的比赛中都很关键。这个先记住，这是它的特征工程重要性。</p><p>那我们先看一看模型。第一类的模型像LR，SVM和KNN。先通体上介绍一下，我们怎么去使用这些模型来进行求解。</p><h2 id="数据集处理">数据集处理</h2><p>拿到这个程序，我们前面这个模块是要读取数据，有训练集和测试集，把数据读取进来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br>train = pd.read_csv(<span class="hljs-string">&#x27;dataset/train.csv&#x27;</span>, index_col=<span class="hljs-number">0</span>)<br>test = pd.read_csv(<span class="hljs-string">&#x27;dataset/test.csv&#x27;</span>, index_col=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>在模型调包之前，还有一项任务是要做数据的探索。数据探索的目的就是看一看这个数据长什么样，给你直观的一个感受。最直接的就是看label，比如<code>Attrition</code>里面它到底长什么样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(train[<span class="hljs-string">&#x27;Attrition&#x27;</span>].value_counts())<br><br>---<br>Attrition<br>No     <span class="hljs-number">988</span><br>Yes    <span class="hljs-number">188</span><br>Name: count, dtype: int64<br></code></pre></td></tr></table></figure><p>非离职的988，离职的188。所以我们可以看到，这个二分类应该是属于不平衡的。就是不离职的类别特征会更多一点，而且它还不是数值类型，如果不是数值类型还要做一个映射处理，所以咱们下面做一个映射处理。yes变为1，no变为0，这样就会把它转化成数值类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 处理Attrition 字段</span><br>train[<span class="hljs-string">&#x27;Attrition&#x27;</span>] = train[<span class="hljs-string">&#x27;Attrition&#x27;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x==<span class="hljs-string">&#x27;Yes&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>还有一些数据探索比较关键是看一看我们的缺失值的个数。这个我们用<code>isna.sum()</code>来进行判断。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 查看数据中每列是否有空值</span><br><span class="hljs-built_in">print</span>(train.isna().<span class="hljs-built_in">sum</span>())<br><br>---<br>Age                         <span class="hljs-number">0</span><br>...<br>YearsWithCurrManager        <span class="hljs-number">0</span><br>dtype: int64<br></code></pre></td></tr></table></figure><p>可以看到数据还是比较完善的，没有什么缺失值。如果缺失值的话在模型预测之前可能需要做一个缺失值补全的任务。</p><p>在探索过程中能发现一个问题，就是有个字段叫<code>EmployeeNumber</code>，这个字段是员工号码，对我们的这个判断来说这个字段应该是没有意义的。为什么没有意义我们可以通过valuecounts来看一看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(train[<span class="hljs-string">&#x27;EmployeeNumber&#x27;</span>].value_counts())<br><br>---<br>EmployeeNumber<br><span class="hljs-number">1938</span>    <span class="hljs-number">1</span><br>...<br><span class="hljs-number">954</span>     <span class="hljs-number">1</span><br>Name: count, Length: <span class="hljs-number">1176</span>, dtype: int64<br></code></pre></td></tr></table></figure><p>她没有太多其他维度的特征，所以这个字段在入模之前可以给它drop掉。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train = train.drop([<span class="hljs-string">&#x27;EmployeeNumber&#x27;</span>], axis=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>还有，<code>StandardHours</code>觉得有意义吗? 它都等于80,一共是1,176个人，所有值都一样。那我们一样，可以给它去掉。就变成:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train = train.drop([<span class="hljs-string">&#x27;EmployeeNumber&#x27;</span>, <span class="hljs-string">&#x27;StandardHours&#x27;</span>], axis=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>这里，我们对训练集和测试集采用同样的操作，直接给它drop掉了。</p><p>这其中的属性<code>axis=1</code>是表示按列的方式进行操作。</p><p>我们找一找分类特征, 分类特征有一种方式,我们可以将它的info属性打印出来看看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">train.info()<br><br>---<br>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;pandas.core.frame.DataFrame&#x27;</span>&gt;<br>Index: <span class="hljs-number">1176</span> entries, <span class="hljs-number">1374</span> to <span class="hljs-number">684</span><br>Data columns (total <span class="hljs-number">35</span> columns):<br> <span class="hljs-comment">#   Column                    Non-Null Count  Dtype </span><br>---  ------                    --------------  ----- <br> <span class="hljs-number">0</span>   Age                       <span class="hljs-number">1176</span> non-null   int64 <br>...<br> <span class="hljs-number">34</span>  YearsWithCurrManager      <span class="hljs-number">1176</span> non-null   int64 <br>dtypes: int64(<span class="hljs-number">27</span>), <span class="hljs-built_in">object</span>(<span class="hljs-number">8</span>)<br>memory usage: <span class="hljs-number">330.8</span>+ KB<br></code></pre></td></tr></table></figure><p>里面有一个Dtype，这个值就是表示是什么类型的数据，如果是<code>int64</code>代表的就是数值类，那就不属于类别，我们就把<code>object</code>类型给它取出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">attr=[<span class="hljs-string">&#x27;Age&#x27;</span>,<span class="hljs-string">&#x27;BusinessTravel&#x27;</span>,<span class="hljs-string">&#x27;Department&#x27;</span>,<span class="hljs-string">&#x27;Education&#x27;</span>,<span class="hljs-string">&#x27;EducationField&#x27;</span>,<span class="hljs-string">&#x27;Gender&#x27;</span>,<span class="hljs-string">&#x27;JobRole&#x27;</span>,<span class="hljs-string">&#x27;MaritalStatus&#x27;</span>,<span class="hljs-string">&#x27;Over18&#x27;</span>,<span class="hljs-string">&#x27;OverTime&#x27;</span>]<br></code></pre></td></tr></table></figure><p>这些类别特征是通过筛选object类型来去提出来的，这个是直接写出来了，都属于类别特征。类别特征它就不属于数值类型，在机器学习入模之前，需要把它转换成为数值类型。</p><p>我们可以使用one-hat编码，除此之外，还有什么其他方法方式进行编码吗？数值编码有两种常见的方法，一种是刚才提到的one-hat，还有一种方法的话叫<code>LabelEncoder</code>,我们把它称为叫做标签编码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">lbe_list=[]<br><span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> attr:<br>    lbe=LabelEncoder()<br>    train[feature]=lbe.fit_transform(train[feature])<br>    test[feature]=lbe.transform(test[feature])<br>    lbe_list.append(lbe)<br></code></pre></td></tr></table></figure><p>如果你有10个类别，这10个类别会分别编码成多少呢？会编码成0-9对吧？</p><p>那针对每一个feature，就是从这个列表里面去进行提取，先去做了一个<code>fit_transform</code>。就是先去fit，同时去做transform，把feature做了一个转换。</p><p>我们是对训练集做了个转换，测试集只要应用就好了。那想想，为什么测试集里面没有做fit？因为训练集你已经fit完了，训练集和测试集标准是统一的，同样的一种编码规则，所以这里就不需要做fit了。</p><p>为什么测试集不用fit，因为我们的编码规则要统一。举个例子，<code>BusinessTravel</code>这个特征，很少旅行我们把它称为叫2，不旅行叫0，经常旅行叫1.在测试集里面我们也有Travel，那如果出现的顺序跟之前的顺序不一样，你还会去做一个fit，它有可能编码的关系就发生了变化。就是说我们的类别特征在训练集里面出现了，在测试集里面也会出现。我们需要让它的编码的规则是一致的，训练集里面如果他指定的<code>Travel_Rarely</code>是2的话，在测试集里面也必须是2。如果我们重新fit，有可能它就不是2而有可能变成1。如果变成1的话两个代表的含义就不一致了，就会造成运算的错误。所以我们在测试集里面是直接应用fit的含义，fit含义的话就是指定关系，fit就是指定我们的labelencoder的关系。transform是应用这种label encoder的关系进行编码。</p><p>又或者，我们可以将这两个数据集合并一下，合并成一个大的数据集，统一进行fit，fit之后再将它们分开。当然，这样做也是可以的。</p><p>我们就用刚才的LabelEncoder直接做transform，把测试集也做一个编码。编码之后，依次的这个类别特征就编码好了，就可以把它输出出来。你可以看一看他的LabelEncoder的这个逻辑关系。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311142107692.png"alt="Alt text" /></p><p>可能里面的文字太小看不清，大家可以自己拿着数据集测试，跑到这一步的时候和一开始的数据进行一个比对。那么对它的这个逻辑就比较了解了。</p><p>机器学习训练一定都是数值类型，不能出现字母，如果出现字母它是处理不了的，就是需要进行标签编码。</p><p>处理好以后的我们就可以用机器学习建模了，下面就是属于建模的环节，这里的建模用的是个分类模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">X_train, X_valid, y_train, y_valid = train_test_split(train_load.drop(<span class="hljs-string">&#x27;Attrition&#x27;</span>,axis=<span class="hljs-number">1</span>), train_load[<span class="hljs-string">&#x27;Attrition&#x27;</span>], test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">2023</span>)<br></code></pre></td></tr></table></figure><p>这里应该是数据<code>train</code>，因为之后这个数据集还要用，所以我做了个保存再读取，所以变成了<code>train_load</code>,test也一样。</p><p>我们在建模的时候又切分了一下数据集，切分用的是<code>train_test_split</code>，这个就是一个比较常见的切分的工具。在前面基础课程中我有一节专门将了数据集的处理，里面也有详细的说到。</p><p>指定它的切分的数量是20%，也就说测试集是20%。至于为什么不拿百分百的数据用于训练，我们之前的课程里也详细的说过，最后会无法评估模型的效果。</p><p>然后我们用20%的数据作为切分，在切分过程中，我们设置了一个<code>random_state</code>，这里是随机数的种子。如果不写随机数的种子的话，同样的模型的参数得出来的结果可能是不一样的，我们就不好评估哪一个模型的参数会更好。所以一般来说我们可以写一个<code>random_state</code>作为随机数的种子。</p><p>我这里的设置的今年的年份，可以参考这样的写法，这样每次运行的结果就不会因为随机数种子的问题产生不一样的情况。</p><h2 id="lr">LR</h2><p>然后咱们来用一个逻辑回归方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">model = LogisticRegression(max_iter=<span class="hljs-number">100</span>, <br>                           verbose=<span class="hljs-literal">True</span>, <br>                           random_state=<span class="hljs-number">33</span>,<br>                           tol=<span class="hljs-number">1e-4</span><br>                          )<br></code></pre></td></tr></table></figure><p>逻辑回归以后调包，去fit它。fit以后我们就可以predict,得到一个结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">model.fit(X_train, y_train)<br><span class="hljs-comment"># 二分类结果， 0或者1</span><br>predict = model.predict(test_load)<br><span class="hljs-built_in">print</span>(predict)<br><br>---<br>[<span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> ... <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p>直接predict得到的是个二分类结果，就是0和1。0和1刚才已经指定了，1代表离职，0代表不离职。我们也可以把它转换成一个概率型的问题，需要在predict的后面加一个probability,函数名称为<code>predict_proba()</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 二分类任务，有2个概率值，label=0的概率，label=1的概率</span><br>predict = model.predict_proba(test_load)[:, <span class="hljs-number">1</span>]<br>test_load[<span class="hljs-string">&#x27;Attrition&#x27;</span>]=predict<br><span class="hljs-built_in">print</span>(test_load[<span class="hljs-string">&#x27;Attrition&#x27;</span>])<br><br>---<br>user_id<br><span class="hljs-number">442</span>     <span class="hljs-number">0.118824</span><br>          ...   <br><span class="hljs-number">1229</span>    <span class="hljs-number">0.173611</span><br></code></pre></td></tr></table></figure><p>这样我们就可以知道它离职的概率是多少，最后把它打印出来。</p><p>就着这个结果，我给大家再回头来说一下<code>random_state</code>,当我们设置为2023的时候，那这个结果，就比如1229这个员工，它的离职概率是0.173611，是不会发生变化的。当我们设置一个其他的参数，或者个干脆删除的话，它预测的值就会发生变化，比如，我删除了<code>random_state</code>之后，结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">user_id<br><span class="hljs-number">442</span>     <span class="hljs-number">0.143042</span><br>          ...   <br><span class="hljs-number">1229</span>    <span class="hljs-number">0.204464</span><br></code></pre></td></tr></table></figure><p>可以看到442和1229这两个员工的概率都变了。再设置回来，那它的结果又会变成原来的值，也就是说，我们模型和数据都没有发生变化的时候，那么对最后的结果发生影响的就是这个种子值。有的时候，我们看到结果发生变化，可能会认为我们模型有可能变好了或变坏了，实际上模型是没有发生任何的变化，我们也没有改变它。</p><p>所以为了去衡量模型的好坏我们把那些变化量给它固定住，让它设置成为一个固定的随机数种子。这样我们再去调参的时候就可以知道这个模型的好坏了。</p><p>机器学习还有个过程叫做超参数的调整，比如说<code>max_iter</code>可能不用100，我们试试看50的好坏等等。怎么去衡量它，要把<code>random_state</code>固定下来，来去对比50好还是100好，这样才能有更清晰的一个判断标准。</p><p>所以<code>ramdom_state</code>是帮助你去衡量模型的好坏的。比如说我们下一次用SVM来去判断，这样我们先用随机数种子也给大家固定起来。</p><p>接下来，如果我们要将概率转化为二分类进行输出，加一个简单的判断，map一下就可以了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 转化为二分类输出</span><br>test_load[<span class="hljs-string">&#x27;Attrition&#x27;</span>]=test_load[<span class="hljs-string">&#x27;Attrition&#x27;</span>].<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x&gt;=<span class="hljs-number">0.5</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>大家可以自己运行一下，好好看一看这个逻辑。可能第一次写机器学习的同学需要后面逐渐熟悉一下，包括我们这些流程的过程，代码呢在我的课程仓库里，大家可以自行去获取：<a href="https://github.com/hivandu/AI_Cheats/tree/main/Core%20BI">CoreBI</a></p><p>以上就把这个流程简单给说完了,这里用的是一个LR的模型，其实可以采用其他的模型一起去做。</p><p>LR就是一个逻辑回归，逻辑回归是属于线性模型，比较简单的一个模型。我们今天先不着重讲解逻辑回归背后的原理，这个咱们之前的基础课程里也有详细的给大家讲过。现在只是让大家先有个整体的概念，知道都有哪些工具可以去使用它。</p><p>LR工具：</p><ul><li><code>from sklearn.linear_model._logistic import LogisticRegression</code></li></ul><p>参数：</p><ul><li><code>penalty</code>:惩罚项，正则化参数，防止过拟合，l1或l2，默认为l2</li><li><code>C</code>，正则化系数λ的倒数，float类型，默认为1.0</li><li><code>solver</code>: 损失函数优化方法，liblinear（默认），lbfgs，newton-cg， sag</li><li><code>random_state</code>: 随机数种子</li><li><code>max_iter</code>: 算法收敛的最大迭代次数，默认为100</li><li><code>tol=0.0001</code>:优化算法停止条件，迭代前后函数差小于tol则终止</li><li><code>verbose=0</code>:日志冗长度int：冗长度；0：不输出训练过程；1：偶尔 输出；&gt;1：对每个子模型都输出</li><li><code>n_jobs=1</code>:并行数，int：个数；-1：跟CPU核数一致；1:默认值</li></ul><p>常用方法： - <code>fit(X, y, sample_weight=None)</code> -<code>fit_transform(X, y=None, **fit_params)</code> -<code>predict(X)</code>，用来预测样本，也就是分类 -<code>predict_proba(X)</code>，输出分类概率。返回每种类别的概率，按照分类类别顺序给出。-<code>score(X, y, sample_weight=None)</code>，返回给定测试集合的平均准确率（meanaccuracy）</p><p>这个工序里面我们怎么去设置参数，比较常见的像<code>penalty</code>，正则化系数<code>C</code>等等。</p><p>随意数种子、迭代次数这些参数都是可以调整的，也可以用它默认的参数直接去创建。</p><p><code>predict(X)</code>和<code>predict_proba(X)</code>这两个分别代表的一个是等于类别，就是0和1输出的结果。还有一个输出的就是个概率值。根据需求，到底是直接看类别还是概率值，可以选择不同的函数来去完成。</p><p>那这边我们就把LR使用的一个流程在我们的项目中，通过这个例子来给大家讲完了。咱们先从整体上去了解，细节的地方下来再慢慢的去看，自己加一些笔记。</p><h2 id="svm">SVM</h2><p>除了LR以外还可以用SVM，这个我们在之前的基础课程也有重点讲过，支持向量机。这个方法在2010年前后很火，这模型在10几年前是主流模型。现在的风头已经被神经网络，就是我们的neuralnetwork，以及大数据模型掩盖住了。但是在2010年左右那阵主流模型就是SVM。不管是做分类任务还是图像识别，它的效果都是不错的。</p><p>它有三种包，分别是SVC，NuSVC和LinearSVC。这个包有啥区别呢？其实一般来说，前两种只要用一个就可以，这两种都是我们的支持向量机，无外乎就是它的参数、配置会有些区别而已，一个是<code>C</code>,一个是<code>nu</code>，用哪一个都可以。推荐大家使用第一个。</p><p>后面有个LinearSVC，我们把它称为线性的。支持向量机一般来说是非线性的特征，它由低维的空间映射到一个高维空间里面去了，所以通常我们用非线性的话用第一种SVC。如果你认为我们的变化是个线性变化，那我们可以用LinearSVC也是一样的。</p><p>这三种方法分别如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">sklearn.svm.SVC(C=<span class="hljs-number">1.0</span>, kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>, degree=<span class="hljs-number">3</span>, gamma=<span class="hljs-string">&#x27;auto&#x27;</span>, coef0=<span class="hljs-number">0.0</span>, shrinking=<span class="hljs-literal">True</span>, probability=<span class="hljs-literal">False</span>, tol=<span class="hljs-number">0.001</span>, cache_size=<span class="hljs-number">200</span>, class_weight=<span class="hljs-literal">None</span>, verbose=<span class="hljs-literal">False</span>, max_iter=<span class="hljs-number">1</span>, decision_function_shape=<span class="hljs-string">&#x27;ovr&#x27;</span>, random_state=<span class="hljs-literal">None</span>)<br><br>sklearn.svm.NuSVC(nu=<span class="hljs-number">0.5</span>, kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>, degree=<span class="hljs-number">3</span>, gamma=<span class="hljs-string">&#x27;auto&#x27;</span>, coef0=<span class="hljs-number">0.0</span>, shrinking=<span class="hljs-literal">True</span>, probability=<span class="hljs-literal">False</span>, tol=<span class="hljs-number">0.001</span>, cache_size=<span class="hljs-number">200</span>, class_weight=<span class="hljs-literal">None</span>, verbose=<span class="hljs-literal">False</span>, max_iter=<span class="hljs-number">1</span>, decision_function_shape=<span class="hljs-string">&#x27;ovr&#x27;</span>, random_state=<span class="hljs-literal">None</span>)<br><br>sklearn.svm.LinearSVC(penalty=<span class="hljs-string">&#x27;l2&#x27;</span>, loss=<span class="hljs-string">&#x27;squared_hinge&#x27;</span>, dual=<span class="hljs-literal">True</span>, tol=<span class="hljs-number">0.0001</span>, C=<span class="hljs-number">1.0</span>, multi_class=<span class="hljs-string">&#x27;ovr&#x27;</span>, fit_intercept=<span class="hljs-literal">True</span>, intercept_scaling=<span class="hljs-number">1</span>, class_weight=<span class="hljs-literal">None</span>, verbose=<span class="hljs-number">0</span>, random_state=<span class="hljs-literal">None</span>, max_iter=<span class="hljs-number">1000</span>)<br></code></pre></td></tr></table></figure><p>常用参数：</p><ul><li><code>C</code>，惩罚系数，类似于LR中的正则化系数，C越大惩罚越大</li><li><code>nu</code>，代表训练集训练的错误率的上限（用于NuSVC）</li><li><code>kernel</code>，核函数类型，RBF, Linear, Poly,Sigmoid,precomputed，默认为RBF径向基核（高斯核函数）</li><li><code>gamma</code>，核函数系数，默认为auto</li><li><code>degree</code>，当指定kernel为'poly'时，表示选择的多项式的最高次数，默认为三次多项式</li><li><code>probability</code>，是否使用概率估计</li><li><code>shrinking</code>，是否进行启发式，SVM只用少量训练样本进行计算</li><li><code>penalty</code>，正则化参数，L1和L2两种参数可选，仅LinearSVC有</li><li><code>loss</code>，损失函数，有‘hinge’和‘squared_hinge’两种可选，前者又称L1损失，后者称为L2损失</li><li><code>tol</code>: 残差收敛条件，默认是0.0001，与LR中的一致</li></ul><p>这些是一些比较常见的一些参数，我们可以做一些配置，这个你自己可以看一看。</p><p>工具的使用啊，其实工具的使用就掉包就好了。我们在学习的时候也稍微了解一下它的一个原理。机器学习和深度学习原理呢，咱们之前课程都详细的讲过，这里简单给大家讲解一下。</p><p>SVM的思想就是把一些原来不太在线性平面上，不好分割的数据，通过一个超平面让它去分割。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311142107693.png"alt="Alt text" /></p><p>比如说左图，这是个物体在一个平面的桌子上面去摆放。这个平面桌子上有两种物体，一种是红色的圆球，一种是绿色的方块。那我们在桌子上去任意的划分，以线性分类器做分类，他们能把它分类出来吗？你去切很多刀，每次的话都是线性的，线性就是一条直线的方法。能不能把红色和绿色给它一分为二，分的比较好呢？</p><p>你会发现很难对不对？怎么画都很难。所以在低维空间中它是无法线性可分的。这个时候，咱们就想了一个方式，用手猛地一拍桌子，这些物体瞬间腾空而起，在他腾空的那一刹那时空静止，咱们再次用手在它的平面处切了一刀。可以看到这个平面在黄色之上的，我们把它分割出来是红色的圆球，在平面之下就是绿色的方块。</p><p>所以在高维空间中我们可以让它变得线性可分。那么SVM的特点就是去找到一个从低维到高维的映射，让它在高维空间中线性可分。现在大家能明白SVM的一个原理了吧？它的作用就是找到一个映射关系，方便你在高维中进行一个线性可分。</p><p>我今天没有着重去推数学的工具，这些如果要推的话可能会花很长的时间，那我只是给大家介绍机器学习的全家桶，先让大家有一个整体的认知。你知道SVM的作用是什么，下一次想到它就会想到我们刚才讲的那个场景。由低维到高维。</p><p>那由低维到高维找到这个空间怎么找呢？我们有一些找的方式。</p><p>去设置一个核函数，这个叫做Kernel。它的Kernel,核就是映射关系。有几种核，一种叫线性核，线性核还是一种桌子平面，只不过把这个桌子变成了另一个桌子。</p><p>多相式核就是一个非线性的空间，更高维了。</p><p>RBF属于高斯核函数，也是一个非线性的关系。</p><p>包括sigmoid核，sigmoid在SVM里面可以设置这4个核函数，sigmoid是其中之一。这个sigmoid如果你之前有看我的前面的「核心基础」部分，那是经常会提到它。我们在构建神经网络的时候，它就是神经网络的激活函数。它也是逻辑回归的一个过程。</p><p>sigmoid的原理就是把线性的映射成为非线性，所以它也是一个非线性的映射。</p><p>我们有很多种映射关系，都可以帮你找一找能不能在这样的高维空间中变得线可分，有可能。所以我们可以尝试不同的核，去做一些切分。</p><p>在使用过程中如果你要用不同的核，尤其是高维的核，你要用SVC了。如果你觉得它就是个线性过程的话，我们就用LinearSVC，它就是个线性的变化。</p><p>针对我们离职员工预测这个问题，它的数据集比较简单，规律也没有那么复杂，所以可以两种方式都尝试一下，看看谁的效果好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">mms = MinMaxScaler(feature_range=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>X_train = mms.fit_transform(X_train)<br>X_valid = mms.fit_transform(X_valid)<br>test_load = mms.fit_transform(test_load)<br><br>model = LinearSVC(max_iter=<span class="hljs-number">1000</span>,<br>                  random_state=<span class="hljs-number">33</span>,<br>                  verbose=<span class="hljs-literal">True</span>)<br><br>model.fit(X_train, y_train)<br>predict = model.predict(test_load)<br><span class="hljs-built_in">print</span>(predict)<br><br>test_load[<span class="hljs-string">&#x27;Attrition&#x27;</span>] = predict<br>test_load[[<span class="hljs-string">&#x27;Attrition&#x27;</span>]].to_csv(<span class="hljs-string">&#x27;dataset/submit_svc.csv&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;submit_svc.csv saved&#x27;</span>)<br></code></pre></td></tr></table></figure><p>我这里用的LinearSVC做一个设置，整个调包过程都是类似的，都是创建好了这个包然后用模型去fit，再去predict，把predict之后的结果进行输出即可。调包的原理、过程都是一样的，只不过你可以调不同的包来去完成。</p><p>以上是SVM的过程, 可以用SVC也可以用LinearSVC。</p><p>那在这段代码里，我在执行LinearSVC之前还做了一个操作，这个操作是一个叫做MinMaxScaler，这个是0~1规范化，它也属于数据预处理的一个环节。</p><p>用0-1规划化的目的是把数据映射归一化。归一化到0-1区间。原来的数据类型如果你去看那个大小，比如说工资就有可能是几千到几万不等，他的数据的范围比较大，工作时长有可能就是从几十到上百不等，可能维度相对小一点。不同的维度我们现在都给他统一。</p><p>统一的目的是啥？如果不统一对我们的模型有没有影响？记住，如果不统一的话，对于这种通过距离来做分类的这样一些模型是有影响的。</p><p>怎么影响呢？举个例子，比如说我要两种类别，一种类别叫做Salary，这个<code>salary</code>以0-1万为例。还有一个我们叫做<code>work hour</code>，可能是由0-100为例。如果不给它统一，哪一个指标会占主导？是Salary还是hour？这两个相对来说哪个更主导？</p><p>应该是属于量纲范围更大的，它的范围越大就越影响整个的模型的计算的过程，所以Salary就会更加的主导一点，还有其他的词段也是一样的。所以为了保持统一，我们就把0-1万都转换成0-1。而workhour也转换成0-1。这样的话我们的权重都是一致的。否则Salary的重要性就变成100倍的workhour，或者其他的维度的指标。</p><p>所以这个规划的过程是跟距离相关的模型，对距离相关的模型我们需要先采用规律化的方法。因为你的距离，他的量纲要保持统一才可以。</p><p>好，以上就是我们整个的过程。SVM就直接吊包去使用。当对它的了解，概念有些认知之后，我们可以直接去调用这个LinearSVC的包。具体的SVM原理，还是推荐大家去看看我之前的一章专门讲解支持向量机的课程。</p><p>那这个项目过程就给大家讲完了。咱们下节课再讲一个例子，带大家一起来看一个男女声音识别的例子。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311142107687.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="BI" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/BI/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
    <category term="BI" scheme="https://hivan.me/tags/BI/"/>
    
  </entry>
  
  <entry>
    <title>33. CV练习： 验证码识别</title>
    <link href="https://hivan.me/33.%20Exercise:%20Captcha%20Recognition/"/>
    <id>https://hivan.me/33.%20Exercise:%20Captcha%20Recognition/</id>
    <published>2023-12-26T23:30:00.000Z</published>
    <updated>2024-01-03T08:35:30.284Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311112206553.png"alt="Alt text" /></p><span id="more"></span><p>Hi, 你好。我是茶桁。</p><p>上一节课，我给大家留了个作业，内容是对验证码进行识别。</p><p>咱们的再把练习内容重复看一下：</p><ol type="1"><li><p>练习内容：训练一个模型，对验证码中的字符进行分类识别，并最终完成验证码识别的任务。</p></li><li><p>数据集：数据集内包含0-9以及A-Z一共36个字符，训练集中每个字符有50张图片，验证集中每个字符有10张图片，验证码数据集是由随机去除的4个字符图片拼接而成。</p></li><li><p>需要的相关知识：</p><ul><li>数据读取</li><li>使用torch搭建、训练、验证模型</li><li>模型预测于图片切分</li></ul></li></ol><p>好，让我们来看看具体的，我们该怎么完成这个练习。</p><h2 id="问题分析">问题分析</h2><p>首先，我们需要一步步的确定我们的问题。第一个问题，肯定是要先建立字符对照表，第二个问题，要定义一个<code>datasets</code>和一个<code>dataloader</code>。第三个问题，是需要定义网络结构。 第四个问题，就是定义模型训练函数。最后，就是验证我们的训练结果。</p><p>先从第一个问题分析，我们可以通过遍历字典，将每一对键值反转，并存储于新的字典中。我们可以按如下方式去做：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">new_dict = &#123;v:k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> old_dict.items()&#125;<br></code></pre></td></tr></table></figure><p>那么第二个问题就简单了，在opencv-python中，可以使用<code>image = cv2.medianBlur(image, kernel_size)</code>进行中值滤波。</p><p>第三个问题，在torch中，卷积于前连接层的定义方法可以是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)<br>fc = nn.Linear(in_features, out_features, bias)<br></code></pre></td></tr></table></figure><p>具体的，还可以参看PyTorch的相关手册。用这两个方法进行组合，就可以定义出一个卷积神经网络结构。</p><p>接下来第四个问题，要定义模型训练函数。那么torch框架的模型训练过程会包含清空梯度、前向传播、计算损失、计算梯度、更新权重等操作。</p><ul><li>清空梯度：目的是消除step与step之间的干扰，即每次都只用一个batch的数据损失计算梯度并更新权重。一般可以放在最前或最后；</li><li>前向传播：使用一个batch的数据跑一边前向传播的过程，生成模型输出结果；</li><li>计算损失：使用定义好的损失函数、模型输出结果以及label计算单个batch的损失值；</li><li>计算梯度：根据损失值，计算模型所有权中在本次优化中所需的梯度值；</li><li>更新权重：使用计算好的梯度值，更新所有权重的值。</li></ul><p>模拟一下单词流程代码，样例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>optimizer.zero_grad() <span class="hljs-comment"># 清空梯度（也可以放在最后一行）</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>output = model(data) <span class="hljs-comment"># 前向传播</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>loss = loss_fn(output, target) <span class="hljs-comment"># 计算损失</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>optimizer.step() <span class="hljs-comment"># 更新权重</span><br></code></pre></td></tr></table></figure><p>好，步骤分析完了，接着咱们来进行实现，这次我们所用到的代码库如下，将其都引入进来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <br><span class="hljs-keyword">import</span> torch <br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><span class="hljs-keyword">import</span> pickle <br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image <br></code></pre></td></tr></table></figure><p>然后我们来看一下数据集，上一课末尾我给过数据集了，有需要的自己去找一下。先将需要用到的数据路径都定义出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data_dir = <span class="hljs-string">&#x27;data/train_data.bin&#x27;</span><br>val_data_dir = <span class="hljs-string">&#x27;data/val_data.bin&#x27;</span><br>verification_code_dir = <span class="hljs-string">&#x27;data/verification_code_data.bin&#x27;</span><br></code></pre></td></tr></table></figure><p>这个数据集保存在二进制文件中，我们需要定义一个函数，读取二进制文件中的图片：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_file</span>(<span class="hljs-params">file_name</span>):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_name, mode=<span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        result = pickle.load(f)<br>    <span class="hljs-keyword">return</span> result<br></code></pre></td></tr></table></figure><p>来让我们查看一下数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data = load_file(train_data_dir)<br>img_test = <span class="hljs-built_in">list</span>()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">1800</span>,<span class="hljs-number">50</span>):<br>    img_test.append(train_data[i][<span class="hljs-number">1</span>])<br>plt.figure()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">37</span>):<br>    plt.subplot(<span class="hljs-number">6</span>,<span class="hljs-number">6</span>,i)<br>    plt.imshow(img_test[i-<span class="hljs-number">1</span>])<br>    plt.xticks([])<br>    plt.yticks([])<br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311112206554.png"alt="Alt text" /></p><p>放大其中单张图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.imshow(train_data[<span class="hljs-number">500</span>][<span class="hljs-number">1</span>])<br>plt.xticks([])<br>plt.yticks([])<br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311112206555.png"alt="Alt text" /></p><p>观察这张图片，我们可以看到字符图片中含有大量的噪声，而噪声会对模型预测结果产生不良影响，因此我们可以在数据预处理时，使用特定的滤波器，消除图片噪声。</p><p>简单观察可知，刚才定义字符字典中，键与值都没有重复项，因此可以将字典中的键与值进行反转，以便我们用值查找键（将模型预测结果转换成可读字符）。将字典中的键与值进行反转（例：<code>dict=&#123;'A':10,'B':11&#125;</code>反转后得到<code>new_dict=&#123;10:'A',11:'B'&#125;</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">char_dict = &#123;<span class="hljs-string">&#x27;0&#x27;</span>:<span class="hljs-number">0</span>,<span class="hljs-string">&#x27;1&#x27;</span>:<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;2&#x27;</span>:<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;3&#x27;</span>:<span class="hljs-number">3</span>,<span class="hljs-string">&#x27;4&#x27;</span>:<span class="hljs-number">4</span>,<span class="hljs-string">&#x27;5&#x27;</span>:<span class="hljs-number">5</span>,<span class="hljs-string">&#x27;6&#x27;</span>:<span class="hljs-number">6</span>,<span class="hljs-string">&#x27;7&#x27;</span>:<span class="hljs-number">7</span>,<span class="hljs-string">&#x27;8&#x27;</span>:<span class="hljs-number">8</span>,<span class="hljs-string">&#x27;9&#x27;</span>:<span class="hljs-number">9</span>,\<br>            <span class="hljs-string">&#x27;A&#x27;</span>:<span class="hljs-number">10</span>,<span class="hljs-string">&#x27;B&#x27;</span>:<span class="hljs-number">11</span>,<span class="hljs-string">&#x27;C&#x27;</span>:<span class="hljs-number">12</span>,<span class="hljs-string">&#x27;D&#x27;</span>:<span class="hljs-number">13</span>,<span class="hljs-string">&#x27;E&#x27;</span>:<span class="hljs-number">14</span>,<span class="hljs-string">&#x27;F&#x27;</span>:<span class="hljs-number">15</span>,<span class="hljs-string">&#x27;G&#x27;</span>:<span class="hljs-number">16</span>,<span class="hljs-string">&#x27;H&#x27;</span>:<span class="hljs-number">17</span>,<span class="hljs-string">&#x27;I&#x27;</span>:<span class="hljs-number">18</span>,<span class="hljs-string">&#x27;J&#x27;</span>:<span class="hljs-number">19</span>,<span class="hljs-string">&#x27;K&#x27;</span>:<span class="hljs-number">20</span>,<span class="hljs-string">&#x27;L&#x27;</span>:<span class="hljs-number">21</span>,<span class="hljs-string">&#x27;M&#x27;</span>:<span class="hljs-number">22</span>,\<br>            <span class="hljs-string">&#x27;N&#x27;</span>:<span class="hljs-number">23</span>,<span class="hljs-string">&#x27;O&#x27;</span>:<span class="hljs-number">24</span>,<span class="hljs-string">&#x27;P&#x27;</span>:<span class="hljs-number">25</span>,<span class="hljs-string">&#x27;Q&#x27;</span>:<span class="hljs-number">26</span>,<span class="hljs-string">&#x27;R&#x27;</span>:<span class="hljs-number">27</span>,<span class="hljs-string">&#x27;S&#x27;</span>:<span class="hljs-number">28</span>,<span class="hljs-string">&#x27;T&#x27;</span>:<span class="hljs-number">29</span>,<span class="hljs-string">&#x27;U&#x27;</span>:<span class="hljs-number">30</span>,<span class="hljs-string">&#x27;V&#x27;</span>:<span class="hljs-number">31</span>,<span class="hljs-string">&#x27;W&#x27;</span>:<span class="hljs-number">32</span>,<span class="hljs-string">&#x27;X&#x27;</span>:<span class="hljs-number">33</span>,<span class="hljs-string">&#x27;Y&#x27;</span>:<span class="hljs-number">34</span>,<span class="hljs-string">&#x27;Z&#x27;</span>:<span class="hljs-number">35</span>&#125;<br>new_char_dict = &#123;v : k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> char_dict.items()&#125;<br></code></pre></td></tr></table></figure><p>然后我们就要定义<code>datasets</code>和<code>dataloader</code>了，我们需要使用<code>torch.utils.data.Dataset</code>作为父类，定义自己的datasets，以便规范自己的数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, file_name, transforms</span>):<br>        self.file_name = file_name <span class="hljs-comment"># 文件名称</span><br>        self.image_label_arr = load_file(self.file_name) <span class="hljs-comment"># 读入二进制文件</span><br>        self.transforms = transforms <span class="hljs-comment"># 图片转换器</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        label, img = self.image_label_arr[index]<br>        img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY) <span class="hljs-comment"># 将图片转为灰度图</span><br>        img = cv2.medianBlur(img, <span class="hljs-number">5</span>) <span class="hljs-comment"># 使用中值模糊除去图片噪音</span><br>        img = self.transforms(img) <span class="hljs-comment"># 对图片进行转换</span><br>        <span class="hljs-keyword">return</span> img, char_dict[label[<span class="hljs-number">0</span>]] <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.image_label_arr)<br></code></pre></td></tr></table></figure><p>接着我们来定义<code>transform</code>和<code>dataloader</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.Compose([transforms.ToPILImage(), <br>                                transforms.Resize([<span class="hljs-number">28</span>,<span class="hljs-number">28</span>]), <span class="hljs-comment"># 将图片尺寸调整为28*28</span><br>                                transforms.ToTensor(), <span class="hljs-comment"># 将图片转为tensor</span><br>                                transforms.Normalize(mean=[<span class="hljs-number">0.5</span>],std=[<span class="hljs-number">0.5</span>])]) <span class="hljs-comment"># 进行归一化处理</span><br><br>train_datasets = MyDataset(train_data_dir, transform)<br>train_loader = DataLoader(dataset=train_datasets,batch_size=<span class="hljs-number">32</span>,shuffle=<span class="hljs-literal">True</span>)<br><br>val_datasets = MyDataset(val_data_dir, transform)<br>val_loader = DataLoader(dataset=val_datasets,batch_size=<span class="hljs-number">32</span>,shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>在数据准备好之后，我们需要定义一个简单的卷积神经网络，神经网络的输入是<code>[batchsize,chanel(1),w(28),h(28)]</code>，输出是36个分类。</p><p>我们的神经网络将使用2个卷积层搭配2个全连接层，这四层的参数设置如下表所示(未标注的直接使用默认参数即可)：</p><ul><li><code>conv1: in_chanel=1, out_chanel=10, kernel_size=5</code></li><li><code>conv2: in_chanel=10, out_chanel=20, kernel_size=3</code></li><li><code>fc1: in_feature=2000, out_feature=500</code></li><li><code>fc2: in_feature=500, out_feature=36</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ConvNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        self.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">5</span>) <br>        self.conv2 = nn.Conv2d(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">3</span>) <br>        self.fc1 = nn.Linear(<span class="hljs-number">20</span> * <span class="hljs-number">10</span> * <span class="hljs-number">10</span>, <span class="hljs-number">500</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">500</span>, <span class="hljs-number">36</span>)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># inputsize:[b,1,28,28]</span><br>        in_size = x.size(<span class="hljs-number">0</span>) <span class="hljs-comment"># b</span><br>        out= self.conv1(x) <span class="hljs-comment"># inputsize:[b,1,28,28] -&gt; outputsize:[b,10,24,24]</span><br>        out = F.relu(out)<br>        out = F.max_pool2d(out, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># inputsize:[b,10,24,24] -&gt; outputsize:[b,10,12,12]</span><br>        out = self.conv2(out) <span class="hljs-comment"># inputsize:[b,10,12,12] -&gt; outputsize:[b,20,10,10]</span><br>        out = F.relu(out)<br>        out = out.view(in_size, -<span class="hljs-number">1</span>) <span class="hljs-comment"># inputsize:[b,20,10,10] -&gt; outputsize:[b,2000]</span><br>        out = self.fc1(out) <span class="hljs-comment"># inputsize:[b,2000] -&gt; outputsize:[b,500]</span><br>        out = F.relu(out)<br>        out = self.fc2(out) <span class="hljs-comment"># inputsize:[b,500] -&gt; outputsize:[b,36]</span><br>        out = F.log_softmax(out, dim = <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><p>接着，我们来定义模型训练函数，需要实现下面四项操作：</p><ol type="1"><li>清空梯度</li><li>前向传播</li><li>计算梯度</li><li>更新权重</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model, train_loader, optimizer, epoch</span>):<br>    model.train()<br>    <span class="hljs-keyword">for</span> batch_idx, (data, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        optimizer.zero_grad()<br>        output = model(data)<br>        loss = F.nll_loss(output, target)<br>        loss.backward()<br>        optimizer.step()<br>        <span class="hljs-keyword">if</span> (batch_idx + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                epoch, batch_idx * <span class="hljs-built_in">len</span>(data), <span class="hljs-built_in">len</span>(train_loader.dataset),<br>                <span class="hljs-number">100.</span> * batch_idx / <span class="hljs-built_in">len</span>(train_loader), loss.item()))<br></code></pre></td></tr></table></figure><p>再来定义模型测试函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>(<span class="hljs-params">model, test_loader</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    test_loss =<span class="hljs-number">0</span><br>    correct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> test_loader:<br>            output = model(data)<br>            test_loss += F.nll_loss(output, target, reduction = <span class="hljs-string">&#x27;sum&#x27;</span>)<br>            pred = output.<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, keepdim = <span class="hljs-literal">True</span>)[<span class="hljs-number">1</span>]<br>            correct += pred.eq(target.view_as(pred)).<span class="hljs-built_in">sum</span>().item()<br>    test_loss /= <span class="hljs-built_in">len</span>(test_loader.dataset)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%) \n&quot;</span>.<span class="hljs-built_in">format</span>(<br>        test_loss, correct, <span class="hljs-built_in">len</span>(test_loader.dataset),<br>        <span class="hljs-number">100.</span>* correct / <span class="hljs-built_in">len</span>(test_loader.dataset)))<br></code></pre></td></tr></table></figure><p>接着是定义模型及优化器，我们将刚刚搭建好的模型结构定义为model，并选择使用Adam优化器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = ConvNet()<br>optimizer = optim.Adam(model.parameters())<br></code></pre></td></tr></table></figure><p>终于可以来进行模型训练与测试了。我们可以先设置epochs数为3，进行模型训练，看看模型精度是多少，是否满足验证码识别的要求。如果模型精度不够，你还可以尝试调整epochs数，重新进行训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python">EPOCHS = <span class="hljs-number">3</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, EPOCHS + <span class="hljs-number">1</span>):<br>    train(model, train_loader, optimizer, epoch)<br>    test(model, val_loader)<br><br>---<br>Train Epoch: <span class="hljs-number">1</span> [<span class="hljs-number">288</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">16</span>%)]Loss: <span class="hljs-number">3.454732</span><br>Train Epoch: <span class="hljs-number">1</span> [<span class="hljs-number">608</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">33</span>%)]Loss: <span class="hljs-number">2.911864</span><br>Train Epoch: <span class="hljs-number">1</span> [<span class="hljs-number">928</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">51</span>%)]Loss: <span class="hljs-number">1.960211</span><br>Train Epoch: <span class="hljs-number">1</span> [<span class="hljs-number">1248</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">68</span>%)]Loss: <span class="hljs-number">0.972134</span><br>Train Epoch: <span class="hljs-number">1</span> [<span class="hljs-number">1568</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">86</span>%)]Loss: <span class="hljs-number">0.420529</span><br><br>Test <span class="hljs-built_in">set</span>: Average loss: <span class="hljs-number">0.3054</span>, Accuracy: <span class="hljs-number">335</span>/<span class="hljs-number">360</span> (<span class="hljs-number">93</span>%) <br><br>Train Epoch: <span class="hljs-number">2</span> [<span class="hljs-number">288</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">16</span>%)]Loss: <span class="hljs-number">0.094786</span><br>Train Epoch: <span class="hljs-number">2</span> [<span class="hljs-number">608</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">33</span>%)]Loss: <span class="hljs-number">0.120601</span><br>Train Epoch: <span class="hljs-number">2</span> [<span class="hljs-number">928</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">51</span>%)]Loss: <span class="hljs-number">0.073640</span><br>Train Epoch: <span class="hljs-number">2</span> [<span class="hljs-number">1248</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">68</span>%)]Loss: <span class="hljs-number">0.058856</span><br>Train Epoch: <span class="hljs-number">2</span> [<span class="hljs-number">1568</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">86</span>%)]Loss: <span class="hljs-number">0.007260</span><br><br>Test <span class="hljs-built_in">set</span>: Average loss: <span class="hljs-number">0.0139</span>, Accuracy: <span class="hljs-number">359</span>/<span class="hljs-number">360</span> (<span class="hljs-number">100</span>%) <br><br>Train Epoch: <span class="hljs-number">3</span> [<span class="hljs-number">288</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">16</span>%)]Loss: <span class="hljs-number">0.002425</span><br>Train Epoch: <span class="hljs-number">3</span> [<span class="hljs-number">608</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">33</span>%)]Loss: <span class="hljs-number">0.004629</span><br>Train Epoch: <span class="hljs-number">3</span> [<span class="hljs-number">928</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">51</span>%)]Loss: <span class="hljs-number">0.005880</span><br>Train Epoch: <span class="hljs-number">3</span> [<span class="hljs-number">1248</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">68</span>%)]Loss: <span class="hljs-number">0.008300</span><br>Train Epoch: <span class="hljs-number">3</span> [<span class="hljs-number">1568</span>/<span class="hljs-number">1800</span> (<span class="hljs-number">86</span>%)]Loss: <span class="hljs-number">0.004973</span><br><br>Test <span class="hljs-built_in">set</span>: Average loss: <span class="hljs-number">0.0039</span>, Accuracy: <span class="hljs-number">360</span>/<span class="hljs-number">360</span> (<span class="hljs-number">100</span>%) <br></code></pre></td></tr></table></figure><p>训练结果看起来相当不错。那么下面我们自然就是要开始进行验证码识别了，我们需要先导入验证码数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">verification_code_data = load_file(verification_code_dir)<br></code></pre></td></tr></table></figure><p>下面我们随便选一张图（图3），看看这个验证码长什么样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">image = verification_code_data[<span class="hljs-number">3</span>]<br>IMG = Image.fromarray(cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB))<br>plt.imshow(IMG)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311112206556.png"alt="Alt text" /></p><p>再来看看中值滤波能对验证码图片产生什么效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 中值滤波效果</span><br>img = cv2.medianBlur(image.copy(), <span class="hljs-number">5</span>)<br>plt.imshow(img)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311112206557.png"alt="Alt text" /></p><p>好，最后来让我们看看识别的实际情况如何：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 查看实际识别结果</span><br>IMAGES = <span class="hljs-built_in">list</span>()<br>NUMS = <span class="hljs-built_in">list</span>()<br><br><span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> verification_code_data:<br>    IMAGES.append(img)<br>    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)<br>    image_1 = img[:, :<span class="hljs-number">80</span>]<br>    image_2 = img[:, <span class="hljs-number">80</span>:<span class="hljs-number">160</span>]<br>    image_3 = img[:, <span class="hljs-number">160</span>:<span class="hljs-number">240</span>]<br>    image_4 = img[:, <span class="hljs-number">240</span>:<span class="hljs-number">320</span>]<br>    img_list = [image_1, image_2, image_3, image_4]<br><br>    nums = []<br>    <span class="hljs-keyword">for</span> one_img <span class="hljs-keyword">in</span> img_list:<br>        one_img = transform(one_img)<br>        one_img = one_img.unsqueeze(<span class="hljs-number">0</span>)<br>        output = model(one_img)<br>        nums.append(new_char_dict[torch.argmax(output).item()])<br>    NUMS.append(<span class="hljs-string">&#x27;Verification_code: &#x27;</span>+<span class="hljs-string">&#x27;&#x27;</span>.join(nums))<br><br>plt.figure(figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>))<br>plt.subplots_adjust(wspace=<span class="hljs-number">0.2</span>, hspace=<span class="hljs-number">0.5</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>):<br>    plt.subplot(<span class="hljs-number">5</span>,<span class="hljs-number">2</span>,i)<br>    plt.title(NUMS[i-<span class="hljs-number">1</span>], fontsize=<span class="hljs-number">25</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br>    plt.imshow(IMAGES[i-<span class="hljs-number">1</span>])<br>    plt.xticks([])<br>    plt.yticks([])<br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311112206558.png"alt="Alt text" /></p><p>相当令人满意的结果。</p><p>本次练习我主要是讲解思路，大家看完课程之后要多加练习，自己反复敲打代码，去琢磨其中的一些逻辑关系，要结合我们之前课程中讲过的知识。</p><p>还有就是，关于一些第三方库，大家要习惯于自行查看手册。这才是正确的学习打开方式。</p><p>好，那这个练习就讲到这里，大家下来记得多练习。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311112206553.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>AI秘籍 - Python篇 PDF发布</title>
    <link href="https://hivan.me/AI%E7%A7%98%E7%B1%8D-Python%E7%AF%87%20PDF%E5%8F%91%E5%B8%83/"/>
    <id>https://hivan.me/AI%E7%A7%98%E7%B1%8D-Python%E7%AF%87%20PDF%E5%8F%91%E5%B8%83/</id>
    <published>2023-12-26T07:30:00.000Z</published>
    <updated>2024-01-03T08:16:15.111Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/20240103161035.png"alt="20240103161035" /></p><span id="more"></span><p>Hi，大家好。我是茶桁。</p><p>距离《茶桁的AI秘籍》开写已经过去小半年了，目前我们完成了三个部分的内容，分别是<ahref="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4NzE4MDQzMg==&amp;action=getalbum&amp;album_id=3035995870421073928&amp;scene=173&amp;subscene=&amp;sessionid=undefined&amp;enterid=0&amp;from_msgid=2648748542&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect">「Python篇」</a>、<ahref="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4NzE4MDQzMg==&amp;action=getalbum&amp;album_id=3074770001140400130&amp;from_itemidx=1&amp;from_msgid=2648748768#wechat_redirect">「数学篇」</a>以及 <ahref="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4NzE4MDQzMg==&amp;action=getalbum&amp;album_id=3123885735829061633&amp;scene=173&amp;subscene=&amp;sessionid=undefined&amp;enterid=0&amp;from_msgid=2648749632&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect">「核心基础」篇</a>，核心基础部分其实已经讲完了，剩下最后一篇是一个项目实战练习将会在明天与大家见面。</p><p>这其中数学篇是初次尝试进行收费阅读，也感谢部分小伙伴的支持。其间也有小伙伴提出了不少错误，在此万分感谢。</p><p>那在整个连载的过程中，我的后台有小伙伴留言说想要电子书，在公众号内进行阅读还是有些不太方便。在平时，茶桁其实很多的知识和内容都是来自于公众号，当然，更多的是来自于书本。</p><p>有闲下来的时间了，茶桁打算给大家<strong>写一篇「工作流」</strong>的内容来帮助小伙伴们平时在阅读的时候更好的收集内容。咱们以后再说。</p><p>本次电子书，先出了Python篇的，满足一些小伙伴的阅读需求。那本电子书是会长期更新的，内容上不会做大的改动，不过一些小的错误更正会时常去做。欢迎大家将本书分享给你的其他小伙伴们。</p><p>之后还会陆续更新「数学篇」和「核心基础篇」的电子书，不过数学篇会收费进行售卖，针对已经购买了专辑的小伙伴，则完全免费发放，并会持续更新。</p><p>还有就是咱们的代码仓库，之前有一些小伙伴反馈Github的仓库无法打开，所以我在Gitee上又上传了一份，现在是包含了两个代码仓库，分别是：</p><ul><li>Github: https://github.com/hivandu/AI_Cheats</li><li>Gitee: https://gitee.com/hivandu/ai_cheats</li></ul><p>可以选择一个自己访问顺畅的进行拉取。</p><p>这个仓库内包含了目前为止整个《AI秘籍的》的全部代码以及数据集，方便小伙伴下来之后进行练习。那仓库中的代码截至运行时间是2023年12月26日，为什么要写这个，因为有些包里的一些参数会做一些变动，到目前为止这些代码都可正常运行，假如过若干时间之后大家发现代码运行不了，可以去看看包的官方文档，也可以后台留言给我进行说明，我会对电子书进行更新。</p><p>最后，感谢大家的支持。还望关注「坍缩的奇点」，日后咱们会有一些干货分享给大家，也会和大家一起来完成一些企业项目，帮助小伙伴们更快的步入到实际工作中去。</p><p><imgsrc="https://github.com/hivandu/notes/blob/main/img/Capture-2023-11-02-164446.png?raw=true"alt="Capture-2023-11-02-164446" /></p><p>电子书获取： 1. Github:https://github.com/hivandu/AI_Cheats/blob/main/Documents/%E8%8C%B6%E6%A1%81%E7%9A%84AI%E7%A7%98%E7%B1%8D%20-%20Python%E7%AF%87%20v1.0.pdf2. Gitee:https://gitee.com/hivandu/ai_cheats/blob/main/Documents/%E8%8C%B6%E6%A1%81%E7%9A%84AI%E7%A7%98%E7%B1%8D%20-%20Python%E7%AF%87%20v1.0.pdf3. 百度网盘链接:https://pan.baidu.com/s/1vSKwFa4zEA9XcK6Zpc2CRg?pwd=pua3 提取码:pua3</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://cdn.jsdelivr.net/gh/hivandu/notes/img/20240103161035.png&quot;
alt=&quot;20240103161035&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="Python" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/Python/"/>
    
    
    <category term="Python" scheme="https://hivan.me/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>32. 深度学习进阶 - Transfer Learning</title>
    <link href="https://hivan.me/32.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20Transfer%20Learning/"/>
    <id>https://hivan.me/32.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20Transfer%20Learning/</id>
    <published>2023-12-23T23:30:00.000Z</published>
    <updated>2023-12-24T05:54:53.573Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311131925028.png"alt="Alt text" /></p><span id="more"></span><p>Hi，你好。我是茶桁。</p><p>之前的课程中，咱们学习了CNN的原理，学习了pooling, fullyconnected是做什么的。还了解了理论上简单的模型也是可以做事情的，只不过在特定的一些情况下要解决问题的时候简单方法效果不太好，所以用了像LSTM，或者RNN、CNN之类的结构。</p><p>这些本质上都是在做特征的提取。一个经典的观念是，神经网络其实一共都可以分成两个部分，第一个部分是特征提取，第二个部分是分类器。像fullyconnected layer，其实就是之后再加上一个Softmax或者logSoftmax，在做分类器的实现。</p><p>前面在进入全连接层之前，也在进入Softmax或者logSoftmax之前，全部做的都是特征提取的事。</p><p>不管你是线性函数，就线性变化全连接的这种网络，还是RNN，LSTM或者CNN等等，在进入Softmax之前，这些都是在做特征提取。</p><p>Hinton当时的说法我觉得说的很有道理，就说「<strong>特征提取的作用是让相似的东西不相似，让不相似的相似</strong>」。</p><p>意思就是，我们对于任何一个神经网络来说，到最终的这个全连接，加上Softmax，之前的这些东西不管你是输入的是一个图片还是几个文字，还是说一串数据。所谓的让看起来相似的东西不相似是如果有两个图片，或者两组数据，他们在我们人看起来是比较类似的。但是假设他们的label不一样的话，我们整个特征提取的过程是把输入的这两个x，人看起来是一样的，在最后输出的这个地方要尽可能的不一样。所以送到分类器里边，它们结果差距才能大。</p><p>如果这两个东西看起来很不一样，就假如说有两只猫，一只猫特别瘦，黑黑的。一只是橘猫，特别的胖，大小也不一样。但这个在图片来说这差距是很大的。我们整个做featureextraction的时候是要把这两张图在最后变成一样，就在最后的时候变得相似。输入的时候不相似，但是经过特征提取其实要把它变相似。这样送入到了Softmax它才会产生分类的作用。</p><p>接下来讲了卷积神经网络的计算过程以及整个模型的搭建是什么样的。然后还讲了RES-NET的原理，这个也需要去理解。</p><h2 id="transfer-learning">Transfer Learning</h2><p>那么现在，咱们今天就跟大家来介绍一个比较重要的概念，深度学习共同的基础部分，就是<code>transfer learning</code>。</p><p>咱们现在的这个深度学习模型变得越来越复杂了。上节课给大家举过这些例子，不同的人提出来了不同的模型，重点给大家介绍了一个RES-NET和Inception model，也称为GoogleNET。</p><p>模型现在其实已经变得越来越复杂，这么复杂的结果是什么呢？结果是我们现在已经很难从头到尾搭建一个模型了。现在的模型结构已经这么复杂了，很少有人能有时间，或者在工作的时候有时间、有精力能从零开始一层一层的去做搭建，这是第一方面。</p><p>第二个方面，大家还发现一个特点。在结构中越接近前边虽然任务不一样，比如解决动物分类或者解决人物分类，但是越靠近前边，它们的特征相似度越高。</p><p>换句话说，有一个RES-NET专门对人物分类，还有一个是是专门做动物，它们分的类别完全不一样。但是就前边这些CNN的结果往往都很相似，而且是越往前越相似。</p><p>这是因为这些过程都是在做特征提取，如果都是一个比较相似的图片任务的话，在这个过程中特征提取其实从刚开始的时候在解析图片上的重要程度，其实要提取的东西都是类似的。</p><p>比方说识别我左手的水和我右手的手机，还有我前面站着一个美女，刚开始都是要识别它的轮廓。然后都要识别它的局部的形状，还要识别颜色...这样的一个直接的结果，其实我们每一层用的filter都是类似的，只要达到一个比较好的结果，前面的这些filter都是类似的。</p><p>filter类似是因为filter控制的是我们要提取什么重要特征。那么我们就发现从前到后，其实越是前边越是比较简单的特征，线、块这些，到后边越来越综合。</p><p>有了这个之后大家就发现，既然现在模型这么复杂，从头到尾要搭建一个模型已经很难了，我们可以直接用这个模型的结构。</p><p>第二我们发现不仅模型的结构可以，模型的权重都也可以。可以用这个模型的权重来训练，直接把这个模型的权重拿过来。</p><p>其实也就是说，我们可以直接下载一个模型，把别人训练好的权重一起拿过来，这些东西就是一堆数字。然后它是在taska上弄的，我把它用到了taskb上。训练的时候让它不要进行反向传播，在进入全连接层的时候再进行反向传播。</p><p>大家把这种学习方式就叫做<code>transfer Learning</code>，迁移学习。我们平时日常在工作的时候经常会这么做。</p><p>客观上来讲，不同的任务，任务越类似肯定迁移的时候越好迁移。所以说其实它和任务的相似度以及和数据量的相对大小很有关系。</p><p>假设我们两个任务，A和B。这两个任务，A是分类狗，B是分类狼，A原本训练数据集是100W，B的训练集是1W。那么这两个任务比较而言，任务相似度非常大，原任务相对新任务数据量比较大，这个时候基本上迁移学习就非常好迁移，我们都可以不去更改进入全连接层之前的所有内容就可以进行迁移，只需要更改全连接层。也就是特征提取的部分完全平移。</p><p>那么如果A任务还是分类狗，B任务是分类汽车。A原本训练数据集是100W，B的训练集是5000W。那这两个任务比较而言，任务相似度非常小，原任务相对新任务数据量是小的，这个时候迁移学习就变得很困难，可能也只有图像线条，颜色这些个特征提取的部分可以迁移，基本是特征提取的最前边的部分。</p><p>所以，TransferLearning的容易程度，在一个二维平面直角坐标系内的两个相关项，也就是x和y轴就是任务相似度和原任务相对新任务数据量的大小。</p><p>如果重新训练，怎么样来transfer呢？说了这么多，还是直接来看一个实例,来看看我们具体该如何做「冻结」。</p><p>用的这个数据集,cifar10，这也是一个很经典的数据，它是十个典型的很常见的物品的分类。</p><p>咱们先引入必要的库，然后down数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> icecream <span class="hljs-keyword">import</span> ic<br><br><br>cifar_10 = torchvision.datasets.CIFAR10(<span class="hljs-string">&#x27;.&#x27;</span>, download=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>这个文件一共170多兆，大部分人物提取的特征差不多，所以权重可以不用更新，用其他相似任务的参数，相当于新模型初始化的时候，理解为更接近在最优点附近。</p><p>它里面的每一个数据的类型是一个PRL的image,要在PyTorch里对这个图片进行使用，我们需要进行一个预处理。我们需要在前面定义一个方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">preprocess = transforms.Compose([<br>    transforms.Resize(<span class="hljs-number">224</span>),<br>    transforms.CenterCrop(<span class="hljs-number">224</span>),<br>    transforms.ToTensor()<br>])<br></code></pre></td></tr></table></figure><p>首先, 我们要先Resize，然后用一个CenterCrop，让图片以中心扩散进行切割。如果有些图片不是正方形，那么第二个操作就是把中间的部分裁一个正方形出来。最后再把它变成一个Tensor。</p><p>然后我们需要修改一下数据获取数据时的<code>transform</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">cifar_10 = torchvision.datasets.CIFAR10(<span class="hljs-string">&#x27;.&#x27;</span>, download=<span class="hljs-literal">True</span>, transform=preprocess)<br></code></pre></td></tr></table></figure><p>现在看一下, cifar_10的数据就变成tensor了，shape是[3,224,224]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">cifar_10[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].shape<br><br>---<br>torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>])<br></code></pre></td></tr></table></figure><p>得到Tensor数据之后，要训练的时候得一次一次的取不同的数值出来，我们要做SGD，随机梯度下降。那么在做这个的时候有一种方法，写个复循环然后每次随机取一些index，再把这些index的值给它取出来，这是一种方法。</p><p>还有一种方法，我们可以直接用<code>DataLoader</code>，声明了之后每次要生成一个迭代器，每次会输出一些内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_loader = torch.utils.data.DataLoader(cifar_10, batch_size=<span class="hljs-number">512</span>, shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>如果要把所有的数据传输进去，它有5万个照片太大了，内存吃不消。所以要把它做成SGD，要每次随机取一个东西。</p><p>然后我们来定义一个RES-NET：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">resnet = torchvision.models.resnet18()<br></code></pre></td></tr></table></figure><p>有了这样的RES-NET之后，它输出的是1000维的，而我们这里其实是需要一个10维的，那我们就需要把它的最后一层给它重新做一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">feature_num = resnet.fc.in_features<br>resnet.fc = nn.Linear(feature_num, <span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><p>如果我们没有这一句，我们可以来看看它会输出什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">ic(resnet(cifar_10[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].unsqueeze(<span class="hljs-number">0</span>)))<br><br>---<br>ic| resnet(cifar_10[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].unsqueeze(<span class="hljs-number">0</span>)): tensor([[-<span class="hljs-number">6.9484e-01</span>, ..., <span class="hljs-number">1008e+00</span>]], grad_fn=&lt;AddmmBackward0&gt;)<br>tensor([[-<span class="hljs-number">6.9484e-01</span>, ..., <span class="hljs-number">1.1008e+00</span>]],<br>       grad_fn=&lt;AddmmBackward0&gt;)<br></code></pre></td></tr></table></figure><p>输出的是一个很长的东西，其实是有1,000维的，这里输出了1,000个。</p><p>现在如果把它的最后一层全连接层改了，变成10分类，因为这个cifar10是一个是分类问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">feature_num = resnet.fc.in_features<br>resnet.fc = nn.Linear(feature_num, <span class="hljs-number">10</span>)<br>ic(resnet(cifar_10[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].unsqueeze(<span class="hljs-number">0</span>)))<br></code></pre></td></tr></table></figure><p>改完之后输出的数据就是10维的了，大家可以自己去跑一下代码，我这里就不贴了。</p><p>接着我们再来生成一个loss函数和一个优化器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">criterion = nn.CrossEntropyLoss()<br>optimizer = torch.optim.SGD(resnet.parameters(), lr=<span class="hljs-number">1e-3</span>, momentum=<span class="hljs-number">0.9</span>)<br></code></pre></td></tr></table></figure><p>criterion是测量尺度、考核标准的意思。parameters是要把所有参数进行拟合，进行重新训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python">epochs = <span class="hljs-number">2</span><br>losses = []<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    epoch_loss = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        ic(epoch, i)<br>        predicts = resnet(images)<br>        loss = criterion(output, labels)<br>        optimizer.zero_grad()<br><br>        loss.backward()<br>        optimizer.step()<br><br>        epoch_loss += loss.item()<br><br>        <span class="hljs-keyword">if</span> i &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Epoch: &#123;&#125; batch: &#123;&#125;, loss ==&gt; &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(epoch, i, epoch_loss / i))<br>    losses.append(epoch_loss / i)<br><br>plt.plot(losses)<br><br>---<br><br>0it [<span class="hljs-number">00</span>:<span class="hljs-number">00</span>, ?it/s]<br>ic| epoch: <span class="hljs-number">0</span>, i: <span class="hljs-number">0</span><br>1it [<span class="hljs-number">00</span>:<span class="hljs-number">53</span>, <span class="hljs-number">53.71</span>s/it]ic| epoch: <span class="hljs-number">0</span>, i: <span class="hljs-number">1</span><br>2it [01:<span class="hljs-number">42</span>, <span class="hljs-number">50.98</span>s/it]<br>Epoch: <span class="hljs-number">0</span> batch: <span class="hljs-number">1</span>, loss ==&gt; <span class="hljs-number">4.71190333366394</span><br>ic| epoch: <span class="hljs-number">0</span>, i: <span class="hljs-number">2</span><br>...<br>98it [<span class="hljs-number">1</span>:<span class="hljs-number">12</span>:04, <span class="hljs-number">44.13</span>s/it]<br>Epoch: <span class="hljs-number">1</span> batch: <span class="hljs-number">97</span>, loss ==&gt; <span class="hljs-number">1.7719330222336287</span><br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311131925029.png"alt="Alt text" /></p><p>现在是这么个结果, 我们先来保存一下，我创建了一个32.log,用于暂时保存咱们的结果。那因为我训练的时候加了一个tqdm，所以也把时间打印了出来，不过为了避免代码上的误解，所以代码我还是给的没有加tqdm的样子。</p><p>现在要迁移怎么迁移呢？很简单，第一步我们需要改一下我们的RES-NET。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">resnet = torchvision.models.resnet18(pretrained=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>我们加一个参数<code>pretrained</code>，然后将值设为<code>True</code>，现在要保留它的数据，保留之前训练的权重。</p><p>第二步要冻结它的这些参数，把RES-NET里边所有的parameters，每一个都有一个requiresgrad，给它定义成false。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> resnet.parameters():<br>    param.requires_grad = <span class="hljs-literal">False</span> <span class="hljs-comment"># frozen weights</span><br></code></pre></td></tr></table></figure><p>设置成false之后进行反向传播的时候这个值就不更新了。不更新的话那就相当于冻结了。</p><p>之前写的<code>resnet.fc</code>就相当于重写了fc分类层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">resnet.fc = nn.Linear(feature_num, <span class="hljs-number">10</span>) <span class="hljs-comment"># rewrite fc classifier</span><br></code></pre></td></tr></table></figure><p>假设现在的任务和原来任务不相似，或者说现在原来数据量和现在数据量相比偏小，那么对于这个RES-NET，不能把它所有的requiresgrad设置成false，要把它前面部分的给它设置成false，后边设置成true。</p><p>重写了这个FCclassifire之后，新声明的参数默认它是需要进行梯度下降的，所以不需要在这写成false。就在这里，这个FC的grad默认是true。</p><p>那到这一步， transfer就结束了，我们可以重新训练来看看。</p><p>你会发现，时间上明显快多了。这个就是因为咱们这次训练的参数少了很多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">0it [<span class="hljs-number">00</span>:<span class="hljs-number">00</span>, ?it/s]ic| epoch: <span class="hljs-number">0</span>, i: <span class="hljs-number">0</span><br>1it [<span class="hljs-number">00</span>:<span class="hljs-number">17</span>, <span class="hljs-number">17.99</span>s/it]ic| epoch: <span class="hljs-number">0</span>, i: <span class="hljs-number">1</span><br>2it [<span class="hljs-number">00</span>:<span class="hljs-number">34</span>, <span class="hljs-number">16.92</span>s/it]<br>Epoch: <span class="hljs-number">0</span> batch: <span class="hljs-number">1</span>, loss ==&gt; <span class="hljs-number">5.019284725189209</span><br>ic| epoch: <span class="hljs-number">0</span>, i: <span class="hljs-number">2</span><br>3it [<span class="hljs-number">00</span>:<span class="hljs-number">50</span>, <span class="hljs-number">16.54</span>s/it]<br>Epoch: <span class="hljs-number">0</span> batch: <span class="hljs-number">2</span>, loss ==&gt; <span class="hljs-number">3.7500953674316406</span><br>ic| epoch: <span class="hljs-number">0</span>, i: <span class="hljs-number">3</span><br>...<br>98it [<span class="hljs-number">26</span>:04, <span class="hljs-number">15.96</span>s/it]<br>Epoch: <span class="hljs-number">1</span> batch: <span class="hljs-number">97</span>, loss ==&gt; <span class="hljs-number">1.108948134884392</span><br>...<br></code></pre></td></tr></table></figure><p>之前我们每一轮训练几乎都要花个50s左右，现在基本在16左右，速度上提升了3倍。从总时间上我们也可以看出来，训练速度提升了好几倍，从原来的一小时12分钟，直接降到了26分钟。并且，loss也有所提升。</p><p>那么我们该怎么去看这个模型的层数，确定哪些是在前面部分，哪些实在后面呢？对于一个模型而言，最简单的办法就是直接print出来，比如说咱们的resnet18：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision.models <span class="hljs-keyword">import</span> resnet18<br><span class="hljs-built_in">print</span>(resnet18())<br><br>---<br>ResNet(<br>  (conv1): Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">7</span>, <span class="hljs-number">7</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), padding=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), bias=<span class="hljs-literal">False</span>)<br>  (bn1): BatchNorm2d(<span class="hljs-number">64</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>  (relu): ReLU(inplace=<span class="hljs-literal">True</span>)<br>  (maxpool): MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>  (layer1): Sequential(<br>    (<span class="hljs-number">0</span>): BasicBlock(<br>      (conv1): Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn1): BatchNorm2d(<span class="hljs-number">64</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      (relu): ReLU(inplace=<span class="hljs-literal">True</span>)<br>      (conv2): Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn2): BatchNorm2d(<span class="hljs-number">64</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>    )<br>    (<span class="hljs-number">1</span>): BasicBlock(<br>      (conv1): Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn1): BatchNorm2d(<span class="hljs-number">64</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      (relu): ReLU(inplace=<span class="hljs-literal">True</span>)<br>      (conv2): Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn2): BatchNorm2d(<span class="hljs-number">64</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>    )<br>  )<br>  (layer2): Sequential(<br>    (<span class="hljs-number">0</span>): BasicBlock(<br>      (conv1): Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn1): BatchNorm2d(<span class="hljs-number">128</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      (relu): ReLU(inplace=<span class="hljs-literal">True</span>)<br>      (conv2): Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn2): BatchNorm2d(<span class="hljs-number">128</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      (downsample): Sequential(<br>        (<span class="hljs-number">0</span>): Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), bias=<span class="hljs-literal">False</span>)<br>        (<span class="hljs-number">1</span>): BatchNorm2d(<span class="hljs-number">128</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      )<br>    )<br>    (<span class="hljs-number">1</span>): BasicBlock(<br>      (conv1): Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn1): BatchNorm2d(<span class="hljs-number">128</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      (relu): ReLU(inplace=<span class="hljs-literal">True</span>)<br>      (conv2): Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn2): BatchNorm2d(<span class="hljs-number">128</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>    )<br>  )<br>  (layer3): Sequential(<br>    (<span class="hljs-number">0</span>): BasicBlock(<br>      (conv1): Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn1): BatchNorm2d(<span class="hljs-number">256</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      (relu): ReLU(inplace=<span class="hljs-literal">True</span>)<br>      (conv2): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn2): BatchNorm2d(<span class="hljs-number">256</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      (downsample): Sequential(<br>        (<span class="hljs-number">0</span>): Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), bias=<span class="hljs-literal">False</span>)<br>        (<span class="hljs-number">1</span>): BatchNorm2d(<span class="hljs-number">256</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      )<br>    )<br>    (<span class="hljs-number">1</span>): BasicBlock(<br>      (conv1): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn1): BatchNorm2d(<span class="hljs-number">256</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      (relu): ReLU(inplace=<span class="hljs-literal">True</span>)<br>      (conv2): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn2): BatchNorm2d(<span class="hljs-number">256</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>    )<br>  )<br>  (layer4): Sequential(<br>    (<span class="hljs-number">0</span>): BasicBlock(<br>      (conv1): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn1): BatchNorm2d(<span class="hljs-number">512</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      (relu): ReLU(inplace=<span class="hljs-literal">True</span>)<br>      (conv2): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn2): BatchNorm2d(<span class="hljs-number">512</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      (downsample): Sequential(<br>        (<span class="hljs-number">0</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), bias=<span class="hljs-literal">False</span>)<br>        (<span class="hljs-number">1</span>): BatchNorm2d(<span class="hljs-number">512</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      )<br>    )<br>    (<span class="hljs-number">1</span>): BasicBlock(<br>      (conv1): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn1): BatchNorm2d(<span class="hljs-number">512</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>      (relu): ReLU(inplace=<span class="hljs-literal">True</span>)<br>      (conv2): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), bias=<span class="hljs-literal">False</span>)<br>      (bn2): BatchNorm2d(<span class="hljs-number">512</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br>    )<br>  )<br>  (avgpool): AdaptiveAvgPool2d(output_size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (fc): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">1000</span>, bias=<span class="hljs-literal">True</span>)<br>)<br></code></pre></td></tr></table></figure><p>这次我将结果打全，我们可以清晰的看到这个模型里从上到下，从前到后的每一层，最后一层是一个fc。</p><p>那除此之后，其实我们可以借用第三方库来进行计算，有一个库叫做<code>torchsummary</code>，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchsummary <span class="hljs-keyword">import</span> summary<br>summary(resnet18(), (<span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br><br>---<br>----------------------------------------------------------------<br>        Layer (<span class="hljs-built_in">type</span>)               Output Shape         Param <span class="hljs-comment">#</span><br>================================================================<br>            Conv2d-<span class="hljs-number">1</span>         [-<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">112</span>, <span class="hljs-number">112</span>]           <span class="hljs-number">9</span>,<span class="hljs-number">408</span><br>            ...<br>AdaptiveAvgPool2d-<span class="hljs-number">67</span>            [-<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]               <span class="hljs-number">0</span><br>           Linear-<span class="hljs-number">68</span>                 [-<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>]         <span class="hljs-number">513</span>,<span class="hljs-number">000</span><br>================================================================<br>Total params: <span class="hljs-number">11</span>,<span class="hljs-number">689</span>,<span class="hljs-number">512</span><br>Trainable params: <span class="hljs-number">11</span>,<span class="hljs-number">689</span>,<span class="hljs-number">512</span><br>Non-trainable params: <span class="hljs-number">0</span><br>----------------------------------------------------------------<br>Input size (MB): <span class="hljs-number">0.57</span><br>Forward/backward <span class="hljs-keyword">pass</span> size (MB): <span class="hljs-number">62.79</span><br>Params size (MB): <span class="hljs-number">44.59</span><br>Estimated Total Size (MB): <span class="hljs-number">107.96</span><br>----------------------------------------------------------------<br></code></pre></td></tr></table></figure><p>这个去监测模型的层数和信息就更好一些，可以很直观的看到每一层以及整个模型的相关信息。不管是你自己的模型还是第三方预先训练好的其实都可以。我们在后面设置了一下输入的大小，设置了之后，summary在后面参数一共多少就一个一个都给你显示出来了。我们刚才输入的(3,224,224)，然后从第一层开始的<code>Output Shape</code>是多少，一层一层的向下就直接有了。</p><p>这两个方式都还是很有用的。</p><p>那么之后做训练的时候大家要对几个数字稍微多一点敏感性，我们来看，首先我们定义一个loss函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy</span>(<span class="hljs-params">y, yhat</span>): <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(y*np.log2(yhat))<br></code></pre></td></tr></table></figure><p>然后我们输入下面几个值做测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">cross_entropy([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0.5</span>]*<span class="hljs-number">2</span>)<br>cross_entropy([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0.2</span>]*<span class="hljs-number">5</span>)<br>cross_entropy([<span class="hljs-number">0</span>]*<span class="hljs-number">9</span> + [<span class="hljs-number">1</span>], [<span class="hljs-number">0.1</span>] * <span class="hljs-number">10</span>)<br>cross_entropy([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0.33</span>] * <span class="hljs-number">3</span>)<br><br>---<br><span class="hljs-number">1.0</span><br><span class="hljs-number">2.321928094887362</span><br><span class="hljs-number">3.321928094887362</span><br><span class="hljs-number">1.5994620704162712</span><br></code></pre></td></tr></table></figure><p>transferlearning基于的是模型从前往后。前面层学的东西比较基础，到后边学的抽象层次越来越高，看到的是更复杂的一些。</p><p>那咱们现在就再来演示一下它到底学的学到都是什么东西。那么为了看一下这个到底学的是什么，我再次贡献一下自己。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311131925030.png"alt="Alt text" /></p><p>这个是早些时候我一个同学帮我画的头像，就拿它来看吧。</p><p>首先，我们前面看到打印结果了，resnet18的第一层是conv1，我们来看看第一层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br>preprocess = transforms.Compose([<br>    transforms.Resize(<span class="hljs-number">224</span>),<br>    transforms.CenterCrop(<span class="hljs-number">224</span>),<br>    transforms.ToTensor()<br>])<br>resnet = torchvision.models.resnet18(pretrained=<span class="hljs-literal">True</span>)<br><br>myself = preprocess(Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./assets/chaheng2.png&#x27;</span>))<br>resnet.conv1(myself.unsqueeze(<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure><p>然后我们就可以看到一堆的tensor数据，这个<code>unsqueeze</code>是将数据改变了一下结构，从<code>myself</code>变成了<code>[[myself]]</code>，改成这样是因为torch每次接收的是一个batch的东西，直接输入一个图片是不行的。</p><p>我们看一下它的这个输出,第一个卷积的输出是什么：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311131925031.png"alt="Alt text" /></p><p>我们可以看到，它的shape是<code>[1, 64, 112, 112]</code>,那这里边的分别是什么？</p><p>第一个维度，这个1是batch的数量。64是filter的channel，所以它输出了64张图片。后面的112和112是一组数据，从这个数据来看，这个图片经过卷积之后，经历了一个缩小的变化。从原来的224缩小到了112，经历了一个下采样。</p><p>接着咱们来看一下具体的数据内容，看看output第0个的内容是什么样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.imshow(output[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].detach())<br></code></pre></td></tr></table></figure><p>因为结果还在内存里，所以我们永乐一个<code>detach()</code>。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311131925032.png"alt="Alt text" /></p><p>它把我的轮廓给提出来了。</p><p>我们再来看看别的是什么样，我们改成<code>[0][2]</code>：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311131925033.png"alt="Alt text" /></p><p>这个貌似是将背景扣了。</p><p>我们不一张一张来看了，咱们来将探索过程写个循环，看一下它到底都做了什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">width = <span class="hljs-number">8</span><br>fig, ax = plt.subplots(output[<span class="hljs-number">0</span>].shape[<span class="hljs-number">0</span>] // width, width, figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>))<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(output[<span class="hljs-number">0</span>].shape[<span class="hljs-number">0</span>]):<br>    ix = np.unravel_index(i, ax.shape)<br>    plt.sca(ax[ix])<br>    ax[ix].title.set_text(<span class="hljs-string">&#x27;filter-&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i))<br><br>    plt.imshow(output[<span class="hljs-number">0</span>][i].detach())<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311131925034.png"alt="Alt text" /></p><p>它这64个学到的几乎每个都不一样，那有些是有用的，有些是没用的。有些是从边缘层面上，比如说filter20就是从边缘上，而有一些，比如filter21就是从颜色上。</p><p>那么如果我们现在想把第二个、第三个、第四个这些都拿出来的话怎么办？当然理论上可以沿着它的结构给一层一层解出来，但是PyTorch里面给咱们的提供了一个比较简单的方法。</p><p>那刚才写的那个代码，其实是在进行前向传播，就我们刚才写代码就是在模拟它的前向传播，forward。PyTorch就给我们提供了一个很方便东西，它可以给前向传播及反向传播的时候注册一个函数。就比如说：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">conv_model = [m <span class="hljs-keyword">for</span> _, m <span class="hljs-keyword">in</span> resnet.named_modules() <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, torch.nn.Conv2d)]<br><br><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> conv_model:<br>    m.register_forward_hook()<br></code></pre></td></tr></table></figure><p>我们现在把resnet里边所有的model拿出来，然后如果这个model它是卷机，给这些所有的模型注册一个函数。这个函数是是他在进行前向传播的时候会自己调用的，就不需要咱们再手动的去写了。</p><p>那我们现在就来将之前写的内容抽象成一个函数<code>visualize_model</code>，在定义这个函数的时候需要注意一下PyTorch的相关API，</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311131925035.png"alt="Alt text" /></p><p>那我们在定义的时候，也就需要一样传递这些参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">visualize_model</span>(<span class="hljs-params">model, input_, output</span>):<br>    width = <span class="hljs-number">8</span><br>    ...<br>    plt.show()<br></code></pre></td></tr></table></figure><p>这样前向传播的时候,它会自动调用。现在我们就可以让它来进行前向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> conv_model:<br>    m.register_forward_hook(visualize_model)<br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    resnet(myself.unsqueeze(<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure><p><code>no_grad</code>的意思是不让它进行反向传播，只进行前向传播。</p><p>我们在观察它每一层的结果的时候，就会发现越到后面就越抽象，我们捡中间某一张贴出来来看。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311131925036.png"alt="Alt text" /></p><p>就基本上，这个时候还能勉强辨认出是个任务头像，再往后的结果，我肉眼已经分辨不出来它是个啥了。就这是整个模型一层一层学到的东西，它里边是从刚开始的时候比较的底层、比较的基础，后边会提取的东西越来越多。</p><p>就咱们在这里所做的这种权重可视化，有一个比较有趣的应用：<code>deep dream</code>，它就是将刚才这些学到的靠后的权重，然后应用到一张图片上。就我们刚刚可视化那种层数再应用到一些新图片上，就会产生这样的效果。如果感兴趣的可以自己试一下。</p><p>好那咱们这个RES-NET和RES-NET可视化，以及transferlearning的内容，到这里就可以告一段落了。整个的深度学习的基础部分，也就到这里结束了。</p><p>最后，我们来留一个小作业。</p><h2 id="作业">作业</h2><p>那么本节课的最后，给大家留一个小作业，稍微还是有点难度的，需要大家自己去查阅相关手册才行，不过知识点都是讲过的。作业内容为<strong>「对验证码进行识别」</strong>。</p><blockquote><ol type="1"><li><p>练习内容：训练一个模型，对验证码中的字符进行分类识别，并最终完成验证码识别的任务。</p></li><li><p>数据集：数据集内包含0-9以及A-Z一共36个字符，训练集中每个字符有50张图片，验证集中每个字符有10张图片，验证码数据集是由随机去除的4个字符图片拼接而成。</p></li><li><p>需要的相关知识：</p></li></ol><ul><li>数据读取</li><li>使用torch搭建、训练、验证模型</li><li>模型预测于图片切分</li></ul></blockquote><p>好，给大家提供下思路，我们将我们需要解决的问题分成四步：第一个，先建立字符对照表，第二个，要定义一个<code>datasets</code>和一个<code>dataloader</code>。第三个，需要定义网络结构。 第四个，定义模型训练函数。最后，就是验证训练结果。</p><p>数据集请关注「坍缩的奇点」后从原文下载。https://mp.weixin.qq.com/s/v_4OOMB_Gg-a1V3a399NEQ</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311131925028.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>31. 深度学习进阶 - 全连接层及网络结构</title>
    <link href="https://hivan.me/31.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%8F%8A%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"/>
    <id>https://hivan.me/31.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%8F%8A%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/</id>
    <published>2023-12-19T23:30:00.000Z</published>
    <updated>2023-12-22T09:26:25.990Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111936981.png"alt="Alt text" /></p><span id="more"></span><p>Hi，你好。我是茶桁。</p><p>之前的课程咱们学习了卷积以及池化，那到底卷积是如何构成卷积神经网络的呢？我们这节课来好好讲一下。</p><h2 id="全连接层">全连接层</h2><p>整个卷积的运算就是经过卷积，再经过pooling，再经过卷积。会把这个图形变的很小。然后再经过pooling，又会一直把我们的特征变得越来越小，之后有一个很重要的层，这个层叫做全连接层。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934379.png"alt="Alt text" /></p><p>后面的几个柱状图就是它的线性变化，就是它的全连接层。</p><p>先是将图片卷积、池化变小，变成很小的高级特征，然后拉平之后进入全连接层进行线性变化。这就是卷积操作的整个工作流，也是为什么卷积操作需要的参数少的原因。</p><p>我们在这里重点说一下全连接层。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934380.png"alt="Alt text" /></p><p>我们做了很多pooling，很多卷积之后，我们会生成一个很厚的一个值。把很厚的这个值给他拉平，在PyTorch里面直接就flatten,或者用reshape直接进行，把它拉平成一个1乘以n的一个向量。然后给这个1乘以n进行熟悉的<code>wx+b</code>。</p><p>我们对它进行线性变化，第一是对它的维度进行了变化。假如要给它变成一个10分类，纬度进行的变化。</p><p>另外一点，我们每一层都会有不同的特征点，这些特征点代表这图像不同的位置把它抽象成的值。然后一层一层的，又是不同的filter的结果，提取出来的不同的特征。比如横向，竖向之类的。机器可能还会自动提取一些颜色，形状等等。</p><p>那么现在我们要把这些东西进行一个综合考量，要把这些信息全部拿起来综合做个判断。比如我们有三个filter,也就是有三层，这三层里面拿出四个位置。那么拉平的画，就变成3乘以4，这里面有12个数值。这12个数值提取出来通过不同的方式了，关注点不同，提出来的12高级特征。</p><p>现在要把这12个高级特征全盘考虑、综合考虑。我们要给这些数据加一个不同的权重。就要给它做一个<code>wi * xi</code>，就给它这些全盘综合做了一个权重的这个赋值。</p><p>所以说，全连接不仅对维度进行了变化，它还对之前提取出来的局部信息进行了综合，这个就是全连接层的作用。既进行了变化又进行了维度信息的综合。</p><p>所以说，大家看一下</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934381.png"alt="Alt text" /></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934382.png"alt="Alt text" /></p><p>这些不同的著名的网络结构，都是进行完之后要进行线性变化，线性变化之后把它变到我们期望的target上，就是最前面的这些东西进行综合。</p><p>算出来这个数值之后，然后用全连接层进行分类。但是全连接层不一定是只能进行分类，其实还进行特征的一个变化。</p><p>进行线性变化完了之后，通过Softmax，然后再给它进行cross-entropy，就可以求出它的loss值了。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934383.png"alt="Alt text" /></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934384.png"alt="Alt text" /></p><p>其实最近几年，就从2019年左右开始呢，其实大家慢慢的不用Softmax和crossentropy了，当然用这个也可以。为什么不用了呢？</p><p>比方我现在有三个图片，IMAGE1、IMAGE2和IMAGE3，对应的label分别是<code>3、5、6</code>。那么要做cross-entropy的时候，就要把3变成<code>[0,0,1,0,0,0,0,0]</code>，然后5和6都要进行变化。然后才能跟Softmax预测出来这个probability做cross-entropy。也就是说，在这里要进行一次one-hot编码。结果后来就发现可以做一个简化操作，进行了Softmax之后给它前面加个log。</p><p>假如说，Softmax之后是<code>0.1, 0.3, 0.3, 0.2, 0.1</code>,给它加个log,就会是一个负的比较大的数字，越接近于1，比方0.99，越接近于1结果会越接近于0，越远离1，这个负的值会越大。</p><p>所以现在大家会有一个非线性变化，叫做logSoftmax，出来的结果就是负的。然后还有一个loss叫做<code>NLLloss</code>,<code>negative log likelihood loss</code>，这个在PyTorch里边也有。</p><p>这个有趣的地方就来了，如果我们它的label是3，直接来看一下log之后的值是不是<code>-3</code>,给它再取个负号，那么就直接说这个的loss是<code>3</code>。如果它的label是5，那么log之后是另外一个值，假如说是<code>-0.7</code>，那么它取5，我们发现结果是<code>-0.7</code>，加个负号，它的loss直接就是<code>0.7</code>。</p><p>这样就不需要进行one-hot编码了，而且也能达到一个效果，就是我们期望的地方越接近于1，loss越接近于0。</p><p>所以，现在在工作中，我们看大量代码都开始这么做了，相当于是一个简化板的Softmax。</p><p>那这个呢就是我们整个卷积神经网络的工作流程，全连接层的作用大家一定要知道。</p><p>好，我们做一个总结。第一节课，给大家讲解卷积的原理。那么什么是卷积神经网络呢？只要用了卷积(Conv)这个操作的网络,它就叫卷积神经网络。所以理论上，你可以让一个图形先经过卷积，再经过RNN，再经过卷积，再经过RNN，都可以。这个你既可以叫它卷积网络，也可以叫它循环神经网络。</p><p>然后呢跟大家说了CNN可以用在很多地方，比方说分类，探测，还有分割，其实背后都是卷积神经网络在做。</p><p>还有给大家讲了filters, padding,stride和channel，它的作用。除此之外，我们讲了Parameterssharing和Location Invariant。</p><p>在整个过程中，我们哪一层做卷积，哪一层做pooling，线性变化做几层，是不是纯靠经验？说白了这个确实还是纯靠经验，所以有一个很重要的特点就是我们需要去借鉴，我们需要去借鉴前人的经验。</p><h2 id="几种神经网络结构">几种神经网络结构</h2><p>我们需要看前人的网络结构是怎么搭的，有几种比较重要的结构，LE-NET5，ALEX-NET。Alex那个net结构就是2012年ImageNet取得第一名的，上面有图。</p><p>它的特点就是第一次用Relu去做了非线性变化,作用就会进行的比较快，它还在GPU上进行运算。</p><p>Relu就是一个非线性变化，如果把它做了卷积操作之后，给它再加个Relu，可以把它值再进行一个非线性变化就可以了，就是把它卷积出来的结果做了一个非线性变化。</p><p>GPU运算的作用是什么呢？GPU为什么重要？</p><p>假设现在有一张1万 * 1万的一张图，有3 *3的卷积核，如果说原始的状态我们得先从左到右再从上到下的做。我们得进行998乘以998次移动。</p><p>有GPU的话，我们可以让其中一部分在GPU的某个地方进行计算，另外一部分同时在GPU的另外一个地方计算，就可以分布式的。因为GPU所做的事情就是把矩阵运算可以分布式的在不同的地方并行运算。</p><p>这就是为什么有GPU玩游戏不卡，因为加载图片的时候它一部分图片在GPU某个地方加载，另外一部分图片在GPU另外一个地方加载，这是同时一起加载的。</p><p>如果年龄在30岁以上的小伙伴应该知道，以前看网页的时候那个大的图片会一行一行显示出来，就90年代末那会儿，图片是一行一行一行显示出来的。而对于GPU的话，显示图片是一块一块一起去渲染的。</p><p>那么对于卷积神经网络来说，这一块一块的filters，也是一起渲染一起计算的。所以说在做一层的计算的时候它就快了。而且如果你的GPU足够多，你还可以让它每一层的filters也并行计算。每一层的filters在每一块上又可以快速计算。</p><p>所以有了GPU的运行速度可以快十几倍，二十几倍，甚至上百倍都可以。</p><p>然后是VGG-NET。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934385.png"alt="Alt text" /></p><p>VGG-NET是第一个真正意义上的深度神经网络，我们看这张图，它门一层都向下做了一个下采样。不断的下采样的结果是可以获得一些非常深的feature，或者一些非常高层次的feature。</p><p>VGG当时取得的效果也非常的好，也学的非常好。但是随着VGG正式的把我们带到深度神经网络这个过程中，我们就发现当网络特别深的时候会产生一个问题。</p><p>我们回忆一下，之前的课程中有讲过，当网络特别深的时候会产生什么问题？</p><p>我们之前课程里有说，当网络特别深的时候就会产生梯度消失。</p><p>首先做这个变化的时候它的体现倒不是说就是会梯度消失，而是和梯度消失很类似。就是这个图片在前面运行的特别长，如果这个filter有几个值比较小，那么值经过filter值会变得很小，再经过一个filter又会变得很小。</p><p>到最后，原来的图像区别还挺大的，经过几次卷积之后呢，就都变成了很小的一些数字，展示出来就近乎一张纯色的图片。</p><p>这个其实在哲学上也可以理解一下，当你的抽象层次特别特别高的时候，全世界的东西都一样。对吧，就很佛系，科学尽头是神学。当你的抽象层次极高的时候，你看全世界所有东西都一样，在CNN里也一样，当你的这个东西足够长的时候，最后得到的东西它都差不多。</p><p>所以为了解决这个问题，就提出来一个重要的神经网络叫做RES-NET。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934386.png"alt="Alt text" /></p><p>这个叫做残差网络，这个残差网络是非常重要的，是微软亚研当年提出来的。</p><p>2015年用了RES-NET造成了计算机视觉的识别率超过了人类眼睛的识别率，所以2016年是AI在产业中开始落地的第一年。</p><p>当然它的原理并不难，但是经过这样的一个修改，使得我们计算机识别网络的准确度超过了人类，然后开始了这个产业落地。</p><p>截图中是RES-NET的一个Block。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResBlock</span>(nn.Module):<br>    ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        out = self.conv(x)<br>        out = self.batch_normal(out)<br>        out = torch.relu(out)<br><br>        <span class="hljs-keyword">return</span> out + x<br></code></pre></td></tr></table></figure><p>向前运算的时候输入x，经过了卷积，之后再给它进行一个Batchnormalization，它的那个值就把小的变大，大的变小。然后再进行一个Relu非线性变化，输出的是out加了个x。</p><p>这句话就是我们所谓的Residual的意思,就是理论上我们只要输出out就行了，但是为啥要加x呢？因为当经过很多层之后，out可能会变成0，变成一个纯色图片。所以把x加上，就是它还是保留了它的主要的图片信息，但是它在out上又有一些小的变化。这就是RES-NET的原理。</p><p>如果我们现在想做一个深的RES-NET的话怎么办？你给它输入一个三维的图片，比方说32个filters。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934387.png"alt="Alt text" /></p><p>然后我们进行了一个ResBlock:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NetResDeep</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">...</span>):<br>        ...<br>        self.resblocks = nn.Sequential(<br>            *[ResBlock(n_chans=n_chans) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_blocks)]<br>        )<br>        ...<br>    ...<br></code></pre></td></tr></table></figure><p>这个地方其实相当于是ResBlock之后，输出的<code>x+out</code>又给它输入到了一个ResBlock，又是一个<code>x+out</code>。</p><p>我们这里<code>Sequential</code>的意思就是做完了这个，它的输出直接给下一个做输出。</p><p>在这个过程中，先让x进来做卷积、做非线性变化、做pooling。然后把它送到一串ResBlock:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NetResDeep</span>(...):<br>    ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        out = F.max_pool2d(torch.relu(self.conv(x)), <span class="hljs-number">2</span>)<br>        out = self.resblocks(out)<br>        out = F.max_pool2d(out, <span class="hljs-number">2</span>)<br>        out = out.view(-<span class="hljs-number">1</span>, <span class="hljs-number">8</span> * <span class="hljs-number">8</span> * self.n_chans)<br>        out = torch.relu(self.fc1(out))<br>        out = self.fc2(out)<br>        ...<br></code></pre></td></tr></table></figure><p>这一串ResBlock,它有很多个ResBlock，一层一层运行下来。之后，做了一个pooling,之后做拉平，拉平之后在做一个全连接，就是对个权重进行线性变化，变化完了之后再加了非线性变化，最后再做一个线性变化。</p><p>这里的<code>fc2</code>，我们定义的维度是10，意思就是把它要变成一个10分类的任务。</p><p>然后我们再给它做个logSoftmax，或者说cross-entropy，或者是NLL，就可以给它进行反向传播了。</p><p>这整个过程就是咱们的RES-NET。</p><p>只要这个网络有ResBlock，或者类似于ResBlock的，它都叫RES-NET。就像只要有卷积这个单元的网络都叫卷积网络一样。</p><p>这句话的意思是说，RES-NET其实有很多种。比方下面这张图，就是一个非常著名的RES-NET。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934388.png"alt="Alt text" /></p><p>咱们刚才写的那个ResBlock是最简单化的ResBlock,这个ResBlock是x进来之后，首先有一个卷积，卷积之后又给它进行了一个Batchnormalization，normalization之后又进行了一个Relu，然后又进行了一个dropout，再之后再给它进行一个Relu，然后再sum，加上x。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934389.png"alt="Alt text" /></p><p>这个是刚才我们写的ResBlock的一个更复杂的版本。这个网络结构是RES-NET的一个经典结构。</p><p>RES-NET的经典结构一共有这么几种：ResNet-18、ResNet-34、ResNet-50、ResNet-101、ResNet-152几种。ResNet-18和ResNet-34的基本结构相同，属于相对浅层的网络，后面3种的基本结构不同于ResNet-18和ResNet-34，属于更深层的网络。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934390.png"alt="Alt text" /></p><p>感兴趣的可以去看看这篇论文：<ahref="https://arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a>。</p><p>这五种结构都可以实现，但是它们的具体实现方法不一样。</p><p>RES-NET内理论上全部是卷积，没有全链接。全连接的部分其实是放在外边的。</p><p>RES-NET它的实现过程含有一点工程上的东西，如果是想要做计算机视觉的小伙伴，就需要想起的去学习一下这个部分。之后我会有专门讲CV的部分，会更详细的讲解。</p><p>然后我们再来了解一个Inception model，直译的话称之为「初创模型」，一般大家都把它叫做inception。它是Google在RES-NET提出来之后提出来的一个神经网络。</p><p>Google的神经网络提出来的这个Inception机制有一个很很奇怪的点，就是它提出来了一个操作叫做<code>1*1 convolutional</code>,意思就是我们把之前的卷积操作的那个<code>kernel_size</code>变成了1*1，就是变成一个点点。变成一个点点之后再加了一个非线性变化。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934391.png"alt="Alt text" /></p><p>这个权重也是刚开始随机的，后来是学习出来的。</p><p>它相当于是把整个前面的图形，整体每个数字乘了一个数，然后再给它进行了一个非线性变化。</p><p>也就是说，如图<code>1 * 1</code>的位置是5，相当于把前面矩阵内所有的数字都乘了个5，然后再进行了一个非线性变化。</p><p>假如现在有一个<code>8 * 8</code>的照片,包含RGB就是<code>8 * 8 * 3</code>，现在有5个<code>1 * 1</code>的卷积核，那么得出的结果应该是多少？</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934392.png"alt="Alt text" /></p><p>如图，也就是说，如果A为5，那么B、C、D应该等于多少？分别应该是<code>8 * 8 * 5</code>。</p><p>所以它其实起到了什么作用？首先我们知道了第一个功能就是改变通道数。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934393.png"alt="Alt text" /></p><p>改变了通道数之后，如果是<code>28 * 28 * 3</code>，这个Inception的机制是对每一层的输入要用多个不同的kernelsize的卷积做操作，做完之后把这些值拼起来，把它再作为下一层的输出。</p><p>这个时候padding就很有用了，保证了值都是<code>28 * 28</code>，就可以连起来了，否则还要做各种reshap就很麻烦。</p><p>Inception第一个操作是它有一个1 *1，什么都没干但它改变了通道数，第二个就是它使用了多个kernelsize给一层做卷积，之后把它的结果全部连起来。</p><p>那么把所有连起来它会产生这样一个结果</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934395.png"alt="Alt text" /></p><p>刚刚说了，inception里面会把多个kernelsize出来的结果连起来，因为有很多个Kernelsize，这个结果就很长，我们希望把它变短，就可以用<code>1 * 1</code>的这个操作给它变短。</p><p>用了16个<code>1 * 1</code>的操作，就可以把这个256层的channel变成16层的channel，变成16层之后再用<code>5 * 5</code>的卷积核去，得到了一个<code>28 * 28 * 32</code>的channel。</p><p>而如果直接用<code>5 * 5</code>再patting的话也可以得到一个<code>28 * 28</code>的channel，但是这两个是有区别的。如果直接用，就是<code>28 * 28</code>个channel，有32个。那么所需要拟合的参数就是160million。</p><p>参数之所以大是因为连在一起的值特别大，特别的深。现在如果想把它变浅的话参数就少了。这个地方叫做BottleneckNetwork,称为瓶颈网络。就是将之前连在一起而特别长的这个channel给它变得特别短，然后在这个短的channel上再做计算，所消耗的参数算下来就只有13million。</p><p>所以<code>1 * 1</code>的操作其实就是因为有了inception这种机制，所以会产生出特别长的结果。如果现在要对特别长的这个结果进行卷积的话，会需要的参数特别的多，而我们可以通过<code>1 * 1</code>的操作把它变短，之后再进行卷积操作，它的权重就少多了。这个就是这个inseption机制。</p><p>所以Inseption还是一样的道理，减少了参数的量，减少了parameters的数量，又降低了模型的复杂度，降低了过拟合，加快了计算速度。</p><p>那么我们卷积神经网络基本上到这里就给大家讲完了。关于更多卷积神经的应用后面会讲到专门的CV方面。</p><p>在这之前，接下来会用几节课分别讲解一下CV、BI和NLP的一些基础，给大家热热场子。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311111936981.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>30. 深度学习进阶 - 池化</title>
    <link href="https://hivan.me/30.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%B1%A0%E5%8C%96/"/>
    <id>https://hivan.me/30.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%B1%A0%E5%8C%96/</id>
    <published>2023-12-16T23:30:00.000Z</published>
    <updated>2023-12-22T08:34:23.163Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311101931081.png"alt="Alt text" /></p><span id="more"></span><p>Hi，你好。我是茶桁。</p><p>上一节课，我们详细的学习了卷积的原理，在这个过程中给大家讲了一个比较重要的概念，叫做<code>input channel</code>，和<code>output channel</code>。</p><p>当然现在不需要直接去实现,卷积的原理PyTorch、或者TensorFlow什么的其实都实现了。但我们现在如果要用PyTorch的卷积操作，它就会有一个inputchannel和output channel的一个写法。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311101652816.png"alt="Alt text" /></p><p>这里的方法是<code>Conv2d</code>，表示这里所应用的filter是一个2d的。那么它如何去做3d的？是将每一层的结果加在一起。</p><p>与此对应的还有一个<code>Conv3d</code>,这个时候filter就是很多个不一样的。</p><p>我们一般在使用的时候，应用的都是<code>Conv2d</code>。感觉上好像觉得Conv3d里，我们每一层的值不一样其实会更好。但其实现在得到的这个filter是咱们人工写的，但是我们为了提取出来大自然中非常非常多的特征，其实我们会让机器自动生成、自动初始化一堆filter，这些filter的结果全部都是随机的。</p><p>就是说，这个filter的结果在深度学习中其实这些结果都是随机的，然后通过训练和反向传播，这些filter会自动地学习出来一个值。也就是说它会自动地收敛到某个数值上，而这个数值在这种环境下卷积应该怎么提取特征，那么我们做成2D的话所需要拟合的参数其实就少了。</p><p>假如filters前边的inputchannel很多、很深，那么这个filters需要拟合的参数也很多，如果是2D的话，只需要拟合2D的这一层就可以了。这就是2D和3D的区别，以及为什么一般要用2D不用3D。</p><p>如果现在要用卷积的话，第一个参数就是input channel。第二个就是outputchannel。output channel其实就是有多少个filters。</p><p>那kernel size指的就是做卷积的时候这个卷积的大小。比方说是3 *3的，那么Kernel size就是3，也可以写成(3,3)。</p><p>还有一个参数叫做stride，这个stride就是步幅。那这个步幅是干什么的？我们一般用filter去卷积图像的时候，在矩阵上是一个单位一个单位从左到右从上到下移动的，这个步幅是为了加快移动，从而设置的间隔。比如<code>[10, 9, 8, 7, 6]</code>，那我就拿一行来举例，知道意思就行了。比如这样一个数列，如果filter是3列，那按顺序就应该先是<code>[10, 9, 8]</code>，然后是<code>[9, 8, 7]</code>，但是我设置了stride就可以跳步来执行。在<code>[10, 9, 8]</code>之后，可以是<code>[8, 7, 6]</code>。stride默认为1。</p><p>下面一个参数， padding。假如是一个6 * 6的图像矩阵，有一个3 *3的filter， 那么对这6 * 6的图像进行卷积，会先变成一个4 * 4，然后变成2 *2。显示出来的结果就是在不断地变小，代表抽象层次越来越高。</p><p>那么因为每一次window都在不断变化，在进行下一轮的时候，如果这中间要加一些什么操作，维度发生变化，会导致每一次中间要连接什么东西的时候维度都得重新去计算。</p><p>也就说维度不断的变化，会导致写代码的时候计算会变得更复杂。</p><p>那第二，就是我们也不希望减少的太快了。举个极端情况，把1万 *1万的图像很快就变成一个2 * 2的了。抽象层次太高信息就少了。</p><p>第三个解释起来比较复杂，我们脑子里想想一下，一个filter在图像上进行从左到右移动，那么在依次进行卷积计算的时候，最左边的一列就只计算了一次，但是中间位置就会被卷入计算多次。我们希望的是边上的的数据也能被计算多次，就是也能被反复的提取。</p><p>要解决这三个问题有一个很简单的方法，就是padding。它的意思就是在这个图形外边加了一圈或者两圈0。如果你要加一圈0的话，<code>padding=1</code>。如果等于2的话，就加两圈0。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311101652817.png"alt="Alt text" /></p><p>接下来，<code>dilation</code>。这个是在我们做图形的分割的时候用的。在图形识别的时候大家现在先不用去学习它。</p><p>我们一张图片进行卷积的时候，会越来越小，这个叫做下采样， downsampling。进行完下采样之后，如果要做图像的切分，我们要把图像里边主体部分全部给它涂黑，别的地方全部涂白，需要基于这个小的采样又把它给扩大，慢慢恢复到原来大小，这个叫做上采样。上采样时，有时候会用到<code>dilation</code>。</p><p>重要的就是这几个参数。这几个参数给大家说完，其实基本上卷积的几个重要的特性就说明白了。</p><h2 id="池化">池化</h2><p>除了卷积之外，还有一个比较重要的操作: <code>pooling</code>,池化操作。</p><p>池化操作其实很简单，我们给定一个图片，卷积操作是选了一个window和filter，做了一个f乘w然后给它做相加.<code>sum(f*w)</code>. pooling是一个很直接的操作,把w这里边所有的值给它取个平均值,也有可能取个最大值。假如是它最大值，那么值最大就代表着是在这个图形里边对他影响最重的这个点。</p><p>那么做了pooling之后，每一次这样一个操作，图形变小了，但是图像基本上保持了原来的样子。就是pooling操作前后的图像是相似的，它取了最重要的信息。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311101652818.png"alt="Alt text" /></p><p>我们现在来思考一下，如果有一个图片，不管是卷积还是pooling都会让其缩小。那我们思考下，既然两个操作都会导致图像缩小，那为什么会存在两个操作呢？</p><p>咱们机器学习里面最头疼的事情就是所谓的过拟合，过拟合就是在训练的时候效果挺好，结果在实际中效果就不好了。</p><p>而控制过拟合最主要的就是能够减少参数，在越少的参数能达到效果的时候，我们期望参数越少越好，在同样的数据量下就越能防止过拟合。</p><p>卷积里面这些值以前的时候是人来确定，但现在其实是期望机器自动的去求，也就是说这个参数是需要自己去求解的。而pooling并不需要去设定参数，它没有参数，这样会减少参数。</p><p>用了pooling之后不仅减少了参数,还减少了接下来x的维度。所以最核心的其实是我们减少了所需要训练的参数。</p><p>之所以用pooling是因为可以减少参数，可以让它的过拟合的问题减弱。但是如果你的数据量本身就很多，或者说模型本身就比较好训练、好收敛，那你没有这个pooling操作其实也是可以的。</p><h2 id="权值共享和位置平移">权值共享和位置平移</h2><p>那么这个时候就要跟大家来讲一个比较重要的概念，叫做权值共享和局部不变性:<code>Parameters Sharing and Location Invariant</code>.这个LoctionInvariant也有人把它叫做shiftingInvariant。CNN的最重要的两个特点，第一个特点就是它的权值共享。</p><p>我们给定一个图片，就之前我那个头像，假如有一个filter，它是3 *3的，那么这3 * 3的这个网格它在每一个窗口上都是和这个filter做的运算。</p><p>那么大家想一下，假如有一个1,000乘以1,000的一个图形，我们这1,000 *1,000的图形我们要把它写成<code>wx+b</code>的话，这个x是100万维的，那么这个w也是100万维的。</p><p>那么如果我们要做训练的话，就要训练100万个w。这是拟合一个线性变化，那么我们如果现在是要去拟合一个卷积，假如outputchannel是10，那我们需要拟合的参数是多少？</p><p>卷积核是3 * 3, 有10个。 那就是9 * 9再乘以10。不管这个地方是1,000 *1,000还是1万 * 1万，我们要拟合的都是卷积核里的这个参数。</p><p>我们做一层卷积，哪怕给了10个卷积核，也是九十个。如果要给它做一层线性变化，得100万个，这两个相差特别大。</p><p>为什么相差这么大？是因为不同的位置上用的filter的值是一样的。filter的参数整个图像共享了。这就是卷积神经网络的权值共享。</p><p>那么我们现在想一下，我们有了这个ParametersSharing，它的作用是什么?</p><p>减少参数量的作用是防止过拟合，防止过拟合的最终体现就是我们在各种计算机视觉上的任务，表现就好。除此之外还有一个特性，它可以大大的提升我们的计算速度。</p><p>本来我们以前如果你有这么多参数的话，要反向传播一次要进行100万个反向传播。现在我们只要进行九十个就行了。</p><p>所以权值共享其实是卷积神经网络为什么效果特别好的原因。</p><p>2012年的时候，计算机视觉的测试效果一下子有了突飞猛进。当时就是因为用了卷积神经网络。</p><p>以前大家在实验室环境下，在训练集上的效果都挺不错，但是一拿到测试集的时候效果就很差。后来用卷积神经网络之后，这个错误率一下就下降了。</p><p>权值共享这个特性因此带来了一个特点，就是LoctionInvariant。就是一个局部的东西，我们把它信息连接在一块了。</p><p>我们分别有两张图片，比如下面这张我以前画的一幅画，我把构图分别改变一下。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311101652819.png"alt="Alt text" /></p><p>这两个图像数据表征上是很不一样，但是眼睛所在的位置经过卷积之后，只要用的是同一个卷积核，产生的结果是相似的。所以这个LoctionInvariant指的意思是，不管这个眼睛在哪我们都能提取出来。</p><p>假设我们在train的时候,眼睛不管在哪，只要把这个filter训练出来了，在test数据集上就算它的位置变了我们依然能够提取出来它的特征，依然能够计算出来和它相似的这个值，这个就叫做LoctionInvariant。</p><p>这两个特性是极其重要的。</p><p>搭建卷积神经网络这个事，说实话最主要的还是要看经验。那么前人的总结就很值得借鉴，下节课，我们就来看看几种经典的神经网络结构。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311101931081.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>29. 深度学习进阶 - 卷积的原理</title>
    <link href="https://hivan.me/29.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%8E%9F%E7%90%86/"/>
    <id>https://hivan.me/29.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%8E%9F%E7%90%86/</id>
    <published>2023-12-12T23:30:00.000Z</published>
    <updated>2023-12-16T08:13:56.991Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246431.png"alt="Alt text" /></p><span id="more"></span><p>Hi,你好。我是茶桁。</p><p>在结束了RNN的学习之后，咱们今天开始来介绍一下CNN。</p><p>CNN是现代的机器深度学习一个很核心的内容，就假如说咱们做图像分类、图像分割，图像的切分等等。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246432.png"alt="Alt text" /></p><p>其实这些过程就是你让计算机能够自动识别，不仅能够识别图像里有什么，还能识别图像里这些东西分别是在什么地方。这种复杂操作其实都是基于啊CNN的变体。要给计算机有识别图像的能力。</p><p>再比方说大无人驾驶汽车，它要识别行人在哪里。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246433.png"alt="Alt text" /></p><p>再比如安防的摄像头，要能够检测出来我们人在哪里。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246434.png"alt="Alt text" /></p><p>这些事情背后都是计算机视觉的问题。</p><p>大概一九五几年、六几年的时候，哈佛大学曾经做过一个研究，给猫的大脑上装了一些电极，让这个猫去看前面的一个幻灯片，然后通过切换幻灯片的内容，然后观察猫的大脑哪些地方活跃。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246435.png"alt="Alt text" /></p><p>就发现两个特点，第一个它有一种一层一层的特性，比方说我换了颜色，它固定的就这几层会活跃，离眼睛远的地方会活跃。如果换了线条，颜色没变，会是另外的一层区域会活跃，不同层其实对于不同的特定变化是不一样的。</p><p>第二个发现，越靠近眼睛的地方，越低级的层次的变化会越明显，比如线条颜色。眼睛越远的距离，线条和颜色没变，但是眼睛变大了或者变小了，那么这些地方它会更明显。</p><p>也就是说，第一个它是有分层的，第二个，它不同的这个层的抽象性是不一样的，对于什么东西的感受力是不一样的。</p><p>沿着这个思路，人们当时就提出来了一些方法。当时人们做计算机视觉，主流不是机器学习。但是人们提出来一个一个这样的filter：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">filter</span> = np.array([<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>],<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>],<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>]<br>])<br></code></pre></td></tr></table></figure><p>这样的filter是人刻意的，主观的提出来的。他们把这个filter去应用到一个一个的图像上。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246436.png"alt="Alt text" /></p><p>比方说我们的图像是<code>a b c d e f g h i j k l</code>，然后按4*4的矩阵相乘，再加起来，比如<spanclass="math inline">\(aw+bx+ey+fz\)</span>，这样就得到了一个新的内容。大家把这个操作就叫做卷积操作。</p><p>看个示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image = np.array([<br>    [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>],<br>])<br>plt.imshow(image)<br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246437.png"alt="Alt text" /></p><p>我们可以看到，这个矩阵的前三列全是10，后两列都是0，最后生成的图像有一个明显的分界，伴随着两个不同的颜色。</p><p>我们现在给这个图像矩阵加上一个filter, 然后按上面的方法进行操作：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246438.png"alt="Alt text" /></p><p>那左上角的3*3的小矩阵的运算结果就是0。</p><p>那同理，我们以此往后算，第二个结果是39, 第三个结果是39....大家后面可以自行计算一下，最后的计算结果就是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">[[<span class="hljs-number">0</span>, <span class="hljs-number">39</span>, <span class="hljs-number">39</span>, <span class="hljs-number">0</span>],<br>[<span class="hljs-number">0</span>, <span class="hljs-number">39</span>, <span class="hljs-number">39</span>, <span class="hljs-number">0</span>],<br>[<span class="hljs-number">0</span>, <span class="hljs-number">39</span>, <span class="hljs-number">39</span>, <span class="hljs-number">0</span>]]<br></code></pre></td></tr></table></figure><p>我们可以看出来，当分割的小矩阵内数据相同的时候，值为0，如果说矩阵内的这个部分图像差距不是很大，那它也是近乎接近于0，意味着差别很小。如果说分割的这个小矩阵左右两边是相反数的时候，两边的差别是最大的，不管最后相加的值是正的还是负的，绝对值下应该是最大的。这个地方其实是图像竖着的边缘。</p><p>那如果我们将filter改一下，改成下面这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">[[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>[-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>]]<br></code></pre></td></tr></table></figure><p>如果是这样，计算的结果就是图像横向的边缘的绝对值最大。</p><p>基于这种原理，我们就可以找到图像所有竖向和横向的边沿，给它拿出来。</p><p>这整个的一个过程，就叫做卷积:convolution。convolution就是两个东西之间互相起作用。最早是出现在信号处理上，两个信号把它做一个合并。</p><p>卷积的操作是为了干什么呢？卷积的操作是用来提取图片的某种特征，抓取图片特征。在上个世纪后期，计算机视觉的老科学家们提出了大量的kernel，当时叫做算子，现在叫做卷积核。</p><p>卷积的操作就是给定一个图片，然后给定一个卷积核，和卷积核一样大小的窗口里边的每个值相乘，相乘之后再做相加。</p><p>假如咱们有一张图片，一般来说，咱们现实生活中图片往往是三维，通常是红绿蓝(RGB)，然后我们让这张图片和这个filter去做相乘的操作。</p><p>这三个层里面每一层都会和filter做一个相乘的操作，咱们就假设这三个层分别为:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">[[a11, a12, a13],[a21, a22, a23],[a31, a32, a33]],<br>[[b11, b12, b13],[b21, b22, b23],[b31, b32, b33]]<br>[[c11, c12, c13],[c21, c22, c23],[c31, c32, c33]]<br></code></pre></td></tr></table></figure><p>然后再假设filter为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[[f11, f12, f13], [f21, f22, f23], [f31, f32, f33]]<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246439.png"alt="Alt text" /></p><p>那这个filter会分别和这三个层进行卷积操作，产生的卷积结果为v1, v2, v3,然后这三个结果再进行相加，最后会产生一个新的层。</p><p>我们来看一下下面这张图：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246440.png"alt="Alt text" /></p><p>这张图显示的是一层的情况，一个filter大小的矩阵被卷积成了一个点，然后这个操作不只是针对一层的，而是对整个一个纵向体积内的所有层都做这样一个操作：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246441.png"alt="Alt text" /></p><p>途中最底下是我们的图片的RGB分层，再经过和filter相乘之后向上会卷积成一个点，那向上之后的Map1,Map2,...原因是每一层都是一个不同的filter计算的结果，这里存在很多个filter，然后分别计算产生了这样一个叠加层。</p><p>再做下一次运算的时候也是一样，这些Map的纵向上经过和filter运算依然会被卷积成一个点。</p><p>就着上面那个简单的图形，咱们来做个演示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv</span>(<span class="hljs-params">image, <span class="hljs-built_in">filter</span></span>):<br>    h = <span class="hljs-built_in">filter</span>.shape[<span class="hljs-number">0</span>]<br>    w = <span class="hljs-built_in">filter</span>.shape[<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(image.shape[<span class="hljs-number">0</span>]):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(image.shape[<span class="hljs-number">1</span>]):<br>            window = image[i: i+h, j: j+w]<br>            <span class="hljs-built_in">print</span>(window)<br><span class="hljs-built_in">filter</span> = np.array([<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>],<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>],<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>]<br>])<br><br>conv(image, <span class="hljs-built_in">filter</span>)<br><br>---<br>[[<span class="hljs-number">10</span> <span class="hljs-number">10</span> <span class="hljs-number">10</span>]<br> [<span class="hljs-number">10</span> <span class="hljs-number">10</span> <span class="hljs-number">10</span>]<br> [<span class="hljs-number">10</span> <span class="hljs-number">10</span> <span class="hljs-number">10</span>]]<br>...<br>[[-<span class="hljs-number">3</span>]<br> [-<span class="hljs-number">3</span>]<br> [-<span class="hljs-number">3</span>]]<br>...<br>[[<span class="hljs-number">10</span> -<span class="hljs-number">3</span> -<span class="hljs-number">3</span>]]<br>[[-<span class="hljs-number">3</span> -<span class="hljs-number">3</span> -<span class="hljs-number">3</span>]]<br>[[-<span class="hljs-number">3</span> -<span class="hljs-number">3</span>]]<br>[[-<span class="hljs-number">3</span>]]<br><br></code></pre></td></tr></table></figure><p>输入一个图片的数据,拿到filter的高宽，然后让filter沿着图片从上到下，从左到右移动。</p><p>我们打印结果能看到，运行到中间的时候会出现一串的[[-3], [-3],[-3]]。因为i会一直运行边上，那么如果要做卷积的话，大小要和filter一直一样，所以咱们在这里需要给他减去一个filter。就是不要运行后边这几个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">...<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(image.shape[<span class="hljs-number">0</span>] - h+<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(image.shape[<span class="hljs-number">1</span>] - w+<span class="hljs-number">1</span>):<br>            ...<br></code></pre></td></tr></table></figure><p>这样就可以了。</p><p>我们每一次其实就是从左到右，从上到下裁剪出来一个一个的window。</p><p>我们让这个window和filter相乘后再相加，我们可以得到什么结果？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i ...:<br>    <span class="hljs-keyword">for</span> j ...<br>        ...<br>        result = np.<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">filter</span> * window)<br>        <span class="hljs-built_in">print</span>(result)<br><br>---<br><span class="hljs-number">0</span><br><span class="hljs-number">39</span><br>...<br><span class="hljs-number">39</span><br><span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>就是计算卷积的结果。</p><p>那我们可以将其改成矩阵的形式, 然后咱们打印出来看看是个啥：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv</span>(<span class="hljs-params">image, <span class="hljs-built_in">filter</span></span>):<br>    r_height = image.shape[<span class="hljs-number">0</span>] - h+<span class="hljs-number">1</span><br>    r_width = image.shape[<span class="hljs-number">1</span>] - w+<span class="hljs-number">1</span><br>    result = np.zeros(shape=(r_height, r_width))<br>    <span class="hljs-keyword">for</span> i ...:<br>        <span class="hljs-keyword">for</span> j ...:<br>            result[i][j] = np.<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">filter</span> * window)<br>    <span class="hljs-keyword">return</span> result<br><br>result = conv(image, <span class="hljs-built_in">filter</span>)<br>plt.imshow(result)<br>plt.show()<br>---<br>array([[ <span class="hljs-number">0.</span>, <span class="hljs-number">39.</span>, <span class="hljs-number">39.</span>,  <span class="hljs-number">0.</span>],<br>       [ <span class="hljs-number">0.</span>, <span class="hljs-number">39.</span>, <span class="hljs-number">39.</span>,  <span class="hljs-number">0.</span>],<br>       [ <span class="hljs-number">0.</span>, <span class="hljs-number">39.</span>, <span class="hljs-number">39.</span>,  <span class="hljs-number">0.</span>],<br>       [ <span class="hljs-number">0.</span>, <span class="hljs-number">39.</span>, <span class="hljs-number">39.</span>,  <span class="hljs-number">0.</span>]])<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246442.png"alt="Alt text" /></p><p>那变成这样的原因是因为原来的图像中间有一个边缘，现在这张图显示的是图片边缘的部分被高亮。</p><p>这样一张图片可能并不太能理解，我拿我的头像来做这个示例好了：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246443.jpg"alt="Alt text" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">myself = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./assets/chaheng.jpg&#x27;</span>).convert(<span class="hljs-string">&#x27;L&#x27;</span>)<br>...<br>plt.imshow(result, cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br></code></pre></td></tr></table></figure><p>为了更明显一点，我将图像改成灰度显示。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246444.png"alt="Alt text" /></p><p>我们可以看到卷积之后的效果，明显边缘都被显示出来了。但是我们也注意到了，竖向的边缘都很明显，但是横向的边缘并不清楚。我们再来对横向进行一下卷积,我们先要增加一个处理多个filter的方法，将原来的conv方法改为single_conv，表示处理单个：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">single_conv</span>(<span class="hljs-params">...</span>):<br>    ...<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv</span>(<span class="hljs-params">image, filters</span>):<br>    results = [single_conv(image, f) <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> filters]<br>    <span class="hljs-keyword">return</span> results<br></code></pre></td></tr></table></figure><p>然后我们的调用需要改一下传递的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">results = conv(image, [h_filter, w_filter])<br></code></pre></td></tr></table></figure><p>既然要传递两个filter,那我们就需要再定义一个横向的filter，然后一起传进去：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 原来的filter</span><br>h_filter = np.array([...])<br><br><span class="hljs-comment"># 新定义的横向filter</span><br>w_filter = np.array([<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>    [-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>]<br>])<br></code></pre></td></tr></table></figure><p>接着我们将原图，竖向的卷积结果和横向的卷积结果都打印出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>plt.imshow(image)<br>plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>plt.imshow(results[<span class="hljs-number">0</span>])<br>plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br>plt.imshow(results[<span class="hljs-number">1</span>])<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246445.png"alt="Alt text" /></p><p>原图变成这个颜色的原因是我在PIL读取图像的时候，将其转为了灰度。我们可以看到第二张图片和第三张图明显在边缘上的区别，一个像是灯光从左边打过来的，一个像是灯光从上面打下来的。</p><p>中间和右边这个，其实都是把边缘提出来了。因为卷积核的不同，中间这个图把竖着的边缘明显提取的比较准确，右边的把横向的提取的比较准确。</p><p>这也是为什么我们之前看得那张图里会有那么多的Map:</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246446.png"alt="Alt text" /></p><p>它的每一层都是一个不同的filter提取出来的，有这么多filter的原因则是每一个filter提取出来的特征都是不一样的。</p><p>我们来看我们刚才定义的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">single</span>(<span class="hljs-params">image, <span class="hljs-built_in">filter</span></span>):<br>    ...<br></code></pre></td></tr></table></figure><p>我们把输入卷积的时候的image这个参数叫做<code>input channel</code>。那在此时此刻，我们这个图像如果是RGB的，它就是三维的，那么<code>input channel</code>就等于3。</p><p><code>filters</code>的个数，就叫做<code>output channel</code>。原因就在于，有多少个<code>filter</code>，那我们的results就有多厚。比如说我们有4个<code>filter</code>,那输出的result就有四层。然后可以接着对results继续应用<code>filter</code>做卷积，那在这一轮的<code>input channel</code>就等于一次的<code>output channel</code>,也就是4。</p><p>这个，就是卷积的原理。</p><p>好，这节课就到这里了，下节课咱们继续学习卷积，来看看在神经网络里如何应用。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311072246431.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>28. 深度学习进阶 - LSTM</title>
    <link href="https://hivan.me/28.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20LSTM/"/>
    <id>https://hivan.me/28.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20LSTM/</id>
    <published>2023-12-09T23:30:00.000Z</published>
    <updated>2023-12-16T08:14:04.718Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054870.png"alt="Alt text" /></p><span id="more"></span><p>Hi, 你好。我是茶桁。</p><p>我们上一节课，用了一个示例来展示了一下我们为什么要用RNN神经网络，它和全连接的神经网络具体有什么区别。</p><p>这节课，我们就着上一节课的内容继续往后讲，没看过上节课的，建议回头去好好看看，特别是对代码的进程顺序好好的弄清楚。</p><p>全连接的模型得很仔细的去改变它的结构，然后再给它加很多东西，效果才能变好：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">self.linear_with_tanh = nn.Sequential(<br>    nn.Linear(<span class="hljs-number">10</span>, self.hidden_size),<br>    nn.Tanh(),<br>    nn.Linear(self.hidden_size, self.hidden_size),<br>    nn.Tanh(),<br>    nn.Linear(self.hidden_size, output_size)<br>)<br></code></pre></td></tr></table></figure><p>但是对于RNN模型来说，我们只用了两个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">self.rnn = nn.RNN(x_size, hidden_size, n_layers, batch_first=<span class="hljs-literal">True</span>)<br>self.out = nn.Linear(hidden_size, output_size) <br></code></pre></td></tr></table></figure><p>这是一个很本质的问题,也比较重要。为什么RNN的模型这么简单，它的效果比更复杂的全连接要好呢？</p><p>这个和我们平时生活中做各种事情其实都很类似，他背后的原因是他的信息保留的更多。RNN模型厉害的本质是在运行的过程中把更多的信息记录下来，而全连接没有记录。</p><p>对于RNN模型，还有两个点大家需要注意。</p><p>第一个，有一种叫做stacked的RNN的模型。我们RNN模型每一次输出都有一个output和hidden，把outputs和hidden作为它的输入再传给另外一个RNN模型，模型就变得更复杂，理论上可以解决些更复杂的场景。我们把这种就叫做stackedRNN。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054871.png"alt="Alt text" /></p><p>还有一种形式，Bidirectional RNN，双向RNN。有一个很著名的文本模型Bert,那个B就是双向的意思。</p><p>我们回过头来看上节课我们讲过的两种网络：</p><p><span class="math display">\[\begin{align*}h_t &amp; = \sigma_h(W_hx_t + U_hh_{t-1} + b_h) \\y_t &amp; = \sigma_y(W_yh_t + b_y)\end{align*}\]</span></p><p>在这个里面，每一时刻的y_t只和y_{t-1}有关系，如果把所有的x一次性给到模型的时候，其实我们在这里可以给它加一个东西：</p><p><span class="math display">\[\begin{align*}h_t &amp; = \sigma_h(W_hx_t + U_hh_{t-1} + V_h * h_{t+1} + b_h)\end{align*}\]</span></p><p>还可以写成这样，那这样的话它实现的就是每一时刻的t既和前一次有关系和后一刻有关系。这样我们每一次的值不仅和前面有关，还和后面有关。就叫做双向RNN。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054872.png"alt="Alt text" /></p><p>对于RNN来说，它有一个很严重的问题，就是之前说过的，它的vanishing和exploding的问题会很明显,也就是梯度消失和爆炸问题。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054873.png"alt="Alt text" /></p><p>想一下，现在如果有一个loss，那它最终的loss是不是对于{x1, x2, ...,xn}都有关系，比方说现在要求<span class="math inline">\(\frac{\partialloss}{\partial w_1}\)</span>, 假如说现在h是100， 那这种调用关系就是</p><p><span class="math display">\[\begin{align*}\frac{\partial loss}{\partial w_1} = \frac{\partial h_{100}}{\partialh_{99}} \cdot \frac{\partial h_{99}}{\partial h_{98}} \cdot ... \cdot\frac{\partial h_{0}}{\partial w_{1}}\end{align*}\]</span></p><p>loss对于w1求偏导的时候，其实loss最先接受的是离他最近的,假如说是h100。h100调用了h99,h99调用h98，就这个调用过程，这一串东西会变得很长。</p><p>我们之前课程说过一些情况，怎么去解决这个问题呢？对于RNN模型来说梯度爆炸很好解决，就直接设定一个阈值就可以了，起码也是能学习的。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054874.png"alt="Alt text" /></p><p>要讲的是想一种方法怎么样来解决梯度消失的问题。这个梯度消失的解决方法，就叫LSTM。要解决梯度消失，就是要用LSTM:Long Short-TermMemory，长短记忆模型，既能保持长信息，又能保持短信息。</p><p>在之前那个很长的过程中，怎么样能够让它不消散呢？LSTM的核心思想是通过门控机制来控制信息的流动和及已的更新，包含了InputGate, Forget Gate，Cell State以及OutputGate。这些会一起协作来处理序列数据。</p><p>其中Input Gate控制着新信息的输入，以及信息对细胞状态的影响。 ForgetGate控制着细胞状态中哪些信息应该被易王，CellState用于传递信息，是LSTM的核心，OutputGate控制着细胞状态如何影响输出。</p><p>这里每一个门控单元都由一个Sigmoid激活函数来控制信息的流动，以及一个Tanh激活函数来确定信息的值。</p><p><span class="math display">\[\begin{align*}Input Gate \\i_t &amp; = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\C&#39;t &amp; = \tanh(W_c \cdot [h{t-1}, x_t] + b_c) \\C_t &amp; = f_t \cdot C_{t-1} + i_t \cdot C&#39;_t \\Forget Gate \\f_t &amp; = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \\C_t &amp; = f_t \cdot C_{t-1} + i_t \cdot C&#39;_t \\Output Gate \\o_t &amp; = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\h_t &amp; = o_t \cdot \tanh(C_t)\end{align*}\]</span></p><p>其中，<span class="math inline">\(h_{t-1}\)</span>是前一个时间步的隐藏状态，<span class="math inline">\(x_t\)</span>是当前时间步的输入，<span class="math inline">\(W_i, W_f, W_o,W_c\)</span> 是权重矩阵，<span class="math inline">\(b_i, b_f, b_o,b_c\)</span> 是偏置。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054875.png"alt="Alt text" /></p><p>LSTM输入的是一个序列数据，可以是文本、时间序列，音频信号等等。那每个时间步的输入是序列中的饿一个元素，比如一个单词、一个时间点的观测值等等。</p><p>假设我们有一个序列 x = [x1, x2, ..., xt]，其中t就代表的是时间步。</p><p>xt进来的时候, 之前我们是只接收一个hidden state,现在我们多接收了一个<spanclass="math inline">\(C_{t-1}\)</span>，这个就是我们的Cell，这一步的<spanclass="math inline">\(C_{t-1}\)</span>其实就是上一步的<spanclass="math inline">\(C_t\)</span>。</p><p>在训练开始时，需要初始化LSTM单元的隐藏状态h0和细胞状态c0。通常我们初始化它们为全零向量。</p><p>最开始的时候，我们要进入Input Gate, 对于每个时间步t,计算输入门的激活值<spanclass="math inline">\(i_t\)</span>，控制新信息的输入。使用Sigmoid函数来计算输入门的值：</p><p><span class="math display">\[i_t = \sigma (W_i \cdot [h_{t-1}, x_t] + b_i)\]</span></p><p>然后，计算新的侯选值<span class="math inline">\(C&#39;_t\)</span>，这是在当前时间步考虑的新信息。使用tanh激活函数来计算侯选值：</p><p><span class="math display">\[C&#39;_t = tanh(W_c \cdot [h_{t-1}, x_t] + b_c)\]</span></p><p>接下来我们就要更新细胞状态了，细胞状态<spanclass="math inline">\(C_t\)</span>更新是通过遗忘门<spanclass="math inline">\(f_t\)</span>和输入门<spanclass="math inline">\(i_t\)</span>控制的。遗忘门控制着哪些信息应该被遗忘，输入门控制新信息对细胞状态的影响：</p><p><span class="math display">\[C_t = f_t \cdot C_{t-1} + i_t \cdot C&#39;_t\]</span></p><p>那遗忘门决定哪些信息应该被遗忘，使用的就是Sigmoid函数计算遗忘门的激活值。</p><p><span class="math display">\[f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)\]</span></p><p>接着，计算输出门<span class="math inline">\(O_t\)</span>,控制着细胞状态如何影响输出和隐藏状态。一样，我们还是使用Sigmoid函数计算。</p><p><span class="math display">\[o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)\]</span></p><p>使用输出门的值<spanclass="math inline">\(o_t\)</span>来计算最终的隐藏状态<spanclass="math inline">\(h_t\)</span>和输出。隐藏状态和输出都是根据细胞状态和输出门的值来计算的：</p><p><span class="math display">\[h_t = o_t \cdot tanh(C_t)\]</span></p><p>接下来就容易了，我们迭代重复上述过程，处理序列中的每一个时间步，直到处理完整个序列。</p><p>LSTM的输出可以是隐藏状态<span class="math inline">\(h_t\)</span>,也可以是细胞状态<span class="math inline">\(C_t\)</span>，具体是取决于应用的需求。</p><p>后来大家就发现了一种改进的LSTM，其中门控机制允许细胞状态窥视现前的细胞状态的信息，而不仅仅是根据当前时间步的输入和隐藏状态来决定。这个机制在LSTM单源种引入了额外的权重和连接，以允许细胞状态在门控过程中访问现前的细胞状态，我们称之为窥视孔连接:Peephole connections。</p><p><span class="math display">\[\begin{align*}f_t = \sigma(W_f \cdot [C_{t-1}, h_{t-1}, x_t] + b_f) \\i_t = \sigma(W_i \cdot [C_{t-1}, h_{t-1}, x_t] + b_i) \\o_t = \sigma(W_o \cdot [C_{t-1}, h_{t-1}, x_t] + b_o) \\\end{align*}\]</span></p><p>之前，我们是xt和x_{t-1}决定的f，那现在又把c_{t-1}加上了。就是多加了一些信息。</p><p>除此之外它有一个方法GRU，这个是2014年提出来的，Geted RecurrentUnit，它是LSTM的一个简化版本。</p><p>它最核心的内容：</p><p><span class="math display">\[\begin{align*}h_t = (1-z_t) \cdot h_{t-1} + z_t \cdot h&#39;_t\end{align*}\]</span></p><p>咱们刚刚是<span class="math inline">\(C_t = f_t \cdot C_{t-1} + i_t\cdotC&#39;_t\)</span>，也就是遗忘加上输入，那我们对过去保留越多的时候，输入就会越小，那对过去保留越小的时候，输入就会越大。</p><p>所以既然f也是1-0，i也是0-1，f大的时候i就小，f小的时候i就大，那么能不能写成f=(1-i)？</p><p>于是，GRU就这样实现了, 它其实最核心的就做了这样一件事， f=(1-i)。</p><p><span class="math display">\[\begin{align*}z_t &amp; = \sigma(W_z \cdot [h_{t-1}, x_t]) \\r_t &amp; = \sigma(W_r \cdot [h_{t-1}, x_t]) \\h&#39;_t &amp; = \tanh(W \cdot [r_t \cdot h_{t-1}, x_t]) \\h_t &amp; = (1-z_t) \cdot h_{t-1} + z_t \cdot h&#39;_t\end{align*}\]</span></p><p>这个z其实和i是一样的东西，只是原作者为了发表论文方便而改了个名称。</p><p><ahref="https://arxiv.org/pdf/1406.1078v3.pdf">https://arxiv.org/pdf/1406.1078v3.pdf</a></p><p><span class="math inline">\(r_t\)</span>是来控制上一时刻的<spanclass="math inline">\(h_t\)</span>在我们此时此刻的重要性、影响程度。那我们可以将<spanclass="math inline">\(r_t \cdot h_{t-1}\)</span>看成是关于及已的，<spanclass="math inline">\(1-z_t\)</span>也是关于记忆的。</p><p>GRU这样做之后有什么好处呢?</p><p>原来我们有三个门: f, i, o，那现在变成了两个，z和r。为什么就更好了呢？我们在PyTorch里面往往用的是GRU。</p><p>大家想一下，是不是少了一个门其实就少了一个矩阵？我们看公式的时候，<spanclass="math inline">\(W_f\)</span>是一个数学符号，但是在背后其实是一个矩阵，是一个矩阵的话少了一个矩阵意味着参数就少多了，运算就更快了等等。</p><p>但其实这些都不是最关键的，最关键的是减少过拟合了。我们之前的课程中一再强调，过拟合之所以产生，最主要的原因是数据不够或者说是模型太复杂。</p><p>但是在现有的数据情况下，为了让数据发挥出最大效力，你把需要训练的模型变简单，参数变少，就没有那么复杂了。</p><p>关于RNN模型，我们后面还会介绍一些具体的示例。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311062054870.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>27. 深度学习进阶 - 为什么RNN</title>
    <link href="https://hivan.me/27.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E4%B8%BA%E4%BB%80%E4%B9%88RNN/"/>
    <id>https://hivan.me/27.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E4%B8%BA%E4%BB%80%E4%B9%88RNN/</id>
    <published>2023-12-05T23:30:00.000Z</published>
    <updated>2023-12-16T08:38:33.650Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/茶桁的AI秘籍_核心基础_27.png"alt="茶桁的AI秘籍_核心基础_27" /></p><span id="more"></span><p>Hi，你好。我是茶桁。</p><p>这节课开始，我们将会讲一个比较重要的一种神经网络，它对应了咱们整个生活中很多类型的一种问题结构，它就是咱们的RNN网络。</p><p>咱们首先回忆一下，上节课咱们学到了一些深度学习的一些进阶基础。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501000.png" /></p><p>学了很多神经网络的Principles,就是它的一些很重要的概念，比方层数维度。再然后咱们讲了Optimizer，一些优化方式。还有weights的initialization，初始化等等。</p><p>那么大家具备了这些知识之后，那我们基本上已经能够解决常见的大概90%的机器学习问题了。</p><p>我们现实生活中绝大多数的机器学习问题，或者说识别问题都可以把它抽象成要么是分类，要么是回归问题。</p><h2 id="一个柯基的例子">一个柯基的例子</h2><p>我们来一个例子，比方说一张图片里这个是什么动物，这显然是一个分类问题。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501001.png"alt="Corgi" /></p><p>但是我们对这个图片的多个物体是什么，还有位置标注出来，那这个在里面前面会有一段是一个分类问题，后面还有一个长的向量，又会是一个回归问题。</p><p>我们只要知道分类和回归最大的区别就是一个返回的是一个类别，另外一个返回的是一个真正的数值。</p><p>那么接下来我们要正是的讲一下两种神经网络，RNN和CNN。这两个的目的是用来加速解决我们之前遇到的分类问题，或者回归问题。</p><p>在这些LSTM和CNN之类的高级的方法出现之前，其实我们用最直接的神经网络是可以解决所有的问题。</p><p>我们还是来看上面的那个例子，还是那张图片，如果要去分类看这图片里的是什么动物，我们把它形式化的表述一下。</p><p>假设我们这张图片现在是258*258的，那每一张图片进来之后，这个图片的饿背后其实都是一个向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 258 * 258</span><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>example_img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;assets/Corgi.png&#x27;</span>)<br>example_img = np.array(example_img)<br><br><span class="hljs-built_in">print</span>(example_img.shape)<br><br>---<br>(<span class="hljs-number">429</span>, <span class="hljs-number">696</span>, <span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p>我们可以看到这张图片在计算机里保存的时候是<code>(429, 696, 3)</code>这样的一组数字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.imshow(example_img)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501002.png"alt="Alt text" /></p><p>我们用plt展示出来，就是这样。</p><p>我们现在就可以讲整个图片变成一个向量，然后把它从立方体变的拉平：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">example_img = example_img.reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(example_img)<br><br>---<br>[[<span class="hljs-number">120</span> <span class="hljs-number">150</span>  <span class="hljs-number">88</span> ...  <span class="hljs-number">43</span>  <span class="hljs-number">39</span>  <span class="hljs-number">38</span>]]<br></code></pre></td></tr></table></figure><p>那现在，我们要给这个图片做分类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dimension, categorical</span>):<br>        <span class="hljs-built_in">super</span>(Model, self).__init__()<br>        self.linear = nn.Linear(in_features=input_dimension, out_features=categorical)<br>        self.softmax = nn.Softmax()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        predict = self.softmax(self.linear(x))<br>        <span class="hljs-keyword">return</span> predict<br>        ...<br></code></pre></td></tr></table></figure><p>这里我们暂停一下，来说说这段代码中的<code>super(...)</code>，为了避免有些小伙伴Python基础不太好，这里说明一下。</p><p>如果有从我Python基础课就看过来的小伙伴，应该知道我在面向对象的时候应该是讲过这个方法。这个是为了在继承父类的时候，我们在重写父类方法的时候，依然可以调用父类方法。方式就是<code>super().父类方法名()</code>。有需要补Python基础的可以回头将我写的Python基础课程好好再看一遍。</p><p>好，我们继续回过头来讲，我们定义好这个Model之后，将图片数据变成一个PyTorch能够处理的一个example，当作训练数据传入train_x。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">train_x = torch.from_numpy(example_img)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;shape:&#123;&#125;, \ntrain_x:&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(train_x.shape, train_x))<br><br>---<br>shape:torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">895752</span>]), <br>train_x:tensor([[<span class="hljs-number">120</span>, <span class="hljs-number">150</span>,  <span class="hljs-number">88</span>,  ...,  <span class="hljs-number">43</span>,  <span class="hljs-number">39</span>,  <span class="hljs-number">38</span>]], dtype=torch.uint8)<br></code></pre></td></tr></table></figure><p>然后进入线性函数，传入<code>in_features</code>为<code>train_x.shape[1]</code>，把它变成一个10分类，再把test_model运行一下，将我们的<code>train_x</code>输入进去就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">test_model = Model(input_dimension=train_x.shape[<span class="hljs-number">1</span>], categorical=<span class="hljs-number">10</span>)<br>output = test_model(train_x.<span class="hljs-built_in">float</span>())<br></code></pre></td></tr></table></figure><p>这样的话,我们就可以产生出一个Softmax，有了这个Softmax，在这我们如果有很多个x，它就会对应我们很多个已知的y。</p><p>然后我们在这里定义一个loss：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">criterion = torch.nn.CrossEntropyLoss()<br></code></pre></td></tr></table></figure><p>再之后我们在做线性的时候之前，肯定是有一些ytrue数据的，肯定是知道它的y的，写个循环它就不断的可以去训练。</p><p>接着我们可以得到这个它的权重，那么在这里这是一张图片，如果这个图片要做回归，要给这个图片打分，那么将<code>out_features</code>换成1就可以了。</p><p>我们在Model里不断的去改它的东西，让它的输出能够满足就可以了。</p><p>不管是用户数据还是气象数据、天文数据、图片、文字，我们都可以把它变成这样的一个x向量。变成x向量之后只要送到一个模型里面，这个模型它能够去做优化，做些调整。那么它就能够去不断的去做优化。</p><p>当然，我们这里还缺一个optimizer：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = torch.optim.SGD(test_model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br></code></pre></td></tr></table></figure><p>我们定义了一个SGD优化器，learning_rate设置了一下，给了一个初始的学习率。</p><p>然后呢再不断的去循环它就可以了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义虚拟的y</span><br>lable = np.random.randint(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">10</span>)<br>train_y = torch.from_numpy(np.array([lable])).<span class="hljs-built_in">float</span>()<br><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    y_true = train_y<br>    y_predict = test_model(train_x.<span class="hljs-built_in">float</span>())<br><br>    <span class="hljs-built_in">print</span>(y_true.shape)<br>    <span class="hljs-built_in">print</span>(y_predict.shape)<br><br>    <span class="hljs-built_in">print</span>(loss)<br></code></pre></td></tr></table></figure><p>我们现在可以将criterion假如到循环里来计算一下loss了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    ...<br>    loss = criterion(y_predict, y_true)<br></code></pre></td></tr></table></figure><p>就是说，我们之前学习的这些内容，不管是图片还是用户的数据、或者文字，其实都是可以变成一个向量，再把向量送入到定义好的模型里，求出它的结果。</p><p>再经过反复的运作，反复的调试来更新它的数据。</p><h2 id="为什么rnn-or-cnn">为什么RNN or CNN</h2><p>那为什么我们还要学习RNN和CNN这些东西呢？我们刚开始学的<code>wx+b</code>的形式，可以把任意的x变成其它的一个output，但是它在解决一些问题的时候效果就不是太好。</p><p>比方说啊，我们要识别一个图像到底是什么的时候，wx+b它是给每一个x一个权重,<span class="math inline">\(w x_i + b\)</span>, 然后最后产出一个值。</p><p>但是图像我们是希望给中间一个区域一个平分，可是现在是一个点一个点的。</p><p>例如我们输入是一个x，输出是一个y。x它包含了多个x:{x1, x2, x3, ...,xn}，那y的输出呢，它是和多个x有关系。如果是在一个曲线上，我们取几个点,{output1, output2, output3}, 那么这个output3就不止和<spanclass="math inline">\(\vec x_3\)</span>有关系，它和前面的output2,output1都有关系。</p><p>也就是说，当下这一时刻的数据其实不仅取不仅取决于今天发生的一些事情，还取决于昨天前天，甚至大前天发生的事情。</p><p>但是我们如果直接进行<code>wxi+b</code>的话，这里xi=x3，wx3+b我们期望输出一个output3，这样就忽略了前边的这些事情。</p><p>与此类似的还有我们写文章，当前这个字和前面是什么字应该是有依赖关系的。其实把它抽象一下的话，会发现在现实生活中其实有很多种依赖关系。</p><p>我们之前讲的wx+b，其实是一对一。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501003.png"alt="Alt text" /></p><p>虽然x的维度可能会很大，y输出的维度也可能很大，但是它一个x就只对应输出一个y。</p><p>而除了one to one 之外，我们还有一些其他的类别：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501004.png"alt="Alt text" /></p><p><code>one to many</code>，就是x输入之后，最后会输出多个y。比方说咱们输入的是一个类别，输出的是一篇文章，分别是第一个单词，第二个单词和第三个单词。</p><p>我们会发现，这三个输出的单词前后是有相关性的。这种就属于是一对多，输出的的这些内容是独立的个体，但是它们之间有相关性。</p><p>后面的<code>many to one</code>，典型的一个应用，你给他输入一句话，输出这个地方，这句话到底是表示正向的还是负向的。那么这句话其实每个单词之间是有依赖关系的，而输出的是一个值。</p><p>那<code>many to many</code>里，前边输入的这个input是一个序列，有依赖关系。输出也是一个序列，有依赖关系。那么这会是一个什么？比方我们的机器翻译，就有可能是这样一个关系，对吧？还有比方说我们会去做那个文本的阅读理解，文本的摘要。</p><p>那还有一个<code>many to many</code>和第一个有什么区别呢？它其实只是更加的实时，比如说同声传译。</p><p>对于这些所有的问题我们给它抽象一下，它每一步的输出就像我们之前学过递归函数一样，是和前一步的输出有关系，还和当前这一步的输入有关系,我们其实学过最典型的一个依赖关系就是这样，就是斐波那契数列或者求阶乘：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">fib</span>(<span class="hljs-params">n</span>):<br>    <span class="hljs-keyword">if</span> n == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> n == <span class="hljs-number">1</span>: <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> fib(n-<span class="hljs-number">1</span>) + fib(n-<span class="hljs-number">2</span>)<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">fac</span>(<span class="hljs-params">n</span>):<br>    <span class="hljs-keyword">if</span> n == <span class="hljs-number">0</span>: <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>    <span class="hljs-keyword">else</span>: <span class="hljs-keyword">return</span> n*fac(n-<span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>): <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#123;&#125;\t&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(fib(i), fac(i)))<br></code></pre></td></tr></table></figure><p>那么这个怎么实现的？我们要实现这个有多种方法，我们可以来看一个具体的案例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RNN</span>(nn.Module):<br>    <span class="hljs-comment"># implement RNN from scratch rather than ysubf nn.RNN</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size</span>):<br>        <span class="hljs-built_in">super</span>(RNN, self).__init__()<br><br>        self.hidden_size = hidden_size<br>        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)<br>        self.i2o = nn.Linear(input_size + hidden_size, output_size)<br>        self.softmax = nn.LogSoftmax(dim=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_tensor, hidden_tensor</span>):<br>        combined = torch.cat((input_tensor, hidden_tensor), <span class="hljs-number">1</span>)<br><br>        hidden = self.i2h(combined)<br>        output = self.i2o(combined)<br></code></pre></td></tr></table></figure><p>这是一个非常经典的RNN的模型，我们来一起来分析它的构成。</p><p>在构造函数内，输入了一个<code>input_size</code>（x向量），还有一个<code>hidden_size</code>。然后在下面做了一个<code>i2h</code>的线性变化，这个线性变化它接受一个的两个参数，<code>in_features</code>是<code>input_size + hidden_size</code>,<code>out_features</code>是<code>hidden_size</code>。</p><p>现在有一个<span class="math inline">\(\vec x\)</span>和一个<spanclass="math inline">\(\vec h\)</span>，将两个向量相加输入进入，然后会输出一个<span class="math inline">\(vech\)</span>一样大小的东西。</p><p>然后下面还有一个<code>i2o</code>，它是将<code>input_size + hidden_size</code>输入之后，输出一个<code>output_size</code>一样大小的东西。</p><p>在输出这两个之后，我们将<code>output_size</code>大小的这个向量，输入到<code>Softmax</code>里面，就会变成一个概率分布。</p><p>然后它继续forward的时候，继续向前运算的时候，它的输入是input和hidden，那它在这里，如果我们要求训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">line_tensor, category_tensor</span>):<br>    hidden = rnn.init_hidden()<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(line_tensor.size()[<span class="hljs-number">0</span>]):<br>        output, hidden = rnn(line_tensor[i], hidden)<br></code></pre></td></tr></table></figure><p>这里它有很多的tensor，比如我们的<code>x:[x1, x2, ..., xn]</code>,这个tensor就是这些个x。那么它在做训练的第一步会取最前面的这个x向量，这个x向量刚开始会有一个随机的hidden向量，这个时候关键的地方就来了，就是它不断的重复:<code>output, hidden = rnn(line_tensor[i], hidden)</code>,我们来看，这个hidden就会一次一次的送进去做更新。</p><p>hidden一开始是随机的，之后t时刻的hidden的值是由上一时刻，也就是t-1时刻的x和hidden来影响的。</p><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livescript">h0 -&gt; random<br><span class="hljs-function"><span class="hljs-params">(x0, h0)</span> -&gt;</span> output1, h1<br><span class="hljs-function"><span class="hljs-params">(x1, h1)</span> -&gt;</span> output2, h2<br>...<br></code></pre></td></tr></table></figure><p>这样，输出的output2不仅是x1的影响，也是受到x0的影响的，这样前后的关系就被连接起来了。</p><p>就比如说我们输入的是一段文字，就比说<code>ChaHeng</code>，输入<code>C</code>的时候，我们会得到一个hidden，然后计算<code>h</code>时候，我们又会得到一个hidden,一直到最后一个<code>g</code>，那我们算这一步的时候，它既包含了<code>g</code>这个字母，还包含了之前<code>n</code>的hidden向量。那<code>n</code>再往上，一直到<code>C</code>都相关，这样它就实现了传递的效果。</p><p>那这个做法有两个人分别提出来了两种。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501005.png"alt="Alt text" /></p><p>之前，我们将神经网络建模为:</p><p><span class="math display">\[\begin{align*}y_t = \sigma(Wx_t + b) \\y_{t+1} = \sigma(Wx_{t+1} + b)\end{align*}\]</span></p><p>现在我们将其更新为两两种方法，一个是Elman network:</p><p><span class="math display">\[\begin{align*}h_t &amp; = \sigma_h(W_hx_t + U_hh_{t-1}+b_h) \\y_t &amp; = \sigma_y(W_yh_t + b_y)\end{align*}\]</span></p><p>还有一个是Jordan networks:</p><p><span class="math display">\[\begin{align*}h_t &amp; = \sigma_h(W_hx_t + U_hy_{t-1}+b_h) \\y_t &amp; = \sigma_y(W_yh_t + b_y)\end{align*}\]</span></p><p>我们看一下区别，其实就是为了加上非线性变化。给h加了一个非线性变化，再给y加了一个非线性变化。</p><p>这两个人都是很著名的计算机科学家，他们提出来的模型有区别，一个是一直在传递这个h，一个是一直在传递y。但是都实现了yt时刻和xt有关，也和x_{t-1}有关。这两个都实现了这样的一种功能，只不过它们中间一直传递的东西不太一样。</p><p>这个就是RNN的内核，它的内核就是这个东西。</p><p>我们接着，就来看一个案例，这个案例中的数据是一个盈利数据,还是老样子，数据集我就放在文末了。</p><p>我们这里是一个两个月每天的盈利指数，其中2点几的是盈利比较多，1点几的就是盈利比较少的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">timeserise_revenue = pd.read_csv(<span class="hljs-string">&#x27;~/mount/Sync/data/AI_Cheats/time_serise_revenue.csv&#x27;</span>)<br>sales_data = pd.read_csv(<span class="hljs-string">&#x27;~/mount/Sync/data/AI_Cheats/time_serise_sale.csv&#x27;</span>)<br><br>timeserise_revenue.drop(axis=<span class="hljs-number">1</span>, columns=<span class="hljs-string">&#x27;Unnamed: 0&#x27;</span>, inplace=<span class="hljs-literal">True</span>)<br>sales_data.drop(axis=<span class="hljs-number">1</span>, columns=<span class="hljs-string">&#x27;Unnamed: 0&#x27;</span>, inplace=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>数据上我就不展示了，大家自己拿到后查看一下。我们现在要做的是，是想根据它前十天的一个数据，来预测一下第11天的数据。</p><p>很简单的方法咱们可以写一个全连接的网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FullyConnected</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x_size, hidden_size, output_size</span>):<br>        <span class="hljs-built_in">super</span>(FullyConnected, self).__init__()<br>        self.hidden_size = hidden_size<br><br>        self.linear_with_tanh = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">10</span>, self.hidden_size),<br>            nn.Tanh(),<br>            nn.Linear(self.hidden_size, self.hidden_size),<br>            nn.Tanh(),<br>            nn.Linear(self.hidden_size, output_size)<br>        )<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        yhat = self.linear_with_tanh(x)<br>        <br>        <span class="hljs-keyword">return</span> yhat<br></code></pre></td></tr></table></figure><p>我们输入10个值对它进行线性变化，再给它进行一个非线性变化，然后重复一遍，最后再来一次线性变化，这样就是最简单的一种线性和非线性变化的网络。</p><p>然后我们处理一下数据，设置一下相关参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">sales_data.drop(axis=<span class="hljs-number">1</span>, columns=<span class="hljs-string">&#x27;Unnamed: 0&#x27;</span>, inplace=<span class="hljs-literal">True</span>)<br>source_data = sales_data<br><br>n_epochs = <span class="hljs-number">30</span><br>hidden_size = <span class="hljs-number">2</span> <span class="hljs-comment"># try to change this parameters </span><br>n_layers = <span class="hljs-number">1</span><br>batch_size = <span class="hljs-number">5</span><br>seq_length = <span class="hljs-number">10</span><br>n_sample_size = <span class="hljs-number">50</span><br><br>x_size = <span class="hljs-number">1</span><br><br>fc_model = FullyConnected(x_size, hidden_size, output_size=seq_length)<br>fc_model = fc_model.double()<br><br>criterion = nn.MSELoss()<br>optimizer = optim.SGD(fc_model.parameters(), lr=<span class="hljs-number">0.01</span>)<br><br>fc_losses = np.zeros(n_epochs) <br><br>plt.imshow(fc_model.state_dict()[<span class="hljs-string">&#x27;linear_with_tanh.0.weight&#x27;</span>])<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501006.png"alt="Alt text" /></p><p>显示了一下一开始的权重。</p><p>之后我们来看一下整个的训练过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">data_loader = torch.utils.data.DataLoader(source_data.values, batch_size=seq_length, shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<br>    epoch_losses = []<br>    <span class="hljs-keyword">for</span> iter_, t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):<br>        random_index = random.randint(<span class="hljs-number">0</span>, t.shape[-<span class="hljs-number">1</span>] - seq_length - <span class="hljs-number">1</span>)<br>        train_x = t[:, random_index: random_index+seq_length]<br>        train_y = t[:, random_index + <span class="hljs-number">1</span>: random_index + seq_length + <span class="hljs-number">1</span>]<br><br>        outputs = fc_model(train_x.double())<br><br>        optimizer.zero_grad()<br>        loss = criterion(outputs, train_y)<br>        loss.backward()<br>        optimizer.step()<br><br>        epoch_losses.append(loss.detach())<br>    fc_losses[epoch] = np.mean(epoch_losses)<br></code></pre></td></tr></table></figure><p>传入的<code>data_loader</code>是每一次随机的取期望的10个数字,这个数字我们就会根据序列来取出x和y,然后把x送到模型里边得到outputs,得到outputs之后又出现熟悉的面孔,我们求它的loss，再通过它的loss做反向传播。</p><p>optimizer做step，就是做全程的更新。</p><p>之后我们可以将每次循环的结果打印出来看看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<br>    ...<br>    <span class="hljs-keyword">for</span> iter_, t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):<br>        ...<br>        <span class="hljs-keyword">if</span> iter_  == <span class="hljs-number">0</span>:<br>            plt.clf()<br>            plt.ion()<br>            plt.title(<span class="hljs-string">&quot;Epoch &#123;&#125;, iter &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch, iter_))<br>            plt.plot(torch.flatten(outputs.detach()),<span class="hljs-string">&#x27;r-&#x27;</span>,linewidth=<span class="hljs-number">1</span>,label=<span class="hljs-string">&#x27;Output&#x27;</span>)<br>            plt.plot(torch.flatten(train_y),<span class="hljs-string">&#x27;c-&#x27;</span>,linewidth=<span class="hljs-number">1</span>,label=<span class="hljs-string">&#x27;Label&#x27;</span>)<br>            plt.plot(torch.flatten(train_x),<span class="hljs-string">&#x27;g-&#x27;</span>,linewidth=<span class="hljs-number">1</span>,label=<span class="hljs-string">&#x27;Input&#x27;</span>)<br>            plt.draw()<br>            plt.pause(<span class="hljs-number">0.05</span>)<br></code></pre></td></tr></table></figure><p>我们就不全展示了，大家可以自行去运行一下。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501007.png"alt="Alt text" /></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501008.png"alt="Alt text" /></p><p>红色是预测值，绿色是输入值，蓝色是实际值。这里我只放了第一张和第30张，也就是本次循环的最后一张。</p><p>那一开始，预测出来值没有和我们实际的值相符，到了30的相较而言是比较相符了。</p><p>我们看看它的loss是否如预期的下降了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.plot(fc_losses)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501009.png"alt="Alt text" /></p><p>看完全连接的模型，再来看看RNN的模型，做一个非常简单的RNN模型，那首先还是定义模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleRNN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x_size, hidden_size, n_layers, batch_size, output_size</span>):<br>        <span class="hljs-built_in">super</span>(SimpleRNN, self).__init__()<br>        self.hidden_size = hidden_size<br>        self.n_layers = n_layers<br>        self.batch_size = batch_size<br>        self.rnn = nn.RNN(x_size, hidden_size, n_layers, batch_first=<span class="hljs-literal">True</span>)<br>        self.out = nn.Linear(hidden_size, output_size) <span class="hljs-comment"># 10 in and 10 out</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, hidden=<span class="hljs-literal">None</span></span>):<br>        hidden = self.__init__hidden()<br>        output, hidden = self.rnn(inputs.<span class="hljs-built_in">float</span>(), hidden.<span class="hljs-built_in">float</span>())<br>        output = self.out(output.<span class="hljs-built_in">float</span>());<br><br>        <span class="hljs-keyword">return</span> output, hidden<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__hidden</span>(<span class="hljs-params">self</span>):<br>        hidden = torch.zeros(self.n_layers, self.batch_size, self.hidden_size, dtype=torch.float64)<br>        <span class="hljs-keyword">return</span> hidden<br></code></pre></td></tr></table></figure><p>我们输入的是<code>x_size</code>，然后然后定义一个<code>hidden_size</code>。这里注意啊，<code>hidden_size</code>是可以改的，越大可以表示的中间层的信息就越多，但意味着需要更多的数据去训练它。</p><p>然后在forward里，可以看到每一步会输出一个output，到最后一步的时候我们把output做一个线性变化，就可以变成期望的这个结果。</p><p>那这个RNN模型其实非常的简单，就是进了一个RNN，然后做了一个线性变化，把output做成线性变化。</p><p>然后我们来看看具体表现如何，那首先一样的是定义参数，数据可以用上一次整理过的数据，不需要再做一次了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">n_epochs = <span class="hljs-number">30</span><br>hidden_size = <span class="hljs-number">2</span> <span class="hljs-comment"># try to change this parameters </span><br>n_layers = <span class="hljs-number">1</span><br>batch_size = <span class="hljs-number">5</span><br>seq_length = <span class="hljs-number">10</span><br>n_sample_size = <span class="hljs-number">50</span><br><br>x_size = <span class="hljs-number">1</span><br>output_size = <span class="hljs-number">1</span><br><br>hidden = <span class="hljs-literal">None</span><br><br>rnn_model = SimpleRNN(x_size, hidden_size, n_layers, seq_length, output_size)<br><br>criterion = nn.MSELoss()<br>optimizer = optim.SGD(rnn_model.parameters(), lr=<span class="hljs-number">0.01</span>)<br><br>rnn_losses = np.zeros(n_epochs) <br></code></pre></td></tr></table></figure><p>然后我们就可以来跑一下了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">data_loader = torch.utils.data.DataLoader(source_data.values, batch_size=seq_length, shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<br>    <span class="hljs-keyword">for</span> iter_, t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):<br>        <span class="hljs-keyword">if</span> t.shape[<span class="hljs-number">0</span>] != seq_length: <span class="hljs-keyword">continue</span> <br><br>        random_index = random.randint(<span class="hljs-number">0</span>, t.shape[-<span class="hljs-number">1</span>] - seq_length - <span class="hljs-number">1</span>)<br>        train_x = t[:, random_index: random_index+seq_length]<br>        train_y = t[:, random_index + <span class="hljs-number">1</span>: random_index + seq_length + <span class="hljs-number">1</span>]<br><br>        outputs, hidden = rnn_model(train_x.double().unsqueeze(<span class="hljs-number">2</span>), hidden)<br><br>        optimizer.zero_grad()<br>        loss = criterion(outputs.double(), train_y.double().unsqueeze(<span class="hljs-number">2</span>))<br>        loss.backward()<br>        optimizer.step()<br><br>        epoch_losses.append(loss.detach())<br>    rnn_losses[epoch] = np.mean(epoch_losses)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501010.png"alt="Alt text" /></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501011.png"alt="Alt text" /></p><p>那RNN模型其实从第三轮的时候效果就已经出现了，我们的x一样，改变了一个模型之后拟合的效果就不一样了。</p><p>我们来看看它的loss：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501012.png"alt="Alt text" /></p><p>RNN模型跑下来，loss是下降到了0.67左右，那我们之前的全连接模型的loss是在0.8以上，还是有一些区别的。我们可以将两个模型的loss打印到一张图上，就更能看出来两个模型的区别了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.plot(rnn_losses, c=<span class="hljs-string">&#x27;red&#x27;</span>)<br>plt.plot(fc_losses, c=<span class="hljs-string">&#x27;green&#x27;</span>)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311051501013.png"alt="Alt text" /></p><p>就可以看到，非常明显。</p><p>举这个例子作用是想说明，<code>wx+b</code>加上非线性变化这种形式其实也能解决问题，但是遇到时间相关，序列相关的问题的时候，解决效果就没有RN模型这么好。</p><p>为什么没有RNN模型好呢？因为RNN模型在这个过程中每一步把前一步的hidden的影响给它保留了下来。就是说它每一步的输出的时候不是单纯的考虑这一步的输出，把之前每一步的x的值其实都保留下来了。这个区别就是为什么要有RNN，以及大家之后什么时候用RNN。</p><p>因为我这边只是做个测试，所以仅仅做了30次epoch，那之后，大家可以尝试一下将epoch改成200或者更多，来看看具体loss会下降到什么程度。</p><p>好，文章最后，就是本文所用的数据集了：</p><blockquote><p>time_serise_revenue.csv</p></blockquote><p>链接: https://pan.baidu.com/s/1dL9XdBgoi3nC2VOC6w_wnw?pwd=qmw6提取码: qmw6 --来自百度网盘超级会员v6的分享</p><blockquote><p>time_serise_sale.csv</p></blockquote><p>链接: https://pan.baidu.com/s/12wMJHzSZk91YPFcaG-K6Eg?pwd=1kmp提取码: 1kmp --来自百度网盘超级会员v6的分享</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://cdn.jsdelivr.net/gh/hivandu/notes/img/茶桁的AI秘籍_核心基础_27.png&quot;
alt=&quot;茶桁的AI秘籍_核心基础_27&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>26. 深度学习进阶 - 深度学习的优化方法</title>
    <link href="https://hivan.me/26.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
    <id>https://hivan.me/26.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</id>
    <published>2023-12-01T23:30:00.000Z</published>
    <updated>2023-12-16T08:14:13.145Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311040226671.png"alt="Alt text" /></p><span id="more"></span><p>Hi, 你好。我是茶桁。</p><p>上一节课中我们预告了，本节课是一个难点，同时也是一个重点，大家要理解清楚。</p><p>我们在做机器学习的时候，会用不同的优化方法。</p><h2 id="sgd">SGD</h2><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311040226673.png"alt="Alt text" /></p><p>上图中左边就是Batch Gradient Descent，中间是Mini-Batch GradientDescent, 最右边则是Stochastic Gradient Descent。</p><p>我们还是直接上代码写一下来看。首先我们先定义两个随机值，一个x，一个ytrue：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>x = np.random.random(size=(<span class="hljs-number">100</span>, <span class="hljs-number">8</span>))<br>ytrue = torch.from_numpy(np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">5</span>, size=(<span class="hljs-number">100</span>, <span class="hljs-number">1</span>)))<br></code></pre></td></tr></table></figure><p>x是一个100<em>8的随机值，ytrue是100</em>1的随机值，在0到5之间，这100个x对应着这100个ytrue的输入。</p><p>然后我们来定义一个Sequential,在里面按顺序放一个线性函数，一个Sigmoid激活函数，然后再来一个线性函数，别忘了咱们上节课所讲的，要注意x的维度大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">linear = torch.nn.Linear(in_features=<span class="hljs-number">8</span>, out_features=<span class="hljs-number">1</span>)<br>sigmoid = torch.nn.Sigmoid()<br>linear2 = torch.nn.Linear(in_features=<span class="hljs-number">1</span>, out_features=<span class="hljs-number">1</span>)<br><br>train_x = torch.from_numpy(x)<br><br>model = torch.nn.Sequential(linear, sigmoid, linear2).double()<br></code></pre></td></tr></table></figure><p>我们先来看一下训练x和ytrue值的大小：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(model(train_x).shape)<br><span class="hljs-built_in">print</span>(ytrue.shape)<br><br>---<br>torch.Size([<span class="hljs-number">100</span>, <span class="hljs-number">1</span>])<br>torch.Size([<span class="hljs-number">100</span>, <span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>然后我们就可以来求loss了，先拿到预测值，然后将预测值和真实值一起放进去求值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_fn = torch.nn.MSELoss()<br>yhat = model(train_x)<br>loss = loss_fn(yhat, ytrue)<br><span class="hljs-built_in">print</span>(loss)<br><br>---<br><span class="hljs-number">36.4703</span><br></code></pre></td></tr></table></figure><p>我们现在可以定义一个optimer，来尝试进行优化，我们来将之前的所做的循环个100次，在其中我们加上反向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">optimer = torch.optim.SGD(model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br><br><span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    yhat = model(train_x)<br>    loss = loss_fn(yhat, ytrue)<br>    loss.backward()<br>    <span class="hljs-built_in">print</span>(loss)<br>    optimer.step()<br><br>---<br>tensor(<span class="hljs-number">194.9302</span>, dtype=torch.float64, grad_fn=&lt;MseLossBackward0&gt;)<br>...<br>tensor(<span class="hljs-number">1.9384</span>, dtype=torch.float64, grad_fn=&lt;MseLossBackward0&gt;)<br><br></code></pre></td></tr></table></figure><p>可以看到，loss会一直降低。从194一直降低到了2左右。</p><p>在求解loss的时候，我们用到了所有的<code>train_x</code>，那这种方式就叫做Batchgradient Descent，批量梯度下降。</p><p>它会对整个数据集计算损失函数相对于模型参数的梯度。梯度是一个矢量，包含了每个参数相对与损失函数的变化率。</p><p>这个方法会使用计算得到的梯度来更新模型的参数。更新规则通常是按照一下方式进行：</p><p><span class="math display">\[\begin{align*}w_{t+1} = w_t - \eta \triangledown w_t\end{align*}\]</span></p><p><span class="math inline">\(w_{t+1}\)</span>是模型参数，<spanclass="math inline">\(\eta\)</span>是学习率， <spanclass="math inline">\(\triangledownw_t\)</span>是损失函数相对于参数的梯度。</p><p>但是在实际的情况下这个方法可能会有一个问题，比如说，我们在随机x的时候参数不是100，而是10^8，维度还是8维。假如它的维度很大，那么会出现的情况就是把x给加载到模型里面运算的时候，消耗的内存就会非常非常大，所需要的运算空间就非常大。</p><p>这也就是这个方法的一个缺点，计算成本非常高，由于需要计算整个训练数据集的梯度，因此在大规模数据集上的计算成本较高。而且可能会卡在局部最小值，难以逃离。说实话，我上面演示的数据也是尝试了几次之后拿到一次满意的，也遇到了在底部震荡的情况。</p><p>在这里可以有一个很简单的方法，我们规定每次就取20个:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span> // <span class="hljs-number">20</span>):<br>        batch_index = np.random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(train_x)), size=<span class="hljs-number">20</span>)<br><br>        yhat = model(train_x[batch_index])<br>        loss = loss_fn(yhat, ytrue[batch_index])<br>        loss.backward()<br>        <span class="hljs-built_in">print</span>(loss)<br>        optimer.step()<br></code></pre></td></tr></table></figure><p>这样做loss也是可以下降的，那这种方法就叫做Mini Batch。</p><p>还有一种方法很极端，就是Stochhastic GradientDescent，就是每次只取一个个数字:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span> // <span class="hljs-number">1</span>):<br>        ...<br></code></pre></td></tr></table></figure><p>这种方法很极端，但是可以每次都可以运行下去。那大家就知道，有这三种不同的优化方式。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311040226674.png"alt="Alt text" /></p><p>这样的话，我们来看一下，上图中的蓝色，绿色和紫色，分别对应哪种训练方式？</p><p>紫色的是Stochastic GradientDescent，因为它每次只取一个点，所以它的loss变化会很大，随机性会很强。换句话说，这一次取得数据好，可能loss会下降，如果数据取得不好，它的这个抖动会很大。</p><p>绿色就是Mini-Batch,我们刚才20个、20个的输入进去，是有的时候涨，有的时候下降。</p><p>最后蓝色的就是Batch Gradient Descent，因为它x最多，所以下降的最稳定。</p><p>但是因为每次x特别多内存，那有可能就满了。内存如果满了，机器就没有时间去运行程序，就会变得特别的慢。</p><h2 id="momentum">MOMENTUM</h2><p>我们上面讲到的了这个式子：</p><p><span class="math display">\[\begin{align*}w_{t+1} = w_t - \eta \triangledown w_t\end{align*}\]</span></p><p>这个是最原始的Grady descent,我们会发现一个问题，就是本来在等高线上进行梯度下降的时候，它找到的不是最快的下降的那条线，在实际情况中，数据量会很多，数量会很大。比方说做图片什么的，动辄几兆几十兆，如果要再加载几百个这个进去，那就会很慢。这个梯度往往可能会变的抖动会很大。</p><p>那有人就想了一个办法去减少抖动。就是我们每一次在计算梯度下降方向的时候，连带着上一次的方向一起考虑，然后取一个比例改变了原本的方向。那这样的话，整个梯度下降的线就会平缓了，抖动也就没有那么大，这个就叫做Momentum,动量。</p><p><span class="math display">\[\begin{align*}v_t &amp; = \gamma \cdot v_{t-1} + \eta \triangledown w_t \\w_{t+1} &amp; = w_t - v_t\end{align*}\]</span></p><p>动量在物理学中就是物体沿某个方向运动的能量。</p><p>之前我们每次的wt是直接去减去学习率乘以梯度，现在还考虑了v{t-1}的值，乘上一个gamma，这个值就是我们刚才说的取了一个比例。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311040226675.png"alt="Alt text" /></p><p>就像这个图一样，原来是红色，加了动量之后就变成蓝色，可以看到更平稳一些。</p><h2 id="rms-prop">RMS-PROP</h2><p>除了动量法之外呢，还有一个RMS-PROP方法，全称为Root mean squareprop。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311040226676.png"alt="Alt text" /></p><p><span class="math display">\[\begin{align*}S_{\frac{\partial loss}{\partial w}} &amp; = \beta S_{\frac{\partialloss}{\partial w}} + (1 - \beta)||\frac{\partial loss}{\partial w} ||^2\\S_{\frac{\partial loss}{\partial b}} &amp; = \beta S_{\frac{\partialloss}{\partial b}} + (1 - \beta)||\frac{\partial loss}{\partial b} ||^2\\w &amp; = w - \alpha \frac{\frac{\partial loss}{\partialw}}{\sqrt{S_{\frac{\partial loss}{\partial w}}}} \\b &amp; = b - \alpha \frac{\frac{\partial loss}{\partialb}}{\sqrt{S_{\frac{\partial loss}{\partial b}}}}\end{align*}\]</span></p><p>这个方法看似复杂，其实也是非常简单。这些方法在PyTorch里其实都有包含，我们可以直接调用。我们在这里还是要理解一下它的原理，之后做事的时候也并不需要真的取从头写这些玩意。</p><p>在讲它之前，我们再回头来说一下刚刚求解的动量法，动量法其实已经做的比较好了，但是还是有一个问题，它每次的rate是人工定义的。也就是我们上述公式中的<spanclass="math inline">\(\gamma\)</span>,这个比例是人工定义的，那在RMS-PROP中就写了一个动态的调整方法。</p><p>这个动态的调整方法就是我们每一次在进行调整w或者b的时候，都会除以一个根号下的<spanclass="math inline">\(S_{\frac{\partial loss}{\partialw}}\)</span>，我们往上看，如果<span class="math inline">\(\frac{\partialloss}{\partial w}\)</span>比较大的话，那么<spanclass="math inline">\(S_{\frac{\partial loss}{\partialw}}\)</span>也就将会比较大，那放在下面的式子中，根号下，也就是<spanclass="math inline">\(\sqrt{S_{\frac{\partial loss}{\partialw}}}\)</span>在分母上，那么w就会更小，反之则会更大。</p><p>所以说，当这一次的梯度很大的时候，这样一个方法就让<spanclass="math inline">\(\frac{\partial loss}{\partialw}\)</span>其实变小了，对b来说也是一样的情况。</p><p>也就说，如果上一次的方向变化的很严重，那么这一次就会稍微的收敛一点，就会动态的有个缩放。那么如果上一次变化的很小，那为了加速它，这个值反而就会变大一些。</p><p>所以说他是实现了一个动态的学习率的变化，当然它前面还有一个初始值，这个<spanclass="math inline">\(\gamma\)</span>需要人为设置，但是在这个<spanclass="math inline">\(\gamma\)</span>基础上它实现了动态的学习速率的变化。</p><p>动态的学习速率考察两个值，一个是前一时刻的变化的快慢，另一个就是它此时此刻变化的快慢。这个就叫做RMS。</p><h2 id="adam">ADAM</h2><p>那我们在这里，其实还有一个方法：ADAM。 <span class="math display">\[\begin{align*}V_{dw} &amp; = \beta_1V_{dw} + (1-\beta_1)dw \\V_{db} &amp; = \beta_1V_{db} + (1-\beta_1)db \\S_{dw} &amp; = \beta_2S_{dw} + (1-\beta_2)||dw||^2 \\S_{db} &amp; = \beta_2S_{db} + (1-\beta_2)||db||^2 \\&amp; V_{dw}^{corrected} = \frac{V_{dw}}{1-\beta_1^t} \\&amp; V_{db}^{corrected} = \frac{V_{db}}{1-\beta_1^t} \\&amp; S_{dw}^{corrected} = \frac{S_{dw}}{1-\beta_2^t} \\&amp; S_{db}^{corrected} = \frac{S_{db}}{1-\beta_2^t} \\w &amp; = w -\alpha\frac{V_{db}^{corrected}}{\sqrt{S_{dw}^{corrected}}+\varepsilon}\\b &amp; = b -\alpha\frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\varepsilon}\\\end{align*}\]</span></p><p>刚刚讲过的RMS特点其实是动态的调整了我们的学习率，之前讲Momentum其实还保持了上一时刻的方向，RMS就没有解决这个问题，RMS把上一时刻的方向给弄没了。</p><p>RMS，它的定义其实就没有考虑上次的方向，它只考虑上次变化的大小。而现在提出来这个ADAM，这个ADAM的意思就是AdaptiveMomentum,还记不记得咱们讲随机森林和Adaboost那一节，我们讲过Adaboost就是AdaptiveBoosting，这里的Adaptive其实就是一个意思，就是自适应动量，也叫动态变化动量。</p><p>ADAM就结合了RMS和动量的两个优点。第一个是他在分母上也加了一个根号下的数，也就做了RMS做的事，然后在分子上还有一个数，这个数就保留了上一时刻的数，比如<spanclass="math inline">\(V_{dw}^{corrected}\)</span>，就保留了上一时刻的V，就保留了上一时刻的方向。</p><p>所以ADAM既是动态的调整了学习率，又保留了上一时刻的方向。</p><p>那除此之外，其实还有一个AdaGrad和L-BFGS方法，不过常用的方法也就是上面详细讲的这几种。</p><p>到此为止，我们进阶神经网络的基础知识就都差不多具备了，接下来我们就该来讲解下卷机和序列，比如说LSTM和RNN、CNN的东西。在这些结束之后，我们还会有Attention机制，Transformer机制，YOLO机制，Segmentation机制，还有强化深度学习其实都是基于这些东西。</p><p>那我们下节课，就先从RNN来说开去。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311040226671.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>25. 深度学习进阶 - 权重初始化，梯度消失和梯度爆炸</title>
    <link href="https://hivan.me/25.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/"/>
    <id>https://hivan.me/25.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/</id>
    <published>2023-11-28T23:30:00.000Z</published>
    <updated>2023-11-29T06:18:47.001Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311031851045.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><span id="more"></span><p>咱们这节课会讲到权重初始化、梯度消失和梯度爆炸。咱们先来看看权重初始化的内容。</p><h2 id="权重初始化">权重初始化</h2><p>机器学习在我们使用的过程中的初始值非常的重要。就比如最简单的<code>wx+b</code>，现在要拟合成一个yhat，w如果初始的过大或者初始的过小其实都会比较有影响。</p><p>假设举个极端情况，就是w拟合的时候刚刚就拟合到了离x很近的地方，我们想象一下，这个时候是不是学习起来就会很快？所以对于深度学习模型权重的初始化是一个非常重要的事情，甚至有人就说把初始化做好了，其实绝大部分事情就已经解决了。</p><p>那么我们怎么样获得一个比较好的初始化的值？首先有这么几个原则</p><ul><li>我们的权重值不能设置为0。</li><li>尽量将权重变成一个随机化的正态分布。而且有更大的X输入，那我们的权重就应该更小。</li></ul><p><span class="math display">\[\begin{align*}loss &amp; = \sum(\hat y - y_i)^2 \\&amp; = \sum(\sum w_ix_i - y_i)^2\end{align*}\]</span></p><p>我们看上面的式子，yhat就是w_i*x_i,这个时候x_i可能是几百万，也可能是几百。我们w_i取值在(-n,n)之间，那当x_i维度特别大的时候，那yhat值算出来的也就会特别大。所以，x_i的维度特别大的时候，我们期望w_i值稍微小一些，否则加出来的yhat可能就会特别大，那最后求出来的loss也会特别大。</p><p>如果loss值特别大，可能就会得到一个非常的梯度。那我们知道，学习的梯度特别大的话，就会发生比较大的震荡。</p><p>所以有一个原则，就是当x的dimension很大的时候,我们期望的它的权重越小。</p><p>那后来就有人提出来了一个比较重要的初始化方法，Xavier初始化。这个方法特别适用于sigmoid激活函数或反正切tanh激活函数，它会根据前一层和当前层的神经元数量来选择初始化的范围，以确保权重不会过大或过小。<span class="math display">\[\begin{align*}均值为0和标准差的正态分布: \sigma &amp; =\sqrt{\frac{2}{n_{inputs}+n_{outputs}}} \\-r和+r之间的均匀分布：r &amp; = \sqrt{\frac{6}{n_{inputs}+n_{outputs}}}\end{align*}\]</span></p><p>然后W的均匀分布就会是这样： <span class="math display">\[W \sim U \Bigg \vert -\frac{\sqrt 6}{\sqrt{n_j + n_{j+1}}}, \frac{\sqrt6}{\sqrt{n_j + n_{j+1}}}\]</span></p><p>这个是一个比较有名的初始化方法，如果要做函数的初始化的话，PyTorch在init里面有一个方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nn.init.xavier_uniform_(tensor, gain=<span class="hljs-number">1.0</span>)<br></code></pre></td></tr></table></figure><p>比如，我们看这样例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.xavier_uniform_(w, gain=nn.calculate_gain(<span class="hljs-string">&#x27;relu&#x27;</span>))<br></code></pre></td></tr></table></figure><blockquote><p>注意:init方法里还有其他的一些方法，大家可以查阅PyTorch的相关文档：https://pytorch.org/docs/stable/nn.init.html</p></blockquote><h2 id="梯度消失与梯度爆炸">梯度消失与梯度爆炸</h2><p>当我们的模型层数特别多的时候</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311031849640.png"alt="Alt text" /></p><p>就比如我们上节课用到的Sequential，我们可以在里面写如非常多的一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">model = nn.Sequential(<br>    nn.Linear(in_features=<span class="hljs-number">10</span>, out_features=<span class="hljs-number">5</span>).double(),<br>    nn.Sigmoid(),<br>    nn.Linear(in_features=<span class="hljs-number">5</span>, out_features=<span class="hljs-number">8</span>).double(),<br>    nn.Sigmoid(),<br>    nn.Linear(in_features=<span class="hljs-number">8</span>, out_features=<span class="hljs-number">8</span>).double(),<br>    nn.Sigmoid(),<br>    ...<br>    nn.Linear(in_features=<span class="hljs-number">8</span>, out_features=<span class="hljs-number">8</span>).double(),<br>    nn.Softmax(),<br>)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311031849642.png"alt="Alt text" /></p><p>这样，在做偏导的时候我们其中几个值特别小，那两个一乘就会乘出来一个特别特别小的数字。最后可能会导致一个结果，<spanclass="math inline">\(\frac{\partial loss}{\partialwi}\)</span>的值就会极小，它的更新就会特别的慢。我们把这种东西就叫做梯度消失，也有人叫梯度弥散。</p><p>以Sigmoid函数为例，其导数为</p><p><span class="math display">\[\begin{align*}\sigma &#39;(x) = \sigma(x)(1-\sigma(x))\end{align*}\]</span></p><p>在x趋近正无穷或者负无穷时，导数接近0。当这种小梯度在多层网络中相乘的时候，梯度会迅速减小，导致梯度消失。</p><p>除此之外还有一种情况叫梯度爆炸，剃度爆炸类似，当模型的层很多的时候，如果其中某两个值很大，例如两个10<sup>2，当这两个乘起来就会变成10</sup>4。乘下来整个loss很大，又会产生一个结果，我们来看这样一个场景：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311031849643.png"alt="Alt text" /></p><p>假如说对于上图中这个函数来说，横轴为x,竖轴为loss，对于这个xi来说，这个地方<spanclass="math inline">\(\frac{\partial loss}{\partialxi}\)</span>已经是一个特别大的数字了。</p><p>假设咱们举个极端的情况（<strong>忽略图中竖轴上的数字</strong>），我们现在loss等于x^4：<spanclass="math inline">\(loss=x^4\)</span>，然后现在<spanclass="math inline">\(\frac{\partial loss}{\partialx^4}\)</span>就等于<spanclass="math inline">\(4x^3\)</span>，我们假设x在A点，当x=10的时候，那<spanclass="math inline">\(4\times x^3 = 4000\)</span>，那我们计算新的xi，就是<span class="math inline">\(x_i = x_i - \alpha\cdot \frac{\partial loss}{\partialx_i}\)</span>，现在给alpha一个比较小的数，我们假设是0.1，那式子就变成<spanclass="math inline">\(10 - 0.1 \times 4000\)</span>，结果就是-390。</p><p>我们把它变到-390之后，本来我们本来做梯度下降更新完，xi期望的是loss要下降，但是我们结合图像来看，xi=-390的时候，loss就变得极其的巨大了，然后我们在继续，(-390)^4，这个loss就已经爆炸了。</p><p>再继续的时候，会发现会在极值上跳来跳去，loss就无法进行收敛了。所以我们也要拒绝这种情况的发生。</p><p>那梯度消失和梯度爆炸这两个问题该如何解决呢？我们来看第一种解决方法：<code>Batch normalization</code>，批量归一化。</p><p>那这个方法的核心思想是对神经网络的每一层的输入进行归一化，使其具有零均值和单位方差。</p><p>那么首先，对于每个mini-batch中的输入数据，计算均值和方差。<spanclass="math inline">\(B = \{x_1...m\}\)</span>; 要学习的参数: <spanclass="math inline">\(\gamma,\beta\)</span>。</p><p><span class="math display">\[\begin{align*}\mu_B &amp; = \frac{1}{m}\sum^m_{i=1}x_i \\\sigma ^2_B &amp; = \frac{1}{m}\sum_{i=1}^m(x_i-\mu_B)^2 \\&amp; \mu 为均值mean， \sigma为方差\end{align*}\]</span></p><p>这里和咱们之前讲x做normalization的时候其实是特别相似，基本上就是一件事。</p><p>然后我们使用均值和方差对输入进行归一化，使得其零均值和单位方差，即将输入标准化为xhat。</p><p><span class="math display">\[\begin{align*}\hat x_i = \frac{x_i - \mu_B}{\sqrt{\sigma ^2_B + \varepsilon}}\end{align*}\]</span></p><p>接着我们对归一化后的输入应用缩放和平移操作，以允许网络学习最佳的变换。</p><p><span class="math display">\[\begin{align*}y_i = \gamma \hat x_i + \beta \equiv BN_{\gamma,\beta}(x_i)\end{align*}\]</span></p><p>输出为<span class="math inline">\(\{y_i =BN_{\gamma,\beta}(x_i)\}\)</span>。</p><p>最后将缩放和平移后的数据传递给激活函数进行非线性变换。</p><p>它会输入一个小批量的x值，</p><p>经过反复的梯度下降，会得到一个gamma和beta，能够知道在这一步x要怎么样进行缩放，在缩放之前会经历刚开始的时候那个normalization一样，把把过小值会变大，把过大值会变小。</p><p>我们在之前的课程中演示过，没看过和忘掉的同学可以往前翻看一下。</p><p>然后在经过这两个可学习的参数进行一个变化，这样它可以做到在每一层x变化不会极度的增大或者极度的缩小，可以让我们的权值保持的比较稳定。</p><p>那除了Batch normalization之外，还有一个方法叫Gradient clipping，它是可以直接将过大的梯度值变小。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311031849644.png"alt="Alt text" /></p><p>它其实很简单，也叫做梯度减脂。</p><p>如果我们求解出来<span class="math inline">\(\frac{\partialloss}{\partialw_i}\)</span>很大，假设原来等于400，我们定义了一个100，那超过100的部分，就全部设置成100。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">train_loss.backward()<br>pt.nn.units.clip_grad_value_(model.parameters(), <span class="hljs-number">100</span>)<br>optimizer.step()<br></code></pre></td></tr></table></figure><p>简单粗暴。那其实梯度爆炸还是比较容易解决的，比较复杂的其实是梯度消失的问题。</p><p>梯度爆炸为什么比较容易解决？梯度爆炸起码是有导数的，只要把这个导数给它放的特别小就行了，有导数起码保证wi可以更新。</p><p>假设alpha，我们的learning_rate等于0.01，乘上一个100，可以保证每次可以有个变化。但是每次这个梯度特别小，假如都快接近于0了，那么1e-10,就算乘上100倍，最后还是一个特别小的数字。所以相较而言，梯度爆炸就更好解决一些，方法更粗暴一些。</p><p>补充一个知识点，这个虽然现在已经用不到了，但是对我们的理解还是有帮助的。方法比较古老。</p><p>就是当我们发现梯度有问题的时候，大概在10年前，那个时候神经网络的模块也不太丰富，很多新出的model，做神经网络的人，一些导数，传播什么的都需要自己写，就我们前几节课写那个神经网络框架的时候做的事。</p><p>有的时候导数写错了，就有一种方法叫做gradient checking，梯度检查。</p><p>这个使用场景非常的少，当你自己发明了一个新的模块，加到这个模型里面的时候会遇到。</p><p>其实很简单，就是把最终的<span class="math inline">\(\frac{\partialloss}{\partialw_i}\)</span>，求解出来的偏导总是不收敛，可能是这个偏导有问题，那么有可能求导的函数写错了。</p><p>那在这个时候就可以做个简单的变化：</p><p><span class="math display">\[\begin{align*}\frac{\partial loss(\theta+\varepsilon)-\partial loss(\theta -\varepsilon)}{2\varepsilon}\end{align*}\]</span></p><p>这其中<span class="math inline">\(\partial loss(\theta +\varepsilon)\)</span>和<span class="math inline">\(\partial loss(\theta- \varepsilon)\)</span>是在参数<spanclass="math inline">\(\theta\)</span>,其实也就是我们的wi上添加和减去微小扰动theta后的损失函数值。</p><p>然后我们计算数值梯度和反向传播计算得到的梯度之间的差异。通常这是通过计算它们之间的差异来完成，然后将其与一个小的阈值，比如1e-7进行比较。如果差异非常小（小于阈值），则可以认为梯度计算是正确的，否则可能就需要从新写一下偏导函数了。</p><p>这个比较难，但不是一个重点，当且仅当自己要发明一个模型的时候。</p><p>那接下来我们来看一下关于Learning_rate和Early Stopping的问题。</p><p>理论上，如果深度学习效果不好，那么我们可以将learningrate调小，可以让所有模型效果变得更好，它可以让所有的loss下降。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311031849645.png"alt="Alt text" /></p><p>但是如果你的learningrate变得特别小，假如说是1e-9，那这样的结果就是w的变化会非常的慢，训练时间就变得很长。为了解决这个问题，就有一些比较简单的方法。</p><p>第一个，我们可以把learningrate和loss设置成一个相关的函数，例如说loss越小的时候，Learningrate越小，或者随着epoch的增大，loss越小。这个就叫learningrate的decay。</p><p>将learningrate或者训练次数和loss设置成一个相关的函数，那么越到后面效果越好的时候，learningrate就会越小。</p><p>还有，我们可能会发现loss连续k次不下降，那我们就可以提前结束训练过程，这个就是EarlyStopping。</p><p>也就是当你发现loss连续k次不下降，或者甚至于在上升，那么这个时候，就可以将最优的这个值给它记录下来。</p><p>咱们可能会经常出现的情况就是值在那里震荡，本来呢已经快接近于最优点了，可是震荡了几次之后，还可能震荡出去了，loss变大了。或者就一直在这个震荡里边出不去，这个时候多学习也没有用，所以就可以早点停止，这个就是EarlyStopping，中文有人称呼它为早停方法。</p><p>好，下节课，咱们要讲一个重点，也是一个难点。就是咱们做机器学习的时候，不同的优化方法。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311031851045.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>24. 深度学习进阶 - 矩阵运算的维度和激活函数</title>
    <link href="https://hivan.me/24.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>https://hivan.me/24.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</id>
    <published>2023-11-25T23:30:00.000Z</published>
    <updated>2023-11-29T06:18:52.997Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030256914.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><span id="more"></span><p>咱们经过前一轮的学习，已经完成了一个小型的神经网络框架。但是这也只是个开始而已，在之后的课程中，针对深度学习我们需要进阶学习。</p><p>我们要学到超参数，优化器，卷积神经网络等等。看起来，任务还是蛮重的。</p><p>行吧，让我们开始。</p><h2 id="矩阵运算的维度">矩阵运算的维度</h2><p>首先，我们之前写了一份拓朴排序的代码。那我们是否了解在神经网络中拓朴排序的作用。我们前面讲过的内容大家可以回忆一下，拓朴排序在咱们的神经网络中的作用不是为了计算方便，是为了能计算。</p><p>换句话说，没有拓朴排序的话，根本就没法计算了。Tensorflow和PyTourh最大的区别就是，Tensorflow在运行之前必须得把拓朴排序建好，PyTorch是在运行的过程中自己根据我们的连接状况一边运行一边建立。但是它们都有拓朴排序。</p><p>拓朴排序后要进行计算，那就要提到维度问题，在进行机器学习的时候一定要确保我们矩阵运算的维度正确。我们来看一下示例就明白我要说的了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.from_numpy(np.random.random(size=(<span class="hljs-number">4</span>, <span class="hljs-number">10</span>)))<br><span class="hljs-built_in">print</span>(x.shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure><p>假如说，现在我们生成了一个4，10的矩阵，也就是4行10列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>linear = nn.Linear(in_features=<span class="hljs-number">10</span>, out_features=<span class="hljs-number">5</span>).double()<br><span class="hljs-built_in">print</span>(linear(x).shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<br></code></pre></td></tr></table></figure><p>然后我们来给他定义一个线性变化，<code>in_features=10</code>，这个就是必须的,然后，out_features=5，假如把它分成5类。</p><p>这个时候，你看他就变成一个四行五列的一个东西了。</p><p>刚才我们说了，<code>in_features=10</code>是必须的，如果这个值我们设置成其他的，比如说8，那就不行了，运行不了。会收到警告：<code>mat1 and mat2 shapes cannot be multiplied (4x10 and 8x5)</code></p><p>我们再给它来一个Softmax</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">nonlinear = nn.Softmax()<br><span class="hljs-built_in">print</span>(nonlinear(linear(x)))<br></code></pre></td></tr></table></figure><p>这样，我们就得到了一个4*5的概率分布。</p><p>我们把这个非线性函数换一下，换成Sigmoid, 之前的Softmax赋值给yhat,咱们做一个多层的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">yhat = nn.Softmax()<br>nonlinear = nn.Sigmoid()<br>linear2 = nn.linear(in_features=n, out_features=<span class="hljs-number">8</span>).double()<br><br><span class="hljs-built_in">print</span>(yhat(linear2(nonlinear(linear(x)))))<br></code></pre></td></tr></table></figure><p>好，这个时候，我还并没有给<code>in_features</code>赋值，我们来想想，这个时候应该赋值是多少？也就是说，我们现在的linear2到底传入的特征是多少？</p><p>我们这里定义的<code>linear</code>和<code>linear2</code>其实就是<code>w*x+b</code>。</p><p>那这里我们来推一下，第一次使用linear的时候，我们得到了4*5的矩阵对吧？nonlinear并没有改变矩阵的维度。现在linear2中，那我们<code>in_features</code>赋值就得是5对吧？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">yhat = nn.Softmax()<br>nonlinear = nn.Sigmoid()<br>linear2 = nn.linear(in_features=<span class="hljs-number">5</span>, out_features=<span class="hljs-number">8</span>).double()<br><br><span class="hljs-built_in">print</span>(yhat(linear2(nonlinear(linear(x)))).shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">8</span>])<br></code></pre></td></tr></table></figure><p>然后我们就得到了一个<code>4*8</code>的维度的矩阵。</p><p>那其实在PyTorch里提供了一种比较简单的方法，就叫做<code>Sequential</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">model = nn.Sequential(<br>    nn.Linear(in_features=<span class="hljs-number">10</span>, out_features=<span class="hljs-number">5</span>).double(),<br>    nn.Sigmoid(),<br>    nn.Linear(in_features=<span class="hljs-number">5</span>, out_features=<span class="hljs-number">8</span>).double(),<br>    nn.Softmax(),<br>)<br><br><span class="hljs-built_in">print</span>(model(x).shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">8</span>])<br></code></pre></td></tr></table></figure><p>这样，我们就把刚才几个函数方法按顺序都一个一个的写在<code>Sequential</code>里，那其实刚才的过程，也就是解释了这个方法的原理。</p><p>接着，我们来写一个<code>ytrue</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">ytrue = torch.randint(<span class="hljs-number">8</span>, (<span class="hljs-number">4</span>, ))<br>loss_fn = nn.CrossEntropyLoss()<br><br><span class="hljs-built_in">print</span>(model(x).shape)<br><span class="hljs-built_in">print</span>(ytrue.shape)<br><br>---<br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">8</span>])<br>torch.Size([<span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure><p>现在ytrue就是CrossEntropyLoss输入的一个label值。</p><p>然后我们就可以进行反向传播了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">loss.backward()<br><br><span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters():<br>  <span class="hljs-built_in">print</span>(p, p.grad)<br>  <br>---<br>Parameter containing:<br>tensor([...])<br>...<br></code></pre></td></tr></table></figure><p>求解反向传播之后就可以得到它的梯度了。然后再经过一轮一轮的训练，就可以把梯度稳定在某个值，这就是神经网络进行学习的一个过程。那主要是在这个过程中，一定要注意矩阵前后的大小。</p><h2 id="激活函数">激活函数</h2><p>然后我们来看看激活函数的重要性。</p><p>在我们之前的课程中，我们提到过一个概念「激活函数」，不知道大家还有没有印象。那么激活函数的作用是什么呢？是实现非线性拟合对吧？</p><p>打比方来说，如果我们现在要拟合一个函数<code>f(x) = w*x+b</code>,你把它再给送到一个g(x)，再比如<code>g(x)=w2*x+b</code>，我们来做一个拟合，那么g(f(x)),那是不是还是一样，<code>g(f(x)) = w2*(w*x+b)+b</code>,然后就变成<code>w2*w*x + w2*b + b</code>，那其实这个就还是一个线性函数。</p><p>我们每一段都给它进行一个线性变化，再进行一个非线性变化，再进行一个线性变化，一段一段这样折起来，理论上它可以拟合任何函数。</p><p>这个怎么理解？其实我们如何用已知的函数去拟合函数在高等数学里边是一个一直在学习，一直在研究的东西。学高数的同学应该知道，高数里面有一个著名的东西叫做傅立叶变化，这是一种线性积分变换，用于函数在时域和频域之间的变换。</p><p>我们给定任意一个复杂的函数，都可以通过sin和cos来把它拟合出来，其关键思想是任何连续、周期或非周期的函数都可以表示为正弦和余弦函数的组合。通过计算不同频率的正弦和余弦成分的系数an和bn，我们可以了解一个函数的频谱特性，即它包含那些频率成分。</p><p><span class="math display">\[\begin{align*}f(x) = a_0 + \sum_{n=1}^0(a_n cos(2\pi nfx) + b_n sin(2\pi n fx))\end{align*}\]</span></p><p>除此之外，我们还有一个泰勒展开。我在数学篇的时候有仔细讲解过这个部分，大家可以回头去读一下我那篇文章，应该是数学篇第13节课，在那里我曾说过，所有的复杂函数都是用泰勒展开转换成多项式函数计算的。</p><p>之前有同学给我私信，也有同学在我文章下留言，说到某个位置看不懂了，还是数学拖了后腿。但是其实只是应用的话无所谓，但是如果想在这个方面有所建树，想要做些不一样的东西出来，还是要把数学的东西好好补一下的。</p><p>OK，那其实呢，我们的深度学习本质上其实就是在做这么一件事情，就是来自动拟合，到底是由什么构成的。</p><p>大家再来想一下，一个比较重要的，就是反向传播和前向传播。这个我们前面的课程里有详细的讲过，就是，我们的前向传播和反向传播的作用是什么。</p><p>那现在我们学完前几节了，回过头来我们想想，前向传播的作用是什么？反向传播的这个作用呢？</p><p>现在，假如说我已经训练出来了一个模型，我要用这个模型去预测。那么第一个问题是，预测的时候需不需要求loss？第二个是我需不需要做反向传播？</p><p>然后我们再来思考一个问题，如果我们需要求loss对于某个参数wi的偏导<spanclass="math inline">\(\frac{\partial loss}{\partialw_i}\)</span>，那么我们首先需要进行反向传播对吧？那我们在进行反向传播之前，能不能不进行前向传播？</p><p>也就是说，我们把这个模型放在这里，一个x，然后输入进去得到一个loss。那么咱们训练了一轮之后，我们能不能在求解的时候不进行前向传播，直接进行反向传播？</p><p>我们只要知道，求loss值需要预测值就明白了。</p><p>那我们继续来思考，loss值和precision、recall等等的关系是什么？这些是什么？我们之前学习过，这些是评测指标对吧？也就是再问，loss和评测指标的关系是什么？</p><p>也就是说，我们能不能用precision，能不能用precision来做我们的loss函数？不能对吧，无法求导。</p><p>所以在整个机器学习的过程中，如果要有反向传播、梯度下降，必须得是可导的。像我们所说的MSE是可导的，<code>cross-entropy</code>也是可以求导的。</p><p>那如果上过我之前课程的同学应该记得，可求导的的函数需要满足什么条件？光滑性和连续性对吧？连续性呢，是可求导的一个必要条件，但不是充分条件，还必须在某个点附近足够光滑，以使得导数存在。</p><p>对于loss函数的设定，第一点，一定是要能求偏导的。第二呢，就是它一定得是一个凸函数:Convexfunctions。</p><p>那什么叫做凸函数呢？如果一个函数上的任意两点连线上的函数值都不低于这两点的函数值的线段，就称为凸函数。常见的比如线性函数，指数函数，幂函数，绝对值函数等都是凸函数。</p><p>想象一下，有一辆车，从a点开到b点，如果这个车在a点到b点的时候方向盘始终是打在一个方向的，那我们就说它是凸函数。</p><p>不过在一些情况下有些函数它不是凸函数，就在数学上专门有一个研究领域，Convexoptimization，凸优化其实就是解决对于这种函数怎么样快速的求出他的基值，另外一个就是对于这种非凸函数怎么把它变成凸函数。</p><p>不同的激活函数它有什么区别呢？在最早的时候，大家用的是<code>Sigmoid</code>：</p><p><span class="math display">\[\begin{align*}\sigma(z)=\frac{1}{1+e^{-z}}\end{align*}\]</span></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030258830.png"alt="Sigmoid" /></p><p>为什么最早用Sigmoid，这是因为Sigmoid有个天然的优势，就是它输出是0-1，而且它处处可导。</p><p>但是后来Sigmoid的结果有个e^x，指数运算就比较费时，这是第一个问题。第二个问题是Sigmoid的输出虽然是在0~1之间，但是平均值是0.5，对于程序来说，我们希望获得均值等于0，STD等于1。我们往往希望把它变成这样的一种函数，这样的话做梯度下降的时候比较好做。</p><p>于是就又提出来了一个更简单的方法，就是反正切函数：<code>Tanh</code>。</p><p><span class="math display">\[\sigma(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\]</span></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030258831.png"alt="Tanh" /></p><p>它的形式和Sigmoid很像，不同的是平均值，它的平均值是0。现在这个用的也挺多。</p><p>但是Tanh和sigmoid一样都有一个小问题，就是它的绝大多数地方loss都等于0,那么wi大部分时候就没有办法学习，也就不会更新。</p><p>为了解决这个问题，就是有人提出来了一种非常简单的方法，就是<code>ReLU</code>：</p><p><span class="math display">\[\begin{align*}ReLU(z) = \begin{cases} z, z&gt;0 \\ 0, otherwise \end{cases}\end{align*}\]</span></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030258832.png"alt="Alt text" /></p><p>这种方法看似非常简单，但其实非常好用。它就是当一个x值经过ReLU的时候，如果它大于0就还保持原来的值，如果不大于0就直接把它变成0。</p><p>这样大家可能会觉得x&lt;0时有这么多值没有办法求导，但其实比起sigmoid来说可求导的范围其实已经变多了。而且你会发现要对他x大于0的地方求偏导非常的简单，就直接等于1。</p><p>可以保证它肯定是可以做更新的，而且ReLU这种函数它是大量的被应用在卷积神经网络里边。</p><p>在咱们后面的课程中，会讲到卷积，它是有一个卷积核，[F1,F2,F3,F4]然后把它经过ReLU之后，可能会变成[F1，0，F3，0]。那我们只要更新F1，F3就可以了，下一次再经过某种方式，在重新把F2和F4我们重新计算一下。</p><p>也就是说现在的<code>wx+b</code>不像以前一样，只有一个<code>w</code>，如果x值等于0，那整个都等于0.而是我们会有一个矩阵，它部分等于0也没关系。而且它的求导会变得非常的快，比求指数的导数快多了。</p><p>那其实这里还有一个小问题，面试的时候可能会问到，就是ReLU其实在0点的时候不可导，怎么办？</p><p>这个很简单，可以在函数里边直接设置一下，直接给他一个0的值就可以了，就是在代码里面加一句话。</p><p>再后来，又有人提出了一种方法：<code>LeakyRelU</code>：</p><p><span class="math display">\[LeakyReLU(z) = \begin{cases} z, z&gt;0 \\ az, otherwise \end{cases}\]</span></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311030258833.png"alt="Alt text" /></p><p>它把小于0的这些地方，也加了一个很小的梯度，这样的话大于0的时候partial就恒等于1，小于的时候partial也恒等于一个值，比如定一个<code>a=0.2</code>,都可以。那这样就可以实现处处有导数。</p><p>但是其实用的也不太多，因为我们事实上发现在这种卷积神经网络里边，我们每一次把部分的权重设置成0不更新，反而可以提升它的训练效率，我们反而可以每次把训练focuson在几个参数上。</p><p>好，下节课，咱们来看看初始化的内容。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311030256914.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>23. 深度学习 - 多维向量自动求导</title>
    <link href="https://hivan.me/23.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%AE%8C%E6%88%90%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A1%86%E6%9E%B6/"/>
    <id>https://hivan.me/23.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%AE%8C%E6%88%90%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A1%86%E6%9E%B6/</id>
    <published>2023-11-21T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:17.365Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509025.png"alt="茶桁的 AI 秘籍 核心能力 23" /></p><span id="more"></span><p>Hi, 你好。我是茶桁。</p><p>前面几节课中，我们从最初的理解神经网络，到讲解函数，多层神经网络，拓朴排序以及自动求导。可以说，最难的部分已经过去了，这节课到了我们来收尾的阶段，没错，生长了这么久，终于到迎接成果的时候了。</p><p>好，让我们开始。</p><blockquote><p>我们还是用上一节课的代码：<code>21.ipynb</code>。</p></blockquote><p>我们上一节课中，实现了自动计算的部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.name))<br>    node.backward()<br></code></pre></td></tr></table></figure><p>结果我就不打印了，节省篇幅。</p><p>那我们到这一步之后，咱们就已经获得了偏导，现在要考虑的问题就是去更新它，去优化它的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">learning_rate = <span class="hljs-number">1e-5</span><br><br><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.value = node.value + -<span class="hljs-number">1</span> * node.gradients[node] * learning_rate<br></code></pre></td></tr></table></figure><p>node 的值去更新，就应该等于它本身的值加上一个 -1乘以它的偏导在乘以一个<code>learning_rate</code>,我们对这个是不是已经很熟悉了？我们从第 8节线性回归的时候就一直在接触这个公式。</p><p>只不过在这个地方，x, y的值也要更新吗？它们的值是不应该去更新的，那要更新的应该是 k, b的值。</p><p>那么在这个地方该怎么办呢？其实很简单，我们添加一个判断就可以了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    <span class="hljs-keyword">if</span> node.is_trainable:<br>        node.value = node.value + -<span class="hljs-number">1</span> * node.gradients[node] * learning_rate<br></code></pre></td></tr></table></figure><p>然后我们给之前定义的类上加一个变量用于判断。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">..., is_trainable=<span class="hljs-literal">False</span></span>):<br>        ...<br>        self.is_trainable = is_trainable<br><br></code></pre></td></tr></table></figure><p>在这里我们默认是不可以训练的，只有少数的一些是需要训练的。</p><p>然后我们在初始化的部分把这个定义的值加上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">node_k = Placeholder(name=<span class="hljs-string">&#x27;k&#x27;</span>, is_trainable=<span class="hljs-literal">True</span>)<br>node_b = Placeholder(name=<span class="hljs-string">&#x27;b&#x27;</span>, is_trainable=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>对了，我们还需要将 Placeholder 做些改变：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Placeholder</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">..., is_trainable=<span class="hljs-literal">False</span></span>):<br>        Node.__init__(.., is_trainable=is_trainable)<br>        ...<br>    ...<br></code></pre></td></tr></table></figure><p>这就意味着，运行 for 循环的时候只有 k 和 b的值会更新，我们再加几句话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    <span class="hljs-keyword">if</span> node.is_trainable:<br>        ...<br>        cmp = <span class="hljs-string">&#x27;large&#x27;</span> <span class="hljs-keyword">if</span> node.gradients[node] &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;small&#x27;</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#123;&#125;的值&#123;&#125;，需要更新。&#x27;</span>.<span class="hljs-built_in">format</span>(node.name, cmp))<br><br>---<br>k的值small，需要更新。<br>b的值small，需要更新。<br></code></pre></td></tr></table></figure><p>我们现在将 forward, backward 和 optimize的三个循环封装乘三个方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">graph_sorted_nodes</span>):<br>    <span class="hljs-comment"># Forward</span><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>        node.forward()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">graph_sorted_nodes</span>):<br>    <span class="hljs-comment"># Backward</span><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.name))<br>        node.backward()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">optimize</span>(<span class="hljs-params">graph_sorted_nodes, learning_rate=<span class="hljs-number">1e-3</span></span>):<br>    <span class="hljs-comment"># optimize</span><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>        <span class="hljs-keyword">if</span> node.is_trainable:<br>            node.value = node.value + -<span class="hljs-number">1</span> * node.gradients[node] * learning_rate<br>            cmp = <span class="hljs-string">&#x27;large&#x27;</span> <span class="hljs-keyword">if</span> node.gradients[node] &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;small&#x27;</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#123;&#125;的值&#123;&#125;，需要更新。&#x27;</span>.<span class="hljs-built_in">format</span>(node.name, cmp))<br><br></code></pre></td></tr></table></figure><p>然后我们再来定义一个 epoch 方法，将 forward 和 backward放进去一起执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">run_one_epoch</span>(<span class="hljs-params">graph_sorted_nodes</span>):<br>    forward(graph_sorted_nodes)<br>    backward(graph_sorted_nodes)<br></code></pre></td></tr></table></figure><p>这样，我们完成一次完整的求值 - 求导 - 更新，就可以写成这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">run_one_epoch(sorted_nodes)<br>optimize(sorted_nodes)<br></code></pre></td></tr></table></figure><p>为了更好的观察，我们将所有的 print 都删掉，然后在 backward方法中写一个观察 loss 的打印函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">graph_sorted_nodes</span>):<br>    <span class="hljs-comment"># Backward</span><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(node, Loss):<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;loss value: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.value))<br>        node.backward()<br></code></pre></td></tr></table></figure><p>然后我们来对刚才完整的过程做个循环：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 完整的一次求值 - 求导 - 更新：</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    run_one_epoch(sorted_nodes)<br>    optimize(sorted_nodes, learning_rate=<span class="hljs-number">1e-1</span>)<br><br>---<br>loss value: <span class="hljs-number">0.12023025149136042</span><br>loss value: <span class="hljs-number">0.11090709486917472</span><br>loss value: <span class="hljs-number">0.10118818479676453</span><br>loss value: <span class="hljs-number">0.09120180962480523</span><br>loss value: <span class="hljs-number">0.08111466190584131</span><br>loss value: <span class="hljs-number">0.0711246044819575</span><br>loss value: <span class="hljs-number">0.061446239826641165</span><br>loss value: <span class="hljs-number">0.05229053883349982</span><br>loss value: <span class="hljs-number">0.043842158831920566</span><br>loss value: <span class="hljs-number">0.036239620745126</span><br></code></pre></td></tr></table></figure><p>可以看到 loss 在一点点的下降。当然，这样循环 10次我们还能观察出来，但是我们如果要成百上千次的去计算它，这样可就不行了，那我们需要将history 存下来，然后用图来显示出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_history = []<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    ...<br>    _loss_node = sorted_nodes[-<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">isinstance</span>(_loss_node, Loss)<br>    loss_history.append(_loss_node.value)<br>    optimize(sorted_nodes, learning_rate=<span class="hljs-number">1e-1</span>)<br><br>plt.plot(loss_history)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509027.png"alt="Alt text" /></p><p>我们现在可以验证一下，我们拟合的 yhat 和真实的 y之间差距有多大，首先我们当然是要获取到每个值的下标，然后用 sigmoid函数来算一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">sorted_nodes<br><br>---<br>[k, y, x, b, Linear, Sigmoid, Loss]<br></code></pre></td></tr></table></figure><p>通过下标来进行计算，k 是 0，x 是 2，b 是 3，y 是 1：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br><br><span class="hljs-comment"># k*x+b</span><br>sigmoid_x = sorted_nodes[<span class="hljs-number">0</span>].value * sorted_nodes[<span class="hljs-number">2</span>].value + sorted_nodes[<span class="hljs-number">3</span>].value<br><span class="hljs-built_in">print</span>(sigmoid(sigmoid_x))<br><br><span class="hljs-comment"># y</span><br><span class="hljs-built_in">print</span>(sorted_nodes[<span class="hljs-number">1</span>].value)<br><br>---<br><span class="hljs-number">0.891165479601981</span><br><span class="hljs-number">0.8988713384533658</span><br></code></pre></td></tr></table></figure><p>可以看到，非常的接近。那说明我们拟合的情况还是不错的。</p><p>好，这里总结一下，就是我们有了拓朴排序，就能向前去计算它的值，通过向前计算的值就可以向后计算它的值。那现在其实我们已经完成了一个mini的深度学习框架的核心内容，咱们能够定义节点，能够前向传播运算，能够反向传播运算，能更新梯度了。</p><p>那接下来是不是就结束了呢？很遗憾，并没有，接着咱们还要考虑如何处理多维数据。咱们现在看到的数据都是x、k、b 的输入，也就是都是一维的。</p><p>然而咱们真实世界中大多数场景下其实都是多维度的，其实都是多维数组。那么多维数组的还需要更新些什么，和现在有什么区别呢？</p><p>我们来接着往后看，因为基本上写法和现在这些几乎完全一样，那我也就不这么细致的讲了。</p><p>为了和之前代码做一个区分，所以我将多维向量计算的代码从新开了个文件，放在了<code>23.ipynb</code>里，小伙伴可以去下载到本地研习。</p><p>那么多维和现在最大的区别在哪里呢？就在于计算的时候，我们就要用到矩阵运算了。只是值变成了矩阵，运算变成的了矩阵运算。好，我们从Node开始来改动它，没什么变化的地方我就直接用<code>...</code>来省略了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>=[]</span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">raise</span> <span class="hljs-literal">NotImplemented</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">raise</span> <span class="hljs-literal">NotImplemented</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Placeholder</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        Node.__init__(self)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, value=<span class="hljs-literal">None</span></span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients = &#123;self:<span class="hljs-number">0</span>&#125;<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.outputs:<br>            grad_cost = n.gradients[self]<br>            self.gradients[self] = grad_cost * <span class="hljs-number">1</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, k, b</span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients = &#123;n: np.zeros_like(n.value) <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.inputs&#125;<br><br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.outputs:<br>            grad_cost = n.gradients[self]<br><br>            self.gradients[self.inputs[<span class="hljs-number">0</span>]] = np.dot(grad_cost, self.inputs[<span class="hljs-number">1</span>].value.T)<br>            self.gradients[self.inputs[<span class="hljs-number">1</span>]] = np.dot(self.inputs[<span class="hljs-number">0</span>].value.T, grad_cost)<br>            self.gradients[self.inputs[<span class="hljs-number">2</span>]] = np.<span class="hljs-built_in">sum</span>(grad_cost, axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-literal">False</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sigmoid</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, node</span>):<br>        Node.__init__(self, [node])<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_sigmoid</span>(<span class="hljs-params">self, x</span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.partial = self._sigmoid(self.x) * (<span class="hljs-number">1</span> - self._sigmoid(self.x))<br>        self.gradients = &#123;n: np.zeros_like(n.value) <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.inputs&#125;<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.outputs:<br>            grad_cost = n.gradients[self]  <br>            self.gradients[self.inputs[<span class="hljs-number">0</span>]] = grad_cost * self.partial<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MSE</span>(<span class="hljs-title class_ inherited__">Node</span>): <span class="hljs-comment"># 也就是之前的 Loss 类</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, y, a</span>):<br>        Node.__init__(self, [y, a])<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        y = self.inputs[<span class="hljs-number">0</span>].value.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        a = self.inputs[<span class="hljs-number">1</span>].value.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">assert</span>(y.shape == a.shape)<br>        self.m = self.inputs[<span class="hljs-number">0</span>].value.shape[<span class="hljs-number">0</span>]<br>        self.diff = y - a<br>        self.value = np.mean(self.diff**<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = (<span class="hljs-number">2</span> / self.m) * self.diff<br>        self.gradients[self.inputs[<span class="hljs-number">1</span>]] = (-<span class="hljs-number">2</span> / self.m) * self.diff<br></code></pre></td></tr></table></figure><p>类完成之后，我们还有一些其他的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_and_backward</span>(<span class="hljs-params">graph</span>): <span class="hljs-comment"># run_one_epoch</span><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> graph:<br>        n.forward()<br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span>  graph[::-<span class="hljs-number">1</span>]:<br>        n.backward()<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">toplogic</span>(<span class="hljs-params">graph</span>):<br>    ...<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_feed_dict_to_graph</span>(<span class="hljs-params">feed_dict</span>):<br>    ...<br><span class="hljs-comment"># 将 sorted_nodes 赋值从新定义了一个方法</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">topological_sort_feed_dict</span>(<span class="hljs-params">feed_dict</span>):<br>    graph = convert_feed_dict_to_graph(feed_dict)<br>    <span class="hljs-keyword">return</span> toplogic(graph)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">optimize</span>(<span class="hljs-params">trainables, learning_rate=<span class="hljs-number">1e-2</span></span>):<br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> trainables:<br>        node.value += -<span class="hljs-number">1</span> * learning_rate * node.gradients[node]<br></code></pre></td></tr></table></figure><p>这样就完成了。可以发现基本上代码没有什么变动，变化比较大的都是各个类中的backward 方法，因为要将其变成使用矩阵运算。</p><p>我们来尝试着用一下这个多维算法，我们还是用波士顿房价的那个数据来做一下尝试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python">X_ = data[<span class="hljs-string">&#x27;data&#x27;</span>]<br>y_ = data[<span class="hljs-string">&#x27;target&#x27;</span>]<br><br><span class="hljs-comment"># Normalize data</span><br>X_ = (X_ - np.mean(X_, axis=<span class="hljs-number">0</span>)) / np.std(X_, axis=<span class="hljs-number">0</span>)<br><br>n_features = X_.shape[<span class="hljs-number">1</span>]<br>n_hidden = <span class="hljs-number">10</span><br>W1_ = np.random.randn(n_features, n_hidden)<br>b1_ = np.zeros(n_hidden)<br>W2_ = np.random.randn(n_hidden, <span class="hljs-number">1</span>)<br>b2_ = np.zeros(<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># Neural network</span><br>X, y = Placeholder(), Placeholder()<br>W1, b1 = Placeholder(), Placeholder()<br>W2, b2 = Placeholder(), Placeholder()<br><br>l1 = Linear(X, W1, b1)<br>s1 = Sigmoid(l1)<br>l2 = Linear(s1, W2, b2)<br>cost = MSE(y, l2)<br><br>feed_dict = &#123;<br>    X: X_,<br>    y: y_,<br>    W1: W1_,<br>    b1: b1_,<br>    W2: W2_,<br>    b2: b2_<br>&#125;<br><br>epochs = <span class="hljs-number">5000</span><br><span class="hljs-comment"># Total number of examples</span><br>m = X_.shape[<span class="hljs-number">0</span>]<br>batch_size = <span class="hljs-number">16</span><br>steps_per_epoch = m // batch_size<br><br>graph = topological_sort_feed_dict(feed_dict)<br>trainables = [W1, b1, W2, b2]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Total number of examples = &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(m))<br></code></pre></td></tr></table></figure><p>我们在中间定义了 l1, s1, l2, cost,分别来实例化四个类。然后我们就需要根据数据来进行迭代计算了，定义一个losses 来保存历史数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">losses = []<br><br>epochs = <span class="hljs-number">100</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    loss = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(steps_per_epoch):<br>        <span class="hljs-comment"># Step 1</span><br>        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)<br><br>        X.value = X_batch<br>        y.value = y_batch<br><br>        <span class="hljs-comment"># Step 2</span><br>        forward_and_backward(graph) <span class="hljs-comment"># set output node not important.</span><br><br>        <span class="hljs-comment"># Step 3</span><br>        rate = <span class="hljs-number">1e-2</span><br>    <br>        optimize(trainables, rate)<br><br>        loss += graph[-<span class="hljs-number">1</span>].value<br>    <br>    <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>: <br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Epoch: &#123;&#125;, Loss: &#123;:.3f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(i+<span class="hljs-number">1</span>, loss/steps_per_epoch))<br>        losses.append(loss/steps_per_epoch)<br><br>---<br>Epoch: <span class="hljs-number">1</span>, Loss: <span class="hljs-number">194.170</span><br>...<br>Epoch: <span class="hljs-number">4901</span>, Loss: <span class="hljs-number">3.137</span><br></code></pre></td></tr></table></figure><p>可以看到它 loss下降的非常快，还记得咱们刚开始的时候在训练波士顿房价数据的时候，那个loss 下降到多少？最低是不是就下降到在第一节课的时候我们的 lose最多下降到了多少 47.34 对吧？那现在呢？直接下降到了3，这是为什么？因为我们的维度多了，维度多了它就准确了。这说明什么？说明大家去谈恋爱的时候，不要盯着对象的一个方面，多方面考察，才能知道这个人是否合适。</p><p>好，现在看起来效果是很好，但是我们想知道到底拟合出来的什么函数，那怎么办？咱们把这个维度降低成三维空间就可以看了。</p><p>现在咱们这个波士顿的所有数据实际上是一个 15 维的数据，15维的数据你根本看不了，咱们现在只要把 x这个里边取一点值，在这个里边稍微把值给它变一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X_ = dataframe[[<span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>]]<br>y_ = data[<span class="hljs-string">&#x27;target&#x27;</span>]<br></code></pre></td></tr></table></figure><p>在咱们之前的课程中对其进行计算的时候就分析过，RM 和 LSTAT是影响最大的两个特征，我们还是来用这个。然后我们将刚才的代码从新运行一遍：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">losses = []<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm_notebook(<span class="hljs-built_in">range</span>(epochs)):<br>    ...<br><br>---<br>Epoch: <span class="hljs-number">1</span>, Loss: <span class="hljs-number">150.122</span><br>...<br>Epoch: <span class="hljs-number">4901</span>, Loss: <span class="hljs-number">16.181</span><br></code></pre></td></tr></table></figure><p>这次下降的就没上次好了。</p><p>现在我们可视化一下这个三维空间来看看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mpl_toolkits.mplot3d <span class="hljs-keyword">import</span> Axes3D<br><br>predicate_results = []<br><span class="hljs-keyword">for</span> rm, ls <span class="hljs-keyword">in</span> X_.values:<br>    X.value = np.array([[rm, ls]])<br>    forward_and_backward(graph)<br>    predicate_results.append(graph[-<span class="hljs-number">2</span>].value[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br><br>%matplotlib widget<br><br>fig = plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>))<br>ax = fig.add_subplot(<span class="hljs-number">111</span>, projection=<span class="hljs-string">&#x27;3d&#x27;</span>)<br><br>X_ = dataframe[[<span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>]].values[:, <span class="hljs-number">0</span>]<br>Y_ = dataframe[[<span class="hljs-string">&#x27;RM&#x27;</span>, <span class="hljs-string">&#x27;LSTAT&#x27;</span>]].values[:, <span class="hljs-number">1</span>]<br><br>Z = predicate_results<br><br>rm_and_lstp_price = ax.plot_trisurf(X_, Y_, Z, color=<span class="hljs-string">&#x27;green&#x27;</span>)<br><br>ax.set_xlabel(<span class="hljs-string">&#x27;RM&#x27;</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;% of lower state&#x27;</span>)<br>ax.set_zlabel(<span class="hljs-string">&#x27;Predicated-Price&#x27;</span>)<br></code></pre></td></tr></table></figure><p>然后我们就能看到一个数据的三维图形，因为我们开启了widget，所以可以进行拖动。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509029.png"alt="Alt text" /></p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509030.png"alt="Alt text" /></p><p>从图形上看，确实符合房间越多，低收入人群越少，房价越高的特性。</p><p>那现在计算机确实帮我们自动的去找到了一个函数，这个函数到底怎么设置咱们都不用关心，它自动就给你求解出来，这个就是深度学习的意义。咱们经过这一系列写出来的东西其实就已经能够做到。</p><p>我觉得这个真的有一种数学之美，它从最简单的东西出发，最后做成了这样一个复杂的东西。确实很深其，并且还都在我们的掌握之中。</p><p>好，大家下来以后记得要多多自己敲代码，多分析其中的一些过程和原理。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311020509025.png&quot;
alt=&quot;茶桁的 AI 秘籍 核心能力 23&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>22. 深度学习 - 自动求导</title>
    <link href="https://hivan.me/22.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <id>https://hivan.me/22.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%9E%E7%8E%B0/</id>
    <published>2023-11-18T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:21.030Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202311012100649.png"alt="Alt text" /></p><span id="more"></span><p>Hi，你好。我是茶桁。</p><p>咱们接着上节课内容继续讲，我们上节课已经了解了拓朴排序的原理，并且简单的模拟实现了。我们这节课就来开始将其中的内容变成具体的计算过程。</p><p><code>linear, sigmoid</code>和<code>loss</code>这三个函数的值具体该如何计算呢？</p><p>我们现在似乎大脑已经有了一个起比较模糊的印象，可以通过它的输入来计算它的点。</p><p>让我们先把最初的父类 Node 改造一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs=[], name=<span class="hljs-literal">None</span></span>):<br>        ...<br>        self.value = <span class="hljs-literal">None</span><br>    <br>    ...<br></code></pre></td></tr></table></figure><p>然后再复制出一个，和<code>Placeholder</code>一样，我们需要继承Node，并且改写这个方法自己独有的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, k, b, name=<span class="hljs-literal">None</span></span>):<br>        Node.__init__(self, inputs=[x, k, b], name=name)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        x, k, b = self.inputs[<span class="hljs-number">0</span>], self.inputs[<span class="hljs-number">1</span>], self.inputs[<span class="hljs-number">2</span>]<br>        self.value = k.value * x.value + b.value<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;我是&#123;&#125;, 我没有人类爸爸，需要自己计算结果&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.value))<br>    ...<br></code></pre></td></tr></table></figure><p>我们新定义的这个类叫<code>Linear</code>, 它会接收 x, k, b。它继承了Node。这个里面的 forward该如何计算呢？我们需要每一个节点都需要一个值，一个变量，因为我们初始化的时候接收的x,k,b 都赋值到了 inputs里，这里我们将其取出来就行了，然后就是线性方程的公式<code>k*x+b</code>，赋值到它自己的value 上。</p><p>然后接着呢，就轮到 Sigmoid 了，一样的，我们定义一个子类来继承Node:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sigmoid</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, name=<span class="hljs-literal">None</span></span>):<br>        Node.__init__(self, inputs=[x], name=name)<br>        self.x = self.inputs[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_sigmoid</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        self.value = self._sigmoid(self.x.value)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;我是&#123;&#125;, 我自己计算了结果&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.value))<br>    ...<br></code></pre></td></tr></table></figure><p>Sigmoid 函数只接收一个参数，就是 x，其公式为1/(1+e^{-x})，我们在这里定义一个新的方法来计算，然后在 forward里把传入的 x取出来，再将其送到这个方法里进行计算，最后将结果返回给它自己的value。</p><p>那下面自然是 Loss 函数了，方式也是一模一样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Loss</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, y, yhat, name=<span class="hljs-literal">None</span></span>):<br>        Node.__init__(self, inputs = [y, yhat], name=name)<br>        self.y = self.inputs[<span class="hljs-number">0</span>]<br>        self.yhat = self.inputs[<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        y_v = np.array(self.y.value)<br>        yhat_v = np.array(self.y_hat.value)<br>        self.value = np.mean((y.value - yhat.value) ** <span class="hljs-number">2</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;我是&#123;&#125;, 我自己计算了结果&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.value))<br><br>    ...<br></code></pre></td></tr></table></figure><p>那我们这里定义成 Loss其实并不确切，因为我们虽然喊它是损失函数，但是其实损失函数的种类也非常多。而这里，我们用的MSE。所以我们应该定义为<code>MSE</code>，不过为了避免歧义，这里还是沿用Loss 好了。</p><p>定义完类之后，我们参数调用的类名也就需要改一下了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">...<br>node_linear = Linear(x=node_x, k=node_k, b=node_b, name=<span class="hljs-string">&#x27;linear&#x27;</span>)<br>node_sigmoid = Sigmoid(x=node_linear, name=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>node_loss = Loss(y=node_y, yhat=node_sigmoid, name=<span class="hljs-string">&#x27;loss&#x27;</span>)<br></code></pre></td></tr></table></figure><p>好，这个时候我们基本完成了，计算之前让我们先看一下<code>sorted_node</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">sorted_node<br><br>---<br>[Placeholder: y,<br> Placeholder: k,<br> Placeholder: x,<br> Placeholder: b,<br> Linear: Linear,<br> Sigmoid: Sigmoid,<br> MSE: Loss]<br></code></pre></td></tr></table></figure><p>没有问题，我们现在可以模拟神经网络的计算过程了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.forward()<br><br>---<br>我是x, 我已经被人类爸爸赋值为<span class="hljs-number">3</span><br>我是b, 我已经被人类爸爸赋值为<span class="hljs-number">0.3737660632429008</span><br>我是k, 我已经被人类爸爸赋值为<span class="hljs-number">0.35915077292816744</span><br>我是y, 我已经被人类爸爸赋值为<span class="hljs-number">0.6087876106387002</span><br>我是Linear, 我没有人类爸爸，需要自己计算结果<span class="hljs-number">1.4512183820274032</span><br>我是Sigmoid, 我没有人类爸爸，需要自己计算结果<span class="hljs-number">0.8101858733432837</span><br>我是Loss, 我没有人类爸爸，需要自己计算结果<span class="hljs-number">0.04056126022042443</span><br></code></pre></td></tr></table></figure><p>咱们这个整个过程就像是数学老师推公式一样，因为这个比较复杂。你不了解这个过程就求解不出来。</p><p>这就是为什么我一直坚持要手写代码的原因。<code>c+v</code>大法确实好，但是肯定是学的不够深刻。表面的东西懂了，但是更具体的为什么不清楚。</p><p>我们可以看到，我们现在已经将 Linear、Sigmoid 和 Loss都将值计算出来了。那我们现在已经实现了从 x 到 loss 的前向传播</p><p>现在我们有了loss，那就又要回到我们之前机器学习要做的事情了，就是将损失函数 loss的值降低。</p><p>之前咱们讲过，要将 loss的值减小，那我们就需要求它的偏导，我们前面课程的求导公式这个时候就需要拿过来了。</p><p>然后我们需要做的事情并不是完成求导就好了，而是要实现「链式求导」。</p><p>那从 Loss 开始反向传播的时候该做些什么？先让我们把“口号”喊出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">...</span>):<br>        ...<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> self.inputs:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;获取∂&#123;&#125; / ∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, n.name))<br></code></pre></td></tr></table></figure><p>这样修改一下Node，然后在其中假如一个反向传播的方法，将口号喊出来。</p><p>然后我们来看一下口号喊的如何，用<code>[::-1]</code>来实现反向获取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    node.backward()<br><br>---<br>获取∂Loss / ∂y<br>获取∂Loss / ∂Sigmoid<br>获取∂Sigmoid / ∂Linear<br>获取∂Linear / ∂x<br>获取∂Linear / ∂k<br>获取∂Linear / ∂b<br></code></pre></td></tr></table></figure><p>这样看着似乎不是太直观，我们再将 node的名称加上去来看就明白很多：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    <span class="hljs-built_in">print</span>(node.name)<br>    node.backward()<br>---<br>Loss<br>获取∂Loss / ∂y<br>获取∂Loss / ∂Sigmoid<br>Sigmoid<br>获取∂Sigmoid / ∂Linear<br>Linear<br>获取∂Linear / ∂x<br>获取∂Linear / ∂k<br>获取∂Linear / ∂b<br>...<br></code></pre></td></tr></table></figure><p>最后的<code>k, y, x, b</code>我就用...代替了，主要是函数。</p><p>那我们就清楚的看到，Loss 获取了两个偏导，然后传到了 Sigmoid，Sigmoid获取到一个，再传到Linear，获取了三个。那现在其实我们只要把这些值能乘起来就可以了。我们要计算步骤都有了，只需要把它乘起来就行了。</p><p>我们先是需要一个变量，用于存储 Loss 对某个值的偏导</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">...</span>):<br>        ...<br>        self.gradients = <span class="hljs-built_in">dict</span>()<br>    ...<br></code></pre></td></tr></table></figure><p>然后我们倒着来看，先来看 Loss:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Loss</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">0</span>].name)<br>        self.gradients[self.inputs[<span class="hljs-number">1</span>]] = <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">1</span>].name)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[0]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">0</span>]]))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[1]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">1</span>]]))<br></code></pre></td></tr></table></figure><p>眼尖的小伙伴应该看出来了，我现在依然还是现在里面进行「喊口号」的动作。主要是先来看一下过程。</p><p>刚才每个 node 都有一个 gradients，它代表的是对某个节点的偏导。</p><p>现在这个节点 self 就是 loss，然后我们<code>self.inputs[0]</code>就是y, <code>self.inputs[1]</code>就是 yhat,也就是<code>node_sigmoid</code>。那么我们现在这个<code>self.gradients[self.inputs[n]]</code>其实就分别是<code>∂loss/∂y</code>和<code>∂loss/∂yhat</code>，我们把对的值分别赋值给它们。</p><p>然后我们再来看 Sigmoid：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sigmoid</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">0</span>].name)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[0]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">0</span>]]))<br></code></pre></td></tr></table></figure><p>我们依次来看哈，这个时候的 self 就是 Sigmoid了，这个时候的<code>sigmoid.inputs[0]</code>应该是 Linear对吧，然后我们整个<code>self.gradients[self.inputs[0]]</code>自然就应该是<code>∂sigmoid/∂linear</code>。</p><p>我们继续，这个时候<code>self.outputs[0]</code>就是 loss,<code>loss.gradients[self]</code>那自然就应该是输出过来的<code>∂loss/∂sigmoid</code>，然后呢，我们需要将这两个部分乘起来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>    self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-string">&#x27;*&#x27;</span>.join([self.outputs[<span class="hljs-number">0</span>].gradients[self], <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">0</span>].name)])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[0]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">0</span>]]))<br></code></pre></td></tr></table></figure><p>接着，我们就需要来看看 Linear 了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>    self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-string">&#x27;*&#x27;</span>.join([self.outputs[<span class="hljs-number">0</span>].gradients[self], <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">0</span>].name)])<br>    self.gradients[self.inputs[<span class="hljs-number">1</span>]] = <span class="hljs-string">&#x27;*&#x27;</span>.join([self.outputs[<span class="hljs-number">0</span>].gradients[self], <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">1</span>].name)])<br>    self.gradients[self.inputs[<span class="hljs-number">2</span>]] = <span class="hljs-string">&#x27;*&#x27;</span>.join([self.outputs[<span class="hljs-number">0</span>].gradients[self], <span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.name, self.inputs[<span class="hljs-number">2</span>].name)])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[0]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">0</span>]]))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[1]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">1</span>]]))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[2]: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.gradients[self.inputs[<span class="hljs-number">2</span>]]))<br></code></pre></td></tr></table></figure><p>和上面的分析一样，我们先来看三个<code>inputs[n]</code>的部分，self在这里是 linear了，这里的<code>self.inputs[n]</code>分别应该是<code>x, k, b</code>对吧，那么它们就应该分别是<code>linear.gradients[x]</code>,<code>linear.gradients[k]</code>和<code>linear.gradients[b]</code>，也就是<code>∂linear/∂x</code>,<code>∂linear/∂k</code>,<code>∂linear/∂b</code>。</p><p>那反过来，<code>outputs</code>就应该反向来找，那么<code>self.outputs[0]</code>这会儿就应该是sigmoid。<code>sigmoid.gradients[self]</code>就是前一个输出过来的<code>∂loss/∂sigmoid * ∂sigmoid/∂linear</code>,那后面以此的[1]和[2]我们也就应该明白了。</p><p>然后后面分别是<code>∂linear/∂x</code>,<code>∂linear/∂k</code>,<code>∂linear/∂b</code>。一样，我们将它们用乘号连接起来。</p><p>公式就应该是：</p><p><span class="math display">\[\begin{align*}\frac{\partial loss}{\partial sigmoid} \cdot \frac{\partialsigmoid}{\partial linear} \cdot \frac{\partial linear}{\partial x} \\\frac{\partial loss}{\partial sigmoid} \cdot \frac{\partialsigmoid}{\partial linear} \cdot \frac{\partial linear}{\partial k} \\\frac{\partial loss}{\partial sigmoid} \cdot \frac{\partialsigmoid}{\partial linear} \cdot \frac{\partial linear}{\partial b} \\\end{align*}\]</span></p><p>那同理，我们还需要写一下<code>Placeholder</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Placeholder</span>(<span class="hljs-params">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;我获取了我自己的 gradients: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.outputs[<span class="hljs-number">0</span>].gradients[self]))<br>    ...<br></code></pre></td></tr></table></figure><p>好，我们来看下我们模拟的情况如何，看看它们是否都如期喊口号了，结合我们之前的前向传播的结果，我们一起来看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.forward()<br>    <br><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.name))<br>    node.backward()<br><br>---<br>Loss<br>[<span class="hljs-number">0</span>]: ∂Loss/∂y<br>[<span class="hljs-number">1</span>]: ∂Loss/∂Sigmoid<br><br>Sigmoid<br>[<span class="hljs-number">0</span>]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear<br><br>Linear<br>[<span class="hljs-number">0</span>]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂x<br>[<span class="hljs-number">1</span>]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂k<br>[<span class="hljs-number">2</span>]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂b<br><br>k<br>我获取了我自己的gradients: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂k<br><br>b<br>我获取了我自己的gradients: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂b<br><br>x<br>我获取了我自己的gradients: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂x<br><br>y<br>我获取了我自己的gradients: ∂Loss/∂y<br></code></pre></td></tr></table></figure><p>好，观察下来没问题，那我们现在还剩下最后一步。就是将这些口号替换成真正的计算的值，其实很简单，就是将我们之前学习过并写过的函数替换进去就可以了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        x, k, b = self.inputs[<span class="hljs-number">0</span>], self.inputs[<span class="hljs-number">1</span>], self.inputs[<span class="hljs-number">2</span>]<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = self.outputs[<span class="hljs-number">0</span>].gradients[self] * k.value<br>        self.gradients[self.inputs[<span class="hljs-number">1</span>]] = self.outputs[<span class="hljs-number">0</span>].gradients[self] * x.value<br>        self.gradients[self.inputs[<span class="hljs-number">2</span>]] = self.outputs[<span class="hljs-number">0</span>].gradients[self] * <span class="hljs-number">1</span><br>        ...<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sigmoid</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        self.value = self._sigmoid(self.x.value)<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = self.outputs[<span class="hljs-number">0</span>].gradients[self] * self.value * (<span class="hljs-number">1</span> - self.value)<br>        ...<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Loss</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        y_v = self.y.value<br>        yhat_v = self.y_hat.value<br>        self.gradients[self.inputs[<span class="hljs-number">0</span>]] = <span class="hljs-number">2</span>*np.mean(y_v - yhat_v)<br>        self.gradients[self.inputs[<span class="hljs-number">1</span>]] = -<span class="hljs-number">2</span>*np.mean(y_v - yhat_v)<br></code></pre></td></tr></table></figure><p>那我们来看下真正计算的结果是怎样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes[::-<span class="hljs-number">1</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(node.name))<br>    node.backward()<br><br>---<br>Loss<br>∂Loss/∂y: -<span class="hljs-number">0.402796525409167</span><br>∂Loss/∂Sigmoid: <span class="hljs-number">0.402796525409167</span><br><br>Sigmoid<br>∂Sigmoid/∂Linear: <span class="hljs-number">0.06194395247945269</span><br><br>Linear<br>∂Linear/∂x: <span class="hljs-number">0.02224721841122111</span><br>∂Linear/∂k: <span class="hljs-number">0.18583185743835806</span><br>∂Linear/∂b: <span class="hljs-number">0.06194395247945269</span><br><br>y<br>gradients: -<span class="hljs-number">0.402796525409167</span><br><br>k<br>gradients: <span class="hljs-number">0.18583185743835806</span><br><br>b<br>gradients: <span class="hljs-number">0.06194395247945269</span><br><br>x<br>gradients: <span class="hljs-number">0.02224721841122111</span><br></code></pre></td></tr></table></figure><p>好，到这里，我们就实现了前向传播和反向传播，让程序自动计算出了它们的偏导值。</p><p>不过我们整个动作还没有结束，就是我们需要将 loss降低到最小才可以。</p><p>那我们下节课，就来完成这一步。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202311012100649.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>21. 深度学习 - 拓朴排序的原理和实现</title>
    <link href="https://hivan.me/21.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E6%8B%93%E6%9C%B4%E6%8E%92%E5%BA%8F%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/"/>
    <id>https://hivan.me/21.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E6%8B%93%E6%9C%B4%E6%8E%92%E5%BA%8F%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/</id>
    <published>2023-11-15T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:25.438Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310311950753.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><p>上节课，我们讲了多层神经网络的原理，并且明白了，数据量是层级无法超过3 层的主要原因。</p><span id="more"></span><p>然后我们用一张图来解释了整个链式求导的过程：</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310311950755.png"alt="Alt text" /></p><p>那么，我们如何将这张图里的节点关系来获得它的求导过程呢？</p><p>假如我们现在定义一个函数<code>get_output</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_output</span>(<span class="hljs-params">graph, node</span>):<br>    outputs = []<br>    <span class="hljs-keyword">for</span> n, links <span class="hljs-keyword">in</span> graph.items():<br>        <span class="hljs-keyword">if</span> node == n: outputs += links<br>    <span class="hljs-keyword">return</span> outputs<br>get_output(computing_graph, <span class="hljs-string">&#x27;k1&#x27;</span>)<br><br>---<br>[<span class="hljs-string">&#x27;L1&#x27;</span>]<br></code></pre></td></tr></table></figure><p>我们可以根据 k1 获得 l1。</p><p>来，让我们整理一下思路，问：如何获得 k1 的偏导：</p><ol type="1"><li>获得 k1 的输出节点</li><li>获得 k1 输出节点的输出节点</li><li>...直到我们找到最后一个节点</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_order = []<br><br>target = <span class="hljs-string">&#x27;k1&#x27;</span><br>out = get_output(computing_graph, target)[<span class="hljs-number">0</span>]<br>computing_order.append(target)<br><br><span class="hljs-keyword">while</span> out:<br>    computing_order.append(out)<br>    out = get_output(computing_graph, out)<br>    <span class="hljs-keyword">if</span> out: out = out[<span class="hljs-number">0</span>]<br><br>computing_order<br><br>---<br>[<span class="hljs-string">&#x27;k1&#x27;</span>, <span class="hljs-string">&#x27;L1&#x27;</span>, <span class="hljs-string">&#x27;sigmoid&#x27;</span>, <span class="hljs-string">&#x27;L2&#x27;</span>, <span class="hljs-string">&#x27;loss&#x27;</span>]<br></code></pre></td></tr></table></figure><p>我们从 k1 出发，它可以获得这么一套顺序。那么现在如果要计算 k1的偏导，我们的这个偏导顺序就等于从后到前给它求解一遍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">order = <span class="hljs-string">&#x27;&#x27;</span><br><br><span class="hljs-keyword">for</span> i, n <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(computing_order[:-<span class="hljs-number">1</span>]):<br>    order += <span class="hljs-string">&#x27;*∂&#123;&#125; / ∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(n, computing_order[i+<span class="hljs-number">1</span>])<br><br>order<br>---<br><span class="hljs-string">&#x27;*∂k1 / ∂L1*∂L1 / ∂sigmoid*∂sigmoid / ∂L2*∂L2 / ∂loss&#x27;</span><br></code></pre></td></tr></table></figure><p>现在 k1的求导顺序计算机就给它自动求解出来了，我们把它放到了一个图里面，然后它自动就求解出来了。只不过唯一的问题是现在这个order 是反着的，需要把它再反过来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i, n <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(computing_order[:-<span class="hljs-number">1</span>]):<br>    order.append((computing_order[i + <span class="hljs-number">1</span>], n))<br>    <span class="hljs-comment"># order += &#x27; * ∂&#123;&#125; / ∂&#123;&#125;&#x27;.format(n, computing_order[i+1])</span><br><br><span class="hljs-string">&#x27; * &#x27;</span>.join([<span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(a, b) <span class="hljs-keyword">for</span> a, b <span class="hljs-keyword">in</span> order[::-<span class="hljs-number">1</span>]])<br><br>---<br><span class="hljs-string">&#x27;∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂k1&#x27;</span><br></code></pre></td></tr></table></figure><p>这个过程用计算机实现之后，我们就可以拿它来看一下其他的参数，比如说<code>b1</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_order = []<br><br>target = <span class="hljs-string">&#x27;b1&#x27;</span><br>out = get_output(computing_graph, target)[<span class="hljs-number">0</span>]<br>computing_order.append(target)<br><br><span class="hljs-keyword">while</span> out:<br>    computing_order.append(out)<br>    out = get_output(computing_graph, out)<br>    <span class="hljs-keyword">if</span> out: out = out[<span class="hljs-number">0</span>]<br><br>order = []<br><br><span class="hljs-keyword">for</span> i, n <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(computing_order[:-<span class="hljs-number">1</span>]):<br>    order.append((computing_order[i + <span class="hljs-number">1</span>], n))<br>    <span class="hljs-comment"># order += &#x27; * ∂&#123;&#125; / ∂&#123;&#125;&#x27;.format(n, computing_order[i+1])</span><br><br><span class="hljs-string">&#x27; * &#x27;</span>.join([<span class="hljs-string">&#x27;∂&#123;&#125;/∂&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(a, b) <span class="hljs-keyword">for</span> a, b <span class="hljs-keyword">in</span> order[::-<span class="hljs-number">1</span>]])<br><br>---<br><span class="hljs-string">&#x27;∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂b1&#x27;</span><br></code></pre></td></tr></table></figure><p>k2:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">...<br>target = <span class="hljs-string">&#x27;k2&#x27;</span><br>...<br><br>---<br><span class="hljs-string">&#x27;∂loss/∂L2 * ∂L2/∂k2&#x27;</span><br></code></pre></td></tr></table></figure><p>到这里，我们能够自动的求解各个参数的导数了。</p><p>然后我们将其封装一下，然后循环一下每一个参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_paramter_partial_order</span>(<span class="hljs-params">p</span>):<br>    ...<br>    target = p<br>    ...<br>    <span class="hljs-keyword">return</span> ...<br><br><span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;b1&#x27;</span>, <span class="hljs-string">&#x27;k1&#x27;</span>, <span class="hljs-string">&#x27;b2&#x27;</span>, <span class="hljs-string">&#x27;k2&#x27;</span>]:<br>    <span class="hljs-built_in">print</span>(get_paramter_partial_order(p))<br><br>---<br>∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂b1<br>∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂k1<br>∂loss/∂L2 * ∂L2/∂b2<br>∂loss/∂L2 * ∂L2/∂k2<br></code></pre></td></tr></table></figure><p>到这一步你就能够发现，每一个参数的导数的偏导我们都可以求解了。而且我们还发现一个问题，不管是<code>['b1', 'k1', 'b2', 'k2']</code>中的哪一个，我们都需要求求解<code>∂loss/∂L2</code>。</p><p>所以现在如果有一个内存能够记录结果，先把<code>∂loss/∂L2</code>的值求解下来，把这个值先存下来，只要算出来这一个值之后，再算<code>['b1', 'k1', 'b2', 'k2']</code>的时候直接拿过来就行了。</p><p>也就是说我们首先需要记录的就是这个值，其次，如果我们把 L2 和 sigmoid的值记下来，求解 b1 和 k1的时候直接拿过来用就行，不需要再去计算一遍，这个时候我们的效率就会提升很多。</p><p>首先把共有的一个基础<code>∂loss/∂L2</code>计算了，第二步，有了<code>∂loss/∂L2</code>，把<code>∂L2/∂sigmoid</code>再记录一遍，第三个是<code>∂sigmoid/∂L1</code>,然后后面以此就是<code>∂L1/∂b1</code>,<code>∂L1/∂k1</code>，<code>∂L2/∂b2</code>, <code>∂L2/∂k2</code>。</p><p>现在的问题就是就是怎么样让计算机自动得到这个顺序，计算机得到这个顺序的时候，把这些值都存在某个地方。</p><p>这个所谓的顺序就是我们非常重要的一个概念，在计算机科学，算法里面非常重要的一个概念：「拓朴排序」。</p><p>那拓朴排序该如何实现呢？来，我们一起来实现一下：</p><p>首先，我们定义一个方法，咱们输入的是一个图，这个图的定义方式是一个Dictionary，然后里面有一些节点，里面的很多个连接的点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">topologic</span>(<span class="hljs-params">graph</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    graph: dict</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">        node: [node1, node2, ..., noden]</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><p>因为我们要把它的结果存在一个变量里边，当我们不断的检查看这个图，看看它是否为空，然后我们来定义两个存储变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">topologic</span>(<span class="hljs-params">graph</span>):<br>    sorted_node = []<br><br>    <span class="hljs-keyword">while</span> graph:<br>        all_inputs = []<br>        all_outputs = <span class="hljs-built_in">list</span>(graph.keys())<br><br>    <span class="hljs-keyword">return</span> sorted_node<br></code></pre></td></tr></table></figure><p>这里的两个变量，<code>all_inputs</code>和<code>all_outputs</code>,一个是用来存储所有的输入节点，一个是存储所有的输出节点。</p><p>我们还记得我们那个图的格式是什么样的吗？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_graph = &#123;<br>    <span class="hljs-string">&#x27;k1&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;b1&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;x&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;L1&#x27;</span>:[<span class="hljs-string">&#x27;sigmoid&#x27;</span>],<br>    <span class="hljs-string">&#x27;sigmoid&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;k2&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;b2&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;L2&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>],<br>    <span class="hljs-string">&#x27;y&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>]<br>&#125;<br></code></pre></td></tr></table></figure><p>我们看这个数据，那所有的输出节点是不是就是其中的<code>key</code>啊？</p><p>打比方说，我们拿一个短小的数据来做示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">simple_graph = &#123;<br>    <span class="hljs-string">&#x27;a&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],<br>    <span class="hljs-string">&#x27;b&#x27;</span>: [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]<br>&#125;<br><br><span class="hljs-built_in">list</span>(simple_graph.keys())<br><br>---<br>[<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>]<br></code></pre></td></tr></table></figure><p>那我们这样就拿到了输出节点，并将其放在了一个列表内。</p><p>这里说点其他的，Python 3.9及以上的版本其实都实现了自带拓朴排序，但是如果你的 Python版本较低，那还是需要自己去实现。这个也是 Python 3.9里面一个比较重要的更新。</p><p>那为什么我们的 value 定义的是一个列表呢？这是因为这个key，也就是输出值可能会输出到好几个函数里面，因为我们现在拿的是一个比较简单的模型，但是在真实场景中，有可能会输出到更多的节点中。</p><p>这里，就获得了所有有输入的节点， <code>simple_graph</code>中，a输出给了[1,2], b 输出给了[2,3]。</p><p>那我们怎么获得所有输入的节点呢？那就应该是 value。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">list</span>(simple_graph.values())<br><br>---<br>[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br></code></pre></td></tr></table></figure><p>这样就获得所有有输入的节点。然后就是怎么样把这两个 list合并。可以有一个简单的方法，一个叫做 reduce 的方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">reduce(<span class="hljs-keyword">lambda</span> a, b: a+b, <span class="hljs-built_in">list</span>(simple_graph.values()))<br><br>---<br>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]<br></code></pre></td></tr></table></figure><p>这样就把它给它连起来了。</p><p>那我们还需要找一个，就是只有输出没有输入的节点，这些该怎么去找呢？其实也就是我们的<code>[k1, b1, k2, b2, y]</code>这些值。</p><p>来，我们还是拿刚才的<code>simple_graph</code>来举例，但是这次我们改一下里面的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">simple_graph = &#123;<br>    <span class="hljs-string">&#x27;a&#x27;</span>: [<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-number">2</span>],<br>    <span class="hljs-string">&#x27;b&#x27;</span>: [<span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-number">4</span>]<br>&#125;<br><br>a = <span class="hljs-built_in">list</span>(simple_graph.keys())<br>b = reduce(<span class="hljs-keyword">lambda</span> a, b: a+b, <span class="hljs-built_in">list</span>(simple_graph.values()))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(b) - <span class="hljs-built_in">set</span>(a)))<br><br>---<br>[<span class="hljs-number">2</span>, <span class="hljs-number">4</span>]<br></code></pre></td></tr></table></figure><p>我们没有用循环，而是将其变成了一个集合，然后利用集合的加减来做。</p><p>我们的实际代码就可以这样写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">topologic</span>(<span class="hljs-params">graph</span>):<br>    sorted_node = []<br><br>    <span class="hljs-keyword">while</span> graph:<br>        all_inputs = reduce(<span class="hljs-keyword">lambda</span> a, b: a+b, <span class="hljs-built_in">list</span>(graph.values()))<br>        all_outputs = <span class="hljs-built_in">list</span>(graph.keys())<br><br>        all_inputs = <span class="hljs-built_in">set</span>(all_inputs)<br>        all_outputs = <span class="hljs-built_in">set</span>(all_outputs)<br><br>        need_remove = all_outputs - all_inputs<br><br>    <span class="hljs-keyword">return</span> sorted_node<br></code></pre></td></tr></table></figure><p>那现在我们继续往后，如果找到了这些只有输出没有输入的节点之后，我们做一个判断，然后定义一个节点，用来保存随机选择的节点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(need_remove) &gt; <span class="hljs-number">0</span>:<br>    node = random.choice(<span class="hljs-built_in">list</span>(need_remove))<br></code></pre></td></tr></table></figure><p>这个时候 x, b, k, y都有可能，那么我们随机找一个就行。然后将这个找到的节点从 graph给它删除。并且将其插入到<code>sorted_node</code>中去，并且返回出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">    <span class="hljs-keyword">if</span> ...:<br>        node = random.choice(<span class="hljs-built_in">list</span>(need_remove))<br>        graph.pop(node)<br>        sorted_node.append(node)<br><br><span class="hljs-keyword">return</span> sorted_node<br></code></pre></td></tr></table></figure><p>然后这里还会出一个小问题，我们还是拿一个示例来说：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">simple_graph = &#123;<br>    <span class="hljs-string">&#x27;a&#x27;</span>: [<span class="hljs-string">&#x27;sigmoid&#x27;</span>],<br>    <span class="hljs-string">&#x27;b&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>],<br>    <span class="hljs-string">&#x27;c&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>]<br>&#125;<br><br>simple_graph.pop(<span class="hljs-string">&#x27;b&#x27;</span>)<br>simple_graph<br><br>---<br>&#123;<span class="hljs-string">&#x27;a&#x27;</span>: [<span class="hljs-string">&#x27;sigmoid&#x27;</span>], <span class="hljs-string">&#x27;c&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>]&#125;<br></code></pre></td></tr></table></figure><p>看，我们在删除 node 的时候，其所对应的 value也就一起删除了，那这个时候，我们最后的输出列表里会丢失最后一个node。所以，我们在判断为最后一个的时候，需要额外的将其加上，放在 pop方法执行之前。那我们整个代码需要调整一下先后顺序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">topologic</span>(<span class="hljs-params">graph</span>):<br>    sorted_node = []<br>    <span class="hljs-keyword">while</span> graph:<br>        ...<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(need_remove) &gt; <span class="hljs-number">0</span>:<br>            node = random.choice(<span class="hljs-built_in">list</span>(need_remove))<br>            sorted_node.append(node)<br>           <br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(graph) == <span class="hljs-number">1</span>: sorted_node += graph[node]         <br>               <br>            graph.pop(node)<br><br>    <span class="hljs-keyword">return</span> sorted_node<br></code></pre></td></tr></table></figure><p>现在其实这个代码就已经 OK 了，我们来再加几句话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">...<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(need_remove) &gt; <span class="hljs-number">0</span>:<br>    ...<br>    <span class="hljs-keyword">for</span> _, links <span class="hljs-keyword">in</span> graph.items():<br>        <span class="hljs-keyword">if</span> node <span class="hljs-keyword">in</span> links: links.remove(node)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-keyword">raise</span> TypeError(<span class="hljs-string">&#x27;This graph has circle, which cannot get topological order.&#x27;</span>)<br>...<br></code></pre></td></tr></table></figure><p>我们把它的连接关系，例如现在选择了 k1，我们要把 k1的连接关系从这些里边给它删掉。</p><p>遍历一下 graph，遍历的时候如果删除的 node在它的输出里边，我们就把它删除。</p><p>加上<code>else</code>判断，如果图不是空的，但是最终没有找到，也就是这两个集合作减法，但是得到一个空集，没有找到，那我们就来输出一个错误：<code>This graph has circle, which cannot get topological order.</code></p><p>现在我们可以来实验一下了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">x, k, b, linear, sigmoid, y, loss = <span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;k&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;linear&#x27;</span>, <span class="hljs-string">&#x27;sigmoid&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&#x27;loss&#x27;</span><br>test_graph = &#123;<br>    x: [linear],<br>    k: [linear],<br>    b: [linear],<br>    linear: [sigmoid],<br>    sigmoid: [loss],<br>    y: [loss]<br>&#125;<br><br>topologic(test_graph)<br><br>---<br>[<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-string">&#x27;k&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>, <span class="hljs-string">&#x27;linear&#x27;</span>, <span class="hljs-string">&#x27;sigmoid&#x27;</span>, <span class="hljs-string">&#x27;loss&#x27;</span>]<br></code></pre></td></tr></table></figure><p>好，现在让我们来声明一个<code>class node</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure><p>然后我们先来抽象一下这些节点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## Our Simple Model Elements</span><br><br>node_x = Node(inputs=<span class="hljs-literal">None</span>, outputs=[node_linear])<br>node_y = Node(inputs=<span class="hljs-literal">None</span>, outputs=[node_loss])<br>node_k = Node(inputs=<span class="hljs-literal">None</span>, outputs=[node_linear])<br>node_b = Node(inputs=<span class="hljs-literal">None</span>, outputs=[node_linear])<br>node_linear = Node(inputs=[node_x, node_k, node_b], outputs=[node_sigmoid])<br>node_sigmoid = Node(inputs=[node_linear], outputs=[node_loss])<br>node_loss = Node(inputs=[node_sigmoid, node_y], outputs=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><p>现在咱们就把图中每个节点已经给它抽象好了，但是我们发现节点写成这个样子代码是比较冗余。打比方说：<code>node_linear = Node(input=[node_x, node_k, node_b], outputs=[node_sigmoid])</code>，既然我们已经告诉程序<code>node_linear</code>这个节点的输入是<code>[node_x, node_k, node_b]</code>，那其实也就是告诉程序这些节点的输出是<code>node_linear</code>。</p><p>好，我们接下来要在<code>class Node</code>里定义一个方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs, outputs</span>):<br>    self.inputs = inputs<br>    self.outputs = outputs<br></code></pre></td></tr></table></figure><p>现在我们根据上面对代码冗余的分析，可以加上这样简单的一句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs=[]</span>):<br>    self.inputs = inputs<br>    self.outputs = []<br><br>    <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> inputs:<br>        node.outputs.append(self)<br></code></pre></td></tr></table></figure><p>把这句加上之后，就可以只在里面输入 inputs 就行了，不用再输入outputs，代码就变得简单多了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## Our Simple Model Elements</span><br><span class="hljs-comment">### version - 02</span><br>node_x = Node()<br>node_y = Node()<br>node_k = Node()<br>node_b = Node()<br>node_linear = Node(inputs=[node_x, node_k, node_b])<br>node_sigmoid = Node(inputs=[node_linear])<br>node_loss = Node(inputs=[node_sigmoid, node_y])<br></code></pre></td></tr></table></figure><p>我们是把每个节点给它做出来了，那么怎么样能够把这个节点给它像串珠子一样串起来变成一张图呢？</p><p>其实我们只要去考察所有的边沿节点就可以了，把所有的 x，y，k 和 b这种外层的函数给个变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">need_expend = [node_x, node_y, node_k, node_b]<br></code></pre></td></tr></table></figure><p>咱们再生成一个变量，这个变量是用来通过外沿这些节点，把连接图给生成出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_graph = defaultdict(<span class="hljs-built_in">list</span>)<br><br><span class="hljs-keyword">while</span> need_expend:<br>    n = need_expend.pop(<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-keyword">if</span> n <span class="hljs-keyword">in</span> computing_graph: <span class="hljs-keyword">continue</span><br><br>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> n.outputs:<br>        computing_graph[n].append(m)<br>        need_expend.append(m)<br></code></pre></td></tr></table></figure><p><code>while</code>里面，当外沿节点的 list不为空的时候，我们就在里面来取一个点，我们就取第一个吧，取出来并删除。</p><p>然后如果这个点我们已经考察过了，那就<code>continue</code>，如果没有，我们对于所有的这个n 里边的<code>outputs</code>，插入到 computing_graph 的 n的位置。再插入到外沿节点的 list内。因为我们现在多了一个扩充节点，所以我们需要给插入进去。</p><p>比方说我们这次找出来了 linear，把 linear也加到这个需要扩充的点一行，然后就可以从 linear 再找到 sigmoid 了。</p><p>来，我们看下现在的这个<code>computing_graph</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_graph<br><br>---<br>defaultdict(<span class="hljs-built_in">list</span>,<br>            &#123;&lt;__main__.Node at <span class="hljs-number">0x12053e080</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x12053e9b0</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053ef50</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x12053d510</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x12053c280</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x1202860e0</span>&gt;],<br>             &lt;__main__.Node at <span class="hljs-number">0x1202860e0</span>&gt;: [&lt;__main__.Node at <span class="hljs-number">0x12053ef50</span>&gt;]&#125;)<br></code></pre></td></tr></table></figure><p>这样就获得出来了，其实是把它变成了刚刚的那个图。这样呢，我们就可以应用<code>topologic</code>来进行拓朴排序了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">topologic(computing_graph)<br><br>---<br>[&lt;__main__.Node at <span class="hljs-number">0x12053c280</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053d510</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053e080</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053e9b0</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053dc30</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x1202860e0</span>&gt;,<br> &lt;__main__.Node at <span class="hljs-number">0x12053ef50</span>&gt;]<br></code></pre></td></tr></table></figure><p>但是我们打出来的内容都是一些内存地址，我们还需要改一下这个程序。我们在我们的<code>class Node</code>里多增加一个方法，用于return 它的名字：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs=[], name=<span class="hljs-literal">None</span></span>):<br>    ...<br>    self.name = name<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;Node:&#123;&#125; &#x27;</span>.<span class="hljs-built_in">format</span>(self.name)<br></code></pre></td></tr></table></figure><p>这样之后，我们还需要改一下节点，在里面增加一个变量<code>name=''</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">node_x = Node(name=<span class="hljs-string">&#x27;x&#x27;</span>)<br>...<br>node_loss = Node(inputs=[node_sigmoid, node_y], name=<span class="hljs-string">&#x27;loss&#x27;</span>)<br></code></pre></td></tr></table></figure><p>每一个都需要加上，我用<code>...</code>简化了代码。</p><p>然后我们再来看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">topologic(computing_graph)<br><br>---<br>[Node:k , Node:x , Node:b , Node:linear , Node:sigmoid , Node:y , Node:loss ]<br></code></pre></td></tr></table></figure><p>然后我们来将这段封装起来，变成一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">feed_dict = &#123;<br>    node_x: <span class="hljs-number">3</span>, <br>    node_y: random.random(),<br>    node_k: random.random(),<br>    node_b: <span class="hljs-number">0.38</span><br>&#125;<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_feed_dict_to_graph</span>(<span class="hljs-params">feed_dict</span>):<br>    need_expend = [n <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> feed_dict]<br>    ...<br><br>    <span class="hljs-keyword">return</span> computing_graph<br></code></pre></td></tr></table></figure><p>一般来说，很多大厂在建立代码的时候，<code>x, y, k, b</code>这种东西会被称为<code>placeholder</code>，我们创建的<code>need_expend</code>会被称为是<code>feed_dict</code>。所以我们做了这样一个修改，将<code>need_expend</code>拿到方法里取重新获取。</p><p>这些节点刚开始的时候没有值，那我们给它一个初始值，我这里的值都是随意给的。</p><p>这样，就不仅把最外沿的节点给找出来了，而且还把值给他送进去了，相对来说就会更简单一些。所有定义出来的节点，我们都可以把它变成图关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">topologic(convert_feed_dict_to_graph(feed_dict))<br><br>---<br>[Node:k , Node:y , Node:b , Node:x , Node:linear , Node:sigmoid , Node:loss ]<br></code></pre></td></tr></table></figure><p>咱们现在再定一个点，我们用一个变量存起来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sorted_nodes = topologic(convert_feed_dict_to_graph(feed_dict))<br></code></pre></td></tr></table></figure><p>那么咱们现在来模拟一下它的计算过程，模拟神经网络的计算过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fowward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;I am &#123;&#125;, I calculate myself value!!!&#x27;</span>.<span class="hljs-built_in">format</span>(self.name))<br><br><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.forward()<br><br>---<br>I am y, I calculate myself value!!!<br>I am x, I calculate myself value!!!<br>I am b, I calculate myself value!!!<br>I am k, I calculate myself value!!!<br>I am linear, I calculate myself value!!!<br>I am sigmoid, I calculate myself value!!!<br>I am loss, I calculate myself value!!!<br></code></pre></td></tr></table></figure><p>我们在<code>Node</code>里定义了一个方法<code>forward</code>，从前往后运算，这个时候我们在每个里面加一个向前运算。</p><p>这个就是拓朴排序的作用，经过排序之后，那需要在后面计算的节点，就一定会放在后面再进行计算。</p><p>好，那我们现在需要区分两个内容，一个是被赋值的内容，一个是需要计算的内容。</p><p>刚才我们说过，在大厂的这些地方，<code>x,y,k,b</code>这种东西都被定义为占位符，那我们来修改一下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inputs=[], name=<span class="hljs-literal">None</span></span>):<br>        ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;I am &#123;&#125;, 我需要自己计算自己的值。&#x27;</span>.<span class="hljs-built_in">format</span>(self.name))<br>    ...<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Placeholder</span>(<span class="hljs-title class_ inherited__">Node</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, name=<span class="hljs-literal">None</span></span>):<br>        Node.__init__(self, name = name)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;I am &#123;&#125;, 我已经被人为赋值了。&#x27;</span>.<span class="hljs-built_in">format</span>(self.name))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;Node:&#123;&#125; &#x27;</span>.<span class="hljs-built_in">format</span>(self.name)<br><br><span class="hljs-comment">### version - 02</span><br>node_x = Placeholder(name=<span class="hljs-string">&#x27;x&#x27;</span>)<br>node_y = Placeholder(name=<span class="hljs-string">&#x27;y&#x27;</span>)<br>node_k = Placeholder(name=<span class="hljs-string">&#x27;k&#x27;</span>)<br>node_b = Placeholder(name=<span class="hljs-string">&#x27;b&#x27;</span>)<br>node_linear = Node(inputs=[node_x, node_k, node_b], name=<span class="hljs-string">&#x27;linear&#x27;</span>)<br>node_sigmoid = Node(inputs=[node_linear], name=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>node_loss = Node(inputs=[node_sigmoid, node_y], name=<span class="hljs-string">&#x27;loss&#x27;</span>)<br></code></pre></td></tr></table></figure><p>我们创建了一个 Placeholder 类，继承了 Node,然后我们取修改初始化方法，它是是没有 input 的，只有一个 name。</p><p>然后 forward 我们改一下，改成打印已经被赋值的语句。父类 Node 里的forward 也改一下，改成需要自己计算自己的值。</p><p>那我们这个时候将赋值的四个节点改成调用 Placeholder。</p><p>接下来，我们需要修改<code>convert_feed_dict_to_graph</code>方法了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_feed_dict_to_graph</span>(<span class="hljs-params">feed_dict</span>):<br>    ...<br>    <span class="hljs-keyword">while</span> need_expend:<br>        ...<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(n, Placeholder): n.value = feed_dict[n]<br>        ...<br>    ...<br></code></pre></td></tr></table></figure><p>我们来检查这个节点是否是 Placeholder，如果是的话，将当前的 feed_dict赋值给 n.value。来看下结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> sorted_nodes:<br>    node.forward()<br><br>---<br>I am b, 我已经被人为赋值了。<br>I am x, 我已经被人为赋值了。<br>I am k, 我已经被人为赋值了。<br>I am y, 我已经被人为赋值了。<br>I am linear, 我需要自己计算自己的值。<br>I am sigmoid, 我需要自己计算自己的值。<br>I am loss, 我需要自己计算自己的值。<br></code></pre></td></tr></table></figure><p>好，到现在为止，咱们只是打了一段文字，问题是对于<code>linear, sigmoid</code>和<code>loss</code>,到底是怎么计算的呢？</p><p>这个问题，咱们放到下一节课里面去讲，现在咱们这篇文章已经超标了，目测应该超过万字了吧。</p><p>好，下节课记得来看咱们具体如何在实现拓朴排序后将计算加进去。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202310311950753.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;上节课，我们讲了多层神经网络的原理，并且明白了，数据量是层级无法超过
3 层的主要原因。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>20. 深度学习 - 多层神经网络</title>
    <link href="https://hivan.me/20.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://hivan.me/20.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2023-11-12T23:30:00.000Z</published>
    <updated>2023-11-25T04:40:28.619Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310694.png"alt="Alt text" /></p><p>Hi，你好。我是茶桁。</p><p>之前两节课的内容，我们讲了一下相关性、显著特征、机器学习是什么，KNN模型以及随机迭代的方式取获取 K 和 B，然后定义了一个损失函数（loss函数），然后我们进行梯度下降。</p><span id="more"></span><p>可以说是又帮大家回顾了一下深度学习的相关知识，但是由于要保证整个内容的连续性，所以这也没办法。</p><p>那么接下来的课程里，咱们要来看一下神经网络，怎么样去拟合更加复杂的函数，什么是激活函数，什么是神经网络，什么是深度学习。</p><p>然后我们还要来学习一下反向传播，以及如何实现自动的反向传播，什么是错误排序以及怎么样自动的去计算元素的gradients。梯度怎么样自动求导。</p><p>从简单的线性回归函数到复杂的神经网络，从人工实现的求导到自动求导。那我们现在来跟大家一起来看一下。</p><p>上一节课结束的时候我们说过，现实生活中绝大多数事情的关系都不是线性的。</p><p>比方说，我工作的特别努力，然后就可以升职加薪了。但是其实有可能工作的努力程度和升职加薪程度之间的关系可能并不是一条直线的函数关系。</p><p>可能一开始不管怎么努力，薪水都没有什么大的变化，可是忽然有了一个机会，薪水涨的幅度很大，但是似乎没怎么努力。再之后，又趋于一条平行横轴的线，不管怎么努力都无法往上有提升。这是不是咱们这些社畜的真实写照？</p><p>在现实生活中有挺多这样的问题，这样的对应关系。比如艾宾浩斯曲线，再比如细菌生长曲线，很多很多。</p><p>经过刚刚的分析我们知道了除了线性函数(kx+b)，还有一种常见的函数关系式，是一种 s 型的一种函数，这种 s形的函数在我们整个计算机科学里我们称呼它为<code>sigmoid</code>函数：</p><p><span class="math display">\[\begin{align*}Sigmoid: f(x) = \sigma (x) = \frac{1}{1+e^{-x}}\end{align*}\]</span></p><p>这是一个非常常见的函数。我们可以节用 NumPy库来用代码将它实现出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br></code></pre></td></tr></table></figure><p>把它的图像描述出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">sub_x = np.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br>plt.plot(sub_x, sigmoid(sub_x))<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310696.png"alt="Alt text" /></p><p>然后我们来利用一下这个函数，我们定义一个随机线性函数，然后和 sigmoid函数一起应用画 5 根不同的线：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_linear</span>(<span class="hljs-params">x</span>):<br>    k, b = random.random(), random.random()<br>    <span class="hljs-keyword">return</span> k*x + b<br><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    plt.plot(sub_x, random_linear(sigmoid(random_linear(sub_x))))    <br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310697.png"alt="Alt text" /></p><p>这个里面起变化的就是 k 和 b 这两个参数，那我们来调节 k 和 b的画，就可以变化这条曲线的样式。</p><p>除了以上这些函数，我们生活中还会遇到更复杂的函数，甚至很有可能是一个复杂的三维图像。</p><p>那这个时候，我们该如何去拟合这么多复杂的函数呢？</p><p>一个比较直接的方法，当然就是我们人为的去观察，比如这个类似于sin(x):</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310698.png"alt="Alt text" /></p><p>还有这个 k*sin(kx+b)+b:</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310699.png"alt="Alt text" /></p><p>当然，这样理论上是可以的，每当我们遇到一个场景之后就自己去提出来一个函数模型，然后把函数模型让机器去拟合出来。</p><p>但是这样就会有一个问题，大家就会发现假如你是那个工作者，那你熊猫眼会很严重，因为我们要看到很多这样的场景。现实生活中的问题实在太多了，每一天我们都可能会遇到新的函数。</p><p>如果我们每观察一个情况就要去考察，去思考它的这个函数模型是什么，你就会发现你的工作量无穷无尽。而且你会发现一个问题：现在函数能够可视化的，但是如果在现实生活中有很多场景的函数是无法可视化的。</p><p>那么这个时候我们就需要其他的一些方法能够拟合更加复杂的函数，这个时候我们怎么样不通过去观察它就能够拟合出复杂的函数呢？其实很简单。</p><p>有一个老头子叫做 Hinton，他是 2018 年图灵奖的获得者。</p><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310700.png"alt="Alt text" /></p><p>他在一九八几年的时候发了一篇文章，经过他多年的研究，发现人的脑能够做出非常复杂的一些行为，其实我们这个神经元的类型都是很有限的，并没有很多奇怪的东西，就是有很多不同的节点，其实就那么几种。人类就能够进行复杂行为，背后其实就是一些基本的神经元的一些组合。</p><p>只不过这些基本的在组合还有一种形式，就是输入进来的会经过一个叫做activateneurons，就是激活单元，去做一个非线性变化。然后经过不断的这种非线性变化，最后就拟合出来非常复杂的信号。</p><p>那非线性变化的这些函数其实都是一样的，就他们背后的逻辑都是一样。只不过有的时候非线性变化的多，有的时候非线性变化的少。</p><p>讲了这么多不直观的东西，我们来看点实际的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    i = random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sub_x)))<br>    output_1 = np.concatenate((random_linear(sub_x[:i]), random_linear(sub_x[i:])))<br>    plt.plot(sub_x, output_1)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310701.png"alt="Alt text" /></p><p>然后我们来做两件事，第一个是将 k,b随机方式改成<code>normalvariate()</code>，第二个在上面的基础上再做一次拆分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_linear</span>(<span class="hljs-params">x</span>):<br>    k, b = random.normalvariate(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>), random.normalvariate(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> k * x + b<br><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    i = random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sub_x)))<br>    linear_output = np.concatenate((random_linear(sub_x[:i]), random_linear(sub_x[i:])))<br>    i_2 = random.choice(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(linear_output)))<br>    output = np.concatenate((sigmoid(linear_output[:i_2]), sigmoid(linear_output[i_2:])))<br><br>    plt.plot(sub_x, output)   <br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310702.png"alt="Alt text" /></p><p>我们来看，这个时候你会发现他生成的这个图像比较奇怪。它生成了很多奇怪的函数，每一次根据不同的参数就形成了不同的函数图像。</p><p>迄今为止就两个函数，一个 sigmoid，一个 randomlinear，但是它生成了很多奇怪的函数。</p><p>面对这种函数，这么多层出不穷列举都列举不完的函数，我们怎么样能够每次遇到一个问题就得去提出它这个函数到底是什么样。而且关键是有可能函数维度高了之后都观察不到它是什么关系。</p><p>所以我们就会去考虑，怎么样能够让机器自动的去拟合出来更复杂的函数呢？</p><p>我们可以用基本模块经过组合拼接，然后就能够形成复杂函数。而在组合拼接的过程中，我们只需要让机器自动的去把K1、K2、B1、B2 等等这些参数给它拟合出来就行。</p><p>也就是说我们可以通过参数的变化来拟合出来各种各样的函数。</p><p>这其实就是深度神经网络的一个核心思想。就是用基本的模块像大家玩积木一样，并不会有很多积木类型给你，只有一些基本的东西，但是通过这些基本的可以造出来特别多复杂的东西。</p><p>这个就是背后的原理，通过函数的复合和叠加。这种变化的引起都是由一个线性函数加上一个非线性函数。</p><p>其实很大程度上由我们大脑里边这种简单东西可以构成复杂东西得到了启示。只不过人的大脑里边，在脑神经科学里面把这种非线性变化呢叫做activate neurons，叫做激活神经元。在程序里，我们把这种非线性函数叫activation function，激活函数。</p><p>激活函数的作用就是为了让我们的程序能够拟合非线性关系。如果没有激活函数，咱们的程序永远只能拟合最简单的线性关系，而现实生活中绝大多数关系并不是并不是线性关系。</p><p>让机器来拟合更加复杂函数的这种方法，和我们的神经网络很像，就是咱们现在做的这个事情，我们就把它命了个名叫做神经网络。</p><p>早些年的时候科学家们有一个理论，人们把一组线性和非线性变化叫做一层。在以前的时候科学家们发现这个层数不能多于三层，就是神经网络的层数不能多于三层。</p><p>为什么不能多于 3层？其实最主要的不是计算量太大的问题，最核心的原因是什么？</p><p>假设我们有一个 f(x) 和一个 x组成一个平面坐标系，在其中有无数的点，当我们在做拟合的时候，发现了一条直线可以拟合，但是实际上呢，当我们将数据量继续放大的时候，才发现我们的拟合的直线偏离的非常厉害。</p><p>我们之前在机器学习的课程里说过，我们要有高精度，就需要有足够的数据量。如果这个时候变成一个三维问题，就需要更多的数据量。没有更多的数据的话，就好比有一个平板在空中，它会摇来摇去，你以为拟合了一个正确的平板，但其实完全不对。</p><p>那这个时候，每当我们所需要拟合的参数多一个，多少数据量认为是足够的？这个不一定。这个和整个问题的复杂程度有关系。</p><p>后来科学家们发现一个规律，在相似的问题下，我们需要拟合的参数多一个，需要的数据就要多一个数量级。</p><p>当变成三层的时候，会发现参数就更多了，而参数变得特别多就会需要特别多的数据量。而早在一九八几年、一九九几年的时候并没有那么多的数据量，就会产生数据量不够的情况，所以模型在现实生活中没法用。</p><p>但是随着到二零零几年，再到二零一几年之后，产生了大量的数据。就给我们做函数拟合提供了数据资源，所以数据量是最重要的，数据量决定了这个东西能不能做。而其他的一些，比方说计算、GPU啊等等，它是加速这个过程的，是让它更方便。</p><p>那么后来我们把层数超过 3层的就叫深度神经网络，机器学习就简称深度学习。这是为什么深度学习在二零一几年的时候才开始火起来。</p><p>那现在我们把上一节课的这个问题再拿过来，现在来想想，如果我们把房价的函数关系也写成类似的，linear和 sigmoid 之间的关系，那会怎么样呢？</p><p>首先，我们的 k 和 b 就会多加一组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_times):<br>    k1 = k1 + (-<span class="hljs-number">1</span>) * loss对k1的偏导<br>    b1 = b1 + (-<span class="hljs-number">1</span>) * loss对b1的偏导<br>    k2 = k2 + (-<span class="hljs-number">1</span>) * loss对k2的偏导<br>    b2 = b2 + (-<span class="hljs-number">1</span>) * loss对b2的偏导<br><br>    loss_ = loss(y, model(X_rm, k1, b1, k2, b2))<br>    ...<br></code></pre></td></tr></table></figure><p>然后我们的 model也会多接受了一组参数，并且我们要将其内部函数关系做一个叠加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model</span>(<span class="hljs-params">x, k1, k2, b1, b2</span>):<br>    linear1_output = k1 * x + b1<br>    sigmoid_output = sigmoid(linear1_output)<br>    linear2_output = k2 * sigmoid_output + b2<br><br>    <span class="hljs-keyword">return</span> linear2_output<br></code></pre></td></tr></table></figure><p>在这个时候我们要求解的时候就会发现有个问题，我们现在的 loss是这样的：</p><p><span class="math display">\[\begin{align*}loss &amp; = \frac{1}{N}(y_i - \hat y)^2 \\&amp; = \frac{1}{N} \begin{bmatrix}y_1 - (k_2 \frac{1}{1+e^{-(k_1x+b_1)}}+b_2)\end{bmatrix} ^2\end{align*}\]</span></p><p>似乎变得有点过于复杂。当前情况下，我们是可以复杂的去求导，但是当函数继续复杂下去的时候，怎么把这个导数求出来呢？</p><p>函数还可以继续叠加，层数还可以写的越来越多。那么怎么样才能给它求出它的导数呢？</p><p>我们再将上面的式子做个变化：</p><p><span class="math display">\[\frac{1}{N}[l_2(\sigma(l_1(x))) -y_1]^2\]</span></p><p>这样我们就可以将问题进行简化，我们上面代码里<code>loss对k1的偏导</code>就可以写成：</p><p><span class="math display">\[\frac{\partial loss}{\partial  l_2} \cdot \frac{\partial l_2}{\partial\sigma} \cdot \frac{\partial \sigma}{\partial l_1} \cdot \frac{\partiall_1}{\partial k_1}\]</span></p><p>同理，<code>loss对b1的偏导</code>就是：</p><p><span class="math display">\[\frac{\partial loss}{\partial  l_2} \cdot \frac{\partial l_2}{\partial\sigma} \cdot \frac{\partial \sigma}{\partial l_1} \cdot \frac{\partiall_1}{\partial b_1}\]</span></p><p>这个时候，问题就变成一个可解决的了。</p><p><span class="math inline">\(\frac{\partial loss}{\partiall_2}\)</span>其实就等于<span class="math inline">\(\frac{2}{N}(l_2 -y_1)\)</span>。</p><p>我们继续往后看第二部分，那么这个时候我们可以得到<spanclass="math inline">\(l_2 = k_2 \cdot \sigma + b_2\)</span>，那<spanclass="math inline">\(\frac{\partial l_2}{\partial\sigma}\)</span>就等于<span class="math inline">\(k_2\)</span>。</p><p>再来看第三部分，<span class="math inline">\(\sigma&#39;(x) =\sigma(x) \cdot (1- \sigma(x)\)</span>, 所以<spanclass="math inline">\(\frac{\partial \sigma}{\partial l_1} = \sigma\cdot (1 - \sigma)\)</span>。</p><p>最后第四部分，<span class="math inline">\(\frac{\partiall_1}{\partial k_1} = x\)</span>。</p><p>这样，我们整个式子就应该变成这样：</p><p><span class="math display">\[\begin{align*}\frac{2}{N}(l_2 - y_1) \cdot k_2 \cdot \sigma \cdot (1 - \sigma) \cdot x\end{align*}\]</span></p><p>这样的话，我们就把 loss 对于 K1的偏导就求出来了，这里算是一个突破。本来看起来是很复杂的的一个问题，我们将其拆分成了这样的一种形式。那这种形式，我们把它称作「链式求导」。</p><p>但是现在其实还有个问题，这整个一串链式求导的东西是我们通过眼睛求出来的，但是现在怎么样让机器自动的把这一串东西写出来？就是机器怎么知道是这些数字乘到一起？</p><p>换句话说，我们现在把这个问题再形式化一下，定义一个问题。</p><p>给定一个模型定义，这个模型里边包含参数<code>&#123;k1, k2, b1, b2&#125;</code>，我们要构建一个程序，让它能够求解出k1,k2,b1,b2 的偏导是多少。</p><p>如果我们想解决这个问题，我们首先要思考一下，<spanclass="math inline">\(k_1, k_2, b_1, b_2, l_1, l_2, \sigma, y_{true},loss\)</span>, 它们之间是一种什么样的关系。</p><p>观察一下我们会发现它们之间的关系是这样的：</p><p><span class="math display">\[\begin{align*}&amp; \{k_1, b_1, x\} \to l_1 \to \sigma, \\&amp; \{\sigma, k_2, b_2\} \to l_2, \\&amp; \{l_2, y_{true}\} \to loss \\&amp; \to 表示的是&#39;输出到&#39;的关系。\end{align*}\]</span></p><p>要用计算机去表示这种关系，是典型的一个数据结构问题，怎么样让计算机合理的去存储它，你会发现这个是一个图案。</p><p>这种节点和节点之间通过关系连接起来的就把它叫做图，我们把它先表示成图的样子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">computing_graph = &#123;<br>    <span class="hljs-string">&#x27;k1&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;b1&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;x&#x27;</span>: [<span class="hljs-string">&#x27;L1&#x27;</span>],<br>    <span class="hljs-string">&#x27;L1&#x27;</span>:[<span class="hljs-string">&#x27;sigmoid&#x27;</span>],<br>    <span class="hljs-string">&#x27;sigmoid&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;k2&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;b2&#x27;</span>: [<span class="hljs-string">&#x27;L2&#x27;</span>],<br>    <span class="hljs-string">&#x27;L2&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>],<br>    <span class="hljs-string">&#x27;y&#x27;</span>: [<span class="hljs-string">&#x27;loss&#x27;</span>]<br>&#125;<br><br>nx.draw(nx.DiGraph(computing_graph), with_labels = <span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p><imgsrc="https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310703.png"alt="Alt text" /></p><p>这个就是我们要表达的一个关系，我们把这个变成图。</p><p>现在我们将给定的一个model，这样一个函数变成了这样一张图。计算机里有现成的各种各样的图算法，我们就可以来计算这个图之间的关系了。</p><p>现在我们就要根据这个图的表示来思考我们如何求 loss 对 K1的偏导。那其实，我们可以发现 k1 在末尾出，一直在向前输入直到loss。换句话说，我们可以通过 k1一直往图的终点去寻找来找到它求导的这个过程。</p><p>也就是说，只要我们的能把模型变成一个图，然后我们就可以根据这些点去找到它们之间节点的对应关系，我们就可以通过这个节点关系来获得它的求导过程了。</p><p>那下一节课呢，我们就继续来看一下，如何将这个图的关系，变成一个自动求导的过程。</p><hr /><p>关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/hivandu/notes/img/扫码_搜索联合传播样式-白色版.png"alt="坍缩的奇点" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://raw.githubusercontent.com/hivandu/notes/main/img/202310310310694.png&quot;
alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Hi，你好。我是茶桁。&lt;/p&gt;
&lt;p&gt;之前两节课的内容，我们讲了一下相关性、显著特征、机器学习是什么，KNN
模型以及随机迭代的方式取获取 K 和 B，然后定义了一个损失函数（loss
函数），然后我们进行梯度下降。&lt;/p&gt;</summary>
    
    
    
    <category term="AI秘籍" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/"/>
    
    <category term="核心能力基础" scheme="https://hivan.me/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="AI" scheme="https://hivan.me/tags/AI/"/>
    
  </entry>
  
</feed>
