<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>09 使用Embedding实现语义检索 - 茶桁.MAMT</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="茶桁.MAMT"><meta name="msapplication-TileImage" content="https://qiniu.hivan.me/picGo/20230601174411.png?imgNote"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="茶桁.MAMT"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Hi，我是茶桁。 过去的8讲，你已熟悉Embedding和Completion接口。Embedding适合用于机器学习中的分类、聚类等传统场景。Completion接口可以用作聊天机器人，也可以用于文案撰写、文本摘要、机器翻译等工作。"><meta property="og:type" content="blog"><meta property="og:title" content="09 使用Embedding实现语义检索"><meta property="og:url" content="https://hivan.me/Implementing-semantic-retrieval-using-Embedding/"><meta property="og:site_name" content="茶桁.MAMT"><meta property="og:description" content="Hi，我是茶桁。 过去的8讲，你已熟悉Embedding和Completion接口。Embedding适合用于机器学习中的分类、聚类等传统场景。Completion接口可以用作聊天机器人，也可以用于文案撰写、文本摘要、机器翻译等工作。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hivan.me/Implementing-semantic-retrieval-using-Embedding/20230601170153.png"><meta property="article:published_time" content="2023-05-16T09:01:08.000Z"><meta property="article:modified_time" content="2023-06-01T13:21:04.307Z"><meta property="article:author" content="Hivan Du"><meta property="article:tag" content="AI"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://hivan.me/Implementing-semantic-retrieval-using-Embedding/20230601170153.png"><meta property="twitter:creator" content="@hivan"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hivan.me/Implementing-semantic-retrieval-using-Embedding/"},"headline":"09 使用Embedding实现语义检索","image":["https://hivan.me/Implementing-semantic-retrieval-using-Embedding/20230601170153.png"],"datePublished":"2023-05-16T09:01:08.000Z","dateModified":"2023-06-01T13:21:04.307Z","author":{"@type":"Person","name":"Hivan Du"},"publisher":{"@type":"Organization","name":"茶桁.MAMT","logo":{"@type":"ImageObject","url":"https://hivan.me/img/logo.svg"}},"description":"Hi，我是茶桁。 过去的8讲，你已熟悉Embedding和Completion接口。Embedding适合用于机器学习中的分类、聚类等传统场景。Completion接口可以用作聊天机器人，也可以用于文案撰写、文本摘要、机器翻译等工作。"}</script><link rel="canonical" href="https://hivan.me/Implementing-semantic-retrieval-using-Embedding/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="茶桁.MAMT" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-05-16T09:01:08.000Z" title="5/16/2023, 5:01:08 PM">2023-05-16</time>发表</span><span class="level-item"><a class="link-muted" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a></span></div></div><h1 class="title is-3 is-size-4-mobile">09 使用Embedding实现语义检索</h1><div class="content"><p>Hi，我是茶桁。</p>
<p>过去的8讲，你已熟悉Embedding和Completion接口。Embedding适合用于机器学习中的分类、聚类等传统场景。Completion接口可以用作聊天机器人，也可以用于文案撰写、文本摘要、机器翻译等工作。</p>
<span id="more"></span>
<p>然而，很多同学可能认为这与他们的日常工作无关。实际上，我们通常在搜索、广告、推荐等业务中使用自然语言处理技术。因此，我们今天来看看如何使用OpenAI的接口来帮助这些需求。</p>
<p>当涉及到优化搜索结果时，OpenAI的Embedding接口可以提供有价值的功能。Embedding接口能够将文本转换为表示其语义特征的向量，这些向量可以用于比较文本之间的相似性，从而优化搜索结果的排序和相关性。</p>
<p>首先，使用OpenAI的嵌入接口，您可以将搜索查询和搜索结果中的文本转换为嵌入向量。通过比较查询向量与结果向量之间的相似度，您可以重新排列搜索结果，以提供更相关和有用的结果。这可以帮助用户更快地找到他们想要的信息，并提供更好的搜索体验。</p>
<p>其次，OpenAI的嵌入接口还可以帮助您改进搜索结果的相关性。通过将用户的上下文和历史记录与搜索查询结合起来，您可以生成更具个性化和定制化的搜索结果。使用嵌入接口，您可以将用户的上下文信息转换为嵌入向量，并与查询向量进行比较，以确定最相关的结果，并在搜索结果中突出显示这些个性化的内容。</p>
<p>此外，OpenAI的嵌入接口还可以用于相似性搜索和聚类分析。您可以使用嵌入向量来比较不同文本之间的相似性，并将相似的文本聚集在一起。这有助于在搜索结果中提供更多相关的选项，并帮助用户发现相关但可能未被明确搜索的内容。</p>
<p>下面，就让我们来一步步的实现：</p>
<h2 id="生成实验数据">生成实验数据</h2>
<p>在演示代码之前，我们需要实验数据。我们通常要在网上找数据集或使用软件包的数据集。但并非总是能找到合适的数据集。这时，我们可以借助 AI 生成数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai, os</span><br><span class="line"></span><br><span class="line">openai.api_key = <span class="string">&quot;YOUR_API_KEY&quot;</span></span><br><span class="line">COMPLETION_MODEL = <span class="string">&#x27;text-davinci-003&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_data_by_prompt</span>(<span class="params">prompt</span>):</span><br><span class="line">    response = openai.Completion.create(</span><br><span class="line">        engine = COMPLETION_MODEL,</span><br><span class="line">        prompt = prompt,</span><br><span class="line">        temperature = <span class="number">0.5</span>,</span><br><span class="line">        max_tokens = <span class="number">2048</span>, <span class="comment"># 根据需要设置生成的标题长度</span></span><br><span class="line">        top_p = <span class="number">1</span>,</span><br><span class="line">        stop=<span class="literal">None</span>,  <span class="comment"># 可选，用于指定生成标题的终止标记</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> response.choices[<span class="number">0</span>].text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义提示词</span></span><br><span class="line">prompt = <span class="string">&quot;请你生成50条亚马逊里的商品的标题，每条在30个字左右，品类是心理类的书籍，标题里往往也会有一些促销类的信息，每行一条。&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行定义类, data接收原始数据</span></span><br><span class="line">data = generate_data_by_prompt(prompt)</span><br></pre></td></tr></table></figure>
<p>为了更贴近实际情况，我们可以精心设计提示语。例如，明确商品为亚马逊产品，品类为心理书籍，并在标题中包含促销信息。将返回结果按行分割并加载到DataFrame中，以查看结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">product_names = data.strip().split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;product_name&#x27;</span>: product_names&#125;)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p>数据结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">			product_name</span><br><span class="line"><span class="number">0</span>	<span class="number">1.</span> 【特价】心理学家的智慧：拥抱接受自己</span><br><span class="line"><span class="number">1</span>	<span class="number">2.</span> 【免费配送】心理学的本质：探索你的内在</span><br><span class="line"><span class="number">2</span>	<span class="number">3.</span> 【限时特价】心理学的洞察力：提升你的自信</span><br><span class="line"><span class="number">3</span>	<span class="number">4.</span> 【<span class="number">2</span>件<span class="number">8</span>折】心理学的力量：走出你的舒适区</span><br><span class="line"><span class="number">4</span>	<span class="number">5.</span> 【全场满减】心理学的技巧：让你的思维更灵活</span><br></pre></td></tr></table></figure>
<p>AI 为我们生成了 50 条商品信息，每一条都带有促销相关的标签。我们需要去掉每一行的标号，以得到干净的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.product_name = df.product_name.apply(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">1</span>].strip())</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">	 product_name</span><br><span class="line"><span class="number">0</span>	【特价】心理学家的智慧：拥抱接受自己</span><br><span class="line"><span class="number">1</span>	【免费配送】心理学的本质：探索你的内在</span><br><span class="line"><span class="number">2</span>	【限时特价】心理学的洞察力：提升你的自信</span><br><span class="line"><span class="number">3</span>	【<span class="number">2</span>件<span class="number">8</span>折】心理学的力量：走出你的舒适区</span><br><span class="line"><span class="number">4</span>	【全场满减】心理学的技巧：让你的思维更灵活</span><br></pre></td></tr></table></figure>
<p>我们可以生成一些3C电子商品，涵盖不同的品类，以便在后面展示搜索效果时更加方便。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">electronic_prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">请你生成50条亚马逊里的商品的标题，每条在30个字左右，品类是3C类电子商品，标题里往往也会有一些促销类的信息，每行一条。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">electronic_data = generate_data_by_prompt(electronic_prompt)</span><br><span class="line">electronic_product_names = electronic_data.strip().split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">electronic_df = pd.DataFrame(&#123;<span class="string">&#x27;product_name&#x27;</span>: electronic_product_names&#125;)</span><br><span class="line">electronic_df.product_name = electronic_df.product_name.apply(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">1</span>].strip())</span><br><span class="line">electronic_df.head()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这一步我们将获取数据和处理写在一起执行</p>
</blockquote>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">	product_name</span><br><span class="line"><span class="number">0</span>	Apple MacBook Pro <span class="number">16</span><span class="string">&quot; 苹果笔记本电脑，限时优惠！</span></span><br><span class="line"><span class="string">1	新款 iPad Pro 11&quot;</span> 平板电脑，4G/WiFi，超薄设计！</span><br><span class="line"><span class="number">2</span>	无线鼠标，蓝牙连接，超长续航，低至半价！</span><br><span class="line"><span class="number">3</span>	三星 Galaxy S20 5G 智能手机，性能升级，极速体验！</span><br><span class="line"><span class="number">4</span>	小米 Redmi Note <span class="number">8</span> Pro 智能手机，<span class="number">4800</span>万超清拍照！</span><br></pre></td></tr></table></figure>
<p>拼接这两个 DataFrame 后，即可用于搜索实验的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = pd.concat([df, electronic_df], axis = <span class="number">0</span>)</span><br><span class="line">df = df.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">display(df)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">		product_name</span><br><span class="line"><span class="number">0</span>	【特价】心理学家的智慧：拥抱接受自己</span><br><span class="line"><span class="number">1</span>	【免费配送】心理学的本质：探索你的内在</span><br><span class="line"><span class="number">2</span>	【限时特价】心理学的洞察力：提升你的自信</span><br><span class="line"><span class="number">3</span>	【<span class="number">2</span>件<span class="number">8</span>折】心理学的力量：走出你的舒适区</span><br><span class="line"><span class="number">4</span>	【全场满减】心理学的技巧：让你的思维更灵活</span><br><span class="line">...	...</span><br><span class="line"><span class="number">73</span>	小米米家电动滑板车，超长续航，超薄设计！</span><br><span class="line"><span class="number">74</span>	小米米家电动折叠车，智能控制，超薄设计，低至半价！</span><br><span class="line"><span class="number">75</span>	小米米家空气净化器Pro，智能检测，超长续航！</span><br><span class="line"><span class="number">76</span>	小米米家净水器，三级净水，智能检测，限时优惠！</span><br><span class="line"><span class="number">77</span>	三星 Galaxy S10 智能手机，双摄像头，性能升级！</span><br><span class="line"><span class="number">78</span> rows × <span class="number">1</span> columns</span><br></pre></td></tr></table></figure>
<p>合并后的数据量不到100条，说明AI返回的条数不到50条。这并不影响我们使用这个数据源。如果需要，可以再次获取数据并进行合并。</p>
<h3 id="通过-embedding-进行语义搜索">通过 Embedding 进行语义搜索</h3>
<p>对于搜索问题，我们可以利用 GPT 模型。大公司如百度、阿里有许多内部复杂的策略和模型。但是许多中小型公司，尤其是刚开始提供搜索功能的公司，则通常使用 Elasticsearch 这个开源项目。Elasticsearch 背后的搜索原理是先分词，然后使用倒排索引。</p>
<p>简单来说，搜索引擎将商品名称（例如“心理学的洞察力：提升你的自信”）拆分成多个单词（如“心理学”、“洞察力”、“提升”、“自信”）。每个标题都是这样切分。然后，建立一个索引，比如“洞察力”这个词，出现过的标题的编号，都按编号顺序跟在“心理”后面。其他的词也是类似。</p>
<p>当用户搜索时，搜索引擎将用户输入的关键词（例如“思维的力量”）拆分为多个单词（如“思维”和“力量”）。然后，搜索引擎查找包含这些单词的标题，并根据出现的单词的数量、权重等找出相关商品。</p>
<p>但是，此策略有缺点。如果有同义词，这么简单地搜索是搜不到的。例如，如果搜索“心理学智慧”，虽然语义上很接近，但是因为“心理”、“智慧”这两个词在标题里都没有出现，所以就无法匹配。为了提升搜索效果，需要做更多的工程研发工作，例如找一个同义词表，将标题中出现的同义词也算上等等。</p>
<p>然而，使用 OpenAI 的 Embedding 接口，可以将一段文本的语义表示为一个向量。向量之间是可以计算距离的，这是在之前的情感分析的零样本分类中演示过的。因此，我们可以将用户的搜索通过 Embedding 接口变为向量。然后，计算它和所有商品标题的余弦距离，找出离我们搜索词最近的几个向量。这些向量实际上是与该商品类似的语义，不一定需要相同的关键词。</p>
<p>根据这个思路，我们可以使用代码来尝试一下。首先，我们需要计算所有商品标题的 Embedding，并将其存储下来。这里的代码与之前使用 Embedding 进行分类和聚类的代码基本相同，不再详细解释。我们使用 backoff 和 batch 处理，以便代码容错并快速处理这些商品标题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> openai.embeddings_utils <span class="keyword">import</span> get_embeddings</span><br><span class="line"><span class="keyword">import</span> openai, os, backoff</span><br><span class="line">embedding_model = <span class="string">&#x27;text-embedding-ada-002&#x27;</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@backoff.on_exception(<span class="params">backoff.expo, openai.error.RateLimitError</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_embeddings_with_backoff</span>(<span class="params">prompts, engine</span>):</span><br><span class="line">    embeddings = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(prompts), batch_size):</span><br><span class="line">        batch = prompts[i:i+batch_size]</span><br><span class="line">        embeddings += get_embeddings(list_of_text = batch,  engine=engine)</span><br><span class="line">    <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line">prompts = df.product_name.tolist()</span><br><span class="line">prompt_batches = [prompts[i:i+batch_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(prompts), batch_size)]</span><br><span class="line"></span><br><span class="line">embeddings = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> prompt_batches:</span><br><span class="line">    batch_embeddings = get_embeddings_with_backoff(prompts = batch, engine = embedding_model)</span><br><span class="line">    embeddings += batch_embeddings</span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;embedding&#x27;</span>] = embeddings</span><br><span class="line">df.to_parquet(<span class="string">&#x27;./data/taobao_product_title.parquet&#x27;</span>, index = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>为了定义一个 search_product 的搜索函数，我们可以接受三个参数：一个名为 df 的数据源，一个名为 query 的搜索词，以及一个名为 n 的参数，表示要返回多少条搜索结果。该函数将执行以下三项操作：</p>
<ol type="1">
<li><p>使用 OpenAI API 将搜索词转换为 Embedding。</p></li>
<li><p>计算该 Embedding 与 DataFrame 中每个 Embedding 的余弦距离。</p></li>
<li><p>根据余弦相似度对搜索结果进行排序，并返回与搜索词最相似的 n 个标题。</p></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> openai.embeddings_utils <span class="keyword">import</span> get_embedding, cosine_similarity</span><br><span class="line"></span><br><span class="line"><span class="comment"># search through the reviews for a specific product</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search_product</span>(<span class="params">df, query, n = <span class="number">3</span>, pprint = <span class="literal">True</span></span>):</span><br><span class="line">    product_embedding = get_embedding(</span><br><span class="line">        query,</span><br><span class="line">        engine = embedding_model</span><br><span class="line">    )</span><br><span class="line">    df[<span class="string">&quot;similarity&quot;</span>] = df.embedding.apply(<span class="keyword">lambda</span> x: cosine_similarity(x, product_embedding))</span><br><span class="line"></span><br><span class="line">    results = (</span><br><span class="line">        df.sort_values(<span class="string">&quot;similarity&quot;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">        .head(n)</span><br><span class="line">        .product_name</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> pprint:</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> results:</span><br><span class="line">            <span class="built_in">print</span>(r)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">results = search_product(df, <span class="string">&quot;心理学智慧&quot;</span>, n = <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>我们就拿刚才举的那个例子，使用"心理学智慧"作为搜索词，调用这个 search_product 函数，然后拿前 3 个返回结果。可以看到，尽管在关键词上完全不同，但是返回的结果里，的确包含了"心理学的洞察力：提升你的自信"这个商品。</p>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">【新品特惠】心理学的智慧：改变你的生活</span><br><span class="line">【新品特惠】心理学的洞察力：提升你的自信</span><br><span class="line">【满减特惠】心理学的智慧：让你的生活更完美</span><br></pre></td></tr></table></figure>
<blockquote>
<p>请注意，由于我们的商品标题是随机生成的，因此您得到的数据集和搜索结果可能与我不同。请根据实际情况测试您想要的搜索词。</p>
</blockquote>
<h3 id="利用-embedding-进行商品推荐的冷启动">利用 Embedding 进行商品推荐的冷启动</h3>
<p>Embedding 向量距离不仅可以用于搜索，还可以用于商品推荐中的冷启动。主流的推荐算法主要依托于用户的“看了又看”等行为信息。即如果有很多用户看了 OPPO 手机，并转而看了 vivo 手机，那么在用户看 OPPO 手机时，我们就可以向他推荐 vivo 手机。但往往新商品或新平台缺乏相关的行为数据。此时，我们同样可以根据商品名称在语义上的相似度来进行商品推荐。</p>
<p>我们的代码实现和上面的搜索例子基本一致，唯一的差别是商品名称的 Embedding 是从 DataFrame 中获取，而不是通过调用 OpenAI 的 Embedding API 获取，因为我们已经计算过一遍嵌入，无需再次请求。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">recommend_product</span>(<span class="params">df, product_name, n=<span class="number">3</span>, pprint=<span class="literal">True</span></span>):</span><br><span class="line">    product_embedding = df[df[<span class="string">&#x27;product_name&#x27;</span>] == product_name].iloc[<span class="number">0</span>].embedding</span><br><span class="line">    df[<span class="string">&quot;similarity&quot;</span>] = df.embedding.apply(<span class="keyword">lambda</span> x: cosine_similarity(x, product_embedding))</span><br><span class="line"></span><br><span class="line">    results = (</span><br><span class="line">        df.sort_values(<span class="string">&quot;similarity&quot;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">        .head(n)</span><br><span class="line">        .product_name</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> pprint:</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> results:</span><br><span class="line">            <span class="built_in">print</span>(r)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">results = recommend_product(df, <span class="string">&quot;【全场满减】心理学的技巧：让你的思维更灵活&quot;</span>, n=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">【全场满减】心理学的技巧：让你的思维更灵活</span><br><span class="line">【全场满减】心理学的奥秘：激发你的潜能</span><br><span class="line">【全场满减】心理学的技巧：改变你的态度</span><br></pre></td></tr></table></figure>
<h3 id="利用-faiss-加速搜索">利用 Faiss 加速搜索</h3>
<p>上面的示例代码还存在一个问题：每次搜索或推荐时，我们都需要计算输入嵌入和所有数据嵌入之间的余弦相似度。在上面的例子中，我们只检索了100条数据，但在实际应用中，即使不像百度或谷歌这样的搜索引擎，检索的内容数也可能达到几百万或上千万。如果每次搜索都要计算几百万次余弦距离，速度肯定会很慢。</p>
<p>为了解决这个问题，我们可以使用一些向量数据库或软件包，它们能够快速搜索相似性。例如，我推荐使用 Facebook 开源的 Faiss Python 包，它的全称是 Facebook AI Similarity Search，可以快速进行高维向量的相似性搜索。</p>
<p>我们可以将 DataFrame 中的嵌入向量加载到 Faiss 索引中，然后让 Faiss 帮助我们快速找到最相似的向量。下面我们来看看效果。</p>
<p>当然，按照惯例，我们需要先安装 Faiss 这个 Python 库。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge faiss-cpu</span><br></pre></td></tr></table></figure>
<blockquote>
<p>faiss有多个版本，包括cpu, gpu等等，你可以到这里看看<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/faiss/blob/main/INSTALL.md">官方文档</a>，选择适合自己的版本安装。因为我是M1， 所以选择了cpu版本。</p>
</blockquote>
<p>把整个 Embedding 变成一个二维矩阵，然后直接将其加载到 Faiss 索引中即可。在之前，我们需要定义好 Faiss 索引的维度数，与我们的 Embedding 向量的维度数相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_embeddings_to_faiss</span>(<span class="params">df</span>):</span><br><span class="line">    embeddings = np.array(df[<span class="string">&#x27;embedding&#x27;</span>].tolist()).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    index = faiss.IndexFlatL2(embeddings.shape[<span class="number">1</span>])</span><br><span class="line">    index.add(embeddings)</span><br><span class="line">    <span class="keyword">return</span> index</span><br><span class="line"></span><br><span class="line">index = load_embeddings_to_faiss(df)</span><br></pre></td></tr></table></figure>
<p>搜索 Faiss 简单易用。我们将查询转换成嵌入形式，再将其转换为 numpy 数组向量。然后，我们只需对刚才生成的索引 index 调用 search 方法，并指定返回的结果数量。返回的仅是索引的 index，即加载在 Faiss 中的第几个索引。根据这个索引，在 DataFrame 中查找对应的行数和商品标题即可获得搜索结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">search_index</span>(<span class="params">index, df, query, k=<span class="number">5</span></span>):</span><br><span class="line">    query_vector = np.array(get_embedding(query, engine=embedding_model)).reshape(<span class="number">1</span>, -<span class="number">1</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    distances, indexes = index.search(query_vector, k)</span><br><span class="line"></span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(indexes)):</span><br><span class="line">        product_names = df.iloc[indexes[i]][<span class="string">&#x27;product_name&#x27;</span>].values.tolist()</span><br><span class="line">        results.append((distances[i], product_names))    </span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">products = search_index(index, df, <span class="string">&quot;心理学智慧&quot;</span>, k=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> distances, product_names <span class="keyword">in</span> products:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(distances)):</span><br><span class="line">        <span class="built_in">print</span>(product_names[i], distances[i])</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">【新品特惠】心理学的智慧：改变你的生活 <span class="number">0.20244475</span></span><br><span class="line">【新品特惠】心理学的洞察力：提升你的自信 <span class="number">0.21296506</span></span><br><span class="line">【满减特惠】心理学的智慧：让你的生活更完美 <span class="number">0.21423605</span></span><br></pre></td></tr></table></figure>
<p>用"自然淡雅背包"这个关键词搜索，结果与我们之前计算的余弦距离排序的结果相同。 Faiss 通过 ANN 近似最近邻算法实现相似性搜索。如需了解 Faiss 的原理，请向 ChatGPT 提问。</p>
<img src="/Implementing-semantic-retrieval-using-Embedding/20230601170153.png" class="" title="img">
<p>Faiss库能够加载的数据量受限于内存大小。如果数据量继续增长，需要使用向量数据库进行搜索。例如，OpenAI推荐的<a target="_blank" rel="noopener" href="https://www.pinecone.io/">Pinecone</a>和<a target="_blank" rel="noopener" href="https://weaviate.io/">Weaviate</a>，也有许多团队使用国人开源产品<a target="_blank" rel="noopener" href="https://milvus.io/">Milvus</a>。虽然使用Embedding的相似度可以快速启动搜索和推荐，但需要更复杂的策略才能实现更好的效果。例如，根据用户反馈的行为更好地排序搜索和推荐结果。但对于提供简单的搜索或推荐功能而言，文本的嵌入相似度是很好的快速启动方式。</p>
<h3 id="小结"><strong>小结</strong></h3>
<p>在这一讲中，我将教给你三个主要的技巧，以便更快速地优化现有业务中的推荐和搜索功能。</p>
<p>首先，即使没有适当的测试数据，我们也可以让AI生成一些数据。这种方法不仅可以节省在线查找数据的时间，还可以根据自己的需求生成具有特定特征的数据。例如，我们可以要求在商品标题中添加一些促销相关信息。</p>
<p>其次，我们可以利用嵌入之间的余弦相似度作为语义相似度，来优化搜索。通过嵌入的相似性，我们不要求搜索词和查询内容之间完全匹配，只要它们的语义信息接近即可。</p>
<p>最后，我们将学习如何使用Faiss这样的Python库或其他向量数据库，以快速检索向量。这样，我们就不必每次搜索都计算整个数据库的余弦相似度。</p>
<p>通过计算嵌入向量并对其进行索引，我们可以将外部知识和信息引入到使用GPT模型的应用程序中。在以后的课程中，我们将学习如何利用这些外部知识开发更复杂的AI应用程序。</p>
<h3 id="课后练习"><strong>课后练习</strong></h3>
<p>搜索中经常会遇到同一关键字具有多重含义的情况。例如，当我们搜索“小米手机”时，结果中应该包括“荣耀V30 Pro”，但不应包括“黑龙江优质小米”。你可以尝试使用嵌入进行语义搜索，看看是否仍然会出现这种问题。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>09 使用Embedding实现语义检索</p><p><a href="https://hivan.me/Implementing-semantic-retrieval-using-Embedding/">https://hivan.me/Implementing-semantic-retrieval-using-Embedding/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hivan Du</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2023-05-16</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2023-06-01</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/AI/">AI</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=6479444288ae9600196fa98e&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="https://afdian.net/item/72907364008511ee904852540025c377" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://qiniu.hivan.me/picGo/20230601221633.jpeg" alt="支付宝"></span></a><a class="button donate" href="https://www.buymeacoffee.com/hivandu" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="https://patreon.com/user?u=89473430" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="doo@hivan.me"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://qiniu.hivan.me/IMG_4603.JPG" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/Use-AI-to-index-and-analyze-documents-and-images/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">10 利用AI索引并分析文献和图片</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/Rewriting-and-Reviewing/"><span class="level-item">08 改写和审核</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://hivan.me/Implementing-semantic-retrieval-using-Embedding/';
            this.page.identifier = 'Implementing-semantic-retrieval-using-Embedding/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'hivan' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://www.gravatar.com/avatar/bdff168cf8a71c11d2712a1679a00c54?s=128" alt="茶桁"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">茶桁</p><p class="is-size-6 is-block">AI游民</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shang Hai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">150</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">15</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hivandu" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hivan"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/hivan"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com/hivan"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4NzE4MDQzMg==&amp;action=getalbum&amp;album_id=2932504849574543360&amp;scene=173&amp;from_msgid=2648747980&amp;from_itemidx=1&amp;count=3&amp;nolastread=1&amp;token=1758883909&amp;lang=zh_CN#wechat_redirect"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.zhihu.com/column/c_1424326166602178560" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">塌缩的奇点</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li><li><a class="level is-mobile" href="https://www.zhihu.com/column/hivandu" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">茶桁-知乎</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="level-start"><span class="level-item">从零开始接触人工智能大模型</span></span><span class="level-end"><span class="level-item tag">21</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-15T07:18:39.000Z">2023-07-15</time></p><p class="title"><a href="/%E4%BD%BF%E7%94%A8Transformers%E8%BF%9B%E8%A1%8C%E8%AF%AD%E9%9F%B3%E8%BD%AC%E6%96%87%E6%9C%AC/">使用 Transformers 进行语音转文本的完整入门指南</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-14T06:30:00.000Z">2023-07-14</time></p><p class="title"><a href="/LLMs%E7%9A%84%E5%AE%9E%E7%94%A8%E4%BB%8B%E7%BB%8D/">LLMs的实用介绍</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-10T14:52:54.000Z">2023-07-10</time></p><p class="title"><a href="/%E5%BF%AB%E9%80%9F%E5%80%BE%E5%90%AC%E5%92%8C%E6%80%BB%E7%BB%93%E9%9F%B3%E9%A2%91%E5%86%85%E5%AE%B9/">19. 快速倾听和总结音频内容</a></p><p class="categories"><a href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-09T17:22:07.000Z">2023-07-10</time></p><p class="title"><a href="/ChatGPT%E4%BB%A3%E7%A0%81%E8%A7%A3%E9%87%8A%E5%99%A8/">ChatGPT代码解释器：如何为我节省数小时的工作</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-06T18:23:02.000Z">2023-07-07</time></p><p class="title"><a href="/%E4%BD%BF%E7%94%A8Python%E5%BA%93unstructured%E6%8F%AD%E7%A7%98%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE/">使用Python库unstructured揭秘文本数据</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a><p class="is-size-7"><span>&copy; 2023 Hivan Du</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>