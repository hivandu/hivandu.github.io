<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Initial exploration of machine learning - 茶桁.MAMT</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="茶桁.MAMT"><meta name="msapplication-TileImage" content="https://qiniu.hivan.me/picGo/20230601174411.png?imgNote"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="茶桁.MAMT"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="The code address of this article is: Example 02   The source code is in ipynb format, and the output content can be viewed."><meta property="og:type" content="blog"><meta property="og:title" content="Initial exploration of machine learning"><meta property="og:url" content="https://hivan.me/example_02/"><meta property="og:site_name" content="茶桁.MAMT"><meta property="og:description" content="The code address of this article is: Example 02   The source code is in ipynb format, and the output content can be viewed."><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210830234710.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210830234858.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210830235114.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210830235135.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210830235243.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210830235601.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000016.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000044.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000111.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000213.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000244.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000322.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000347.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000603.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000636.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000735.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000804.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000924.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831001008.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831001044.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831001111.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831001135.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831001212.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831001251.png?imgNote"><meta property="article:published_time" content="2021-09-02T12:59:46.685Z"><meta property="article:modified_time" content="2023-06-02T03:49:09.832Z"><meta property="article:author" content="Hivan Du"><meta property="article:tag" content="AI,人工智能,代码,大语言模型"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://qiniu.hivan.me/picGo/20210830234710.png?imgNote"><meta property="twitter:creator" content="@hivan"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hivan.me/example_02/"},"headline":"Initial exploration of machine learning","image":[],"datePublished":"2021-09-02T12:59:46.685Z","dateModified":"2023-06-02T03:49:09.832Z","author":{"@type":"Person","name":"Hivan Du"},"publisher":{"@type":"Organization","name":"茶桁.MAMT","logo":{"@type":"ImageObject","url":"https://hivan.me/img/logo.svg"}},"description":"The code address of this article is: Example 02   The source code is in ipynb format, and the output content can be viewed."}</script><link rel="canonical" href="https://hivan.me/example_02/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="茶桁.MAMT" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-09-02T12:59:46.685Z" title="9/2/2021, 8:59:46 PM">2021-09-02</time>发表</span></div></div><h1 class="title is-3 is-size-4-mobile">Initial exploration of machine learning</h1><div class="content"><blockquote>
<p>The code address of this article is: <a
target="_blank" rel="noopener" href="https://github.com/hivandu/practise/blob/master/AI-basic/example_02.ipynb">Example
02</a></p>
</blockquote>
<blockquote>
<p>The source code is in ipynb format, and the output content can be
viewed. <span id="more"></span> ## Gradient</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">k</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">3</span> * (k ** <span class="hljs-number">2</span>) + <span class="hljs-number">7</span> * k - <span class="hljs-number">10</span><br>  <br><span class="hljs-comment"># -b / 2a = -7 / 6</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partial</span>(<span class="hljs-params">k</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">6</span> * k + <span class="hljs-number">7</span><br><br>k = random.randint(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br>alpha = <span class="hljs-number">1e-3</span> <span class="hljs-comment"># 0.001</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>):<br>    k = k + (-<span class="hljs-number">1</span>) * partial(k) *alpha<br>    <span class="hljs-built_in">print</span>(k, loss(k))<br>    <br><span class="hljs-comment"># out</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">7.959 124.32404299999999</span><br><span class="hljs-string">-7.918246 122.66813714954799</span><br><span class="hljs-string">show more (open the raw output data in a text editor) ...</span><br><span class="hljs-string">-1.1833014444482555 -14.082503185837805</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h2 id="cutting-problem">Cutting Problem</h2>
<p>All the dynamic programming:</p>
<ol type="1">
<li>sub-problems</li>
<li>Overlapping sub-problems</li>
<li>parse solution</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> lru_cache<br><span class="hljs-comment"># least recent used</span><br><br>prices = [<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">17</span>, <span class="hljs-number">17</span>, <span class="hljs-number">20</span>, <span class="hljs-number">24</span>, <span class="hljs-number">30</span>, <span class="hljs-number">33</span>]<br>complete_price = defaultdict(<span class="hljs-built_in">int</span>)<br><span class="hljs-keyword">for</span> i, p <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(prices): complete_price[i+<span class="hljs-number">1</span>] = p<br>  <br>solution = &#123;&#125;<br><br>cache = &#123;&#125;<br><span class="hljs-comment">#&lt;- if when n .... is huge. size(cache)</span><br><span class="hljs-comment"># keep most important information.</span><br><br><span class="hljs-meta">@lru_cache(<span class="hljs-params">maxsize=<span class="hljs-number">2</span>**<span class="hljs-number">10</span></span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">r</span>(<span class="hljs-params">n</span>):<br>    <span class="hljs-comment"># a very classical dynamic programming problem</span><br>    <span class="hljs-comment"># if n in cache: return cache[n]</span><br><br>    candidates = [(complete_price[n], (n, <span class="hljs-number">0</span>))] + \<br>                 [(r(i) + r(n-i), (i, n - i)) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n)]<br><br>    optimal_price, split = <span class="hljs-built_in">max</span>(candidates)<br><br>    solution[n] = split<br><br>    <span class="hljs-comment"># cache[n] = optimal_price</span><br><br>    <span class="hljs-keyword">return</span> optimal_price<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_solution</span>(<span class="hljs-params">n, cut_solution</span>):<br>    left, right = cut_solution[n]<br><br>    <span class="hljs-keyword">if</span> left == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> right == <span class="hljs-number">0</span>: <span class="hljs-keyword">return</span> [left+right, ]<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> parse_solution(left, cut_solution) + parse_solution(right, cut_solution)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-built_in">print</span>(r(<span class="hljs-number">19</span>))<br>    <span class="hljs-built_in">print</span>(parse_solution(<span class="hljs-number">19</span>, solution))<br>    <br><span class="hljs-comment"># out</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">55</span><br><span class="hljs-string">[11, 6, 2]</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h2 id="dynamic">Dynamic</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> wraps<br><span class="hljs-keyword">from</span> icecream <span class="hljs-keyword">import</span> ic<br><br>original_price = [<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-number">10</span>,<span class="hljs-number">17</span>,<span class="hljs-number">17</span>,<span class="hljs-number">20</span>,<span class="hljs-number">24</span>,<span class="hljs-number">30</span>,<span class="hljs-number">33</span>]<br>price = defaultdict(<span class="hljs-built_in">int</span>)<br><br><span class="hljs-keyword">for</span> i, p <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(original_price):<br>    price[i+<span class="hljs-number">1</span>] = p<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">memo</span>(<span class="hljs-params">func</span>):<br>    cache = &#123;&#125;<br><span class="hljs-meta">    @wraps(<span class="hljs-params">func</span>)</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_wrap</span>(<span class="hljs-params">n</span>):<br>        <span class="hljs-keyword">if</span> n <span class="hljs-keyword">in</span> cache: result = cache[n]<br>        <span class="hljs-keyword">else</span>:<br>            result = func(n)<br>            cache[n] = result<br>        <span class="hljs-keyword">return</span> result<br>    <span class="hljs-keyword">return</span> _wrap<br>  <br><span class="hljs-meta">@memo</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">r</span>(<span class="hljs-params">n</span>):<br>    max_price, split_point = <span class="hljs-built_in">max</span>(<br>        [(price[n],<span class="hljs-number">0</span>)] + [(r(i) + r(n-i), i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n)], key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">0</span>]<br>    )<br>    solution[n]  = (split_point, n-split_point)<br>    <span class="hljs-keyword">return</span> max_price<br>  <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">not_cut</span>(<span class="hljs-params">split</span>): <span class="hljs-keyword">return</span> split == <span class="hljs-number">0</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_solution</span>(<span class="hljs-params">target_length, revenue_solution</span>):<br>    left, right = revenue_solution[target_length]<br>    <span class="hljs-keyword">if</span> not_cut(left): <span class="hljs-keyword">return</span> [right]<br>    <span class="hljs-keyword">return</span> parse_solution(left, revenue_solution) + parse_solution(right, revenue_solution)<br>  <br>solution = &#123;&#125;<br>r(<span class="hljs-number">50</span>)<br>ic(parse_solution(<span class="hljs-number">20</span>,solution))<br>ic(parse_solution(<span class="hljs-number">19</span>,solution))<br>ic(parse_solution(<span class="hljs-number">27</span>,solution))<br><br><span class="hljs-comment"># out</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">ic| parse_solution(20,solution): [10, 10]</span><br><span class="hljs-string">ic| parse_solution(19,solution): [2, 6, 11]</span><br><span class="hljs-string">ic| parse_solution(27,solution): [6, 10, 11]</span><br><span class="hljs-string">[6, 10, 11]</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h2 id="gradient-descent">Gradient descent</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">from</span> icecream <span class="hljs-keyword">import</span> ic<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">func</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">10</span> * x**<span class="hljs-number">2</span> + <span class="hljs-number">32</span>*x + <span class="hljs-number">9</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">20</span> *x + <span class="hljs-number">32</span><br>  <br>x = np.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br>steps = []<br>x_star = random.choice(x)<br>alpha = <span class="hljs-number">1e-3</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    x_star = x_star + -<span class="hljs-number">1</span>*gradient(x_star)*alpha<br>    steps.append(x_star)<br><br>    ic(x_star, func(x_star))<br><br>fig, ax = plt.subplots()<br>ax.plot(x, func(x))<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">ic| x_star: 9.368, func(x_star): 1186.3702400000002</span><br><span class="hljs-string">ic| x_star: 9.14864, func(x_star): 1138.732618496</span><br><span class="hljs-string">show more (open the raw output data in a text editor) ...</span><br><span class="hljs-string">ic| x_star: -0.1157435825983131, func(x_star): 5.430171125980905</span><br><span class="hljs-string">[&lt;matplotlib.lines.Line2D at 0x7fd6d19545d0&gt;]</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">for</span> i, s <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(steps):<br>    ax.annotate(<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>), (s, func(s)))<br>    <br>plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210830234710.png?imgNote"
alt="image-20210830234709856" /></p>
<h2 id="k-means-finding-centers">k-means-finding-centers</h2>
<h3 id="k-means">K-means</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pylab <span class="hljs-keyword">import</span> mpl<br><br>mpl.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;FangSong&#x27;</span>] <span class="hljs-comment"># Specify the default font</span><br>mpl.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span> <span class="hljs-comment"># Solve the problem that the minus sign&#x27;-&#x27; is displayed as a square in the saved image</span><br><br>coordination_source = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">&#123;name:&#x27;兰州&#x27;, geoCoord:[103.73, 36.03]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;嘉峪关&#x27;, geoCoord:[98.17, 39.47]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;西宁&#x27;, geoCoord:[101.74, 36.56]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;成都&#x27;, geoCoord:[104.06, 30.67]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;石家庄&#x27;, geoCoord:[114.48, 38.03]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;拉萨&#x27;, geoCoord:[102.73, 25.04]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;贵阳&#x27;, geoCoord:[106.71, 26.57]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;武汉&#x27;, geoCoord:[114.31, 30.52]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;郑州&#x27;, geoCoord:[113.65, 34.76]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;济南&#x27;, geoCoord:[117, 36.65]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;南京&#x27;, geoCoord:[118.78, 32.04]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;合肥&#x27;, geoCoord:[117.27, 31.86]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;杭州&#x27;, geoCoord:[120.19, 30.26]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;南昌&#x27;, geoCoord:[115.89, 28.68]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;福州&#x27;, geoCoord:[119.3, 26.08]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;广州&#x27;, geoCoord:[113.23, 23.16]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;长沙&#x27;, geoCoord:[113, 28.21]&#125;,</span><br><span class="hljs-string">//&#123;name:&#x27;海口&#x27;, geoCoord:[110.35, 20.02]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;沈阳&#x27;, geoCoord:[123.38, 41.8]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;长春&#x27;, geoCoord:[125.35, 43.88]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;哈尔滨&#x27;, geoCoord:[126.63, 45.75]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;太原&#x27;, geoCoord:[112.53, 37.87]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;西安&#x27;, geoCoord:[108.95, 34.27]&#125;,</span><br><span class="hljs-string">//&#123;name:&#x27;台湾&#x27;, geoCoord:[121.30, 25.03]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;北京&#x27;, geoCoord:[116.46, 39.92]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;上海&#x27;, geoCoord:[121.48, 31.22]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;重庆&#x27;, geoCoord:[106.54, 29.59]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;天津&#x27;, geoCoord:[117.2, 39.13]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;呼和浩特&#x27;, geoCoord:[111.65, 40.82]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;南宁&#x27;, geoCoord:[108.33, 22.84]&#125;,</span><br><span class="hljs-string">//&#123;name:&#x27;西藏&#x27;, geoCoord:[91.11, 29.97]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;银川&#x27;, geoCoord:[106.27, 38.47]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;乌鲁木齐&#x27;, geoCoord:[87.68, 43.77]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;香港&#x27;, geoCoord:[114.17, 22.28]&#125;,</span><br><span class="hljs-string">&#123;name:&#x27;澳门&#x27;, geoCoord:[113.54, 22.19]&#125;</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="feacutre-extractor">Feacutre Extractor</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python">city_location = &#123;<br>    <span class="hljs-string">&#x27;香港&#x27;</span>: (<span class="hljs-number">114.17</span>, <span class="hljs-number">22.28</span>)<br>&#125;<br>test_string = <span class="hljs-string">&quot;&#123;name:&#x27;兰州&#x27;, geoCoord:[103.73, 36.03]&#125;,&quot;</span><br><br><span class="hljs-keyword">import</span> re<br><br>pattern = re.<span class="hljs-built_in">compile</span>(<span class="hljs-string">r&quot;name:&#x27;(\w+)&#x27;,\s+geoCoord:\[(\d+.\d+),\s(\d+.\d+)\]&quot;</span>)<br><br><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> coordination_source.split(<span class="hljs-string">&#x27;\n&#x27;</span>):<br>    city_info = pattern.findall(line)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> city_info: <span class="hljs-keyword">continue</span><br>    <br>    <span class="hljs-comment"># following: we find the city info</span><br>    <br>    city, long, lat = city_info[<span class="hljs-number">0</span>]<br>    <br>    long, lat = <span class="hljs-built_in">float</span>(long), <span class="hljs-built_in">float</span>(lat)<br>    <br>    city_location[city] = (long, lat)<br><br>city_location<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">&#123;&#x27;香港&#x27;: (114.17, 22.28),</span><br><span class="hljs-string"> &#x27;兰州&#x27;: (103.73, 36.03),</span><br><span class="hljs-string">show more (open the raw output data in a text editor) ...</span><br><span class="hljs-string"> &#x27;澳门&#x27;: (113.54, 22.19)&#125;</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">geo_distance</span>(<span class="hljs-params">origin, destination</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Calculate the Haversine distance.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    origin : tuple of float</span><br><span class="hljs-string">        (lat, long)</span><br><span class="hljs-string">    destination : tuple of float</span><br><span class="hljs-string">        (lat, long)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    distance_in_km : float</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Examples</span><br><span class="hljs-string">    --------</span><br><span class="hljs-string">    &gt;&gt;&gt; origin = (48.1372, 11.5756)  # Munich</span><br><span class="hljs-string">    &gt;&gt;&gt; destination = (52.5186, 13.4083)  # Berlin</span><br><span class="hljs-string">    &gt;&gt;&gt; round(distance(origin, destination), 1)</span><br><span class="hljs-string">    504.2</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    lon1, lat1 = origin<br>    lon2, lat2 = destination<br>    radius = <span class="hljs-number">6371</span>  <span class="hljs-comment"># km</span><br><br>    dlat = math.radians(lat2 - lat1)<br>    dlon = math.radians(lon2 - lon1)<br>    a = (math.sin(dlat / <span class="hljs-number">2</span>) * math.sin(dlat / <span class="hljs-number">2</span>) +<br>         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *<br>         math.sin(dlon / <span class="hljs-number">2</span>) * math.sin(dlon / <span class="hljs-number">2</span>))<br>    c = <span class="hljs-number">2</span> * math.atan2(math.sqrt(a), math.sqrt(<span class="hljs-number">1</span> - a))<br>    d = radius * c<br><br>    <span class="hljs-keyword">return</span> d<br></code></pre></td></tr></table></figure>
<h3 id="vector-distances">Vector Distances</h3>
<ul>
<li>余弦距离 Cosine Distance</li>
<li>欧几里得距离 Euclidean Distance</li>
<li>曼哈顿距离 Manhattan distance or Manhattan length</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> networkx <span class="hljs-keyword">as</span> nx<br><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br>%matplotlib inline<br><br><span class="hljs-comment"># set plt, show chinese</span><br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>]  = [<span class="hljs-string">&#x27;Arial Unicode MS&#x27;</span>]<br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>]  = <span class="hljs-literal">False</span><br><br>city_graph = nx.Graph()<br>city_graph.add_nodes_from(<span class="hljs-built_in">list</span>(city_location.keys()))<br>nx.draw(city_graph, city_location, with_labels=<span class="hljs-literal">True</span>, node_size=<span class="hljs-number">30</span>)<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210830234858.png?imgNote"
alt="image-20210830234858640" /></p>
<h3 id="k-means-initial-k-random-centers">K-means: Initial k random
centers</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs python">k = <span class="hljs-number">10</span><br><span class="hljs-keyword">import</span> random<br>all_x = []<br>all_y = []<br><br><span class="hljs-keyword">for</span> _, location <span class="hljs-keyword">in</span> city_location.items():<br>    x, y = location<br>    <br>    all_x.append(x)<br>    all_y.append(y)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_random_center</span>(<span class="hljs-params">all_x, all_y</span>):<br>    r_x = random.uniform(<span class="hljs-built_in">min</span>(all_x), <span class="hljs-built_in">max</span>(all_x))<br>    r_y = random.uniform(<span class="hljs-built_in">min</span>(all_y), <span class="hljs-built_in">max</span>(all_y))<br>    <br>    <span class="hljs-keyword">return</span> r_x, r_y<br><br>get_random_center(all_x, all_y)<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">(93.61182991130997, 37.01816228131414)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>K = <span class="hljs-number">5</span><br>centers = &#123;<span class="hljs-string">&#x27;&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i+<span class="hljs-number">1</span>): get_random_center(all_x, all_y) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K)&#125;<br><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><br>closet_points = defaultdict(<span class="hljs-built_in">list</span>)<br><span class="hljs-keyword">for</span> x, y, <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(all_x, all_y):<br>    closet_c, closet_dis = <span class="hljs-built_in">min</span>([(k, geo_distance((x, y), centers[k])) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> centers], key=<span class="hljs-keyword">lambda</span> t: t[<span class="hljs-number">1</span>])    <br>    <br>    closet_points[closet_c].append([x, y])<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">iterate_once</span>(<span class="hljs-params">centers, closet_points, threshold=<span class="hljs-number">5</span></span>):<br>    have_changed = <span class="hljs-literal">False</span><br>    <br>    <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> closet_points:<br>        former_center = centers[c]<br><br>        neighbors = closet_points[c]<br><br>        neighbors_center = np.mean(neighbors, axis=<span class="hljs-number">0</span>)<br><br>        <span class="hljs-keyword">if</span> geo_distance(neighbors_center, former_center) &gt; threshold:<br>            centers[c] = neighbors_center<br>            have_changed = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">pass</span> <span class="hljs-comment">## keep former center</span><br>        <br>    <span class="hljs-keyword">return</span> centers, have_changed<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">kmeans</span>(<span class="hljs-params">Xs, k, threshold=<span class="hljs-number">5</span></span>):<br>    all_x = Xs[:, <span class="hljs-number">0</span>]<br>    all_y = Xs[:, <span class="hljs-number">1</span>]<br>    <br>    K = k<br>    centers = &#123;<span class="hljs-string">&#x27;&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i+<span class="hljs-number">1</span>): get_random_center(all_x, all_y) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K)&#125;<br>    changed = <span class="hljs-literal">True</span><br>    <br>    <span class="hljs-keyword">while</span> changed:<br>        closet_points = defaultdict(<span class="hljs-built_in">list</span>)<br><br>        <span class="hljs-keyword">for</span> x, y, <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(all_x, all_y):<br>            closet_c, closet_dis = <span class="hljs-built_in">min</span>([(k, geo_distance((x, y), centers[k])) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> centers], key=<span class="hljs-keyword">lambda</span> t: t[<span class="hljs-number">1</span>])    <br>            closet_points[closet_c].append([x, y])   <br>            <br>        centers, changed = iterate_once(centers, closet_points, threshold)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;iteration&#x27;</span>)<br><br>    <span class="hljs-keyword">return</span> centers<br><br>kmeans(np.array(<span class="hljs-built_in">list</span>(city_location.values())), k=<span class="hljs-number">5</span>, threshold=<span class="hljs-number">5</span>)<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">iteration</span><br><span class="hljs-string">iteration</span><br><span class="hljs-string">iteration</span><br><span class="hljs-string">iteration</span><br><span class="hljs-string">iteration</span><br><span class="hljs-string">&#123;&#x27;1&#x27;: array([99.518, 38.86 ]),</span><br><span class="hljs-string"> &#x27;2&#x27;: array([117.833,  39.861]),</span><br><span class="hljs-string"> &#x27;3&#x27;: array([91.11, 29.97]),</span><br><span class="hljs-string"> &#x27;4&#x27;: array([106.81,  27.  ]),</span><br><span class="hljs-string"> &#x27;5&#x27;: array([116.87166667,  27.6275    ])&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>plt.scatter(all_x, all_y)<br>plt.scatter(*<span class="hljs-built_in">zip</span>(*centers.values()))<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210830235114.png?imgNote"
alt="image-20210830235114060" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> c, points <span class="hljs-keyword">in</span> closet_points.items():<br>    plt.scatter(*<span class="hljs-built_in">zip</span>(*points))<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210830235135.png?imgNote"
alt="image-20210830235135375" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">city_location_with_station = &#123;<br>    <span class="hljs-string">&#x27;能源站-&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i): position <span class="hljs-keyword">for</span> i, position <span class="hljs-keyword">in</span> centers.items()<br>&#125;<br>city_location_with_station<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">&#123;&#x27;能源站-1&#x27;: (108.82946246581274, 26.05763939719317),</span><br><span class="hljs-string"> &#x27;能源站-2&#x27;: (97.96769355736322, 22.166113183141032),</span><br><span class="hljs-string"> &#x27;能源站-3&#x27;: (114.05390380408154, 38.7698708467224),</span><br><span class="hljs-string"> &#x27;能源站-4&#x27;: (118.49242085311417, 28.665716162786204),</span><br><span class="hljs-string"> &#x27;能源站-5&#x27;: (125.08287617496866, 25.55784683330647)&#125;</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">draw_cities</span>(<span class="hljs-params">citise, color=<span class="hljs-literal">None</span></span>):<br>    city_graph = nx.Graph()<br>    city_graph.add_nodes_from(<span class="hljs-built_in">list</span>(citise.keys()))<br>    nx.draw(city_graph, citise, node_color=color, with_labels=<span class="hljs-literal">True</span>, node_size=<span class="hljs-number">30</span>)<br><br>%matplotlib inline<br><br>plt.figure(<span class="hljs-number">1</span>,figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">12</span>)) <br>draw_cities(city_location_with_station, color=<span class="hljs-string">&#x27;green&#x27;</span>)<br>draw_cities(city_location, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210830235243.png?imgNote"
alt="image-20210830235243564" /></p>
<h2 id="about-the-dataset">About the dataset</h2>
<blockquote>
<p>This contains data of news headlines published over a period of 15
years. From the reputable Australian news source ABC (Australian
Broadcasting Corp.) Site: http://www.abc.net.au/ Prepared by Rohit
Kulkarni</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">from</span> sklearn.feature_extraction <span class="hljs-keyword">import</span> text<br><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer<br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br><span class="hljs-keyword">from</span> nltk.tokenize <span class="hljs-keyword">import</span> RegexpTokenizer<br><span class="hljs-keyword">from</span> nltk.stem.snowball <span class="hljs-keyword">import</span> SnowballStemmer<br><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br>%matplotlib inline<br><br><br>data = pd.read_csv(<span class="hljs-string">&quot;./data/abcnews-date-text.csv&quot;</span>,error_bad_lines=<span class="hljs-literal">False</span>,usecols =[<span class="hljs-string">&quot;headline_text&quot;</span>])<br>data.head()<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">headline_text</span><br><span class="hljs-string">0	aba decides against community broadcasting lic...</span><br><span class="hljs-string">1	act fire witnesses must be aware of defamation</span><br><span class="hljs-string">2	a g calls for infrastructure protection summit</span><br><span class="hljs-string">3	air nz staff in aust strike for pay rise</span><br><span class="hljs-string">4	air nz strike to affect australian travellers</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>data.to_csv(<span class="hljs-string">&#x27;abcnews.csv&#x27;</span>, index=<span class="hljs-literal">False</span>, encoding=<span class="hljs-string">&#x27;utf8&#x27;</span>)<br>data.info()<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span><br><span class="hljs-string">RangeIndex: 1103665 entries, 0 to 1103664</span><br><span class="hljs-string">Data columns (total 1 columns):</span><br><span class="hljs-string"> #   Column         Non-Null Count    Dtype </span><br><span class="hljs-string">---  ------         --------------    ----- </span><br><span class="hljs-string"> 0   headline_text  1103665 non-null  object</span><br><span class="hljs-string">dtypes: object(1)</span><br><span class="hljs-string">memory usage: 8.4+ MB</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h2 id="deleting-dupliate-headlinesif-any">Deleting dupliate
headlines(if any)</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">data[data[<span class="hljs-string">&#x27;headline_text&#x27;</span>].duplicated(keep=<span class="hljs-literal">False</span>)].sort_values(<span class="hljs-string">&#x27;headline_text&#x27;</span>).head(<span class="hljs-number">8</span>)<br>data = data.drop_duplicates(<span class="hljs-string">&#x27;headline_text&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h2 id="nlp">NLP</h2>
<h3 id="preparing-data-for-vectorizaion">Preparing data for
vectorizaion</h3>
<p>However, when doing natural language processing, words must be
converted into vectors that machine learning algorithms can make use of.
If your goal is to do machine learning on text data, like movie reviews
or tweets or anything else, you need to convert the text data into
numbers. This process is sometimes referred to as “embedding” or
“vectorization”.</p>
<p>In terms of vectorization, it is important to remember that it isn’t
merely turning a single word into a single number. While words can be
transformed into numbers, an entire document can be translated into a
vector. Not only can a vector have more than one dimension, but with
text data vectors are usually high-dimensional. This is because each
dimension of your feature data will correspond to a word, and the
language in the documents you are examining will have thousands of
words.</p>
<h3 id="tf-idf">TF-IDF</h3>
<p>In information retrieval, tf–idf or TFIDF, short for term
frequency–inverse document frequency, is a numerical statistic that is
intended to reflect how important a word is to a document in a
collection or corpus. It is often used as a weighting factor in searches
of information retrieval, text mining, and user modeling. The tf-idf
value increases proportionally to the number of times a word appears in
the document and is offset by the frequency of the word in the corpus,
which helps to adjust for the fact that some words appear more
frequently in general. Nowadays, tf-idf is one of the most popular
term-weighting schemes; 83% of text-based recommender systems in the
domain of digital libraries use tf-idf.</p>
<p>Variations of the tf–idf weighting scheme are often used by search
engines as a central tool in scoring and ranking a document's relevance
given a user query. tf–idf can be successfully used for stop-words
filtering in various subject fields, including text summarization and
classification.</p>
<p>One of the simplest ranking functions is computed by summing the
tf–idf for each query term; many more sophisticated ranking functions
are variants of this simple model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">punc = [<span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;&quot;&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;?&#x27;</span>, <span class="hljs-string">&#x27;!&#x27;</span>, <span class="hljs-string">&#x27;:&#x27;</span>, <span class="hljs-string">&#x27;;&#x27;</span>, <span class="hljs-string">&#x27;(&#x27;</span>, <span class="hljs-string">&#x27;)&#x27;</span>, <span class="hljs-string">&#x27;[&#x27;</span>, <span class="hljs-string">&#x27;]&#x27;</span>, <span class="hljs-string">&#x27;&#123;&#x27;</span>, <span class="hljs-string">&#x27;&#125;&#x27;</span>,<span class="hljs-string">&quot;%&quot;</span>]<br>stop_words = text.ENGLISH_STOP_WORDS.union(punc)<br>desc = data[<span class="hljs-string">&#x27;headline_text&#x27;</span>].values<br>vectorizer = TfidfVectorizer(stop_words = stop_words)<br>X = vectorizer.fit_transform(desc)<br><br>word_features = vectorizer.get_feature_names()<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(word_features))<br><span class="hljs-built_in">print</span>(word_features[<span class="hljs-number">5000</span>:<span class="hljs-number">5100</span>])<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">96397</span><br><span class="hljs-string">[&#x27;abyss&#x27;, &#x27;ac&#x27;, &#x27;aca&#x27;, &#x27;acacia&#x27;, &#x27;acacias&#x27;, &#x27;acadamy&#x27;, &#x27;academia&#x27;, &#x27;academic&#x27;, &#x27;academics&#x27;, &#x27;academies&#x27;, &#x27;academy&#x27;, &#x27;academys&#x27;, &#x27;acai&#x27;, &#x27;acapulco&#x27;, &#x27;acars&#x27;, &#x27;acason&#x27;, &#x27;acasuso&#x27;, &#x27;acb&#x27;, &#x27;acbf&#x27;, &#x27;acc&#x27;, &#x27;acca&#x27;, &#x27;accan&#x27;, &#x27;accc&#x27;, &#x27;acccc&#x27;, &#x27;acccs&#x27;, &#x27;acccused&#x27;, &#x27;acce&#x27;, &#x27;accedes&#x27;, &#x27;accelerant&#x27;, &#x27;accelerants&#x27;, &#x27;accelerate&#x27;, &#x27;accelerated&#x27;, &#x27;accelerates&#x27;, &#x27;accelerating&#x27;, &#x27;acceleration&#x27;, &#x27;accelerator&#x27;, &#x27;accen&#x27;, &#x27;accent&#x27;, &#x27;accents&#x27;, &#x27;accentuate&#x27;, &#x27;accentuates&#x27;, &#x27;accentuating&#x27;, &#x27;accenture&#x27;, &#x27;accept&#x27;, &#x27;acceptability&#x27;, &#x27;acceptable&#x27;, &#x27;acceptably&#x27;, &#x27;acceptance&#x27;, &#x27;acceptances&#x27;, &#x27;accepted&#x27;, &#x27;accepting&#x27;, &#x27;acceptor&#x27;, &#x27;acceptors&#x27;, &#x27;accepts&#x27;, &#x27;accerate&#x27;, &#x27;acces&#x27;, &#x27;access&#x27;, &#x27;accessary&#x27;, &#x27;accessed&#x27;, &#x27;accesses&#x27;, &#x27;accessibility&#x27;, &#x27;accessible&#x27;, &#x27;accessing&#x27;, &#x27;accessories&#x27;, &#x27;accessory&#x27;, &#x27;accesss&#x27;, &#x27;acci&#x27;, &#x27;accid&#x27;, &#x27;accide&#x27;, &#x27;acciden&#x27;, &#x27;accidenatlly&#x27;, &#x27;accidenbt&#x27;, &#x27;accident&#x27;, &#x27;accidental&#x27;, &#x27;accidentally&#x27;, &#x27;accidently&#x27;, &#x27;accidents&#x27;, &#x27;acciona&#x27;, &#x27;accis&#x27;, &#x27;acclaim&#x27;, &#x27;acclaimed&#x27;, &#x27;acclamation&#x27;, &#x27;acclimatise&#x27;, &#x27;acco&#x27;, &#x27;accolade&#x27;, &#x27;accolades&#x27;, &#x27;accom&#x27;, &#x27;accomm&#x27;, &#x27;accommoda&#x27;, &#x27;accommodate&#x27;, &#x27;accommodated&#x27;, &#x27;accommodates&#x27;, &#x27;accommodating&#x27;, &#x27;accommodation&#x27;, &#x27;accomo&#x27;, &#x27;accomodation&#x27;, &#x27;accomommodation&#x27;, &#x27;accompanied&#x27;, &#x27;accompanies&#x27;, &#x27;accompaniment&#x27;]</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="stemming">Stemming</h3>
<p>Stemming is the process of reducing a word into its stem, i.e. its
root form. The root form is not necessarily a word by itself, but it can
be used to generate words by concatenating the right suffix. For
example, the words fish, fishes and fishing all stem into fish, which is
a correct word. On the other side, the words study, studies and studying
stems into studi, which is not an English word.</p>
<h3 id="tokenizing">Tokenizing</h3>
<p>Tokenization is breaking the sentence into words and punctuation,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">stemmer = SnowballStemmer(<span class="hljs-string">&#x27;english&#x27;</span>)<br>tokenizer = RegexpTokenizer(<span class="hljs-string">r&#x27;[a-zA-Z\&#x27;]+&#x27;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-keyword">return</span> [stemmer.stem(word) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> tokenizer.tokenize(text.lower())]<br></code></pre></td></tr></table></figure>
<p><strong>Vectorization with stop words(words irrelevant to the model),
stemming and tokenizing</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)<br>X2 = vectorizer2.fit_transform(desc)<br>word_features2 = vectorizer2.get_feature_names()<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(word_features2))<br><span class="hljs-built_in">print</span>(word_features2[:<span class="hljs-number">50</span>]) <br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">65232</span><br><span class="hljs-string">[&quot;&#x27;a&quot;, &quot;&#x27;i&quot;, &quot;&#x27;s&quot;, &quot;&#x27;t&quot;, &#x27;aa&#x27;, &#x27;aaa&#x27;, &#x27;aaahhh&#x27;, &#x27;aac&#x27;, &#x27;aacc&#x27;, &#x27;aaco&#x27;, &#x27;aacta&#x27;, &#x27;aad&#x27;, &#x27;aadmi&#x27;, &#x27;aag&#x27;, &#x27;aagaard&#x27;, &#x27;aagard&#x27;, &#x27;aah&#x27;, &#x27;aalto&#x27;, &#x27;aam&#x27;, &#x27;aamer&#x27;, &#x27;aami&#x27;, &#x27;aamodt&#x27;, &#x27;aandahl&#x27;, &#x27;aant&#x27;, &#x27;aap&#x27;, &#x27;aapa&#x27;, &#x27;aapt&#x27;, &#x27;aar&#x27;, &#x27;aaradhna&#x27;, &#x27;aardman&#x27;, &#x27;aardvark&#x27;, &#x27;aargau&#x27;, &#x27;aaron&#x27;, &#x27;aaronpaul&#x27;, &#x27;aarwun&#x27;, &#x27;aat&#x27;, &#x27;ab&#x27;, &#x27;aba&#x27;, &#x27;abaaoud&#x27;, &#x27;ababa&#x27;, &#x27;aback&#x27;, &#x27;abadi&#x27;, &#x27;abadon&#x27;, &#x27;abal&#x27;, &#x27;abalon&#x27;, &#x27;abalonv&#x27;, &#x27;abama&#x27;, &#x27;abandon&#x27;, &#x27;abandond&#x27;, &#x27;abandong&#x27;]</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = <span class="hljs-number">1000</span>)<br>X3 = vectorizer3.fit_transform(desc)<br>words = vectorizer3.get_feature_names()<br></code></pre></td></tr></table></figure>
<p>For this, we will use k-means clustering algorithm. ### K-means
clustering (Source <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm">Wikipedia</a>)</p>
<h3 id="elbow-method-to-select-number-of-clusters">Elbow method to
select number of clusters</h3>
<p>This method looks at the percentage of variance explained as a
function of the number of clusters: One should choose a number of
clusters so that adding another cluster doesn't give much better
modeling of the data. More precisely, if one plots the percentage of
variance explained by the clusters against the number of clusters, the
first clusters will add much information (explain a lot of variance),
but at some point the marginal gain will drop, giving an angle in the
graph. The number of clusters is chosen at this point, hence the "elbow
criterion". This "elbow" cannot always be unambiguously identified.
Percentage of variance explained is the ratio of the between-group
variance to the total variance, also known as an F-test. A slight
variation of this method plots the curvature of the within group
variance.</p>
<h4
id="basically-number-of-clusters-the-x-axis-value-of-the-point-that-is-the-corner-of-the-elbowthe-plot-looks-often-looks-like-an-elbow"><strong>Basically,
number of clusters = the x-axis value of the point that is the corner of
the "elbow"(the plot looks often looks like an elbow)</strong></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br>wcss = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>):<br>    kmeans = KMeans(n_clusters=i,init=<span class="hljs-string">&#x27;k-means++&#x27;</span>,max_iter=<span class="hljs-number">300</span>,n_init=<span class="hljs-number">10</span>,random_state=<span class="hljs-number">0</span>)<br>    kmeans.fit(X3)<br>    wcss.append(kmeans.inertia_)<br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>),wcss)<br>plt.title(<span class="hljs-string">&#x27;The Elbow Method&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Number of clusters&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;WCSS&#x27;</span>)<br>plt.savefig(<span class="hljs-string">&#x27;elbow.png&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210830235601.png?imgNote"
alt="image-20210830235601231" /></p>
<p>As more than one elbows have been generated, I will have to select
right amount of clusters by trial and error. So, I will showcase the
results of different amount of clusters to find out the right amount of
clusters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(words[<span class="hljs-number">250</span>:<span class="hljs-number">300</span>])<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">[&#x27;decis&#x27;, &#x27;declar&#x27;, &#x27;defenc&#x27;, &#x27;defend&#x27;, &#x27;delay&#x27;, &#x27;deliv&#x27;, &#x27;demand&#x27;, &#x27;deni&#x27;, &#x27;despit&#x27;, &#x27;destroy&#x27;, &#x27;detent&#x27;, &#x27;develop&#x27;, &#x27;die&#x27;, &#x27;director&#x27;, &#x27;disabl&#x27;, &#x27;disast&#x27;, &#x27;discuss&#x27;, &#x27;diseas&#x27;, &#x27;dismiss&#x27;, &#x27;disput&#x27;, &#x27;doctor&#x27;, &#x27;dog&#x27;, &#x27;dollar&#x27;, &#x27;domest&#x27;, &#x27;donald&#x27;, &#x27;donat&#x27;, &#x27;doubl&#x27;, &#x27;doubt&#x27;, &#x27;draw&#x27;, &#x27;dri&#x27;, &#x27;drink&#x27;, &#x27;drive&#x27;, &#x27;driver&#x27;, &#x27;drop&#x27;, &#x27;drought&#x27;, &#x27;drown&#x27;, &#x27;drug&#x27;, &#x27;drum&#x27;, &#x27;dump&#x27;, &#x27;dure&#x27;, &#x27;e&#x27;, &#x27;eagl&#x27;, &#x27;earli&#x27;, &#x27;eas&#x27;, &#x27;east&#x27;, &#x27;econom&#x27;, &#x27;economi&#x27;, &#x27;edg&#x27;, &#x27;educ&#x27;, &#x27;effort&#x27;]</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="clusters">3 Clusters</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">kmeans = KMeans(n_clusters = <span class="hljs-number">3</span>, n_init = <span class="hljs-number">20</span>, n_jobs = <span class="hljs-number">1</span>) <span class="hljs-comment"># n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)</span><br>kmeans.fit(X3)<br><span class="hljs-comment"># We look at 3 the clusters generated by k-means.</span><br>common_words = kmeans.cluster_centers_.argsort()[:,-<span class="hljs-number">1</span>:-<span class="hljs-number">26</span>:-<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> num, centroid <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(common_words):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">str</span>(num) + <span class="hljs-string">&#x27; : &#x27;</span> + <span class="hljs-string">&#x27;, &#x27;</span>.join(words[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> centroid))<br>    <br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">0 : new, say, plan, win, council, govt, australia, report, kill, fund, urg, court, warn, water, australian, nsw, open, chang, year, qld, interview, wa, death, face, crash</span><br><span class="hljs-string">1 : polic, investig, probe, man, search, offic, hunt, miss, arrest, death, car, shoot, drug, seek, attack, assault, say, murder, crash, charg, driver, suspect, fatal, raid, station</span><br><span class="hljs-string">2 : man, charg, murder, court, face, jail, assault, stab, die, death, drug, guilti, child, sex, accus, attack, woman, crash, arrest, car, kill, miss, sydney, alleg, plead</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="clusters-1">5 Clusters</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">kmeans = KMeans(n_clusters = <span class="hljs-number">5</span>, n_init = <span class="hljs-number">20</span>, n_jobs = <span class="hljs-number">1</span>)<br>kmeans.fit(X3)<br><span class="hljs-comment"># We look at 5 the clusters generated by k-means.</span><br>common_words = kmeans.cluster_centers_.argsort()[:,-<span class="hljs-number">1</span>:-<span class="hljs-number">26</span>:-<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> num, centroid <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(common_words):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">str</span>(num) + <span class="hljs-string">&#x27; : &#x27;</span> + <span class="hljs-string">&#x27;, &#x27;</span>.join(words[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> centroid))<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">0 : man, plan, charg, court, govt, australia, face, murder, accus, jail, assault, stab, urg, drug, death, attack, child, sex, die, woman, guilti, say, alleg, told, car</span><br><span class="hljs-string">1 : new, zealand, law, year, plan, open, polic, home, hospit, centr, deal, set, hope, australia, look, appoint, announc, chief, say, south, minist, govt, rule, servic, welcom</span><br><span class="hljs-string">2 : say, win, kill, report, australian, warn, interview, open, water, fund, nsw, crash, death, urg, year, chang, wa, sydney, claim, qld, hit, attack, world, set, health</span><br><span class="hljs-string">3 : council, plan, consid, fund, rate, urg, seek, new, merger, water, land, develop, reject, say, mayor, vote, chang, elect, rise, meet, park, push, want, govt, approv</span><br><span class="hljs-string">4 : polic, investig, man, probe, search, offic, hunt, miss, arrest, death, car, charg, shoot, drug, seek, attack, assault, murder, crash, say, driver, fatal, suspect, raid, woman</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="clusters-2">6 Clusters</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">kmeans = KMeans(n_clusters = <span class="hljs-number">6</span>, n_init = <span class="hljs-number">20</span>, n_jobs = <span class="hljs-number">1</span>)<br>kmeans.fit(X3)<br><span class="hljs-comment"># We look at 6 the clusters generated by k-means.</span><br>common_words = kmeans.cluster_centers_.argsort()[:,-<span class="hljs-number">1</span>:-<span class="hljs-number">26</span>:-<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> num, centroid <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(common_words):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">str</span>(num) + <span class="hljs-string">&#x27; : &#x27;</span> + <span class="hljs-string">&#x27;, &#x27;</span>.join(words[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> centroid))<br>    <br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">0 : council, govt, australia, report, warn, urg, fund, australian, water, nsw, chang, qld, wa, health, elect, rural, countri, hour, sa, boost, climat, govern, servic, south, consid</span><br><span class="hljs-string">1 : man, charg, murder, court, face, jail, assault, stab, die, death, drug, guilti, child, sex, accus, attack, woman, crash, arrest, car, kill, miss, sydney, plead, alleg</span><br><span class="hljs-string">2 : polic, investig, probe, man, search, offic, hunt, miss, arrest, death, car, shoot, drug, seek, attack, crash, assault, murder, charg, driver, say, fatal, suspect, raid, warn</span><br><span class="hljs-string">3 : win, kill, court, interview, crash, open, death, sydney, face, year, claim, hit, attack, world, set, final, day, hous, die, home, jail, talk, return, cup, hospit</span><br><span class="hljs-string">4 : new, zealand, law, year, plan, open, council, polic, home, hospit, centr, deal, set, hope, australia, appoint, look, announc, chief, say, govt, south, minist, mayor, welcom</span><br><span class="hljs-string">5 : say, plan, council, govt, water, need, group, chang, labor, minist, govern, opposit, public, mp, health, union, green, hous, develop, resid, report, expert, cut, australia, mayor</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="clusters-3">8 Clusters</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">kmeans = KMeans(n_clusters = <span class="hljs-number">8</span>, n_init = <span class="hljs-number">20</span>, n_jobs = <span class="hljs-number">1</span>)<br>kmeans.fit(X3)<br><span class="hljs-comment"># Finally, we look at 8 the clusters generated by k-means.</span><br>common_words = kmeans.cluster_centers_.argsort()[:,-<span class="hljs-number">1</span>:-<span class="hljs-number">26</span>:-<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> num, centroid <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(common_words):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">str</span>(num) + <span class="hljs-string">&#x27; : &#x27;</span> + <span class="hljs-string">&#x27;, &#x27;</span>.join(words[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> centroid))<br>    <br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">0 : polic, say, man, miss, arrest, jail, investig, car, search, murder, attack, crash, kill, probe, die, hunt, shoot, assault, offic, drug, stab, accus, fatal, guilti, bodi</span><br><span class="hljs-string">1 : death, hous, polic, toll, investig, man, probe, inquest, rise, woman, coron, blaze, price, public, white, babi, sentenc, famili, road, spark, jail, prompt, blame, custodi, report</span><br><span class="hljs-string">2 : plan, council, govt, water, new, say, develop, hous, group, chang, unveil, reject, park, urg, centr, public, expans, green, resid, health, reveal, labor, govern, opposit, power</span><br><span class="hljs-string">3 : court, face, man, accus, told, hear, murder, high, case, appear, rule, charg, alleg, appeal, drug, jail, woman, death, assault, order, sex, stab, challeng, teen, polic</span><br><span class="hljs-string">4 : australia, govt, kill, report, warn, australian, urg, fund, nsw, interview, water, open, crash, qld, chang, wa, year, day, claim, hit, attack, sydney, set, health, world</span><br><span class="hljs-string">5 : new, council, zealand, law, fund, year, consid, water, urg, open, say, seek, rate, centr, mayor, govt, elect, look, develop, land, deal, hope, set, push, home</span><br><span class="hljs-string">6 : win, award, cup, titl, open, gold, stage, world, final, tour, elect, australia, lead, seri, aussi, claim, second, australian, big, england, grand, m, battl, race, record</span><br><span class="hljs-string">7 : charg, man, murder, face, assault, drug, polic, child, sex, woman, teen, death, stab, drop, alleg, attack, rape, men, guilti, shoot, bail, sydney, fatal, driver, yo</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>Because even I didn't know what kind of clusters would be generated,
I will describe them in comments.</p>
<h2 id="other-discussions">Other discussions</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords<br><br><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer<br><span class="hljs-keyword">import</span> gensim<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><span class="hljs-keyword">import</span> string<br><span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> WordNetLemmatizer, PorterStemmer<br><span class="hljs-keyword">from</span> nltk.tokenize <span class="hljs-keyword">import</span> word_tokenize<br><span class="hljs-keyword">import</span> pyLDAvis.gensim_models<br><span class="hljs-keyword">from</span> wordcloud <span class="hljs-keyword">import</span> WordCloud, STOPWORDS<br><span class="hljs-keyword">from</span> textblob <span class="hljs-keyword">import</span> TextBlob<br><span class="hljs-keyword">from</span> spacy <span class="hljs-keyword">import</span> displacy<br><span class="hljs-keyword">import</span> nltk<br><br><span class="hljs-keyword">import</span> warnings<br><br>warnings.filterwarnings(<span class="hljs-string">&#x27;ignore&#x27;</span>)<br><br><span class="hljs-comment"># set plt</span><br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;Arial Unicode MS&#x27;</span>]<br>plt.rcParams.update(&#123;<span class="hljs-string">&#x27;font.size&#x27;</span>: <span class="hljs-number">12</span>&#125;)<br>plt.rcParams.update(&#123;<span class="hljs-string">&#x27;figure.figsize&#x27;</span>: [<span class="hljs-number">16</span>, <span class="hljs-number">12</span>]&#125;)<br><span class="hljs-comment"># plt.figure(figsize = [20, 20])</span><br>plt.style.use(<span class="hljs-string">&#x27;seaborn-whitegrid&#x27;</span>)<br><br>df = pd.read_csv(<span class="hljs-string">&#x27;../data/abcnews-date-text.csv&#x27;</span>, nrows = <span class="hljs-number">10000</span>)<br>df.head()<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">publish_date	headline_text</span><br><span class="hljs-string">0	20030219	aba decides against community broadcasting lic...</span><br><span class="hljs-string">1	20030219	act fire witnesses must be aware of defamation</span><br><span class="hljs-string">2	20030219	a g calls for infrastructure protection summit</span><br><span class="hljs-string">3	20030219	air nz staff in aust strike for pay rise</span><br><span class="hljs-string">4	20030219	air nz strike to affect australian travellers</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>The data set contains only two columns, the release date and the news
title.</p>
<p>For simplicity, I will explore the first 10,000 rows in this dataset.
Since the titles are sorted by publish_date, they are actually two
months from February 19, 2003 to April 7, 2003.</p>
<h3 id="number-of-characters-present-in-each-sentence">Number of
characters present in each sentence</h3>
<p>Visualization of text statistics is a simple but insightful
technique.</p>
<p>They include:</p>
<p>Word frequency analysis, sentence length analysis, average word
length analysis, etc.</p>
<p>These really help to explore the basic characteristics of text
data.</p>
<p>For this, we will mainly use histograms (continuous data) and bar
graphs (categorical data).</p>
<p>First, let me look at the number of characters in each sentence. This
can give us a rough idea of the length of news headlines.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&#x27;headline_text&#x27;</span>].<span class="hljs-built_in">str</span>.<span class="hljs-built_in">len</span>().hist()<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831000016.png?imgNote"
alt="image-20210831000016641" /></p>
<h3 id="number-of-words-appearing-in-each-news-headline">number of words
appearing in each news headline</h3>
<p>The histogram shows that the range of news headlines is 10 to 70
characters, usually between 25 and 55 characters.</p>
<p>Now, we will continue to explore the data verbatim. Let's plot the
number of words that appear in each news headline.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&#x27;headline_text&#x27;</span>].<span class="hljs-built_in">str</span>.split().<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x)).hist()<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831000044.png?imgNote"
alt="image-20210831000044656" /></p>
<h3 id="analysing-word-length">Analysing word length</h3>
<p>Obviously, the number of words in news headlines is in the range of 2
to 12, and most of them are between 5 and 7.</p>
<p>Next, let's check the average word length in each sentence.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&#x27;headline_text&#x27;</span>].<span class="hljs-built_in">str</span>.split().apply(<span class="hljs-keyword">lambda</span> x : [<span class="hljs-built_in">len</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x]).<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: np.mean(x)).hist()<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831000111.png?imgNote"
alt="image-20210831000111622" /></p>
<p>The average word length is between 3 and 9, and the most common
length is 5. Does this mean that people use very short words in news
headlines?</p>
<p>Let us find out.</p>
<p>One reason that may not be the case is stop words. Stop words are the
most commonly used words in any language (such as "the", "a", "an",
etc.). Since the length of these words may be small, these words may
cause the above graphics to be skewed to the left.</p>
<p>Analyzing the number and types of stop words can give us some
in-depth understanding of the data.</p>
<p>To get a corpus containing stop words, you can use the <a
target="_blank" rel="noopener" href="https://www.nltk.org/?ref=hackernoon.com">nltk library</a>. Nltk
contains stop words from multiple languages. Since we only deal with
English news, I will filter English stop words from the corpus.</p>
<h3 id="analysing-stopwords">Analysing stopwords</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Fetch stopwords</span><br><span class="hljs-keyword">import</span> nltk<br>nltk.download(<span class="hljs-string">&#x27;stopwords&#x27;</span>)<br>stop=<span class="hljs-built_in">set</span>(stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>))<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">[nltk_data] Downloading package stopwords to /Users/xx/nltk_data...</span><br><span class="hljs-string">[nltk_data]   Package stopwords is already up-to-date!</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><br><span class="hljs-comment"># Create corpus</span><br>corpus=[]<br>new= df[<span class="hljs-string">&#x27;headline_text&#x27;</span>].<span class="hljs-built_in">str</span>.split()<br>new=new.values.tolist()<br>corpus=[word <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> new <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> i]<br><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br>dic=defaultdict(<span class="hljs-built_in">int</span>)<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> corpus:<br>    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> stop:<br>        dic[word]+=<span class="hljs-number">1</span><br>        <br><span class="hljs-comment"># Plot top stopwords</span><br><br>top=<span class="hljs-built_in">sorted</span>(dic.items(), key=<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">1</span>],reverse=<span class="hljs-literal">True</span>)[:<span class="hljs-number">10</span>] <br>x,y=<span class="hljs-built_in">zip</span>(*top)<br>plt.bar(x,y)<br></code></pre></td></tr></table></figure>
<p>Draw popular stop words</p>
<p><img src="https://qiniu.hivan.me/picGo/20210831000213.png?imgNote"
alt="image-20210831000213620" /></p>
<h3 id="most-common-words">Most common words</h3>
<p>We can clearly see that in the news headlines, stop words such as
"to", "in" and "for" dominate.</p>
<p>So now that we know which stop words appear frequently in our text,
let's check which words other than these stop words appear
frequently.</p>
<p>We will use the counter function in the collection library to count
the occurrence of each word and store it in a list of tuples. This is a
very useful feature when we are dealing with word-level analysis in
natural language processing.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">counter=Counter(corpus)<br>most=counter.most_common()<br><br>x, y=[], []<br><span class="hljs-keyword">for</span> word,count <span class="hljs-keyword">in</span> most[:<span class="hljs-number">40</span>]:<br>    <span class="hljs-keyword">if</span> (word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop):<br>        x.append(word)<br>        y.append(count)<br>        <br>sns.barplot(x=y,y=x)<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831000244.png?imgNote"
alt="image-20210831000244040" /></p>
<p>Wow! In the past 15 years, "America", "Iraq" and "War" have dominated
the headlines.</p>
<p>"We" here may mean the United States or us (you and me). We are not a
stop word, but when we look at the other words in the picture, they are
all related to the United States-the Iraq War and "we" here may mean the
United States.</p>
<h2 id="ngram-analysis">Ngram analysis</h2>
<p>Ngram is a continuous sequence of n words. For example, "Riverbank",
"Three Musketeers" and so on. If the number of words is two, it is
called a double word. For 3 characters, it is called a trigram, and so
on.</p>
<p>Viewing the most common n-grams can give you a better understanding
of the context in which the word is used.</p>
<h3 id="bigram-analysis">Bigram analysis</h3>
<p>To build our vocabulary, we will use Countvectorizer. Countvectorizer
is a simple method for labeling, vectorizing and representing corpora in
an appropriate form. Can <a
target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?ref=hackernoon.com">be
found</a> in <a
target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?ref=hackernoon.com">sklearn.feature_engineering.text</a></p>
<p>Therefore, we will analyze the top news in all news headlines.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_top_ngram</span>(<span class="hljs-params">corpus, n=<span class="hljs-literal">None</span></span>):<br>    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)<br>    bag_of_words = vec.transform(corpus)<br>    sum_words = bag_of_words.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>) <br>    words_freq = [(word, sum_words[<span class="hljs-number">0</span>, idx]) <br>                  <span class="hljs-keyword">for</span> word, idx <span class="hljs-keyword">in</span> vec.vocabulary_.items()]<br>    words_freq =<span class="hljs-built_in">sorted</span>(words_freq, key = <span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> words_freq[:<span class="hljs-number">10</span>]<br><br>top_n_bigrams=get_top_ngram(df[<span class="hljs-string">&#x27;headline_text&#x27;</span>],<span class="hljs-number">2</span>)[:<span class="hljs-number">10</span>]<br>x,y=<span class="hljs-built_in">map</span>(<span class="hljs-built_in">list</span>,<span class="hljs-built_in">zip</span>(*top_n_bigrams))<br>sns.barplot(x=y,y=x)<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831000322.png?imgNote"
alt="image-20210831000322862" /></p>
<h3 id="trigram-analysis">Trigram analysis</h3>
<p>We can observe that dualisms such as "anti-war" and "killed" related
to war dominate the headlines.</p>
<p>How about triples?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">top_tri_grams=get_top_ngram(df[<span class="hljs-string">&#x27;headline_text&#x27;</span>],n=<span class="hljs-number">3</span>)<br>x,y=<span class="hljs-built_in">map</span>(<span class="hljs-built_in">list</span>,<span class="hljs-built_in">zip</span>(*top_tri_grams))<br>sns.barplot(x=y,y=x)<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831000347.png?imgNote"
alt="image-20210831000347490" /></p>
<p>We can see that many of these hexagrams are a combination of "face
the court" and "anti-war protest." This means that we should spend some
effort on data cleaning to see if we can combine these synonyms into a
clean token.</p>
<h2 id="topic-modelling">Topic modelling</h2>
<h3 id="use-pyldavis-for-topic-modeling-exploration">Use pyLDAvis for
topic modeling exploration</h3>
<p>Topic modeling is the process of using unsupervised learning
techniques to extract the main topics that appear in the document
set.</p>
<p><a
target="_blank" rel="noopener" href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158?ref=hackernoon.com">Latent
Dirichlet Allocation</a> (LDA) is an easy-to-use and efficient topic
modeling model. Each document is represented by a topic distribution,
and each topic is represented by a word distribution.</p>
<p>Once the documents are classified into topics, you can delve into the
data for each topic or topic group.</p>
<p>But before entering topic modeling, we must do some preprocessing of
the data. we will:</p>
<p>Tokenization: The process of converting sentences into tokens or word
lists. remove stopwordslemmatize: Reduce the deformed form of each word
to a common base or root. Convert to word bag: word bag is a dictionary
where the key is the word (or ngram/tokens) and the value is the number
of times each word appears in the corpus.</p>
<p>With NLTK, you can easily tokenize and formalize:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> nltk<br>nltk.download(<span class="hljs-string">&#x27;punkt&#x27;</span>)<br>nltk.download(<span class="hljs-string">&#x27;wordnet&#x27;</span>)<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">[nltk_data] Downloading package punkt to /Users/xx/nltk_data...</span><br><span class="hljs-string">[nltk_data]   Package punkt is already up-to-date!</span><br><span class="hljs-string">[nltk_data] Downloading package wordnet to /Users/xx/nltk_data...</span><br><span class="hljs-string">[nltk_data]   Unzipping corpora/wordnet.zip.</span><br><span class="hljs-string">True</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_news</span>(<span class="hljs-params">df</span>):<br>    corpus=[]<br>    stem=PorterStemmer()<br>    lem=WordNetLemmatizer()<br>    <span class="hljs-keyword">for</span> news <span class="hljs-keyword">in</span> df[<span class="hljs-string">&#x27;headline_text&#x27;</span>]:<br>        words=[w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> word_tokenize(news) <span class="hljs-keyword">if</span> (w <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop)]<br>        <br>        words=[lem.lemmatize(w) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> words <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(w)&gt;<span class="hljs-number">2</span>]<br>        <br>        corpus.append(words)<br>    <span class="hljs-keyword">return</span> corpus<br>  <br>corpus = preprocess_news(df)<br><br><span class="hljs-comment"># Now, let&#x27;s use gensim to create a bag of words model</span><br>dic=gensim.corpora.Dictionary(corpus)<br>bow_corpus = [dic.doc2bow(doc) <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> corpus]<br><br><span class="hljs-comment"># We can finally create the LDA model:</span><br>lda_model =  gensim.models.LdaMulticore(bow_corpus, <br>                                   num_topics = <span class="hljs-number">4</span>, <br>                                   id2word = dic,                                    <br>                                   passes = <span class="hljs-number">10</span>,<br>                                   workers = <span class="hljs-number">2</span>)<br><br>lda_model.show_topics()<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">[(0,</span><br><span class="hljs-string">  &#x27;0.010*&quot;say&quot; + 0.007*&quot;cup&quot; + 0.006*&quot;war&quot; + 0.005*&quot;world&quot; + 0.005*&quot;back&quot; + 0.005*&quot;plan&quot; + 0.005*&quot;green&quot; + 0.004*&quot;win&quot; + 0.004*&quot;woman&quot; + 0.004*&quot;new&quot;&#x27;),</span><br><span class="hljs-string"> (1,</span><br><span class="hljs-string">  &#x27;0.010*&quot;govt&quot; + 0.009*&quot;war&quot; + 0.009*&quot;new&quot; + 0.007*&quot;may&quot; + 0.005*&quot;sars&quot; + 0.005*&quot;call&quot; + 0.005*&quot;protest&quot; + 0.005*&quot;boost&quot; + 0.005*&quot;group&quot; + 0.004*&quot;hospital&quot;&#x27;),</span><br><span class="hljs-string"> (2,</span><br><span class="hljs-string">  &#x27;0.018*&quot;police&quot; + 0.015*&quot;baghdad&quot; + 0.014*&quot;man&quot; + 0.005*&quot;missing&quot; + 0.005*&quot;claim&quot; + 0.005*&quot;court&quot; + 0.005*&quot;australia&quot; + 0.004*&quot;move&quot; + 0.004*&quot;murder&quot; + 0.004*&quot;charged&quot;&#x27;),</span><br><span class="hljs-string"> (3,</span><br><span class="hljs-string">  &#x27;0.030*&quot;iraq&quot; + 0.015*&quot;war&quot; + 0.007*&quot;iraqi&quot; + 0.007*&quot;council&quot; + 0.006*&quot;troop&quot; + 0.005*&quot;killed&quot; + 0.004*&quot;crash&quot; + 0.004*&quot;soldier&quot; + 0.004*&quot;open&quot; + 0.004*&quot;say&quot;&#x27;)]</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>Theme 0 represents things related to the Iraq war and the police.
Theme 3 shows Australia's involvement in the Iraq War.</p>
<p>You can print all the topics and try to understand them, but there
are tools that can help you run this data exploration more effectively.
pyLDAvis is such a tool, it can interactively visualize the results of
LDA.</p>
<h3 id="visualize-the-topics">Visualize the topics</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">pyLDAvis.enable_notebook()<br>vis = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dic)<br>vis<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831000603.png?imgNote"
alt="image-20210831000602995" /></p>
<p>On the left, the area of each circle represents the importance of the
topic relative to the corpus. Because there are four themes, we have
four circles.</p>
<p>The distance between the center of the circle indicates the
similarity between themes. Here you can see that Topic 3 and Topic 4
overlap, which indicates that the themes are more similar. On the right,
the histogram of each topic shows the top 30 related words. For example,
in topic 1, the most relevant words are "police", "new", "may", "war",
etc.</p>
<p>Therefore, in our case, we can see many war-related words and topics
in the news headlines.</p>
<h3 id="wordclouds">Wordclouds</h3>
<p>Wordcloud is a great way to represent text data. The size and color
of each word appearing in the word cloud indicate its frequency or
importance.</p>
<p>It is easy to create a <a
target="_blank" rel="noopener" href="https://amueller.github.io/word_cloud/index.html?ref=hackernoon.com">wordcloud</a>
<a
target="_blank" rel="noopener" href="https://amueller.github.io/word_cloud/index.html?ref=hackernoon.com">using
python</a>, but we need to provide data in the form of a corpus.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">stopwords = <span class="hljs-built_in">set</span>(STOPWORDS)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">show_wordcloud</span>(<span class="hljs-params">data, title = <span class="hljs-literal">None</span></span>):<br>    wordcloud = WordCloud(<br>        background_color=<span class="hljs-string">&#x27;white&#x27;</span>,<br>        stopwords=stopwords,<br>        max_words=<span class="hljs-number">100</span>,<br>        max_font_size=<span class="hljs-number">30</span>, <br>        scale=<span class="hljs-number">3</span>,<br>        random_state=<span class="hljs-number">1</span> <br>        )<br>    <br>    wordcloud=wordcloud.generate(<span class="hljs-built_in">str</span>(data))<br><br>    fig = plt.figure(<span class="hljs-number">1</span>, figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">12</span>))<br>    plt.axis(<span class="hljs-string">&#x27;off&#x27;</span>)<br> <br>    plt.imshow(wordcloud)<br>    plt.show()<br>    <br>show_wordcloud(corpus)<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831000636.png?imgNote"
alt="image-20210831000635924" /></p>
<p>Similarly, you can see that terms related to war are highlighted,
indicating that these words often appear in news headlines.</p>
<p>There are many parameters that can be adjusted. Some of the most
famous are:</p>
<p>stopwords: stop a group of words appearing in the image. max_words:
Indicates the maximum number of words to be displayed. max_font_size:
Maximum font size.</p>
<p>There are many other options to create beautiful word clouds. For
more detailed information, you can refer to here.</p>
<h2 id="text-sentiment">Text sentiment</h2>
<p>Sentiment analysis is a very common natural language processing task
in which we determine whether the text is positive, negative or neutral.
This is very useful for finding sentiments related to comments and
comments, allowing us to gain some valuable insights from text data.</p>
<p>There are many projects that can help you use python for sentiment
analysis. I personally like <a
target="_blank" rel="noopener" href="https://github.com/sloria/TextBlob?ref=hackernoon.com">TextBlob</a>
and <a
target="_blank" rel="noopener" href="https://github.com/cjhutto/vaderSentiment?ref=hackernoon.com">Vader
Sentiment.</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> textblob <span class="hljs-keyword">import</span> TextBlob<br>TextBlob(<span class="hljs-string">&#x27;100 people killed in Iraq&#x27;</span>).sentiment<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Sentiment(polarity=-0.2, subjectivity=0.0)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="textblob">Textblob</h3>
<p>Textblob is a python library built on top of nltk. It has been around
for a while and is very easy to use.</p>
<p>The sentiment function of TextBlob returns two attributes:</p>
<p>Polarity: It is a floating-point number in the range of [-1,1], where
1 means a positive statement and -1 means a negative statement.
Subjectivity: refers to how personal opinions and feelings affect
someone’s judgment. The subjectivity is expressed as a floating point
value with a range of [0,1].</p>
<p>I will run this feature on news headlines.</p>
<p>TextBlob claims that the text "100 people killed in Iraq" is
negative, not a view or feeling, but a statement of fact. I think we can
agree to TextBlob here.</p>
<p>Now that we know how to calculate these sentiment scores, we can use
histograms to visualize them and explore the data further.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">polarity</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-keyword">return</span> TextBlob(text).sentiment.polarity<br><br>df[<span class="hljs-string">&#x27;polarity_score&#x27;</span>]=df[<span class="hljs-string">&#x27;headline_text&#x27;</span>].\<br>   apply(<span class="hljs-keyword">lambda</span> x : polarity(x))<br>df[<span class="hljs-string">&#x27;polarity_score&#x27;</span>].hist()<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831000735.png?imgNote"
alt="image-20210831000735233" /></p>
<p>You will see that the polarity is mainly between 0.00 and 0.20. This
shows that most news headlines are neutral.</p>
<p>Let's categorize news as negative, positive, and neutral based on the
scores for a more in-depth study.</p>
<h3 id="postive-negative-or-neutral">Postive , Negative or Neutral
?</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sentiment</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">if</span> x&lt;<span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;neg&#x27;</span><br>    <span class="hljs-keyword">elif</span> x==<span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;neu&#x27;</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;pos&#x27;</span><br>    <br>df[<span class="hljs-string">&#x27;polarity&#x27;</span>]=df[<span class="hljs-string">&#x27;polarity_score&#x27;</span>].\<br>   <span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: sentiment(x))<br>  <br>plt.bar(df.polarity.value_counts().index,<br>        df.polarity.value_counts())<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831000804.png?imgNote"
alt="image-20210831000804842" /></p>
<p>Yes, 70% of news is neutral, only 18% of positive news and 11% of
negative news.</p>
<p>Let's look at the positive and negative headlines.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">df[df[<span class="hljs-string">&#x27;polarity&#x27;</span>]==<span class="hljs-string">&#x27;neg&#x27;</span>][<span class="hljs-string">&#x27;headline_text&#x27;</span>].head(<span class="hljs-number">5</span>)<br><br><span class="hljs-comment"># output</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">7     aussie qualifier stosur wastes four memphis match</span><br><span class="hljs-string">23               carews freak goal leaves roma in ruins</span><br><span class="hljs-string">28     council chief executive fails to secure position</span><br><span class="hljs-string">34                   dargo fire threat expected to rise</span><br><span class="hljs-string">40        direct anger at govt not soldiers crean urges</span><br><span class="hljs-string">Name: headline_text, dtype: object</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="vader">Vader</h3>
<p>The next library we are going to discuss is VADER. Vader is better at
detecting negative emotions. It is very useful in the context of social
media text sentiment analysis.</p>
<p>The VADER or Valence Aware dictionary and sentiment reasoner is an
open source sentiment analyzer pre-built library based on
rules/dictionaries and is protected by the MIT license.</p>
<p>The VADER sentiment analysis class returns a dictionary that contains
the possibility that the text appears positive, negative, and neutral.
Then, we can filter and select the emotion with the highest
probability.</p>
<p>We will use VADER to perform the same analysis and check if the
difference is large.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.sentiment.vader <span class="hljs-keyword">import</span> SentimentIntensityAnalyzer<br><br>nltk.download(<span class="hljs-string">&#x27;vader_lexicon&#x27;</span>)<br>sid = SentimentIntensityAnalyzer()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_vader_score</span>(<span class="hljs-params">sent</span>):<br>    <span class="hljs-comment"># Polarity score returns dictionary</span><br>    ss = sid.polarity_scores(sent)<br>    <span class="hljs-comment">#return ss</span><br>    <span class="hljs-keyword">return</span> np.argmax(<span class="hljs-built_in">list</span>(ss.values())[:-<span class="hljs-number">1</span>])<br>  <br> <br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">[nltk_data] Downloading package vader_lexicon to</span><br><span class="hljs-string">[nltk_data]     /Users/xx/nltk_data...</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>df[<span class="hljs-string">&#x27;polarity&#x27;</span>]=df[<span class="hljs-string">&#x27;headline_text&#x27;</span>].\<br>    <span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: get_vader_score(x))<br>polarity=df[<span class="hljs-string">&#x27;polarity&#x27;</span>].replace(&#123;<span class="hljs-number">0</span>:<span class="hljs-string">&#x27;neg&#x27;</span>,<span class="hljs-number">1</span>:<span class="hljs-string">&#x27;neu&#x27;</span>,<span class="hljs-number">2</span>:<span class="hljs-string">&#x27;pos&#x27;</span>&#125;)<br><br>plt.bar(polarity.value_counts().index,<br>        polarity.value_counts())<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831000924.png?imgNote"
alt="image-20210831000924225" /></p>
<p>Yes, the distribution is slightly different. There are even more
headlines classified as neutral 85%, and the number of negative news
headlines has increased (to 13%).</p>
<h2 id="named-entity-recognition">Named Entity Recognition</h2>
<p>Named entity recognition is an information extraction method in which
entities existing in the text are classified into predefined entity
types, such as "person", "location", "organization" and so on. By using
NER, we can gain insight into the entities that exist in a given text
data set of entity types.</p>
<p>Let us consider an example of a news article.</p>
<p>In the above news, the named entity recognition model should be able
to recognize Entities, such as RBI as an organization, Mumbai and India
as Places, etc.</p>
<p>There are three standard libraries for named entity recognition:</p>
<ul>
<li><a
target="_blank" rel="noopener" href="https://nlp.stanford.edu/software/CRF-NER.shtml?ref=hackernoon.com">Stanford
Nell</a></li>
<li><a target="_blank" rel="noopener" href="https://spacy.io/?ref=hackernoon.com">space</a></li>
<li><a target="_blank" rel="noopener" href="https://www.nltk.org/?ref=hackernoon.com">NLTK</a></li>
</ul>
<p><strong>I will use spaCy</strong>, which is an open source library
for advanced natural language processing tasks. It is written in Cython
and is known for its industrial applications. In addition to NER,
<strong>spaCy also provides many other functions, such as pos mark, word
to vector conversion, etc. </strong></p>
<p><a
target="_blank" rel="noopener" href="https://spacy.io/api/annotation?ref=hackernoon.com#section-named-entities">SpaCy’s
Named Entity Recognition</a> has been published in <a
href="https://catalog.ldc.upenn.edu%20/LDC2013T19?ref=hackernoon.com">OntoNotes
5</a> has been trained on the corpus and supports the following entity
types</p>
<p>There are three kinds of <a
target="_blank" rel="noopener" href="https://spacy.io/models/en/?ref=hackernoon.com">pre-trained models
for English</a> in SpaCy. I will use <em>en_core_web_sm</em> to complete
our task, but you can try other models.</p>
<p>To use it, we must first download it:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># !python -m spacy download en_core_web_sm</span><br><br><span class="hljs-comment"># Now we can initialize the language model:</span><br><br><span class="hljs-keyword">import</span> spacy<br><span class="hljs-keyword">from</span> spacy <span class="hljs-keyword">import</span> displacy<br><span class="hljs-keyword">import</span> en_core_web_sm<br><br>nlp = en_core_web_sm.load()<br><br><span class="hljs-comment"># nlp = spacy.load(&quot;en_core_web_sm&quot;)</span><br><br><span class="hljs-comment"># One of the advantages of Spacy is that we only need to apply the nlp function once, and the entire background pipeline will return the objects we need</span><br><br>doc=nlp(<span class="hljs-string">&#x27;India and Iran have agreed to boost the economic \</span><br><span class="hljs-string">viability of the strategic Chabahar port through various measures, \</span><br><span class="hljs-string">including larger subsidies to merchant shipping firms using the facility, \</span><br><span class="hljs-string">people familiar with the development said on Thursday.&#x27;</span>)<br><br>[(x.text,x.label_) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> doc.ents]<br><br> <br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">[(&#x27;India&#x27;, &#x27;GPE&#x27;), (&#x27;Iran&#x27;, &#x27;GPE&#x27;), (&#x27;Chabahar&#x27;, &#x27;GPE&#x27;), (&#x27;Thursday&#x27;, &#x27;DATE&#x27;)]</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>We can see that India and Iran are confirmed as geographic locations
(GPE), Chabahar is confirmed as a person, and Thursday is confirmed as a
date.</p>
<p>We can also use the display module in spaCy to visualize the
output.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> spacy <span class="hljs-keyword">import</span> displacy<br><br>displacy.render(doc, style=<span class="hljs-string">&#x27;ent&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831001008.png?imgNote"
alt="image-20210831001008590" /></p>
<p>This can make sentences with recognized entities look very neat, and
each entity type is marked with a different color.</p>
<p>Now that we know how to perform NER, we can further explore the data
by performing various visualizations on the named entities extracted
from the data set.</p>
<p>First, we will run named entity recognition on news headlines and
store entity types.</p>
<h3 id="ner-analysis">NER Analysis</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ner</span>(<span class="hljs-params">text</span>):<br>    doc=nlp(text)<br>    <span class="hljs-keyword">return</span> [X.label_ <span class="hljs-keyword">for</span> X <span class="hljs-keyword">in</span> doc.ents]<br>  <br>ent=df[<span class="hljs-string">&#x27;headline_text&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x : ner(x))<br>ent=[x <span class="hljs-keyword">for</span> sub <span class="hljs-keyword">in</span> ent <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> sub]<br>counter=Counter(ent)<br>count=counter.most_common()<br><br><span class="hljs-comment"># Now, we can visualize the entity frequency:</span><br>x,y=<span class="hljs-built_in">map</span>(<span class="hljs-built_in">list</span>,<span class="hljs-built_in">zip</span>(*count))<br>sns.barplot(x=y,y=x)<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831001044.png?imgNote"
alt="image-20210831001044045" /></p>
<p>Now we can see that GPE and ORG dominate the headlines, followed by
the PERSON entity.</p>
<p>We can also visualize the most common tokens for each entity. Let's
check which places appear the most in news headlines.</p>
<h3 id="most-common-gpe">Most common GPE</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ner</span>(<span class="hljs-params">text,ent=<span class="hljs-string">&quot;GPE&quot;</span></span>):<br>    doc=nlp(text)<br>    <span class="hljs-keyword">return</span> [X.text <span class="hljs-keyword">for</span> X <span class="hljs-keyword">in</span> doc.ents <span class="hljs-keyword">if</span> X.label_ == ent]<br>  <br>gpe=df[<span class="hljs-string">&#x27;headline_text&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: ner(x,<span class="hljs-string">&quot;GPE&quot;</span>))<br>gpe=[i <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> gpe <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x]<br>counter=Counter(gpe)<br><br>x,y=<span class="hljs-built_in">map</span>(<span class="hljs-built_in">list</span>,<span class="hljs-built_in">zip</span>(*counter.most_common(<span class="hljs-number">10</span>)))<br>sns.barplot(y,x)<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831001111.png?imgNote"
alt="image-20210831001111535" /></p>
<p>I think we can confirm the fact that "America" means America in news
headlines. Let's also find the most common names that appear on news
headlines.</p>
<h3 id="most-common-person">Most common person</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">per=df[<span class="hljs-string">&#x27;headline_text&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: ner(x,<span class="hljs-string">&quot;PERSON&quot;</span>))<br>per=[i <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> per <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x]<br>counter=Counter(per)<br><br>x,y=<span class="hljs-built_in">map</span>(<span class="hljs-built_in">list</span>,<span class="hljs-built_in">zip</span>(*counter.most_common(<span class="hljs-number">10</span>)))<br>sns.barplot(y,x)<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831001135.png?imgNote"
alt="image-20210831001135765" /></p>
<p>Saddam Hussein and George Bush served as presidents of Iraq and the
United States during the war. In addition, we can see that the model is
far from perfect to classify "vic govt" or "nsw govt" as individuals
rather than government agencies.</p>
<h3 id="pos-tagging">Pos tagging</h3>
<p>Use nltk for all parts of speech markup, but there are other
libraries that can do the job well (spaacy, textblob).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> nltk<br>nltk.download(<span class="hljs-string">&#x27;averaged_perceptron_tagger&#x27;</span>)<br><br>sentence=<span class="hljs-string">&quot;The greatest comeback stories in 2019&quot;</span><br>tokens=word_tokenize(sentence)<br>nltk.pos_tag(tokens)<br><br><span class="hljs-comment"># Notice:</span><br><span class="hljs-comment"># You can also use the spacy.displacy module to visualize the sentence part of the speech and its dependency graph.</span><br><br>doc = nlp(<span class="hljs-string">&#x27;The greatest comeback stories in 2019&#x27;</span>)<br>displacy.render(doc, style=<span class="hljs-string">&#x27;dep&#x27;</span>, jupyter=<span class="hljs-literal">True</span>, options=&#123;<span class="hljs-string">&#x27;distance&#x27;</span>: <span class="hljs-number">90</span>&#125;)<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831001212.png?imgNote"
alt="image-20210831001212360" /></p>
<p>We can observe various dependency labels here. For example, the DET
tag indicates the relationship between the word "the" and the noun
"stories".</p>
<p>You can check the list of dependency labels and their meanings <a
target="_blank" rel="noopener" href="https://universaldependencies.org/u/dep/index.html?ref=hackernoon.com">here</a>.</p>
<p>Okay, now that we know what a POS tag is, let's use it to explore the
title data set.</p>
<h3 id="analysing-pos-tags">Analysing pos tags</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pos</span>(<span class="hljs-params">text</span>):<br>    pos=nltk.pos_tag(word_tokenize(text))<br>    pos=<span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">list</span>,<span class="hljs-built_in">zip</span>(*pos)))[<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">return</span> pos<br>  <br>tags=df[<span class="hljs-string">&#x27;headline_text&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x : pos(x))<br>tags=[x <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> tags <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> l]<br>counter=Counter(tags)<br>x,y=<span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">list</span>,<span class="hljs-built_in">zip</span>(*counter.most_common(<span class="hljs-number">7</span>))))<br><br>sns.barplot(x=y,y=x)<br></code></pre></td></tr></table></figure>
<p><img src="https://qiniu.hivan.me/picGo/20210831001251.png?imgNote"
alt="image-20210831001251251" /></p>
<p>We can clearly see that nouns (NN) dominate in news headlines,
followed by adjectives (JJ). This is typical for news reports, and for
art forms, higher adjective (ADJ) frequencies may happen a lot.</p>
<p>You can investigate this in more depth by investigating the most
common singular nouns in news headlines. Let us find out.</p>
<p>Nouns such as "war", "Iraq", and "person" dominate the news
headlines. You can use the above functions to visualize and check other
parts of the voice.</p>
<h3 id="most-common-nouns">Most common Nouns</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_adjs</span>(<span class="hljs-params">text</span>):<br>    adj=[]<br>    pos=nltk.pos_tag(word_tokenize(text))<br>    <span class="hljs-keyword">for</span> word,tag <span class="hljs-keyword">in</span> pos:<br>        <span class="hljs-keyword">if</span> tag==<span class="hljs-string">&#x27;NN&#x27;</span>:<br>            adj.append(word)<br>    <span class="hljs-keyword">return</span> adj<br><br><br>words=df[<span class="hljs-string">&#x27;headline_text&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x : get_adjs(x))<br>words=[x <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> words <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> l]<br>counter=Counter(words)<br><br>x,y=<span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">list</span>,<span class="hljs-built_in">zip</span>(*counter.most_common(<span class="hljs-number">7</span>))))<br>sns.barplot(x=y,y=x)<br></code></pre></td></tr></table></figure>
<h3 id="dependency-graph">Dependency graph</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">doc = nlp(<span class="hljs-string">&#x27;She sells seashells by  the seashore&#x27;</span>)<br>displacy.render(doc, style=<span class="hljs-string">&#x27;dep&#x27;</span>, jupyter=<span class="hljs-literal">True</span>, options=&#123;<span class="hljs-string">&#x27;distance&#x27;</span>: <span class="hljs-number">90</span>&#125;)<br></code></pre></td></tr></table></figure>
<h2 id="text-readability">Text readability</h2>
<h3 id="textstat">Textstat</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> textstat <span class="hljs-keyword">import</span> flesch_reading_ease<br>df[<span class="hljs-string">&#x27;headline_text&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x : flesch_reading_ease(x)).hist()<br></code></pre></td></tr></table></figure>
<h3 id="complex-headlines">complex headlines?</h3>
<p>Almost all readability scores exceed 60. This means that an average
of 11-year-old students can read and understand news headlines. Let's
check all news headlines with a readability score below 5.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">x=[i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(reading)) <span class="hljs-keyword">if</span> reading[i]&lt;<span class="hljs-number">5</span>]<br><br> <br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">rror loading preloads:</span><br><span class="hljs-string">Failed to fetch dynamically imported module: https://file+.vscode-resource.vscode-webview.net/Users/xx/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/datascience-ui/errorRenderer/errorRenderer.js</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>news.iloc[x][<span class="hljs-string">&#x27;headline_text&#x27;</span>].head()<br><br> <br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Error loading preloads:</span><br><span class="hljs-string">Failed to fetch dynamically imported module: https://file+.vscode-resource.vscode-webview.net/Users/xx/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/datascience-ui/errorRenderer/errorRenderer.js</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="final-thoughts">Final thoughts</h3>
<p>In this article, we discussed and implemented various exploratory
data analysis methods for text data. Some are common and little known,
but all of them can be an excellent addition to your data exploration
toolkit.</p>
<p>Hope you will find some of them useful for your current and future
projects.</p>
<p>To make data exploration easier, I created a "exploratory data
analysis of natural language processing templates", which you can use
for your work.</p>
<p>In addition, you may have seen that for each chart in this article,
there is a code snippet to create it. Just click the button below the
chart.</p>
<p>Happy exploring!</p>
<p>From:
https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Initial exploration of machine learning</p><p><a href="https://hivan.me/example_02/">https://hivan.me/example_02/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hivan Du</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-09-02</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2023-06-02</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=6479444288ae9600196fa98e&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="https://afdian.net/item/72907364008511ee904852540025c377" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://qiniu.hivan.me/picGo/20230601221633.jpeg" alt="支付宝"></span></a><a class="button donate" href="https://www.buymeacoffee.com/hivandu" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="https://patreon.com/user?u=89473430" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="doo@hivan.me"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://qiniu.hivan.me/IMG_4603.JPG" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/example_07/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Advanced Deep Learning</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/example_03/"><span class="level-item">Machine Learning Part-01</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://hivan.me/example_02/';
            this.page.identifier = 'example_02/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'hivan' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://www.gravatar.com/avatar/bdff168cf8a71c11d2712a1679a00c54?s=128" alt="茶桁"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">茶桁</p><p class="is-size-6 is-block">AI游民</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shang Hai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">164</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">21</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hivandu" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hivan"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/hivan"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com/hivan"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4NzE4MDQzMg==&amp;action=getalbum&amp;album_id=2932504849574543360&amp;scene=173&amp;from_msgid=2648747980&amp;from_itemidx=1&amp;count=3&amp;nolastread=1&amp;token=1758883909&amp;lang=zh_CN#wechat_redirect"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.zhihu.com/column/c_1424326166602178560" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">塌缩的奇点</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li><li><a class="level is-mobile" href="https://www.zhihu.com/column/hivandu" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">茶桁-知乎</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI%E7%A7%98%E7%B1%8D/"><span class="level-start"><span class="level-item">AI秘籍</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI%E7%A7%98%E7%B1%8D/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="level-start"><span class="level-item">从零开始接触人工智能大模型</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-04T08:13:00.000Z">2023-08-04</time></p><p class="title"><a href="/python-Built-in-functions/">7. Python的内置函数</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Python/">Python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-03T06:07:17.000Z">2023-08-03</time></p><p class="title"><a href="/Higher-order-functions/">6. Python的高阶函数</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Python/">Python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-02T11:23:14.000Z">2023-08-02</time></p><p class="title"><a href="/2023_8_3_Kalman/">卡尔曼滤波器的非数学介绍</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-01T15:43:14.000Z">2023-08-01</time></p><p class="title"><a href="/Modular-programming/">5. 模块化编程</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Python/">Python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-01T08:15:43.000Z">2023-08-01</time></p><p class="title"><a href="/Python-process-control/">4. Python的流程控制</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Python/">Python</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a><p class="is-size-7"><span>&copy; 2023 Hivan Du</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>