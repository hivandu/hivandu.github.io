<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Initial exploration of machine learning - 茶桁.MAMT</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="茶桁.MAMT"><meta name="msapplication-TileImage" content="https://qiniu.hivan.me/picGo/20230601174411.png?imgNote"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="茶桁.MAMT"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="The code address of this article is: Example 02   The source code is in ipynb format, and the output content can be viewed."><meta property="og:type" content="blog"><meta property="og:title" content="Initial exploration of machine learning"><meta property="og:url" content="https://hivan.me/example_02/"><meta property="og:site_name" content="茶桁.MAMT"><meta property="og:description" content="The code address of this article is: Example 02   The source code is in ipynb format, and the output content can be viewed."><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210830234710.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210830234858.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210830235114.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210830235135.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210830235243.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210830235601.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000016.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000044.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000111.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000213.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000244.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000322.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000347.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000603.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000636.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000735.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000804.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831000924.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831001008.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831001044.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831001111.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831001135.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831001212.png?imgNote"><meta property="og:image" content="https://qiniu.hivan.me/picGo/20210831001251.png?imgNote"><meta property="article:published_time" content="2021-09-02T12:59:46.685Z"><meta property="article:modified_time" content="2023-06-02T03:49:09.832Z"><meta property="article:author" content="Hivan Du"><meta property="article:tag" content="AI,人工智能,代码,大语言模型"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://qiniu.hivan.me/picGo/20210830234710.png?imgNote"><meta property="twitter:creator" content="@hivan"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hivan.me/example_02/"},"headline":"Initial exploration of machine learning","image":[],"datePublished":"2021-09-02T12:59:46.685Z","dateModified":"2023-06-02T03:49:09.832Z","author":{"@type":"Person","name":"Hivan Du"},"publisher":{"@type":"Organization","name":"茶桁.MAMT","logo":{"@type":"ImageObject","url":"https://hivan.me/img/logo.svg"}},"description":"The code address of this article is: Example 02   The source code is in ipynb format, and the output content can be viewed."}</script><link rel="canonical" href="https://hivan.me/example_02/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="茶桁.MAMT" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-09-02T12:59:46.685Z" title="9/2/2021, 8:59:46 PM">2021-09-02</time>发表</span></div></div><h1 class="title is-3 is-size-4-mobile">Initial exploration of machine learning</h1><div class="content"><blockquote>
<p>The code address of this article is: <a target="_blank" rel="noopener" href="https://github.com/hivandu/practise/blob/master/AI-basic/example_02.ipynb">Example 02</a></p>
</blockquote>
<blockquote>
<p>The source code is in ipynb format, and the output content can be viewed. <span id="more"></span> ## Gradient</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">k</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">3</span> * (k ** <span class="number">2</span>) + <span class="number">7</span> * k - <span class="number">10</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># -b / 2a = -7 / 6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">partial</span>(<span class="params">k</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">6</span> * k + <span class="number">7</span></span><br><span class="line"></span><br><span class="line">k = random.randint(-<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">alpha = <span class="number">1e-3</span> <span class="comment"># 0.001</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    k = k + (-<span class="number">1</span>) * partial(k) *alpha</span><br><span class="line">    <span class="built_in">print</span>(k, loss(k))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># out</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">7.959 124.32404299999999</span></span><br><span class="line"><span class="string">-7.918246 122.66813714954799</span></span><br><span class="line"><span class="string">show more (open the raw output data in a text editor) ...</span></span><br><span class="line"><span class="string">-1.1833014444482555 -14.082503185837805</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="cutting-problem">Cutting Problem</h2>
<p>All the dynamic programming:</p>
<ol type="1">
<li>sub-problems</li>
<li>Overlapping sub-problems</li>
<li>parse solution</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"><span class="comment"># least recent used</span></span><br><span class="line"></span><br><span class="line">prices = [<span class="number">1</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">17</span>, <span class="number">17</span>, <span class="number">20</span>, <span class="number">24</span>, <span class="number">30</span>, <span class="number">33</span>]</span><br><span class="line">complete_price = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(prices): complete_price[i+<span class="number">1</span>] = p</span><br><span class="line">  </span><br><span class="line">solution = &#123;&#125;</span><br><span class="line"></span><br><span class="line">cache = &#123;&#125;</span><br><span class="line"><span class="comment">#&lt;- if when n .... is huge. size(cache)</span></span><br><span class="line"><span class="comment"># keep most important information.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@lru_cache(<span class="params">maxsize=<span class="number">2</span>**<span class="number">10</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">r</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="comment"># a very classical dynamic programming problem</span></span><br><span class="line">    <span class="comment"># if n in cache: return cache[n]</span></span><br><span class="line"></span><br><span class="line">    candidates = [(complete_price[n], (n, <span class="number">0</span>))] + \</span><br><span class="line">                 [(r(i) + r(n-i), (i, n - i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n)]</span><br><span class="line"></span><br><span class="line">    optimal_price, split = <span class="built_in">max</span>(candidates)</span><br><span class="line"></span><br><span class="line">    solution[n] = split</span><br><span class="line"></span><br><span class="line">    <span class="comment"># cache[n] = optimal_price</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> optimal_price</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_solution</span>(<span class="params">n, cut_solution</span>):</span><br><span class="line">    left, right = cut_solution[n]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> left == <span class="number">0</span> <span class="keyword">or</span> right == <span class="number">0</span>: <span class="keyword">return</span> [left+right, ]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> parse_solution(left, cut_solution) + parse_solution(right, cut_solution)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="built_in">print</span>(r(<span class="number">19</span>))</span><br><span class="line">    <span class="built_in">print</span>(parse_solution(<span class="number">19</span>, solution))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># out</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">55</span></span><br><span class="line"><span class="string">[11, 6, 2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="dynamic">Dynamic</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="keyword">from</span> icecream <span class="keyword">import</span> ic</span><br><span class="line"></span><br><span class="line">original_price = [<span class="number">1</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">17</span>,<span class="number">17</span>,<span class="number">20</span>,<span class="number">24</span>,<span class="number">30</span>,<span class="number">33</span>]</span><br><span class="line">price = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(original_price):</span><br><span class="line">    price[i+<span class="number">1</span>] = p</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">memo</span>(<span class="params">func</span>):</span><br><span class="line">    cache = &#123;&#125;</span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_wrap</span>(<span class="params">n</span>):</span><br><span class="line">        <span class="keyword">if</span> n <span class="keyword">in</span> cache: result = cache[n]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            result = func(n)</span><br><span class="line">            cache[n] = result</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> _wrap</span><br><span class="line">  </span><br><span class="line"><span class="meta">@memo</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">r</span>(<span class="params">n</span>):</span><br><span class="line">    max_price, split_point = <span class="built_in">max</span>(</span><br><span class="line">        [(price[n],<span class="number">0</span>)] + [(r(i) + r(n-i), i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n)], key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>]</span><br><span class="line">    )</span><br><span class="line">    solution[n]  = (split_point, n-split_point)</span><br><span class="line">    <span class="keyword">return</span> max_price</span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">not_cut</span>(<span class="params">split</span>): <span class="keyword">return</span> split == <span class="number">0</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_solution</span>(<span class="params">target_length, revenue_solution</span>):</span><br><span class="line">    left, right = revenue_solution[target_length]</span><br><span class="line">    <span class="keyword">if</span> not_cut(left): <span class="keyword">return</span> [right]</span><br><span class="line">    <span class="keyword">return</span> parse_solution(left, revenue_solution) + parse_solution(right, revenue_solution)</span><br><span class="line">  </span><br><span class="line">solution = &#123;&#125;</span><br><span class="line">r(<span class="number">50</span>)</span><br><span class="line">ic(parse_solution(<span class="number">20</span>,solution))</span><br><span class="line">ic(parse_solution(<span class="number">19</span>,solution))</span><br><span class="line">ic(parse_solution(<span class="number">27</span>,solution))</span><br><span class="line"></span><br><span class="line"><span class="comment"># out</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">ic| parse_solution(20,solution): [10, 10]</span></span><br><span class="line"><span class="string">ic| parse_solution(19,solution): [2, 6, 11]</span></span><br><span class="line"><span class="string">ic| parse_solution(27,solution): [6, 10, 11]</span></span><br><span class="line"><span class="string">[6, 10, 11]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="gradient-descent">Gradient descent</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> icecream <span class="keyword">import</span> ic</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">10</span> * x**<span class="number">2</span> + <span class="number">32</span>*x + <span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">20</span> *x + <span class="number">32</span></span><br><span class="line">  </span><br><span class="line">x = np.linspace(-<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">steps = []</span><br><span class="line">x_star = random.choice(x)</span><br><span class="line">alpha = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    x_star = x_star + -<span class="number">1</span>*gradient(x_star)*alpha</span><br><span class="line">    steps.append(x_star)</span><br><span class="line"></span><br><span class="line">    ic(x_star, func(x_star))</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(x, func(x))</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">ic| x_star: 9.368, func(x_star): 1186.3702400000002</span></span><br><span class="line"><span class="string">ic| x_star: 9.14864, func(x_star): 1138.732618496</span></span><br><span class="line"><span class="string">show more (open the raw output data in a text editor) ...</span></span><br><span class="line"><span class="string">ic| x_star: -0.1157435825983131, func(x_star): 5.430171125980905</span></span><br><span class="line"><span class="string">[&lt;matplotlib.lines.Line2D at 0x7fd6d19545d0&gt;]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, s <span class="keyword">in</span> <span class="built_in">enumerate</span>(steps):</span><br><span class="line">    ax.annotate(<span class="built_in">str</span>(i+<span class="number">1</span>), (s, func(s)))</span><br><span class="line">    </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210830234710.png?imgNote" alt="image-20210830234709856" /><figcaption aria-hidden="true">image-20210830234709856</figcaption>
</figure>
<h2 id="k-means-finding-centers">k-means-finding-centers</h2>
<h3 id="k-means">K-means</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl</span><br><span class="line"></span><br><span class="line">mpl.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;FangSong&#x27;</span>] <span class="comment"># Specify the default font</span></span><br><span class="line">mpl.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span> <span class="comment"># Solve the problem that the minus sign&#x27;-&#x27; is displayed as a square in the saved image</span></span><br><span class="line"></span><br><span class="line">coordination_source = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;name:&#x27;兰州&#x27;, geoCoord:[103.73, 36.03]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;嘉峪关&#x27;, geoCoord:[98.17, 39.47]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;西宁&#x27;, geoCoord:[101.74, 36.56]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;成都&#x27;, geoCoord:[104.06, 30.67]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;石家庄&#x27;, geoCoord:[114.48, 38.03]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;拉萨&#x27;, geoCoord:[102.73, 25.04]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;贵阳&#x27;, geoCoord:[106.71, 26.57]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;武汉&#x27;, geoCoord:[114.31, 30.52]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;郑州&#x27;, geoCoord:[113.65, 34.76]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;济南&#x27;, geoCoord:[117, 36.65]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;南京&#x27;, geoCoord:[118.78, 32.04]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;合肥&#x27;, geoCoord:[117.27, 31.86]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;杭州&#x27;, geoCoord:[120.19, 30.26]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;南昌&#x27;, geoCoord:[115.89, 28.68]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;福州&#x27;, geoCoord:[119.3, 26.08]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;广州&#x27;, geoCoord:[113.23, 23.16]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;长沙&#x27;, geoCoord:[113, 28.21]&#125;,</span></span><br><span class="line"><span class="string">//&#123;name:&#x27;海口&#x27;, geoCoord:[110.35, 20.02]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;沈阳&#x27;, geoCoord:[123.38, 41.8]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;长春&#x27;, geoCoord:[125.35, 43.88]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;哈尔滨&#x27;, geoCoord:[126.63, 45.75]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;太原&#x27;, geoCoord:[112.53, 37.87]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;西安&#x27;, geoCoord:[108.95, 34.27]&#125;,</span></span><br><span class="line"><span class="string">//&#123;name:&#x27;台湾&#x27;, geoCoord:[121.30, 25.03]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;北京&#x27;, geoCoord:[116.46, 39.92]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;上海&#x27;, geoCoord:[121.48, 31.22]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;重庆&#x27;, geoCoord:[106.54, 29.59]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;天津&#x27;, geoCoord:[117.2, 39.13]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;呼和浩特&#x27;, geoCoord:[111.65, 40.82]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;南宁&#x27;, geoCoord:[108.33, 22.84]&#125;,</span></span><br><span class="line"><span class="string">//&#123;name:&#x27;西藏&#x27;, geoCoord:[91.11, 29.97]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;银川&#x27;, geoCoord:[106.27, 38.47]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;乌鲁木齐&#x27;, geoCoord:[87.68, 43.77]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;香港&#x27;, geoCoord:[114.17, 22.28]&#125;,</span></span><br><span class="line"><span class="string">&#123;name:&#x27;澳门&#x27;, geoCoord:[113.54, 22.19]&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="feacutre-extractor">Feacutre Extractor</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">city_location = &#123;</span><br><span class="line">    <span class="string">&#x27;香港&#x27;</span>: (<span class="number">114.17</span>, <span class="number">22.28</span>)</span><br><span class="line">&#125;</span><br><span class="line">test_string = <span class="string">&quot;&#123;name:&#x27;兰州&#x27;, geoCoord:[103.73, 36.03]&#125;,&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;name:&#x27;(\w+)&#x27;,\s+geoCoord:\[(\d+.\d+),\s(\d+.\d+)\]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> coordination_source.split(<span class="string">&#x27;\n&#x27;</span>):</span><br><span class="line">    city_info = pattern.findall(line)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> city_info: <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># following: we find the city info</span></span><br><span class="line">    </span><br><span class="line">    city, long, lat = city_info[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    long, lat = <span class="built_in">float</span>(long), <span class="built_in">float</span>(lat)</span><br><span class="line">    </span><br><span class="line">    city_location[city] = (long, lat)</span><br><span class="line"></span><br><span class="line">city_location</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;&#x27;香港&#x27;: (114.17, 22.28),</span></span><br><span class="line"><span class="string"> &#x27;兰州&#x27;: (103.73, 36.03),</span></span><br><span class="line"><span class="string">show more (open the raw output data in a text editor) ...</span></span><br><span class="line"><span class="string"> &#x27;澳门&#x27;: (113.54, 22.19)&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">geo_distance</span>(<span class="params">origin, destination</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculate the Haversine distance.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    origin : tuple of float</span></span><br><span class="line"><span class="string">        (lat, long)</span></span><br><span class="line"><span class="string">    destination : tuple of float</span></span><br><span class="line"><span class="string">        (lat, long)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    distance_in_km : float</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; origin = (48.1372, 11.5756)  # Munich</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; destination = (52.5186, 13.4083)  # Berlin</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; round(distance(origin, destination), 1)</span></span><br><span class="line"><span class="string">    504.2</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    lon1, lat1 = origin</span><br><span class="line">    lon2, lat2 = destination</span><br><span class="line">    radius = <span class="number">6371</span>  <span class="comment"># km</span></span><br><span class="line"></span><br><span class="line">    dlat = math.radians(lat2 - lat1)</span><br><span class="line">    dlon = math.radians(lon2 - lon1)</span><br><span class="line">    a = (math.sin(dlat / <span class="number">2</span>) * math.sin(dlat / <span class="number">2</span>) +</span><br><span class="line">         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *</span><br><span class="line">         math.sin(dlon / <span class="number">2</span>) * math.sin(dlon / <span class="number">2</span>))</span><br><span class="line">    c = <span class="number">2</span> * math.atan2(math.sqrt(a), math.sqrt(<span class="number">1</span> - a))</span><br><span class="line">    d = radius * c</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure>
<h3 id="vector-distances">Vector Distances</h3>
<ul>
<li>余弦距离 Cosine Distance</li>
<li>欧几里得距离 Euclidean Distance</li>
<li>曼哈顿距离 Manhattan distance or Manhattan length</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># set plt, show chinese</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]  = [<span class="string">&#x27;Arial Unicode MS&#x27;</span>]</span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]  = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">city_graph = nx.Graph()</span><br><span class="line">city_graph.add_nodes_from(<span class="built_in">list</span>(city_location.keys()))</span><br><span class="line">nx.draw(city_graph, city_location, with_labels=<span class="literal">True</span>, node_size=<span class="number">30</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210830234858.png?imgNote" alt="image-20210830234858640" /><figcaption aria-hidden="true">image-20210830234858640</figcaption>
</figure>
<h3 id="k-means-initial-k-random-centers">K-means: Initial k random centers</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">k = <span class="number">10</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">all_x = []</span><br><span class="line">all_y = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, location <span class="keyword">in</span> city_location.items():</span><br><span class="line">    x, y = location</span><br><span class="line">    </span><br><span class="line">    all_x.append(x)</span><br><span class="line">    all_y.append(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_random_center</span>(<span class="params">all_x, all_y</span>):</span><br><span class="line">    r_x = random.uniform(<span class="built_in">min</span>(all_x), <span class="built_in">max</span>(all_x))</span><br><span class="line">    r_y = random.uniform(<span class="built_in">min</span>(all_y), <span class="built_in">max</span>(all_y))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> r_x, r_y</span><br><span class="line"></span><br><span class="line">get_random_center(all_x, all_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(93.61182991130997, 37.01816228131414)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">K = <span class="number">5</span></span><br><span class="line">centers = &#123;<span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>): get_random_center(all_x, all_y) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K)&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">closet_points = defaultdict(<span class="built_in">list</span>)</span><br><span class="line"><span class="keyword">for</span> x, y, <span class="keyword">in</span> <span class="built_in">zip</span>(all_x, all_y):</span><br><span class="line">    closet_c, closet_dis = <span class="built_in">min</span>([(k, geo_distance((x, y), centers[k])) <span class="keyword">for</span> k <span class="keyword">in</span> centers], key=<span class="keyword">lambda</span> t: t[<span class="number">1</span>])    </span><br><span class="line">    </span><br><span class="line">    closet_points[closet_c].append([x, y])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">iterate_once</span>(<span class="params">centers, closet_points, threshold=<span class="number">5</span></span>):</span><br><span class="line">    have_changed = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> closet_points:</span><br><span class="line">        former_center = centers[c]</span><br><span class="line"></span><br><span class="line">        neighbors = closet_points[c]</span><br><span class="line"></span><br><span class="line">        neighbors_center = np.mean(neighbors, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> geo_distance(neighbors_center, former_center) &gt; threshold:</span><br><span class="line">            centers[c] = neighbors_center</span><br><span class="line">            have_changed = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment">## keep former center</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> centers, have_changed</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kmeans</span>(<span class="params">Xs, k, threshold=<span class="number">5</span></span>):</span><br><span class="line">    all_x = Xs[:, <span class="number">0</span>]</span><br><span class="line">    all_y = Xs[:, <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    K = k</span><br><span class="line">    centers = &#123;<span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>): get_random_center(all_x, all_y) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K)&#125;</span><br><span class="line">    changed = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> changed:</span><br><span class="line">        closet_points = defaultdict(<span class="built_in">list</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> x, y, <span class="keyword">in</span> <span class="built_in">zip</span>(all_x, all_y):</span><br><span class="line">            closet_c, closet_dis = <span class="built_in">min</span>([(k, geo_distance((x, y), centers[k])) <span class="keyword">for</span> k <span class="keyword">in</span> centers], key=<span class="keyword">lambda</span> t: t[<span class="number">1</span>])    </span><br><span class="line">            closet_points[closet_c].append([x, y])   </span><br><span class="line">            </span><br><span class="line">        centers, changed = iterate_once(centers, closet_points, threshold)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;iteration&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> centers</span><br><span class="line"></span><br><span class="line">kmeans(np.array(<span class="built_in">list</span>(city_location.values())), k=<span class="number">5</span>, threshold=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">iteration</span></span><br><span class="line"><span class="string">iteration</span></span><br><span class="line"><span class="string">iteration</span></span><br><span class="line"><span class="string">iteration</span></span><br><span class="line"><span class="string">iteration</span></span><br><span class="line"><span class="string">&#123;&#x27;1&#x27;: array([99.518, 38.86 ]),</span></span><br><span class="line"><span class="string"> &#x27;2&#x27;: array([117.833,  39.861]),</span></span><br><span class="line"><span class="string"> &#x27;3&#x27;: array([91.11, 29.97]),</span></span><br><span class="line"><span class="string"> &#x27;4&#x27;: array([106.81,  27.  ]),</span></span><br><span class="line"><span class="string"> &#x27;5&#x27;: array([116.87166667,  27.6275    ])&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">plt.scatter(all_x, all_y)</span><br><span class="line">plt.scatter(*<span class="built_in">zip</span>(*centers.values()))</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210830235114.png?imgNote" alt="image-20210830235114060" /><figcaption aria-hidden="true">image-20210830235114060</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> c, points <span class="keyword">in</span> closet_points.items():</span><br><span class="line">    plt.scatter(*<span class="built_in">zip</span>(*points))</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210830235135.png?imgNote" alt="image-20210830235135375" /><figcaption aria-hidden="true">image-20210830235135375</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">city_location_with_station = &#123;</span><br><span class="line">    <span class="string">&#x27;能源站-&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i): position <span class="keyword">for</span> i, position <span class="keyword">in</span> centers.items()</span><br><span class="line">&#125;</span><br><span class="line">city_location_with_station</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;&#x27;能源站-1&#x27;: (108.82946246581274, 26.05763939719317),</span></span><br><span class="line"><span class="string"> &#x27;能源站-2&#x27;: (97.96769355736322, 22.166113183141032),</span></span><br><span class="line"><span class="string"> &#x27;能源站-3&#x27;: (114.05390380408154, 38.7698708467224),</span></span><br><span class="line"><span class="string"> &#x27;能源站-4&#x27;: (118.49242085311417, 28.665716162786204),</span></span><br><span class="line"><span class="string"> &#x27;能源站-5&#x27;: (125.08287617496866, 25.55784683330647)&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_cities</span>(<span class="params">citise, color=<span class="literal">None</span></span>):</span><br><span class="line">    city_graph = nx.Graph()</span><br><span class="line">    city_graph.add_nodes_from(<span class="built_in">list</span>(citise.keys()))</span><br><span class="line">    nx.draw(city_graph, citise, node_color=color, with_labels=<span class="literal">True</span>, node_size=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>,figsize=(<span class="number">12</span>,<span class="number">12</span>)) </span><br><span class="line">draw_cities(city_location_with_station, color=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">draw_cities(city_location, color=<span class="string">&#x27;red&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210830235243.png?imgNote" alt="image-20210830235243564" /><figcaption aria-hidden="true">image-20210830235243564</figcaption>
</figure>
<h2 id="about-the-dataset">About the dataset</h2>
<blockquote>
<p>This contains data of news headlines published over a period of 15 years. From the reputable Australian news source ABC (Australian Broadcasting Corp.) Site: http://www.abc.net.au/ Prepared by Rohit Kulkarni</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> text</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> RegexpTokenizer</span><br><span class="line"><span class="keyword">from</span> nltk.stem.snowball <span class="keyword">import</span> SnowballStemmer</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">&quot;./data/abcnews-date-text.csv&quot;</span>,error_bad_lines=<span class="literal">False</span>,usecols =[<span class="string">&quot;headline_text&quot;</span>])</span><br><span class="line">data.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">headline_text</span></span><br><span class="line"><span class="string">0	aba decides against community broadcasting lic...</span></span><br><span class="line"><span class="string">1	act fire witnesses must be aware of defamation</span></span><br><span class="line"><span class="string">2	a g calls for infrastructure protection summit</span></span><br><span class="line"><span class="string">3	air nz staff in aust strike for pay rise</span></span><br><span class="line"><span class="string">4	air nz strike to affect australian travellers</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">data.to_csv(<span class="string">&#x27;abcnews.csv&#x27;</span>, index=<span class="literal">False</span>, encoding=<span class="string">&#x27;utf8&#x27;</span>)</span><br><span class="line">data.info()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span></span><br><span class="line"><span class="string">RangeIndex: 1103665 entries, 0 to 1103664</span></span><br><span class="line"><span class="string">Data columns (total 1 columns):</span></span><br><span class="line"><span class="string"> #   Column         Non-Null Count    Dtype </span></span><br><span class="line"><span class="string">---  ------         --------------    ----- </span></span><br><span class="line"><span class="string"> 0   headline_text  1103665 non-null  object</span></span><br><span class="line"><span class="string">dtypes: object(1)</span></span><br><span class="line"><span class="string">memory usage: 8.4+ MB</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="deleting-dupliate-headlinesif-any">Deleting dupliate headlines(if any)</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data[data[<span class="string">&#x27;headline_text&#x27;</span>].duplicated(keep=<span class="literal">False</span>)].sort_values(<span class="string">&#x27;headline_text&#x27;</span>).head(<span class="number">8</span>)</span><br><span class="line">data = data.drop_duplicates(<span class="string">&#x27;headline_text&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="nlp">NLP</h2>
<h3 id="preparing-data-for-vectorizaion">Preparing data for vectorizaion</h3>
<p>However, when doing natural language processing, words must be converted into vectors that machine learning algorithms can make use of. If your goal is to do machine learning on text data, like movie reviews or tweets or anything else, you need to convert the text data into numbers. This process is sometimes referred to as “embedding” or “vectorization”.</p>
<p>In terms of vectorization, it is important to remember that it isn’t merely turning a single word into a single number. While words can be transformed into numbers, an entire document can be translated into a vector. Not only can a vector have more than one dimension, but with text data vectors are usually high-dimensional. This is because each dimension of your feature data will correspond to a word, and the language in the documents you are examining will have thousands of words.</p>
<h3 id="tf-idf">TF-IDF</h3>
<p>In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf.</p>
<p>Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.</p>
<p>One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">punc = [<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;&quot;&#x27;</span>, <span class="string">&quot;&#x27;&quot;</span>, <span class="string">&#x27;?&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;:&#x27;</span>, <span class="string">&#x27;;&#x27;</span>, <span class="string">&#x27;(&#x27;</span>, <span class="string">&#x27;)&#x27;</span>, <span class="string">&#x27;[&#x27;</span>, <span class="string">&#x27;]&#x27;</span>, <span class="string">&#x27;&#123;&#x27;</span>, <span class="string">&#x27;&#125;&#x27;</span>,<span class="string">&quot;%&quot;</span>]</span><br><span class="line">stop_words = text.ENGLISH_STOP_WORDS.union(punc)</span><br><span class="line">desc = data[<span class="string">&#x27;headline_text&#x27;</span>].values</span><br><span class="line">vectorizer = TfidfVectorizer(stop_words = stop_words)</span><br><span class="line">X = vectorizer.fit_transform(desc)</span><br><span class="line"></span><br><span class="line">word_features = vectorizer.get_feature_names()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(word_features))</span><br><span class="line"><span class="built_in">print</span>(word_features[<span class="number">5000</span>:<span class="number">5100</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">96397</span></span><br><span class="line"><span class="string">[&#x27;abyss&#x27;, &#x27;ac&#x27;, &#x27;aca&#x27;, &#x27;acacia&#x27;, &#x27;acacias&#x27;, &#x27;acadamy&#x27;, &#x27;academia&#x27;, &#x27;academic&#x27;, &#x27;academics&#x27;, &#x27;academies&#x27;, &#x27;academy&#x27;, &#x27;academys&#x27;, &#x27;acai&#x27;, &#x27;acapulco&#x27;, &#x27;acars&#x27;, &#x27;acason&#x27;, &#x27;acasuso&#x27;, &#x27;acb&#x27;, &#x27;acbf&#x27;, &#x27;acc&#x27;, &#x27;acca&#x27;, &#x27;accan&#x27;, &#x27;accc&#x27;, &#x27;acccc&#x27;, &#x27;acccs&#x27;, &#x27;acccused&#x27;, &#x27;acce&#x27;, &#x27;accedes&#x27;, &#x27;accelerant&#x27;, &#x27;accelerants&#x27;, &#x27;accelerate&#x27;, &#x27;accelerated&#x27;, &#x27;accelerates&#x27;, &#x27;accelerating&#x27;, &#x27;acceleration&#x27;, &#x27;accelerator&#x27;, &#x27;accen&#x27;, &#x27;accent&#x27;, &#x27;accents&#x27;, &#x27;accentuate&#x27;, &#x27;accentuates&#x27;, &#x27;accentuating&#x27;, &#x27;accenture&#x27;, &#x27;accept&#x27;, &#x27;acceptability&#x27;, &#x27;acceptable&#x27;, &#x27;acceptably&#x27;, &#x27;acceptance&#x27;, &#x27;acceptances&#x27;, &#x27;accepted&#x27;, &#x27;accepting&#x27;, &#x27;acceptor&#x27;, &#x27;acceptors&#x27;, &#x27;accepts&#x27;, &#x27;accerate&#x27;, &#x27;acces&#x27;, &#x27;access&#x27;, &#x27;accessary&#x27;, &#x27;accessed&#x27;, &#x27;accesses&#x27;, &#x27;accessibility&#x27;, &#x27;accessible&#x27;, &#x27;accessing&#x27;, &#x27;accessories&#x27;, &#x27;accessory&#x27;, &#x27;accesss&#x27;, &#x27;acci&#x27;, &#x27;accid&#x27;, &#x27;accide&#x27;, &#x27;acciden&#x27;, &#x27;accidenatlly&#x27;, &#x27;accidenbt&#x27;, &#x27;accident&#x27;, &#x27;accidental&#x27;, &#x27;accidentally&#x27;, &#x27;accidently&#x27;, &#x27;accidents&#x27;, &#x27;acciona&#x27;, &#x27;accis&#x27;, &#x27;acclaim&#x27;, &#x27;acclaimed&#x27;, &#x27;acclamation&#x27;, &#x27;acclimatise&#x27;, &#x27;acco&#x27;, &#x27;accolade&#x27;, &#x27;accolades&#x27;, &#x27;accom&#x27;, &#x27;accomm&#x27;, &#x27;accommoda&#x27;, &#x27;accommodate&#x27;, &#x27;accommodated&#x27;, &#x27;accommodates&#x27;, &#x27;accommodating&#x27;, &#x27;accommodation&#x27;, &#x27;accomo&#x27;, &#x27;accomodation&#x27;, &#x27;accomommodation&#x27;, &#x27;accompanied&#x27;, &#x27;accompanies&#x27;, &#x27;accompaniment&#x27;]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="stemming">Stemming</h3>
<p>Stemming is the process of reducing a word into its stem, i.e. its root form. The root form is not necessarily a word by itself, but it can be used to generate words by concatenating the right suffix. For example, the words fish, fishes and fishing all stem into fish, which is a correct word. On the other side, the words study, studies and studying stems into studi, which is not an English word.</p>
<h3 id="tokenizing">Tokenizing</h3>
<p>Tokenization is breaking the sentence into words and punctuation,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stemmer = SnowballStemmer(<span class="string">&#x27;english&#x27;</span>)</span><br><span class="line">tokenizer = RegexpTokenizer(<span class="string">r&#x27;[a-zA-Z\&#x27;]+&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="keyword">return</span> [stemmer.stem(word) <span class="keyword">for</span> word <span class="keyword">in</span> tokenizer.tokenize(text.lower())]</span><br></pre></td></tr></table></figure>
<p><strong>Vectorization with stop words(words irrelevant to the model), stemming and tokenizing</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)</span><br><span class="line">X2 = vectorizer2.fit_transform(desc)</span><br><span class="line">word_features2 = vectorizer2.get_feature_names()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(word_features2))</span><br><span class="line"><span class="built_in">print</span>(word_features2[:<span class="number">50</span>]) </span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">65232</span></span><br><span class="line"><span class="string">[&quot;&#x27;a&quot;, &quot;&#x27;i&quot;, &quot;&#x27;s&quot;, &quot;&#x27;t&quot;, &#x27;aa&#x27;, &#x27;aaa&#x27;, &#x27;aaahhh&#x27;, &#x27;aac&#x27;, &#x27;aacc&#x27;, &#x27;aaco&#x27;, &#x27;aacta&#x27;, &#x27;aad&#x27;, &#x27;aadmi&#x27;, &#x27;aag&#x27;, &#x27;aagaard&#x27;, &#x27;aagard&#x27;, &#x27;aah&#x27;, &#x27;aalto&#x27;, &#x27;aam&#x27;, &#x27;aamer&#x27;, &#x27;aami&#x27;, &#x27;aamodt&#x27;, &#x27;aandahl&#x27;, &#x27;aant&#x27;, &#x27;aap&#x27;, &#x27;aapa&#x27;, &#x27;aapt&#x27;, &#x27;aar&#x27;, &#x27;aaradhna&#x27;, &#x27;aardman&#x27;, &#x27;aardvark&#x27;, &#x27;aargau&#x27;, &#x27;aaron&#x27;, &#x27;aaronpaul&#x27;, &#x27;aarwun&#x27;, &#x27;aat&#x27;, &#x27;ab&#x27;, &#x27;aba&#x27;, &#x27;abaaoud&#x27;, &#x27;ababa&#x27;, &#x27;aback&#x27;, &#x27;abadi&#x27;, &#x27;abadon&#x27;, &#x27;abal&#x27;, &#x27;abalon&#x27;, &#x27;abalonv&#x27;, &#x27;abama&#x27;, &#x27;abandon&#x27;, &#x27;abandond&#x27;, &#x27;abandong&#x27;]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = <span class="number">1000</span>)</span><br><span class="line">X3 = vectorizer3.fit_transform(desc)</span><br><span class="line">words = vectorizer3.get_feature_names()</span><br></pre></td></tr></table></figure>
<p>For this, we will use k-means clustering algorithm. ### K-means clustering (Source <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm">Wikipedia</a>)</p>
<h3 id="elbow-method-to-select-number-of-clusters">Elbow method to select number of clusters</h3>
<p>This method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the "elbow criterion". This "elbow" cannot always be unambiguously identified. Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance.</p>
<h4 id="basically-number-of-clusters-the-x-axis-value-of-the-point-that-is-the-corner-of-the-elbowthe-plot-looks-often-looks-like-an-elbow"><strong>Basically, number of clusters = the x-axis value of the point that is the corner of the "elbow"(the plot looks often looks like an elbow)</strong></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">wcss = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">11</span>):</span><br><span class="line">    kmeans = KMeans(n_clusters=i,init=<span class="string">&#x27;k-means++&#x27;</span>,max_iter=<span class="number">300</span>,n_init=<span class="number">10</span>,random_state=<span class="number">0</span>)</span><br><span class="line">    kmeans.fit(X3)</span><br><span class="line">    wcss.append(kmeans.inertia_)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">11</span>),wcss)</span><br><span class="line">plt.title(<span class="string">&#x27;The Elbow Method&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of clusters&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;WCSS&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;elbow.png&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210830235601.png?imgNote" alt="image-20210830235601231" /><figcaption aria-hidden="true">image-20210830235601231</figcaption>
</figure>
<p>As more than one elbows have been generated, I will have to select right amount of clusters by trial and error. So, I will showcase the results of different amount of clusters to find out the right amount of clusters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(words[<span class="number">250</span>:<span class="number">300</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[&#x27;decis&#x27;, &#x27;declar&#x27;, &#x27;defenc&#x27;, &#x27;defend&#x27;, &#x27;delay&#x27;, &#x27;deliv&#x27;, &#x27;demand&#x27;, &#x27;deni&#x27;, &#x27;despit&#x27;, &#x27;destroy&#x27;, &#x27;detent&#x27;, &#x27;develop&#x27;, &#x27;die&#x27;, &#x27;director&#x27;, &#x27;disabl&#x27;, &#x27;disast&#x27;, &#x27;discuss&#x27;, &#x27;diseas&#x27;, &#x27;dismiss&#x27;, &#x27;disput&#x27;, &#x27;doctor&#x27;, &#x27;dog&#x27;, &#x27;dollar&#x27;, &#x27;domest&#x27;, &#x27;donald&#x27;, &#x27;donat&#x27;, &#x27;doubl&#x27;, &#x27;doubt&#x27;, &#x27;draw&#x27;, &#x27;dri&#x27;, &#x27;drink&#x27;, &#x27;drive&#x27;, &#x27;driver&#x27;, &#x27;drop&#x27;, &#x27;drought&#x27;, &#x27;drown&#x27;, &#x27;drug&#x27;, &#x27;drum&#x27;, &#x27;dump&#x27;, &#x27;dure&#x27;, &#x27;e&#x27;, &#x27;eagl&#x27;, &#x27;earli&#x27;, &#x27;eas&#x27;, &#x27;east&#x27;, &#x27;econom&#x27;, &#x27;economi&#x27;, &#x27;edg&#x27;, &#x27;educ&#x27;, &#x27;effort&#x27;]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="clusters">3 Clusters</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">kmeans = KMeans(n_clusters = <span class="number">3</span>, n_init = <span class="number">20</span>, n_jobs = <span class="number">1</span>) <span class="comment"># n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)</span></span><br><span class="line">kmeans.fit(X3)</span><br><span class="line"><span class="comment"># We look at 3 the clusters generated by k-means.</span></span><br><span class="line">common_words = kmeans.cluster_centers_.argsort()[:,-<span class="number">1</span>:-<span class="number">26</span>:-<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> num, centroid <span class="keyword">in</span> <span class="built_in">enumerate</span>(common_words):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">str</span>(num) + <span class="string">&#x27; : &#x27;</span> + <span class="string">&#x27;, &#x27;</span>.join(words[word] <span class="keyword">for</span> word <span class="keyword">in</span> centroid))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0 : new, say, plan, win, council, govt, australia, report, kill, fund, urg, court, warn, water, australian, nsw, open, chang, year, qld, interview, wa, death, face, crash</span></span><br><span class="line"><span class="string">1 : polic, investig, probe, man, search, offic, hunt, miss, arrest, death, car, shoot, drug, seek, attack, assault, say, murder, crash, charg, driver, suspect, fatal, raid, station</span></span><br><span class="line"><span class="string">2 : man, charg, murder, court, face, jail, assault, stab, die, death, drug, guilti, child, sex, accus, attack, woman, crash, arrest, car, kill, miss, sydney, alleg, plead</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="clusters-1">5 Clusters</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">kmeans = KMeans(n_clusters = <span class="number">5</span>, n_init = <span class="number">20</span>, n_jobs = <span class="number">1</span>)</span><br><span class="line">kmeans.fit(X3)</span><br><span class="line"><span class="comment"># We look at 5 the clusters generated by k-means.</span></span><br><span class="line">common_words = kmeans.cluster_centers_.argsort()[:,-<span class="number">1</span>:-<span class="number">26</span>:-<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> num, centroid <span class="keyword">in</span> <span class="built_in">enumerate</span>(common_words):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">str</span>(num) + <span class="string">&#x27; : &#x27;</span> + <span class="string">&#x27;, &#x27;</span>.join(words[word] <span class="keyword">for</span> word <span class="keyword">in</span> centroid))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0 : man, plan, charg, court, govt, australia, face, murder, accus, jail, assault, stab, urg, drug, death, attack, child, sex, die, woman, guilti, say, alleg, told, car</span></span><br><span class="line"><span class="string">1 : new, zealand, law, year, plan, open, polic, home, hospit, centr, deal, set, hope, australia, look, appoint, announc, chief, say, south, minist, govt, rule, servic, welcom</span></span><br><span class="line"><span class="string">2 : say, win, kill, report, australian, warn, interview, open, water, fund, nsw, crash, death, urg, year, chang, wa, sydney, claim, qld, hit, attack, world, set, health</span></span><br><span class="line"><span class="string">3 : council, plan, consid, fund, rate, urg, seek, new, merger, water, land, develop, reject, say, mayor, vote, chang, elect, rise, meet, park, push, want, govt, approv</span></span><br><span class="line"><span class="string">4 : polic, investig, man, probe, search, offic, hunt, miss, arrest, death, car, charg, shoot, drug, seek, attack, assault, murder, crash, say, driver, fatal, suspect, raid, woman</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="clusters-2">6 Clusters</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">kmeans = KMeans(n_clusters = <span class="number">6</span>, n_init = <span class="number">20</span>, n_jobs = <span class="number">1</span>)</span><br><span class="line">kmeans.fit(X3)</span><br><span class="line"><span class="comment"># We look at 6 the clusters generated by k-means.</span></span><br><span class="line">common_words = kmeans.cluster_centers_.argsort()[:,-<span class="number">1</span>:-<span class="number">26</span>:-<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> num, centroid <span class="keyword">in</span> <span class="built_in">enumerate</span>(common_words):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">str</span>(num) + <span class="string">&#x27; : &#x27;</span> + <span class="string">&#x27;, &#x27;</span>.join(words[word] <span class="keyword">for</span> word <span class="keyword">in</span> centroid))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0 : council, govt, australia, report, warn, urg, fund, australian, water, nsw, chang, qld, wa, health, elect, rural, countri, hour, sa, boost, climat, govern, servic, south, consid</span></span><br><span class="line"><span class="string">1 : man, charg, murder, court, face, jail, assault, stab, die, death, drug, guilti, child, sex, accus, attack, woman, crash, arrest, car, kill, miss, sydney, plead, alleg</span></span><br><span class="line"><span class="string">2 : polic, investig, probe, man, search, offic, hunt, miss, arrest, death, car, shoot, drug, seek, attack, crash, assault, murder, charg, driver, say, fatal, suspect, raid, warn</span></span><br><span class="line"><span class="string">3 : win, kill, court, interview, crash, open, death, sydney, face, year, claim, hit, attack, world, set, final, day, hous, die, home, jail, talk, return, cup, hospit</span></span><br><span class="line"><span class="string">4 : new, zealand, law, year, plan, open, council, polic, home, hospit, centr, deal, set, hope, australia, appoint, look, announc, chief, say, govt, south, minist, mayor, welcom</span></span><br><span class="line"><span class="string">5 : say, plan, council, govt, water, need, group, chang, labor, minist, govern, opposit, public, mp, health, union, green, hous, develop, resid, report, expert, cut, australia, mayor</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="clusters-3">8 Clusters</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">kmeans = KMeans(n_clusters = <span class="number">8</span>, n_init = <span class="number">20</span>, n_jobs = <span class="number">1</span>)</span><br><span class="line">kmeans.fit(X3)</span><br><span class="line"><span class="comment"># Finally, we look at 8 the clusters generated by k-means.</span></span><br><span class="line">common_words = kmeans.cluster_centers_.argsort()[:,-<span class="number">1</span>:-<span class="number">26</span>:-<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> num, centroid <span class="keyword">in</span> <span class="built_in">enumerate</span>(common_words):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">str</span>(num) + <span class="string">&#x27; : &#x27;</span> + <span class="string">&#x27;, &#x27;</span>.join(words[word] <span class="keyword">for</span> word <span class="keyword">in</span> centroid))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0 : polic, say, man, miss, arrest, jail, investig, car, search, murder, attack, crash, kill, probe, die, hunt, shoot, assault, offic, drug, stab, accus, fatal, guilti, bodi</span></span><br><span class="line"><span class="string">1 : death, hous, polic, toll, investig, man, probe, inquest, rise, woman, coron, blaze, price, public, white, babi, sentenc, famili, road, spark, jail, prompt, blame, custodi, report</span></span><br><span class="line"><span class="string">2 : plan, council, govt, water, new, say, develop, hous, group, chang, unveil, reject, park, urg, centr, public, expans, green, resid, health, reveal, labor, govern, opposit, power</span></span><br><span class="line"><span class="string">3 : court, face, man, accus, told, hear, murder, high, case, appear, rule, charg, alleg, appeal, drug, jail, woman, death, assault, order, sex, stab, challeng, teen, polic</span></span><br><span class="line"><span class="string">4 : australia, govt, kill, report, warn, australian, urg, fund, nsw, interview, water, open, crash, qld, chang, wa, year, day, claim, hit, attack, sydney, set, health, world</span></span><br><span class="line"><span class="string">5 : new, council, zealand, law, fund, year, consid, water, urg, open, say, seek, rate, centr, mayor, govt, elect, look, develop, land, deal, hope, set, push, home</span></span><br><span class="line"><span class="string">6 : win, award, cup, titl, open, gold, stage, world, final, tour, elect, australia, lead, seri, aussi, claim, second, australian, big, england, grand, m, battl, race, record</span></span><br><span class="line"><span class="string">7 : charg, man, murder, face, assault, drug, polic, child, sex, woman, teen, death, stab, drop, alleg, attack, rape, men, guilti, shoot, bail, sydney, fatal, driver, yo</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>Because even I didn't know what kind of clusters would be generated, I will describe them in comments.</p>
<h2 id="other-discussions">Other discussions</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer, PorterStemmer</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">import</span> pyLDAvis.gensim_models</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud, STOPWORDS</span><br><span class="line"><span class="keyword">from</span> textblob <span class="keyword">import</span> TextBlob</span><br><span class="line"><span class="keyword">from</span> spacy <span class="keyword">import</span> displacy</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set plt</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;Arial Unicode MS&#x27;</span>]</span><br><span class="line">plt.rcParams.update(&#123;<span class="string">&#x27;font.size&#x27;</span>: <span class="number">12</span>&#125;)</span><br><span class="line">plt.rcParams.update(&#123;<span class="string">&#x27;figure.figsize&#x27;</span>: [<span class="number">16</span>, <span class="number">12</span>]&#125;)</span><br><span class="line"><span class="comment"># plt.figure(figsize = [20, 20])</span></span><br><span class="line">plt.style.use(<span class="string">&#x27;seaborn-whitegrid&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;../data/abcnews-date-text.csv&#x27;</span>, nrows = <span class="number">10000</span>)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">publish_date	headline_text</span></span><br><span class="line"><span class="string">0	20030219	aba decides against community broadcasting lic...</span></span><br><span class="line"><span class="string">1	20030219	act fire witnesses must be aware of defamation</span></span><br><span class="line"><span class="string">2	20030219	a g calls for infrastructure protection summit</span></span><br><span class="line"><span class="string">3	20030219	air nz staff in aust strike for pay rise</span></span><br><span class="line"><span class="string">4	20030219	air nz strike to affect australian travellers</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>The data set contains only two columns, the release date and the news title.</p>
<p>For simplicity, I will explore the first 10,000 rows in this dataset. Since the titles are sorted by publish_date, they are actually two months from February 19, 2003 to April 7, 2003.</p>
<h3 id="number-of-characters-present-in-each-sentence">Number of characters present in each sentence</h3>
<p>Visualization of text statistics is a simple but insightful technique.</p>
<p>They include:</p>
<p>Word frequency analysis, sentence length analysis, average word length analysis, etc.</p>
<p>These really help to explore the basic characteristics of text data.</p>
<p>For this, we will mainly use histograms (continuous data) and bar graphs (categorical data).</p>
<p>First, let me look at the number of characters in each sentence. This can give us a rough idea of the length of news headlines.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;headline_text&#x27;</span>].<span class="built_in">str</span>.<span class="built_in">len</span>().hist()</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831000016.png?imgNote" alt="image-20210831000016641" /><figcaption aria-hidden="true">image-20210831000016641</figcaption>
</figure>
<h3 id="number-of-words-appearing-in-each-news-headline">number of words appearing in each news headline</h3>
<p>The histogram shows that the range of news headlines is 10 to 70 characters, usually between 25 and 55 characters.</p>
<p>Now, we will continue to explore the data verbatim. Let's plot the number of words that appear in each news headline.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;headline_text&#x27;</span>].<span class="built_in">str</span>.split().<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x)).hist()</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831000044.png?imgNote" alt="image-20210831000044656" /><figcaption aria-hidden="true">image-20210831000044656</figcaption>
</figure>
<h3 id="analysing-word-length">Analysing word length</h3>
<p>Obviously, the number of words in news headlines is in the range of 2 to 12, and most of them are between 5 and 7.</p>
<p>Next, let's check the average word length in each sentence.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;headline_text&#x27;</span>].<span class="built_in">str</span>.split().apply(<span class="keyword">lambda</span> x : [<span class="built_in">len</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> x]).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: np.mean(x)).hist()</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831000111.png?imgNote" alt="image-20210831000111622" /><figcaption aria-hidden="true">image-20210831000111622</figcaption>
</figure>
<p>The average word length is between 3 and 9, and the most common length is 5. Does this mean that people use very short words in news headlines?</p>
<p>Let us find out.</p>
<p>One reason that may not be the case is stop words. Stop words are the most commonly used words in any language (such as "the", "a", "an", etc.). Since the length of these words may be small, these words may cause the above graphics to be skewed to the left.</p>
<p>Analyzing the number and types of stop words can give us some in-depth understanding of the data.</p>
<p>To get a corpus containing stop words, you can use the <a target="_blank" rel="noopener" href="https://www.nltk.org/?ref=hackernoon.com">nltk library</a>. Nltk contains stop words from multiple languages. Since we only deal with English news, I will filter English stop words from the corpus.</p>
<h3 id="analysing-stopwords">Analysing stopwords</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fetch stopwords</span></span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.download(<span class="string">&#x27;stopwords&#x27;</span>)</span><br><span class="line">stop=<span class="built_in">set</span>(stopwords.words(<span class="string">&#x27;english&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[nltk_data] Downloading package stopwords to /Users/xx/nltk_data...</span></span><br><span class="line"><span class="string">[nltk_data]   Package stopwords is already up-to-date!</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create corpus</span></span><br><span class="line">corpus=[]</span><br><span class="line">new= df[<span class="string">&#x27;headline_text&#x27;</span>].<span class="built_in">str</span>.split()</span><br><span class="line">new=new.values.tolist()</span><br><span class="line">corpus=[word <span class="keyword">for</span> i <span class="keyword">in</span> new <span class="keyword">for</span> word <span class="keyword">in</span> i]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">dic=defaultdict(<span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> corpus:</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> stop:</span><br><span class="line">        dic[word]+=<span class="number">1</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># Plot top stopwords</span></span><br><span class="line"></span><br><span class="line">top=<span class="built_in">sorted</span>(dic.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>],reverse=<span class="literal">True</span>)[:<span class="number">10</span>] </span><br><span class="line">x,y=<span class="built_in">zip</span>(*top)</span><br><span class="line">plt.bar(x,y)</span><br></pre></td></tr></table></figure>
<p>Draw popular stop words</p>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831000213.png?imgNote" alt="image-20210831000213620" /><figcaption aria-hidden="true">image-20210831000213620</figcaption>
</figure>
<h3 id="most-common-words">Most common words</h3>
<p>We can clearly see that in the news headlines, stop words such as "to", "in" and "for" dominate.</p>
<p>So now that we know which stop words appear frequently in our text, let's check which words other than these stop words appear frequently.</p>
<p>We will use the counter function in the collection library to count the occurrence of each word and store it in a list of tuples. This is a very useful feature when we are dealing with word-level analysis in natural language processing.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">counter=Counter(corpus)</span><br><span class="line">most=counter.most_common()</span><br><span class="line"></span><br><span class="line">x, y=[], []</span><br><span class="line"><span class="keyword">for</span> word,count <span class="keyword">in</span> most[:<span class="number">40</span>]:</span><br><span class="line">    <span class="keyword">if</span> (word <span class="keyword">not</span> <span class="keyword">in</span> stop):</span><br><span class="line">        x.append(word)</span><br><span class="line">        y.append(count)</span><br><span class="line">        </span><br><span class="line">sns.barplot(x=y,y=x)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831000244.png?imgNote" alt="image-20210831000244040" /><figcaption aria-hidden="true">image-20210831000244040</figcaption>
</figure>
<p>Wow! In the past 15 years, "America", "Iraq" and "War" have dominated the headlines.</p>
<p>"We" here may mean the United States or us (you and me). We are not a stop word, but when we look at the other words in the picture, they are all related to the United States-the Iraq War and "we" here may mean the United States.</p>
<h2 id="ngram-analysis">Ngram analysis</h2>
<p>Ngram is a continuous sequence of n words. For example, "Riverbank", "Three Musketeers" and so on. If the number of words is two, it is called a double word. For 3 characters, it is called a trigram, and so on.</p>
<p>Viewing the most common n-grams can give you a better understanding of the context in which the word is used.</p>
<h3 id="bigram-analysis">Bigram analysis</h3>
<p>To build our vocabulary, we will use Countvectorizer. Countvectorizer is a simple method for labeling, vectorizing and representing corpora in an appropriate form. Can <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?ref=hackernoon.com">be found</a> in <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?ref=hackernoon.com">sklearn.feature_engineering.text</a></p>
<p>Therefore, we will analyze the top news in all news headlines.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_top_ngram</span>(<span class="params">corpus, n=<span class="literal">None</span></span>):</span><br><span class="line">    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)</span><br><span class="line">    bag_of_words = vec.transform(corpus)</span><br><span class="line">    sum_words = bag_of_words.<span class="built_in">sum</span>(axis=<span class="number">0</span>) </span><br><span class="line">    words_freq = [(word, sum_words[<span class="number">0</span>, idx]) </span><br><span class="line">                  <span class="keyword">for</span> word, idx <span class="keyword">in</span> vec.vocabulary_.items()]</span><br><span class="line">    words_freq =<span class="built_in">sorted</span>(words_freq, key = <span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> words_freq[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">top_n_bigrams=get_top_ngram(df[<span class="string">&#x27;headline_text&#x27;</span>],<span class="number">2</span>)[:<span class="number">10</span>]</span><br><span class="line">x,y=<span class="built_in">map</span>(<span class="built_in">list</span>,<span class="built_in">zip</span>(*top_n_bigrams))</span><br><span class="line">sns.barplot(x=y,y=x)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831000322.png?imgNote" alt="image-20210831000322862" /><figcaption aria-hidden="true">image-20210831000322862</figcaption>
</figure>
<h3 id="trigram-analysis">Trigram analysis</h3>
<p>We can observe that dualisms such as "anti-war" and "killed" related to war dominate the headlines.</p>
<p>How about triples?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">top_tri_grams=get_top_ngram(df[<span class="string">&#x27;headline_text&#x27;</span>],n=<span class="number">3</span>)</span><br><span class="line">x,y=<span class="built_in">map</span>(<span class="built_in">list</span>,<span class="built_in">zip</span>(*top_tri_grams))</span><br><span class="line">sns.barplot(x=y,y=x)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831000347.png?imgNote" alt="image-20210831000347490" /><figcaption aria-hidden="true">image-20210831000347490</figcaption>
</figure>
<p>We can see that many of these hexagrams are a combination of "face the court" and "anti-war protest." This means that we should spend some effort on data cleaning to see if we can combine these synonyms into a clean token.</p>
<h2 id="topic-modelling">Topic modelling</h2>
<h3 id="use-pyldavis-for-topic-modeling-exploration">Use pyLDAvis for topic modeling exploration</h3>
<p>Topic modeling is the process of using unsupervised learning techniques to extract the main topics that appear in the document set.</p>
<p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158?ref=hackernoon.com">Latent Dirichlet Allocation</a> (LDA) is an easy-to-use and efficient topic modeling model. Each document is represented by a topic distribution, and each topic is represented by a word distribution.</p>
<p>Once the documents are classified into topics, you can delve into the data for each topic or topic group.</p>
<p>But before entering topic modeling, we must do some preprocessing of the data. we will:</p>
<p>Tokenization: The process of converting sentences into tokens or word lists. remove stopwordslemmatize: Reduce the deformed form of each word to a common base or root. Convert to word bag: word bag is a dictionary where the key is the word (or ngram/tokens) and the value is the number of times each word appears in the corpus.</p>
<p>With NLTK, you can easily tokenize and formalize:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.download(<span class="string">&#x27;punkt&#x27;</span>)</span><br><span class="line">nltk.download(<span class="string">&#x27;wordnet&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[nltk_data] Downloading package punkt to /Users/xx/nltk_data...</span></span><br><span class="line"><span class="string">[nltk_data]   Package punkt is already up-to-date!</span></span><br><span class="line"><span class="string">[nltk_data] Downloading package wordnet to /Users/xx/nltk_data...</span></span><br><span class="line"><span class="string">[nltk_data]   Unzipping corpora/wordnet.zip.</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_news</span>(<span class="params">df</span>):</span><br><span class="line">    corpus=[]</span><br><span class="line">    stem=PorterStemmer()</span><br><span class="line">    lem=WordNetLemmatizer()</span><br><span class="line">    <span class="keyword">for</span> news <span class="keyword">in</span> df[<span class="string">&#x27;headline_text&#x27;</span>]:</span><br><span class="line">        words=[w <span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(news) <span class="keyword">if</span> (w <span class="keyword">not</span> <span class="keyword">in</span> stop)]</span><br><span class="line">        </span><br><span class="line">        words=[lem.lemmatize(w) <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> <span class="built_in">len</span>(w)&gt;<span class="number">2</span>]</span><br><span class="line">        </span><br><span class="line">        corpus.append(words)</span><br><span class="line">    <span class="keyword">return</span> corpus</span><br><span class="line">  </span><br><span class="line">corpus = preprocess_news(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, let&#x27;s use gensim to create a bag of words model</span></span><br><span class="line">dic=gensim.corpora.Dictionary(corpus)</span><br><span class="line">bow_corpus = [dic.doc2bow(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> corpus]</span><br><span class="line"></span><br><span class="line"><span class="comment"># We can finally create the LDA model:</span></span><br><span class="line">lda_model =  gensim.models.LdaMulticore(bow_corpus, </span><br><span class="line">                                   num_topics = <span class="number">4</span>, </span><br><span class="line">                                   id2word = dic,                                    </span><br><span class="line">                                   passes = <span class="number">10</span>,</span><br><span class="line">                                   workers = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">lda_model.show_topics()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[(0,</span></span><br><span class="line"><span class="string">  &#x27;0.010*&quot;say&quot; + 0.007*&quot;cup&quot; + 0.006*&quot;war&quot; + 0.005*&quot;world&quot; + 0.005*&quot;back&quot; + 0.005*&quot;plan&quot; + 0.005*&quot;green&quot; + 0.004*&quot;win&quot; + 0.004*&quot;woman&quot; + 0.004*&quot;new&quot;&#x27;),</span></span><br><span class="line"><span class="string"> (1,</span></span><br><span class="line"><span class="string">  &#x27;0.010*&quot;govt&quot; + 0.009*&quot;war&quot; + 0.009*&quot;new&quot; + 0.007*&quot;may&quot; + 0.005*&quot;sars&quot; + 0.005*&quot;call&quot; + 0.005*&quot;protest&quot; + 0.005*&quot;boost&quot; + 0.005*&quot;group&quot; + 0.004*&quot;hospital&quot;&#x27;),</span></span><br><span class="line"><span class="string"> (2,</span></span><br><span class="line"><span class="string">  &#x27;0.018*&quot;police&quot; + 0.015*&quot;baghdad&quot; + 0.014*&quot;man&quot; + 0.005*&quot;missing&quot; + 0.005*&quot;claim&quot; + 0.005*&quot;court&quot; + 0.005*&quot;australia&quot; + 0.004*&quot;move&quot; + 0.004*&quot;murder&quot; + 0.004*&quot;charged&quot;&#x27;),</span></span><br><span class="line"><span class="string"> (3,</span></span><br><span class="line"><span class="string">  &#x27;0.030*&quot;iraq&quot; + 0.015*&quot;war&quot; + 0.007*&quot;iraqi&quot; + 0.007*&quot;council&quot; + 0.006*&quot;troop&quot; + 0.005*&quot;killed&quot; + 0.004*&quot;crash&quot; + 0.004*&quot;soldier&quot; + 0.004*&quot;open&quot; + 0.004*&quot;say&quot;&#x27;)]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>Theme 0 represents things related to the Iraq war and the police. Theme 3 shows Australia's involvement in the Iraq War.</p>
<p>You can print all the topics and try to understand them, but there are tools that can help you run this data exploration more effectively. pyLDAvis is such a tool, it can interactively visualize the results of LDA.</p>
<h3 id="visualize-the-topics">Visualize the topics</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pyLDAvis.enable_notebook()</span><br><span class="line">vis = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dic)</span><br><span class="line">vis</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831000603.png?imgNote" alt="image-20210831000602995" /><figcaption aria-hidden="true">image-20210831000602995</figcaption>
</figure>
<p>On the left, the area of each circle represents the importance of the topic relative to the corpus. Because there are four themes, we have four circles.</p>
<p>The distance between the center of the circle indicates the similarity between themes. Here you can see that Topic 3 and Topic 4 overlap, which indicates that the themes are more similar. On the right, the histogram of each topic shows the top 30 related words. For example, in topic 1, the most relevant words are "police", "new", "may", "war", etc.</p>
<p>Therefore, in our case, we can see many war-related words and topics in the news headlines.</p>
<h3 id="wordclouds">Wordclouds</h3>
<p>Wordcloud is a great way to represent text data. The size and color of each word appearing in the word cloud indicate its frequency or importance.</p>
<p>It is easy to create a <a target="_blank" rel="noopener" href="https://amueller.github.io/word_cloud/index.html?ref=hackernoon.com">wordcloud</a> <a target="_blank" rel="noopener" href="https://amueller.github.io/word_cloud/index.html?ref=hackernoon.com">using python</a>, but we need to provide data in the form of a corpus.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">stopwords = <span class="built_in">set</span>(STOPWORDS)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_wordcloud</span>(<span class="params">data, title = <span class="literal">None</span></span>):</span><br><span class="line">    wordcloud = WordCloud(</span><br><span class="line">        background_color=<span class="string">&#x27;white&#x27;</span>,</span><br><span class="line">        stopwords=stopwords,</span><br><span class="line">        max_words=<span class="number">100</span>,</span><br><span class="line">        max_font_size=<span class="number">30</span>, </span><br><span class="line">        scale=<span class="number">3</span>,</span><br><span class="line">        random_state=<span class="number">1</span> </span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    wordcloud=wordcloud.generate(<span class="built_in">str</span>(data))</span><br><span class="line"></span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, figsize=(<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">    plt.imshow(wordcloud)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">show_wordcloud(corpus)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831000636.png?imgNote" alt="image-20210831000635924" /><figcaption aria-hidden="true">image-20210831000635924</figcaption>
</figure>
<p>Similarly, you can see that terms related to war are highlighted, indicating that these words often appear in news headlines.</p>
<p>There are many parameters that can be adjusted. Some of the most famous are:</p>
<p>stopwords: stop a group of words appearing in the image. max_words: Indicates the maximum number of words to be displayed. max_font_size: Maximum font size.</p>
<p>There are many other options to create beautiful word clouds. For more detailed information, you can refer to here.</p>
<h2 id="text-sentiment">Text sentiment</h2>
<p>Sentiment analysis is a very common natural language processing task in which we determine whether the text is positive, negative or neutral. This is very useful for finding sentiments related to comments and comments, allowing us to gain some valuable insights from text data.</p>
<p>There are many projects that can help you use python for sentiment analysis. I personally like <a target="_blank" rel="noopener" href="https://github.com/sloria/TextBlob?ref=hackernoon.com">TextBlob</a> and <a target="_blank" rel="noopener" href="https://github.com/cjhutto/vaderSentiment?ref=hackernoon.com">Vader Sentiment.</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> textblob <span class="keyword">import</span> TextBlob</span><br><span class="line">TextBlob(<span class="string">&#x27;100 people killed in Iraq&#x27;</span>).sentiment</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Sentiment(polarity=-0.2, subjectivity=0.0)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="textblob">Textblob</h3>
<p>Textblob is a python library built on top of nltk. It has been around for a while and is very easy to use.</p>
<p>The sentiment function of TextBlob returns two attributes:</p>
<p>Polarity: It is a floating-point number in the range of [-1,1], where 1 means a positive statement and -1 means a negative statement. Subjectivity: refers to how personal opinions and feelings affect someone’s judgment. The subjectivity is expressed as a floating point value with a range of [0,1].</p>
<p>I will run this feature on news headlines.</p>
<p>TextBlob claims that the text "100 people killed in Iraq" is negative, not a view or feeling, but a statement of fact. I think we can agree to TextBlob here.</p>
<p>Now that we know how to calculate these sentiment scores, we can use histograms to visualize them and explore the data further.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">polarity</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="keyword">return</span> TextBlob(text).sentiment.polarity</span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;polarity_score&#x27;</span>]=df[<span class="string">&#x27;headline_text&#x27;</span>].\</span><br><span class="line">   apply(<span class="keyword">lambda</span> x : polarity(x))</span><br><span class="line">df[<span class="string">&#x27;polarity_score&#x27;</span>].hist()</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831000735.png?imgNote" alt="image-20210831000735233" /><figcaption aria-hidden="true">image-20210831000735233</figcaption>
</figure>
<p>You will see that the polarity is mainly between 0.00 and 0.20. This shows that most news headlines are neutral.</p>
<p>Let's categorize news as negative, positive, and neutral based on the scores for a more in-depth study.</p>
<h3 id="postive-negative-or-neutral">Postive , Negative or Neutral ?</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sentiment</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">if</span> x&lt;<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;neg&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;neu&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;pos&#x27;</span></span><br><span class="line">    </span><br><span class="line">df[<span class="string">&#x27;polarity&#x27;</span>]=df[<span class="string">&#x27;polarity_score&#x27;</span>].\</span><br><span class="line">   <span class="built_in">map</span>(<span class="keyword">lambda</span> x: sentiment(x))</span><br><span class="line">  </span><br><span class="line">plt.bar(df.polarity.value_counts().index,</span><br><span class="line">        df.polarity.value_counts())</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831000804.png?imgNote" alt="image-20210831000804842" /><figcaption aria-hidden="true">image-20210831000804842</figcaption>
</figure>
<p>Yes, 70% of news is neutral, only 18% of positive news and 11% of negative news.</p>
<p>Let's look at the positive and negative headlines.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">df[df[<span class="string">&#x27;polarity&#x27;</span>]==<span class="string">&#x27;neg&#x27;</span>][<span class="string">&#x27;headline_text&#x27;</span>].head(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">7     aussie qualifier stosur wastes four memphis match</span></span><br><span class="line"><span class="string">23               carews freak goal leaves roma in ruins</span></span><br><span class="line"><span class="string">28     council chief executive fails to secure position</span></span><br><span class="line"><span class="string">34                   dargo fire threat expected to rise</span></span><br><span class="line"><span class="string">40        direct anger at govt not soldiers crean urges</span></span><br><span class="line"><span class="string">Name: headline_text, dtype: object</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="vader">Vader</h3>
<p>The next library we are going to discuss is VADER. Vader is better at detecting negative emotions. It is very useful in the context of social media text sentiment analysis.</p>
<p>The VADER or Valence Aware dictionary and sentiment reasoner is an open source sentiment analyzer pre-built library based on rules/dictionaries and is protected by the MIT license.</p>
<p>The VADER sentiment analysis class returns a dictionary that contains the possibility that the text appears positive, negative, and neutral. Then, we can filter and select the emotion with the highest probability.</p>
<p>We will use VADER to perform the same analysis and check if the difference is large.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.sentiment.vader <span class="keyword">import</span> SentimentIntensityAnalyzer</span><br><span class="line"></span><br><span class="line">nltk.download(<span class="string">&#x27;vader_lexicon&#x27;</span>)</span><br><span class="line">sid = SentimentIntensityAnalyzer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_vader_score</span>(<span class="params">sent</span>):</span><br><span class="line">    <span class="comment"># Polarity score returns dictionary</span></span><br><span class="line">    ss = sid.polarity_scores(sent)</span><br><span class="line">    <span class="comment">#return ss</span></span><br><span class="line">    <span class="keyword">return</span> np.argmax(<span class="built_in">list</span>(ss.values())[:-<span class="number">1</span>])</span><br><span class="line">  </span><br><span class="line"> </span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[nltk_data] Downloading package vader_lexicon to</span></span><br><span class="line"><span class="string">[nltk_data]     /Users/xx/nltk_data...</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;polarity&#x27;</span>]=df[<span class="string">&#x27;headline_text&#x27;</span>].\</span><br><span class="line">    <span class="built_in">map</span>(<span class="keyword">lambda</span> x: get_vader_score(x))</span><br><span class="line">polarity=df[<span class="string">&#x27;polarity&#x27;</span>].replace(&#123;<span class="number">0</span>:<span class="string">&#x27;neg&#x27;</span>,<span class="number">1</span>:<span class="string">&#x27;neu&#x27;</span>,<span class="number">2</span>:<span class="string">&#x27;pos&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">plt.bar(polarity.value_counts().index,</span><br><span class="line">        polarity.value_counts())</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831000924.png?imgNote" alt="image-20210831000924225" /><figcaption aria-hidden="true">image-20210831000924225</figcaption>
</figure>
<p>Yes, the distribution is slightly different. There are even more headlines classified as neutral 85%, and the number of negative news headlines has increased (to 13%).</p>
<h2 id="named-entity-recognition">Named Entity Recognition</h2>
<p>Named entity recognition is an information extraction method in which entities existing in the text are classified into predefined entity types, such as "person", "location", "organization" and so on. By using NER, we can gain insight into the entities that exist in a given text data set of entity types.</p>
<p>Let us consider an example of a news article.</p>
<p>In the above news, the named entity recognition model should be able to recognize Entities, such as RBI as an organization, Mumbai and India as Places, etc.</p>
<p>There are three standard libraries for named entity recognition:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://nlp.stanford.edu/software/CRF-NER.shtml?ref=hackernoon.com">Stanford Nell</a></li>
<li><a target="_blank" rel="noopener" href="https://spacy.io/?ref=hackernoon.com">space</a></li>
<li><a target="_blank" rel="noopener" href="https://www.nltk.org/?ref=hackernoon.com">NLTK</a></li>
</ul>
<p><strong>I will use spaCy</strong>, which is an open source library for advanced natural language processing tasks. It is written in Cython and is known for its industrial applications. In addition to NER, <strong>spaCy also provides many other functions, such as pos mark, word to vector conversion, etc. </strong></p>
<p><a target="_blank" rel="noopener" href="https://spacy.io/api/annotation?ref=hackernoon.com#section-named-entities">SpaCy’s Named Entity Recognition</a> has been published in <a href="https://catalog.ldc.upenn.edu%20/LDC2013T19?ref=hackernoon.com">OntoNotes 5</a> has been trained on the corpus and supports the following entity types</p>
<p>There are three kinds of <a target="_blank" rel="noopener" href="https://spacy.io/models/en/?ref=hackernoon.com">pre-trained models for English</a> in SpaCy. I will use <em>en_core_web_sm</em> to complete our task, but you can try other models.</p>
<p>To use it, we must first download it:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !python -m spacy download en_core_web_sm</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Now we can initialize the language model:</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> spacy <span class="keyword">import</span> displacy</span><br><span class="line"><span class="keyword">import</span> en_core_web_sm</span><br><span class="line"></span><br><span class="line">nlp = en_core_web_sm.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># nlp = spacy.load(&quot;en_core_web_sm&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># One of the advantages of Spacy is that we only need to apply the nlp function once, and the entire background pipeline will return the objects we need</span></span><br><span class="line"></span><br><span class="line">doc=nlp(<span class="string">&#x27;India and Iran have agreed to boost the economic \</span></span><br><span class="line"><span class="string">viability of the strategic Chabahar port through various measures, \</span></span><br><span class="line"><span class="string">including larger subsidies to merchant shipping firms using the facility, \</span></span><br><span class="line"><span class="string">people familiar with the development said on Thursday.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">[(x.text,x.label_) <span class="keyword">for</span> x <span class="keyword">in</span> doc.ents]</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[(&#x27;India&#x27;, &#x27;GPE&#x27;), (&#x27;Iran&#x27;, &#x27;GPE&#x27;), (&#x27;Chabahar&#x27;, &#x27;GPE&#x27;), (&#x27;Thursday&#x27;, &#x27;DATE&#x27;)]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>We can see that India and Iran are confirmed as geographic locations (GPE), Chabahar is confirmed as a person, and Thursday is confirmed as a date.</p>
<p>We can also use the display module in spaCy to visualize the output.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> spacy <span class="keyword">import</span> displacy</span><br><span class="line"></span><br><span class="line">displacy.render(doc, style=<span class="string">&#x27;ent&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831001008.png?imgNote" alt="image-20210831001008590" /><figcaption aria-hidden="true">image-20210831001008590</figcaption>
</figure>
<p>This can make sentences with recognized entities look very neat, and each entity type is marked with a different color.</p>
<p>Now that we know how to perform NER, we can further explore the data by performing various visualizations on the named entities extracted from the data set.</p>
<p>First, we will run named entity recognition on news headlines and store entity types.</p>
<h3 id="ner-analysis">NER Analysis</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ner</span>(<span class="params">text</span>):</span><br><span class="line">    doc=nlp(text)</span><br><span class="line">    <span class="keyword">return</span> [X.label_ <span class="keyword">for</span> X <span class="keyword">in</span> doc.ents]</span><br><span class="line">  </span><br><span class="line">ent=df[<span class="string">&#x27;headline_text&#x27;</span>].apply(<span class="keyword">lambda</span> x : ner(x))</span><br><span class="line">ent=[x <span class="keyword">for</span> sub <span class="keyword">in</span> ent <span class="keyword">for</span> x <span class="keyword">in</span> sub]</span><br><span class="line">counter=Counter(ent)</span><br><span class="line">count=counter.most_common()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we can visualize the entity frequency:</span></span><br><span class="line">x,y=<span class="built_in">map</span>(<span class="built_in">list</span>,<span class="built_in">zip</span>(*count))</span><br><span class="line">sns.barplot(x=y,y=x)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831001044.png?imgNote" alt="image-20210831001044045" /><figcaption aria-hidden="true">image-20210831001044045</figcaption>
</figure>
<p>Now we can see that GPE and ORG dominate the headlines, followed by the PERSON entity.</p>
<p>We can also visualize the most common tokens for each entity. Let's check which places appear the most in news headlines.</p>
<h3 id="most-common-gpe">Most common GPE</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ner</span>(<span class="params">text,ent=<span class="string">&quot;GPE&quot;</span></span>):</span><br><span class="line">    doc=nlp(text)</span><br><span class="line">    <span class="keyword">return</span> [X.text <span class="keyword">for</span> X <span class="keyword">in</span> doc.ents <span class="keyword">if</span> X.label_ == ent]</span><br><span class="line">  </span><br><span class="line">gpe=df[<span class="string">&#x27;headline_text&#x27;</span>].apply(<span class="keyword">lambda</span> x: ner(x,<span class="string">&quot;GPE&quot;</span>))</span><br><span class="line">gpe=[i <span class="keyword">for</span> x <span class="keyword">in</span> gpe <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">counter=Counter(gpe)</span><br><span class="line"></span><br><span class="line">x,y=<span class="built_in">map</span>(<span class="built_in">list</span>,<span class="built_in">zip</span>(*counter.most_common(<span class="number">10</span>)))</span><br><span class="line">sns.barplot(y,x)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831001111.png?imgNote" alt="image-20210831001111535" /><figcaption aria-hidden="true">image-20210831001111535</figcaption>
</figure>
<p>I think we can confirm the fact that "America" means America in news headlines. Let's also find the most common names that appear on news headlines.</p>
<h3 id="most-common-person">Most common person</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">per=df[<span class="string">&#x27;headline_text&#x27;</span>].apply(<span class="keyword">lambda</span> x: ner(x,<span class="string">&quot;PERSON&quot;</span>))</span><br><span class="line">per=[i <span class="keyword">for</span> x <span class="keyword">in</span> per <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">counter=Counter(per)</span><br><span class="line"></span><br><span class="line">x,y=<span class="built_in">map</span>(<span class="built_in">list</span>,<span class="built_in">zip</span>(*counter.most_common(<span class="number">10</span>)))</span><br><span class="line">sns.barplot(y,x)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831001135.png?imgNote" alt="image-20210831001135765" /><figcaption aria-hidden="true">image-20210831001135765</figcaption>
</figure>
<p>Saddam Hussein and George Bush served as presidents of Iraq and the United States during the war. In addition, we can see that the model is far from perfect to classify "vic govt" or "nsw govt" as individuals rather than government agencies.</p>
<h3 id="pos-tagging">Pos tagging</h3>
<p>Use nltk for all parts of speech markup, but there are other libraries that can do the job well (spaacy, textblob).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.download(<span class="string">&#x27;averaged_perceptron_tagger&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sentence=<span class="string">&quot;The greatest comeback stories in 2019&quot;</span></span><br><span class="line">tokens=word_tokenize(sentence)</span><br><span class="line">nltk.pos_tag(tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Notice:</span></span><br><span class="line"><span class="comment"># You can also use the spacy.displacy module to visualize the sentence part of the speech and its dependency graph.</span></span><br><span class="line"></span><br><span class="line">doc = nlp(<span class="string">&#x27;The greatest comeback stories in 2019&#x27;</span>)</span><br><span class="line">displacy.render(doc, style=<span class="string">&#x27;dep&#x27;</span>, jupyter=<span class="literal">True</span>, options=&#123;<span class="string">&#x27;distance&#x27;</span>: <span class="number">90</span>&#125;)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831001212.png?imgNote" alt="image-20210831001212360" /><figcaption aria-hidden="true">image-20210831001212360</figcaption>
</figure>
<p>We can observe various dependency labels here. For example, the DET tag indicates the relationship between the word "the" and the noun "stories".</p>
<p>You can check the list of dependency labels and their meanings <a target="_blank" rel="noopener" href="https://universaldependencies.org/u/dep/index.html?ref=hackernoon.com">here</a>.</p>
<p>Okay, now that we know what a POS tag is, let's use it to explore the title data set.</p>
<h3 id="analysing-pos-tags">Analysing pos tags</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pos</span>(<span class="params">text</span>):</span><br><span class="line">    pos=nltk.pos_tag(word_tokenize(text))</span><br><span class="line">    pos=<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">list</span>,<span class="built_in">zip</span>(*pos)))[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> pos</span><br><span class="line">  </span><br><span class="line">tags=df[<span class="string">&#x27;headline_text&#x27;</span>].apply(<span class="keyword">lambda</span> x : pos(x))</span><br><span class="line">tags=[x <span class="keyword">for</span> l <span class="keyword">in</span> tags <span class="keyword">for</span> x <span class="keyword">in</span> l]</span><br><span class="line">counter=Counter(tags)</span><br><span class="line">x,y=<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">list</span>,<span class="built_in">zip</span>(*counter.most_common(<span class="number">7</span>))))</span><br><span class="line"></span><br><span class="line">sns.barplot(x=y,y=x)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://qiniu.hivan.me/picGo/20210831001251.png?imgNote" alt="image-20210831001251251" /><figcaption aria-hidden="true">image-20210831001251251</figcaption>
</figure>
<p>We can clearly see that nouns (NN) dominate in news headlines, followed by adjectives (JJ). This is typical for news reports, and for art forms, higher adjective (ADJ) frequencies may happen a lot.</p>
<p>You can investigate this in more depth by investigating the most common singular nouns in news headlines. Let us find out.</p>
<p>Nouns such as "war", "Iraq", and "person" dominate the news headlines. You can use the above functions to visualize and check other parts of the voice.</p>
<h3 id="most-common-nouns">Most common Nouns</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_adjs</span>(<span class="params">text</span>):</span><br><span class="line">    adj=[]</span><br><span class="line">    pos=nltk.pos_tag(word_tokenize(text))</span><br><span class="line">    <span class="keyword">for</span> word,tag <span class="keyword">in</span> pos:</span><br><span class="line">        <span class="keyword">if</span> tag==<span class="string">&#x27;NN&#x27;</span>:</span><br><span class="line">            adj.append(word)</span><br><span class="line">    <span class="keyword">return</span> adj</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">words=df[<span class="string">&#x27;headline_text&#x27;</span>].apply(<span class="keyword">lambda</span> x : get_adjs(x))</span><br><span class="line">words=[x <span class="keyword">for</span> l <span class="keyword">in</span> words <span class="keyword">for</span> x <span class="keyword">in</span> l]</span><br><span class="line">counter=Counter(words)</span><br><span class="line"></span><br><span class="line">x,y=<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">list</span>,<span class="built_in">zip</span>(*counter.most_common(<span class="number">7</span>))))</span><br><span class="line">sns.barplot(x=y,y=x)</span><br></pre></td></tr></table></figure>
<h3 id="dependency-graph">Dependency graph</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">doc = nlp(<span class="string">&#x27;She sells seashells by  the seashore&#x27;</span>)</span><br><span class="line">displacy.render(doc, style=<span class="string">&#x27;dep&#x27;</span>, jupyter=<span class="literal">True</span>, options=&#123;<span class="string">&#x27;distance&#x27;</span>: <span class="number">90</span>&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="text-readability">Text readability</h2>
<h3 id="textstat">Textstat</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> textstat <span class="keyword">import</span> flesch_reading_ease</span><br><span class="line">df[<span class="string">&#x27;headline_text&#x27;</span>].apply(<span class="keyword">lambda</span> x : flesch_reading_ease(x)).hist()</span><br></pre></td></tr></table></figure>
<h3 id="complex-headlines">complex headlines?</h3>
<p>Almost all readability scores exceed 60. This means that an average of 11-year-old students can read and understand news headlines. Let's check all news headlines with a readability score below 5.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x=[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(reading)) <span class="keyword">if</span> reading[i]&lt;<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">rror loading preloads:</span></span><br><span class="line"><span class="string">Failed to fetch dynamically imported module: https://file+.vscode-resource.vscode-webview.net/Users/xx/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/datascience-ui/errorRenderer/errorRenderer.js</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">news.iloc[x][<span class="string">&#x27;headline_text&#x27;</span>].head()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Error loading preloads:</span></span><br><span class="line"><span class="string">Failed to fetch dynamically imported module: https://file+.vscode-resource.vscode-webview.net/Users/xx/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/datascience-ui/errorRenderer/errorRenderer.js</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="final-thoughts">Final thoughts</h3>
<p>In this article, we discussed and implemented various exploratory data analysis methods for text data. Some are common and little known, but all of them can be an excellent addition to your data exploration toolkit.</p>
<p>Hope you will find some of them useful for your current and future projects.</p>
<p>To make data exploration easier, I created a "exploratory data analysis of natural language processing templates", which you can use for your work.</p>
<p>In addition, you may have seen that for each chart in this article, there is a code snippet to create it. Just click the button below the chart.</p>
<p>Happy exploring!</p>
<p>From: https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Initial exploration of machine learning</p><p><a href="https://hivan.me/example_02/">https://hivan.me/example_02/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hivan Du</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-09-02</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2023-06-02</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=6479444288ae9600196fa98e&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="https://afdian.net/item/72907364008511ee904852540025c377" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://qiniu.hivan.me/picGo/20230601221633.jpeg" alt="支付宝"></span></a><a class="button donate" href="https://www.buymeacoffee.com/hivandu" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="https://patreon.com/user?u=89473430" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="doo@hivan.me"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://qiniu.hivan.me/IMG_4603.JPG" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/example_07/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Advanced Deep Learning</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/example_03/"><span class="level-item">Machine Learning Part-01</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://hivan.me/example_02/';
            this.page.identifier = 'example_02/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'hivan' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://www.gravatar.com/avatar/bdff168cf8a71c11d2712a1679a00c54?s=128" alt="茶桁"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">茶桁</p><p class="is-size-6 is-block">AI游民</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shang Hai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">143</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">13</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hivandu" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hivan"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/hivan"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com/hivan"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.zhihu.com/column/c_1424326166602178560" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">塌缩的奇点</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li><li><a class="level is-mobile" href="https://www.zhihu.com/column/hivandu" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">茶桁-知乎</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="level-start"><span class="level-item">从零开始接触人工智能大模型</span></span><span class="level-end"><span class="level-item tag">19</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-30T11:48:02.000Z">2023-06-30</time></p><p class="title"><a href="/%E6%88%91%E4%BB%AC%E6%97%A0%E6%B3%95%E9%80%9A%E8%BF%87%E6%94%B9%E9%80%A0%E8%87%AA%E5%B7%B1%E6%91%86%E8%84%B1%E6%B0%94%E5%80%99%E5%8D%B1%E6%9C%BA/">观点：我们无法通过改造自己摆脱气候危机</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-14T14:36:13.000Z">2023-06-14</time></p><p class="title"><a href="/%E5%88%A9%E7%94%A8LangChain%E8%AE%A9AI%E5%81%9A%E5%86%B3%E7%AD%96/">17. 利用LangChain让AI做决策</a></p><p class="categories"><a href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-10T14:16:00.000Z">2023-06-10</time></p><p class="title"><a href="/Langchain%E8%AE%A9AI%E6%8B%A5%E6%9C%89%E8%AE%B0%E5%BF%86%E5%8A%9B/">16. Langchain让AI拥有记忆力</a></p><p class="categories"><a href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-05T04:23:27.000Z">2023-06-05</time></p><p class="title"><a href="/%E4%BD%BF%E7%94%A8LLMChain%E8%BF%9E%E6%8E%A5Google%E5%92%8C%E8%AE%A1%E7%AE%97%E5%99%A8/">15. 使用LLMChain连接Google和计算器</a></p><p class="categories"><a href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-02T09:14:18.000Z">2023-06-02</time></p><p class="title"><a href="/%E4%BD%BF%E7%94%A8%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%E7%AE%80%E5%8C%96%E5%A4%9A%E6%AD%A5%E6%8F%90%E7%A4%BA%E8%AF%AD/">14. 使用链式调用简化多步提示语</a></p><p class="categories"><a href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a><p class="is-size-7"><span>&copy; 2023 Hivan Du</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>