<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>11 用好开源模型节约成本 - 茶桁.MAMT</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="茶桁.MAMT"><meta name="msapplication-TileImage" content="https://qiniu.hivan.me/picGo/20230601174411.png?imgNote"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="茶桁.MAMT"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Hi， 大家好，我是茶桁。 直奔主题，我们来谈谈成本这件事。"><meta property="og:type" content="blog"><meta property="og:title" content="11 用好开源模型节约成本"><meta property="og:url" content="https://hivan.me/Save-costs-with-an-open-source-model/"><meta property="og:site_name" content="茶桁.MAMT"><meta property="og:description" content="Hi， 大家好，我是茶桁。 直奔主题，我们来谈谈成本这件事。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hivan.me/Save-costs-with-an-open-source-model/20230601171522.png"><meta property="og:image" content="https://hivan.me/Save-costs-with-an-open-source-model/20230601171529.png"><meta property="article:published_time" content="2023-05-20T09:14:42.000Z"><meta property="article:modified_time" content="2023-06-01T13:02:08.461Z"><meta property="article:author" content="Hivan Du"><meta property="article:tag" content="AI"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://hivan.me/Save-costs-with-an-open-source-model/20230601171522.png"><meta property="twitter:creator" content="@hivan"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hivan.me/Save-costs-with-an-open-source-model/"},"headline":"11 用好开源模型节约成本","image":["https://hivan.me/Save-costs-with-an-open-source-model/20230601171522.png","https://hivan.me/Save-costs-with-an-open-source-model/20230601171529.png"],"datePublished":"2023-05-20T09:14:42.000Z","dateModified":"2023-06-01T13:02:08.461Z","author":{"@type":"Person","name":"Hivan Du"},"publisher":{"@type":"Organization","name":"茶桁.MAMT","logo":{"@type":"ImageObject","url":"https://hivan.me/img/logo.svg"}},"description":"Hi， 大家好，我是茶桁。 直奔主题，我们来谈谈成本这件事。"}</script><link rel="canonical" href="https://hivan.me/Save-costs-with-an-open-source-model/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="茶桁.MAMT" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-05-20T09:14:42.000Z" title="5/20/2023, 5:14:42 PM">2023-05-20</time>发表</span><span class="level-item"><a class="link-muted" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a></span></div></div><h1 class="title is-3 is-size-4-mobile">11 用好开源模型节约成本</h1><div class="content"><p>Hi， 大家好，我是茶桁。</p>
<p>直奔主题，我们来谈谈成本这件事。</p>
<span id="more"></span>
<p>大家应该都知道，ChatGPT对免费用户是有5美元的API调用额度的，说是这么说，可是那是以前，现在新注册的小伙伴应该都发现自己的API Key根本无法调用API，原因是这个免费额度似乎已经失效了。而我可以直接说，在我从第一节到第10节的课程中所用到的金额，已经超过这个数目了。也就是说，我这10节课API调用成本就已经超过了40元人民币。</p>
<p>看到这大家大概能理解我这个课程为什么改为付费课程了吧？</p>
<p>对于 <code>ChatCompletion</code> 的接口来说，为了更好地使用它，我们需要传入更多的上下文信息，以便更准确地进行文本生成。不过要注意的是，实际消耗的 Token 数量可能比我们感觉的要多。此外，除了费用之外，数据安全也是我们需要考虑的一个问题。由于每个国家的数据监管要求不同，不是所有的数据都适合通过 OpenAI 的 API 来处理。因此，我们需要寻找一个除 OpenAI 以外的解决方案。幸运的是，有一些开源的大语言模型可以帮助我们解决这个问题。通过利用这些开源的模型，中小型公司也可以轻松地获得更准确、更安全的文本生成服务。</p>
<h3 id="在-colab-中使用-gpu">在 Colab 中使用 GPU</h3>
<p>在本课中，我们需要使用一些开源模型。但是，并不是所有人的电脑都配备了强劲的 NVIDIA GPU。因此，我建议您使用 Colab 运行相应的笔记本，并注意将运行环境设置为 GPU。</p>
<p>如下图，选择 <code>代码执行程序-&gt;更改运行时类型</code>,然后在硬件加速器上选择 <code>GPU</code> 就可以了。</p>
<img src="/Save-costs-with-an-open-source-model/20230601171522.png" class="" title="img">
<img src="/Save-costs-with-an-open-source-model/20230601171529.png" class="" title="img">
<p>当然，有的小伙伴应该是看到有一个警告：想要使用付费GPU，购买额外的计算单元。 暂时不需要理会它，只要用的多，Colab的GPU是有免费额度的。</p>
<h3 id="开源伙伴huggingfaceembedding">开源伙伴：HuggingfaceEmbedding</h3>
<p>在第四讲中，我们使用了 Google 开源的 T5 模型来比较零样本分类效果。尽管该模型的效果不如 OpenAI 的 API，准确率只有90%，但也算是相当不错的了。这也让我们想到，上一讲中使用的 llama-index 向量搜索部分是否可以使用开源模型的 Embedding 进行替换呢？</p>
<p>答案是肯定的，llama-index 允许您直接定义一个定制化的 Embedding。我将相关代码放在了下面，您可以参考它进行操作。这种方法不仅可以大大增加文本长度，还可以保持原有的关键思想。</p>
<p>在继续之前，我们需要安装一下 <code>sentence-transformers</code> 这个库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge sentence-transformers</span><br></pre></td></tr></table></figure>
<p>然后执行如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需库</span></span><br><span class="line"><span class="keyword">import</span> openai, os</span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, LangchainEmbedding, ServiceContext, StorageContext, GPTVectorStoreIndex, load_index_from_storage</span><br><span class="line"><span class="keyword">from</span> llama_index.vector_stores.faiss <span class="keyword">import</span> FaissVectorStore</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings.huggingface <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.node_parser <span class="keyword">import</span> SimpleNodeParser</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Markdown, display</span><br><span class="line"></span><br><span class="line"><span class="comment"># API Key</span></span><br><span class="line">os.environ[<span class="string">&#x27;OPENAI_API_KEY&#x27;</span>] = <span class="string">&#x27;OPENAI_API_KEY&#x27;</span></span><br><span class="line">openai.api_key = <span class="string">&quot;&quot;</span> </span><br><span class="line"></span><br><span class="line">text_splitter = CharacterTextSplitter(separator = <span class="string">&#x27;\n\n&#x27;</span>, chunk_size = <span class="number">100</span>, chunk_overlap = <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">dimension = <span class="number">768</span></span><br><span class="line">faiss_index = faiss.IndexFlatIP(dimension)</span><br><span class="line"></span><br><span class="line">parser = SimpleNodeParser(text_splitter = text_splitter)</span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&#x27;./data/faq/&#x27;</span>).load_data()</span><br><span class="line">nodes = parser.get_nodes_from_documents(documents)</span><br><span class="line"></span><br><span class="line">embed_model = LangchainEmbedding(HuggingFaceEmbeddings(</span><br><span class="line">    model_name = <span class="string">&#x27;sentence-transformers/paraphrase-multilingual-mpnet-base-v2&#x27;</span></span><br><span class="line">))</span><br><span class="line"></span><br><span class="line">vector_store = FaissVectorStore(faiss_index = faiss_index)</span><br><span class="line">service_context = ServiceContext.from_defaults(embed_model = embed_model)</span><br><span class="line">index = GPTVectorStoreIndex.from_documents(documents = documents, service_context=service_context)</span><br><span class="line">query_engine = index.as_query_engine()</span><br></pre></td></tr></table></figure>
<p>我们使用了一个面向电商 FAQ 的纯文本文件作为输入。里面包含了预设好的 FAQ 问答对。为了确保我们没有使用 OpenAI 的 API，我们将 openai.api_key 设置为空字符串。然后定义了一个 <code>embeded_model</code>，它封装了 HuggingFaceEmbeddings 类。HuggingFaceEmbeddings 可以下载、加载并计算输入文本的嵌入向量，因为 HuggingFace 为基于 transformers 的模型定义了一个标准，所以你可以使用一套代码来使用所有 transformers 类型的模型。 <code>sentence-transformers</code>是目前效果最好的语义搜索模型，它包括一系列预训练模型，其中 <code>paraphrase-multilingual-mpnet-base-v2</code> 模型支持多语言和将语句和段落转换为向量。由于我们的示例都是中文，所以选择了这个模型。你可以根据实际问题选择适合自己的模型。</p>
<p>我们使用 Faiss 库作为向量索引库，定义向量维度为 768，与 <code>paraphrase-multilingual-mpnet-base-v2</code> 模型的维度相同。</p>
<p>对于文档的切分，我们使用 CharacterTextSplitter，并对参数进行了调整。我们使用 "" 作为分段符， <code>chunk_size</code> 设置为 100， <code>chunk_overlap</code> 设为最大 100。</p>
<p>Embedding 使用了 3198 个 Token，不过这些 Token 都是通过 sentence_transformers 模型计算的，不需要花费额外成本。完成索引创建后，我们可以使用常见的电商类型 FAQ 问题测试。</p>
<p>接下来让我们问点问题试试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openai.api_key = os.environ.get(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line">response = query_engine.query(<span class="string">&quot;请问你们海南能发货吗？&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<p>输出的结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A: 是的，我们支持海南地区的发货。您可以在下单时选择您的收货地址，我们会根据您的地址信息提供相应的物流服务。</span><br></pre></td></tr></table></figure>
<p>继续第二个问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">response = query_engine.query(<span class="string">&quot;你们用那些快递公司送货？&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们与顺丰速运、圆通速递、申通快递、韵达快递、中通快递、百世快递等多家知名快递公司合作。</span><br></pre></td></tr></table></figure>
<p>第三个问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">response = query_engine.query(<span class="string">&quot;你们的退货政策是怎么样的？&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(response)　</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A: 自收到商品之日起<span class="number">7</span>天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。</span><br></pre></td></tr></table></figure>
<p>我们使用 Embedding 模式来查询。通过三个典型问题的测试，AI 的回答都正确，效果不错。</p>
<h3 id="使用chatglm">使用ChatGLM</h3>
<p>通过上面的代码，我们已经完成了生成 Embedding 和利用 Embedding 进行搜索的过程。在实际的问答过程中，我们仍然在使用 OpenAI 的 Completion API。是否有可能将其替换为其他模型呢？我们可以尝试一下来自清华大学的 ChatGLM 语言模型，看看中文的开源语言模型是否具备基本的知识理解和推理能力。为此，我们需要安装一些依赖包。由于 icetk 没有 Conda 的源，因此我们使用 pip 进行安装。当然，在 Conda 的包管理器中也同样可以找到它。</p>
<blockquote>
<p>这里大家可能会遇到一些坑，比如，protobuf这个库的版本不兼容，这里有一个矛盾点，就是icetk这个库依赖的是3.19以下的版本，而transformers却需要以上的版本。</p>
</blockquote>
<p>遇到上述问题的时候，我们可以这样操作一下：</p>
<p>先安装最新版本的，然后将其中的builder,py下载下来，再从新安装3.18版本，接着替换builder.py就可以了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置本地环境为UTF-8</span></span><br><span class="line"><span class="keyword">import</span> locale</span><br><span class="line">locale.setlocale(locale.LC_ALL, <span class="string">&#x27;en_US.UTF-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装最新版本</span></span><br><span class="line">!pip install protobuf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询路径</span></span><br><span class="line"><span class="keyword">import</span> google.protobuf <span class="keyword">as</span> protobuf</span><br><span class="line"><span class="built_in">print</span>(protobuf.__path__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载文件</span></span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> files</span><br><span class="line">files.download(<span class="string">&#x27;/usr/local/lib/python3.10/dist-packages/google/protobuf/internal/builder.py&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载并重新安装</span></span><br><span class="line">!pip uninstall protobuf</span><br><span class="line">!pip install protobuf=<span class="number">3.18</span><span class="number">.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上传文件</span></span><br><span class="line">uploadded = files.upload()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 替换文件</span></span><br><span class="line">shutil.move(<span class="string">&#x27;builder.py&#x27;</span>, <span class="string">&#x27;/usr/local/lib/python3.10/dist-packages/google/protobuf/internal/builder.py&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>然后，我们就可以继续了，以下代码我在本地（M1 Mac）跑了一遍，在120分钟之后，仍然还在继续，最后我不得不放弃转而使用Colab了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!pip install icetk</span><br><span class="line">!pip install cpm_kernels</span><br><span class="line">!pip install transformers</span><br></pre></td></tr></table></figure>
<p>我们可以通过 transformers 加载模型。最大的一个 <a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-130B">ChatGLM</a> 模型有 1300 亿个参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;THUDM/chatglm-6b-int4&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;THUDM/chatglm-6b-int4&quot;</span>, trust_remote_code=<span class="literal">True</span>).half().cuda()</span><br><span class="line">model = model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<p>为了能够运行这个大型模型，我们选择了一个裁剪后的 60 亿个参数的版本，使用 int-4 量化的方式。我们的模型名字是 chatglm-6b-int4，代表 6B 的参数量。我们希望通过 GPU 进行模型的计算，所以在加载模型的时候调用了.cuda()。加载模型时，我们设置了 trust_remote_code = true 参数，以便确认您信任该模型的代码，它不会造成恶意的破坏。如果您想要用 CPU 运行，可以使用下面的代码。</p>
<blockquote>
<p>虽然建议使用GPU，但是如果你想用CPU的话，可以把模型加载的代码替换一下：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;THUDM/chatglm-6b-int4&quot;</span>, trust_remote_code=<span class="literal">True</span>).<span class="built_in">float</span>()</span><br></pre></td></tr></table></figure>
<p>特别是在Mac M1下，你除了使用CPU运行的话，似乎没有其他办法。</p>
<blockquote>
<p>写给M1用户</p>
</blockquote>
<p>ChatGLM如果想要在M1内运行的话，可能你会遇到如下报错：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Could <span class="keyword">not</span> find module <span class="string">&#x27;nvcuda.dll&#x27;</span> </span><br><span class="line">或者 </span><br><span class="line">RuntimeError: Unknown platform: darwin (MacOS)</span><br></pre></td></tr></table></figure>
<p>如果遇到这种问题的话，可以参看官方文档这里：</p>
<p>https://github.com/THUDM/ChatGLM-6B/issues/6#issuecomment-1471303336</p>
<p>然后我们来测试模型进行回答：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">自收到商品之日起7天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">根据以上信息，请回答下面的问题：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Q: 你们的退货政策是怎么样的？</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">response, history = model.chat(tokenizer, question, history=[])</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们的退货政策是在产品未使用、包装完好的情况下，自收到商品之日起<span class="number">7</span>天内可以退货。请注意，某些特殊商品可能不支持退货，在购买前请查看商品详情页面的退货政策，以了解具体情况。</span><br></pre></td></tr></table></figure>
<p>ChatGLM的回答是根据我们提供的上下文来的。如果我们去掉提供的FAQ的内容，它的回答会有所变化。</p>
<p>下一个问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Q: 你们的退货政策是怎么样的？</span></span><br><span class="line"><span class="string">A: </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">response, history = model.chat(tokenizer, question, history=[])</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">我们公司的政策是，如果您在购买后的<span class="number">7</span>天内发现我们的产品不符合您的期望，可以申请退货。退货条件如下：</span><br><span class="line"><span class="number">1.</span> 符合退货政策要求的产品；</span><br><span class="line"><span class="number">2.</span> 未使用或损坏的产品；</span><br><span class="line"><span class="number">3.</span> 符合退货地点的要求，即退货地址与购买地址相同；</span><br><span class="line"><span class="number">4.</span> 符合退货流程的要求，即您需要提供有效的退货单并完成退货流程。</span><br><span class="line">如果符合上述退货政策要求，我们将尽快处理您的退货申请，并为您退款。请注意，如果您已经使用了我们的产品，或者产品有损坏或变质等情况，我们将无法为您提供退货服务。</span><br></pre></td></tr></table></figure>
<p>在没有合适的上下文信息时，AI编造了一些本来不存在的要求，例如“退货地址与购买地址相同”。另一个例子是，当我们的上下文只提到可以将物品送到海南地区时，AI能否判断是否支持送到三亚市。</p>
<p>继续问问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">我们支持全国大部分省份的配送，包括北京、上海、天津、重庆、河北、山西、辽宁、吉林、黑龙江、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">根据以上信息，请回答下面的问题：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Q: 你们能配送到三亚吗？</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">response, history = model.chat(tokenizer, question, history=[])</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">是的，我们支持全国大部分省份的配送，包括三亚市。</span><br></pre></td></tr></table></figure>
<p>ChatGLM可以配送到三亚。如果想知道它是否可以送到哈尔滨，可以简单地在上下文中排除东三省并询问。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">我们支持全国大部分省份的配送，包括北京、上海、天津、重庆、河北、山西、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆.但是不能配送到东三省</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">根据以上信息，请回答下面的问题：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Q: 你们能配送到哈尔滨吗？</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">response, history = model.chat(tokenizer, question, history=[])</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">很抱歉，我们目前不能配送到哈尔滨。</span><br></pre></td></tr></table></figure>
<p>ChatGLM 可以回答我们无法发送到哈尔滨的问题，因此我们可以使用 ChatGLM 来处理 FAQ。</p>
<h3 id="封装llm">封装LLM</h3>
<p>我们使用原始的 ChatGLM 模型代码，无法直接通过查询来访问 llama-index 获取答案。要实现这一点，我们将其封装为 LLM 类，使我们的索引使用指定的大语言模型即可。您可以查看相应的 <a target="_blank" rel="noopener" href="https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html">llama-index 文档。</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai, os</span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> SimpleDirectoryReader, LangchainEmbedding, ServiceContext,</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings.huggingface <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.node_parser <span class="keyword">import</span> SimpleNodeParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.llms.base <span class="keyword">import</span> LLM</span><br><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> LLMPredictor</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>, <span class="type">List</span>, Mapping, <span class="type">Any</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomLLM</span>(<span class="title class_ inherited__">LLM</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_call</span>(<span class="params">self, prompt: <span class="built_in">str</span>, stop: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">str</span>]] = <span class="literal">None</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        response, history = model.chat(tokenizer, prompt, history=[])</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_identifying_params</span>(<span class="params">self</span>) -&gt; Mapping[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;name_of_model&quot;</span>: <span class="string">&quot;chatglm-6b-int4&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_llm_type</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;custom&quot;</span></span><br></pre></td></tr></table></figure>
<p>我们将 CustomLLM 对象传入 index 构造函数并重新运行问题，以观察其效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> SpacyTextSplitter</span><br><span class="line"></span><br><span class="line">llm_predictor = LLMPredictor(llm=CustomLLM())</span><br><span class="line"></span><br><span class="line">text_splitter = CharacterTextSplitter(separator=<span class="string">&quot;\n\n&quot;</span>, chunk_size=<span class="number">100</span>, chunk_overlap=<span class="number">20</span>)</span><br><span class="line">parser = SimpleNodeParser(text_splitter=text_splitter)</span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&#x27;./data/faq/&#x27;</span>).load_data()</span><br><span class="line">nodes = parser.get_nodes_from_documents(documents)</span><br><span class="line"></span><br><span class="line">embed_model = LangchainEmbedding(HuggingFaceEmbeddings(</span><br><span class="line">    model_name=<span class="string">&quot;sentence-transformers/paraphrase-multilingual-mpnet-base-v2&quot;</span></span><br><span class="line">))</span><br><span class="line">service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)</span><br><span class="line"></span><br><span class="line">dimension = <span class="number">768</span></span><br><span class="line">faiss_index = faiss.IndexFlatIP(dimension)</span><br><span class="line">vector_store = FaissVectorStore(faiss_index = faiss_index)</span><br><span class="line">service_context = ServiceContext.from_defaults(embed_model = embed_model)</span><br><span class="line">index = GPTVectorStoreIndex.from_documents(documents = documents, service_context=service_context)</span><br><span class="line">query_engine = index.as_query_engine()</span><br><span class="line">response = index.query(</span><br><span class="line">    <span class="string">&quot;请问你们海南能发货吗？&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">海南能发货。</span><br></pre></td></tr></table></figure>
<p>现在，我们可以直接使用 ChatGLM 的模型进行 FAQ问答。我们的解决方案是使用 paraphrase-multilingual-mpnet-base-v2 模型计算 Embedding 进行语义搜索，然后通过 chatglm-6b-int4 模型来解决问答。这两个模型可以在家用级别的显卡上运行。非常厉害！</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>11 用好开源模型节约成本</p><p><a href="https://hivan.me/Save-costs-with-an-open-source-model/">https://hivan.me/Save-costs-with-an-open-source-model/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hivan Du</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2023-05-20</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2023-06-01</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/AI/">AI</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=6479444288ae9600196fa98e&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="https://afdian.net/item/72907364008511ee904852540025c377" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://qiniu.hivan.me/picGo/20230601221633.jpeg" alt="支付宝"></span></a><a class="button donate" href="https://www.buymeacoffee.com/hivandu" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="https://patreon.com/user?u=89473430" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="doo@hivan.me"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://qiniu.hivan.me/IMG_4603.JPG" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/AI-create-a-excel-plugin/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">12 AI帮你写个小插件，轻松处理Excel文件</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/Use-AI-to-index-and-analyze-documents-and-images/"><span class="level-item">10 利用AI索引并分析文献和图片</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://hivan.me/Save-costs-with-an-open-source-model/';
            this.page.identifier = 'Save-costs-with-an-open-source-model/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'hivan' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://www.gravatar.com/avatar/bdff168cf8a71c11d2712a1679a00c54?s=128" alt="茶桁"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">茶桁</p><p class="is-size-6 is-block">AI游民</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shang Hai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">150</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">15</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hivandu" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hivan"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/hivan"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com/hivan"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4NzE4MDQzMg==&amp;action=getalbum&amp;album_id=2932504849574543360&amp;scene=173&amp;from_msgid=2648747980&amp;from_itemidx=1&amp;count=3&amp;nolastread=1&amp;token=1758883909&amp;lang=zh_CN#wechat_redirect"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.zhihu.com/column/c_1424326166602178560" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">塌缩的奇点</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li><li><a class="level is-mobile" href="https://www.zhihu.com/column/hivandu" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">茶桁-知乎</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="level-start"><span class="level-item">从零开始接触人工智能大模型</span></span><span class="level-end"><span class="level-item tag">21</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-15T07:18:39.000Z">2023-07-15</time></p><p class="title"><a href="/%E4%BD%BF%E7%94%A8Transformers%E8%BF%9B%E8%A1%8C%E8%AF%AD%E9%9F%B3%E8%BD%AC%E6%96%87%E6%9C%AC/">使用 Transformers 进行语音转文本的完整入门指南</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-14T06:30:00.000Z">2023-07-14</time></p><p class="title"><a href="/LLMs%E7%9A%84%E5%AE%9E%E7%94%A8%E4%BB%8B%E7%BB%8D/">LLMs的实用介绍</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-10T14:52:54.000Z">2023-07-10</time></p><p class="title"><a href="/%E5%BF%AB%E9%80%9F%E5%80%BE%E5%90%AC%E5%92%8C%E6%80%BB%E7%BB%93%E9%9F%B3%E9%A2%91%E5%86%85%E5%AE%B9/">19. 快速倾听和总结音频内容</a></p><p class="categories"><a href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/">从零开始接触人工智能大模型</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-09T17:22:07.000Z">2023-07-10</time></p><p class="title"><a href="/ChatGPT%E4%BB%A3%E7%A0%81%E8%A7%A3%E9%87%8A%E5%99%A8/">ChatGPT代码解释器：如何为我节省数小时的工作</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-06T18:23:02.000Z">2023-07-07</time></p><p class="title"><a href="/%E4%BD%BF%E7%94%A8Python%E5%BA%93unstructured%E6%8F%AD%E7%A7%98%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE/">使用Python库unstructured揭秘文本数据</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a><p class="is-size-7"><span>&copy; 2023 Hivan Du</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>