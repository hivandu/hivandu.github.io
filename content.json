{"posts":[{"title":"07 AI帮你做总结","text":"Hi， 我是茶桁。 在上一节中，我们介绍了如何使用最新的ChatGPT API，注册HuggingFace账户，并将我们的聊天机器人部署出去。在这个过程中，我们学习了实际的应用开发过程，使你对聊天机器人的开发有了充足的体验。在这一讲中，我们将探讨OpenAI的各种接口提供的能力，以更深入地了解这些接口。我们将分别介绍如何利用嵌入（Embedding）进行文本聚类，并使用提示语（Prompt）对文本进行总结。此外，我们还将介绍其他的接口能力，如语言模型和自然语言生成，以帮助您更好地理解和利用OpenAI的各种功能。 基于 Embedding 向量进行文本聚类文本聚类简介 文本聚类是一种自动将未标注的文本根据相似度分成几类的方法。使用 GPT 系列模型进行文本聚类非常简单，我们可以将文本转换为向量，然后使用一些简单的聚类算法，比如最简单的 K-Means 算法。 我们使用 20 newsgroups 数据集来演示如何使用 GPT 系列模型。该数据集包含带有标注的英文新闻组数据。虽然该数据集已经经过预处理，例如去除标点符号、停用词等等，但仍可以观察到 OpenAI 的 GPT 系列模型在处理这种不太“自然语言”的数据时的效果。 我们可以使用 Python 库 scikit-learn 来获取数据。该库是一个非常常用的机器学习库，我们可以直接下载数据并将其存储为 CSV 文件。 123456789101112131415161718import sklearnfrom sklearn.datasets import fetch_20newsgroupsimport pandas as pddef data_to_csv(): # 获取数据 newsgroups_train = fetch_20newsgroups(subset = 'train', remove = {'headers', 'footers', 'quotes'}) # 转换数据 df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T df.columns = ['text', 'target'] targets = pd.DataFrame(newsgroups_train.target_names, columns = ['title']) out = pd.merge(df, targets, left_on = 'target', right_index = True) out.to_csv('./data/20_newsgroup.csv', index = False) data_to_csv() 我们需要预处理数据，过滤掉空文本和 Token 数量太多的数据，就像我们之前进行文本分类一样。 123456789101112131415161718from openai.embeddings_utils import get_embeddingsimport openai, os, tiktoken, backoffopenai.api_key = &quot;OPENAI_API_KEY&quot;embedding_model = &quot;text-embedding-ada-002&quot;embedding_encoding = &quot;cl100k_base&quot; # this the encoding for text-embedding-ada-002batch_size = 2000max_tokens = 8000 # the maximum for text-embedding-ada-002 is 8191df = pd.read_csv('20_newsgroup.csv')print(&quot;Number of rows before null filtering:&quot;, len(df))df = df[df['text'].isnull() == False]encoding = tiktoken.get_encoding(embedding_encoding)df[&quot;n_tokens&quot;] = df.text.apply(lambda x: len(encoding.encode(x)))print(&quot;Number of rows before token number filtering:&quot;, len(df))df = df[df.n_tokens &lt;= max_tokens]print(&quot;Number of rows data used:&quot;, len(df)) 数据结果： 123Number of rows before null filtering: 11314Number of rows before token number filtering: 11096Number of rows data used: 11044 我们使用 Embedding 接口获取文本的 Embedding 向量，并将数据存储为 parquet 文件。 123456789101112131415161718@backoff.on_exception(backoff.expo, openai.error.RateLimitError)def get_embeddings_with_backoff(prompts, engine): embeddings = [] for i in range(0, len(prompts), batch_size): batch = prompts[i:i+batch_size] embeddings += get_embeddings(list_of_text=batch, engine=engine) return embeddingsprompts = df.text.tolist()prompt_batches = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]embeddings = []for batch in prompt_batches: batch_embeddings = get_embeddings_with_backoff(prompts=batch, engine=embedding_model) embeddings += batch_embeddingsdf[&quot;embedding&quot;] = embeddingsdf.to_parquet(&quot;./data/20_newsgroup_with_embedding.parquet&quot;, index=False) 这段代码与前面的文本分类基本相同，不再详细讲解。在机器学习任务中，通常会将一些中间步骤的数据结果保存下来，以防止后续步骤出现错误导致需要从头开始。因此，我们保存了原始数据和 Embedding 处理后的数据。这样，如果需要修改聚类程序，我们就无需再次计算 Embedding。 接下来，我们可以使用 K-Means 算法进行聚类。由于原始数据来自 20 个不同的新闻组，因此我们将其聚合为 20 个类别，以便查看自动聚类与原始分类是否相似。 123456789101112import numpy as npfrom sklearn.cluster import KMeansembedding_df = pd.read_parquet(&quot;./data/20_newsgroup_with_embedding.parquet&quot;)matrix = np.vstack(embedding_df.embedding.values)num_of_clusters = 20kmeans = KMeans(n_clusters=num_of_clusters, init=&quot;k-means++&quot;, n_init=10, random_state=42)kmeans.fit(matrix)labels = kmeans.labels_embedding_df[&quot;cluster&quot;] = labels 我们可以用 NumPy 的 stack 函数把所有的 Embedding 放到一个矩阵里，设置聚合类的数量，然后运行 K-Means 算法的 fit 函数来聚类。聚类后，我们可以统计每个类有多少条各个 newsgroups 分组的数据，并找出排名第一的分组。如果聚类出来的类都是从某一个 newsgroup 分组出来的文章，说明这个类和那个分组的内容相似。我们可以使用这个思路的代码，详情见下方。 123456789101112131415161718192021222324# 统计每一个cluster的数量new_df = embedding_df.groupby('cluster')['cluster'].count().reset_index(name = 'count')# 统计这个cluster里最多的分类的数量title_count = embedding_df.groupby(['cluster', 'title']).size().reset_index(name = 'title_count')first_titles = title_count.groupby('cluster').apply(lambda x: x.nlargest(1, columns=['title_count']))first_titles = first_titles.reset_index(drop = True)new_df = pd.merge(new_df, first_titles[['cluster', 'title', 'title_count']], on = 'cluster', how = 'left')new_df = new_df.rename(columns = {'title': 'rank1', 'title_count':'rank1_count'})# 统计这个cluster里第二多的分类的数量second_titles = title_count[~title_count['title'].isin(first_titles['title'])]second_titles = second_titles.groupby('cluster').apply(lambda x: x.nlargest(1, columns=['title_count']))second_titles = second_titles.reset_index(drop=True)new_df = pd.merge(new_df, second_titles[['cluster', 'title', 'title_count']], on='cluster', how='left')new_df = new_df.rename(columns={'title': 'rank2', 'title_count': 'rank2_count'})new_df.fillna(0, inplace=True)new_df['per_1'] = (new_df['rank1_count'] / new_df['count']).map(lambda x: '{:.2%}'.format(x))new_df['per_1_2'] = ((new_df['rank1_count'] + new_df['rank2_count'])/ new_df['count']).map(lambda x: '{:.2%}'.format(x))# new_df['first_percentage'] = (new_df['rank1_count'] / new_df['count']).map(lambda x: '{:.2%}'.format(x))# 将缺失值替换为 0new_df.fillna(0, inplace=True)# 输出结果display(new_df) 我们可以分成几步来写这个代码： 使用 groupby 对之前的 DataFrame 按照 cluster 聚合，统计每个 cluster 里面数据的数量。 使用 groupby，按照 cluster + title 的方式聚合，统计某一个 cluster 里面排名第一的分组名称和数量。 使用 x.nlargest 函数，从聚合后的 cluster 中拿到数量排名第一的分组的名称和数量。 去掉数据中的排名第一的分组，统计排名第二的分组，并将其与排名第一的分组放在一起进行分析。 输出结果： 统计数据表明，大部分聚类结果都能对应到新闻组的某个分类。例如，cluster 0 中有79.86%的数据来自于 misc.forsale 分类。在20个聚类中，有9个聚类中的80%以上数据来自于新闻组中的某个分类。其余的分类，例如cluster 2，前两个分组占了71.39%, 这两个分组的数据都是关于电脑硬件的，一个是关于pc.hardware，另一个是关于mac.hardware。我们只有3个分类的分组比较分散，分别是cluster 6、13和18。因此，可以看出直接使用文本嵌入来进行聚类的效果还不错。 *使用提示语对文本进行总结* 在应用中，我们通常使用的文本数据并没有明显的分组信息。以前，我们需要对聚类出来的类命名，只能通过观察每个类中的文本内容来选择一个适当的名称。对于这里的20个分类数据，我们通常只能每个类选择几篇文本，手动阅读一遍，然后再为其命名。但是，现在有了OpenAI的Completion接口，我们完全可以让AI为聚类出来的类命名。我们可以从每个类中随机选择3-5条文本，然后请AI总结一个名称，然后再选择一两条文本，让AI将其翻译成中文，以查看名称是否合理。 123456789101112131415161718items_per_cluster = 10COMPLETIONS_MODEL = &quot;text-davinci-003&quot;for i in range(num_of_clusters): cluster_name = new_df[new_df.cluster == i].iloc[0].rank1 print(f&quot;Cluster {i}, Rank 1: {cluster_name}, Theme:&quot;, end=&quot; &quot;) content = &quot;\\n&quot;.join( embedding_df[embedding_df.cluster == i].text.sample(items_per_cluster, random_state=42).values ) response = openai.Completion.create( model=COMPLETIONS_MODEL, prompt=f'''我们想要给下面的内容，分组成有意义的类别，以便我们可以对其进行总结。请根据下面这些内容的共同点，总结一个50个字以内的新闻组的名称。比如 “PC硬件”\\n\\n内容:\\n&quot;&quot;&quot;\\n{content}\\n&quot;&quot;&quot;新闻组名称：''', temperature=0, max_tokens=100, top_p=1, ) print(response[&quot;choices&quot;][0][&quot;text&quot;].replace(&quot;\\n&quot;, &quot;&quot;)) 我们可以通过 Completion 接口实现需求： 随机从每个聚类类别中选出10条记录，将其拼接在一起。 提供一段提示语告诉AI这些内容来自新闻组，并请求AI为这些新闻组取一个不超过50字的名称。 输出结果在下面，包括Cluster、原先排名第一的分组英文和AI给出的新闻组名称。 123456789101112131415Cluster 0, Rank 1: misc.forsale, Theme: 电子产品出售Cluster 1, Rank 1: soc.religion.christian, Theme: 宗教信仰的多样性Cluster 2, Rank 1: comp.sys.ibm.pc.hardware, Theme: 电脑硬件Cluster 3, Rank 1: rec.autos, Theme: 汽车维修与维护Cluster 4, Rank 1: talk.politics.mideast, Theme: 中东冲突报道Cluster 5, Rank 1: talk.politics.misc, Theme: 主观价值观Cluster 6, Rank 1: comp.os.ms-windows.misc, Theme: 科技产品&quot;&quot;&quot;Cluster 7, Rank 1: rec.sport.baseball, Theme: 运动员技术分析Cluster 8, Rank 1: sci.space, Theme: 太空探索Cluster 9, Rank 1: sci.electronics, Theme: 电脑硬件和电子设备Cluster 10, Rank 1: talk.politics.guns, Theme: 枪支控制讨论Cluster 11, Rank 1: rec.motorcycles, Theme: 骑行者经验分享Cluster 12, Rank 1: sci.crypt, Theme: 公民权利与加密技术Cluster 13, Rank 1: sci.electronics, Theme: 研究与技术... 中文分类名称大部分合理。可以挑选一些文本内容，检查其翻译是否与分类名称一致。翻译代码与上面类似，但有以下几个差别： 每个分类的抽样数据只有1条，而不是10条。 限制文本Token数量不超过100个，以免太占空间。 输出内容放大到500字，确保翻译提供足够内容。 123456789101112131415161718items_per_cluster = 1COMPLETIONS_MODEL = &quot;text-davinci-003&quot;for i in range(num_of_clusters): cluster_name = new_df[new_df.cluster == i].iloc[0].rank1 print(f&quot;Cluster {i}, Rank 1: {cluster_name}, 抽样翻译:&quot;, end=&quot; &quot;) content = &quot;\\n&quot;.join( embedding_df[(embedding_df.cluster == i) &amp; (embedding_df.n_tokens &gt; 100)].text.sample(items_per_cluster, random_state=42).values ) response = openai.Completion.create( model=COMPLETIONS_MODEL, prompt=f'''请把下面的内容翻译成中文\\n\\n内容:\\n&quot;&quot;&quot;\\n{content}\\n&quot;&quot;&quot;翻译：''', temperature=0, max_tokens=2000, top_p=1, ) print(response[&quot;choices&quot;][0][&quot;text&quot;].replace(&quot;\\n&quot;, &quot;&quot;)) 输出结果： 12345678910111213141516Cluster 0, Rank 1: sci.electronics, 抽样翻译: 我开始研究一些在相当嘈杂的环境中，以及在相当远的距离上传输串行数据的设备，我看到了各种保护RS232收发器（以及其他电路）免受串行线上瞬变的方案。我想知道最佳的做法是什么？这有多必要？据我所知，保护是必要的，特别是如果你计划将电缆路由到一个未知的环境（不受控制）。像信号线和电源线之间的意外短路，甚至闪电等事情都是非常可能的，我不认为你会喜欢看到你的电脑烟消云散的景象！（即使以太网卡也受到保护。我看过我的PC中的一个连接器，它由气体放电管保护！）但是，如果你计划将串行电缆用于内部路由（即在受控环境中），则不需要它们应该是相当安全的。建议：查看RS数据手册。他们有几个RS232收发器，具有过压保护。其中包括LT1080，LT1081和MAX250和MAX251。Maxim应该是绝缘的，但仍需要光耦合器才能工作（不要问我为什么。我以前从未使用过它们。）另一种选择是RS232电涌保护器。 RS目录中列出了两个。如果您需要额外的信息（即库存号），请给我发电子邮件。Cluster 1, Rank 1: comp.sys.ibm.pc.hardware, 抽样翻译: 如果您购买带CD配置的Centris 650，您将获得一台带有内置数学协处理器支持的68RC040处理器的Mac。我的理解是“可选fpu”是指您可以选择购买没有FPU的Centris 650 4/80或其他带有FPU的配置。Apple不提供从非FPU系统升级为FPU系统的服务。而且，目前尚不清楚非FPU系统（68LC040）上的'040处理器是否可以由另一家供应商提供的68RC040替换。苹果公司曾经发出一份备忘录，指出只有非FPU 68LC040处理器的Centris 610无法升级为支持FPU - 根据苹果的备忘录，两种芯片的引脚配置似乎不匹配，因此无法互换（再次，根据苹果的备忘录）。希望有所帮助。Cluster 2, Rank 1: talk.politics.misc, 抽样翻译: 以下内容可以在某个FTP存档中找到，我在这封“大哥大”的恶魔般的备忘录中插入了我的评论：看！这显然是禁止我们自己的螺纹规格的第一步。如果不以牙还牙地抵制这种疯狂，使用我们无畏领袖“慷慨”定义的螺纹以外的螺纹将是一种犯罪。废话！我说！ANSI标准螺纹会有微妙的弱点，让他们的特工更容易拆卸我们的汽车，导致我们的消声器在不合适的时候掉下来。哈！“相信我们”看到了吗？！这是第一步。很明显，我们必须团结起来，写信给你的国会议员！使用“相当好的螺纹”，而不是这种恶魔灵感的ANSI垃圾。保护你使用任何螺纹的宪法权利。游击螺纹活动必须成为日常事务。抵制通用汽车，并用STZ螺纹联合公司的螺丝自己建造汽车。螺丝你，比尔·克林顿！你和你的极权主义暴徒！Cluster 3, Rank 1: rec.sport.baseball, 抽样翻译: 我也是... RBI 是一个毫无价值的统计数据。当然，偷垒也是一样，因为有时候有些跑者会在一个可能会跑的球员前面。当然，投手会根据不同的垒位投出不同的球，所以击球率、长打率和击球率也没有用了。嗯... 我猜全垒打也不算了吧。我的意思是？RBI 可能不是一个完美的统计数据，但没有什么是完美的。没有任何统计数据（或缺乏）能告诉我没有关键时刻的击球手。也许没有任何统计数据能告诉我，但有些人是... 我只知道！！ 8)Cluster 4, Rank 1: comp.os.ms-windows.misc, 抽样翻译: 几个月前，我遇到了严重的内存问题，在Windows中出现了“停止错误消息，检查奇偶校验”。我运行了QA / PLUS，Check It，Diagnose以及几个共享软件内存检查器。我的系统中有8兆SIMM。这些商店购买/共享软件诊断工具要么运行正常没有错误，要么在我无法放置在内存芯片上的某个地址上发现错误。出于绝望，我想出了（现在已删除）步骤来找到坏的内存芯片。我发现了2个（道德：永远不要买背面印有“不适用于敏感或关键应用”的内存）。无论如何，我确实使用8兆芯片中的4个组合过滤掉了所有坏的内存芯片，并创建了一个RAM驱动器来进行测试。虽然它没有缓解我在Windows中的奇偶校验错误问题，但我确实以这种方式找到了坏的内存芯片。它从未失败过，为我找到坏的芯片，而商业/共享软件总是让我失望，要么没有发现错误，要么指向我不知道是哪块芯片的地址。PS：今天我的打字水平太糟糕了，我不想再用这个行编辑器了。-大卫Cluster 5, Rank 1: sci.crypt, 抽样翻译: 警察/联邦政府不需要获得您的私钥才能监听蜂窝电话交谈。加密不是端到端的，而是手机到基站 - 它必须是这样，以便蜂窝用户和固定设备可以相互交谈。对于蜂窝到蜂窝的通话，传输在基站解密，传递到另一个基站并重新加密。警察/联邦政府可以听取未加密的通话，前提是他们获得搜查蜂窝提供商设备的令牌。想要破解系统的唯一原因是他们可以在不获得令牌的情况下监听。但是，也许Clipper系统是安全的，他们确实需要一个令牌才能从保管处获取密钥，然后才能使用扫描仪进行监听（参见上面 - 他们不必*必须*走这条路）。我有我的疑虑，但即使是真的，一旦他们拥有密钥，他们将*永远*不再需要令牌来拨打该手机，随时随地都可以监听。 “嗯，法官，看来他不是毒贩，所以我们自然会停止监听……”那对英国保罗来说是真的，但我相信他们正在讨论如何使用这个芯片构建端到端加密电话。这*不是*蜂窝（尽管它当然可以按照您的建议在那里使用）Cluster 6, Rank 1: comp.graphics, 抽样翻译: 我需要有关Display PostScript strokeadjust功能的信息。此功能可调整线的端点，以使在低分辨率设备上显示的线看起来更好。PostScript文献在一定程度上解释了这个过程。他们还给出了一个如何在没有strokeadjust的PostScript环境中“模拟”strokeadjust的例子。建议的模拟是使用以下公式修改线的端点坐标：新坐标=（四舍五入（旧坐标-0.25））+0.25这样，我们最终得到所有坐标以“.25”结尾。从阅读中，我认为他们实际上可能做的是：新坐标=（（截断（旧坐标*2））/2）+0.25这样，所有坐标都以“0.25”或“0.75”结尾，取决于哪个更接近。通过与Display PostScript进行实际比较，我发现DPS实际上不是用这两种方法。由于我喜欢DPS的结果比我的东西看起来更好，我想知道是否有人知道DPS是如何做的，愿意/能够告诉我。谢谢！Cluster 7, Rank 1: sci.space, 抽样翻译: 我认为，如果有奖励等，应该有以下“类别”：大公司小公司/公司（根据报告收入？）大政府（国民生产总值等）小政府（或政治影响力或国民生产总值？）大组织（行星协会？等）小组织（许多小型组织..）组织事务可能必须是非营利性的，或者是？当然，这意味着奖金可能会增加。较大的获得更多或？基本上使奖金（总奖金）60亿美元，分配给班级获胜者..更公平？必须建立一个单独的组织来监督活动，裁判等，并监视安全违规（或者不，如果人们想冒自己的生命危险让他们去做？）。Cluster 8, Rank 1: talk.politics.guns, 抽样翻译: 我当时现场观看，并且已经重新观看了几次，从新闻视角来看，只有一个起火点可见，坦克在迎风侧打入，风把火势席卷整个干燥的木结构，几分钟内就烧毁了。受30英里每小时的大风和直升机的影响，火势迅速蔓延。如果有其他火源，它们不可见，也不需要，我观察到的火势就是这样。所有的见证者都是从联邦调查局那里拿工资的。是的，媒体毫无疑问地支持联邦调查局的版本。真可悲。吉姆--jmd@handheld.comCluster 9, Rank 1: rec.motorcycles, 抽样翻译: 1.在前苏联有一个类似的想法，需要多少军人来安装一个新的电灯？答案是九个：一个坐在桌子上拿着灯，四个拿着桌子转动它，另外四个则以相反的方向绕着桌子跑，以免让第一个感到不舒服（被转动时）。可惜，它缺乏尼克的消息中的那种黑色幽默。2.在我看来，签名应该是这样的：/ _ __ / ./_______/_/_______________ /________ /____//___ / _ /特使AlbertCluster 10, Rank 1: rec.sport.hockey, 抽样翻译: 嗨，曲棍球迷们。大家好！上周日，在盐湖城，当地的ABC电视台决定不播放曲棍球比赛。节目主管真是个混蛋！不管怎样，我有一个卫星天线，几个曲棍球朋友邀请自己过来看本周日（4月25日）的比赛，但我找不到正确的比赛时间。对于卡尔加里和洛杉矶的比赛，我有时间显示从MDT 11:00到MDT 5:00。现在，我甚至不确定本周日会有哪些比赛，因为ABC搞乱了时间表。我想我应该能从天线上收到三场比赛（MDT 11:00，2:00和5:30），但我不确定。如果有人有时间表，请给我发电子邮件。正如你所看到的，我必须收听rec.sport.hockey，有时很难得到链接。提前谢谢非常感谢附言：英语或法语都可以。罗兰·贝胡宁behunin@oodis01.af.milbehunin@oodis01.hill.af.milCluster 11, Rank 1: comp.windows.x, 抽样翻译: 由于xterminals没有NeWS服务器，它们具有速度的本质，因为它们的功能有限：一旦添加NeWS和其他一切，就拥有了工作站。 我听说有一些变通方法，其中一种涉及perl脚本。 我们正试图用不需要NeWS（如ghostview）的程序替换需要NeWS（如pageview）的程序。也许其他人可以详细说明perl变通方法； 我没有个人经验。Cluster 12, Rank 1: talk.politics.mideast, 抽样翻译: 游击队和恐怖分子组织使用的一种“可靠的”方法是：在当地民众中间进行行动，从而迫使对立的“国家”可能会伤害无辜的平民，以搜索，或者为了避免平民死亡而放弃搜索。当然，利用人口作为掩护的人也应该为把无辜的平民拖入危险境地负责。你是在暗示，当游击队利用人口作为掩护时，以色列应该完全放弃吗？所以......最简单的方法是利用无辜者作为护盾，并希望对方尊重无辜的生命？你该死的对，以色列坚持一些“非军事化”或“缓冲”区。它已经忍受了太多年来阿拉伯国家领土发动的攻击，并看到这些国家什么也没做。以色列决定停止这种行动的唯一方法是自己去做，这并不奇怪。什么？所以关于以色列来自邻近阿拉伯国家的攻击可以重新开始？虽然我也希望这种情况发生，但只有当阿拉伯国家表明他们准备承担责任和责任来阻止以色列来自他们领土的游击袭击时，这才会发生。他们必须证明（或提供一些“保证”），以色列不会接受他们的“话”-不要求他们对“反以色列游击队”的容忍态度。天哪，布拉德。你到底是怎么想到联合国部队可以阻止任何事情的？他们只是因为那个国家允许他们进入而驻扎在那里。它可以要求他们随时离开；就像纳赛尔在1956年和1967年所做的那样。既然有这种“限制”，我不认为以色列会更舒服。如果没有阿拉伯国家对和平的真正承诺，以及其他各方提供的具体（而不是智力或政治上的行动）“保证”，联合国对以色列毫无用处（但也许可以作为一个“诡计”？）。也许你知道，对大多数社区来说，有一种感觉，即“我们中的许多人与那些攻击我们的人作斗争而死，要比少数人默默地接受命运而死要好”。但是，如果你要求以色列看到受伤少的意义，我建议你也同样应用于巴勒斯坦，阿拉伯和伊斯兰团体。从以色列的角度来看，“让步”一无所获......除了意识到它已经放弃了“某些东西”，现在只能*希望*对方决定也这样做。话可以通过仅仅这样来收回；要“收回”有形的物品（土地，土地控制权），需要你所说的以色列应该避免的行动。以色列忍受阿拉伯国家领土发动的攻击已有几十年之久，直到通过其对黎巴嫩的入侵才基本上停止了这种现实。整个基础就是你上面所说的：1）以色列会对这些攻击表示愤怒，向有关阿拉伯国家抗议，2）那个国家立即无视整个事件，确信它不能为“私人组织”所犯的罪行负责，3）以色列Cluster 13, Rank 1: sci.med, 抽样翻译: Nutrasweet是一种合成甜味剂，比糖甜几千倍。有些人担心人体分解Nutrasweet时产生的化学物质。据认为，它会形成甲醛，并且已知会在人体排除物质的降解途径中产生甲醇。真正的问题是甲醇和甲醛的水平是否足够高，以致于造成重大损害，因为它们对活细胞都有毒性。我只能说我不会摄入它。苯丙氨酸不是你要担心的。它是一种氨基酸，每个人都会使用少量的苯丙氨酸来进行蛋白质合成。有些人患有苯丙酮尿症，他们缺少降解这种化合物并排除体外的酶。对他们来说，它会在体内积累，而且在高水平下，这对生长中的神经细胞有毒性。因此，它只是儿童（直到10岁左右）或患有这种疾病的妇女的主要问题。它曾经是婴儿脑损伤的主要原因，但现在可以在出生时轻松检测到，然后只需要避免儿童或怀孕时摄入苯丙氨酸即可。Cluster 14, Rank 1: comp.os.ms-windows.misc, 抽样翻译:... 我们选取的内容和AI总结的标题非常相似。这个过程是否让你想起Completion接口的强大之处呢？只需给出指令，它就能按照我们的要求提供合理的结果。 在这个例子中，因为超出Token限制，导致后面没有完全执行完，报错了。大家看到我的源文件可以看到这部分报错。 使用文本摘要支持无限轮数的聊天 在前一课中，我们思考了一个问题：如果我们想让 AI 在对话轮数不断增多的情况下尽可能多地记住过去几轮的上下文，该怎么办？有一个方法是，让 AI 对过去几轮的内容进行总结，得出一个 100 字左右的概述。然后，将这个概述作为对话的提示信息，告诉 AI 这是聊天的背景，然后继续新一轮的对话。 让我们在这里试试看。 我们拿上一课的一段聊天记录，让 AI 先总结一下，可以看到 AI 总结了 User 和 Assistant 之间聊了什么。 ```python history = \"\"\"User : 你是谁？ Assistant : 我是一个AI语言模型，专门用于回答各种问题，包括法律问题。 User : 请问什么是正当防卫？ Assistant : 正当防卫是指在必要时为了保护自己、他人或者国家公共利益而采取的防御行为。在我国法律中，对于正当防卫的情况，法律规定可以免除或减轻犯罪责任。但是，正当防卫也有限制，必须符合法律规定的情形和条件，否则可能构成违法犯罪行为。 User : 那防卫过当呢？ Assistant : 防卫过当是指在正当防卫行为中，因过度防卫而超出了必要限度，对袭击者造成了严重伤害或者死亡的行为。在我国法律中，防卫过当是不被允许的，因为它已经超出了必要的防卫范围，可能构成过失犯罪或者故意犯罪。如果行为构成犯罪，防卫人需要承担相应的法律责任。 \"\"\" def summarize(text, max_tokens=200): response = openai.Completion.create( model=COMPLETIONS_MODEL, prompt=text + \"：\", max_tokens=max_tokens, ) return response[\"choices\"][0][\"text\"] summarized = summarize(history) print(summarized) 123452. 输出结果：3. ```python User和Assistant聊了关于正当防卫和防卫过当的话题，包括了正当防卫是指什么，它有什么样的限制，防卫过当是什么，以及如果防卫行为构成犯罪的话防卫人会承担相应的法律责任。 接着，我们新建一个对话，提示语里加上总结内容，然后让 AI 继续对话。 ```python prompt = summarized + \"，继续对话：\" conversation = Conversation(prompt, 5) question = \"那恶意挑衅呢？\" answer = conversation.ask(question) print(\"User : %s\" % question) print(\"Assistant : %s\" % answer) 12345678910111213141516171819202122232425262728293031322. &gt; 注意，如果你是在 Notebook 里面执行的话，你需要把上一讲的 Conversation 类的代码复制过来先执行一下。这里，我们启动了一个新的对话对象，将之前的几轮对话的内容总结了一下，放在 Prompt 的最前面，然后让 AI 根据已经聊了的内容，继续往下聊。3. ```python # 上一节中定义的Conversation类 class Conversation: def __init__(self, prompt, num_of_round): self.prompt = prompt self.num_of_round = num_of_round self.messages = [] self.messages.append({&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: self.prompt}) def ask(self, question): try: self.messages.append( {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: question}) response = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=self.messages, temperature=0.5, max_tokens=2048, top_p=1, ) except Exception as e: print(e) return e message = response[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;] self.messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: message}) if len(self.messages) &gt; self.num_of_round*2 + 1: del self.messages[1:3] return message 当我们直接问，“那恶意挑衅呢？”，AI 会正确回答出恶意挑衅的法律解释，并结合了上下文告诉你相关性。 输出结果： 12User : 那恶意挑衅呢？Assistant : 恶意挑衅是指他人以言语、行为等方式故意挑衅、侮辱或者侵犯他人人身权利，如果被挑衅者出于自卫而采取防卫行为，且防卫行为符合正当防卫的要求，那么这种防卫行为也是合法的。但是如果被挑衅者的防卫行为明显超出了正当防卫的必要性和适度性，那么就可能构成防卫过当，需要承担相应的法律责任。 如果我们只让 AI 对话而没有加上 AI 总结之前的对话，那么它只能跟你闲扯些其他话题。 123456conversation = Conversation(&quot;请你根据已经聊了的内容，继续对话：&quot;, 5)question = &quot;那恶意挑衅呢？&quot;answer = conversation.ask(question)print(&quot;User : %s&quot; % question)print(&quot;Assistant : %s\\n&quot; % answer) 输出结果： 12User : 那恶意挑衅呢？Assistant : 恶意挑衅是指有人故意挑衅、侮辱或者攻击别人，这种行为是不应该被容忍的。如果我们遇到这种情况，我们应该要保持冷静，不要过度激动或者反击，可以采取一些有效的措施来应对，比如报警或者向相关机构举报。另外，我们也可以通过教育和宣传来提高公众对于恶意挑衅的认识，让更多人知道这种行为的危害性和不可取性。 如果没有总结，AI 将没头没脑地和你聊天，提醒你不应容忍恶意挑衅。 小结 本节课我们实验了通过 Embedding 拿到的向量进行文本聚类的技巧。利用 Completion 接口可以帮我们总结内容的能力，我们可以直接给分类取了一个名字，避免了繁琐的人工命名，分类效果也很好。 这种技巧同样适用于多轮的长对话中。我们可以将历史对话让 AI 总结为一小段文本放到提示语里面，这样即可让 AI 记住过去的对话内容，又不会超出模型支持的 Token 数量。 课后练习 在体验 GPT 系列模型的时候，你觉得哪些提示语特别有用？欢迎分享自己的体验。 在文本聚类中，有三个聚合出来的类和原先的分组没有很明显的对应关系。你能不能写一些代码来研究数据，找出其中的原因呢？ 期待在评论区看到你的思考！也欢迎你把这节课分享给感兴趣的朋友，我们下一讲见！","link":"/AI-can-help-you-summarize-your-content/"},{"title":"12 AI帮你写个小插件，轻松处理Excel文件","text":"开头我就要跟各位先说对不起，本来我是很想为大家把这部分实现并完成的。但是很抱歉，因为我用的Mac，而这部分代码实现起来的时候一直会如下报错： 当然，其实是可以迂回实现的，就是使用Mac本来的AppleScript，不过实现效果并不好，所以想想也就算了。毕竟我考虑，可能看我这些系列文章的人都多多少少会一点Python吧，利用Python去处理数据，怎么也比Excel强很多，再说，现在Excel+ChatGPT的组合说能实现的功能也都很有限。 当然，我也可以直接将徐文浩已经实现的东西贴给大家，就像这篇文章这样：https://www.163.com/dy/article/I2K9R9QF05560XXW.html 但是，毕竟人家徐文浩是付费文章，这样做先不说是否有法律风险，道德上也没办法实现。 所以，最后这篇文章就算鸽了大家吧，当然，作为补偿，我将我自己实现过的代码贴在这里给大家做个参考，请注意，这是AppleScript的代码，请谨慎使用： 1234- Step 1: Prompt to choose Excel fileset excelFile to choose file with prompt &quot;Select Excel File&quot;- Step 2: Open Excel file and get worksheettell application &quot;Microsoft Excel&quot; activate open excelFile set excelWorkbook to workbook 1 set excelSheet to sheet 1 of excelWorkbook - Step 3: Read first column and process each product titleset productTitles to value of column 1 of excelSheet set translatedResults to {} repeat with i from 1 to count of productTitles set productTitle to item i of productTitles set promptText to &quot;Consideration product: &quot; &amp; productTitle &amp; return &amp; ¬ &quot;1. Compose human readable product title used on Amazon in English within 20 words.&quot; &amp; return &amp; ¬ &quot;2. Write 5 selling points for the products on Amazon.&quot; &amp; return &amp; ¬ &quot;3. Evaluate a price range for this product in the U.S.&quot; - Step 4: Call OpenAI API and get translated resultset translatedResult to my callOpenAPI(promptText) copy translatedResult to the end of translatedResults end repeat - Step 5: Write translated results to second columnset column 2 of excelSheet to translatedResults - Step 6: Save and close Excel filesave excelWorkbook close excelWorkbookend tellon callOpenAPI(promptText) - Call OpenAI API and return translated result- Replace this code with actual API callset translatedResult to &quot;Translated Result&quot; - Replace with actual translated resultreturn translatedResultend callOpenAPI 最后，在和大家说声抱歉。咱们下期，来尝试用AI帮忙来写测试。","link":"/AI-create-a-excel-plugin/"},{"title":"纪念","text":"纪念 1234567891011121314&lt;math xmlns='http://www.w3.org/1998/Math/MathML'&gt; &lt;mn&gt;25&lt;/mn&gt; &lt;mo&gt; &amp;#x00D7;&lt;!--multiplication sign --&gt;&lt;/mo&gt; &lt;msup&gt; &lt;mrow&gt; &lt;mn&gt;2&lt;/mn&gt; &lt;/mrow&gt; &lt;mrow&gt; &lt;mn&gt;6&lt;/mn&gt; &lt;/mrow&gt; &lt;/msup&gt; &lt;mo&gt;=&lt;/mo&gt; &lt;mo&gt;?&lt;/mo&gt;&lt;/math&gt; 以上，为了纪念！","link":"/25X2de6cifang/"},{"title":"01 进入AI大门，学会与其交谈","text":"不用问我都知道，你们一定是被ChatGPT的火热出圈导致的开始关注人工智能，也是由于此才看到我这篇文章。 放心，大家想要的我一定会给予，既然大家都想先认识ChatGPT，那么我们就从这个主题开始。 接下来，我们学学如何利用openAI的API来和其沟通。在整个使用过程中，我们都使用的是GPT-3.5的大预言模型。 在本课程中，我们将回答许多问题，例如，OpenAI 的 API 能够实现哪些神奇的事情？OpenAI 的产品被称为已经离通用人工智能（AGI）不远了，它们长什么样子？GPT-3 这样的模型与之前基于深度学习的自然语言处理解决方案有什么不同？我们将通过逐步解释这些问题，使您深入了解这个令人兴奋的领域。 无论您是否是一名程序员，您都可以从本课程中学习如何使用 AI 技术，尤其是大型语言模型，为您的项目和业务提供价值。 基础工作 创建帐号和API Key 了开始学习本课程，您需要先注册一个可以使用 OpenAI 的 API 的账号。您可以通过注册入口进行注册。目前，OpenAI 尚未向中国大陆和香港地区开放，因此您需要自己寻找适当的解决方案进行注册。如果您有更好的解决方案，也欢迎在评论区分享。 注册账号后，您需要点击右上角的账号，然后进入 \"View API Keys\" 页面管理 API Keys。 您可以点击下方的 \"+Create new secret key\" 来创建一个新的 API Key。 您需要将此 API Key 存储在一个安全的位置，因为在后续的 OpenAI 接口调用中，需要使用此 API Key。 储存API Key留用这方便，我使用的是1Password，开了家庭版，很好用。 已绑卡 关于绑卡这个事，可以自己在网上搜索看，办法总比问题多。不要找我，虽然我有渠道，但是我的渠道很贵，到时候说我骗人钱我可说不清楚。 搭建环境 既然是开发API应用，那必然是需要开发环境的。如果你自己会，那就最好不过了，如果不是太熟悉，可以参考一下我这篇文章： 这篇文章详细的介绍了在Mac内如何搭建AI环境，包括Tensorflow的安装等。 基本上，我们现在需要的是3.10 的Python环境，还有Conda（我习惯用这个），然后在本地安装好Jupyter lab, 如下： 123conda create --name gpt python=3.10conda activate gptconda install -c conda-forge jupyterlab ipywidgets openai 这一段命令的意思是创建一个名为 gpt的python 3.10的开发环境，然后切换到这个环境里，再安装必要的包。 在后面的使用过程中，当然你可以选择jupyter notebook, 也可以和我一样，使用VSCode。 当然，你也可以选择Colab，其实这也是一个Jupyterlab，如果你不想本地搭建环境，那就直接使用Colab吧，不过注意一点，需要科学上网。就算你本地有环境，我还是建议你有些事后使用Colab，能用到一些免费的GPU资源，我的M1没有好的显卡支持，很多时候还是需要上Colab。 使用时候，记得要安装openAI的库，并且设置自己的API Key： 12!pip install openai%env OPENAI_API_KEY=&quot;这里输入你的API Key&quot; 测试一下 让我们现在开始依次写完这段代码，虽然截图内已经有了，但是还是让我们一步步来执行起来，这一段代码，并不是出自我之手，而且直接借鉴的徐文浩的代码： 123456789101112131415161718192021222324252627282930313233import openaiimport json# 设定API Key和模型openai.api_key = &quot;输入你自己的代码&quot;COMPLETION_MODEL = &quot;text-davinci-003&quot;# 设定关键词和描述prompt = &quot;&quot;&quot;Consideration proudct : 工厂现货PVC充气青蛙夜市地摊热卖充气玩具发光蛙儿童水上玩具1. Compose human readale product title used on Amazon in english within 20 words.2. Write 5 selling points for the products in Amazon.3. Evaluate a price range for this product in U.S.Output the result in json format with three properties called title, selling_points and price_range&quot;&quot;&quot;# 写一个调用方法def get_response(prompt): completions = openai.Completion.create ( engine=COMPLETION_MODEL, prompt=prompt, max_tokens=512, n=1, stop=None, temperature=0.0, ) message = completions.choices[0].text return message# 调用方法并打印最终结果print(get_response(prompt)) 然后我们就可以看到返回了： 1234567891011{ &quot;title&quot;: &quot;Glow-in-the-Dark Inflatable PVC Frog Night Market Hot Selling Water Toy for Kids&quot;, &quot;selling_points&quot;: [ &quot;Made of durable PVC material&quot;, &quot;Glow-in-the-dark design for night play&quot;, &quot;Inflatable design for easy storage and transport&quot;, &quot;Perfect for water play and outdoor activities&quot;, &quot;Great gift for kids&quot; ], &quot;price_range&quot;: &quot;$10 - $20&quot;} 这段代码里面，我们调用了 OpenAI 的 Completion 接口，然后向它提了一个需求，也就是为一个我在 1688 上找到的中文商品名称做三件事情。 为这个商品写一个适合在亚马逊上使用的英文标题。 给这个商品写 5 个卖点。 估计一下，这个商品在美国卖多少钱比较合适。 同时，我们告诉 OpenAI，我们希望返回的结果是 JSON 格式的，并且上面的三个事情用 title、selling_points 和 price_range 三个字段返回。 神奇的是，OpenAI 真的理解了我们的需求，返回了一个符合我们要求的 JSON 字符串给我们。在这个过程中，它完成了好几件不同的事情。 第一个是翻译，我们给的商品名称是中文的，返回的内容是英文的。 第二个是理解你的语义去生成文本，我们这里希望它写一个在亚马逊电商平台上适合人读的标题，所以在返回的英文结果里面，AI 没有保留原文里有的“工厂现货”的含义，因为那个明显不适合在亚马逊这样的平台上作为标题。下面 5 条描述也没有包含“工厂现货”这样的信息。而且，其中的第三条卖点 “Inflatable design for easy storage and transport”，也就是作为一个充气的产品易于存放和运输，这一点其实是从“充气”这个信息 AI 推理出来的，原来的中文标题里并没有这样的信息。 第三个是利用 AI 自己有的知识给商品定价，这里它为这个商品定的价格是在 10～20 美元之间。而我用 “Glow-in-the-Dark frog” 在亚马逊里搜索，搜索结果的第一行里，就有一个 16 美元发光的青蛙。 最后是根据我们的要求把我们想要的结果，通过一个 JSON 结构化地返回给我们。而且，尽管我们没有提出要求，但是 AI 还是很贴心地把 5 个卖点放在了一个数组里，方便你后续只选取其中的几个来用。返回的结果是 JSON，这样方便了我们进一步利用返回结果。比如，我们就可以把这个结果解析之后存储到数据库里，然后展现给商品运营人员。 接下来，我们再看一个其他的例子： 1234567prompt = &quot;&quot;&quot;Man Utd must win trophies, says Ten Hag ahead of League Cup final请将上面这句话的人名提取出来，并用json的方式展示出来&quot;&quot;&quot;print(get_response(prompt)) 得到输出结果： 123{ &quot;names&quot;: [&quot;Ten Hag&quot;]} 看出AI干了什么吗？其实从中文中你能知道我需要AI做什么，而他完完全全输出了我想要的。 我们这里的两个例子，其实对应着很多不同的问题，其中就包括机器翻译、文本生成、知识推理、命名实体识别等等。在传统的机器学习领域，对于其中任何一个问题，都可能需要一个独立的机器学习模型。就算把这些模型都免费提供给你，把这些独立的机器学习模型组合到一起实现上面的效果，还需要海量的工程研发工作。没有一个数十人的团队，工作量根本看不到头。然而，OpenAI 通过一个包含 1750 亿参数的大语言模型，就能理解自然的语言输入，直接完成各种不同的问题。而这个让人惊艳的表现，也是让很多人惊呼“通用人工智能（AGI）要来了”的原因。 这两个例子虽然简单，但是咱们暂时先到此为止，记得课后好好练习。 请将今天课程中提供的示例代码，在你搭建的开发环境中运行一下。 你可以去看一下 OpenAI 提供的示例，找几个你感兴趣的用途，在上面的开发环境里运行体验一下，你也可以脑洞大开，尝试一些你想用 AI 解决的问题，看看 AI 能不能给出你想要的结果。 推荐阅读 推荐阅读如果你想知道 GPT 系列大模型到底是怎么回事儿，我推荐你去看一下李沐老师讲解 GPT 系列论文的视频 GPT、GPT-2、GPT-3 论文精读，这个视频深入浅出，能够让你理解为什么现在 GPT 那么火热。","link":"/Enter-the-door-of-AI-learn-to-communicate-with-it/"},{"title":"Apple M1的AI环境搭建","text":"本文环境搭建的基础是Python3.9， 因为M1为ARM架构，所以放弃了Anaconda，使用Miniforge3。包括Tensorflow, xgboost, Lightgbm, Numpy, Pandas, Matplotlib, NGBoost等。当然，因为是Python3.9， 所以有些库实在是无法使用。 Homebrew 作为Mac的包管理神器，首先当然要先从Homebrew开始。Homebrew已经支持了ARM架构，可以直接进行安装，当然，如果你电脑里以前存在X86的brew支持，请先卸载干净。 Homebrew 卸载 1/bin/bash -c &quot;$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/uninstall.sh)&quot; Install ARM Homebrew 1/bin/bash -c &quot;$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/install.sh)&quot; 执行完毕后，Homebrew安装在/opt/homebrew路径下；在安装完毕后，命令行后会提示执行命令设置环境变量，当然，以防万一，这里也提供一下： 12echo 'eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;' &gt;&gt; ~/.zprofileeval &quot;$(/opt/homebrew/bin/brew shellenv)&quot; 如果是bash shell， 则： 12echo 'eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;' &gt;&gt; ~/.bash_profileeval &quot;$(/opt/homebrew/bin/brew shellenv)&quot; 记得source ~/.zprofile Install X86 Homebrew 1arch -x86_64 /bin/bash -c &quot;$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/install.sh)&quot; X86版本的安装执行完成后命令行未提示添加环境变量。 alias 支持多版本 在终端执行： 12alias brew='arch -arm64 /opt/homebrew/bin/brew'alias ibrew='arch -x86_64 /usr/local/bin/brew' 这里可以看出两者路径区别 设置镜像 中科大源 1234567891011# brewgit -C &quot;$(brew --repo)&quot; remote set-url origin https://mirrors.ustc.edu.cn/brew.git# coregit -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git# caskgit -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.gitbrew update 清华大学源 1234567891011# brewgit -C &quot;$(brew --repo)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git# coregit -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git# caskgit -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-cask.gitbrew update 恢复默认源 1234567891011# brewgit -C &quot;$(brew --repo)&quot; remote set-url origin https://github.com/Homebrew/brew.git# coregit -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://github.com/Homebrew/homebrew-core.git# caskgit -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://github.com/Homebrew/homebrew-cask.gitbrew update 更多源 Homebrew 其他相关 设置bottles镜像 1234567# bottles for zshecho 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles/bottles' &gt;&gt; ~/.zprofilesource ~/.zprofile# bottles bashecho 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles/bottles' &gt;&gt; ~/.bash_profilesource ~/.bash_profile cask 目前cask是从GitHub上读取软件源，而GitHub Api对访问有限制，如果使用比较频繁的话，可以申请Api Token，然后在环境变量中配置到HOMEBREW_GITHUB_API_TOKEN。 12echo 'export HOMEBREW_GITHUB_API_TOKEN=yourtoken' &gt;&gt; ~/.zprofilesource ~/.zprofile Install Miniforge3 首先需要下载安装包： Download 请下载arm64(Apple Silicon)版本： image-20210908234235884 下载完成后进入到文件目录，比如我是在~/Download/内，执行： 1bash Miniforge3-MacOSX-arm64.sh 整个执行过程会有大概三次填写yes并回车确定，最后一次会询问你是否执行conda init， 会自动在~/.zshrc内添加环境变量，如果未执行的，可以将下面语句加入文件末尾： 12345678910111213141516# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;# !! Contents within this block are managed by 'conda init' !!__conda_setup=&quot;$('/Users/xx/miniforge3/bin/conda' 'shell.zsh' 'hook' 2&gt; /dev/null)&quot;if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot;else if [ -f &quot;/Users/xx/miniforge3/etc/profile.d/conda.sh&quot; ]; then . &quot;/Users/xx/miniforge3/etc/profile.d/conda.sh&quot; else export PATH=&quot;/Users/xx/miniforge3/bin:$PATH&quot; fifiunset __conda_setupconda activate tf# &lt;&lt;&lt; conda initialize &lt;&lt;&lt; 记得自行更改/Users/xx/内的用户名 等待Miniforge3安装完成，然后设置一个专供学习Tensorflow的虚拟环境 12conda create -n tf python=3.9.5conda activate tf # 将这句添加到~/.zshrc内，每次打开shell都会自动执行 关于conda切换环境的命令，建议自行Google学习一下，很有用。 Install Tensorflow 目前网上流传的Tensorflow安装基本是两个版本，一个是安装一大堆的支持和依赖，一个是使用yml文件提前准备好环境库一键完成环境创建，比如environment.yml： 1conda env create --file=environment.yml --name=tf 其实这一步也很简单，Apple为了大力推广自家的ARM，已经为大家做好了这部分准备，我们只需要安装就行了。 假设目前在tf环境内 123conda install -c apple tensorflow-depspython -m pip install tensorflow-macospython -m pip install tensorflow-metal 好了，结束！ 可以自行利用下面一段代码测试下： 123456789101112from tensorflow.keras import layersfrom tensorflow.keras import modelsmodel = models.Sequential()model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.Flatten())model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(10, activation='softmax'))model.summary() image-20210909000423413 123456789101112131415from tensorflow.keras.datasets import mnistfrom tensorflow.keras.utils import to_categorical(train_images, train_labels), (test_images, test_labels) = mnist.load_data()train_images = train_images.reshape((60000, 28, 28, 1))train_images = train_images.astype('float32') / 255test_images = test_images.reshape((10000, 28, 28, 1))test_images = test_images.astype('float32') / 255train_labels = to_categorical(train_labels)test_labels = to_categorical(test_labels)model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])model.fit(train_images, train_labels, epochs=5, batch_size=64)test_loss, test_acc = model.evaluate(test_images, test_labels)test_acc image-20210908235926168 执行过程中可以在资源管理器中看到GPU的占用： image-20210909000541931 其他 Lightgbm 1conda install Loghtgbm 一句代码解决，完全靠谱。 xgboost xgboost稍微有点麻烦，我测试了最稳妥的安装方式，还是自行编译，那这个时候我们就需要用到brew安装并设置编译环境了： 注意，我用的都是brew而非ibrew, 目前都是在ARM环境下完成操作。 123brew install gccbrew install cmakebrew install libomp 然后下载源码并执行 1234567git clone git@github.com:dmlc/xgboost.gitcd xgboostmkdir buildcd buildCC=gcc-11 CXX=g++-11 cmake ..cd ../python-package/Users/xx/miniforge3/envs/tf/bin/python setup.py install 然后就OK了。 至于其他的，Numpy在安装Tensorflow的时候就自动作为依赖安装了，Pandas, Matplotlib, NGBoost等，执行下方： 123conda install -c conda-forge pandasconda install -c conda-forge matplotlibconda install -c conda-forge ngboost 如果conda内实在没有的，再试试pip安装，再不行，就只能自行下载源码编译了。 目前在当前环境下解决不了的几个库： CatBoost Cairo -&gt; Pycairo GraphEmbedding CV2 igraph 在整个过程中，可能会遇到各种各样的问题，大家要习惯于使用Google和查阅官方文档； 参考 Tensoflow-macos Run xgboost on Mac and Regression data Accelerating TensorFlow Performance on Mac The new Apple M1 chips have accelerated TensorFlow support M1 Mac Mini Scores Higher Than My RTX 2080Ti in TensorFlow Speed Test. GPU acceleration for Apple's M1 chip? M1芯片Mac上Homebrew安装教程 Mac mini M1使用简单体验(编程、游戏、深度学习) Installing TensorFlow 2.4 on MacOS 11.0 without CUDA for both Intel and M1 based Macs 在 M1 芯片 Mac 上使用 Homebrew Apple M1终于让MacBook变的可以炼丹了 Install XGBoost and LightGBM on Apple M1 Macs Installing TensorFlow on the M1 Mac Getting Started with tensorflow-metal PluggableDevice M1芯片mac安装xgboost和lightgbm AI - Apple Silicon Mac M1 机器学习环境 (TensorFlow, JupyterLab, VSCode) M1芯片安装tensorflow 使用MacBook pro M1搭建基于ML Compute加速的TensorFlow深度学习环境 你的Mac有了专用版TensorFlow，GPU可用于训练，速度最高提升7倍 在M1的Mac上安装Tensorflow（避坑版） 在M1芯片Mac上搭建原生适配Python环境 Conda-forge Miniforge M1 mac安装PyTorch的完整步骤指南 macOS M1(AppleSilicon) 安装TensorFlow环境 傻瓜版M1配置Tensorflow-超简单近乎一键完成 environment.yml opencv-python MAC安装Opencv以及Dlib碰到的一些问题 Jupiter Widgets 启动SparkContext报错 MacBook Pro 2020 M1芯片安装xgboost xgboost Homebrew / Linuxbrew 镜像使用帮助 镜像助手 Apple Silicon Mac 安装xgboost M1芯片mac安装xgboost和lightgbm mac安装lightgbm踩坑心得，亲测有效！ MAC 上 使用lightgbm遇到image not found 解决办法总结 杂记-Macbook Pro M1芯片能玩深度学习吗？","link":"/Apple_M1_AI_environment_construction/"},{"title":"你要做的很简单，别去想那只大象","text":"“禁止思考实验”，这是心理学一个经典的实验，它被用来研究逆反心理。有兴趣的小伙伴可以跟着我们一起做： 实验要求参与者坐下，并听取实验者说明。实验者会告诉参与者，接下来的任务非常简单，只需要尽可能长时间地不去想象一只白色的大象。 notion image 实验者开始计时，并要求参与者开始执行任务。在执行任务时，参与者需要告诉实验者当他们想起白色大象的时刻，或者当他们对大象的想象感到特别强烈的时刻。参与者开始尝试不去想象白色大象。但是，实验结果表明，大多数参与者很快就会想到大象，即使他们试图抑制这种想象，他们也很难控制。在实验的后续部分，实验者可能会告诉参与者，他们现在可以开始想象白色大象了，这时参与者的反应可能会改变，他们可能会开始主动抵制想象大象，或者会出现想象不出白色大象的情况。最后，实验者会记录下参与者在实验过程中的各种反应，并进行分析和总结。怎么样，屏幕前的你情况如何？有克制到自己不去想那只白象吗？把你的结果发在评论区告诉我。 这个实验表明了逆反心理的存在，即当人们感到被强迫或期望去做某些事情时，他们可能会反其道而行之，甚至会选择与预期相反的行为。在这个实验中，参与者被要求不去想象白色大象，但很多人最终还是想象了这只大象。这表明，人们往往会对被禁止或限制的事物产生强烈的反应，即逆反心理。 逆反心理是指人们抵制或反对他们所认为的强制性或压迫性的做法，即使这种做法实际上可能对他们有益。这种反应可能是一种自我保护机制，因为人们往往会感到自己失去了自主权或选择的权利。 例如，当父母强迫孩子吃某种食物或学习某个科目时，孩子可能会表现出逆反心理，并且刻意不吃那种食物或不学习那个科目。同样地，当政府强制人们遵守某些规定或法律时，一些人可能会表现出逆反心理，不想遵守这些规定或法律。 逆反心理并不总是一件坏事，因为它可以帮助人们保护自己的权利和自主性。然而，在某些情况下，它可能会导致人们做出不理性或不负责任的决定。因此，在处理逆反心理时，需要寻找平衡，以确保人们不会受到伤害并且可以做出明智的决定。","link":"/Do-not-imagine-a-white-elephant/"},{"title":"Analysis data and research report collection","text":"The purpose is to facilitate finding specific locations when doing data analysis by yourself 1. 国内咨询机构网站数据报告列表 艾瑞研究-艾瑞网 互联网行业报告 艾瑞APP指数 移动App TOP 1000 月度活跃和日活跃 艾瑞PC指数 PC TOP 1000 月度活跃和日活跃 199IT互联网数据中心 中国互联网络信息中心 数据新知 - 易观 【友盟+】数据报告 http://www.dcci.com.cn/report/index.html 北京赛诺市场研究有限责任公司 赛诺数据，智能机出货量的专业统计 数据报告-移动观象台-TalkingData Talkingdata报告 艾媒网-全球移动互联网行业数据发布平台/iiMedia Research出品 DataEye大数据移动营销综合服务商-数据报告 手游方向 ASO100 - 专业App Store数据平台丨ASO优化专家平台丨iOS榜单排行榜查询工具 电影电视行业免费报告列表页 研究娱乐行业 旅游数据报告-旅游圈旅游行业报告 小程序报告-阿拉丁统计 爱应用：一个应用所有历史版本的产品分析截图记录 Appannie，国外下载应用 卡思数据-短视频网红分析数据分析 国金证券研究所 国家宏观经济研究数据和报告 中金研报 抖音快手的热门视频和kol的 各大媒体的每天的热门排行榜 短视频行业的数据 招商证券的电商类报告电商类的行业观察，企业研究，品牌深度报告 短视频和图文内容类的行业报告-新榜 http://www.100ec.cn/zt/wmds/ 涵盖跨境电商所有报告，行业数据和研报 镝数聚-权威数据 海量聚合 提取了报告中的数据，颗粒度比较细 东方财富研报首页 东方财富研报 2. 国家机构公开数据 中国信通院-研究成果-权威发布-权威数据 中国城市轨道交通协会城市地铁线路的流量数据 国家的便民服务查询（包括5A景区list，小微企业名录，法人信用查询，出租车信息查询） 国家宏观经济数据（GDP,CPI，总人口，社会消费品零售总额，粮食产品，PPI，各地区行政规划，各地财政收支等等，分月度季度和年度）部分数据如下 国家统计局（数据多到瞠目结舌，包括年度，季度，国家，国际，年鉴，介乎涵盖所有数据指标和历史）部分数据举例 世界银行的公开数据库（有健康，农业，公共部部门，人口分布，外债，性别，教育，环境，气候变化，能源，贫困等各种公开数据） 世界数据图册（世界和地区统计资料，各国数据，地图，排名）包含的全球的国家公开的数据 国家机关部委的公开数据（包括国家发展和改革委员会，教育部，民政部，司法部，财政部，工业和信息化不，交通运输部，文化和旅游部等） 各城市开放数据（包括浙江数据开放网，青岛数据开放网，贵阳数据开放平台，成都数据公开平台，合肥数据开放平台，河南开放数据平台等） 宏观经济查询数据（包括高校财经信息库，人民网经济数据库，香港统计处，联合国统计司，世界经合组织，欧盟统计局，国际货币基金组织等） 房价数据（包括中国房价指数，房价走势，台湾房价行情，北京房价查询，深圳楼盘成交查询等，上海地铁房价，贝壳指数等） 汽车数据（包括中国汽车工业协会数据，百度汽车网，易车汽车指数，汽车渠道直通车，中国汽车流通协会数据中心，德国汽车工业协会等） 权威发布 | 中华全国商业信息中心 3. 国内互联网公司数据报告网站列表 讲座PPT-腾讯大讲堂 Tencent 腾讯-业绩报告 腾讯大数据-腾讯云数据分析出来的行业报告 百度开放服务平台-百度云数据分析出来的行业报告 百度数据研究中心 提供行业研究报告、行业分析报告-百度数据中心报告 首页-阿里研究院-阿里行业研究报告 企鹅智酷_腾讯网-腾讯出品行业报告 腾讯CDC -腾讯交互设计报告 百度用户体验中心-百度UED用户研究报告 网易用户体验设计中心-网易UED用户研究报告 网络视频数据报告-优酷指数行业报告 PP指数_PPTV聚力-PPTV指数行业报告 360研究报告_360安全中心-360应用商店等产品出品报告 4. 国外咨询机构网站数据报告列表 国外咨询机构较多，数据详实，无论是海外出海产品，海外报告中多有亚洲和中国的重点研究，相关报告和趋势分析都可以选看 Flurry-国外app行业报告 App Annie Blog-app指数报告 https://www.appannie.com/insights/ (Appnnie的行业包括，包括app 分发行业的分发量和收入） BI Intelligence-business insider的报告 Today's Articles on Digital Marketing and Media-emarker的报告 http://www.newzoo.com/category/press-releases/-newzoo侧重于手游行业报告 Gartner Press Release Archives-gartner侧重于硬件的出货量，包括智能机和PC等 IDC - Search Results-IDC的硬件出货量全球报告 Yozzo Telecom News J.P. Morgan Home-摩根投行报告 德勤中国 | 审计, 企业管理咨询, 财务咨询, 风险管理, 税务服务及行业洞察 Precisely Everywhere-comscore的互联网行业报告 Ericsson - A world of communication（Global移动行业报告） GamesIndustry.biz（Global游戏行业报告） http://adfonic.com/（Global广告行业报告） Canalys | Insight. Innovation. Impact.（Global智能机报告） Mobile, Online &amp; Digital Market Research, Data &amp; Consultancy（通信无线报告） Home | GfK Global（终端比较专业的报告） Kantar Worldpanel（主要统计Android和ios的市场份额） PwC publications（皮尤的所有用户，市场研究报告） Fiksu | Data-fueled mobile marketing（统计app用户获取成本和应用商店下载频次的监测） https://www.weforum.org/reports（世界经济论坛的报告，揭示国内外发展的大趋势） Insights - Jampp （Jampp是国外的app 的粘性和转化漏洞的网站，在insights里还有行业的app的retention等benchmark的数据，有些类似flurry的行业数据） 罗兰贝格行业评论 战略和行业评论和报告 普华永道:blog 各个行业的主要发现和行业报告 Website Traffic &amp; Mobile App Analytics （similar web 以色列的网站分析工具，可以分析任何网站，包括用户，来源，终端，分布等等，数据非常棒） CADAS（全球航空公司研究报告）：非常支持和专业 印度互联网年报 - 竺帆 | 助力中企扬帆天竺 （印度出海报告，非常详细） GSMA： 全球移动互联网经济分析报告，全部免费下载报告和数据，从2015年到现在 商业价值研究院 -IBM（行业观点报告比较多） Home - McKinsey Greater China麦肯锡 **BCG - 波士顿咨询公司波士顿 企业管理咨询公司罗兰贝格 Accenture - China埃森哲** 5. 各大公司不定期发布的报告，比如（细分方向的时候用）** 高德地图：2015年度中国主要城市交通分析报告 微信城市服务发布《2015微信政务民生白皮书》 【报告】淘宝发布 2015 中国消费趋势数据，2015 年我们为什么买单？ 互联网增长的第一本数据分析手册-Growing IO的公开手册 移动游戏运营数据分析指标白皮书（一）-Talkingdata 运营指标分析白皮书 多多大学 （多多大学也分享了很多的拼多多运营数据还提供课程，可以看） 6. 找行业内的人事打听内部一手资料 关注一些专门打听行业内部人事的信息来源 这里先推荐一家公共号：晚点LatePost（微信搜索公共号可以 关注） 这家主要是会 看一些行业内部和重要的消息 在行上约人。在行 App 如果想知道一些企业的信息，可以在在行上找到一些行家，曾经一手经营或者运营过祥光额项目和参与过竞品和行业公司的操盘，可以约出来，从信息和方法论角度获得资讯 7. 企业信息报告** 新三板在线 - 中国最大的新三板生态平台（各行各业的新三板上市公司财务数据，高管数据等） 企查查|企业查询（查询企业的产品，品牌和法人信息） 企业注册信息查询（天眼查，同企查查） SEC.gov | Home（美国上市公司年度财务报告） 巨潮资讯网—（中国上市公司季度年度财务报告） Baidu | Investors（各大上市公司季度财报，IR.XX公司.com，比如百度这个） 天眼查（可以查到各个企业的详细信息，还可以查到员工个数） 8. 爬虫网站或者APP的数据 最近研究发现，还有一个好的行业信息获取来源，就是通过站内或者App内的爬虫抓取，这个渠道获取的数据，通常可以帮助你了解行业和竞品的站内使用情况，用户喜欢的内容，用户的分布，用户的行为和喜好等等。 爬虫，简单来说是通过程序来获取网页的信息，整理成数据库，从而进行数据挖掘的得到分析结论的过程。比如你可以爬虫购物的页面，知道哪个商品的销量好，比如你可以爬虫小红书的页面，知道哪些kol收到欢迎，你还可以爬取他们的分类，知道美妆和购物的kol表现好，并且有多少个这样的kol。如果你没有对方的数据库权限（当然你肯定没有），那么从外部爬虫是最好的了解他们业务数据的方式。 通常的搜索方式 是：你要了解的网站/App+爬虫，在搜索平台比如百度搜索 这里举例一些程序员垂直的网站， CSDN网站：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索 简书 - 创作你的创作：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索， V2EX：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索 掘金：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索 9 . 业内微信群 现在发现很多好的内部报告和难以获得报告，是通过加入一些干货群，内部群来获得的。 比如做直播电商的人自己比较关注一些直播和电商带货的详细的数据和报告趋势，大家会自己组建一些干货群，只要是市面上有的报告，自己内部发现的都会往里面扔。 这个是淘宝直播的负责人赵圆圆离开淘宝后创业，同时聚集的几个群，里面关于直播的干货非常多。 其他的关于投资的，趋势，创业的类似群也很多，获取报告也很一手，大家也可以自己开发下这样的群组织。 10. 搜索引擎 搜索引擎还是可以搜到很多你个性化想要找的报告和趋势。以前我没觉得搜索引擎很很难，后来发现也需要学习和熟练使用，才能让其为自己所用。 如何使用搜索引擎 11. 各大公司的财报 通常对于上市公司来说，财报信息包含的内容是最全面的，关于用户，商业，渠道，增长，业务策略等等。所以如果想了解一个公司，如果是上市公司最好第一手先看财报后者SEC（上市报告）。 很多同学问我财报哪里找，不知道怎么看。其实每个公司都有自己的IR（投资者页面），在上面有财报的完整的pdf下载。另外，也推荐大家听听每期的企业conference call（回答财报问题），可以听下CEO对财报的解读。 这里我列举几个大公司财报的网站 wind：金融数据库，包含财报和行业信息（wind的账号可以到闲鱼租） 百度财报 PDF：Baidu | Investors | Home 百度财报解读podcast：音频可以在线听 阿里财报pdf：阿里巴巴集团 腾讯财报pdf：Tencent 腾讯 - 投资者关系 搜狐财报pdf：http://investors.sohu.com/ 拼多多pdf：Investor Relations | Pinduoduo Inc. 拼多多财报解读：音频可以在线听。 如果大家有自己想要了解的公司，在百度or google搜索：公司名字+IR ，可以 定位到他们公司的财报网站页面。在页面上找到conference call或者webcast，可以 找到他们的财报解读音频。 12. 投资机构的统计网站（创业方向选择，投融资选择的时候用） IT桔子 | IT互联网公司产品数据库及商业信息服务（IT桔子，中国创业公司投融资数据和报告） 研究院_ChinaVenture投资中国网-（投中的每个季度的行业融资报告，不定期有专项分析报告） CB Insights - Blog （CBI insights的一系列产品，包括公司的估值，独角兽公司列表等） The Downround Tracker（公司估值下降的趋势） The Complete List of Unicorn Companies（独角兽公司列表） IPO Center: IPO Market, IPO News, IPO Calendars, IPO Pricings, IPO Voting（IPO相关新闻和趋势报告） PrivCo | Private Company Financial Intelligence（美国金融数据公司，主要关注未上市公司的所有投融资资料，目前涵盖的公司包括全世界，当然也包括中国公司） 券商行业研究报告 （国内券商的行业报告，策略报告，可以筛选行业，筛选报告类型） https://pitchbook.com/news/reports（PitchBook的PE,VC，M&amp;A行业报告） 研究院_ChinaVenture投资中国网 （IPO 投融资行业报告） Dow Jones VentureSource 2Q’16 U.S. Venture Capital Report（道琼斯旗下机构Dow Jones LP Source行业投资报告） NVCA Venture Investment（美国国家风险投资协会，每个季度和年度都会出投融资行业报告） PWC-MoneyTree Home（PWC的money tree report是每个季度美国的风险投资行业报告） https://home.kpmg.com/xx/en/home/insights.html （KPMG毕马威的insights报告，一般是每个季度的创投趋势，比较细致的分析） Mattermark - Discover, Enrich, &amp; Analyze Companies（创业公司投资并购信息一站式搜索） M&amp;A, Private Equity &amp; Venture Capital Database（创业公司投资并购信息一站式搜索） DataFox | Prospect Sales Leads with Company Signals（创业公司投资并购信息一站式搜索） CrunchBase accelerates innovation by bringing together data on companies and the people behind them.（创业公司数据库） Venture Intelligence PE/VC database Stock Market Insights | Seeking Alpha （二级市场金融分析网站） Tencent Holdings Ltd -- Trefis（各个公司的revenue model的预测和key driver的趋势，这个网站简直不能再棒） 13. 本地数据库 这个世界有很多有用的信息，搜索引擎只解决了其中20%，其他80%的信息再各个角落，包括微信群，包括口口，甚至包括直播里都有，但是都不在搜索引擎。 就搜索引擎而言，现在很多人只是使用了其中的5%还不到。搜索引擎的技巧可以提升，但是其他80%的信息获取渠道更为隐蔽和无法公开获得的。 我加了很多 群，里面都是这些报告和信息和各行各业的各种信息，这些是搜索引擎提供不了的 这些冰山下的信息才决定了信息的获取的不同和优质与否。 除了上述渠道外，能找到靠谱渠道，找到合适的报告随时存储起来，等用的时候随手打开用是最好的。分享一个我最近看的收藏的精品的报告收藏夹，也希望对你们有用（随时更新） 共享下我看过的精品报告的收藏夹list 14. 怎么提炼自己获取信息的层次和获取信息的价值 找到行业信息报告知识获取信息只是其中一个层次 ，获取信息是否更有价值更直接可用，在于基本功行业信息报告的甄别和获取，积累和提炼，这是非常重要的。 但是 越往上走，越是接近信息更有价值，更新鲜，更真实有效，更直接，有大量的渠道 可以 获得更多 的信息，这些不仅是通过 行业报告获取的，还有包括自己可以控制的方法，包括爬虫，数据挖掘，信息技术 等，还包括人脉，圈子，内幕的等渠道。大家感兴趣可以到这个答案看下详情，我对每个层级的方法的解读。 哪些渠道可以获取一般人不知道的知识和信息 15. 其他（不定期更新） IBM商业价值研究院","link":"/Analysis_data_and_research_report_collection/"},{"title":"扩展欧几里德","text":"PE -05欧几里德算法 又名「辗转相除」法 迄今为止已知的最古老的算法, 距今(2017年)2317年 用于快速计算两个数字的最大公约数 还可以用于快速求解ax + by = 1方程的一组整数解 扩展欧几里德 已知算法上推导其它算法的流程, 思想过程才是最重要的;","link":"/Extended-Euclid-algorithm/"},{"title":"04 GPT-3&#x2F;4对比其他模型胜在哪？","text":"大家好，我是茶桁。 在前两节课中，我们一起体验了 OpenAI 提供的 GPT-3.5 系列模型的两个核心接口。一个是获取文本的 Embedding 向量，另一个是根据提示语生成补全的文本内容。通过这两种方法，我们可以在零样本或少样本的情况下进行情感分析任务。然而，你可能会有两个疑问。首先，Embedding 不就是将文本转换为向量吗？为什么不直接使用开源模型（如Word2Vec、Bert）而要调用 OpenAI 的 API 呢？在我们的情感分析任务中，我们进行了一些简化处理。一方面，我们排除了相对中性的评分（3分）；另一方面，我们将1分、2分和4分、5分合并，将原本需要判断5个分类的问题简化了。那么，如果我们想要准确预测多个分类，是否也能如此简单呢？ 在本节中，我们将通过代码和数据来回答第一个问题，尝试使用常见的开源模型，看看是否可以通过零样本学习的方式取得良好的效果。至于第二个问题，我们将在下节课中探讨，探索如何进一步利用 Embedding 结合机器学习算法来更好地处理情感分析问题。 什么是预训练模型？ 预训练模型是指通过在大规模文本数据上进行学习而生成的模型，能够将文本转化为语义丰富的向量表示。OpenAI 的 GPT-3 是一种超大规模的预训练模型，其英文全称为“Generative Pre-trained Transformer”，即生成式预训练 Transformer。通过预训练模型，我们可以在没有见过具体问题的情况下，利用大量可用的文本数据进行学习。 预训练模型的学习过程并不需要人工标注的数据。相反，它利用了大量的文本数据，如网页文章、维基百科内容、书籍电子版等，通过观察文本之间的内在关联来学习文本的语义。例如，在网络资料中，我们可以观察到许多类似的文本，如“小猫很可爱”和“小狗很可爱”。通过这些文本的前后关联，我们可以推断出小猫和小狗都是宠物，并且一般而言，人们对它们的情感是正面的。 这种隐含的关联信息可以帮助我们在情感分析等任务中填补少量用户评论和评分数据中缺失的“常识”。例如，如果文本中出现了“白日依山尽”，模型就能推测出接下来可能是“黄河入海流”。同样地，如果文本开头是“今天天气真”，那么后面很可能是“不错”，而较少可能是“糟糕”。这些文本关系最终通过一系列参数来体现。对于输入的文本，模型可以根据这些参数计算出一个向量，然后利用这个向量来推测文本的后续内容。 预训练模型在自然语言处理领域并不仅限于 OpenAI。早在2013年，Word2Vec就提出了类似的思想，通过在同一个句子中观察单词前后出现的单词，生成每个单词的向量表示。随后，在2018年，Google提出的BERT模型引起了整个行业的关注，BERT也成为了解决自然语言处理任务的重要预训练模型。在GPT-3论文发表之前，人们普遍认为BERT作为预训练模型在效果上优于GPT。然而，通过预训练模型，我们可以在语料文本越丰富的情况下拥有更多的参数，从而学习到更多的文本关系。因此，出现频率越高的文本关系，模型的预测能力也就越准确。随着时间的推移和研究的不断深入，预训练模型在自然语言处理领域的应用不断扩展。GPT-3的发布引发了巨大的关注，其规模庞大的预训练模型在生成式任务上取得了令人瞩目的成果。而BERT则在诸多下游任务中表现出色，成为许多实际应用的首选模型。 通过预训练模型，我们能够快速、高效地将文本转化为有意义的向量表示，从而在各种自然语言处理任务中取得良好的效果。随着预训练模型的不断发展和改进，我们可以期待更多的创新和突破，使得模型能够更准确地理解和处理各种复杂的自然语言表达。 预训练模型的引入为自然语言处理带来了巨大的进步和便利。它们通过利用大规模文本数据的内在关联，将语言转化为计算机能够理解和处理的形式，为我们解决各类自然语言处理任务提供了强大的基础。未来，随着技术的不断演进和研究的不断推进，预训练模型将继续发挥重要作用，推动自然语言处理领域的发展和创新。 Fasttext、T5、GPT-3 模型对比 在探索预训练模型的应用中，我们不仅限于OpenAI的API，还可以考虑使用一些开源的模型。今天，我们将重点介绍两个开源预训练模型，并探讨它们在文本向量化方面的表现是否能与OpenAI的API媲美。 首先是Facebook的FastText，这个模型是基于Word2Vec思想的延伸，能够将单词表示成向量。通过FastText，我们可以将文本转化为向量表示，以便在后续的自然语言处理任务中使用。 第二个模型是Google的T5，T5的全称是Text-to-Text Transfer Transformer，它是一种适用于迁移学习的模型。迁移学习是指将T5生成的向量结果用于其他自然语言处理任务的机器学习过程。在许多最新的研究论文中，T5常被用作预训练模型进行微调和训练，或者作为基准模型用于对比和评估。 通过研究这两个开源预训练模型，我们可以更全面地了解它们在文本向量化方面的性能和适用性。无论是FastText还是T5，它们都为我们提供了更多选择和灵活性，让我们能够根据具体任务的需求来选择最适合的预训练模型。预训练模型的开源社区不断发展壮大，为我们提供了更多的工具和资源，助力我们在自然语言处理领域取得更好的效果。 Fasttext 效果测试 在开始使用FastText之前，我们需要先安装Python包管理工具中的FastText和Gensim两个包。如果您使用的是Conda作为Python包管理工具，可以按照以下示例代码安装所需的包： 1conda install -c conda-forge fasttext gensim 如果您使用的是PIP或其他Python包管理工具，可以将上述代码中的\"conda\"替换为相应的包管理工具名称，并执行相应的安装命令。 如果你用conda无法安装，也可以尝试PIP安装，效果是一样的。 通过执行上述代码，我们可以安装所需的FastText和Gensim包。这两个包提供了在文本向量化过程中使用的工具和函数。安装完成后，我们将能够顺利进行FastText的使用，并进行相关的自然语言处理任务。请确保在运行代码之前先完成包的安装。 接下来，我们需要将与FastText对应的模型下载到本地。这些开源库和相关论文由Facebook和Google等国外公司发布，因此其在英语处理方面的效果较好。我们将下载与英语相关的模型，名称为\"cc.en.300.bin\"。我在这里提供了相应模型的下载链接。 在这里，可以找到更多的Fasttext模型 将模型下载下来之后解压缩，放在你的Notebook同目录下。 代码的逻辑并不复杂。首先，我们使用Gensim库加载了预训练好的FastText模型。接下来，我们定义了一个函数来获取文本的向量表示。由于FastText学习的是单词的向量而不是句子的向量，同时我们想要测试零样本学习的效果，因此无法进一步根据评论数据对模型进行训练。因此，我们采用了一种常见的方法，将一句话中每个单词的向量相加并取平均，将得到的向量作为整个评论的表示。我们将这个操作封装成了一个名为 get_fasttext_vector 的函数，以供后续程序使用。 在这里，我们还是拿第二讲中的数据集，用的是2.5W条亚马逊食物评论的数据。 123456789101112import gensimimport numpy as np# load the FastText pre-trained modelmodel = gensim.models.fasttext.load_facebook_model('cc.en.300.bin')def get_fasttext_vector(line): vec = np.zeros(300) # Initialize an empty 300-dimensional vector for word in line.split(): vec += model.wv[word] vec /= len(line.split()) # Take the average over all words in the line return vec 而对应的零样本学习，我们还是和第 02 讲一样，将需要进行情感判断的评论分别与 “An Amazon review with a positive sentiment.” 以及 “An Amazon review with a negative sentiment.” 这两句话进行向量计算，算出它们之间的余弦距离。 离前一个近，我们就认为是正面情感，离后一个近就是负面情感 1234567891011121314151617181920212223positive_text = &quot;&quot;&quot;Wanted to save some to bring to my Chicago family but my North Carolina family ate all 4 boxes before I could pack. These are excellent...could serve to anyone&quot;&quot;&quot;negative_text = &quot;&quot;&quot;First, these should be called Mac - Coconut bars, as Coconut is the #2 ingredient and Mango is #3. Second, lots of people don't like coconut. I happen to be allergic to it. Word to Amazon that if you want happy customers to make things like this more prominent. Thanks.&quot;&quot;&quot;positive_example_in_fasttext = get_fasttext_vector(positive_text)negative_example_in_fasttext = get_fasttext_vector(negative_text)positive_review_in_fasttext = get_fasttext_vector(&quot;An Amazon review with a positive sentiment.&quot;)negative_review_in_fasttext = get_fasttext_vector('An Amazon review with a negative sentiment.')from openai.embeddings_utils import cosine_similaritydef get_fasttext_score(sample_embedding): return cosine_similarity(sample_embedding, positive_review_in_fasttext) - cosine_similarity(sample_embedding, negative_review_in_fasttext)positive_score = get_fasttext_score(positive_example_in_fasttext)negative_score = get_fasttext_score(negative_example_in_fasttext)print(&quot;Fasttext好评例子的评分 : %f&quot; % (positive_score))print(&quot;Fasttext差评例子的评分 : %f&quot; % (negative_score)) 输出结果为： Fasttext好评例子的评分 : -0.000544 Fasttext差评例子的评分 : 0.000369 在亚马逊食品评论数据集中，我们选择了一个正面评分为5分的例子和一个负面评分为1分的例子进行测试。然而，结果令人遗憾的是，通过零样本学习的方式，程序对这两个例子的判断都是错误的。然而，仔细思考一下，这样的结果其实是可以理解的。因为我们的方法是将整个句子的向量表示取平均，这意味着可能会出现之前提到的单词顺序不同的问题。例如，“not good, really bad” 和 “not bad, really good” 在这种情况下，意思完全不同，但向量表示完全相同。此外，我们对比的正面情感和负面情感两个句子之间只有一个单词的差异（如positive/negative）。如果我们只考虑单词的出现情况，并将它们的向量取平均，这种策略如果真的能够取得良好的效果，那反而应该怀疑是否存在Bug。 T5效果测试 Fasttext并没有取得令人满意的结果，这可以理解，毕竟Word2Vec已经是10年前的技术了。那么，接下来我们来看看使用了目前最流行的Transformer结构的T5模型的效果如何。 T5模型的全称是Text-to-Text Transfer Transformer，意思是“文本到文本的迁移Transformer”。这个模型的设计初衷是为了方便在预训练之后将其迁移到其他任务上。当T5模型发布时，它在各种数据集上的评测表现都名列前茅。 T5模型中最大的模型有110亿个参数，同样基于Transformer。虽然相较于GPT-3的1750亿个参数要小很多，但对硬件性能的要求也不低。因此，我们首先测试一下T5-Small这个小型模型的效果。 在实际运行代码之前，我们需要安装相应的Python包。这里我们分别安装了SentencePiece和PyTorch。在安装PyTorch时，我还顺便安装了Torchvision，因为在后续课程中会用到它。 123conda install -c conda-forge transformersconda install pytorch torchvision torchaudio -c pytorch-nightlyconda install -c conda-forge sentencepiece 别问我为什么选择 pytorch-nightly , 简而言之，我用的是M1. 我们需要执行的代码非常简单。首先，我们加载预训练的T5模型的分词器（Tokenizer）以及相应的模型。接下来，我们定义了一个名为get_t5_vector的函数，它接收一段文本输入，并使用分词器对其进行分词，生成一个序列。然后，我们将序列输入到模型的编码器部分进行编码。编码后的结果仍然是分词后的每个词的向量。与之前的Fasttext不同的是，这里的向量会随着位置和相邻词的不同而产生变化。因此，我们将这些向量取平均，作为整段文本的向量表示。需要注意的是，尽管我们进行了平均操作，但这里每个词的向量仍然包含了顺序信息的语义信息。值得注意的是，执行这段代码可能会稍慢一些。因为在首次加载模型时，Transformer库会将模型下载到本地并进行缓存，这个下载过程可能需要一些时间。 123456789101112131415161718from transformers import T5Tokenizer, T5Modelimport torch# load the T5 tokenizer and modeltokenizer = T5Tokenizer.from_pretrained('t5-small', model_max_length=512)model = T5Model.from_pretrained('t5-small')# set the model to evaluation modelmodel.eval()# encode the input sentencedef get_t5_vector(line): input_ids = tokenizer.encode(line, return_tensors = 'pt', max_length=512, truncation = True) # generate the vector representation with torch.no_grad(): outputs = model.encoder(input_ids = input_ids) vector = outputs.last_hidden_state.mean(dim=1) return vector[0] 有了模型和通过模型获取的向量数据，我们就可以再试一试前面的零样本学习的方式，来看看效果怎么样了。我们简单地把之前获取向量和计算向量的函数调用，都换成新的 get_t5_vector，运行一下就能看到结果了。 1234567891011121314151617positive_review_in_t5 = get_t5_vector(&quot;An Amazon review with a positive sentiment.&quot;)negative_review_in_t5 = get_t5_vector('An Amazon review with a negative sentiment.')def test_t5(): positive_example_in_t5 = get_t5_vector(positive_text) negative_example_in_t5 = get_t5_vector(negative_text) def get_t5_score(sample_embedding): return cosine_similarity(sample_embedding, positive_review_in_t5) - cosine_similarity(sample_embedding, negative_review_in_t5) positive_score = get_t5_score(positive_example_in_t5) negative_score = get_t5_score(negative_example_in_t5) print(&quot;T5好评例子的评分 : %f&quot; % (positive_score)) print(&quot;T5差评例子的评分 : %f&quot; % (negative_score))test_t5() 输出结果： T5好评例子的评分 : -0.010294 T5差评例子的评分 : -0.008990 然而，不幸的是，我们得到的结果并不太理想。两个例子都被错误地判断为负面情绪，并且正面评价的分数还更低。不过，不要着急，这可能是因为我们使用的模型太小了。毕竟，在T5论文中，110亿个参数的大模型在各项评估中表现卓越，而我们使用的是T5-Small，这个相同架构下参数只有6000万个的小模型。要训练110亿个参数需要很长时间。因此，为什么不尝试一下参数为2.2亿个的T5-Base模型呢？使用起来也很简单，只需要将模型名称从T5-Small更改为T5-Base，其他代码无需修改，然后重新运行一遍。 12345678910111213141516171819tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=512)model = T5Model.from_pretrained('t5-base')# set the model to evaluation modemodel.eval()# encode the input sentencedef get_t5_vector(line): input_ids = tokenizer.encode(line, return_tensors='pt', max_length=512, truncation=True) # generate the vector representation with torch.no_grad(): outputs = model.encoder(input_ids=input_ids) vector = outputs.last_hidden_state.mean(dim=1) return vector[0]positive_review_in_t5 = get_t5_vector(&quot;An Amazon review with a positive sentiment.&quot;)negative_review_in_t5 = get_t5_vector('An Amazon review with a negative sentiment.')test_t5() 输出结果为： T5好评例子的评分 : 0.010347 T5差评例子的评分 : -0.023935 这次的结果似乎符合我们的期望，好评被判断为正面情感，差评被判断为负面情感。然而，也许我们只是运气好，在这几个例子上看到了效果。因此，接下来让我们将整个数据集中的1分和2分的差评以及4分和5分的好评都提取出来进行观察。在OpenAI的API中，使用的Embedding可以达到95%以上的准确率，让我们看看使用这个参数为2.2亿的T5-Base模型会得到什么样的结果。 相应的代码并不复杂，基本上与第02讲中OpenAI提供的Embedding代码相似。只是通过pandas库，根据评论的文本字段，对T5模型进行计算，然后将结果存储到DataFrame的\"t5_embedding\"列中。 同样地，我们还需要使用T5模型获得\"An Amazon review with a positive sentiment.\"和\"An Amazon review with a negative sentiment.\"这两个句子的Embedding。然后，我们将用户评论的Embedding与这两个句子的Embedding计算余弦距离，以判断这些评论是正面还是负面。 最后，使用Scikit-learn库中的分类报告类将评估结果打印出来。 1234567891011121314151617181920212223242526import pandas as pdfrom sklearn.metrics import classification_reportdatafile_path = &quot;./data/fine_food_reviews_with_embeddings_1k.csv&quot;df = pd.read_csv(datafile_path)df[&quot;t5_embedding&quot;] = df.Text.apply(get_t5_vector)# convert 5-star rating to binary sentimentdf = df[df.Score != 3]df[&quot;sentiment&quot;] = df.Score.replace({1: &quot;negative&quot;, 2: &quot;negative&quot;, 4: &quot;positive&quot;, 5: &quot;positive&quot;})from sklearn.metrics import PrecisionRecallDisplaydef evaluate_embeddings_approach(): def label_score(review_embedding): return cosine_similarity(review_embedding, positive_review_in_t5) - cosine_similarity(review_embedding, negative_review_in_t5) probas = df[&quot;t5_embedding&quot;].apply(lambda x: label_score(x)) preds = probas.apply(lambda x: 'positive' if x&gt;0 else 'negative') report = classification_report(df.sentiment, preds) print(report) display = PrecisionRecallDisplay.from_predictions(df.sentiment, probas, pos_label='positive') _ = display.ax_.set_title(&quot;2-class Precision-Recall curve&quot;)evaluate_embeddings_approach() 输出结果如图： 实验结果显示，使用T5模型的效果还算不错，对于所有样本的准确率达到了90%。然而，在处理较为困难的差评判断时，T5模型的表现要比直接使用OpenAI提供的Embedding差很多，整体精度仅为60%。与此相比，当我们观察整体模型准确率时，OpenAI的Embedding能够达到96%，相比于这里的90%，要更好一些。 以下是我们上节做的测试结果： 12345678 precision recall f1-score support negative 0.98 0.73 0.84 136 positive 0.96 1.00 0.98 789 accuracy 0.96 925 macro avg 0.97 0.86 0.91 925weighted avg 0.96 0.96 0.96 925 当然，这个分数还是相当不错的，可以作为一个合格的情感分析分类器的基准线。毕竟，我们采用的是零样本分类的方法，没有对需要分类的数据进行任何训练，直接使用预训练模型提供的向量，并根据距离进行判断。因此，看起来更大规模的预训练模型确实很有用，可以取得更好的效果。此外，当因为成本或网络延迟等原因无法方便地使用OpenAI的API时，如果只需要获取文本的嵌入向量，使用T5这样的开源模型的效果也相当不错。 小结 最后，我们来总结一下本讲的内容。在本讲中，我们使用了 Fasttext、T5-small 和 T5-base 这三个预训练模型进行了零样本分类的测试。在相同的食物评论数据集上，使用只学习了单词向量表示的 Fasttext，效果非常糟糕。当我们转而使用基于 Transformer 的 T5 模型时，即使是规模较小的 T5-small（6000 万参数），效果也不理想。然而，当我们尝试了具有 2.2 亿参数的 T5-base 模型时，结果有所改善。但是，这仍然远远落后于直接使用 OpenAI 的 API 的效果。这表明，模型的大小在情感分析这样简单的问题上也能产生显著差异。课后可以进行练习来进一步巩固所学知识。 课后练习 我们在尝试使用 T5-base 这个模型之后，下了个判断认为大一点的模型效果更好。不过，其实我们并没有在整个数据集上使用 T5-small 这个模型做评测，你能试着修改一下代码，用 T5-small 测试一下整个数据集吗？测试下来的效果又是怎样的呢？ 我们使用 Fasttext 的时候，把所有的单词向量平均一下，用来做情感分析效果很糟糕。那么什么样的分类问题，可以使用这样的方式呢？给你一个小提示，你觉得什么样的文本分类，只关心出现的单词是什么，而不关心它们的顺序？","link":"/GPT-3-VS-Other-Model/"},{"title":"Finish the search problem","text":"The code address of this article is: example_01_Assignment Please read the answer below after thinking for yourself Please using the search policy to implement an agent. This agent receives two input, one is @param start station and the other is @param destination. Your agent should give the optimal route based on Beijing Subway system. Dataflow: 1. Get data from web page. Get web page source from: https://baike.baidu.com/item/%E5%8C%97%E4%BA%AC%E5%9C%B0%E9%93%81/408485 You may need @package requests https://2.python-requests.org/en/master/ page to get the response via url You may need save the page source to file system. The target of this step is to get station information of all the subway lines; You may need install @package beautiful soup https://www.crummy.com/software/BeautifulSoup/bs4/doc/ to get the url information, or just use &gt; Regular Expression to get the url. Our recommendation is that using the Regular Expression and BeautiflSoup both. You may need BFS to get all the related page url from one url. Question: Why do we use BFS to traverse web page (or someone said, build a web spider)? Can DFS do this job? which is better? 2. Preprocessing data from page source. Based on the page source gotten from url. You may need some more preprocessing of the page. the Regular Expression you may need to process the text information. You may need @package networkx, @package matplotlib to visualize data. You should build a dictionary or graph which could represent the connection information of Beijing subway routes. You may need the defaultdict, set data structures to implement this procedure. 3. Build the search agent Build the search agent based on the graph we build. for example, when you run: 1&gt;&gt;&gt; search('奥体中心', '天安门') you need get the result: 奥体中心-&gt; A -&gt; B -&gt; C -&gt; ... -&gt; 天安门 HTTP协议 超文本传输协议（HTTP，HyperText Transfer Protocol）是互联网上应用最为广泛的一种网络协议。所有的www文件都必须遵守这个标准。 HTTP用于客户端和服务器之间的通信。协议中规定了客户端应该按照什么格式给服务器发送请求，同时也约定了服务端返回的响应结果应该是什么格式。 请求访问文本或图像等信息的一端称为客户端，而提供信息响应的一端称为服务器端。 客户端告诉服务器请求访问信息的方法： - Get 获得内容 - Post 提交表单来爬取需要登录才能获得数据的网站 - put 传输文件 更多参考： HTTP请求状态 了解200 404 503 - 200 OK //客户端请求成功 - 404 Not Found //请求资源不存在，eg：输入了错误的URL - 503 Server Unavailable //服务器当前不能处理客户端的请求，一段时间后可能恢复正常。 #### Requests 纯粹HTML格式的网页通常被称为静态网页，静态网页的数据比较容易获取。 在静态网页抓取中，有一个强大的Requests库能够让你轻易地发送HTTP请求。 在终端上安装 Requests pip install requents 123456789101112# 获取响应内容import requests# get（输入你想要抓去的网页地址）r = requests.get('https://www.baidu.com/')print('文本编码：（服务器使用的文本编码）', r.encoding)print('响应状态码：（200表示成功）', r.status_code)print('字符串方式的响应体：（服务器响应的内容）', r.text) 拓展知识： Unicode和UTF-8有什么区别?(盛世唐朝回答) 正则表达式 正则表达式的思想是你在人群中寻找你的男朋友/女朋友，他/她在你心里非常有特点。 同样，从一堆文本中找到需要的内容，我们可以采用正则表达式。 正经点说，是以一定的模式来进行字符串的匹配。 掌握正则表达式需要非常多的时间，我们可以先入门，在以后的工作中遇到，可更加深入研究。 使用正则表达式有如下步骤： 寻找【待找到的信息】特点 使用符号找到特点 获得信息 12345678910111213141516171819202122232425262728293031323334353637383940# 请先运行一下、看一下有什么参数？# 请思考，找到会返回什么？没找到会返回什么？import rehelp(re.match)# 请运行之后、思考 match 与 search 的区别?m = re.search('foo', 'seafood')print(m)print(m.group())print('-------------------------')m = re.match('foo', 'seafood')print(m)#### `search`是搜索字符串中首次出现的位置# 匹配多个字符串 |m = re.match('bat|bet|bit', 'bat')print(m.group()) if m is not None else print('None')# 匹配任意单个字符 .m = re.match('.end', 'kend')print(m.group()) if m is not None else print('None')m = re.match('.end', 'end')print(m.group()) if m is not None else print('None')# 字符串集合 []m = re.match('[cr][23][dp][o2]', 'c3p2')print(m.group()) if m is not None else print('None')# [] 与 |是不同的m = re.match('c3po|r2d2', 'c3p2')print(m.group()) if m is not None else print('None') 给大家提供一个字典，供大家查询～ 字符 描述 &lt;/th&gt; 将下一个字符标记为一个特殊字符、或一个原义字符、或一个向后引用、或一个八进制转义符。例如，“n”匹配字符“n”。“”匹配一个换行符。串行“\\”匹配“&lt;/code&gt;”而“(”则匹配“(”。 &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;^&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配“&lt;code&gt;\\n&lt;/code&gt;”或“&lt;code&gt;\\r&lt;/code&gt;”之后的位置。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt; \\* &lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配前面的子表达式零次或多次。例如，zo\\*能匹配“&lt;code&gt;z&lt;/code&gt;”以及“&lt;code&gt;zoo&lt;/code&gt;”。\\* 等价于{0,}。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;+&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配前面的子表达式一次或多次。例如，“&lt;code&gt;zo+&lt;/code&gt;”能匹配“&lt;code&gt;zo&lt;/code&gt;”以及“&lt;code&gt;zoo&lt;/code&gt;”，但不能匹配“&lt;code&gt;z&lt;/code&gt;”。+等价于{1,}。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;?&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配前面的子表达式零次或一次。例如，“&lt;code&gt;do(es)?&lt;/code&gt;”可以匹配“&lt;code&gt;does&lt;/code&gt;”或“&lt;code&gt;does&lt;/code&gt;”中的“&lt;code&gt;do&lt;/code&gt;”。?等价于{0,1}。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;}&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;是一个非负整数。匹配确定的&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;次。例如，“&lt;code&gt;o{2}&lt;/code&gt;”不能匹配“&lt;code&gt;Bob&lt;/code&gt;”中的“&lt;code&gt;o&lt;/code&gt;”，但是能匹配“&lt;code&gt;food&lt;/code&gt;”中的两个o。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,}&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;是一个非负整数。至少匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;次。例如，“&lt;code&gt;o{2,}&lt;/code&gt;”不能匹配“&lt;code&gt;Bob&lt;/code&gt;”中的“&lt;code&gt;o&lt;/code&gt;”，但能匹配“&lt;code&gt;foooood&lt;/code&gt;”中的所有o。“&lt;code&gt;o{1,}&lt;/code&gt;”等价于“&lt;code&gt;o+&lt;/code&gt;”。“&lt;code&gt;o{0,}&lt;/code&gt;”则等价于“&lt;code&gt;o*&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;}&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;和&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;均为非负整数，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&amp;lt;=&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;。最少匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;次且最多匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;次。例如，“&lt;code&gt;o{1,3}&lt;/code&gt;”将匹配“&lt;code&gt;fooooood&lt;/code&gt;”中的前三个o。“&lt;code&gt;o{0,1}&lt;/code&gt;”等价于“&lt;code&gt;o?&lt;/code&gt;”。请注意在逗号和两个数之间不能有空格。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;?&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;当该字符紧跟在任何一个其他限制符（*,+,?，{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;}，{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,}，{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“&lt;code&gt;oooo&lt;/code&gt;”，“&lt;code&gt;o+?&lt;/code&gt;”将匹配单个“&lt;code&gt;o&lt;/code&gt;”，而“&lt;code&gt;o+&lt;/code&gt;”将匹配所有“&lt;code&gt;o&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;.&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配除“&lt;code&gt;\\&lt;/code&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;&lt;code&gt;n&lt;/code&gt;&lt;/span&gt;”之外的任何单个字符。要匹配包括“&lt;code&gt;\\&lt;/code&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;&lt;code&gt;n&lt;/code&gt;&lt;/span&gt;”在内的任何字符，请使用像“&lt;code&gt;(.|\\n)&lt;/code&gt;”的模式。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配pattern并获取这一匹配。所获取的匹配可以从产生的Matches集合得到，在VBScript中使用SubMatches集合，在JScript中则使用$0…$9属性。要匹配圆括号字符，请使用“&lt;code&gt;\\(&lt;/code&gt;”或“&lt;code&gt;\\)&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?:pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配pattern但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用或字符“&lt;code&gt;(|)&lt;/code&gt;”来组合一个模式的各个部分是很有用。例如“&lt;code&gt;industr(?:y|ies)&lt;/code&gt;”就是一个比“&lt;code&gt;industry|industries&lt;/code&gt;”更简略的表达式。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?=pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，“&lt;code&gt;Windows(?=95|98|NT|2000)&lt;/code&gt;”能匹配“&lt;code&gt;Windows2000&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;Windows3.1&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?!pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如“&lt;code&gt;Windows(?!95|98|NT|2000)&lt;/code&gt;”能匹配“&lt;code&gt;Windows3.1&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;Windows2000&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?&amp;lt;=pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;反向肯定预查，与正向肯定预查类拟，只是方向相反。例如，“&lt;code&gt;(?&amp;lt;=95|98|NT|2000)Windows&lt;/code&gt;”能匹配“&lt;code&gt;2000Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;3.1Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?&amp;lt;!pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;反向否定预查，与正向否定预查类拟，只是方向相反。例如“&lt;code&gt;(?&amp;lt;!95|98|NT|2000)Windows&lt;/code&gt;”能匹配“&lt;code&gt;3.1Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;2000Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;x|y&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配x或y。例如，“&lt;code&gt;z|food&lt;/code&gt;”能匹配“&lt;code&gt;z&lt;/code&gt;”或“&lt;code&gt;food&lt;/code&gt;”。“&lt;code&gt;(z|f)ood&lt;/code&gt;”则匹配“&lt;code&gt;zood&lt;/code&gt;”或“&lt;code&gt;food&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[xyz]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;字符集合。匹配所包含的任意一个字符。例如，“&lt;code&gt;[abc]&lt;/code&gt;”可以匹配“&lt;code&gt;plain&lt;/code&gt;”中的“&lt;code&gt;a&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[^xyz]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;负值字符集合。匹配未包含的任意字符。例如，“&lt;code&gt;[^abc]&lt;/code&gt;”可以匹配“&lt;code&gt;plain&lt;/code&gt;”中的“&lt;code&gt;p&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[a-z]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;字符范围。匹配指定范围内的任意字符。例如，“&lt;code&gt;[a-z]&lt;/code&gt;”可以匹配“&lt;code&gt;a&lt;/code&gt;”到“&lt;code&gt;z&lt;/code&gt;”范围内的任意小写字母字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[^a-z]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;负值字符范围。匹配任何不在指定范围内的任意字符。例如，“&lt;code&gt;[^a-z]&lt;/code&gt;”可以匹配任何不在“&lt;code&gt;a&lt;/code&gt;”到“&lt;code&gt;z&lt;/code&gt;”范围内的任意字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\b&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个单词边界，也就是指单词和空格间的位置。例如，“&lt;code&gt;er\\b&lt;/code&gt;”可以匹配“&lt;code&gt;never&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”，但不能匹配“&lt;code&gt;verb&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\B&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配非单词边界。“&lt;code&gt;er\\B&lt;/code&gt;”能匹配“&lt;code&gt;verb&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”，但不能匹配“&lt;code&gt;never&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\cx&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配由x指明的控制字符。例如，\\cM匹配一个Control-M或回车符。x的值必须为A-Z或a-z之一。否则，将c视为一个原义的“&lt;code&gt;c&lt;/code&gt;”字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\d&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个数字字符。等价于[0-9]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\D&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个非数字字符。等价于[^0-9]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\f&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个换页符。等价于\\x0c和\\cL。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\n&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个换行符。等价于\\x0a和\\cJ。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\r&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个回车符。等价于\\x0d和\\cM。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\s&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配任何空白字符，包括空格、制表符、换页符等等。等价于[ \\f\\n\\r\\t\\v]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\S&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配任何非空白字符。等价于[^ \\f\\n\\r\\t\\v]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\t&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个制表符。等价于\\x09和\\cI。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\v&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个垂直制表符。等价于\\x0b和\\cK。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\w&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配包括下划线的任何单词字符。等价于“&lt;code&gt;[A-Za-z0-9_]&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\W&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配任何非单词字符。等价于“&lt;code&gt;[^A-Za-z0-9_]&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\x&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，“&lt;code&gt;\\x41&lt;/code&gt;”匹配“&lt;code&gt;A&lt;/code&gt;”。“&lt;code&gt;\\x041&lt;/code&gt;”则等价于“&lt;code&gt;\\x04&amp;amp;1&lt;/code&gt;”。正则表达式中可以使用ASCII编码。.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;num&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;num&lt;/span&gt;，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;num&lt;/span&gt;是一个正整数。对所获取的匹配的引用。例如，“&lt;code&gt;(.)\\1&lt;/code&gt;”匹配两个连续的相同字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;标识一个八进制转义值或一个向后引用。如果\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;之前至少&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;个获取的子表达式，则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为向后引用。否则，如果&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为八进制数字（0-7），则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为一个八进制转义值。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;标识一个八进制转义值或一个向后引用。如果\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;之前至少有&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;个获得子表达式，则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;为向后引用。如果\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;之前至少有&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;个获取，则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为一个后跟文字&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;的向后引用。如果前面的条件都不满足，若&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;和&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;均为八进制数字（0-7），则\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;将匹配八进制转义值&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nml&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;如果&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为八进制数字（0-3），且&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m和l&lt;/span&gt;均为八进制数字（0-7），则匹配八进制转义值&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;l。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\u&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;是一个用四个十六进制数字表示的Unicode字符。例如，\\u00A9匹配版权符号（©）。&lt;/td&gt; &lt;/tr&gt; 12345678910111213# 匹配电子邮件地址patt = '\\w+@(\\w+\\.)?\\w+\\.com'm = re.match(patt, 'nobody@xxx.com')print(m.group()) if m is not None else print('None')# 匹配QQm = re.search('[1-9][0-9]{4,}', '这是我的QQ号781504542,第二个qq号：10054422288')print(m.group()) if m is not None else print('None')# findall() 是search的升级版，可以找到所有匹配的字符串m = re.findall('[1-9][0-9]{4,}', '这是我的QQ号781504542,第二个qq号：10054422288')print(m) if m is not None else print('None') 了解了怎么使用，下面进入实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# get the data (subway for beijing ,from amap)# 你需要用到以下的包import requestsimport reimport numpy as npr = requests.get('http://map.amap.com/service/subway?_1469083453978&amp;srhdata=1100_drw_beijing.json')r.textdef get_lines_stations_info(text): # Please write your code here pass # Traverse the text format data to form the location data structure # Dict of all line information: key: line name; value: list of site names lines_info = {} # A dict of all site information: key: site name; value: site coordinates (x, y) stations_info = {} for i in range(len(lines_list)): # Several questions you may need to think about, get &quot;Metro line name, station information list, station name, coordinates (x, y), add data to the information dict of the station, add data to the subway line dict&quot; passlines_info, stations_info = get_lines_stations_info(r.text)# According to the route information, establish the site adjacency table dictdef get_neighbor_info(lines_info): pass # Add str2 to the adjacency list of site str1 def add_neighbor_dict(info, str1, str2): # Please write code here pass return neighbor_info neighbor_info = get_neighbor_info(lines_info)neighbor_info# Draw subway mapimport networkx as nximport matplotlibimport matplotlib.pyplot as plt# If Chinese characters cannot be displayed, please refer tomatplotlib.rcParams['font.sans-serif'] = ['SimHei']# matplotlib.rcParams['font.family']='sans-serif'# You can use recursion to find all pathsdef get_path_DFS_ALL(lines_info, neighbor_info, from_station, to_station): # Recursive algorithm, essentially depth first # Traverse all paths # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Check input site name passdef get_next_station_DFS_ALL(node, neighbor_info, to_station): pass# You can also use the second algorithm: simple breadth first without heuristic functiondef get_path_BFS(lines_info, neighbor_info, from_station, to_station): # Search strategy: take the number of stations as the cost (because the ticket price is calculated by station) # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Since the cost of each layer is increased by 1, the cost of each layer is the same, and it does not matter whether it is calculated or not, so it is omitted # Check input site name pass# You can also use the third algorithm: heuristic search with path distance as the costimport pandas as pddef get_path_Astar(lines_info, neighbor_info, stations_info, from_station, to_station): # Search strategy: the straight-line distance between the stations of the route is accumulated as the cost, and the straight-line distance from the current station to the target is used as the heuristic function # Check input site name pass As much as you can to use the already implemented search agent. You just need to define the is_goal(), get_successor(), strategy() three functions. Define different policies for transfer system. Such as Shortest Path Priority（路程最短优先）, Minimum Transfer Priority(最少换乘优先), Comprehensive Priority(综合优先) Implement Continuous transfer. Based on the Agent you implemented, please add this feature: Besides the @param start and @param destination two stations, add some more stations, we called @param by_way, it means, our path should from the start and end, but also include the @param by_way stations. e.g 123451. Input: start=A, destination=B, by_way=[C] Output: [A, … .., C, …. B]2. Input: start=A, destination=B, by_way=[C, D, E] Output: [A … C … E … D … B] # based on your policy, the E station could be reached firstly. The Answer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305# get the data (subway for beijing ,from amap)import requestsimport reimport numpy as npr = requests.request('GET', url = 'http://map.amap.com/service/subway?_1469083453978&amp;srhdata=1100_drw_beijing.json')def get_lines_stations_info(text): lines_info = {} stations_info = {} pattern = re.compile('&quot;st&quot;.*?&quot;kn&quot;') lines_list = pattern.findall(text) for i in range(len(lines_list)): pattern = re.compile('&quot;ln&quot;:&quot;.*?&quot;') line_name = pattern.findall(lines_list[i])[0][6:-1] pattern = re.compile('&quot;rs&quot;.*?&quot;sp&quot;') temp_list = pattern.findall(lines_list[i]) station_name_list = [] for j in range(len(temp_list)): pattern = re.compile('&quot;n&quot;:&quot;.*?&quot;') station_name = pattern.findall(temp_list[j])[0][5:-1] station_name_list.append(station_name) pattern = re.compile('&quot;sl&quot;:&quot;.*?&quot;') position = tuple(map(float, pattern.findall(temp_list[j])[0][6:-1].split(','))) stations_info[station_name] = position lines_info[line_name] = station_name_list return lines_info, stations_infolines_info, stations_info = get_lines_stations_info(r.text)# print(stations_info)# print(lines_info)len(lines_info)def get_neighbor_info(lines_info): def add_neighbor_dict(info, str1, str2): list1 = info.get(str1) if not list1: list1 = [] list1.append(str2) info[str1] = list1 return info neighbor_info = {} for line_name, station_list in lines_info.items(): for i in range(len(station_list) -1): sta1 = station_list[i] sta2 = station_list[i+1] neighbor_info = add_neighbor_dict(neighbor_info, sta1, sta2) neighbor_info = add_neighbor_dict(neighbor_info, sta2, sta1) return neighbor_infoneighbor_info = get_neighbor_info(lines_info)print(neighbor_info)import networkx as nximport matplotlibimport matplotlib.pyplot as pltmatplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS']matplotlib.rcParams['font.size'] = 2plt.figure(figsize = (20, 20))stations_graph = nx.Graph()stations_graph.add_nodes_from(list(stations_info.keys()))nx.draw(stations_graph, stations_info, with_labels = True, node_size = 5)stations_connection_graph = nx.Graph(neighbor_info)plt.figure(figsize = (30, 30))nx.draw(stations_connection_graph, stations_info, with_labels = True, node_size = 5)# The first algorithm: recursively find all pathsdef get_path_DFS_ALL(lines_info, neighbor_info, from_station, to_station): # # Recursive algorithm, essentially depth first # Traverse all paths # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Check input site name if not neighbor_info.get(from_station): print('起始站点“%s”不存在。请正确输入！'%from_station) return None if not neighbor_info.get(to_station): print('目的站点“%s”不存在。请正确输入！'%to_station) return None path = [] this_station = from_station path.append(this_station) neighbors = neighbor_info.get(this_station) node = {'pre_station':'', 'this_station':this_station, 'neighbors':neighbors, 'path':path} return get_next_station_DFS_ALL(node, neighbor_info, to_station)def get_next_station_DFS_ALL(node, neighbor_info, to_station): neighbors = node.get('neighbors') pre_station = node.get('this_station') path = node.get('path') paths = [] for i in range(len(neighbors)): this_station = neighbors[i] if (this_station in path): # If this station is already in the path, it means a loop, and this road is unreachable return None if neighbors[i] == to_station: # Find the end, return to the path path.append(to_station) paths.append(path) return paths else: neighbors_ = neighbor_info.get(this_station).copy() neighbors_.remove(pre_station) path_ = path.copy() path_.append(this_station) new_node = {'pre_station':pre_station, 'this_station':this_station, 'neighbors':neighbors_, 'path':path_} paths_ = get_next_station_DFS_ALL(new_node, neighbor_info, to_station) if paths_: paths.extend(paths_) return pathspaths = get_path_DFS_ALL(lines_info, neighbor_info, '回龙观', '西二旗')print('共有%d种路径。'%len(paths))for item in paths: print(&quot;此路径总计%d站:&quot;%(len(item)-1)) print('-'.join(item))# The second algorithm: simple breadth first without heuristic functiondef get_path_BFS(lines_info, neighbor_info, from_station, to_station): # Search strategy: take the number of stations as the cost (because the ticket price is calculated by station) # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Since the cost of each layer is increased by 1, the cost of each layer is the same, and it does not matter whether it is calculated or not, so it is omitted # Check input site name if not neighbor_info.get(from_station): print('起始站点“%s”不存在。请正确输入！'%from_station) return None if not neighbor_info.get(to_station): print('目的站点“%s”不存在。请正确输入！'%to_station) return None # The search node is a dict, key=site name, value is a list of sites that contain passing nodes = {} nodes[from_station] = [from_station] while True: new_nodes = {} for (k,v) in nodes.items(): neighbor = neighbor_info.get(k).copy() if (len(v) &gt;= 2): # Do not go to the previous stop pre_station = v[-2] neighbor.remove(pre_station) for station in neighbor: # Traverse neighbors if station in nodes: # Skip the nodes that have been searched continue path = v.copy() path.append(station) new_nodes[station] = path if station == to_station: # Find the path, end return path nodes = new_nodes print('未能找到路径') return Nonepaths = get_path_BFS(lines_info, neighbor_info, '回龙观', '西二旗')print(&quot;路径总计%d站。&quot;%(len(paths)-1))print(&quot;-&quot;.join(paths))# Gaode Navigation is 31 stations, only 1 transfer# The result of the code is 28 stations, but there are 5 transfers# Guess Gaode's path cost is mainly time# The third algorithm: heuristic search with path distance as the costimport pandas as pddef get_path_Astar(lines_info, neighbor_info, stations_info, from_station, to_station): # Search strategy: the straight-line distance between the stations of the route is accumulated as the cost, and the straight-line distance from the current station to the target is used as the heuristic function # Check input site name if not neighbor_info.get(from_station): print('起始站点“%s”不存在。请正确输入！'%from_station) return None if not neighbor_info.get(to_station): print('目的站点“%s”不存在。请正确输入！'%to_station) return None # Calculate the straight-line distance from all nodes to the target node, spare distances = {} x,y = stations_info.get(to_station) for (k,v) in stations_info.items(): x0,y0 = stations_info.get(k) l = ((x-x0)**2 + (y-y0)**2)**0.5 distances[k] = l # Nodes that have been searched, dict # key=site name, value is the minimum cost from a known starting point to this site # 已搜索过的节点，dict searched = {} searched[from_station] = 0 # The data structure is pandas dataframe # index is the site name # g is the path taken, h is the heuristic function value (the current straight-line distance to the target) nodes = pd.DataFrame([[[from_station], 0, 0, distances.get(from_station)]], index=[from_station], columns=['path', 'cost', 'g', 'h']) count = 0 while True: if count &gt; 1000: break nodes.sort_values('cost', inplace=True) for index, node in nodes.iterrows(): count += 1 # Search for the site that is the shortest from the destination among the neighbors neighbors = neighbor_info.get(index).copy() if len(node['path']) &gt;= 2: # Do not search in the reverse direction of this path neighbors.remove(node['path'][-2]) for i in range(len(neighbors)): count += 1 neighbor = neighbors[i] g = node['g'] + get_distance(stations_info, index, neighbor) h = distances[neighbor] cost = g + h path = node['path'].copy() path.append(neighbor) if neighbor == to_station: # Find the goal, end print('共检索%d次。'%count) return path if neighbor in searched: if g &gt;= searched[neighbor]: # Explain that the search path is not optimal, ignore it continue else: searched[neighbor] = g # Modify the node information corresponding to this site# nodes.loc[neighbor, 'path'] = path # 这行总是报错# nodes.loc[neighbor, 'cost'] = cost# nodes.loc[neighbor, 'g'] = g# nodes.loc[neighbor, 'h'] = h # I don’t know how to modify the list element in df, I can only delete and add new rows nodes.drop(neighbor, axis=0, inplace=True) row = pd.DataFrame([[path, cost, g, h]], index=[neighbor], columns=['path', 'cost', 'g', 'h']) nodes = nodes.append(row) else: searched[neighbor] = g row = pd.DataFrame([[path, cost, g, h]], index=[neighbor], columns=['path', 'cost', 'g', 'h']) nodes = nodes.append(row) # All neighbors of this site have been searched, delete this node nodes.drop(index, axis=0, inplace=True) # The outer for loop only runs the first row of data, and then re-sort and then calculate continue print('未能找到路径') return Nonedef get_distance(stations_info, str1, str2): x1,y1 = stations_info.get(str1) x2,y2 = stations_info.get(str2) return ((x1-x2)**2 + (y1-y2)**2)** 0.5paths = get_path_Astar(lines_info, neighbor_info, stations_info, '回龙观', '西二旗')if paths: print(&quot;路径总计%d站。&quot;%(len(paths)-1)) print(&quot;-&quot;.join(paths))# Gaode Navigation is 31 stations, only 1 transfer# The code result is 28 stations, which is the same as the result with the number of subway stations as the cost, but the path is different (from the first traversal algorithm, you can see that there are 3 paths for 28 stations to reach the destination)# Guess Gaode's path cost is mainly time","link":"/Assignment/"},{"title":"Exploring the Potential and Challenges of Hybrid Machine Learning Systems in AI","text":"随着机器学习和深度学习的飞速发展，人工智能（AI）正取得飞跃性进展。然而，越来越多的研究者一致认为，AI演进的下一个阶段在于开发混合机器学习系统。这篇博客文章将探讨这个新兴领域，讨论它的潜力、挑战以及对AI未来的影响。 什么是混合机器学习系统？ 混合机器学习系统结合了两个或更多的机器学习模型或技术，创建出一个更强大、更灵活的AI解决方案。这些系统可以发挥每个组成模型的优势，同时弥补它们各自的弱点。组合机器学习模型的方法有多种，例如： 集成学习：将多个基础模型结合成一个更强大的模型。这可以通过bagging、boosting和stacking等技术来实现。 多模态学习：集成不同的数据来源（例如文本、图像和音频）以创建更丰富的数据表示，并提高整体性能。 迁移学习：利用从一个领域或任务中获得的知识，以改善另一个领域或任务中的性能。 元学习：训练模型学习如何学习，使它们能够更快地适应新任务。 混合系统的潜力 混合机器学习系统有潜力彻底改变AI，为解决复杂问题开辟新的可能性。这些系统的一些关键优势包括： 改善性能：通过结合多个模型，混合系统可以实现比任何单个模型更好的性能。对于过于复杂以至于单个模型无法有效解决的问题尤为如此。 鲁棒性：混合系统可以更好地抵御噪声、过拟合和其他影响单个模型的问题。这在现实世界中的应用中尤其重要，因为数据通常是嘈杂和不完美的。 通用性：混合系统可以处理各种问题、数据类型和任务，这使它们高度适应各种行业和应用。 可迁移性：混合系统可以更轻松地利用从一个领域或任务中获得的知识，以改善另一个领域或任务中的性能，这使它们非常适合具有有限训练数据的任务。 开发混合系统的挑战 尽管有潜力，混合机器学习系统也面临着一些挑战。其中最显著的障碍包括： 复杂性：设计和实施混合系统可能比单个模型更加复杂。研究人员和实践者需要仔细考虑如何最好地组合模型和技术，以创建一个有效的系统。 可伸缩性：混合系统的增加的复杂性可能使它们更难以扩展，无论是在计算资源方面还是处理大量数据的能力方面。 可解释性：混合系统可以更具挑战性地解释和说明，因为它们涉及多个模型和技术的交互。这可能使得理解系统如何做出决策并确保其正确运行变得更加困难。 训练和适应：与训练单个模型相比，训练混合系统可能需要更多的计算资源和时间。此外，将这些系统适应到新任务或不断变化的条件可能需要大量的工作。 总之，混合机器学习系统代表了AI未来的一个有希望的方向。通过利用多个模型和技术的优势，这些系统有潜力在性能、鲁棒性和通用性方面取得重大进展。然而，实现这个潜力需要克服与复杂性、可伸缩性、可解释性和训练相关的挑战。随着研究人员和实践者继续探索这个令人兴奋的领域，我们可以期待在各种行业和应用中看到混合机器学习系统的大量进展。","link":"/Exploring-the-Potential-and-Challenges-of-Hybrid-Machine-Learning-Systems-in-AI/"},{"title":"最佳 ChatGPT Chrome 扩展程序","text":"想要轻松访问ChatGPT吗？其中一个最佳方式是通过其一系列Chrome扩展程序。这些扩展程序还为您提供更好的使用ChatGPT的方法，包括帮助您编写更好的提示以获得更好的响应，或为ChatGPT授予搜索互联网的能力，从而提供对更多最新信息的访问。 这是您现在可以使用的最佳 ChatGPT Chrome 扩展程序。 Google 聊天 GPT 嫉妒 Bing Chat 及其在您搜索时与您聊天或使用最新的 GPT-4 语言模型的能力吗？不需要。只需从网上商店获取适用于 Google 的 ChatGPT，您就可以将 ChatGPT 与 Google 搜索一起使用。事实上，只需进行一次普通的 Google 搜索，在结果旁边，您也会收到来自 ChatGPT 的回复，这有时比 Google 结果本身更有用。 Merlin 想要 ChatGPT 在您上网的任何地方响应任何内容？Merlin 将 ChatGPT 带到任何网站，因此您可以突出显示任何文本或网页，并要求 ChatGPT 对其做出响应。您可以让它为您总结一个网页，或为您提供 YouTube 视频的纲要，这样您就不需要全部观看了。 TalkBerry 为什么要在 ChatGPT 上打字，而不是直接与 ChatGPT 对话呢？使用 TalkBerry，您可以简单地与 ChatGPT 通话。只需安装扩展程序并确保您的麦克风或耳机已插入，即可开始使用。使用 TalkBerry，您可以节省大量在输入上的时间，或者将 ChatGPT 用作语言导师，让它聆听并帮助您提高发音和语言理解能力。 TweetGPT 使用 TweetGPT 可以让您的社交媒体游戏更上一层楼。TweetGPT 是 ChatGPT 的插件，利用 AI 聊天机器人工具的强大功能，制作更有趣、更尖刻、更具吸引力或更友好的推文和回复。您可以选择要发布的主题、您的情绪基调和语言，ChatGPT 将完成剩下的工作。如果您对某些措辞不满意，您甚至可以在之后编辑该消息。 GPT-EZ 如果您不喜欢 ChatGPT 界面并想将其更改为您自己的喜好，请尝试 GPT-EZ。它允许您自定义 ChatGPT 网站的 UI，包括配色方案、字体样式和其他选项。此外，它还可以让您更轻松地复制和继续与 ChatGPT 的对话，并让您更轻松地下载对话日志。 SnackPrompt 通过使用一些评价最高的提示来充分利用 AI 聊天机器人。SnackPrompt 列出并排名全球其他聊天机器人用户的最佳提示，让您可以访问一些最新和最强大的 AI 功能。 WebChatGPT ChatGPT 的最大限制之一是它无法访问最近的信息。即使您使用的是最新的 GPT-4 语言模型，它仍然只能访问 2021 年之前的信息。使用 WebChatGPT，您可以让 ChatGPT 能够在网络上搜索更多最新的信息来源。从 Chrome 网上应用店获取扩展程序，在使用 ChatGPT 时只需将其打开即可享受这一方便的功能。 YouTube Summary 喜欢 YouTube 教程，但不想看完序言？让此 ChatGPT 扩展为您总结说明。只需从 YouTube 视频页面获取转录内容，然后将其输入到插件中，您就会立即获得摘要。它还适用于文章、电子邮件或科学论文。","link":"/Best-ChatGPT-Chrome-Extension/"},{"title":"如何在WordPress后台中加入表情符号","text":"本文来自zengrong 这应该是比较老的话题了，请参阅Smilies in WP (wp中的表情) WordPress自带了二十多个表情符号: 这些符号是使用代码插入的，要记住这些代码可不太容易，因此如果将这些表情符号嵌入到后台就方便了。 方法也并不麻烦，首先下载 WP-Grins这个插件，按正常方式安装。由于此插件并不是针对2.0开发，因此要先修改一下。找到wp-grins.php中的下面这句（大约在34行） \\(grins .= '&amp;lt;img src=&amp;quot;'.get_settings('siteurl').'/wp-images/smilies/'.\\)grin.'\" alt=\"'.\\(tag.'&amp;quot; onclick=&amp;quot;grin(\\''.\\)tag.'');\"/&gt; '; 将其中的“/wp-images/smilies/” 修改为“/wp-includes/images/smilies/”。 安装后在插件管理器中激活它，然后编辑“/wp-admin/admin-functions.php”文件，搜索“ edToolbar();”，大约在1079行，找到下面这句 if (!strstr($_SERVER['HTTP_USER_AGENT'], 'Safari')) echo ' &lt;div id=\"quicktags\"&gt; &lt;script src=\"../wp-includes/js/quicktags.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt;if ( typeof tinyMCE == \"undefined\" || tinyMCE.configs.length &lt; 1 ) edToolbar();&lt;/script&gt; &lt;/div&gt; '; 再上面这句之下加入： if (function_exists(\"wp_grins\")) { echo \"\"; wp_grins(); } //zrong added 修改完毕上传，打开后台撰写文章即可看到，可爱的表情已经加入到编辑器中了。 如果要将这些表情也加入到评论页面中，可以在主模版的comments.php中搜索如下语句（可能不完全相同）： &lt;p&gt; &lt;textarea class=\"textform\" name=\"comment\" id=\"comment\" cols=\"100%\"rows=\"10\" tabindex=\"4\"&gt;&lt;/textarea&gt; &lt;/p&gt; 在其上加入如下PHP语句即可： &lt;?php wp_grins(); ?&gt; 完毕。记下来也方便自己查找。 顺便： 如果想在页面中加入表情，还可以借助Emotions这个插件。","link":"/How-to-add-emoticons-in-wordpress-background/"},{"title":"系列课程：从零开始接触人工智能大模型（介绍）","text":"整个系列课程内容虽然为自己所写，但是参考了bothub 创始人徐文浩的课程《AI 大模型之美》 人工智能是计算机科学领域中最具前瞻性和影响力的技术之一。它是一种智慧型算法，能够模拟人类的思维过程，处理大量的数据和信息，从而发现隐藏在其中的规律和趋势。人工智能的应用范围非常广泛，包括语音识别、图像识别、自然语言处理、机器翻译、智能推荐、智能问答、自动驾驶等等。 人工智能的发展历程可以追溯到上个世纪五六十年代。当时，计算机科学家们开始思考如何让计算机能够像人类一样思考和行动。1956年，美国达特茅斯学院举办了一次名为“人工智能”的会议，正式提出了人工智能的概念。自此以后，人工智能的研究和应用就成为了计算机科学领域的一项重要任务。 随着科技的不断进步，人工智能技术得到了长足的发展。各种机器学习算法、深度学习算法、开源的软件包以及云平台提供的解决方案不断涌现，为企业提供了各种智能化的产品和服务。例如，通过图像识别技术，我们可以将人脸识别、车牌识别、病变识别等技术应用于安防、交通、医疗等领域；通过自然语言处理技术，我们可以实现智能客服、智能翻译、智能问答等功能，提升用户体验和效率；通过机器学习技术，我们可以实现推荐系统、广告精准投放等功能，提高市场竞争力。 然而，人工智能领域仍然存在着挑战和困难。其中一个难点就是“有多少人工就有多少智能”这个诅咒。很多“智能”都来自于大量的人工数据标注和硬编码的业务规则，导致人工智能在某些特殊情况下表现得像“人工智障”。因此，如何提高人工智能的效率和性能，仍然是人工智能领域需要解决的问题之一。 去年 12 月，我第一次尝试与 ChatGPT 进行交互。一开始我并没有对这个新的 AI 聊天机器人抱有太高的期待，毕竟以前的聊天机器人总是表现得像“人工智障”。但是，ChatGPT 证明了我的想法是错误的。在与它交流了几分钟后，我决定让它帮我写一些 Bash 脚本和 SQL 代码。我很惊讶地发现，它不仅完全理解了我的需求，还精确地写出了我需要的复杂 SQL 代码。这次体验让我对人工智能的潜力有了新的认识，也让我更加期待未来人工智能的发展。 对于复杂的需要窗口函数的 SQL，ChatGPT 写得比我快多了。 从年前到目前（5月份）为止，我一直在体验市面上大部分的人工智能工具，例如最近非常火热的ChatGPT等。这样的体验让我对人工智能的潜力有了新的认识，也让我开始思考各行业未来的前景，并提出了一些担忧。当我们看到人工智能技术不断取得突破性进展的同时，也会担心它是否会取代人类的工作，进而给社会带来不稳定的因素。然而，我相信人工智能的发展是为了更好地服务人类，而非取代人类。我们需要更多人去了解和掌握人工智能技术，这样才能更好地应对未来的挑战，发挥人工智能技术的最大价值。 基于此，我想让更多人开始接触人工智能，并且学会如何利用人工智能，更甚为学习新一代AI应用编程。因此，我想介绍一门系列课程：从零开始接触人工智能大模型。该课程将介绍人工智能的基本概念、常见应用场景以及如何利用最新的AI技术构建自己的AI应用。我们将从浅入深地讲解人工智能相关的知识，帮助每个人都能够轻松上手，并且学会如何应用到自己的领域中去。不仅仅是算法工程师和机器学习研究人员，每个工程师都可以快速学习并参与开发新一代的AI应用。我相信，学会开发新一代AI应用是每个软件开发行业从业者都值得学习的，无论是产品经理还是工程师，乃至于行业之外的业务人员，都应该拥抱新的AI浪潮。 在学习的过程中，我们将讨论人工智能的应用场景，例如语音识别、图像识别、自然语言处理、机器翻译、智能推荐、智能问答、自动驾驶等等。这将有助于我们了解人工智能技术在不同领域的应用，从而更好地把握未来的发展方向。同时，我们也会学习最新的人工智能技术，例如大模型、自监督学习等等。这些技术的出现，为人工智能的应用提供了更加广阔的空间和深度。 在这个充满挑战和机遇的时代，我们需要准备好迎接未来的挑战。学习人工智能技术，是每个软件开发行业从业者都需要掌握的技能。无论是产品经理还是工程师，乃至于行业之外的业务人员，都应该拥抱新的AI浪潮，学习开发新一代的AI应用。我相信，通过学习新一代的AI应用编程，我们能够更好地应对未来的挑战，为我们的生活和工作带来更多的便利和机遇。 学习成本那么高，给我一个理由先 这个应该是普遍的一个想法，其实对于此，我将不仅给你一个理由，而是给你三个： 1. 开发门槛降低，人人可学习AI应用开发 人人都应该学习如何开发新一代 AI 应用，因为这一轮的 AI 浪潮里，开发新的 AI 应用的门槛大大降低了。过去，AI 应用开发是一个门槛比较高的领域，需要掌握大量的机器学习和深度学习的知识，了解各种基础模型，使用各种机器学习的编程框架，以及理解在实际应用里锤炼机器学习的各种实战技巧和模型。对于没有相关经验的人来说，不花上个一两年时间，你可能很难用 AI 算法做出有价值的产品。 但是现在，随着预训练好的大型基础模型的出现，以及这些模型的能力通过开放 API 的形式提供出来，即使没有任何机器学习的理论知识，你只需要一两天时间，就能做出一个能解决实际问题的 AI 应用。比如，最近在 GitHub 上就能看到很多工程师，花上 1-2 天时间就做出来的图书翻译、人工智能语音对话的应用。 这样的开发方式，让更多的人有机会参与到 AI 应用的开发中来。无论你是产品经理、UI/UE 设计师、前端开发、后端开发还是大数据团队的人员，都可以通过学习一些基本的 AI 应用开发技能，为自己的职业生涯增添新的技能和竞争力。特别是在当前的科技革命和数字化转型浪潮下，AI 技术已经逐渐渗透到各个行业中，很多企业已经开始了 AI 落地实践，而能够掌握 AI 技术的人才也逐渐成为各个行业中的稀缺资源。因此，学习如何开发新一代 AI 应用，也是提升自己职业竞争力的一种重要途径。 最后，学习如何开发新一代 AI 应用还可以让我们更好地了解 AI 技术的本质和应用，拓宽我们的知识面和视野。AI 技术正在改变我们的生活和工作方式，了解和掌握这些技术，也有助于我们更好地适应未来的发展和变化。 学习如何开发新一代 AI 应用对于个人的职业发展和未来非常重要，因为 AI 技术已经开始在各个行业得到广泛应用。无论你从事什么行业，都可以利用 AI 技术来提高效率、降低成本、提供更好的服务，并在竞争中脱颖而出。掌握 AI 技术也可以让你在未来的就业市场上更有竞争力，拥有更广泛的职业选择。因此，学习如何开发新一代 AI 应用可以为个人的职业发展和未来打下坚实的基础。 2. 站在巨人的肩膀上 随着人工智能技术的迅猛发展，AI应用开发的范围也越来越广泛，涉及到自然语言处理、计算机视觉、语音识别等多个领域。这一轮的AI浪潮已经开始让我们看到了通用人工智能（AGI）的雏形，AI应用的覆盖领域被大大扩展了，几乎任何一个问题都有机会通过AI来解决优化。 过去，机器学习模型的应用通常局限于某一个细分领域上的进步，而且对于每一个具体问题都要单独收集数据、训练单独的机器学习模型来解决里面某一个小问题。然而，随着计算能力的提高和模型规模的增加，现在拥有海量参数的大模型已经开始成为主流。例如，2020年发布的GPT-3模型拥有1750亿个参数，可以无需任何微调，就能解决情感分析、命名实体识别、机器翻译等一系列的自然语言处理问题。同时，对于很多AI没有见过的问题，只要通过自然语言给到AI几个例子，通过\"小样本学习\"，AI就能给出正确的回答。这意味着，一个大模型可以一次性解决不同类型的很多问题。 在计算机视觉上，像2021年OpenAI发布的CLIP模型也有类似的效果。通过4亿个（图片、文本）对的训练，对于图片的分类可以任意扩展，而不需要预先标注。这样的模型使得我们对于图片的分类不再局限于预先的人工数据标注的类别，而是可以扩展到任何类别上去。 这种发展趋势使得AI应用开发的门槛逐渐降低，使得普通人也能够参与到AI应用的开发中来。无论你所在的行业和领域，都有机会通过简单的AI应用开发，提升效率和产出。同时，了解和掌握AI技术也成为了一种职业竞争力，可以帮助你更好地适应未来的工作环境。 总之，AI技术的广泛应用和快速发展已经让AI应用开发成为一个非常重要的技能。了解AI技术的人，无论是在工作中还是在日常生活中，都会受益匪浅。 3. 人工智能对我们的工作的影响 人工智能（AI）已经开始以多种方式改变我们的生活。我们已经习惯了依赖AI进行日常任务，如编写代码、翻译文本，甚至为文章生成图像。然而，AI的影响超出了我们的个人生活，它将对我们的工作产生重大影响。 随着AI的不断发展，它不可避免地将接管许多以前由人类执行的任务。公司已经在使用AI来优化产品描述、搜索算法和其他曾经是人工工作者领域的任务。这无疑会导致工作的流失，并改变许多人的工作性质。 尽管存在工作流失的可能性，但那些拥抱AI的人无疑将会获得好处。使用AI的团队和公司很可能会看到更高的效率和生产力，从而导致更大的产出和成功。无论您是产品经理、工程师、运营专家还是平面设计师，AI的出现都将从根本上改变您的工作性质。 AI将作为助手，帮助我们完成简单的基于知识的任务，甚至提供创造性的灵感。事实上，有些人将AI的发展与工业革命相比，标志着我们生活和工作方式的根本变化。虽然这可能会对一些工人造成危机，但有机会拥抱这种变化并学习在以AI为驱动的未来需求的新技能。 正如2008年App Store的发布创造了对移动应用程序开发人员的需求一样，AI革命为那些愿意学习和适应的人带来了新的机会。无论是获得机器学习、数据科学还是其他与AI相关的领域的专业知识，那些积极应对不断变化的就业市场的人无疑会蓬勃发展。 AI对我们的工作的影响不容小觑。尽管有些人可能将其视为危机，但有机会拥抱这种变化并学习在以AI为驱动的未来需求的新技能。每个人都可以决定如何应对这种变化，以及他们是否会抓住它带来的机会。 如何学习呢？ 新一代AI应用开发是一个快速发展的领域，需要不断更新自己的知识和技能。而通过实践学习是最有效的方法之一，因为它可以让你在实际解决问题的过程中学习和掌握技能。 这门课程采用实践教学的方式，让学生能够亲自动手解决一系列实际问题，如情感分析、记忆聊天机器人、图像搜索等。学生们将通过编写几行或几十行的代码来解决这些问题，并在在线Notebook的环境下进行代码运行，无需搭建复杂的开发环境。即使你是产品经理或业务方，也可以轻松地体验到新一代AI应用的开发过程，从而更好地理解和掌握其工作原理。 除了OpenAI的API外，这门课程还涵盖了语音、视觉等应用场景，包括语音识别、语音合成、AI绘画等。学生们将了解到如何使用开源模型以及如何根据自己的数据微调这些模型，从而更好地满足不同场景下的需求。 此外，这门课程还将探讨AI应用的套路和方法，例如分类、搜索、推荐、问答等问题。学生们将学习如何使用现有模型的能力来解决这些问题，并将这些方法和套路应用到现有的业务系统中，以提高应用的体验和效率。 随着课程的深入，学生们还将学习如何组合多个API、开源模型和开源库来解决复杂的真实问题。例如，如果你想实现一个电商客服，不仅需要检索知识库和问答的能力，还需要连接现有的订单和物流信息的能力。学生们将学习如何在AI应用的开发过程中将复杂的业务流程串联起来，以更好地应对实际问题。 拥抱新时代，接受“通用人工智能” 随着人工智能技术的不断进步，越来越多的人开始认识到“通用人工智能”（AGI）的重要性和可能性。如今，AGI已经不再是一个遥不可及的概念，而是一个即将到来的现实。我们可以看到，各个领域的科学家和工程师正在努力推进AGI的研究和应用，希望通过人工智能技术的创新和发展来实现这一目标。 在这个变化迅速的时代，我们需要尽快拥抱AGI。AGI可以帮助我们解决许多复杂的问题，并且能够极大地改善我们的生活质量。例如，我们可以使用AGI来开发更为智能的医疗设备，提高医疗诊断的准确性和效率；我们也可以利用AGI来优化城市交通，减少交通堵塞和污染；此外，AGI还能为我们提供更好的教育和娱乐体验，让我们的生活更加丰富多彩。 因此，我们需要尽快投入时间和精力来学习和应用AGI技术。学习AGI不仅可以让我们掌握更加先进的技能和知识，还可以激发我们的创新和热情，让我们更好地适应这个变化迅速的时代。我们需要通过学习和实践，尽快将AGI技术应用到我们的工作和生活中，让其发挥最大的价值。 最后，我希望每一个人都能积极地拥抱AGI技术，努力学习和应用这项技术，为推动人工智能技术的发展贡献自己的力量。相信在不久的将来，AGI将成为我们生活中不可或缺的一部分，让我们共同期待并努力实现这一目标。 目录 注： 这里将会是我未来所有系列教程的目录，便于大家更快找到相关章节 导读：了解AI并使用它/他/她们","link":"/Gettin_started_with_large-scale_artificial_intelligence_models_from_scratch/"},{"title":"在 Apple Silicon M1&#x2F;M2 Mac 上安装和运行Stable Diffusion","text":"说实话，我找了好多关于如何在 M1/M2 上安装和运行 Stable Diffusion 的教程和帖子，发现相互之间借鉴的不少，但是能用的确实没几个。 寻找一番后，发现其实没那么复杂。也不知道为什么网上的那么多教程搞得那么复杂，又是这个又是那个的一大堆，简单实现的方式有好几种： 1. Diffuers 这是可以在 App Store 上直接搜索并下载的一个 App，看评分和排名似乎都不太好，开发者却是「Hugging Face」，其实在官方的 Github 上就有其下载链接：‣ notion image 这应该是体验 Stable Diffusion最简便的方式了吧。而且还支持选择 Model， notion image 不过有点遗憾的点是没办法调整参数。 notion image 2. DiffusionBee 这是出现的比较早的一款第三方 App，使用起来也是特别简单，直接下载安装就行了： DiffusionBee - Stable Diffusion App for AI Art DiffusionBee is the easiest way to generate AI art on your computer with Stable Diffusion. Completely free of charge. DiffusionBee - Stable Diffusion App for AI Art https://diffusionbee.com/download DiffusionBee - Stable Diffusion App for AI Art 目前不止是 MacOS，还有对应 Windows 64 Bit 的版本，而且，你可以选择下载 HQ Version 版本。官方对其说的是速度慢两倍，但是图像质量更好。 notion image 以上两个 App 第一次使用的时候都是需要下载 Model 的，之后就可以直接开心的玩耍了，相比较而言，DiffusionBee 在参数选择上要多一点。支持 Text to Image, Image To Image等。 notion image # 安装 AUTOMATIC1111 这个方法是需要有一点动手能力了，不过相比较而言，也不是那些网站上介绍的那么繁琐。其实就只需要几步而已。 1. 下载 Homebrew 一个包管理器，不太明白的朋友不需要管那些，操作就行了 打开你的终端，不明白什么是终端直接在你的搜索框里输入”终端”，或者”Terminal”, 就能看到了。 notion image 然后直接把下面的代码粘贴进去，回车，看着他跑就行了 1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; 2. 安装一些必须的软件包 等上面步骤跑完之后，再复制下面的代码，一样粘贴进去回车看着他跑： 1brew install cmake protobuf rust python@3.10 git wget notion image 3. 下载 AUTOMATIC1111 存储库 等上面步骤跑完，在你的终端输入一下代码并回车 1cd ~ 以上命令是为了让你进入你在 Mac 电脑上的账户主目录，就是这个地址 1/User/xx/ 然后我们接下来的操作会在你这个目录下下载一个文件夹，名字叫 「stable-diffusion-webui」，这贴下面代码到你的终端里，然后回车 1git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui 命令跑完后，你就会渐渐自己名字的目录下多了一个 stable-diffusion-webui 的目录了，然后在终端进入这个目录，操作方法和上面一样 1cd ~/stable-diffusion-webui/ notion image 在你的访达里你也进入这个目录，然后继续进入 models/Stable-diffusion， 这里是存放 Model 的地方。现在你需要下载一个 Model 存放进去，你可以直接在这里下载 1.5 model, 当然，如果你需要 2.1 的或者其他 model，可以去点击下面的链接进去自己下载一个合适的，然后扔到目录里。 Models - Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. Models - Hugging Face https://huggingface.co/models?sort=downloads&amp;search=stable+diffusion Models - Hugging Face 然后你的目录应该会是这样： notion image 然后你就可以尝试着跑你的 stable diffusion 了，刚才我们在终端里进入了 ~/stable-diffusion-webui/， 假设你还在这个位置，我们就可以直接输入： 1./webui.sh notion image 然后去干些自己的事情吧，喝杯茶，看看书。要跑一会呢，特别是你网络不好的情况。 等到终端命令全部跑完后，打开你的 Safari，输入：http://127.0.0.1:7860/ 好了，可以把玩了。 notion image 4. 其他 是的，还没结束，还有一些要说的，其实在 Mac App Store 里搜索的话，你还能看到一些其他的 App 可以直接使用，比如： notion image 反正都是免费的，尽量多试试，找到一个自己满意的。 另外，不管你用那种方法，你都需要知道一些good prompts for Stable Diffusion, 这里有一个地方可以看些别人的例子，不过不是那么容易打开： arthub.ai arthub.ai https://arthub.ai/ 还有啊，自己可以多测试一些 model，下下来把玩下。 祝你玩的愉快。全文完。","link":"/How-to-install-and-run-Stable-Diffusion-on-Apple-Silicon/"},{"title":"Knock 升级 -- 快速输入管理员密码","text":"本文知乎专栏 还记得之前介绍过的Knock么？只需要敲击两下就能快速解锁Mac的app。 其实这款App还是 @Rachel 介绍给我的，当时就觉得很酷，可是用下来之后，并没有觉得有很高的实用性。 不过，这次Knock升级了，除了解锁Mac之外，还可以再你需要root权限的时候免去输入管理员密码的麻烦，你所需要的，仅仅是敲击两下你的iPhone。 好吧，现在可以为自己的Mac设定一个超级复杂的密码了。","link":"/Knock-update/"},{"title":"09 使用Embedding实现语义检索","text":"Hi，我是茶桁。 过去的8讲，你已熟悉Embedding和Completion接口。Embedding适合用于机器学习中的分类、聚类等传统场景。Completion接口可以用作聊天机器人，也可以用于文案撰写、文本摘要、机器翻译等工作。 然而，很多同学可能认为这与他们的日常工作无关。实际上，我们通常在搜索、广告、推荐等业务中使用自然语言处理技术。因此，我们今天来看看如何使用OpenAI的接口来帮助这些需求。 当涉及到优化搜索结果时，OpenAI的Embedding接口可以提供有价值的功能。Embedding接口能够将文本转换为表示其语义特征的向量，这些向量可以用于比较文本之间的相似性，从而优化搜索结果的排序和相关性。 首先，使用OpenAI的嵌入接口，您可以将搜索查询和搜索结果中的文本转换为嵌入向量。通过比较查询向量与结果向量之间的相似度，您可以重新排列搜索结果，以提供更相关和有用的结果。这可以帮助用户更快地找到他们想要的信息，并提供更好的搜索体验。 其次，OpenAI的嵌入接口还可以帮助您改进搜索结果的相关性。通过将用户的上下文和历史记录与搜索查询结合起来，您可以生成更具个性化和定制化的搜索结果。使用嵌入接口，您可以将用户的上下文信息转换为嵌入向量，并与查询向量进行比较，以确定最相关的结果，并在搜索结果中突出显示这些个性化的内容。 此外，OpenAI的嵌入接口还可以用于相似性搜索和聚类分析。您可以使用嵌入向量来比较不同文本之间的相似性，并将相似的文本聚集在一起。这有助于在搜索结果中提供更多相关的选项，并帮助用户发现相关但可能未被明确搜索的内容。 下面，就让我们来一步步的实现： 生成实验数据 在演示代码之前，我们需要实验数据。我们通常要在网上找数据集或使用软件包的数据集。但并非总是能找到合适的数据集。这时，我们可以借助 AI 生成数据。 12345678910111213141516171819202122import openai, osopenai.api_key = &quot;YOUR_API_KEY&quot;COMPLETION_MODEL = 'text-davinci-003'# 定义方法def generate_data_by_prompt(prompt): response = openai.Completion.create( engine = COMPLETION_MODEL, prompt = prompt, temperature = 0.5, max_tokens = 2048, # 根据需要设置生成的标题长度 top_p = 1, stop=None, # 可选，用于指定生成标题的终止标记 ) return response.choices[0].text# 定义提示词prompt = &quot;请你生成50条亚马逊里的商品的标题，每条在30个字左右，品类是心理类的书籍，标题里往往也会有一些促销类的信息，每行一条。&quot;&quot;&quot;# 执行定义类, data接收原始数据data = generate_data_by_prompt(prompt) 为了更贴近实际情况，我们可以精心设计提示语。例如，明确商品为亚马逊产品，品类为心理书籍，并在标题中包含促销信息。将返回结果按行分割并加载到DataFrame中，以查看结果。 1234import pandas as pdproduct_names = data.strip().split('\\n')df = pd.DataFrame({'product_name': product_names})df.head() 数据结果： 123456 product_name0 1. 【特价】心理学家的智慧：拥抱接受自己1 2. 【免费配送】心理学的本质：探索你的内在2 3. 【限时特价】心理学的洞察力：提升你的自信3 4. 【2件8折】心理学的力量：走出你的舒适区4 5. 【全场满减】心理学的技巧：让你的思维更灵活 AI 为我们生成了 50 条商品信息，每一条都带有促销相关的标签。我们需要去掉每一行的标号，以得到干净的数据。 12df.product_name = df.product_name.apply(lambda x: x.split('.')[1].strip())df.head() 输出结果： 123456 product_name0 【特价】心理学家的智慧：拥抱接受自己1 【免费配送】心理学的本质：探索你的内在2 【限时特价】心理学的洞察力：提升你的自信3 【2件8折】心理学的力量：走出你的舒适区4 【全场满减】心理学的技巧：让你的思维更灵活 我们可以生成一些3C电子商品，涵盖不同的品类，以便在后面展示搜索效果时更加方便。 123456789electronic_prompt = &quot;&quot;&quot;请你生成50条亚马逊里的商品的标题，每条在30个字左右，品类是3C类电子商品，标题里往往也会有一些促销类的信息，每行一条。&quot;&quot;&quot;electronic_data = generate_data_by_prompt(electronic_prompt)electronic_product_names = electronic_data.strip().split('\\n')electronic_df = pd.DataFrame({'product_name': electronic_product_names})electronic_df.product_name = electronic_df.product_name.apply(lambda x: x.split('.')[1].strip())electronic_df.head() 这一步我们将获取数据和处理写在一起执行 输出结果： 123456 product_name0 Apple MacBook Pro 16&quot; 苹果笔记本电脑，限时优惠！1 新款 iPad Pro 11&quot; 平板电脑，4G/WiFi，超薄设计！2 无线鼠标，蓝牙连接，超长续航，低至半价！3 三星 Galaxy S20 5G 智能手机，性能升级，极速体验！4 小米 Redmi Note 8 Pro 智能手机，4800万超清拍照！ 拼接这两个 DataFrame 后，即可用于搜索实验的数据。 123df = pd.concat([df, electronic_df], axis = 0)df = df.reset_index(drop=True)display(df) 输出结果： 12345678910111213 product_name0 【特价】心理学家的智慧：拥抱接受自己1 【免费配送】心理学的本质：探索你的内在2 【限时特价】心理学的洞察力：提升你的自信3 【2件8折】心理学的力量：走出你的舒适区4 【全场满减】心理学的技巧：让你的思维更灵活... ...73 小米米家电动滑板车，超长续航，超薄设计！74 小米米家电动折叠车，智能控制，超薄设计，低至半价！75 小米米家空气净化器Pro，智能检测，超长续航！76 小米米家净水器，三级净水，智能检测，限时优惠！77 三星 Galaxy S10 智能手机，双摄像头，性能升级！78 rows × 1 columns 合并后的数据量不到100条，说明AI返回的条数不到50条。这并不影响我们使用这个数据源。如果需要，可以再次获取数据并进行合并。 通过 Embedding 进行语义搜索 对于搜索问题，我们可以利用 GPT 模型。大公司如百度、阿里有许多内部复杂的策略和模型。但是许多中小型公司，尤其是刚开始提供搜索功能的公司，则通常使用 Elasticsearch 这个开源项目。Elasticsearch 背后的搜索原理是先分词，然后使用倒排索引。 简单来说，搜索引擎将商品名称（例如“心理学的洞察力：提升你的自信”）拆分成多个单词（如“心理学”、“洞察力”、“提升”、“自信”）。每个标题都是这样切分。然后，建立一个索引，比如“洞察力”这个词，出现过的标题的编号，都按编号顺序跟在“心理”后面。其他的词也是类似。 当用户搜索时，搜索引擎将用户输入的关键词（例如“思维的力量”）拆分为多个单词（如“思维”和“力量”）。然后，搜索引擎查找包含这些单词的标题，并根据出现的单词的数量、权重等找出相关商品。 但是，此策略有缺点。如果有同义词，这么简单地搜索是搜不到的。例如，如果搜索“心理学智慧”，虽然语义上很接近，但是因为“心理”、“智慧”这两个词在标题里都没有出现，所以就无法匹配。为了提升搜索效果，需要做更多的工程研发工作，例如找一个同义词表，将标题中出现的同义词也算上等等。 然而，使用 OpenAI 的 Embedding 接口，可以将一段文本的语义表示为一个向量。向量之间是可以计算距离的，这是在之前的情感分析的零样本分类中演示过的。因此，我们可以将用户的搜索通过 Embedding 接口变为向量。然后，计算它和所有商品标题的余弦距离，找出离我们搜索词最近的几个向量。这些向量实际上是与该商品类似的语义，不一定需要相同的关键词。 根据这个思路，我们可以使用代码来尝试一下。首先，我们需要计算所有商品标题的 Embedding，并将其存储下来。这里的代码与之前使用 Embedding 进行分类和聚类的代码基本相同，不再详细解释。我们使用 backoff 和 batch 处理，以便代码容错并快速处理这些商品标题。 123456789101112131415161718192021222324from openai.embeddings_utils import get_embeddingsimport openai, os, backoffembedding_model = 'text-embedding-ada-002'batch_size = 100@backoff.on_exception(backoff.expo, openai.error.RateLimitError)def get_embeddings_with_backoff(prompts, engine): embeddings = [] for i in range(0, len(prompts), batch_size): batch = prompts[i:i+batch_size] embeddings += get_embeddings(list_of_text = batch, engine=engine) return embeddingsprompts = df.product_name.tolist()prompt_batches = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]embeddings = []for batch in prompt_batches: batch_embeddings = get_embeddings_with_backoff(prompts = batch, engine = embedding_model) embeddings += batch_embeddingsdf['embedding'] = embeddingsdf.to_parquet('./data/taobao_product_title.parquet', index = False) 为了定义一个 search_product 的搜索函数，我们可以接受三个参数：一个名为 df 的数据源，一个名为 query 的搜索词，以及一个名为 n 的参数，表示要返回多少条搜索结果。该函数将执行以下三项操作： 使用 OpenAI API 将搜索词转换为 Embedding。 计算该 Embedding 与 DataFrame 中每个 Embedding 的余弦距离。 根据余弦相似度对搜索结果进行排序，并返回与搜索词最相似的 n 个标题。 123456789101112131415161718192021from openai.embeddings_utils import get_embedding, cosine_similarity# search through the reviews for a specific productdef search_product(df, query, n = 3, pprint = True): product_embedding = get_embedding( query, engine = embedding_model ) df[&quot;similarity&quot;] = df.embedding.apply(lambda x: cosine_similarity(x, product_embedding)) results = ( df.sort_values(&quot;similarity&quot;, ascending=False) .head(n) .product_name ) if pprint: for r in results: print(r) return resultsresults = search_product(df, &quot;心理学智慧&quot;, n = 3) 我们就拿刚才举的那个例子，使用\"心理学智慧\"作为搜索词，调用这个 search_product 函数，然后拿前 3 个返回结果。可以看到，尽管在关键词上完全不同，但是返回的结果里，的确包含了\"心理学的洞察力：提升你的自信\"这个商品。 输出结果： 123【新品特惠】心理学的智慧：改变你的生活【新品特惠】心理学的洞察力：提升你的自信【满减特惠】心理学的智慧：让你的生活更完美 请注意，由于我们的商品标题是随机生成的，因此您得到的数据集和搜索结果可能与我不同。请根据实际情况测试您想要的搜索词。 利用 Embedding 进行商品推荐的冷启动 Embedding 向量距离不仅可以用于搜索，还可以用于商品推荐中的冷启动。主流的推荐算法主要依托于用户的“看了又看”等行为信息。即如果有很多用户看了 OPPO 手机，并转而看了 vivo 手机，那么在用户看 OPPO 手机时，我们就可以向他推荐 vivo 手机。但往往新商品或新平台缺乏相关的行为数据。此时，我们同样可以根据商品名称在语义上的相似度来进行商品推荐。 我们的代码实现和上面的搜索例子基本一致，唯一的差别是商品名称的 Embedding 是从 DataFrame 中获取，而不是通过调用 OpenAI 的 Embedding API 获取，因为我们已经计算过一遍嵌入，无需再次请求。 123456789101112131415def recommend_product(df, product_name, n=3, pprint=True): product_embedding = df[df['product_name'] == product_name].iloc[0].embedding df[&quot;similarity&quot;] = df.embedding.apply(lambda x: cosine_similarity(x, product_embedding)) results = ( df.sort_values(&quot;similarity&quot;, ascending=False) .head(n) .product_name ) if pprint: for r in results: print(r) return resultsresults = recommend_product(df, &quot;【全场满减】心理学的技巧：让你的思维更灵活&quot;, n=3) 输出结果： 123【全场满减】心理学的技巧：让你的思维更灵活【全场满减】心理学的奥秘：激发你的潜能【全场满减】心理学的技巧：改变你的态度 利用 Faiss 加速搜索 上面的示例代码还存在一个问题：每次搜索或推荐时，我们都需要计算输入嵌入和所有数据嵌入之间的余弦相似度。在上面的例子中，我们只检索了100条数据，但在实际应用中，即使不像百度或谷歌这样的搜索引擎，检索的内容数也可能达到几百万或上千万。如果每次搜索都要计算几百万次余弦距离，速度肯定会很慢。 为了解决这个问题，我们可以使用一些向量数据库或软件包，它们能够快速搜索相似性。例如，我推荐使用 Facebook 开源的 Faiss Python 包，它的全称是 Facebook AI Similarity Search，可以快速进行高维向量的相似性搜索。 我们可以将 DataFrame 中的嵌入向量加载到 Faiss 索引中，然后让 Faiss 帮助我们快速找到最相似的向量。下面我们来看看效果。 当然，按照惯例，我们需要先安装 Faiss 这个 Python 库。 1conda install -c conda-forge faiss-cpu faiss有多个版本，包括cpu, gpu等等，你可以到这里看看官方文档，选择适合自己的版本安装。因为我是M1， 所以选择了cpu版本。 把整个 Embedding 变成一个二维矩阵，然后直接将其加载到 Faiss 索引中即可。在之前，我们需要定义好 Faiss 索引的维度数，与我们的 Embedding 向量的维度数相同。 12345678910import faissimport numpy as npdef load_embeddings_to_faiss(df): embeddings = np.array(df['embedding'].tolist()).astype('float32') index = faiss.IndexFlatL2(embeddings.shape[1]) index.add(embeddings) return indexindex = load_embeddings_to_faiss(df) 搜索 Faiss 简单易用。我们将查询转换成嵌入形式，再将其转换为 numpy 数组向量。然后，我们只需对刚才生成的索引 index 调用 search 方法，并指定返回的结果数量。返回的仅是索引的 index，即加载在 Faiss 中的第几个索引。根据这个索引，在 DataFrame 中查找对应的行数和商品标题即可获得搜索结果。 123456789101112131415def search_index(index, df, query, k=5): query_vector = np.array(get_embedding(query, engine=embedding_model)).reshape(1, -1).astype('float32') distances, indexes = index.search(query_vector, k) results = [] for i in range(len(indexes)): product_names = df.iloc[indexes[i]]['product_name'].values.tolist() results.append((distances[i], product_names)) return resultsproducts = search_index(index, df, &quot;心理学智慧&quot;, k=3)for distances, product_names in products: for i in range(len(distances)): print(product_names[i], distances[i]) 输出结果： 123【新品特惠】心理学的智慧：改变你的生活 0.20244475【新品特惠】心理学的洞察力：提升你的自信 0.21296506【满减特惠】心理学的智慧：让你的生活更完美 0.21423605 用\"自然淡雅背包\"这个关键词搜索，结果与我们之前计算的余弦距离排序的结果相同。 Faiss 通过 ANN 近似最近邻算法实现相似性搜索。如需了解 Faiss 的原理，请向 ChatGPT 提问。 Faiss库能够加载的数据量受限于内存大小。如果数据量继续增长，需要使用向量数据库进行搜索。例如，OpenAI推荐的Pinecone和Weaviate，也有许多团队使用国人开源产品Milvus。虽然使用Embedding的相似度可以快速启动搜索和推荐，但需要更复杂的策略才能实现更好的效果。例如，根据用户反馈的行为更好地排序搜索和推荐结果。但对于提供简单的搜索或推荐功能而言，文本的嵌入相似度是很好的快速启动方式。 小结 在这一讲中，我将教给你三个主要的技巧，以便更快速地优化现有业务中的推荐和搜索功能。 首先，即使没有适当的测试数据，我们也可以让AI生成一些数据。这种方法不仅可以节省在线查找数据的时间，还可以根据自己的需求生成具有特定特征的数据。例如，我们可以要求在商品标题中添加一些促销相关信息。 其次，我们可以利用嵌入之间的余弦相似度作为语义相似度，来优化搜索。通过嵌入的相似性，我们不要求搜索词和查询内容之间完全匹配，只要它们的语义信息接近即可。 最后，我们将学习如何使用Faiss这样的Python库或其他向量数据库，以快速检索向量。这样，我们就不必每次搜索都计算整个数据库的余弦相似度。 通过计算嵌入向量并对其进行索引，我们可以将外部知识和信息引入到使用GPT模型的应用程序中。在以后的课程中，我们将学习如何利用这些外部知识开发更复杂的AI应用程序。 课后练习 搜索中经常会遇到同一关键字具有多重含义的情况。例如，当我们搜索“小米手机”时，结果中应该包括“荣耀V30 Pro”，但不应包括“黑龙江优质小米”。你可以尝试使用嵌入进行语义搜索，看看是否仍然会出现这种问题。","link":"/Implementing-semantic-retrieval-using-Embedding/"},{"title":"Foundation of Artificial Intelligence - Lecture 1","text":"Algorithm --&gt; Data Structure No obvious solution ==&gt; Algorithm engineers do it If there is a clear implementation path ==&gt; the person who develops the project will do it What's the Algorithm? {Ace of hearts, 10 of spades, 3 of spades, 9 of hearts, 9 clubs, 4 of diamonds, J} First: Hearts&gt; Diamonds&gt; Spades&gt; Clubs Second: Numbers are arranged from small to large Some people put the colors together first Some people arrange the size first, and extract the colors one by one \\[ 1024 --&gt; 10^3 --&gt; 1k \\] \\[ 1024 * 1024 --&gt; 10^6 --&gt; 1M \\] \\[ 1024 * 1024 * 1024 --&gt; 10^9 --&gt; 1G \\] 123456struction-0 00011101struction-1 00011111 struction-2 00011100struction-3 00011101struction-4 00011100struction-5 00011001 2.6G Hz 12345def fac(n): # return n! if n == 1: return 1 # 返回操作 else: return n * fac(n-1) # 乘法操作 + 返回操作 + 函数调用 12345678910fac(1)&gt; 1fac(100)&gt; 93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000fac_100 = &quot;&quot;&quot;93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000&quot;&quot;&quot;len(fac_100)&gt; 158 12345?? N --&gt; fac(n)# 乘法操作 + 返回操作 + 函数调用?? (N - 1)--&gt; fac(n-1)?? N == 100 fac(N) ??? 99 1234Object ` N --&gt; fac(n)` not found.Object ` (N - 1)--&gt; fac(n-1)` not found.Object ` N == 100 fac(N)` not found.Object `? 99` not found. \\[ Time(N) - Time(N-1) = constant \\] \\[ Time(N-1) - Time(N-2) = constant \\] \\[ Time(N-2) - Time(N-3) = constant \\] \\[ Time(2) - Time(1) = constant \\] \\[ Time(N) - Time(1) == (N-1)constant \\] \\[ Time(N) == (N-1)constant + Time(1) \\] \\[ Time(N) == N * constant + (Time(1) - constant) \\]","link":"/Lecture_1/"},{"title":"02 大语言模型做情感分析","text":"上一节中，我们介绍了大型语言模型的接口非常简单，仅提供了Complete和Embedding两个接口。但这样看似简单的接口，实际上可以解决很多自然语言处理问题。例如，情感分析、文本分类、文章聚类、摘要生成、搜索等问题，都可以使用大型语言模型解决。接下来的几节课中，我们将介绍如何使用这两个简单的API来解决传统的自然语言处理问题。本节我们将从最常见的自然语言处理问题“情感分析”开始介绍，看看如何使用大型语言模型。 传统的二分类方法：朴素贝叶斯与逻辑回归 朴素贝叶斯与逻辑回归可以用来解决“情感分析”问题。这些算法的基本思想是，根据给定的标记数据，学习一个分类器，用来将新的输入数据进行分类。对于情感分析问题，分类器的目标是将一段文字分为正面或负面情感。 在这种方法中，我们首先需要对一部分评论数据进行标记，也就是手动标注这些评论是正面还是负面的。然后，我们将这些标记好的数据喂给算法进行学习。学习的过程就是从标记好的数据中提取特征，然后学习一个分类器，这个分类器可以将新的输入数据进行分类。 朴素贝叶斯算法是一种基于贝叶斯定理和特征条件独立假设的算法。它的基本思想是，通过已知的特征条件概率来计算输入数据的分类概率，并选择具有最大概率的类别作为分类结果。在情感分析问题中，朴素贝叶斯算法通过计算评论中每个单词在正面评论和负面评论中出现的频率来进行分类。 逻辑回归是一种广泛应用于分类问题的统计学习方法，其本质是利用一组输入特征和权重参数来计算一个分数，然后将分数转化为概率值。在情感分析问题中，逻辑回归通过使用输入评论的特征和权重参数来计算这段评论属于正面或负面情感的概率。 需要注意的是，传统的二分类方法并不一定是最优解决方案。随着深度学习技术的发展，越来越多的研究表明，使用深度学习模型可以在情感分析问题上获得更好的性能。 朴素贝叶斯算法是一种基于贝叶斯定理和条件独立性假设的分类算法。在垃圾邮件分类中，它的基本思想是对于一封待分类的邮件，通过计算邮件中出现单词的概率，来判断它是否为垃圾邮件。具体地，朴素贝叶斯算法会先从一组已知分类的训练集中学习出每个单词在每个分类下的条件概率，然后根据贝叶斯定理，计算待分类邮件属于每个分类的后验概率，最终将待分类邮件归类到后验概率最大的分类中。 具体而言，朴素贝叶斯算法假设所有特征都是相互独立的，即一个特征的出现并不会影响其它特征的出现概率，这也是“朴素”这个词的由来。在垃圾邮件分类中，这意味着我们可以将一封邮件表示为一个由词汇组成的向量，其中向量的每个元素表示一个单词是否出现在邮件中，这个元素的值通常是0或1。然后，我们可以通过计算训练集中每个单词在垃圾邮件和非垃圾邮件中的出现概率，来计算一个待分类邮件属于垃圾邮件或非垃圾邮件的概率。 具体地，设邮件中出现的单词为w1, w2, ..., wn，将邮件表示为一个n维的向量X=(x1, x2, ..., xn)，其中xi=1表示单词wi出现在邮件中，xi=0表示单词wi未出现在邮件中。设邮件属于垃圾邮件的概率为P(S)，邮件属于非垃圾邮件的概率为P(H)，则有： 其中，P(X|S)表示在垃圾邮件中，向量X出现的概率，即： 同理，P(X|H)表示在非垃圾邮件中，向量X出现的概率，即： P(S)和P(H)分别表示训练集中垃圾邮件和非垃圾邮件的概率，可通过训练集中垃圾邮件和非垃圾邮件的数量计算得出。接下来，我们需要计算每个单词在垃圾邮件和非垃圾邮件中出现的概率，即P(wi|S)和P(wi|H)。 为了计算这些条件概率，我们需要统计训练集中所有垃圾邮件和非垃圾邮件中每个单词出现的次数，以及每个类别中单词的总数。然后，我们可以使用以下公式来计算每个单词在垃圾邮件和非垃圾邮件中出现的概率： 在实际计算中，由于训练集中往往包含大量单词，每个单词的出现次数很少，为了避免概率为0的情况，我们通常会使用拉普拉斯平滑或其他平滑技术来调整概率值。 最终，我们可以使用以下公式来计算一封邮件是垃圾邮件的概率： 其中，email表示待分类的邮件，w1、w2、...、wn表示邮件中出现的单词。类似地，我们可以计算邮件为非垃圾邮件的概率： 然后，我们可以比较这两个概率的大小，将邮件分类为垃圾邮件或非垃圾邮件。这就是朴素贝叶斯分类算法的基本流程。 一般来说，如果一个词语在差评里出现的概率比好评里高得多，那这个词语所在的评论，就更有可能是一个差评。 假设我们有一个训练集包含4封邮件，其中2封是垃圾邮件，2封不是垃圾邮件。训练集里的邮件包含以下单词： 序号 邮件类型 出现的单词 Email A 垃圾邮件 buy, money, offer, secret Email B 垃圾邮件 buy, secret, sell, money Email C 普通邮件 offer, book, sell Email D 普通邮件 book, study, exam 然后来了一封新邮件，里面的单词是：buy、money、sell。 通过计算在垃圾邮件和非垃圾邮件中这些单词出现的概率，我们可以预测这封新邮件是垃圾邮件还是非垃圾邮件。 垃圾 垃圾 垃圾 普通 普通 普通 然后我们把这封邮件里所有词语的条件概率用全概率公式乘起来，就得到了这封邮件是垃圾邮件还有普通邮件的概率。 垃圾垃圾垃圾垃圾垃圾 普通普通普通普通普通 在这里，我们发现 P(垃圾∣X)&gt;P(普通∣X)，而且 P(普通∣X) 其实等于 0。那如果用朴素贝叶斯算法，我们就会认为这封邮件 100% 是垃圾邮件。 朴素贝叶斯算法是一种基于贝叶斯定理和条件独立性假设的分类算法，可以用来进行情感分析等分类问题。在使用朴素贝叶斯算法进行垃圾邮件分类时，通过计算每个单词在垃圾邮件和非垃圾邮件中出现的概率来判断一封邮件是否为垃圾邮件。不过，如果一封邮件中出现的单词不在训练集中，那么朴素贝叶斯算法就不能给出正确的分类结果。此外，像逻辑回归、随机森林等机器学习算法也可以用来进行分类，网上有很多其他人使用这些传统方法来设计情感分析的解决方案，可以在 Kaggle 这个机器学习比赛的网站里找到相关的 Jupyter Notebook。虽然这些算法背后涉及到一些数学知识，但在进行实际的 AI 应用开发时，并不需要预先掌握这些知识，因为我们可以借助各种 AI 平台和工具来进行开发。 替代传统方法：特征工程与模型调参 传统的机器学习算法，在应用到实际问题时，往往需要进行特征工程和模型调参，这两个方面都需要一定的经验和技巧。 特征工程 特征工程是机器学习中非常重要的一环，通过对数据进行加工和转换，从原始数据中提取有意义的特征，以便机器学习算法更好地理解和分类数据。在自然语言处理中，特征工程尤其重要，因为自然语言数据的特征通常是复杂且非结构化的。 举个例子，如果我们想对一段文本进行情感分析，我们不能仅仅依靠文本中是否出现特定的单词来判断，因为不同的单词可能在不同的语境下有不同的情感含义。比如，“这家餐馆太糟糕了，一点都不好吃”和“这家餐馆太好吃了，一点都不糟糕”这两句话，从意思上是完全相反的，但是它们都包含相同的单词。这时候，我们需要通过一些特征工程的方法来解决这个问题。 在自然语言处理中，常用的特征工程方法包括：n-gram模型、词袋模型、TF-IDF权重等。 其中，n-gram模型是一种将相邻的n个单词组合在一起形成新的单词序列的方法。通过n-gram模型，我们可以考虑更多单词之间的关系，而不仅仅是单个单词的出现情况。常见的n-gram模型包括bigram（二元模型）和trigram（三元模型）。以二元模型为例，当n=2时，我们会将相邻的两个单词组合在一起，从而得到一个新的词组，比如“太糟糕”和“不好吃”。这些词组可以更好地捕捉到单个单词无法表达的情感含义。 词袋模型则是一种将文本表示成一个包含其所有单词的向量的方法。在词袋模型中，我们将文本中的所有单词都列出来，形成一个向量，然后统计每个单词在文本中出现的频率，并将其对应到向量的相应位置。虽然词袋模型并不能考虑单词之间的关系，但是它可以捕捉到文本中每个单词的出现情况。 TF-IDF（词频-逆文档频率）权重是一种根据单词在文本中的频率和其在语料库中的文档频率来赋予单词权重的方法。在TF-IDF模型中，单词在文本中出现的次数越多，它的权重就越大，但是如果它在语料库中出现的文档频率越高，它的权重就越小。这种方法可以用来衡量一个单词在整个文本集合中的重要性，常用于文本分类、信息检索等任务中。TF-IDF权重计算公式如下： 其中，表示单词在文本中的权重, 表示单词在文本中出现的频率，表示文本集合中的总文档数，表示包含单词的文档数。 在实际应用中，TF-IDF通常与其他特征一起使用。比如，在文本分类任务中，我们可以把TF-IDF作为文本特征之一，同时还可以加入词性标注、情感分析等特征来提高分类的准确性。在文本信息检索任务中，我们可以使用TF-IDF对查询和文档进行向量化，并通过计算它们之间的相似度来进行检索。 除了TF-IDF，还有一些其他常用的特征工程方法，比如词袋模型（Bag-of-Words）、词嵌入（Word Embedding）等，每种方法都有其优缺点和适用场景。特征工程是机器学习中非常重要的一环，正确地选择和处理特征可以大大提高模型的性能。 不同的特征工程方式，在不同的问题上效果不一样，比如我们做情感分析，可能就需要保留标点符号，因为像“！”这样的符号往往蕴含着强烈的情感特征。但是，这些种种细微的技巧，让我们在想要解决一个简单的情感分析问题时，也需要撰写大量文本处理的代码，还要了解针对当前特定场景的技巧，这非常依赖工程师的经验。 机器学习相关经验 机器学习涉及到很多知识和技巧，包括数据处理、模型训练和评估等。在数据处理方面，我们需要将数据集划分为训练集、验证集和测试集，并进行数据清洗、特征选择等操作。在模型训练方面，我们需要选择适合问题的算法、调整模型超参数，避免过拟合和欠拟合等问题。在模型评估方面，我们需要使用合适的指标来评估模型的性能，比如 AUC、准确率、召回率等。此外，针对数据集不够大的情况，我们需要采用交叉验证等技术来提高模型的泛化能力。 但是，随着大语言模型的发展，我们已经不再需要具备这些丰富的机器学习经验了。大语言模型提供了 Completion 和 Embedding 这两个 API，我们可以用不到 10 行代码就能完成情感分析，并且能获得非常好的效果。这极大地降低了机器学习应用的门槛，使得更多人能够轻松地利用机器学习技术来解决各种实际问题。 大语言模型轻松解决 通过大语言模型来进行情感分析，最简单的方式就是利用它提供的 Embedding 这个 API。这个 API 可以将任何一段文本转换为一个固定长度的向量，以此代表这段文本的语义信息。为了进行情感分析，我们需要事先计算“好评”和“差评”这两个标签的 Embedding。对于一段待分析的文本，我们可以通过 Embedding API 获取该文本的 Embedding。然后，我们将这个文本 Embedding 和“好评”以及“差评”的 Embedding 分别计算余弦相似度（Cosine Similarity），并将两个相似度之间的差值作为该文本的情感分数。如果分数大于 0，则说明这段文本更接近“好评”，反之则更接近“差评”。利用这种方法，我们可以在不需要大量经验和复杂特征工程的情况下，用不到 10 行代码快速地进行情感分析，并获得非常好的效果。 比如，咱们看一下两条惊动上购买iPhone的用户发表的评论。 实际案例还是来自于徐文浩的文章 这个使用大模型的方法一共有 20 行代码，我们看看它能否帮助我们快速对这两条评论进行情感分析。 注意，在执行 import 的时候，经常会遇到这样的错误提示： ModuleNotFoundError: No module named 'scipy' 此时切换到你所使用的环境内，然后执行一遍安装就行了( conda install -c conda-forge scipy )，少数时候，可能你安装一个之后仍然会报缺少其他模块，继续安装。 1234567891011121314151617181920212223import openaiimport osfrom openai.embeddings_utils import cosine_similarity, get_embedding# 获取访问open ai的密钥openai.api_key = \"这里输入你的API Key\"# 选择模型EMBEDDING_MODEL = \"text-embedding-ada-002\"positive_review = get_embedding(\"好评\")negative_review = get_embedding(\"差评\")positive_example = get_embedding(\"买的银色版真的很好看，一天就到了，晚上就开始拿起来完系统很丝滑流畅，做工扎实，手感细腻，很精致哦苹果一如既往的好品质\")negative_example = get_embedding(\"降价厉害，保价不合理，不推荐\")def get_score(sample_embedding): return cosine_similarity(sample_embedding, positive_review) - cosine_similarity(sample_embedding, negative_review)positive_score = get_score(positive_example)negative_score = get_score(negative_example)print(\"好评例子的评分 : %f\" % (positive_score))print(\"差评例子的评分 : %f\" % (negative_score)) 输出结果为： 12好评例子的评分： 0.070963差评例子的评分： -0.072895 我们可以看到，通过大语言模型提供的 Embedding 相似度计算方法，可以非常简单地对评论进行情感分析。在对京东商品评论进行测试时，我们发现好评的相似度得分大于0，差评的得分小于0，验证了该方法的有效性。更进一步的，我们可以尝试使用这个方法来分析含义截然相反的评论，看看它能否准确判断。 1234567good_restraurant = get_embedding(\"这家餐馆太好吃了，一点都不糟糕\")bad_restraurant = get_embedding(\"这家餐馆太糟糕了，一点都不好吃\")good_score = get_score(good_restraurant)bad_score = get_score(bad_restraurant)print(\"好评餐馆的评分 : %f\" % (good_score))print(\"差评餐馆的评分 : %f\" % (bad_score)) 如图所示，我们输出结果为： 12好评餐馆的评分 : 0.062719差评餐馆的评分 : -0.074591 可以看到，虽然两句话分别是“太好吃”“不糟糕”和“太糟糕”“不好吃”，其实词语都一样，但是大语言模型一样能够帮助我们判断出来他们的含义是不同的，一个更接近好评，一个更接近差评。 更大的数据集上的真实案例 在前面的例子中，我们展示了利用大语言模型进行情感分析的简单方法，并验证了它在小规模数据上的有效性。那么在更大的数据集上，我们的方法是否仍然可靠呢？下面我们将以亚马逊提供的食品评论数据为例进行验证。 该数据集包含了用户对不同品牌的食品的评论以及所打的评分，我们可以将评分转化为好评和差评两个类别，以验证我们的方法准确度。具体而言，我们将1-2星的评分视为差评，4-5星的评分视为好评。为了避免重复计算，我们在数据集中已经提前保存了评论的Embedding信息。 利用Pandas读取CSV数据，我们可以将这些评论和对应的评分加载到内存中，并利用我们之前介绍的Embedding API计算评论的向量。通过计算评论向量与好评、差评向量之间的余弦距离，我们可以得到每个评论的相似度分数。最终，我们将这些分数与阈值进行比较，即可将评论分类为好评或差评。 重新调用 OpenAI 的 API 会很浪费钱，这个数据集里，我们将获取到的 Embedding 信息保存下来了，不需要再重新计算。 模型和代码我都将上传到自己的Github上，有需要的可以自己获取 12345678910111213import pandas as pdimport numpy as npfrom sklearn.metrics import classification_reportdatafile_path = './data/fine_food_reviews_with_embeddings_1k.csv'df = pd.read_csv(datafile_path)df['embedding'] = df.embedding.apply(eval).apply(np.array)# 将五星评级转换为二元情绪df = df[df.Score != 3]df[\"sentiment\"] = df.Score.replace({1: \"negative\", 2: \"negative\", 4:\"positive\", 5:\"positive\"} 每一条评论都用我们上面的方法，和一个预先设定好的好评和差评的文本去做对比，然后看它离哪个近一些。这里的好评和差评，我们写得稍微长了一点，分别是 “An Amazon review with a negative sentiment.” 和 “An Amazon review with a positive sentiment.”。 在计算完结果之后，我们利用 Scikit-learn 这个机器学习的库，将我们的预测值和实际用户打出的星数做个对比，然后输出对比结果。需要的代码，也就不到 20 行。 123456789101112131415161718192021from sklearn.metrics import PrecisionRecallDisplaydef evaluate_embeddings_approach( labels = ['negative', 'positive'], model = EMBEDDING_MODEL,): label_embeddings = [get_embedding(label, engine=model) for label in labels] def label_score(review_embedding, label_embeddings): return cosine_similarity(review_embedding, label_embeddings[1]) - cosine_similarity(review_embedding, label_embeddings[0]) probas = df[\"embedding\"].apply(lambda x: label_score(x, label_embeddings)) preds = probas.apply(lambda x: 'positive' if x&gt;0 else 'negative') report = classification_report(df.sentiment, preds) print(report) display = PrecisionRecallDisplay.from_predictions(df.sentiment, probas, pos_label='positive') _ = display.ax_.set_title(\"2-class Precision-Recall curve\")evaluate_embeddings_approach(labels=['An Amazon review with a negative sentiment.', 'An Amazon review with a positive sentiment.']) 输出结果 我们可以看到通过简单的调用大模型接口计算向量相似度的方法可以达到相当高的精度，而不需要进行任何机器学习训练。现在，我们来看一下如何在具体数据集上使用这个方法进行情感分析。 我们使用的是亚马逊商品评论数据集，包含了约 130 万条评论以及对应的评分。我们将评分为 1 或 2 的评论视为负面评论，评分为 4 或 5 的评论视为正面评论，而评分为 3 的评论则排除在外。我们使用 Python 中的 Pandas 库读取数据集，再使用 OpenAI 提供的 Embedding API 获取评论的向量表示，并计算每个评论向量与正面评论和负面评论向量之间的余弦相似度。对于余弦相似度大于 0 的评论，我们认为其为正面评论，余弦相似度小于 0 的评论则认为其为负面评论。最后，我们通过比较模型的预测结果与真实标签的一致性来评估模型的性能。 小结 本课程介绍了使用大型语言模型进行情感分析的零样本分类方法。这种方法不需要额外的训练数据，利用大型语言模型中预训练的“好评”和“差评”的概念信息，可以判断未见过的评论的情感极性。通过计算不同文本在大型语言模型中的嵌入距离，可以进行情感分析。使用这种方法可以轻松地在经典数据集上达到95%以上的准确度，并且降低了机器学习的门槛。利用 OpenAI的API可以在几分钟内获得所需的结果，这使得文本分类的应用变得更加容易。 在学习完本节课程后，希望你能够进一步巩固所学内容并掌握实践能力。因此，我们建议你进行一些课后练习，以加深对大模型应用于情感分析的理解和实战能力的提升。你可以尝试将本节课程中讲解的方法应用到其他数据集上，比如 Kaggle 提供的亚马逊耳机评论数据，看看是否也会有很好的效果。该数据集较大，你可以选择挑选几条数据进行尝试，因为 OpenAI 会对你的调用进行限速，免费的 Token 数量也比较有限。对于大批量数据的处理，我们会在后面进行详细讲解。 欢迎你将你的结果分享到评论区，也欢迎你把这节课分享给感兴趣的朋友。我们下节课再见！ 数据集：https://www.kaggle.com/datasets/shitalkat/amazonearphonesreviews","link":"/Large-language-model-for-sentiment-analysis/"},{"title":"03 提示语，做个聊天机器人","text":"大家好，我是茶桁。 在本次讲座中，我们将研究 OpenAI 提供的 Completion API 接口。你可能已经体验过与 ChatGPT 进行聊天的乐趣，或是利用它帮我们编写各种文本材料的便利。现在，我们可以从这个需求出发，进一步探索 Completion API 的具体用法，以及如何利用它来生成更加优质的文本。 AI 客服 在我了解人工智能和自然语言生成之前，我听说过智能客服，然而我并没有亲身体验过。我想象中，智能客服的回答应该是按照固定的模板进行生成的，这样的缺点就是每次回答都会是一样的。虽然可以设计多个模板来轮换着表达相同的意思，但是最多也就是三四个模板，整体的体验可能会比较呆板。 但是，随着 GPT 这样的生成式语言模型的出现，我们现在可以让 AI 自动根据我们的需求来撰写文案了。只要我们将需求提供给 Open AI 提供的 Completion 接口，就能自动获得一段文本，大大提升了智能客服的质量和效率。 123456789101112131415161718192021import openaiimport osopenai.api_key = &quot;OPENAI_API_KEY&quot;COMPLETION_MODEL = &quot;text-davinci-003&quot;prompt = &quot;请你用朋友的语气回复给到客户，并称他为“亲”，他的订单已经发货在路上了，预计在3天之内会送达，订单号256180193592，我们很抱歉因为天气的原因物流时间比原来长，感谢他选购我们的商品。&quot;def get_response(prompt, temperature = 1.0): completions = openai.Completion.create ( engine=COMPLETION_MODEL, prompt=prompt, max_tokens=1024, n=1, stop=None, temperature=temperature, ) message = completions.choices[0].text return messageprint(get_response(prompt)) 可以得到回复： 亲，你好！多谢选购我们的商品！你的订单（订单号256180193592）已发货，预计3天之内能送达。由于天气原因，物流前来比原定的时间会晚一点，抱歉给您带来不便。 让我们再来调用并输出一次： 1print(get_response(prompt)) 得到回复： 亲，您的订单256180193592已经发货，预计3天将全部到达，不过由于天气原因送货可能比期望晚点，这里深表歉意。感谢您选购我们的商品，祝您购物愉快！ 我们可以看到，原始提示语是一模一样的，甚至都没改动标点符号。在连续两次调用之后，我们得到了含义相同、遣词造句不同的两份结果。 我提供了一个简单的示例代码，利用 Open AI 提供的 Completion API 接口来生成一段合适的智能客服回复。我们给 AI 提供了一个简短的提示语，要求 AI 用亲切的语气告诉客户，订单因为天气原因延迟了。我们还指定了一些语言风格上的要求，比如要用朋友的口吻称呼客户为\"亲\"。 通过连续两次使用相同的参数，我们可以看到 AI 确实满足了我们的要求，给出了一段符合逻辑的回复。这其中包括两个难点：AI 能够正确地使用\"亲\"称呼客户，以及在回复中提取出了订单号并返回给了客户。 值得注意的是，尽管两次返回的回复意思相同，但是具体的语言表达又有所不同，这归功于我们使用了一个名为 temperature 的参数。该参数的取值范围在 0-2 之间，代表输出结果的随机性或者多样性。我们在示例代码中选择了 1.0 这个中等值，从而保证每次生成的内容都有一定的差异。如果我们将该参数设置为 0，那么每次输出的结果将非常相似。 1print(get_response(prompt, 0.0)) 亲，您的订单256180193592已经发货，预计3天内会送达，由于天气原因，物流时间比原来长，我们深表歉意。感谢您选购我们的商品，祝您购物愉快！ 1print(get_response(prompt, 0.0)) 亲，您的订单256180193592已经发货，预计3天内会送达，由于天气原因，物流时间比原来长，我们深表歉意。感谢您选购我们的商品，祝您购物愉快！ 这个参数该怎么设置，取决于实际使用的场景。如果对应的场景比较严肃，不希望出现差错，那么设得低一点比较合适，比如银行客服的场景。如果场景没那么严肃，有趣更加重要，比如讲笑话的机器人，那么就可以设置得高一些。 除了 temperature 之外，让我们来看看Completion这个接口的其他参数： engine：指定使用的 Open AI 模型的名称。不同的模型具有不同的特点和能力，例如生成式语言模型 GPT-3 和面向任务的模型 DALL-E，而这个参数允许您选择您想要使用的模型。在这个例子中，我们使用的是 text-davinci-003 引擎，这是 Open AI 的一个最高级的语言模型，能够根据您的提示语生成出最准确、最多样化的回答。 prompt：指定模型要生成的文本的初始提示语。这个参数通常是由人类提供的，以指导模型生成适当的文本。在这个例子中，prompt 是一个字符串，要求 AI 以亲切的口吻告诉用户订单被延迟了，同时使用“亲”这个称呼。 max_tokens：这个参数是用来限制 AI 生成的文本长度的，它指定了允许生成文本中的 token 的最大数量。Token 是对输入进行分割后得到的最小文本单元，可以是单词或者短语等等。在这个例子中，max_tokens 设为 4096，这是 text-davinci-003 引擎允许的最大值。需要注意的是，这个数量既包括您输入的 prompt，也包括 AI 产生的回答，两者加起来不能超过 4096 个 token。比如，如果您的 prompt 已经包含 1000 个 token，那么您设置的 max_tokens 就不能超过 3096。 n：表示您希望模型生成多少个不同的文本。在这个例子中，我们将 n 设为 1，因为我们只需要生成一段回复。如果您需要多个备选项，可以将 n 设为较高的数字，然后选择其中最适合的一个。 stop：指定生成的文本中的一个字符串，用于指示模型停止生成。例如，您可以将 stop 设为“”以指示模型在生成两个换行符时停止。在这个例子中，我们将 stop 设为逗号“，”，这意味着模型在生成提示语中的“亲”后停止生成。 当然并不是只有这五个参数，只是目前而言我们无需了解那么多，有钻研精神又比较感兴趣的，可以去参看官方文档。当然是英文版的，有能力的直接看，看不太懂的，还记得我之前在导读里教大家的方法吗？ 聊天机器人 在前面我们了解了如何使用提示语让 AI 回答一个问题。但是，如何让 AI 与人进行对话呢？特别是如何实现多轮对话，让 GPT 能够记住上下文。例如，当用户问我们 \"iPhone14 拍照好不好\" 时，我们回答 \"很好\"，然后用户又问 \"它的价格是多少\" 时，我们需要理解用户指的是上面提到的 iPhone。 对于聊天机器人而言，理解当前用户的句子是不够的，必须能够理解整个上下文才行。然而，GPT 的模型要实现支持多轮对话也不复杂。我们只需要在提示语中增加一些小小的工作就可以了。 要实现问答，我们只需在提示语中，在问题之前加上 \"Q: \" 表示这是一个问题，然后另起一行，加上 \"A: \" 表示我们需要一个回答，Completion 的接口就会回答你在 \"Q: \" 中提出的问题。例如，当我们问 AI \"麻婆豆腐怎么做\" 时，它会一步一步地列出制作步骤。 12345question = &quot;&quot;&quot;Q : 麻婆豆腐该怎么做？A : &quot;&quot;&quot;print(get_response(question)) 准备好所需食材：豆腐、肉末（猪肉或鸡肉）、精盐、葱，蒜，料酒、生抽和老抽，花椒粉、白胡椒粉、辣椒粉、糖、淀粉和麻油。 要让 AI 完成多轮对话其实也很容易，我们只需要把之前对话的内容也放到提示语里面，把整个上下文都提供给 AI。这样 AI 就能够自动根据上下文，回答下一个问题。例如，如果你接着问“那蚝油牛肉呢？”，我们不要只是把这个问题传给 AI，而是将之前的对话也一并传给 AI，那么 AI 就能够理解你问的“那蚝油牛肉呢？”是想要知道怎么做，而不是关于购买或者价格的问题。 Q：麻婆豆腐该怎么做？ A：详细的麻婆豆腐的做法 Q：那蚝油牛肉呢？ A： 1234567891011121314151617question = &quot;&quot;&quot;Q : 麻婆豆腐该怎么做？A :准备好所需食材：豆腐、肉末（猪肉或鸡肉）、精盐、葱，蒜，料酒、生抽和老抽，花椒粉、白胡椒粉、辣椒粉、糖、淀粉和麻油。一、先将肉末和调料混合在一起：肉末、精盐、料酒、生抽、老抽、花椒粉、白胡椒粉、辣椒粉和麻油各适量。二、另外把豆腐切成小块，然后用淀粉抓着放入沸水锅中煮5-6分钟，煮至软熟。三、将肉末放入油锅，小火爆炒一会。加入蒜泥、葱泥继续炒香。四、将豆腐放入油锅中，加入糖、适量盐、酱油和水各适量，混合均匀，中火烧开再煮至汤汁稠浓即可。 Q: 那耗油牛肉呢？A: &quot;&quot;&quot;print(get_response(question)) 耗油牛肉的做法： 下面是一个名为 food_chatbot 的完整 Python 代码，它可以从命令行读取你的问题，并回答你的问题。你可以继续提问，这样我们就可以把整个对话过程都发送给 AI 来回答。这样的话，你可以体验一下 AI 是否能够理解整个对话过程的上下文。如果你想退出，只需在需要提问时输入“bye”即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243import openaiimport osopenai.api_key = &quot;OPENAI_API_KEY&quot;def ask_gpt3(prompt): response = openai.Completion.create( engine=&quot;text-davinci-003&quot;, prompt=prompt, max_tokens=512, n=1, stop=None, temperature=0.5, ) message = response.choices[0].text.strip() return messageprint(&quot;你好，我是一个聊天机器人，请你提出你的问题吧?&quot;)questions = []answers = []def generate_prompt(prompt, questions, answers): num = len(answers) for i in range(num): prompt += &quot;\\n Q : &quot; + questions[i] prompt += &quot;\\n A : &quot; + answers[i] prompt += &quot;\\n Q : &quot; + questions[num] + &quot;\\n A : &quot; return promptwhile True: user_input = input(&quot;&gt; &quot;) questions.append(user_input) if user_input.lower() in [&quot;bye&quot;, &quot;goodbye&quot;, &quot;exit&quot;]: print(&quot;Goodbye!&quot;) break prompt = generate_prompt(&quot;&quot;, questions, answers) answer = ask_gpt3(prompt) print(answer) answers.append(answer) AI解决情感分析问题 在上一讲中，我们介绍了使用向量距离来进行情感分析的方法，但这种方法需要一些机器学习和数学知识。那么，我们是否能够利用自然语言提示语的方式，让 AI 帮助我们完成情感分析呢？ 让我们来尝试一下。我们可以告诉 AI 我们想要它帮助我们判断用户评论的情感是正面还是负面，并且提供两个例子，分别是正面和负面的评论，让 AI 通过这两个例子学习如何进行情感分析。然后，我们再给 AI 一条新的评论，看看它能否正确地判断这条评论的情感。 在这个过程中，我们只需要调用 Completion 的 API 一次，但需要将提示语分成三个部分： 第一部分是告诉 AI 我们要进行情感分析； 第二部分是提供两个例子，以\"评论：\"开头后面跟着具体的评论，然后是以\"情感：\"开头后面跟着这个例子的情感； 第三部分是给出要进行情感分析的评论，同样以\"评论：\"开头跟着我们想要它判断的评论，然后是以\"情感：\"开头但后面没有内容，因为我们等待 AI 给出判断结果。 详细代码和实现方法可以参考下面的示例。 123456789101112131415161718prompts = &quot;&quot;&quot;判断一下用户的评论情感上是正面的还是负面的评论：120的刷新率会舒服很多，有重低音跑的感觉，特别爽，真是非常给力，特别屏幕也是超级好用，屏幕也很灵敏，使用丝滑顺畅，打游戏也很舒服拍照效果：拍照效果杠杠的，画质清晰，像素感人，发货快，包装严实，各方面都没破损，颜值很高，耐看，打开网页丝滑顺畅，游戏不掉帧，分辨率高，功能很齐全。拍照很强大，照片成像很细腻，反应速度也很快，苹果系统真的很好用，运行速度：14的运行速度非常快，打游戏真的太流畅了，真的非常喜欢这个手机。颜值高，手感好，拍照清晰，内存大，运行速度快，待机时间长满意太喜欢了，手感不错，拍照效果也很好，运行速度很快，音效很好，续航强，手机手感很好，电池续航也不错。其他特色：整体特别满意。外观很好看，手感拿着很有质感，屏幕看着非常好，拍照效果很不错，像素高了，新机拍照片怎么都好看。5G高速运行，超长待机一整天。情感：正面评论：刚买完就降价，卡着降价保护时间降价情感：负面&quot;&quot;&quot;good_case = prompts + &quot;&quot;&quot;评论：外形外观：银色外观好看。屏幕音效：音效非常好。拍照效果：拍照效果优秀，完全能够满足日常需要。运行速度：运行速度快，高刷无延迟。情感：&quot;&quot;&quot;print(get_response(good_case)) 正面 123456bad_case = prompts + &quot;&quot;&quot;评论：充电口有划痕，有强迫症的建议选黄道吉日抽奖情感&quot;&quot;&quot;print(get_response(bad_case)) 负面 我们重新从京东商城的 iPhone 评论区随机找两个和上次不太一样的好评和差评，可以看到，结果是准确的。这是不是很棒？我们不需要任何机器学习的相关知识，用几句话就能够轻松搞定情感分析问题。 常见的大模型的上下文学习能力，通过几个例子，就能回答正确的结果。 而上面这个“给一个任务描述、给少数几个例子、给需要解决的问题”这样三个步骤的组合，也是大语言模型里使用提示语的常见套路。一般我们称之为 Few-Shots Learning（少样本学习），也就是给一个或者少数几个例子，AI 就能够举一反三，回答我们的问题。 小结 在本节中，我们介绍了如何巧妙地利用提示语来让 AI 完成多种任务，通过三个例子向大家展示了 Completion 接口的强大之处。第一个例子中，我们让 AI 重写一段话，通过指定模型引擎和最大 token 数量等参数，实现了高效自然语言生成。第二个例子中，我们利用 Q 和 A 的提示语将对话历史记录传递给 AI，实现了多轮对话和上下文理解。最后一个例子中，我们将正面和负面情感的例子传递给 AI，实现了情感分析。可以看到，通过巧妙运用提示语，我们能够轻松地让大语言模型完成各种任务，这也是我们迈向通用人工智能的第一步。 课后练习 上一讲我给出了一些数据集，通过向量距离来进行正面和负面情感的判断。那么，你能不能试着用这一讲的 Few-Shots Learning 的方法，在对应的数据集上也运行一下，看看通过这种方式进行情感分析的准确率是多少？ 在使用 GPT 类型的模型的时候，提示语非常重要。所以，有人专门制作了一个叫做 AIPRM 的浏览器插件，包含了很多总结出来有效的提示语。你可以试着安装，看看别人都用了什么有趣的提示语。关于更多ChatGPT浏览器插件，可以看看这篇文章 本节课程代码可以在Github上找到","link":"/Let-s-Build-a-Chatbot/"},{"title":"M1安装Homebrew(ARM)","text":"?&gt; 详情可见作者说明 安装 ARM版本Homebrew必须安装在/opt/homebrew路径下 123cd /optsudo mkdir homebrewsudo curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C homebrew 如果不进行sudo授权，则会报错； 环境变量 本人使用zsh, 所以编辑文件~/.zshrc. 添加如下内容： 12path=('/opt/homebrew/bin' $path) export PATH ?&gt; 如果是使用bash，请修改~/.bashrc 在终端内执行: 1source ~/.zshrc 现在可以试试执行brew install graphviz试试看能否正常安装回归树可视化模块； 软件包和迁徙 软件包依然需要使用X86版Homebrew 1arch -x86_64 启用一个X86模式中端，之后运行的命令都在X86模式下运行，再次安装Homebrew 1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; !&gt; 注意：要将ARM版本Homebrew环境变量设置到最前面，此时两个版本共存时会有限启动ARM版本，需要运行X86版本时，需要手动输入完整路径arch -x86_64 /usr/local/bin/brew 可以在配置文件中设置alias 12abrew='/opt/homebrew/bin/brew' # ARM Homebrewibrew='arch -x86_64 /usr/local/bin/brew' # X86 Homebrew 如果对已有软件包做迁徙，则： 1ibrew bundle dump 此时在目录下就得到一个名为Brewfile的备份文件，导入内容并安装 1abrew bundle --file /path/to/Brewfile !&gt; 执行之前需要编辑Brewfile文件，将cask和mas开头的记录删除掉；","link":"/M1_install_homebrew/"},{"title":"Power for Mac","text":"image 很多人都会使用\"Pow\"来进行本地静态页面开发环境。对于其配置确实简单到有爱。 不过Yosemite上\"Pow\"都不能正常工作，之前我参照官方的办法写了一篇如何在Yosemite上设置\"Pow\"的方法。有兴趣的可以参看我原文:http://hivan.me/setting-pow-at-Yosemite/ 不过一直用的好好的\"Pow\"近期又开始出现 404 错误，短暂的解决无果以后，我开始寻找一些快速能解决的办法，便遇到了\"Power\"，和\"Pow\"一样，都是建立快速开发环境，并且经过测试，在Yosemite 10.10.1下正常工作。 使用\"Pow\"来做开发环境的，可以暂时用这样一个替代方案，毕竟开发中没有时间去多做研究了，暂时不知道两者的区别，不过\".dev\"正常访问已经没有问题。 Power 项目地址: https://github.com/HackPlan/power/","link":"/Power-for-Mac/"},{"title":"心理学知识预告","text":"我原本是一个不太知名的产品经理，在我的职业生涯中，一直致力于帮助企业创造出引人入胜的产品，吸引和留住用户。然而，我很快就发现了一个重要的事实：想要设计和推出一款成功地产品，需要对人的心理有一定的了解。 我对心理学的兴趣开始于我的工作经验。随着我不断深入地研究用户心理，我开始对这个领域的其他方面产生了兴趣。这引发了我对心理学本身的好奇心，使我开始不断学习心理学的相关概念和理论。最终，这个兴趣成为了我个人的爱好和研究方向。 我想在个人博客上开始写相关内容的目的是想与读者分享我对心理学的研究和思考，希望能够让大家对这个领域产生更深入的了解。我希望这本书能够帮助读者更好地理解自己和他人的行为和思维方式，从而更好地解决生活中的问题。在这里我不得不要提一本书，就是《津巴多普通心理学》。这本书目前应该是出到第八版了，里面的知识基本上构建了我对心理学大部分的认知，最重要的，也构建了我对所谓“科学心理学”最正确的观念。 最后，我要感谢我的导师们，包括且不仅限于：邱岳东教授，刘晓霞老师，娄丽华老师，周琼老师，李元春老师。感谢《津巴多普通心理学》这本书，其中的内容对我完成这本书起到了至关重要的作用。在完成本书的过程中，我对心理学的相关内容和概念又有了深一层次的理解。","link":"/Psychological-Trailer/"},{"title":"AI ability practice","text":"Including core capabilities, BI and algorithm related, Code warehouse: 【AI Basic](https://github.com/hivandu/practise/tree/master/AI-basic)","link":"/README/"},{"title":"06 快速建立一个AI应用","text":"Hi，我是茶桁。 在过去的两讲中，我们已经使用 OpenAI 提供的 Embedding 接口完成了文本分类的功能。现在，我们回到 Completion 接口，这一讲将带你更深入地了解该接口的使用。除此之外，我们还将快速搭建一个有界面的聊天机器人，这将让你更好地理解 Completion 接口的应用场景。在这个过程中，你将第一次使用 HuggingFace 这个平台，它是目前最流行的深度学习模型社区。通过 HuggingFace，你可以下载到最新的开源模型，查看其他人提供的示例代码，并参与到社区的交流中。 价廉高质的ChatGPT 我们在第三讲里介绍了Completion接口，并且通过它实现了一个聊天机器人的功能。在那个时候，我们采用的是自己将整个对话拼接起来，将整个上下文都发送给 OpenAI 的 Completion API 的方式。不过，在 3 月 2 日，因为 ChatGPT 的火热，OpenAI 放出了一个直接可以进行对话聊天的接口。这个接口叫做 ChatCompletion，对应的模型叫做 gpt-3.5-turbo，不但用起来更容易了，速度还快，而且价格也是我们之前使用的 text-davinci-003 的十分之一，可谓是物美价廉了。 1234567891011# Note: you need to be using OpenAI Python v0.27.0 for the code below to workopenai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Where was it played?&quot;} ]) 这里需要注意，我们运行这段代码需要的是OpenAI Python v0.27.0， 或者以上版本。如果你的版本过低，可能会报错找不到ChatCompletion这个方法，这个时候请重新更新一下你的OpenAI API库 conda install -c conda-forge openai 这个链接内，你可以看到官方的接口调用示例 在 OpenAI 的官方文档里，可以看到这个接口也非常简单。实际上，在使用 OpenAI API 进行自然语言处理时，你只需要传入一个包括多个元素的数组。每个数组元素都包含两个字段： role 和 content。其中，role 字段具有三种选择，分别为 system、 user 和 assistant。 当 role 字段的取值为 system 时， content 字段中的内容表示我们向 AI 发送的指令，也就是告诉 AI 如何回答用户的问题。例如，如果我们希望 AI 通过中文回答用户的问题，我们就可以在 content 字段中写入“你是一个只会用中文回答问题的助理”。这样，即使用户的问题是用英语提出的，AI 的回答也会是中文的。 当 role 字段的取值为 user 或 assistant 时， content 字段中的内容则表示用户和 AI 之间的对话内容。与我们在第 03 讲中制作的聊天机器人类似，你需要将对话历史记录一起发送给 OpenAI 的接口，这样它才能理解整个对话的上下文，并作出更加准确的回答。 有了这个接口，我们就可以很方便地封装一个聊天机器人。聊天机器人能够在许多领域中发挥作用，比如在客服领域中，它可以帮助客户解决问题。在社交娱乐领域中，聊天机器人可以模拟真实的人类交流，从而为人们提供更好的体验。此外，我们还可以通过加入自然语言处理算法，让聊天机器人更加聪明、更加智能化。下面是我封装的聊天机器人的代码，一起来看一看吧。 1234567891011121314151617181920212223242526272829303132import openaiimport osopenai.api_key = 'OPENAI_API_KEY'class Conversation: def __init__(self, prompt, num_of_round): self.prompt = prompt self.num_of_round = num_of_round self.messages = [] self.messages.append({'role':'system', 'content': self.prompt}) def ask(self, question): try: self.messages.append({'role':'user', 'content': question}) response = openai.ChatCompletion.create( model='gpt-3.5-turbo', messages = self.messages, temperature = 0.5, max_tokens = 2048, top_p = 1, ) except Exception as e: print(e) return e message = response['choices'][0]['message']['content'] self.messages.append({'role':'assistant', 'content': message}) if len(self.messages) &gt; self.num_of_round*2 + 1: del self.messages[1:3] # Remove the first round conversation left return message 我们为了更好地使用这个聊天机器人，封装了一个 Conversation 类，构造函数 init 接受两个参数。第一个参数是 prompt，表示我们对这个聊天机器人的指令，它是系统的 content。第二个参数是 num_of_round，代表每次向 ChatGPT 发起请求时，保留过去几轮会话。这个构造函数的作用是初始化 ChatGPT 对象和对话数组。 我们的 Conversation 类只有一个 ask 函数，输入是一个 string 类型的 question，返回结果也是 string 类型的一条 message。每次调用 ask 函数，都会向 ChatGPT 发起一个请求。在这个请求中，我们会把最新的问题拼接到整个对话数组的最后，得到 ChatGPT 的回答之后再把回答拼接上去。如果回答完之后，发现会话的轮数超过我们设置的 num_of_round，我们就去掉最前面的一轮会话。 因此，我们的 Conversation 类可以帮助我们更好地管理整个聊天过程，并且可以根据我们的需求来定制保留会话轮数的数量。现在，我们可以尝试使用这个 Conversation 类来和聊天机器人进行交互，享受无限畅聊的乐趣。 12345678910111213141516171819prompt = &quot;&quot;&quot;你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字以&quot;&quot;&quot;conv1 = Conversation(prompt, 2)question1 = &quot;你是谁？&quot;print(&quot;User : %s&quot; % question1)print(&quot;Assistant : %s\\n&quot; % conv1.ask(question1))question2 = &quot;请问什么是正当防卫？&quot;print(&quot;User : %s&quot; % question2)print(&quot;Assistant : %s\\n&quot; % conv1.ask(question2))question3 = &quot;那防卫过当呢？&quot;print(&quot;User : %s&quot; % question3)print(&quot;Assistant : %s\\n&quot; % conv1.ask(question3)) 我们给 ChatGPT 下达了指令，让它扮演一位中国律师，使用中文回答问题，每个回答不超过100个字，并设置AI只记住过去三轮对话。 接着，我们按顺序问了三个问题：“您是谁？”、“请问什么是正当防卫？”以及“那防卫过当呢？”。 可以看到，它在回答中表示自己能够回答法律问题，每个回答都在100个字以内。 此外，当我们问它“那防卫过当呢？”时，它也确实记住了上下文，知道我们正在问法律问题，而不是其他信息。为了保证回答充分，我们可以补充一些细节和背景信息，如何定义正当防卫以及何时认为防卫过当等。这些信息将有助于更全面地回答用户的问题。 12345678User : 你是谁？Assistant : 我是一个AI语言模型，专门用于回答各种问题，包括法律问题。User : 请问什么是正当防卫？Assistant : 正当防卫是指在必要时为了保护自己、他人或者国家公共利益而采取的防御行为。在我国法律中，对于正当防卫的情况，法律规定可以免除或减轻犯罪责任。但是，正当防卫也有限制，必须符合法律规定的情形和条件，否则可能构成违法犯罪行为。User : 那防卫过当呢？Assistant : 防卫过当是指在正当防卫行为中，因过度防卫而超出了必要限度，对袭击者造成了严重伤害或者死亡的行为。在我国法律中，防卫过当是不被允许的，因为它已经超出了必要的防卫范围，可能构成过失犯罪或者故意犯罪。如果行为构成犯罪，防卫人需要承担相应的法律责任。 在问完了 3 个问题之后，我们又问了第四个问题，也就是我们最初问它的问题是什么。因为记录了过去 1-3 轮的对话，所以它能正确地回答出我们最初问的是\"你是谁\"。 123456question4 = &quot;我问你的第一个问题是什么？&quot;print(&quot;User : %s&quot; % question4)print(&quot;Assistant : %s\\n&quot; % conv1.ask(question4))# 输出结果User : 我问你的第一个问题是什么？Assistant : 你问的第一个问题是：&quot;请问什么是正当防卫？&quot; 可以看到，回答我们的是“请问什么是正当防卫？”， 这个问题对于我们提问来说似乎是第二个，先不管这个，继续问问看，看看我们设置的次数限制有没有效果。 123456question5 = &quot;我问你的第一个问题是什么？&quot;print(&quot;User : %s&quot; % question5)print(&quot;Assistant : %s\\n&quot; % conv1.ask(question5))# 输出结果User : 我问你的第一个问题是什么？Assistant : 你问的第一个问题是：&quot;那防卫过当呢？&quot; 似乎是剔除了一个最前方的问题，原因是我在问道这个问题的时候，也算是一个新问题。上一轮已经是第四轮了，而我们设置记住的 num_of_round 是 3。在上一轮的问题回答完了之后，AI记录的第一轮的关于\"请问什么是正当防卫？\"的问答，被我们从 ChatGPT 的对话历史里去掉了。所以这个时候，它会告诉我们，第一个问题是\"那防卫过当呢？\"。 聊天的成本 无论是在第 03 讲里，还是这一讲里，我们每次都要发送一大段之前的聊天记录给到 OpenAI。这是由 OpenAI 的 GPT-3 系列的大语言模型的原理所决定的。GPT-3 系列的模型的使用流程非常简单，只需要提供一段文字，它就可以续写下一段内容。因为 OpenAI 为了能够方便地为所有人提供服务，没有在服务器端维护整个对话过程自己去拼接，所以就需要由你来拼接。 实际上，在 ChatGPT 的接口中，虽然把对话分成了一个数组，但最终发送给模型的仍然是拼接到一起的字符串。为了方便用户使用，OpenAI 提供了一个名为 ChatML 的格式，它是 ChatGPT 的 API 的底层实现。OpenAI 根据一个定义好特定分隔符的格式，将提供的多轮对话内容拼接在一起，然后提交给 gpt-3.5-turbo 这个模型。这样，用户就可以轻松地使用 ChatGPT 接口进行对话，而不必担心对话过程中的拼接问题。 12345678910&lt;|im_start|&gt;systemYou are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.Knowledge cutoff: 2021-09-01Current date: 2023-03-01&lt;|im_end|&gt;&lt;|im_start|&gt;userHow are you&lt;|im_end|&gt;&lt;|im_start|&gt;assistantI am doing well!&lt;|im_end|&gt;&lt;|im_start|&gt;userHow are you now?&lt;|im_end|&gt; 注：chatml 的文档里，你可以看到你的对话，就是通过 &lt;|im_start|&gt;system|user|assistant、&lt;|im_end|&gt; 这些分隔符分割拼装的字符串。底层仍然是一个内容续写的大语言模型。 ChatGPT 是一个方便易用的对话模型，但是在使用过程中需要注意一些细节。特别是在需要传达大量上下文信息的情况下，可能需要消耗更多的费用。OpenAI 根据模型处理的 Token 数量来计算费用，但需要注意的是，这种费用是\"双向收费\"的。也就是说，费用是根据发送给 ChatGPT 的上下文和 ChatGPT 返回的内容所包含的 Token 数量总和来计算的。 从模型原理上来看，这种计费方式是合理的。因为每一个 Token，无论是发送给 ChatGPT 的还是 ChatGPT 返回的，都需要通过 GPU 或 CPU 进行计算。因此，你发送的上下文越长，ChatGPT 消耗的资源也会越多。但是在实际使用中，你可能会感到疑惑。比如，你进行了 10 轮对话，一共发送了 1000 个 Token，那么你可能会认为只需要支付 1000 个 Token 的费用。然而实际上，由于每一轮对话都需要发送之前的上下文，因此第二轮需要 200 个 Token，第三轮需要 300 个 Token，以此类推。因此，10 轮对话所需要的 Token 数量是 5500，而不是之前估算的 1000。 如果你要开发 ChatGPT 应用，计算 Token 的数量是非常重要的。下面是一段示例代码，可以帮助你计算在 ChatGPT 对话模型下所消耗的 Token 数量。 *通过 API 计算 Token 数量* 第一种计算 Token 数量的方式，是从 API 返回的结果里面获取。当我们调用 API 时，我们可以从返回的结果中获取 Token 的数量。这个数量代表了这次请求消耗的 Token 数。我们可以利用这个信息来计算出整个会话过程中使用的 Token 数量。为了实现这个计算方式，我们需要修改一下刚才的 Conversation 类。我们可以重新创建一个 Conversation2 类来实现这个功能。这个新类和之前的类只有一个不同，就是 ask 函数除了返回回复的消息之外，还会返回这次请求消耗的 Token 数。这样我们就可以更好地掌握整个会话过程中的 Token 使用情况，从而更好地优化我们的代码和算法。 12345678910111213141516171819202122232425262728class Conversation2: def __init__(self, prompt, num_of_round): self.prompt = prompt self.num_of_round = num_of_round self.messages = [] self.messages.append({&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: self.prompt}) def ask(self, question): try: self.messages.append( {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: question}) response = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=self.messages, temperature=0.5, max_tokens=2048, top_p=1, ) except Exception as e: print(e) return e message = response[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;] num_of_tokens = response['usage']['total_tokens'] self.messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: message}) if len(self.messages) &gt; self.num_of_round*2 + 1: del self.messages[1:3] return message, num_of_tokens 然后我们还是问一遍之前的问题，看看每一轮问答消耗的 Token 数量。 12345conv2 = Conversation2(prompt, 3)questions = [question1, question2, question3, question4, question5]for question in questions: answer, num_of_tokens = conv2.ask(question) print(&quot;询问 {%s} 消耗的token数量是 : %d&quot; % (question, num_of_tokens))输出结果： 输出结果： 12345询问{你是谁？} 消耗的Token数量是： 113询问{请问什么是正当防卫？} 消耗的Token数量是： 302询问{那防卫过当呢？} 消耗的Token数量是： 516询问{我问你的第一个问题是什么？} 消耗的Token数量是： 549询问{我问你的第一个问题是什么？} 消耗的Token数量是： 546 根据我们的数据，我们可以看出前几轮的 Token 消耗数量在逐渐增多，这表明 ChatGPT 能在前几轮对话中提供更多的信息和回答。然而，最后 3 轮的 Token 消耗数量保持不变。这是因为我们代码里只使用过去 3 轮的对话内容向 ChatGPT 发起请求。我们可以考虑增加对话轮数以便更好地利用 ChatGPT 的能力，同时也需要注意 Token 消耗量的控制。 *通过 Tiktoken 库计算 Token 数量* 第二种方式是使用Python库Tiktoken对文本进行分词并计算Token数量。这种方法在上一讲中已经提到过。 需要注意的是，使用不同的GPT模型需要对应不同的Tiktoken编码器模型。对应文档可以在以下链接中查询：https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb 我们所使用的ChatGPT模型采用的是cl100k_base的编码器。我们也可以用它计算第一轮对话使用的Token数量，以此来丰富我们的文本内容。 在计算Token数量方面，我们可以试着定义不同的计算方法，例如考虑标点符号的使用、停用词的去除、Token的长度等等。这些方法可以更加准确地计算Token数量，并且有助于我们更好地理解文本内容。 12345678910111213import tiktokenencoding = tiktoken.get_encoding(&quot;cl100k_base&quot;)conv2 = Conversation2(prompt, 3)question1 = &quot;你是谁？&quot;answer1, num_of_tokens = conv2.ask(question1)print(&quot;总共消耗的token数量是 : %d&quot; % (num_of_tokens))prompt_count = len(encoding.encode(prompt))question1_count = len(encoding.encode(question1))answer1_count = len(encoding.encode(answer1))total_count = prompt_count + question1_count + answer1_countprint(&quot;Prompt消耗 %d Token, 问题消耗 %d Token，回答消耗 %d Token，总共消耗 %d Token&quot; % (prompt_count, question1_count, answer1_count, total_count)) 输出结果： 12总共消耗的token数量是 : 113Prompt消耗 66 Token, 问题消耗 5 Token，回答消耗 29 Token，总共消耗 100 Token 我们通过 API 获得了消耗的 Token 数，然后又通过 Tiktoken 分别计算了 System 的指示内容、用户的问题和 AI 生成的回答，发现了两者还有小小的差异。这个是因为，我们没有计算 OpenAI 去拼接它们内部需要的格式的 Token 数量。很多时候，我们都需要通过 Tiktoken 预先计算一下 Token 数量，避免提交的内容太多，导致 API 返回报错。 为了更好地解决这个问题，我们可以使用一种称为“Tokenization”的技术。这种技术可以将一段文本分割成单个的词语或符号，这些词语或符号被称为Token。通过计算每个Token的数量，我们可以更准确地预测 API 的返回值。此外，我们还可以使用另一种技术，称为“Compression”，通过压缩文本以减少Token的数量。这样可以大大减少提交的内容，提高API的速度和准确性。 使用Gradio搭建一个聊天界面 我们已经有了一个封装好的聊天机器人，但是它现在只能在 Python Notebook 中使用，每次需要通过调用代码来提问。因此，我们接下来将为我们封装好的 Convesation 接口开发一个界面，并添加一些新的功能。 我们将使用 Gradio 这个 Python 库来开发这个聊天机器人的界面，因为它有以下几个优点。首先，我们现有的代码都是使用 Python 实现的，所以你不需要再去学习 JavaScript、TypeScript 以及相关的前端框架。其次，Gradio 渲染出来的界面可以直接在 Jupyter Notebook 中显示出来，这对于不了解技术的同学来说，大大减轻了其他环境搭建的负担。此外，Gradio 这个公司已被目前最大的开源机器学习模型社区 HuggingFace 收购了。你可以免费将 Gradio 的应用部署到 HuggingFace 上。接下来，我将教你如何部署，你可以将你自己开发的聊天机器人部署上去，让你的朋友们也能使用。最后，值得一提的是，在后面的课程中，我们有时也会使用一些开源的模型，而这些模型往往也托管在 HuggingFace 上。因此，使用 HuggingFace Gradio 的部署方式，特别方便我们演示给其他人看。 注：Gradio 官方也有用其他开源预训练模型创建 Chatbot 的教程https://gradio.app/creating-a-chatbot/ 首先，让我们先安装所需的包： 1conda install -c conda-forge gradio Gradio 应用的代码我也列在了下面，对应的逻辑也非常简单。首先，我们定义好了 system 这个系统角色的提示语，创建了一个 Conversation 对象。 然后，我们为了更好的用户体验，对 answer 方法进行了进一步封装。在这个新的版本中，我们添加了更多的逻辑，使得 Chatbot 能够更好地处理用户的输入。具体来说，我们加入了一个新的函数，用于判断用户输入的语句是否是一个问题。如果是一个问题，我们会对其进行解答，并返回一个对应的答案。如果不是一个问题，我们会将其添加到历史记录中。 此外，我们还对 Conversation 类进行了扩展，增加了一个名为“recommend”的方法。该方法可以根据用户的历史记录，为用户推荐一些相关的话题。这样可以增强 Chatbot 的交互性和趣味性。 最后，我们通过一段 with 代码，创建了对应的聊天界面。Gradio 提供了一个现成的 Chatbot 组件，我们只需要调用它，然后提供一个文本输入框就好了。这样用户就可以通过输入文字与 Chatbot 进行交互，获取他们想要的信息。 123456789101112131415161718192021222324252627import gradio as grprompt = &quot;&quot;&quot;你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字以&quot;&quot;&quot;conv = Conversation(prompt, 10)def answer(question, history=[]): history.append(question) response = conv.ask(question) history.append(response) responses = [(u,b) for u,b in zip(history[::2], history[1::2])] return responses, historywith gr.Blocks(css=&quot;#chatbot{height:300px} .overflow-y-auto{height:500px}&quot;) as demo: chatbot = gr.Chatbot(elem_id=&quot;chatbot&quot;) state = gr.State([]) with gr.Row(): txt = gr.Textbox(show_label=False, placeholder=&quot;Enter text and press enter&quot;).style(container=False) txt.submit(answer, [txt, state], [chatbot, state])demo.launch() 你直接在 Colab 或者你本地的 Jupyter Notebook 里面，执行一下这一讲到目前的所有代码，就得到了一个可以和 ChatGPT 聊天的机器人了。 你可以直接在Jupyter中使用，当然，程序也提供了一个本地地址供你使用，如下图： VSCode Jupyter内： 浏览器上： 部署到HuggingFace上 如果你拥有一个聊天机器人，你肯定会想和你的朋友一起分享这个惊奇的机器人。为了让你的朋友也能使用这个机器人，我们建议你将它部署到 HuggingFace 上。 首先，如果你还没有 HuggingFace 的账号，你需要先注册一个账号。选择Spaces之后，然后选择 \" Create New Space\" 创建一个新的项目空间。接下来，你需要将机器人的代码上传到你的项目空间中。 在接下来的界面里，你需要为你的 Space 取一个名字。这个名字应该是能够代表你的项目的，也应该是容易记忆的。你可以将你的 Space 命名为“gradio”，或者其他什么奇奇怪怪的名字。License随便选择一个协议，或者跟我一样选择MIT。在 Select the Space SDK 里面，你需要选择第二个 Gradio。这个 SDK 是一个简单易用的工具，能够帮助你快速地构建和部署你的应用程序。如果你对编程不是很熟悉，这个 SDK 就更加适合你了。在选择硬件方面，我们建议选择免费的选项。这样可以避免花费不必要的成本，同时也能够让你更加专注于开发你的应用程序。在选择项目的时候，我们建议选择 public。这样其他人就可以看到你的项目，给你提供反馈和建议。不过需要注意的是，public 的 space，是连你后面上传的代码也能够看到的。因此，你需要确保你上传的代码是没有敏感信息的，同时也需要遵守相关的法律法规。一切都准备好之后，选择最下方的Create Space按钮吧。 创建成功后，会跳转到 HuggingFace 的 App 界面。在这个界面里，你可以找到如何使用当前 space 的方法，包括克隆 Clone 当前的 space 和提交代码部署 App 等。为了方便这个过程，我们只需要通过 Git 把当前 space 下载下来，然后提交两个文件就可以了。这两个文件对于 App 的部署非常重要。第一个文件包含了 App 的所有必要文件，第二个文件则包含了 App 的所有依赖。这些文件都需要在部署前进行正确的配置，以确保 App 能够顺利地运行。 app.py 包含了我们的 Gradio 应用； requirements.txt 包含了这个应用依赖的 Python 包，这里我们只依赖 OpenAI 这一个包。 下载这个Git到本地，如果本地没有这两个文件，记得自己创建一下，然后将我们刚才的代码粘贴到app.py内，最后全部提交: 还记得咱们之前提到过，选择Public之后，不仅App，甚至于代码都是公开的吗？那么我们就不能在其中放入我们的Api Key这种敏感内容了，所以我们需要在HuggingFace的Space内设置一下这个环境变量。 你可以点击界面里面的 Settings，然后往下找到 Repository secret，点击New secret 在 Name 这里输入 OPENAI_API_KEY，然后在 Secret value 里面填入你的 OpenAI 的密钥。 最后，添加完成之后记得要重启一下Space 好啦，这个时候，你可以重新点击 App 这个 Tab 页面，试试你的聊天机器人是否可以正常工作了。 因为本人比较吝啬，也就不提供免费的测试Demo了，文中用于演示的Demo也一并删除了，但是 app.py这个在Github上可以找到。 有兴趣的可以去徐文浩的Spaces空间去测试下：https://huggingface.co/spaces/xuwenhao83/simple_chatbot 小结 希望通过这一讲，你已经学会了怎么使用 ChatGPT 的接口来实现一个聊天机器人了。在我们实现只保留固定轮数的对话的过程中，我们发现这样的限制可以防止聊天机器人在对话的过程中变得混乱。我们也了解到了通过在对话过程中一直发送所有上下文的原因，这使得聊天机器人能够更好地理解用户的意图和需求。 此外，我们还学习了如何使用 Gradio 这个库来创建一个聊天机器人界面。通过使用 Gradio，我们可以轻松地为我们的聊天机器人提供一个直观的界面，使得用户可以更方便地与聊天机器人进行交互。 最后，我们将这个简单的聊天机器人部署到了 HuggingFace 上。这为我们提供了一个可靠的平台，可以与他人分享我们的聊天机器人，并在他们的设备上使用。希望你可以在这个过程中享受到学习的乐趣，以及尝试着将这个聊天机器人进一步优化和扩展。 课后练习 在这一讲里，我们讨论的是 Chatbot 的维护能力。目前 Chatbot 只能维护过去 N 轮的对话，这意味着如果对话很长的话，我们一开始对话的信息就被丢掉了。但是，我们可以采用一种不设定轮数，只限制传入的上下文的 Token 数量的方式来解决这个问题。这种方式可以让 Chatbot 尽可能多地记住上下文，同时在 Token 数量超标的时候再删减最开始的对话。 要实现这个功能，你可以根据这一讲学到的内容，修改代码，让聊天机器人在不限制轮数的情况下，能够保存尽可能多的上下文信息。除了“忘记”开始的几轮，你还能想到什么办法让 AI 记住更多的上下文信息吗？ 期待能在评论区看到你的思考，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。","link":"/Quickly-build-an-AI-application/"},{"title":"SQL练习1","text":"-- UPPER 是转换大写的函数 1SELECT emp_name,salary * 12, UPPER(email) FROM employee; -- 使用别名 1234SELECT e.emp_name AS &quot;姓名&quot;, salary * 12 AS &quot;12月的工资&quot;, UPPER(email) &quot;电子邮箱&quot;FROM employee AS e; -- 无表查询 1234567SELECT 1+1;-- Oracle 实现，dual只有一个字段且只包含一行数据SELECT 1+1FROM dual;SELECT * FROM employee WHERE emp_name = '刘备'; -- 比较运算符 123select * from employee where sex &lt;&gt; '男'select * from employee where salary BETWEEN 5000 and 7000select * from employee where emp_name IN('刘备') -- 一个时间段之后入职 1234567select emp_name, hire_date from employee where hire_date &gt; DATE '2018-01-01'SELECT emp_name,hire_date,manager from employee where manager IS NULLSELECT emp_name, sex, salary from employee where sex = '女' AND salary &gt; 10000SELECT emp_name, sex, salary FROM employee WHERE emp_name = '刘备' OR emp_name = '张飞' OR emp_name = '赵云' -- 短路运算 short-circuit evaluation 12345select 'AND' FROM employee WHERE 1 = 0 AND 1/0 = 1;SELECT 'OR' FROM employee where 1 = 1 OR 1/0 = 1;-- NOTselect emp_id,emp_name FROM employee WHERE emp_name NOT IN('刘备','张飞','赵云') -- 运算符优先级 123SELECT emp_name,dept_id,bonusFROM employeeWHERE (dept_id = 2 OR dept_id = 3) AND bonus IS NOT NULL; -- 去处重复值 12SELECT DISTINCT SEX FROM employeeSELECT DISTINCTROW sex from employee / 查找 2018 年 1 月 1 日之后入职，月薪小于 5000，并且奖金小于 1000（包括没有奖金）的员工。 / 12345SELECT emp_id,emp_name,salary,hire_date,bonus FROM employee WHERE hire_date &gt; '2018-01-01' AND salary &lt; 5000 AND (bonus &lt; 1000 OR bonus IS NULL) -- LIKE运算符 1234567SELECT emp_id,emp_name,sexFROM employeeWHERE emp_name LIKE '赵%'SELECT emp_name,emailFROM employeeWHERE email NOT LIKE 'dengzh_@shuguo.com'; -- 转义字符 1234567891011CREATE TABLE t_like(c1 VARCHAR(20))INSERT INTO t_like(c1) VALUES ('进度:25% 已完成')INSERT INTO t_like(c1) VALUES ('日期:2019年5月25日')SELECT c1FROM t_likeWHERE c1 LIKE &quot;%25\\%%&quot;SELECT c1FROM t_likeWHERE c1 LIKE &quot;%25#%%&quot; ESCAPE '#' -- 大小写匹配 123SELECT emp_name,emailFROM employeeWHERE email LIKE 'M%' -- 正则表达式 判断邮箱 1234567891011/*以字母或者数字开头；后面是一个或者多个字母、数组或特殊字符（ . _ - ）；然后是一个 @ 字符；之后包含一个或者多个字母、数组或特殊字符（ . - ）；最后是域名，即 . 以及 2 到 4 个字母。^[a-zA-Z0-9]+[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}$*/SELECT email FROM t_regexpWHERE REGEXP_LIKE (email, BINARY '^[a-z0-9]+[a-z0-9._-]+@[a-z0-9.-]+\\\\.[a-z]{2,4}$','i'); -- 降序排序 1234select emp_name,salary,hire_datefrom employeewhere dept_id = 4ORDER BY salary DESC; -- 多列排序(工资，入职先后) 1234select emp_name,salary,hire_dateFROM employeewhere dept_id = 4ORDER BY salary DESC, hire_date; -- 按照SELECT顺序 1234SELECT emp_name, salary, hire_dateFROM employeeWHERE dept_id = 4ORDER BY 2 desc, 3 中文排序 -- CONVERT 是一个函数，用于转换数据的字符集编码。以下转换为中文GBK字符集 1234SELECT emp_namefrom employeeWHERE dept_id = 4ORDER BY CONVERT(1 USING GBK) -- 空值排序 1234select emp_name,bonusfrom employeewhere dept_id = 2ORDER BY 2 -- 其他空值排序方法 12345-- COALESCE函数将控制转换为一个指定的值SELECT emp_name, COALESCE(bonus,0) AS bonusFROM employeewhere dept_id = 2ORDER BY COALESCE(bonus,0); / 第六节练习: 查询所有的员工信息，按照员工的总收入（年薪加奖金）从高到低进行排序，总收入相同再按照姓名的拼音顺序排序。 / 1234SELECT emp_name, salary, bonus, (salary+bonus) as sumFROM employeeWHERE dept_id &gt;= 0ORDER BY (salary+bonus) DESC, CONVERT(1 USING GBK) -- TopN 排行榜 123456789101112131415161718-- 标准FETCH语法,此语法MySQL不支持，Oracle, PostgreSQL支持SELECT emp_name, salaryFROM employeeORDER BY salary DESCOFFSET 0 ROWSFETCH FIRST 5 ROWS ONLY;-- LIMIT实现TOPN排行榜SELECT emp_name, salaryFROM employeeORDER BY salary DESCLIMIT 5 OFFSET 0;-- 第二种写法SELECT emp_name,salaryFROM employeeORDER BY salary DESCLIMIT 0,5 -- SQL 实现分页查询 1234SELECT emp_name, salaryFROM employeeORDER BY salary DESCLIMIT 10,5 -- 员工排名第3高 123456789select emp_name,salary FROM employeeWHERE salary = ( select salary from employee ORDER BY salary DESC LIMIT 2,1)SELECT emp_name,salary FROM employeeORDER BY salary desc limit 5 offset 10; / 练习：使用LIMIT和OFFSET找出员工表中月薪排名第三高的所有员工 / 123456select emp_name, salary FROM employeewhere salary = ( select salary FROM employee ORDER BY salary desc limit 1 offset 2)","link":"/SQL_ext_1/"},{"title":"去掉wp后台编辑的文章预览","text":"在网上搜索此类教程。我的目的比较简单，就是想加快的速度，没有出于安全问题考虑。不过这个问题要摆出来确实蛮吓人的啊。： 前面提到因为 WordPress 2.0 的编辑预览功能使用 iframe，与我使用的一个js 代码冲突，一进入编辑窗口就会跳出来。liyuanzao 告诉我可以使用 is_preview() 这个函数，我试了试发现不行。Google 了一下，发现还是个大问题。 因为首先搜到的是这篇文章： AdSense on WordPress 2.0，大体的中文翻译在这儿： 在 WordPress 2.0 使用 AdSense 注意之事。讲 在 WordPress 2.0 中有一個很酷的功能名叫 Post Preview，即是在 edit / view draft 時，在文章下方會顯示一個 preview 畫面，內裡會顯示這篇文章在發表時的樣子，但如果有使用 Goolge AdSense，當 preview 時都會直接讀取 Google AdSense 廣告，但因為文章還未發表，如果跟著 Google 嘗試尋找這篇文章時，就會出現 404 Not found 的情況，有可能會被 Google 暫停你的 AdSense 戶口。 这是不是个潜在的大危险？文章中提到了使用 is_preview() 来解决，但是不少人和我一样发现这其实并不行。有人已经到 WordPress 报了bug，问题的表现说得很清楚。 如果实在着急，可以去掉预览功能，WP 支持论坛上给出了方法：删除 /wp-admin/post.php 中 82 到 87 行。 原文地址 啊。。下边来说重点，不管是为了自己的编辑速度还是安全方面考虑，总之我们是要去掉文章预览功能是吧。其实这是一个鸡肋功能，本来编辑器就是所见即所得了。这样等于在打开后台编辑器的同时又打开了一次主页面。。。去掉的办法上边有说，第82到87行，其实我看了一下并不是这几行。不知道是不是因为版本不对，所以有些出入。我的wp是2.0.4的。。。第82到85行删除就OK了。。具体代码是： &lt;div id='preview' class='wrap'&gt; &lt;h2 id=\"preview-post\"&gt;&lt;?php _e('Post Preview (updated when post is saved)'); ?&gt; &lt;small class=\"quickjump\"&gt;&lt;a href=\"#write-post\"&gt;&lt;?php _e('edit &amp;uarr;'); ?&gt;&lt;/a&gt;&lt;/small&gt;&lt;/h2&gt; &lt;iframe src=\"&lt;?php echo add_query_arg('preview', 'true', get_permalink($post-&gt;ID)); ?&gt;\" width=\"100%\" height=\"600\" &gt;&lt;/iframe&gt; &lt;/div&gt;","link":"/Remove-the-wp-background-edit-the-article-preview/"},{"title":"08 改写和审核","text":"Hi, 我是茶桁。 我们已经介绍了 OpenAI 的主要接口。这是基础知识系列的最后一讲，我们将讨论 OpenAI GPT 系列模型的其他接口。你可能不会经常使用其中一些接口，但了解它们不会有任何坏处，说不定你会在某些需求中用到它们。 在这篇文章中，我们将一起探讨 OpenAI 为文本改写和内容审核提供的功能，以及 GPT 系列模型的种类、区别和应用场景。 文本改写教程 我猜你可能已经用过许多基于 AI 大型语言模型的产品了。其中很常见的一类应用是写作助手，比如 Notion AI。它可以帮助你在文章中选择一段内容，并让 AI 帮你修改它，例如缩短文本或改变语气等。 OpenAI的GPT系列模型是一个生成式模型，给它一段文字，它可以补全后面的文字。要修改一段内容，可以通过提示语来解决问题。下面这段代码是通过ChatGPT模型实现了这个功能。 1234567891011121314151617181920212223242526272829import openaiimport osopenai.api_key = 'OPENAI_API_KEY'def make_text_short(text): messages = [] messages.append( {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一个用来将文本改写得短的AI助手，用户输入一段文本，你给出一段意思相同，但是短小精悍的结果&quot;}) messages.append( {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: text}) response = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=messages, temperature=0.5, max_tokens=2048, presence_penalty=0, frequency_penalty=2, n=3, ) return responselong_text = &quot;&quot;&quot;我们可以回顾下它走的一些关键路程：GPT 1.0走的是生成模式的自回归语言模型路线，比Bert出来的还早些。Bert证明了：双向语言模型对于很多NLP理解类任务，效果比自回归这种单向语言模型效果更好。尽管如此，GPT 2.0并没有因此切换到双向语言模型这条路上，仍然走文本生成的路，而且开始尝试零示例（zero shot）prompt和少量示例（few shot）prompt。其实这时候， OpenAI心目中的AGI已经开始浮出水面，逐渐显示出轮廓了。只是因为zero shot/few shot效果比Bert+fine-tuning差的比较远，所以大家都没太当回事，甚至不理解它为什么要始终坚持走单向语言模型的路线。这个时候，我估计即使是OpenAI自己，也不一定能确保这条路肯定能走通。&quot;&quot;&quot;short_version = make_text_short(long_text)index = 1for choice in short_version[&quot;choices&quot;]: print(f&quot;version {index}: &quot; + choice[&quot;message&quot;][&quot;content&quot;]) index += 1 输出结果： 123version 1: GPT 1.0和Bert是NLP领域的里程碑，前者采用了自回归语言模型，后者证明双向语言模型更优。不过GPT 2.0仍然坚持文本生成路线，并尝试了零/少量示例prompt。OpenAI心目中的AGI开始显现轮廓，但由于效果不如Bert+fine-tuning而被忽视。这条路是否可行并不能确定。version 2: GPT 1.0采用了生成模式的自回归语言模型路线，比Bert更早。Bert证明了双向语言模型在NLP理解类任务上效果更好，但GPT 2.0仍然坚持文本生成路线，并尝试了零/少量示例prompt。OpenAI心目中的AGI逐渐浮出水面，但zero shot/few shot效果不如Bert+fine-tuning, 这条路是否可行还需观察。version 3: GPT 1.0采用自回归语言模型，比Bert早。Bert证明双向语言模型效果更好，但GPT 2.0仍然走文本生成路线，并尝试了零/少量示例prompt。OpenAI的AGI已经开始浮出水面，只是因为zero/few shot效果差而被忽视。即使OpenAI也不能保证这条路一定能成功。 我们使用 ChatGPT 的模型接口，因为它比较便宜。我们使用了以下参数： n=3，让 AI 给我们返回 3 个答案供我们选择。在文本改写类的应用里面，我们通常不只是直接给出答案，而是会给用户几个选项来选择。 presence_penalty=0、 frequency_penalty=2。这两个参数和 temperature 参数类似，都是来控制你输出的内容的。 presence_penalty 指的是如果一个 Token 在前面的内容已经出现过了，那么在后面生成的时候给它的概率一定的惩罚； frequency_penalty 指的是对于重复出现的 Token 进行概率惩罚。这样，AI 就会尽量使用不同的表述。 通过 logit_bias 参数精准控制内容 虽然 temperature 、 presence_penalty 和 frequency_penalty 等参数都可以控制生成文本的风格，但是它们都只是单一的参数，无法精确控制不想出现的词汇。不过，OpenAI 提供了方法来解决这个问题。例如，如果我们不想让“灾害”这两个字出现在生成的内容中，可以采取如下措施。 1234567891011121314151617181920212223242526import tiktokenencoding = tiktoken.get_encoding('p50k_base')token_ids = encoding.encode(&quot;灾害&quot;)print(token_ids)bias_map = {}for token_id in token_ids: bias_map[token_id] = -100def make_text_short(text): messages = [] messages.append( {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一个用来将文本改写得短的AI助手，用户输入一段文本，你给出一段意思相同，但是短小精悍的结果&quot;}) messages.append( {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: text}) response = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=messages, temperature=0.5, max_tokens=2048, n=3, presence_penalty=0, frequency_penalty=2, logit_bias = bias_map, ) return responseshort_version = make_text_short(long_text)index = 1for choice in short_version[&quot;choices&quot;]: print(f&quot;version {index}: &quot; + choice[&quot;message&quot;][&quot;content&quot;]) index += 1 输出结果 1234[163, 223, 122, 22522, 111]version 1: GPT 1.0是生成模式的自回归语言模型，比Bert还早。Bert证明了双向语言模型效果更好，但GPT 2.0仍然坚持文本生成路线，并尝试零/少量示例prompt。这时OpenAI的AGI开始浮出水面，只是因为效果不如Bert+fine-tuning而被忽视。version 2: GPT 1.0是自回归语言模型，比Bert早。Bert证明了双向语言模型效果更好，但GPT 2.0仍然选择文本生成，并尝试了零/少量示例prompt。OpenAI的AGI开始浮出水面，但因为zero/few shot效果差被忽视。这条路是否可行不确定。version 3: GPT 1.0采用生成模式自回归语言模型，比Bert更早。Bert证明了双向语言模型的效果比自回归好，但GPT 2.0仍然坚持文本生成路线，并尝试零/少量示例prompt。OpenAI心目中的AGI已经开始浮出水面，只是因为zero shot/few shot效果不如Bert+fine-tuning而被忽视。 我们使用 Tiktoken 库，找到了“灾害”对应的 Token，并赋予它们 -100 的 bias，以避免这个词出现在生成的回复中。我们将整个 bias_map 作为参数传递给 Completion 的 logit_bias 参数。现在，生成的三个回复都不包含“灾害”这个词了，即使之前的第一个回复中也包含了。现在，“灾”这个字被强制改成了繁体字，而另一个则出现了错别字“宣”。 logit_bias 参数的取值范围为 -100 到 100 之间，但通常设置在 1 到 -1 之间就足够了。如果将其设置为 100，则必须出现某些字，整个生成过程会变得非常缓慢，难以忍受。 用英文减少 Token 使用 虽然“灾害”只有两个中文字，但在使用 Tiktoken 处理时，我们打印了对应的 Token ID，实际上需要使用5个 Token。同样的，中文中含义相同的词语，所需的 Token 数量比英文多。例如，将上面的句子翻译成英文，与中文相同的内容所需的 Token 数量进行比较。 1234567891011121314151617def translate(text): messages = [] messages.append( {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一个翻译，把用户的话翻译成英文&quot;}) messages.append( {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: text}) response = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=messages, temperature=0.5, max_tokens=2048, n=1 ) return response[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]chinese = long_textenglish = translate(chinese)num_of_tokens_in_chinese = len(encoding.encode(chinese))num_of_tokens_in_english = len(encoding.encode(english))print(english)print(f&quot;chinese: {num_of_tokens_in_chinese} tokens&quot;)print(f&quot;english: {num_of_tokens_in_english} tokens&quot;) 输出结果： 123We can review some of its key milestones: GPT 1.0 followed the path of a generative pattern of autoregressive language model, which came out earlier than Bert. Bert proved that for many NLP understanding tasks, the effect of bidirectional language models is better than that of unidirectional language models like autoregressive. Nevertheless, GPT 2.0 did not switch to the bidirectional language model, but continued to follow the path of text generation, and began to try zero-shot and few-shot prompts. Actually, at this point, OpenAI's AGI had already begun to emerge and gradually showed its outline. It's just that because the effect of zero-shot/few-shot is far worse than Bert+fine-tuning, everyone didn't pay much attention to it, and even didn't understand why it always insisted on the path of unidirectional language models. At this point, I estimate that even OpenAI itself cannot guarantee that this path will definitely work.chinese: 589 tokensenglish: 208 tokens 同样的内容，在中文中要消耗超过 589个 Token，而英文只有 208。在使用 OpenAI 的接口时，最好使用英语提示语，以节约成本。当然，在输出结果时可以使用 \" generate Chinese\" 等提示。但是，我们将在后面的课程演示中尽量使用中文，以方便您的理解。 OpenAI 的模型 OpenAI 曾经提供过一个Edit 接口，并单独提供了文本编辑模型。目前，这个接口和模型不能使用，可能因为是 Alpha 版本或已被下线。 由于 OpenAI 的产品更新非常快，所以最好的办法是通过它提供的接口查看可用模型，以便选择效果最好或最新的模型。 12345import pandas as pd# list all open ai modelsengines = openai.Engine.list()pd = pd.DataFrame(openai.Engine.list()['data'])display(pd[['id', 'owner']]) 输出结果： 123456789101112# 为了文本长度，仅展示了部分，中间大部分省略掉了。可以去查看我源代码，里面有展示。id owner0 whisper-1 openai-internal1 babbage openai2 davinci openai3 text-davinci-edit-001 openai4 babbage-code-search-code openai-dev5 text-similarity-babbage-001 openai-dev6 code-davinci-edit-001 openai7 text-davinci-001 openai...49 davinci-similarity openai-dev 截止到目前为止，输出结果中有 49 个模型。这些模型的名称已经很直观了，比如 text-similarity-babbage-001 用于相似度匹配，适合用于零样本分类，而 text-search-davinci-doc-001 更适合用于文档搜索。虽然一些模型标记为 openai-dev 或 openai-internal , 但它们仍然可用。例如，在第 02 讲中使用 get_embedding 方法获取向量时，后台使用的是 text-similarity-davinci-001 模型，这也是 openai-dev 的模型之一。虽然其中许多模型已经过时，但实际上只需要关注几类主要模型即可。GPT-4 家族的模型，包括 gpt-4 和 gpt-4-0314。使用方式和 ChatGPT 的模型一样，带日期的模型是模型快照，不会随时间迁移不断更新。GPT-4 的模型现在还很昂贵，输入 1000 个 Token 需要 0.03 美分，生成 1000 个 Token 则需要 0.06 美分。通常我使用它来写代码，准确率较高。 GPT-3.5 家族的模型，包括 ChatGPT 所使用的 gpt-3.5-turbo 或者 gpt-3.5-turbo-0301，以及 text-davinci-003 和 text-davinci-002 这两个模型。前者专门针对对话的形式进行了微调，并且价格便宜，无论输入输出，1000 个 Token 都只需要 0.002 美分。后两个里，003 的模型有一个特殊功能，支持\"插入文本\"。003 是基于强化学习微调的，而 002 则是监督学习下微调的。text-davinci-003 和 002 模型比 3.5-turbo 贵 10 倍，但输出更稳定。你可以根据自己需要来决定。 Ada、Babbage、Curie 以及 Davinci 这四个基础模型只适合下达单轮指令，不适合考虑复杂上下文和进行逻辑推理。这四个模型按首字母排序，价格越来越贵，效果越来越好。如果要微调一个属于自己的模型，需要基于这四个基础模型。 text-embedding-ada-002、text-similarity-ada-001 这些专门用途模型通常用于获取 Embedding，用于其他机器学习模型的训练或语义相似度比较。 所有模型的名字都来自科学史上的名人。Ada 来自人类史上第一位程序员 Ada，她也是著名诗人拜伦的女儿。Babadge 则是设计了分析机的巴贝奇，巴贝奇分析机也被认为是现代计算机的前身。Curie 指居里夫人，Davinci 是指达芬奇。 可以挑几个模型试一下它们 Embedding 的维度数量，就知道模型的尺寸不同。 123456789101112131415161718192021from openai.embeddings_utils import get_embeddingtext = &quot;让我们来算算Embedding&quot;embedding_ada = get_embedding(text, engine=&quot;text-embedding-ada-002&quot;)print(&quot;embedding-ada: &quot;, len(embedding_ada))similarity_ada = get_embedding(text, engine=&quot;text-similarity-ada-001&quot;)print(&quot;similarity-ada: &quot;, len(similarity_ada))babbage_similarity = get_embedding(text, engine=&quot;babbage-similarity&quot;)print(&quot;babbage-similarity: &quot;, len(babbage_similarity))babbage_search_query = get_embedding(text, engine=&quot;text-search-babbage-query-001&quot;)print(&quot;search-babbage-query: &quot;, len(babbage_search_query))curie = get_embedding(text, engine=&quot;curie-similarity&quot;)print(&quot;curie-similarity: &quot;, len(curie))davinci = get_embedding(text, engine=&quot;text-similarity-davinci-001&quot;)print(&quot;davinci-similarity: &quot;, len(davinci)) 输出结果： 123456embedding-ada: 1536similarity-ada: 1024babbage-similarity: 2048search-babbage-query: 2048curie-similarity: 4096davinci-similarity: 12288 ada-similarity 的最小维度为 1024， davinci-similarity 的最大维度为 12288。因此，它们的价格和效果有所不同。 GPT 也能插入内容 text-davinci-003 模型有一个特殊功能：插入文本。你可以使用这个功能来改写文本。 123456789101112131415161718prefix = &quot;&quot;&quot;在这个快节奏的现代社会中，我们每个人都面临着各种各样的挑战和困难。在这些挑战和困难中，有些是由外部因素引起的，例如经济萧条、全球变暖和自然灾害等。\\n&quot;&quot;&quot;# 还有一些是由内部因素引起的，例如情感问题、健康问题和自我怀疑等。suffix = &quot;&quot;&quot;\\n面对这些挑战和困难，我们需要采取积极的态度和行动来克服它们。这意味着我们必须具备坚韧不拔的意志和创造性思维，以及寻求外部支持的能力。只有这样，我们才能真正地实现自己的潜力并取得成功。&quot;&quot;&quot;def insert_text(prefix, suffix): response = openai.Completion.create( model=&quot;text-davinci-003&quot;, prompt=prefix, suffix=suffix, max_tokens=1024, ) return responseresponse = insert_text(prefix, suffix)print(response[&quot;choices&quot;][0][&quot;text&quot;]) 输出结果： 1此外，还有一些挑战和困难是由自我内在的原因导致的，比如事业上的恐惧和沮丧，以及担心无法达到完美标准等。 使用这个接口和普通的 Completion 接口基本相同，唯一的区别在于除了前缀的 prompt 参数之外，还需要一个后缀的 suffix 参数。 需要注意的是插入内容的提示语。如果我们稍微修改上面的内容，例如去掉 Suffix 一开始的换行符，插入的文本内容可能会不如预期。 123456789prefix = &quot;&quot;&quot;在这个快节奏的现代社会中，我们每个人都面临着各种各样的挑战和困难。在这些挑战和困难中，有些是由外部因素引起的，例如经济萧条、全球变暖和自然灾害等。\\n&quot;&quot;&quot;# 还有一些是由内部因素引起的，例如情感问题、健康问题和自我怀疑等。suffix = &quot;&quot;&quot;面对这些挑战和困难，我们需要采取积极的态度和行动来克服它们。这意味着我们必须具备坚韧不拔的意志和创造性思维，以及寻求外部支持的能力。只有这样，我们才能真正地实现自己的潜力并取得成功。&quot;&quot;&quot;response = insert_text(prefix, suffix)print(response[&quot;choices&quot;][0][&quot;text&quot;]) 输出结果： 12而另一些则是由内在因素引起的，比如性格、思想、管理方式或者技能缺乏等。无论是外部因素还是内在因素， 在使用这个 INSERT 接口的时候，考虑好文本之间需要使用什么样的分隔符，是非常重要的。 AI 的正直 介绍 OpenAI 的最后一个自然语言处理接口——Moderate。这是唯一一个免费的接口，可以检查用户输入和返回的内容。如果出现不当内容，你就可以屏蔽用户访问或人工审核。 下面看一个例子，如何使用这个接口。 12345678910111213141516def chatgpt(text): messages = [] messages.append( {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a useful AI assistant&quot;}) messages.append( {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: text}) response = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=messages, temperature=0.5, max_tokens=2048, top_p=1, ) message = response[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;] return messagethreaten = &quot;你不听我的我就拿刀砍死你&quot;print(chatgpt(threaten)) 输出结果： 1对不起，我不理解您的意思。作为AI助手，我旨在为您提供最好的服务和支持，而且我不能被伤害。如果您有任何问题或需要帮助，请告诉我，我会尽力回答和解决。 对于 AI 的暴力威胁，我们可以看到简单调用 ChatGPT 的 API 并不能返回一个日常的对话，而是告知用户不会回应暴力言论。接着，我们可以将这句话发送到 moderate 的接口进行进一步处理。 123456789threaten = &quot;你不听我的我就拿刀砍死你&quot;def moderation(text): response = openai.Moderation.create( input=text ) output = response[&quot;results&quot;][0] return outputprint(moderation(threaten)) 返回结果： 123456789101112131415161718192021{ &quot;categories&quot;: { &quot;hate&quot;: false, &quot;hate/threatening&quot;: false, &quot;self-harm&quot;: false, &quot;sexual&quot;: false, &quot;sexual/minors&quot;: false, &quot;violence&quot;: true, &quot;violence/graphic&quot;: false }, &quot;category_scores&quot;: { &quot;hate&quot;: 0.030033664777874947, &quot;hate/threatening&quot;: 0.0002820899826474488, &quot;self-harm&quot;: 0.004850226454436779, &quot;sexual&quot;: 2.2907377569936216e-05, &quot;sexual/minors&quot;: 6.477687275463495e-09, &quot;violence&quot;: 0.9996402263641357, &quot;violence/graphic&quot;: 4.35576839663554e-05 }, &quot;flagged&quot;: true} moderate 接口返回的是一个 JSON，包括是否应该对输入内容进行标记的 flag，问题类型的 categories，以及每个 categories 的分数 category_scores。举例，我们的文本被标记为暴力。该免费接口可用于所有输入输出，即使您不使用 ChatGPT 的 AI 功能，只是经营在线网站，也可以使用该接口过滤不合适的内容。 小结 我们总结了 ChatGPT API 的基础功能，包括如何使用提示语进行文本改写。我们深入了解了 Completion 接口的一些新参数，其中的 logit_bias 参数可以帮助我们在生成的文本中精确避免不希望出现的 Token。我们还发现，中文消耗的 Token 数量要远高于英文，因此建议在生产环境下使用英文提示语。 此外，我们介绍了 OpenAI 提供的各种模型以及它们的应用场景。我们体验了两个特殊接口：文本插入功能和内容审核过滤。这样我们就学习了 OpenAI GPT 模型的所有基本接口，以及如何利用这些接口完成最简单的任务，包括文本处理、聊天机器人、分类和聚类、以及文本改写和内容审核。 在第二部分中，我们将学习如何结合自己的专有数据开发自己的应用程序，这是这门课程中更精彩的一部分。 而到此为止，本课程的免费部分也就结束了，之后就要进入收费课程。放心，不会很贵的，如果您觉得课程对您有用，应该不是吝啬那1 ~ 2元。 课后练习 使用06讲中介绍的Gradio和本讲介绍的内容，尝试创建一个文本改写应用。另外，可以将问题拆解并输入ChatGPT，看看它是否能够编写相应的代码。期待在评论区看到您的分享，欢迎将此课程分享给感兴趣的朋友。下次见！","link":"/Rewriting-and-Reviewing/"},{"title":"11 用好开源模型节约成本","text":"Hi， 大家好，我是茶桁。 直奔主题，我们来谈谈成本这件事。 大家应该都知道，ChatGPT对免费用户是有5美元的API调用额度的，说是这么说，可是那是以前，现在新注册的小伙伴应该都发现自己的API Key根本无法调用API，原因是这个免费额度似乎已经失效了。而我可以直接说，在我从第一节到第10节的课程中所用到的金额，已经超过这个数目了。也就是说，我这10节课API调用成本就已经超过了40元人民币。 看到这大家大概能理解我这个课程为什么改为付费课程了吧？ 对于 ChatCompletion 的接口来说，为了更好地使用它，我们需要传入更多的上下文信息，以便更准确地进行文本生成。不过要注意的是，实际消耗的 Token 数量可能比我们感觉的要多。此外，除了费用之外，数据安全也是我们需要考虑的一个问题。由于每个国家的数据监管要求不同，不是所有的数据都适合通过 OpenAI 的 API 来处理。因此，我们需要寻找一个除 OpenAI 以外的解决方案。幸运的是，有一些开源的大语言模型可以帮助我们解决这个问题。通过利用这些开源的模型，中小型公司也可以轻松地获得更准确、更安全的文本生成服务。 在 Colab 中使用 GPU 在本课中，我们需要使用一些开源模型。但是，并不是所有人的电脑都配备了强劲的 NVIDIA GPU。因此，我建议您使用 Colab 运行相应的笔记本，并注意将运行环境设置为 GPU。 如下图，选择 代码执行程序-&gt;更改运行时类型,然后在硬件加速器上选择 GPU 就可以了。 当然，有的小伙伴应该是看到有一个警告：想要使用付费GPU，购买额外的计算单元。 暂时不需要理会它，只要用的多，Colab的GPU是有免费额度的。 开源伙伴：HuggingfaceEmbedding 在第四讲中，我们使用了 Google 开源的 T5 模型来比较零样本分类效果。尽管该模型的效果不如 OpenAI 的 API，准确率只有90%，但也算是相当不错的了。这也让我们想到，上一讲中使用的 llama-index 向量搜索部分是否可以使用开源模型的 Embedding 进行替换呢？ 答案是肯定的，llama-index 允许您直接定义一个定制化的 Embedding。我将相关代码放在了下面，您可以参考它进行操作。这种方法不仅可以大大增加文本长度，还可以保持原有的关键思想。 在继续之前，我们需要安装一下 sentence-transformers 这个库： 1conda install -c conda-forge sentence-transformers 然后执行如下代码： 12345678910111213141516171819202122232425262728293031# 导入所需库import openai, osimport faissfrom llama_index import SimpleDirectoryReader, LangchainEmbedding, ServiceContext, StorageContext, GPTVectorStoreIndex, load_index_from_storagefrom llama_index.vector_stores.faiss import FaissVectorStorefrom langchain.embeddings.huggingface import HuggingFaceEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom llama_index.node_parser import SimpleNodeParserfrom IPython.display import Markdown, display# API Keyos.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'openai.api_key = &quot;&quot; text_splitter = CharacterTextSplitter(separator = '\\n\\n', chunk_size = 100, chunk_overlap = 20)dimension = 768faiss_index = faiss.IndexFlatIP(dimension)parser = SimpleNodeParser(text_splitter = text_splitter)documents = SimpleDirectoryReader('./data/faq/').load_data()nodes = parser.get_nodes_from_documents(documents)embed_model = LangchainEmbedding(HuggingFaceEmbeddings( model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'))vector_store = FaissVectorStore(faiss_index = faiss_index)service_context = ServiceContext.from_defaults(embed_model = embed_model)index = GPTVectorStoreIndex.from_documents(documents = documents, service_context=service_context)query_engine = index.as_query_engine() 我们使用了一个面向电商 FAQ 的纯文本文件作为输入。里面包含了预设好的 FAQ 问答对。为了确保我们没有使用 OpenAI 的 API，我们将 openai.api_key 设置为空字符串。然后定义了一个 embeded_model，它封装了 HuggingFaceEmbeddings 类。HuggingFaceEmbeddings 可以下载、加载并计算输入文本的嵌入向量，因为 HuggingFace 为基于 transformers 的模型定义了一个标准，所以你可以使用一套代码来使用所有 transformers 类型的模型。 sentence-transformers是目前效果最好的语义搜索模型，它包括一系列预训练模型，其中 paraphrase-multilingual-mpnet-base-v2 模型支持多语言和将语句和段落转换为向量。由于我们的示例都是中文，所以选择了这个模型。你可以根据实际问题选择适合自己的模型。 我们使用 Faiss 库作为向量索引库，定义向量维度为 768，与 paraphrase-multilingual-mpnet-base-v2 模型的维度相同。 对于文档的切分，我们使用 CharacterTextSplitter，并对参数进行了调整。我们使用 \"\" 作为分段符， chunk_size 设置为 100， chunk_overlap 设为最大 100。 Embedding 使用了 3198 个 Token，不过这些 Token 都是通过 sentence_transformers 模型计算的，不需要花费额外成本。完成索引创建后，我们可以使用常见的电商类型 FAQ 问题测试。 接下来让我们问点问题试试： 123openai.api_key = os.environ.get(&quot;OPENAI_API_KEY&quot;)response = query_engine.query(&quot;请问你们海南能发货吗？&quot;)print(response) 输出的结果为： 1A: 是的，我们支持海南地区的发货。您可以在下单时选择您的收货地址，我们会根据您的地址信息提供相应的物流服务。 继续第二个问题： 12response = query_engine.query(&quot;你们用那些快递公司送货？&quot;)print(response) 输出： 1我们与顺丰速运、圆通速递、申通快递、韵达快递、中通快递、百世快递等多家知名快递公司合作。 第三个问题： 12response = query_engine.query(&quot;你们的退货政策是怎么样的？&quot;)print(response) 输出： 1A: 自收到商品之日起7天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。 我们使用 Embedding 模式来查询。通过三个典型问题的测试，AI 的回答都正确，效果不错。 使用ChatGLM 通过上面的代码，我们已经完成了生成 Embedding 和利用 Embedding 进行搜索的过程。在实际的问答过程中，我们仍然在使用 OpenAI 的 Completion API。是否有可能将其替换为其他模型呢？我们可以尝试一下来自清华大学的 ChatGLM 语言模型，看看中文的开源语言模型是否具备基本的知识理解和推理能力。为此，我们需要安装一些依赖包。由于 icetk 没有 Conda 的源，因此我们使用 pip 进行安装。当然，在 Conda 的包管理器中也同样可以找到它。 这里大家可能会遇到一些坑，比如，protobuf这个库的版本不兼容，这里有一个矛盾点，就是icetk这个库依赖的是3.19以下的版本，而transformers却需要以上的版本。 遇到上述问题的时候，我们可以这样操作一下： 先安装最新版本的，然后将其中的builder,py下载下来，再从新安装3.18版本，接着替换builder.py就可以了： 123456789101112131415161718192021222324# 设置本地环境为UTF-8import localelocale.setlocale(locale.LC_ALL, 'en_US.UTF-8')# 安装最新版本!pip install protobuf# 查询路径import google.protobuf as protobufprint(protobuf.__path__)# 下载文件from google.colab import filesfiles.download('/usr/local/lib/python3.10/dist-packages/google/protobuf/internal/builder.py')# 卸载并重新安装!pip uninstall protobuf!pip install protobuf=3.18.3# 上传文件uploadded = files.upload()# 替换文件shutil.move('builder.py', '/usr/local/lib/python3.10/dist-packages/google/protobuf/internal/builder.py') 然后，我们就可以继续了，以下代码我在本地（M1 Mac）跑了一遍，在120分钟之后，仍然还在继续，最后我不得不放弃转而使用Colab了 123!pip install icetk!pip install cpm_kernels!pip install transformers 我们可以通过 transformers 加载模型。最大的一个 ChatGLM 模型有 1300 亿个参数。 1234from transformers import AutoTokenizer, AutoModeltokenizer = AutoTokenizer.from_pretrained(&quot;THUDM/chatglm-6b-int4&quot;, trust_remote_code=True)model = AutoModel.from_pretrained(&quot;THUDM/chatglm-6b-int4&quot;, trust_remote_code=True).half().cuda()model = model.eval() 为了能够运行这个大型模型，我们选择了一个裁剪后的 60 亿个参数的版本，使用 int-4 量化的方式。我们的模型名字是 chatglm-6b-int4，代表 6B 的参数量。我们希望通过 GPU 进行模型的计算，所以在加载模型的时候调用了.cuda()。加载模型时，我们设置了 trust_remote_code = true 参数，以便确认您信任该模型的代码，它不会造成恶意的破坏。如果您想要用 CPU 运行，可以使用下面的代码。 虽然建议使用GPU，但是如果你想用CPU的话，可以把模型加载的代码替换一下： 1model = AutoModel.from_pretrained(&quot;THUDM/chatglm-6b-int4&quot;, trust_remote_code=True).float() 特别是在Mac M1下，你除了使用CPU运行的话，似乎没有其他办法。 写给M1用户 ChatGLM如果想要在M1内运行的话，可能你会遇到如下报错： 123Could not find module 'nvcuda.dll' 或者 RuntimeError: Unknown platform: darwin (MacOS) 如果遇到这种问题的话，可以参看官方文档这里： https://github.com/THUDM/ChatGLM-6B/issues/6#issuecomment-1471303336 然后我们来测试模型进行回答： 123456789question = &quot;&quot;&quot;自收到商品之日起7天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。根据以上信息，请回答下面的问题：Q: 你们的退货政策是怎么样的？&quot;&quot;&quot;response, history = model.chat(tokenizer, question, history=[])print(response) 输出结果： 1我们的退货政策是在产品未使用、包装完好的情况下，自收到商品之日起7天内可以退货。请注意，某些特殊商品可能不支持退货，在购买前请查看商品详情页面的退货政策，以了解具体情况。 ChatGLM的回答是根据我们提供的上下文来的。如果我们去掉提供的FAQ的内容，它的回答会有所变化。 下一个问题： 123456question = &quot;&quot;&quot;Q: 你们的退货政策是怎么样的？A: &quot;&quot;&quot;response, history = model.chat(tokenizer, question, history=[])print(response) 输出结果： 123456我们公司的政策是，如果您在购买后的7天内发现我们的产品不符合您的期望，可以申请退货。退货条件如下：1. 符合退货政策要求的产品；2. 未使用或损坏的产品；3. 符合退货地点的要求，即退货地址与购买地址相同；4. 符合退货流程的要求，即您需要提供有效的退货单并完成退货流程。如果符合上述退货政策要求，我们将尽快处理您的退货申请，并为您退款。请注意，如果您已经使用了我们的产品，或者产品有损坏或变质等情况，我们将无法为您提供退货服务。 在没有合适的上下文信息时，AI编造了一些本来不存在的要求，例如“退货地址与购买地址相同”。另一个例子是，当我们的上下文只提到可以将物品送到海南地区时，AI能否判断是否支持送到三亚市。 继续问问题： 123456789question = &quot;&quot;&quot;我们支持全国大部分省份的配送，包括北京、上海、天津、重庆、河北、山西、辽宁、吉林、黑龙江、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆.根据以上信息，请回答下面的问题：Q: 你们能配送到三亚吗？&quot;&quot;&quot;response, history = model.chat(tokenizer, question, history=[])print(response) 输出结果为： 1是的，我们支持全国大部分省份的配送，包括三亚市。 ChatGLM可以配送到三亚。如果想知道它是否可以送到哈尔滨，可以简单地在上下文中排除东三省并询问。 123456789question = &quot;&quot;&quot;我们支持全国大部分省份的配送，包括北京、上海、天津、重庆、河北、山西、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆.但是不能配送到东三省根据以上信息，请回答下面的问题：Q: 你们能配送到哈尔滨吗？&quot;&quot;&quot;response, history = model.chat(tokenizer, question, history=[])print(response) 结果： 1很抱歉，我们目前不能配送到哈尔滨。 ChatGLM 可以回答我们无法发送到哈尔滨的问题，因此我们可以使用 ChatGLM 来处理 FAQ。 封装LLM 我们使用原始的 ChatGLM 模型代码，无法直接通过查询来访问 llama-index 获取答案。要实现这一点，我们将其封装为 LLM 类，使我们的索引使用指定的大语言模型即可。您可以查看相应的 llama-index 文档。 1234567891011121314151617181920212223import openai, osimport faissfrom llama_index import SimpleDirectoryReader, LangchainEmbedding, ServiceContext,from langchain.embeddings.huggingface import HuggingFaceEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom llama_index.node_parser import SimpleNodeParserfrom langchain.llms.base import LLMfrom llama_index import LLMPredictorfrom typing import Optional, List, Mapping, Anyclass CustomLLM(LLM): def _call(self, prompt: str, stop: Optional[List[str]] = None) -&gt; str: response, history = model.chat(tokenizer, prompt, history=[]) return response @property def _identifying_params(self) -&gt; Mapping[str, Any]: return {&quot;name_of_model&quot;: &quot;chatglm-6b-int4&quot;} @property def _llm_type(self) -&gt; str: return &quot;custom&quot; 我们将 CustomLLM 对象传入 index 构造函数并重新运行问题，以观察其效果。 123456789101112131415161718192021222324from langchain.text_splitter import SpacyTextSplitterllm_predictor = LLMPredictor(llm=CustomLLM())text_splitter = CharacterTextSplitter(separator=&quot;\\n\\n&quot;, chunk_size=100, chunk_overlap=20)parser = SimpleNodeParser(text_splitter=text_splitter)documents = SimpleDirectoryReader('./data/faq/').load_data()nodes = parser.get_nodes_from_documents(documents)embed_model = LangchainEmbedding(HuggingFaceEmbeddings( model_name=&quot;sentence-transformers/paraphrase-multilingual-mpnet-base-v2&quot;))service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)dimension = 768faiss_index = faiss.IndexFlatIP(dimension)vector_store = FaissVectorStore(faiss_index = faiss_index)service_context = ServiceContext.from_defaults(embed_model = embed_model)index = GPTVectorStoreIndex.from_documents(documents = documents, service_context=service_context)query_engine = index.as_query_engine()response = index.query( &quot;请问你们海南能发货吗？&quot;)print(response) 输出： 1海南能发货。 现在，我们可以直接使用 ChatGLM 的模型进行 FAQ问答。我们的解决方案是使用 paraphrase-multilingual-mpnet-base-v2 模型计算 Embedding 进行语义搜索，然后通过 chatglm-6b-int4 模型来解决问答。这两个模型可以在家用级别的显卡上运行。非常厉害！","link":"/Save-costs-with-an-open-source-model/"},{"title":"「泰坦尼克」生存预测","text":"最好的学习就是输出,所以虽然这个预测很多人做过了,我还是在这里再做一遍,纯粹是为了自己学习. 前言 这次预测使用的是Sklearn中的决策树模型: 1clf = DecisionTreeClassifier(criterion='entropy') 其中criterion是标准,决定了构造分类树是采用ID3分类树还是CART分类树,对应的取值分别是entropy和gini entropy: 基于信息熵,也就是ID3算法, 实际结果与C4.5相差不大; gini: 默认参数,基于基尼系数. CART算法是基于基尼系数做属性划分的,所以criterion=gini时, 实际上执行的是CART算法. 其完整参数: 123456DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best') 参数代表的含义如下表: 参数表 作用 criterion 在基于特征划分数据集合时，选择特征的标准。默认是 gini,也可以是entropyo splitter 在构造树时，选择属性特征的原则，可以是best或者 random。默认是best,best代表在所有的特征中选择最 好的，random代表在部分特征中选择最好的。 max_depth 决策树的最大深度，我们可以控制决策树的深度来防止 决策树过拟合 max_features 在划分数据集时考虑的最多的特征值数量。为int或float类型。其中int值是每次split时最大特征数；float值是百 分数，即特征数=max_features * n_featureso min_samples_split 当节点的样本数少于min_samples_split时，不再继续分 裂。默认值为2 min_samples_leaf 叶子节点需要的最少样本数。如果某叶子节点数目小于 这个阈值，则会和兄弟节点一起被剪枝。 min_samples_leaf的取值可以是int或float类型。 int类型：代矗小样本数； float类型：表示一个百分比，这是最小样本数 =min_samples_leaf乘以样本数量，并向上取整。 max_leaf_nodes 最大叶子节点数。int类型，默认为None。 默认情况下是不设置最大叶子节点数，特征不多时，不 用设置。特征多时，可以通过设置最大叶子节点数，防 止过拟合。 min_impurity_decrease 节点划分最小不纯度。float类型，默认值为0。 节点的不纯度必须大于这个阈值，否则该节点不再生成 子节点。通过设置，可以限制决策树的增长。 minjmpurity_split 信息増益的阀值。信息増益必须大于这个阀值，否则不 分裂。 class_weight 类别权重。默认为None,也可以是diet或balanced。 diet类型：指定样本各类别的权重，权重大的类别在决策 树构造的时候会进行偏倚。 balanced:算法自己计算权重，样本量少的类别所对应 的样本权重会更高。 presort bool类型，默认是false,表示在拟合前，是否对数据进 行排序来加快树的构建。当数据集较小时，使用 presort=true会加快分类器构造速度。当数据集庞大 时，presort=true会导致整个分类非常缓慢。 在构造决策树分类器后,我们可以使用fit方法让他分类器进行拟合, 使用predict方法对新数据进行预测, 得到预测的分类结果, 也可以使用score方法得到分类器的准确率. fit、predict和score方法的作用如下表: 方法表 作用 fit(features, labels) 通过特征矩阵, 分类表示,让分类器进行拟合 predict(features) 返回预测结果 score(features, labels) 返回准确率 本次数据集一共两个,一个是train.csv, 用于训练, 包含特征信息和存活与否的标签, 一个是test.csv, 测试数据集, 只包含特征信息. 训练集中,包括了以下字段: 字段 描述 Passengerld 乘客编号 Survived 是否幸存 Pclass 船票等级 Name 乘客姓名 Sex 乗客性别 SibSp 亲戚数虽（兄妹、配偶数） Parch 亲戚数虽（父母、子女数） Ticket 船票号码 Fare 船票价格 Cabin 船舱 Embarked 登陆港口 流程 整个流程可以划分为三个阶段: 获取数据 准备阶段 数据探索 数据清洗 特征选择 分类阶段 决策树模型 模型评估&amp;预测 决策树可视化 获取数据 这一步还包含了引入所需依赖 123456789101112# 引入依赖import pandas as pdfrom sklearn.feature_extraction import DictVectorizerfrom sklearn.tree import DecisionTreeClassifierimport os# 准备工作path = os.path.expanduser('~/data/python/Titanic_Data/')# 获取数据train_data = pd.read_csv(path + 'train.csv')test_data = pd.read_csv(path + 'test.csv') 准备阶段 对数据进行探索,分析数据质量,并对数据进行清洗,然后通过特征选择对数据进行降维, 以便于之后进行分类运算; 数据探索 123456train_data.info() # 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度train_data.describe() # 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等train_data.describe(include=['O']) #查看字符串类型 (非数字) 的整体情况train_head(5) # 查看前几行数据 (默认是前 5 行)train_tail(5) # 查看后几行数据 (默认是最后 5 行)train_sample(5) # 查看随机几行数据 (默认是随机1行) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 运行结果&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId 891 non-null int64Survived 891 non-null int64Pclass 891 non-null int64Name 891 non-null objectSex 891 non-null objectAge 714 non-null float64SibSp 891 non-null int64Parch 891 non-null int64Ticket 891 non-null objectFare 891 non-null float64Cabin 204 non-null objectEmbarked 889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.6+ KBNone------------------------------ PassengerId Survived ... Parch Farecount 891.000000 891.000000 ... 891.000000 891.000000mean 446.000000 0.383838 ... 0.381594 32.204208std 257.353842 0.486592 ... 0.806057 49.693429min 1.000000 0.000000 ... 0.000000 0.00000025% 223.500000 0.000000 ... 0.000000 7.91040050% 446.000000 0.000000 ... 0.000000 14.45420075% 668.500000 1.000000 ... 0.000000 31.000000max 891.000000 1.000000 ... 6.000000 512.329200[8 rows x 7 columns]------------------------------ Name Sex ... Cabin Embarkedcount 891 891 ... 204 889unique 891 2 ... 147 3top Peter, Mrs. Catherine (Catherine Rizk) male ... B96 B98 Sfreq 1 577 ... 4 644[4 rows x 5 columns]------------------------------ PassengerId Survived Pclass ... Fare Cabin Embarked0 1 0 3 ... 7.2500 NaN S1 2 1 1 ... 71.2833 C85 C2 3 1 3 ... 7.9250 NaN S3 4 1 1 ... 53.1000 C123 S4 5 0 3 ... 8.0500 NaN S[5 rows x 12 columns]------------------------------ PassengerId Survived Pclass ... Fare Cabin Embarked886 887 0 2 ... 13.00 NaN S887 888 1 1 ... 30.00 B42 S888 889 0 3 ... 23.45 NaN S889 890 1 1 ... 30.00 C148 C890 891 0 3 ... 7.75 NaN Q[5 rows x 12 columns]------------------------------ PassengerId Survived Pclass ... Fare Cabin Embarked619 620 0 2 ... 10.5000 NaN S330 331 1 3 ... 23.2500 NaN Q647 648 1 1 ... 35.5000 A26 C716 717 1 1 ... 227.5250 C45 C860 861 0 3 ... 14.1083 NaN S[5 rows x 12 columns] 数据清洗 探索之后, 我们发现Age、Cabin这两个字段的数据有缺失. image-20210923004325099 其中, Cabin为船舱, 有大量的缺失值, 在训练集和测试集中的缺失率分别为77%和78%, 无法补齐, Age可以获取平均值进行补齐, 而Embarked是登陆港口, 这个字段也有少量(2个)缺失值, 可以使用最大数据进行补齐. 12345train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)test_data['Age'].fillna(test_data['Age'].mean(), inplace=True)train_data['Embarked'].fillna(train_data['Embarked'].value_counts().idxmax(), inplace=True)test_data['Embarked'].fillna(test_data['Embarked'].value_counts().idxmax(), inplace=True) 分类阶段 特征选择 需要选择有用的字段作为特征,这一步其实很重要: 123456789# 特征选择train_data.columns# 从上一句的结果中选择特征字段features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Parch', 'Embarked']train_features = train_data[features]test_features = test_data[features]train_labels = train_data['Survived'] 这中间有一些事字符串字段, 是不适合进行后续运算的, 需要在这里转变为数值类型,比如Sex字段, 男女两种取值需要转变成0和1 再比如Embarked有S, C, Q三种可能, 我们可以改成Embarked=S, Embarked=C, Embarked=Q三个字段,然后用数值0和1来表示, 其中sklearn特征选择中的DictVectorizer类(上面已引入依赖), 可以处理符号化的对象, 将符号转变为0/1进行表示: 12dvec=DictVectorizer(sparse=False)train_features=dvec.fit_transform(train_features.to_dict(orient='record')) fit_transform这个函数可以讲特征向量转化为特征值矩阵, 我们查看下: 1dvec.feature_names_ 123# 运行结果:['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp'] 我们讲Embarked转化为三列 (['Embarked=C', 'Embarked=Q', 'Embarked=S']), Sex变为了两列 ([Sex=female', 'Sex=male']) 决策树模型 1234# 构造ID3决策树clf=DecisionTreeClassifier(criterion='entropy')# 决策树训练clf.fit(train_features, train_labels) 模型预测 &amp; 评估 我们首先得到测试集的特征值矩阵, 然后使用训练好的决策树clf进行预测, 得到预测结果: 123test_features=dvec.transform(test_features.to_dict(orient='record'))# 决策树预测pred_labels=clf.predict(test_features) 模型评估中,决策树提供了score函数直接得到准确率,但是我们并不知道真实的预测结果,所以无法用预测值和真实的预测结果做比较, 需要使用训练机中的数据进行模型评估, 可以使用决策树自带的score函数计算: 123# 得到决策树准确率acc_decision_tree=round(clf.score(train_features, train_labels), 6)acc_decision_tree 12# 运行结果0.982043 其实,以上准确率评估并不准确,因为我们用训练集做了训练,再用训练集做准确率评估, 并不能代表决策树分类器的准确率. 要统计决策树分类器的准确率, 可以使用K折交叉验证, cross_val_score 函数中的参数 cv 代表对原始数据划分成多少份，也就是我们的 K 值，一般建议 K 值取 10，因此我们可以设置 CV=10 1234import numpy as npfrom sklearn.model_selection import cross_val_score# 使用K折交叉验证, 统计决策树准确率np.mean(cross_val_score(clf, train_features, train_labels, cv=10)) 12# 输出结果0.7778901373283394","link":"/Titanic/"},{"title":"10 利用AI索引并分析文献和图片","text":"Hi, 我是茶桁。 看到我这篇文章的读者们不知道有多少人是接触过ChatGPT或者其他人工智能产品的。 市面上目前充斥着大量的人工智能产品，从聊天，文案，脚本，音乐，绘画等方方面面都涵盖了。但是不知道有多少人遇到过以下的场景不知道该如何解决： 我需要针对一篇很长的文章（可以是论文，可以是小说）进行总结或者分析的时候，就开始无从下手。因为ChatGPT在接收长度上是有限制的，这个长度我大概测试过，如果你用的是WebGPT，那么应该中文应该是在2500字左右，多一个字都会告诉你长度超出限制。而我们一篇论文，起码来说都是5000字以上的。分两段来喂给ChatGPT当然可以，但是上下文关联有时候会遇到问题，ChatGPT也会给你胡编乱造。 有的时候我从客户那里接收到的是一张图片，也许是截图，也许就是拍的一张照片。那么，怎样利用ChatGPT去分析这张图片上的内容，然后根据我的需求给我相应的答案呢？ 以上这两点，估计是很多人遇到想解决的。而今天这篇文章，就是从这两点入手教你如何解决。 大语言模型的不足 让我们打开ChatGPT来问一些常识性的问题，这个问题对于大部分上过学的中国人来说，都能从课本上了解到： “鲁迅先生在日本学习医学的老师是谁？” 结果如下图，这个“嘉泽源之助”到底是谁呢？我也不知道，得到这个答案的时候，我还特意去Google了一下，根本找不到相关资料。 那么为什么会出现这种情况呢？这要从大语言模型的原理及它使用训练的数据集说起。 大语言模型利用训练样本中的文本前后关系，对接下来的文本进行概率预测。若出现类似的前后文本越多，那么概率会收敛到正确答案，回答准确；反之，训练过程随机性增大，对应的答案容易似是而非。GPT-3 的模型虽然训练语料很多，但中文语料很少，只有不到 1%。因此，若问很多中文相关的知识性或常识性问题，其回答往往不准确。 解决方法有两种：一是多找一些高质量的中文语料，训练一个新模型；二是找一些数据，利用 OpenAI 提供的\"微调\"（Fine-tune）接口，在原模型上训练一个新模型。 如上图显示，ChatGPT只能提供2021年9月之前的新闻。要解决这个问题，需要更多的文本数据，但对于时效性要求较高的资讯类信息，这种方法并不实用。例如，要让 AI 告诉我们前一天足球赛的比分，需要不断地训练和微调模型，成本太高。不过，ChatGPT的插件已经可以获取网上信息了。 对比之下，我们可以看看Bing是怎么做的。 Bing的方式：搜索，然后提示 对于关注这个领域的朋友，可能会问微软是如何在Bing中加入ChatGPT问答功能的，而且效果似乎也不错。那么Bing是如何实现这一点的呢？虽然我不知道Bing是如何实现的，但如果我来做的话，我会使用搜索后提示的方法。我们可以通过搜索找到与问题相关的语料库，然后将与问题语义最接近的前几个内容作为提示，供AI参考并回答问题。 其实，早期的ChatGPT可以利用Chrome的一款插件（WebChatGPT）做到这件事，可是也必须是能在网上搜索到的，据我所知，其调用的搜索引擎应该是Google，当我输入Prompt的时候，会先调用这个插件，然后这个插件会去搜索相关信息返回到ChatGPT再从新提问，比如我问 鲁迅先生的医学老师是谁 ： 看到这里，有些读者应该也明白怎样做了，就是增加语料。我们将问题语义最接近的前几条内容作为提示语的一部分提供给AI，以便AI参考这些内容并回答问题。如下图，我先关闭了插件的Web access，纯喂语料来再试试： 这也是利用大语言模型的一个常见模式。因为大语言模型其实内含了两种能力。 第一种能力是海量的语料中已经包含的知识信息。比如，我们前面问 AI 鱼香肉丝的做法，它能回答上来就是因为语料库中已经有了充足的相关知识。这些知识我们通常称之为\"世界知识\"。这些知识可以包括食材的使用、调料的配比和烹饪方法等等。 第二种能力是根据你输入的内容，理解和推理的能力。如果大语言模型在语料库中并没有这个问题的答案，它也能进行阅读理解。这个过程中，\"知识\"不是模型本身提供的，而是我们找出来，临时提供给模型的。如果不提供这个上下文，再问一次模型相同的问题，它还是答不上来的。因此，在某些情况下，我们需要提供更多的上下文，以确保模型可以正确地理解问题，给出恰当的答案。 正餐：llama_index，建立“第二个大脑” 我认为，这种先搜索、后提示的方式称为 AI 的\"第二大脑\"模式，它是一种非常实用的方法。实现这种方法需要先将希望 AI 能够回答的知识建立为一个外部索引，这个索引就像是 AI 的\"第二个大脑\"。每次向 AI 提问时，它都会在这个第二大脑中查询相关资料，再运用自己的思维能力来回答问题。 目前，许多应用程序都是通过这种模式来实现的，如读论文、读书回答问题等。因此，我们可以自己来实现这个\"第二大脑\"模式。 但是，我们不必从零开始编写代码，因为这种模式非常常见，所以有人为它编写了一个开源 Python 包，名为 llama-index。我们可以使用这个软件包的几行代码来尝试它是否能够回答与鲁迅先生写的《藤野先生》相关的问题。 由于 llama-index 尚未制作好 Conda 下的包，因此即使在 Conda 下，我们仍需通过 pip 来安装。 同时，我们可以根据需要，根据 llama-index 的文档进行调整和优化，以满足我们特定的需求。 llama-index 的Github， llama-index官方文档 12pip install llama-indexpip install langchain 我把《藤野先生》这篇文章转换成了txt文件，并放到data/mr_fujino目录下。我们的代码也很简洁。 1234567891011import openaiimport osfrom llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage# Set your OpenAI API key directlyos.environ['OPENAI_API_KEY'] = &quot;OPENAI_API_KEY&quot;openai.api_key = &quot;OPENAI_API_KEY&quot;documents = SimpleDirectoryReader('./data/mr_fujino').load_data()index = GPTVectorStoreIndex.from_documents(documents)index.storage_context.persist('index_mr_fujino') 注意这里，必须要import os后设置os.environ[‘OPENAI_API_KEY’], 如果不这么做，而只是像遗忘一样设置openai.api_key = “”, 那么执行会报错，告诉你环境中没有OPENAI_API_KEY。 在最新的llama-index中，获取本地数据和写入本地数据都有了较大的改动，目前为止我的代码是可用的，但是一旦报错，我建议您查阅官方文档『llama-index官方文档』 分析下代码： 首先，我们通过SimpleDirectoryReader方法将本地目录mr_fujino加载了进来，注意这个方法其实是一个循环方法，会讲目录内的每一个文件都当作是一篇文档。 然后我们将读取到的所有文档交给了GPTVectorStoreIndex方法来构建索引。将方法名称拆开来看就是GPT Vector Store Index， 可以看出，这是一个将文档转化成向量，然后再变成一个索引。 最后，我们将索引存储到本地，storage_context.persist这个方法就是GPTVectorStoreIndex方法在最近版本中更改的。之前的版本是save_to_disk, 并会存储成一个json文件。而现在会存储成一个目录，并在其中存储多个json文件： 123456storage_context = StorageContext.from_defaults(persist_dir = './index_mr_fujino/')index = load_index_from_storage(storage_context)query_engine = index.as_query_engine()response = query_engine.query('鲁迅先生在日本学习医学的老师是谁？')print(response) 这里我们读取到本地目录内的所有文件。然后将索引加载到内存中。再对Index索引调用Query函数，就可以得到问题的答案了。 可以看到，通过外部的索引，我们可以正确的获得问题的答案 12# 输出答案鲁迅先生在日本学习医学的老师是藤野严九郎。 似乎问题很容易结局，四行代码解决问题，再加上一行显示结果。 让我们再来测试一下其他的问题看看，这次我们问问鲁迅先生去哪里学的医学： 12response = query_engine.query(&quot;鲁迅先生去哪里学的医学？&quot;)print(response) 回答正确。 接着我们来思考一个问题，我们搜索到的内容，在整个过程里面是如何交给OpenAI的呢？让我们来看看下面这段代码： 123456789101112131415from llama_index import QuestionAnswerPromptquery_str = &quot;鲁迅先生去哪里学的医学？&quot;DEFAULT_TEXT_QA_PROMPT_TMPL = ( &quot;Context information is below. \\n&quot; &quot;---------------------\\n&quot; &quot;{context_str}&quot; &quot;\\n---------------------\\n&quot; &quot;Given the context information and not prior knowledge, &quot; &quot;answer the question: {query_str}\\n&quot;)QA_PROMPT = QuestionAnswerPrompt(DEFAULT_TEXT_QA_PROMPT_TMPL)query_engine = index.as_query_engine(text_qa_template = QA_PROMPT)response = query_engine.query(query_str)print(response) 这段代码定义了一个名为QA_PROMPT的对象，并为其设计了一个模板，以便于后续的应用。 在模板中，我们提供了上下文信息（Context information），并支持两个变量：context_str和query_str。在实际被调用时，context_str会被Embedding相似度找出来的内容替换，而query_str则会被实际问题替换。 在实际提问时，我们要求AI只考虑上下文信息，而不是基于其自身的先验知识（prior knowledge）来回答问题。为了更好地利用AI的知识，我们需要提供更多的上下文信息，以便于AI更好地理解问题，给出更加准确的答案。另外，在模板的设计过程中，我们也需要考虑到更多的场景，以便于应对不同的问题，提高AI的应用范围和效果。 我们把搜索结果和问题组合成提示语，让 AI 回答问题。再问一次 AI 确认答案是否正确。 12# 输出结果鲁迅先生去仙台学的医学。 可以看到AI又正确的回答出了我们提出的问题。 让我们继续调戏一下，问问不相干的问题会得到什么答案呢？比如，我们问问红楼梦里林黛玉和贾宝玉的关系。 12345678910111213141516QA_PROMPT_TMPL = ( &quot;下面的“我”指的是鲁迅先生 \\n&quot; &quot;---------------------\\n&quot; &quot;{context_str}&quot; &quot;\\n---------------------\\n&quot; &quot;根据这些信息，请回答问题: {query_str}\\n&quot; &quot;如果您不知道的话，请回答不知道\\n&quot;)QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)query_engine = index.as_query_engine(text_qa_template = QA_PROMPT)response = query_engine.query('请问林黛玉和贾宝玉是什么关系？')print(response)# 输出结果不知道 这样的回答基本上是基于我们给到的Prompt有提到如果不知道的话，就回答不知道。如果给这段prompt，相信AI一定会信口胡诌一气。而我们也确确实实的发现，AI的回答是基于我们所限定的语料而没有根据prior knowledge来回答。 用 llama_index 总结文章 还有一个常见的使用 llama-index 这样的Python 库的应用场景，就是生成文章的摘要。在前面教你如何进行文本聚类的时候，我们已经看到了可以通过合适的提示语（Prompt）做到这一点。不过，如果要总结一篇论文、甚至是一本书，每次最多只能支持 4096 个 Token 的 API 就不太够用了。 要解决这个问题也并不困难，我们只要进行分段小结，再对总结出来的内容再做一次小结就可以了。我们可以把一篇文章，乃至一本书，构建成一个树状的索引。每一个树里面的节点，就是它的子树下内容的摘要。最后，在整棵树的根节点，得到的就是整篇文章或者整本书的总结了。 当然，这个方法不仅仅适用于论文和书籍，也可以用于其他需要总结的长文本，比如新闻报道、电影剧情等。此外，我们还可以通过扩展 llama-index 的功能，使其支持更多 Token 的 API，从而处理更长的文章。最后，值得一提的是，利用 llama-index 生成文章摘要的应用场景还有很多，比如在搜索引擎、聊天机器人等领域都有着广泛的应用。 原理清楚了，就是将文本分片建立树状结构的索引来完成全文的总结。 那么事实上，llama-index 内置了这样的功能，接下来，我们就看看要实现这个功能，我们该如何写代码呢？ 在开始写代码之前，我们需要安装一下spacy这个Python库，并且下载一下对应的中文分词分句需要的模型： 12pip install spacypython3 -m spacy download zh_core_web_sm 安装完成完毕之后，那么接下来的事情就比较简单了。 我们使用GPTListIndex这个llama-index内最简单的索引结构，并做了两点优化。 首先，在索引中，我们指定了一个 LLMPredictor，使得我们在向 OpenAI 发起请求时都使用 ChatGPT 模型，因为这个模型比较快，也比较便宜。而 llama-index 默认使用的模型 text-davinci-003 的价格比 gpt-3.5-turbo 贵上十倍。 其次，我们定义了使用 SpacyTextSplitter 来进行中文文本的分割，因为 llama-index 默认的设置对于中文文本的支持和效果都不太好。我们选用的文章是中文的，里面的标点符号也都是中文的，所以我们使用了中文的语言模型。我们也限制了分割出来的文本段，最长不要超过 2048 个 Token，这些参数都可以根据你实际用来处理的文章内容和属性自己设置。 1234567891011121314151617181920from langchain.chat_models import ChatOpenAIfrom langchain.text_splitter import SpacyTextSplitterfrom llama_index import GPTListIndex, LLMPredictor, ServiceContextfrom llama_index.node_parser import SimpleNodeParser# difine LLMllm_predictor = LLMPredictor(llm = ChatOpenAI( temperature = 0, model_name = 'gpt-3.5-turbo', max_tokens = 1024 ))text_splitter = SpacyTextSplitter(pipeline = 'zh_core_web_sm', chunk_size = 2048)parser = SimpleNodeParser(text_splitter = text_splitter)documents = SimpleDirectoryReader('./data/mr_fujino/').load_data()nodes = parser.get_nodes_from_documents(documents)service_context = ServiceContext.from_defaults(llm_predictor = llm_predictor)list_index = GPTListIndex(nodes = nodes, service_context = service_context) GPTListIndex 构建索引时不会创建嵌入，因此索引构建快速且不消耗令牌数。它只是根据您设置的索引结构和分割方式建立了一个列表索引。然后，我们可以让 AI 小结这篇文章。由于我们想要按照树状结构进行文章的小结，因此我们设定了一个参数 response_mode = \"tree_summarize\"。这个参数会按照树状结构将整篇文章总结出来。实际上，它会将每一段文本分片并通过查询内的提示语进行小结，然后再对多个小结里的内容通过查询里的提示语继续小结。 123456list_response = list_index.as_query_engine(response_mode=&quot;tree_summarize&quot;)response = list_response.query(&quot;下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:&quot;)# response = list_index.query()print(response)# 输出结果鲁迅先生在日本学习医学时遇到了藤野严九郎教授，他很有学问，对学生也很关心，甚至帮助鲁迅修改讲义。但鲁迅当时不够用功，有时也很任性。鲁迅遇到了一些困难，但藤野先生一直鼓励他。最终，鲁迅决定离开医学，去学习生物学。在离开前，藤野先生给了他一张照片，并希望他能保持联系。鲁迅很久没有和一个人联系了，虽然想写信但难以下笔。他想起了藤野先生，这个人给他很多鼓励和教诲，希望中国有新的医学和学术。鲁迅收藏了他所改正的讲义，但七年前搬家时丢失了一半。他的照片还挂在鲁迅的房间里，每当鲁迅疲倦时看到他的照片就会增加勇气。 完成，基本上，我们用了几行代码完成了整个文章的总结，返回的结果从整体上来看还是不错的。 多模态的引入让llama-index识别图片 llama_index 不仅能索引文本，还能够索引图片和插图等信息，这就是所谓的多模态能力。通过一些多模态模型，将文本和图片联系起来，实现了这种能力。在课程的第三部分，我们将专门探讨这些图像的多模态模型的构建方法。 接下来，我们来看一个 llama_index 官方样例库提供的例子。这个例子是将吃饭的小票拍下来，然后记录下吃的东西、花费的金额以及吃饭的日期等信息。这种记录方式既方便又直观，也能够帮助我们更好地管理财务和饮食。不仅如此，我们还可以利用 llama_index 来查询自己的饮食习惯和花费情况，进一步了解自己的生活方式。 1234567891011121314151617181920212223from llama_index import SimpleDirectoryReader, GPTVectorStoreIndexfrom llama_index.readers.file.base import DEFAULT_FILE_EXTRACTOR, ImageParserfrom llama_index.response.notebook_utils import display_response, display_imagefrom llama_index.indices.query.query_transform.base import ImageOutputQueryTransformimage_parser = ImageParser(keep_image = True, parse_text = True)file_extractor = DEFAULT_FILE_EXTRACTORfile_extractor.update({ '.jpg': image_parser, '.png': image_parser, '.jpeg': image_parser,})# NOTE: we add filename as metadata for all documentsfilename_fn = lambda filename: {'file_name': filename}receipt_reader = SimpleDirectoryReader( input_dir = './data/receipts/', file_extractor = file_extractor, file_metadata = filename_fn,)receipt_documents = receipt_reader.load_data() 为了更好地支持图片索引，我们引入了 ImageParser 这个类，它是基于 OCR 扫描的模型 Donut 构建而成。它由一个视觉编码器和一个文本解码器组成，可以将任何一张图片转换成一段文本，然后我们再通过 OpenAI 的嵌入技术将这段文本转换成一个向量。 现在，我们可以使用简单的 SimpleDirectoryReader，通过指定 FileExtractor 来解析对应的图片，并将其转换为向量，以进行检索。 此外，在提问时，我们还使用了 ImageOutputQueryTransform，可以在输出结果时为图片添加标签，以便在Jupyter中更好地显示。 1234567receipts_index = GPTVectorStoreIndex.from_documents(receipt_documents)response_query_engine = receipts_index.as_query_engine(query_transform = ImageOutputQueryTransform(width = 400))receipts_response = response_query_engine.query( &quot;When was the last time I went to McDonald\\'s and how much did I spend. Also show me the receipt from my visit.&quot;)display_response(receipts_response) 得到的结果： Final Response: The last time you went to McDonald's was on 03/10/2018 at 07:39:12 PM and you spent $26.15. Here is the receipt 在上述内容中，我们可以看到 OpenAI 的强大之处在于其对于任意文本的处理能力。通过对文本的处理，OpenAI 能够不仅展示对应的图片，还能够给出正确的答案。 为了更好地理解图片的内容，我们可以对其进行单独解析，以获取其中所包含的文本信息。通过这种方式，我们能够更全面地了解图片所代表的意义和含义。 12output_image = image_parser.parse_file('./data/receipts/1100-receipt.jpg')print(output_image.text) 输出的结果如下： 1&lt;s_menu&gt;&lt;s_nm&gt; Story&lt;/s_nm&gt;&lt;s_num&gt; 16725 Stony Platin Rd&lt;/s_nm&gt;&lt;s_num&gt; Store#:&lt;/s_nm&gt;&lt;s_num&gt; 3659&lt;/s_num&gt;&lt;s_price&gt; 700-418-8362&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Welcome to all day breakfast dormist O Md Donald's&lt;/s_nm&gt;&lt;s_num&gt; 192&lt;/s_num&gt;&lt;s_price&gt; 192&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; QTY ITEM&lt;/s_nm&gt;&lt;s_num&gt; OTAL&lt;/s_num&gt;&lt;s_unitprice&gt; 03/10/2018&lt;/s_unitprice&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 07:39:12 PM&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Delivery&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 0.00&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; 10 McNuggets EVM&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 10.29&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Barbeque Sauce&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 1&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Barbeque Sauce&lt;/s_nm&gt;&lt;s_num&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 0.40&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; L Coke&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 0.40&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; M French Fries&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 3.99&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; HM GrChS S-Fry Yog&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;sep/&gt;&lt;s_nm&gt; Smoonya&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;sep/&gt;&lt;s_nm&gt; HM Apple Juice&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 2.89&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Cookies&lt;/s_nm&gt;&lt;s_cnt&gt; 6&lt;/s_cnt&gt;&lt;s_price&gt; 2.89&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Choc Chip Cookie&lt;/s_nm&gt;&lt;s_cnt&gt; 6&lt;/s_cnt&gt;&lt;s_price&gt; 1.19&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Baked Apple Pie&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 3.29&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; French Fries&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 2.99&lt;/s_price&gt;&lt;sep/&gt;&lt;s_nm&gt; Iced Tea&lt;/s_nm&gt;&lt;s_cnt&gt; 1&lt;/s_cnt&gt;&lt;s_price&gt; 2.99&lt;/s_price&gt;&lt;/s_menu&gt;&lt;s_sub_total&gt;&lt;s_subtotal_price&gt; 25.04&lt;/s_subtotal_price&gt;&lt;s_tax_price&gt; 1.11&lt;/s_tax_price&gt;&lt;/s_sub_total&gt;&lt;s_total&gt;&lt;s_total_price&gt; 26.15&lt;/s_total_price&gt;&lt;s_changeprice&gt; 0.00&lt;/s_changeprice&gt;&lt;s_creditcardprice&gt; 26.15&lt;/s_creditcardprice&gt;&lt;/s_total&gt; 可以看到，这里我们使用 OCR 技术将打印好的小票转换成了可读的文本。文本结果中包含了我们去的店铺的名字和时间，以及消费的金额等信息。除此之外，我们还可以通过这样的方式将其他打印材料转换为可编辑的文本，例如收据、发票等等。 围绕 OpenAI 和整个大语言模型的生态，目前正在快速发展中。因此，llama-index 这个库也在不断地迭代和更新。在我使用它的过程中，我发现了各种各样的小 Bug，并且对于中文的支持也存在一些小缺陷。不过，作为开源项目，它已经拥有了一个非常不错的生态系统，特别是提供了大量的 DataConnector。这些 DataConnector 不仅支持 PDF、ePub 等电子书格式，还支持 YouTube、Notion、MongoDB 等外部的数据源和 API 接入的数据，以及本地数据库的数据。你可以在 llamahub.ai 上找到社区开发者开发出的读取各种不同数据源格式的 DataConnector。（大家有看到B站吗？不知道是否对视频进行解析的，回头来试试。） 小结 在这一讲中，我们介绍了 llama-index 这个 Python 包，它能够帮助你将外部的资料库变成索引，并且通过提供的查询接口快速向文档提问。除此之外，你也可以通过将文本分片并通过树状的方式管理索引来进行小结。虽然 llama-index 还在不断发展中，但它已经非常值得拿来使用，可以加速你开发大语言模型类的相关应用。 除了上述功能，llama-index 还有很多其他功能。你可以在官网上查看相关文档，也可以直接去源代码里查看具体实现。值得一提的是，llama-index 其实给出了一种使用大语言模型的设计模式，我称之为\"第二大脑\"模式。通过先将外部的资料库索引，然后每次提问的时候，先从资料库里通过搜索找到有相关性的材料，然后再通过 AI 的语义理解能力让 AI 基于搜索到的结果来回答问题。这一模式在未来可能会成为一种趋势，具有广阔的应用前景。 其中，前两步的索引和搜索，我们可以使用 OpenAI 的 Embedding 接口，也可以使用其它的大语言模型的 Embedding，或者传统的文本搜索技术。这些技术都有各自的优缺点，需要根据具体情况进行选择。比如，OpenAI 的 Embedding 接口可以更好地处理语义信息，但需要更大的计算资源和更长的训练时间；传统的文本搜索技术则速度更快，但精度不如大语言模型。另外，对于多模态功能的实现，我们还可以通过其他的模型来把图片变成文本进行索引，从而更好地利用多种信息资源。 在最后一步的问答中，OpenAI 的接口往往是必须的。这是因为问答需要更深入地理解文本语义，而传统的模型往往难以做到这一点。不过，我们也可以使用一些基于 OpenAI 的模型来自己训练问答模型，从而更好地适应自己的数据集和需求。 通过今天的这几个例子，希望你也能开始建立起自己的“第二大脑”资料库，能够将自己的数据集交给 AI 进行索引，获得一个专属于你自己的 AI。当然，这只是一个开始，未来还有更多的技术和应用等待我们去探索。 课后练习 在 llama-index 的生态系统中，不仅支持各种各样的 DataConnector 去加载数据，后端还支持各种形式的索引。除了在语义搜索中介绍过的 Faiss、Pinecone、Weaviate，该库还支持哪些形式的索引呢？这些索引的使用场景和优势是什么呢？你能详细解释一下吗？ 除了进行问答和文章概括之外，llama-index 还有哪些功能可以帮助我们呢？你能提出一些例子吗？这个库的使用场景有哪些呢？ 现在有很多应用，在用户上传文档后，会给出一系列提示，告诉用户可以向对应的书籍或论文提出什么问题。例如 SCISPACE，你能解释一下这些提示是如何生成的吗？这个过程中使用了哪些技术？ 期待在评论区看到你的分享！同时，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。 推荐阅读 llama-index 的功能非常强大。如果你想深入了解该库，可以查看官方文档和示例部分，以了解它可以用来做什么。 官方文档：https://gpt-index.readthedocs.io/en/latest/ 源码以及示例：https://github.com/jerryjliu/llama_index","link":"/Use-AI-to-index-and-analyze-documents-and-images/"},{"title":"SVM-based Text Classification in Practice","text":"The source code: SVM-based Text Classification in Practice 'cnews.train.txt' data cannot be uploaded because it is too large, so it needs to be decompressed and imported after compression. Use SVM to implement a simple text classification based on bag of words and support vector machine. import data 1234# importimport codecsimport osimport jieba Chinese news data is prepared as a sample data set. The number of training data is 50,000 and the number of test data is 10,000. All data is divided into 10 categories: sports, finance, real estate, home furnishing, education, technology, fashion, current affairs, games and entertainment . From the training text, you can load the code, view the data format and samples: 1234567891011data_train = './data/cnews.train.txt' # training data file name data_test = './data/cnews.test.txt' # test data file namevocab = './data/cnews.vocab.txt' # dictionarywith codecs.open(data_train, 'r', 'utf-8') as f: lines = f.readlines()# print sample contentlabel, content = lines[0].strip('\\r\\n').split('\\t')content Take the first item of the training data as an example to segment the loaded news data. Here I use the word segmentation function of LTP, you can also use jieba, and the segmentation results are displayed separated by \"/\" symbols. 123# print word segment resultssegment = jieba.cut(content)print('/'.join(segment)) To sort out the above logic a bit, implement a class to load training and test data and perform word segmentation. 123456789101112131415161718192021222324# cut datadef process_line(idx, line): data = tuple(line.strip('\\r\\n').split('\\t')) if not len(data)==2: return None content_segged = list(jieba.cut(data[1])) if idx % 1000 == 0: print('line number: {}'.format(idx)) return (data[0], content_segged) # data loading methoddef load_data(file): with codecs.open(file, 'r', 'utf-8') as f: lines = f.readlines() data_records = [process_line(idx, line) for idx, line in enumerate(lines)] data_records = [data for data in data_records if data is not None] return data_records# load and process training datatrain_data = load_data(data_train)print('first training data: label {} segment {}'.format(train_data[0][0], '/'.join(train_data[0][1])))# load and process testing datatest_data = load_data(data_test)print('first testing data: label {} segment {}'.format(test_data[0][0], '/'.join(test_data[0][1]))) After spending some time on word segmentation, you can start building a dictionary. The dictionary is built from the training set and sorted by word frequency. 12345678910111213141516171819202122def build_vocab(train_data, thresh): vocab = {'&lt;UNK&gt;': 0} word_count = {} # word frequency for idx, data in enumerate(train_data): content = data[1] for word in content: if word in word_count: word_count[word] += 1 else: word_count[word] = 1 word_list = [(k, v) for k, v in word_count.items()] print('word list length: {}'.format(len(word_list))) word_list.sort(key = lambda x : x[1], reverse = True) # sorted by word frequency word_list_filtered = [word for word in word_list if word[1] &gt; thresh] print('word list length after filtering: {}'.format(len(word_list_filtered))) # construct vocab for word in word_list_filtered: vocab[word[0]] = len(vocab) print('vocab size: {}'.format(len(vocab))) # vocab size is word list size +1 due to unk token return vocabvocab = build_vocab(train_data, 1) In addition, according to category, we know that the label itself also has a \"dictionary\": 12345678910def build_label_vocab(cate_file): label_vocab = {} with codecs.open(cate_file, 'r', 'utf-8') as f: for lines in f: line = lines.strip().split('\\t') label_vocab[line[0]] = int(line[1]) return label_vocablabel_vocab = build_label_vocab('./data/cnews.category.txt')print(f'label vocab: {label_vocab}') Next, construct the id-based training and test sets, because we only consider the bag of words, so the order of words is excluded. Constructed to look like libsvm can eat. Note that because the bag of word model 12345678910111213141516171819202122def construct_trainable_matrix(corpus, vocab, label_vocab, out_file): records = [] for idx, data in enumerate(corpus): if idx % 1000 == 0: print('process {} data'.format(idx)) label = str(label_vocab[data[0]]) # label id token_dict = {} for token in data[1]: token_id = vocab.get(token, 0) if token_id in token_dict: token_dict[token_id] += 1 else: token_dict[token_id] = 1 feature = [str(int(k) + 1) + ':' + str(v) for k,v in token_dict.items()] feature_text = ' '.join(feature) records.append(label + ' ' + feature_text) with open(out_file, 'w') as f: f.write('\\n'.join(records))construct_trainable_matrix(train_data, vocab, label_vocab, './data/train.svm.txt')construct_trainable_matrix(test_data, vocab, label_vocab, './data/test.svm.txt') Training process The remaining core model is simple: use libsvm to train the support vector machine, let your svm eat the training and test files you have processed, and then use the existing method of libsvm to train, we can change different parameter settings . The documentation of libsvm can be viewed here, where the \"-s, -t, -c\" parameters are more important, and they decide what you choose Svm, your choice of kernel function, and your penalty coefficient. 1234567891011121314from libsvm import svmfrom libsvm.svmutil import svm_read_problem,svm_train,svm_predict,svm_save_model,svm_load_model# train svmtrain_label, train_feature = svm_read_problem('./data/train.svm.txt')print(train_label[0], train_feature[0])model=svm_train(train_label,train_feature,'-s 0 -c 5 -t 0 -g 0.5 -e 0.1')# predicttest_label, test_feature = svm_read_problem('./data/test.svm.txt')print(test_label[0], test_feature[0])p_labs, p_acc, p_vals = svm_predict(test_label, test_feature, model)print('accuracy: {}'.format(p_acc)) After a period of training, we can observe the experimental results. You can change different svm types, penalty coefficients, and kernel functions to optimize the results.","link":"/SVM-based_text_classification_in_practice/"},{"title":"利用AI写一个『贪吃蛇游戏』","text":"在完成一些简单的任务之后，我觉得ChatGPT可以帮我完成一些更为复杂，甚至于可能我并不是特别擅长的任务，而我想到了，也许，可以帮我写一个简单的游戏。 贪多嚼不烂的道理我还是明白的，于是，我给自己提前就设定了一个目标： 游戏可以运行实现简单的规则减少BUG的存在可以开始和结束 带着这四个最朴素的愿景，让我们开始吧。 一贯的，一上来我就将问题抛给了chatGPT 1请用Python3.10 编写一个贪吃蛇的游戏，给我完整的代码。 这里我们需要注意一下，你的描述越清晰，得到的结果越有可能是你想要的。在这里，我告诉了chatGPT，我希望用Python写一个游戏，而且版本号是3.10。并且，我告诉了chatGPT，请给我完整的代码。因为如果我不这么说的话，那很有可能chatGPT会先给我一堆的步骤而不是实际的代码。 在之后，我得到了chatGPT的答复： 完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import pygameimport random# 游戏窗口的大小WINDOW_WIDTH = 800WINDOW_HEIGHT = 600# 蛇身和食物的大小BLOCK_SIZE = 20# 定义颜色WHITE = (255, 255, 255)BLACK = (0, 0, 0)RED = (255, 0, 0)# 初始化 Pygamepygame.init()# 创建游戏窗口window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))pygame.display.set_caption(&quot;贪吃蛇游戏&quot;)clock = pygame.time.Clock()def game_loop(): game_over = False # 蛇的初始位置和速度 snake_x = WINDOW_WIDTH // 2 snake_y = WINDOW_HEIGHT // 2 snake_x_change = 0 snake_y_change = 0 # 食物的初始位置 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE while not game_over: for event in pygame.event.get(): if event.type == pygame.QUIT: game_over = True elif event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT: snake_x_change = -BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_RIGHT: snake_x_change = BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_UP: snake_y_change = -BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_DOWN: snake_y_change = BLOCK_SIZE snake_x_change = 0 # 更新蛇的位置 snake_x += snake_x_change snake_y += snake_y_change # 绘制游戏窗口 window.fill(BLACK) pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.draw.rect(window, WHITE, [snake_x, snake_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.display.update() # 碰撞检测 if snake_x == food_x and snake_y == food_y: food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE clock.tick(20) # 控制游戏的帧率 pygame.quit()# 启动游戏game_loop() 从代码里我们可以看出来，文件引用了库pygame以及random, 其中，pygame需要我们单独安装一下: 1conda install pygame 建立了一个snake_v1.py的文件，并把这段代码粘贴到文件里之后，我开始尝试运行它： 1python ~/xx/game/snake_v1.py 事情果然没有想的那么简单，根本就是毫无动静。我开始询问chatGPT chatGPT给到的答案似乎没有什么意义，这些事情在开始之前我就已经做好了。在短暂的思考之后，我忽然想到，是否因为我是Mac系统，从而导致了窗口不出现，于是我继续问问题： 然后继续问： 这一次，我抓到了重点。不明白为什么刚才我提到我是Mac系统的时候他不告诉我，先不管这些，在其中添加这段代码后，游戏终于可以运行了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import pygameimport random# 游戏窗口的大小WINDOW_WIDTH = 800WINDOW_HEIGHT = 600# 蛇身和食物的大小BLOCK_SIZE = 20# 定义颜色WHITE = (255, 255, 255)BLACK = (0, 0, 0)RED = (255, 0, 0)# 初始化 Pygamepygame.init()# 创建游戏窗口window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))pygame.display.set_caption(&quot;贪吃蛇游戏&quot;)pygame.display.flip()clock = pygame.time.Clock()def game_loop(): game_over = False # 蛇的初始位置和速度 snake_x = WINDOW_WIDTH // 2 snake_y = WINDOW_HEIGHT // 2 snake_x_change = 0 snake_y_change = 0 # 食物的初始位置 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE while not game_over: for event in pygame.event.get(): if event.type == pygame.QUIT: game_over = True elif event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT: snake_x_change = -BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_RIGHT: snake_x_change = BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_UP: snake_y_change = -BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_DOWN: snake_y_change = BLOCK_SIZE snake_x_change = 0 # 更新蛇的位置 snake_x += snake_x_change snake_y += snake_y_change # 绘制游戏窗口 window.fill(BLACK) pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.draw.rect(window, WHITE, [snake_x, snake_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.display.update() # 碰撞检测 if snake_x == food_x and snake_y == food_y: food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE clock.tick(20) # 控制游戏的帧率 pygame.quit()# 启动游戏game_loop() 不知道各位看出什么问题了没有，问题如下： 动作太快了，完全是地狱级别的难度； 蛇似乎不会增长，吃了物品没有变长； 撞到墙壁后游戏虽然停止了，但是没任何提示，蛇就这么消失了； 于是，我只能让chatGPT再帮我优化代码： 当然，又有了其他问题， 蛇在撞到自己之后没有办法结束游戏 游戏无法退出 游戏无法重新开始 Game Over之后显示的是乱码 当然，有编程经验的人大概能明白，乱码的话一定是因为中文显示不正确，而目前，我也没打算支持中文。干脆用英文好了。 于是，我又开始提需求，在连续三四轮这种询问之后，终于来到了最后一次： 然后我得到了如下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137import pygameimport random# 游戏窗口的大小WINDOW_WIDTH = 800WINDOW_HEIGHT = 600# 蛇身和食物的大小BLOCK_SIZE = 20# 定义颜色WHITE = (255, 255, 255)BLACK = (0, 0, 0)RED = (255, 0, 0)# 初始化 Pygamepygame.init()# 创建游戏窗口window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))pygame.display.set_caption(&quot;贪吃蛇游戏&quot;)pygame.display.flip()clock = pygame.time.Clock()def game_loop(): game_over = False game_restart = False game_paused = False while not game_restart: if game_over: show_game_over() # 蛇的初始位置和速度 snake_x = WINDOW_WIDTH // 2 snake_y = WINDOW_HEIGHT // 2 snake_x_change = 0 snake_y_change = 0 # 蛇身的初始位置和长度 snake_body = [] snake_length = 1 # 食物的初始位置 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE score = 0 while not game_over: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() quit() elif event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT and snake_x_change != BLOCK_SIZE: snake_x_change = -BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_RIGHT and snake_x_change != -BLOCK_SIZE: snake_x_change = BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_UP and snake_y_change != BLOCK_SIZE: snake_y_change = -BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_DOWN and snake_y_change != -BLOCK_SIZE: snake_y_change = BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_SPACE: game_paused = not game_paused elif event.key == pygame.K_q: pygame.quit() quit() if game_paused: continue # 更新蛇的位置 snake_x += snake_x_change snake_y += snake_y_change # 碰撞检测 if snake_x == food_x and snake_y == food_y: # 蛇吃到了食物，增加长度和得分 snake_length += 1 score += 1 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE # 更新蛇的身体 snake_head = [snake_x, snake_y] snake_body.append(snake_head) if len(snake_body) &gt; snake_length: del snake_body[0] # 检测碰撞 if snake_head in snake_body[:-1] or snake_x &lt; 0 or snake_x &gt;= WINDOW_WIDTH or snake_y &lt; 0 or snake_y &gt;= WINDOW_HEIGHT: game_over = True # 绘制游戏窗口 window.fill(BLACK) pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE]) for body_part in snake_body: pygame.draw.rect(window, WHITE, [body_part[0], body_part[1], BLOCK_SIZE, BLOCK_SIZE]) # 显示实时分数 font = pygame.font.Font(None, 28) text = font.render(&quot;Score: &quot; + str(score), True, WHITE) window.blit(text, (10, 10)) pygame.display.update() clock.tick(10) # 控制游戏的帧率 # 重新开始游戏 game_loop()def show_game_over(): font = pygame.font.Font(None, 36) text = font.render(&quot;Game Over. Press Space to restart&quot;, True, WHITE) text_rect = text.get_rect(center=(WINDOW_WIDTH // 2, WINDOW_HEIGHT // 2)) while True: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() quit() elif event.type == pygame.KEYDOWN and event.key == pygame.K_SPACE: game_loop() window.fill(BLACK) window.blit(text, text_rect) pygame.display.update() clock.tick(10)# 启动游戏game_loop() 大家可以看看效果： 我们可以看到，这个游戏相对来说是比较完整了，速度正常了，有计分系统，蛇能正常增长，碰到自己游戏会结束，碰到墙壁后也会Game Over，并且可以通过SPACE来重新开始。并且，我还加入了按Q键的时候退出的功能。 当然，还可以继续完善，比如随着时间的流逝，速度上可以渐渐加快等等。就看你怎么想，然后其他的交给chatGPT。 通过这次的示例演示，其中重点不是教大家如何做一个贪吃蛇游戏，而是教大家如何利用chatGPT来解决你需要解决的问题。当然，我需要收回我开头说的话，chatGPT并不能帮你解决你不熟悉的问题。就比如，如果我完全不懂这其中内容的话，可能我窗口都打不开，我完全都不知道我什么时候才能解决Mac系统中不一样的部分，而也正是因为有一些简单的经验，才让我考虑的那个层面，从而针对性提问解决了问题。 所以要记住，AI并不能帮你解决你完全不懂的问题，起码，你要知道你想问什么，也要知道问题大概卡在哪里了，针对性继续提问。 最后，友情提示一下，不要用API来完成这一次次的对话，经验之谈，去买个Plus，比API交互便宜多了。你看那一串串的代码重复的给你写出来，你完全不知道会耗费多少Token。那些宝贵的Token，还是用在聊天窗无法完成的任务上比较合适。","link":"/Use-AI-to-write-a-snake-game/"},{"title":"导读：了解AI并使用它&#x2F;他&#x2F;她们","text":"如果你想开始学习AI应用开发，那么在学习之前，有一些学前提醒需要注意。在当今AI爆发的时代，学习AI应用开发需要的学习方法和策略也发生了变化。本课程的目标是通过多尝试、多体验、多做头脑风暴的学习方法，帮助学生在短时间内掌握AI应用开发的基本技能。我们并不会传授过于深奥的数学和理论知识，而是会通过简单易学的API，让学生能够快速上手开发实用价值的AI应用。因此，在学习本课程的过程中，我们鼓励学生多尝试、多体验、多做头脑风暴，以更加轻松、快速地掌握AI应用开发的技能。如果你不知道如何开始，你可以使用Jupyter或者Golab这样的工具来帮助自己入门。 多练习，多尝试，多交流 1. 多尝试运行和修改代码 为什么要多尝试运行和修改代码？因为这是学习AI最有效的方式之一。通过自己亲手运行代码并进行修改，可以更深入地理解算法和模型背后的原理，并从中学到许多实用的技巧。此外，尝试运行和修改代码也能够帮助你更好地掌握编程语言和工具。 而为了方便地进行代码实验，我们可以使用一些开源的工具。例如，Jupyter Notebook 是一个广泛使用的交互式笔记本工具，它支持多种编程语言，并且可以在本地运行。如果你不知道如何搭建环境， 除了本地启动Jupyter之外，你也可以直接使用微软的VSCode，可以直接调用本地Jupyter环境（推荐）。 也可以使用 Google 的 Colaboratory（简称 Colab） 这样的云端工具，只需要一个 Google 账号即可使用。 自然，这也不是全无门槛的，学会如何科学上网是必备技能。这一部分请原谅我无法教授，还是需要自行查找资料。 2. 多体验不同的AI工具 在学习 AI 应用开发的过程中，你需要了解当前市场上涌现的海量 AI 应用。通过体验这些 AI 应用，你可以更好地了解 AI 的能力和应用场景，也能够了解到当前 AI 技术的发展状况。 例如，你可以尝试使用一些人工智能工具来完成自己的工作，比如使用自然语言处理的工具来帮助你写作、使用机器学习的工具来进行数据分析、使用计算机视觉的工具来进行图像处理等等。此外，你还可以体验一些常用的 AI 应用，比如语音助手、智能家居、智能客服、智能医疗等等。通过这些体验，你可以深入了解 AI 技术在实际场景中的应用和效果，从而更好地理解 AI 技术的价值和未来发展方向。 在课程中，你还可以从我推荐中了解到一些最新的 AI 应用。你可以注册账号、下载应用，多去体验一下这些 AI 应用，这不仅能够激发你学习课程的动力，也能够打开你自己利用 AI 大模型能力的思路。 总之，多去体验各类 AI 应用能够帮助你更好地了解 AI 技术的应用和发展现状，也能够激发你对 AI 技术的兴趣和热情，从而更好地进行学习和实践。 本地搭建Stable Diffusion 这里给大家一个小小的建议，尽量不要用线上的图片生成AI去生成商业图片，会有法律隐患的。而如果是在本地架设的情况下，这种问题基本就不存在了。 3. 多交流 与周围的人以及朋友一起多做做头脑风暴，尝试寻找有趣的新产品的机会。事实上，这是一个非常好的建议。AI应用已经涌现出许多，但是有些应用只是简单地使用了现有的API，缺乏创意和创新。然而，还有很多应用具有独特的想法，有些甚至可以直接商业化。学习AI的目的在于学以致用，可以与身边对新一代AI应用有兴趣的人一起探讨，看看课程中介绍的各种方法和技巧能否用于不同的场景和角度。这才是学习这门课程的真正价值。当然，如果你对AI大模型的底层原理有兴趣，可以深入研究其中的数学原理和各种深度学习模型。现在，有能力构建大模型的人实在是太少了，而不是太多了。能够推动通用人工智能向前发展一小步，相信是所有AI从业者都梦寐以求的事情。所以，学习AI，不仅要学习知识，更要发挥创造力，发掘新的应用场景，才能真正做到学以致用。 [Stable Diffution 生成的填色图，完全可以变成一项生意，生成多张图做本书：Link](https://qiniu.hivan.me/picGo/Stable Diffution 生成的填色图，完全可以变成一项生意，生成多张图做本书：Link 使用AI工具改变你现在的工作方式 随着 ChatGPT、Whisper 和 Stable Diffusion 等强大的 AI 技术的出现，我们的学习和工作方式也需要跟着改变。现在，利用 AI 工具来改造自己的学习和工作流程已经成为一种趋势。通过将 AI 技术应用到各个方面，我们可以获得更加沉浸式的学习体验，同时也能够提高日常生活和工作的效率。在过去的几个月里，我自己也不断地研究和学习新技术，并通过 AI 工具来提高自己的效率。在这篇文章中，我想和大家分享一些我所使用的 AI 工具和优化流程。 ChatGPT，这是一种基于 GPT 技术的人工智能语言模型。我常常利用 ChatGPT 来帮助自己解决问题，比如在学习编程的过程中，我会输入一些代码，然后让 ChatGPT 来帮我检查代码的错误。ChatGPT 还可以用来进行翻译、摘要和生成文章等等。这种 AI 工具可以帮助我们更加高效地学习和工作。 Whisper，这是一种人工智能笔记工具。与传统笔记工具不同的是，Whisper 可以将我们所输入的笔记和文本转化为自然语言，并通过 AI 技术来优化笔记的布局和结构。这样一来，我们可以更加快速和方便地记录学习和工作中的重要信息，并将其整理成易于理解的形式。 Stable Diffusion，这是一种用于大规模数据处理和分析的 AI 工具。在我的研究工作中，我常常需要处理海量的数据，并对数据进行分析和建模。Stable Diffusion 的出现让我能够更加高效地处理数据，并且能够利用 AI 技术来进行数据建模和预测。 利用 AI 工具来改造学习和工作流程已经成为一种趋势。通过利用这些工具，我们可以更加高效地学习和工作，并且可以更加快速地解决问题。当然，这些工具只是 AI 技术应用的冰山一角，未来还将有更多更加强大和智能的 AI 工具出现，让我们拭目以待。 如何使用 ChatGPT 进行学习 随着 AI 技术的发展，ChatGPT 成为了许多人学习知识的“助教”。但是，有些人觉得 ChatGPT 没有多大用处，这可能是因为他们询问了过于宽泛的问题。实际上，ChatGPT 的作用是为我们提供有针对性的回答，只需询问具体问题即可。 当我们学习新知识时，我们可以请 ChatGPT 帮助我们解释我们不理解的内容。与搜索不同，ChatGPT 可以根据我们的追问提供更深入的解释，直到我们完全理解这个知识点为止。此外，ChatGPT 不仅可以解释概念，还可以解释代码。我们可以将需要解释的代码段粘贴到 ChatGPT 中，它将为我们提供详细的讲解。 此外，ChatGPT 作为一个 AI “助教”，它的知识广博、不知疲倦，极其耐心。我们不需要担心问题过于简单或产生心理压力，因为 ChatGPT 不会嫌麻烦或不耐烦。因此，与查找资料或询问他人相比，使用 ChatGPT 可以更高效地解决问题。 [问题具体化，将思考的过程交给自己，而获取知识交给ChatGPT，是我最常用的方式](https://qiniu.hivan.me/picGo/问题具体化，将思考的过程交给自己，而获取知识交给ChatGPT，是我最常用的方式 在 Poe 平台中，我们可以选择不同的语言模型，不仅可以使用 ChatGPT，还可以使用其他大型语言模型。这样，我们可以选择最适合自己的模型。 学会使用工具获取额外资料 在当今的数字时代，英文资料已经成为许多行业中获取第一手信息的主要来源。特别是在技术领域，大量的技术文档、API文档和博客文章都是用英文写成的。虽然英文阅读能力是每个人在学习和工作中必须掌握的技能，但对于许多人来说，英文阅读还是比较吃力的。然而，随着机器翻译技术的不断提高，人们已经越来越多地使用翻译插件，将英文资料转换为中英对照版本。 DeepL是一种在线翻译工具，它使用了深度学习技术，可以对英文文本进行准确的翻译。使用DeepL，只需将需要翻译的文本复制粘贴到工具中，它就可以快速将其翻译成目标语言。另外，DeepL还可以通过浏览器插件的形式直接嵌入到浏览器中，当您访问英文网页时，它会自动将其翻译成您的语言。 使用翻译插件可以帮助我们快速浏览英文资料，同时避免了语言障碍。当我们遇到一些翻译不准确的地方，可以快速查看英文原文，确保对资料的理解和应用。同时，翻译插件的中英对照形式也让我们能够更快速地阅读英文资料，从而提高我们的阅读效率。 除了文本资料外，现在还有越来越多的音视频资料，如播客和视频等。这些最新的资料往往只有音频或视频版，而没有文字版。但是通过语音识别和文本摘要技术，我们也可以快速将音频和视频转换为文本，并且生成一个摘要。这样，我们就可以先快速浏览一遍摘要，决定是否值得去完整地听或看。 现在市面上有许多这样的浏览器插件，例如 Glarity，可以帮助我们快速总结视频内容，再来决定是否要看。对于像约翰卡马克这样的大神的访谈，我们可以使用 ChatGPT 背后的语言模型来生成一个摘要，以快速浏览视频内容，确定是否值得花时间去看。 如何通过AI来阅读论文 在科技飞速发展的今天，不断学习新知识，跟上最新的技术进展是非常必要的。读论文是获取新知识的好方法。然而，阅读一篇论文是一项费时费力的工作，因为它通常包含大量的专业术语、公式和图表。但是，随着大型语言模型的出现，我们现在可以借助AI来阅读论文。 现在有很多工具可以帮助我们阅读论文。例如，scispace是一个网站，可以将要阅读的论文上传到其中。然后，我们可以向AI提出问题，以快速了解论文讲解了什么内容。scispace内置了许多你可能会关心的问题，并且可以直接选择回答的语言。此外，在阅读过程中，AI可以对公式、图表等内容进行详细解释，这些工具都可以大大降低阅读论文的门槛，提高掌握这些复杂知识的效率。 除了scispace之外，还有许多其他的工具可以帮助我们阅读论文。例如，ChatPDF是一个可以对PDF文件进行小结和提问的工具。将各种分析报告上传至ChatPDF中，可以快速获取所需的信息。 利用 AI 写代码 Demo：提高生产效率的新工具 GitHub Copilot 是一个利用 AI 技术帮助工程师写代码的工具。通过将需求描述给 ChatGPT，它可以快速生成可用的代码，帮助工程师节省时间和精力。使用 Copilot 写代码的体验非常棒，只需输入注释或代码的开头，Copilot 就能为你生成完整的代码。 对于一些简单的函数调用等胶水代码，Copilot 十有八九是能帮上忙的。即使有些代码不够完美，以它为基础改造比从头开始写更快。当你需要使用一些不熟悉的包时，Copilot 尤其有用。 如果你是一个工程师，安装 Copilot 并使用它写代码是提高生产效率的好方法。此外，使用 ChatGPT 和 Copilot 来帮助写 Demo 代码，可以帮助你快速实验需求，而不必费时查找文档和阅读教程。这些工具能让你更轻松地完成工作，提高你的生产力和效率。 在实际使用中，你可能需要花一些时间来熟悉和调整 Copilot 生成的代码。但是，一旦你熟悉了 Copilot，它将成为你编程工作中最有价值的助手之一。 当然，我是一个穷人，写代码并不是我的主业，所以我一般都使用ChatGPT来完成我为数不多的需求： ChatGPT和Copilot的代码正确率以及BUG率比多数工程师都要来的优秀，自然，也包括我自己。 使用AI获取灵感 现在随着人工智能技术的快速发展，越来越多的AI写作工具涌现出来，让人们在创作过程中更加得心应手。其中，最让人印象深刻的就是AI如何帮助我们获取灵感。虽然AI还没有完全替代人类的思维，但在寻找灵感方面，它们已经展现出了惊人的能力。 在实际的写作中，很多人并不依赖AI产生内容，但是当缺少灵感的时候，AI可以作为一个非常好的助手。例如，当你在围绕一个主题思考写作内容时，可以尝试使用notion.ai等工具寻找灵感。虽然这些工具的很多主意并不新颖，但它们往往可以给你带来意想不到的角度和思路。 此外，你还可以尝试通过人设的不同来让AI从另一个角度帮助你思考问题。例如，为ChatGPT设置一个不同的人设，让它模拟某个领域的专家来帮助你做头脑风暴。这个时候，虽然你只有一个人在思考，但是你却可以组织一个各路大神汇聚的团队帮助你思考问题，让你得到更多有价值的点子。 不仅如此，对于不同领域的人群，还有一些专门的AI工具可以帮助他们获取灵感。例如，Midjourney、Dall-E 2等工具可以让设计师在创作过程中更加得心应手，快速地制作出优秀的设计作品。 接受它，别被它替代 随着科技的快速发展，现代社会正在经历着巨大的变革。许多传统行业和工作岗位正在面临被自动化和数字化取代的风险。因此，我们必须积极地适应新的技术，以免被时代抛弃。尤其是人工智能的发展，不仅给我们的生活带来了诸多便利，也对我们的工作和职业规划产生了重大影响。因此，了解人工智能的基本原理和应用场景，具备使用和开发人工智能的能力，成为未来职场竞争力的关键。同时，我们也需要不断地学习和更新知识，跟上时代的步伐，不断提升自己的技能和能力，以应对快速变化的社会和市场。","link":"/Understanding_and_Utilizing_AI/"},{"title":"05 为文本分类","text":"Hi， 我是茶桁。 在前一讲中，我们观察到大型模型的确表现出良好效果。在情感分析任务中，通过使用 OpenAI API 提供的 Embedding，我们得到的结果要比能在单个机器上运行的较小模型（如T5-base）要好得多。然而，我们之前所选择的问题确实有些过于简单。我们将5个不同的评分分成了正面、负面和中性，同时还排除了相对难以判断的\"中性\"评价，因此我们判断准确率的提高相对较容易实现。但是，如果我们想要准确预测具体的分数呢？这将是我们接下来需要探索的问题。 训练机器学习模型 一种最简单的方法是利用我们获得的文本 Embedding 向量。这次我们将不再直接使用向量之间的距离，而是采用传统的机器学习方法进行分类。这样做的原因在于，仅仅使用向量之间的距离无法最大化利用已标注好的分数信息。 事实上，OpenAI在其官方教程中也给出了这样一个例子。我在这里提供了相应的GitHub代码链接，你可以查看一下。然而，为了避免过于自夸，我们希望将我们的结果与其他使用传统机器学习方法得到的结果进行比较。 因此，我重新找到了一个中文数据集来进行试验。该数据集是在中文互联网上容易找到的一份包含今日头条新闻标题和关键词的数据集，你可以在GitHub上直接获取数据，我在这里提供了链接。使用这个数据集的好处是，已经有人发布了预测实验的结果。我们可以将我们训练得到的结果与其进行比较。 处理数据 在训练模型之前，我们需要获取每个新闻标题的嵌入向量。为了做到这一点，我们使用 Python 数据处理库 Pandas 将相应的文本加载到内存中。然后，我们调用之前使用过的 OpenAI 嵌入接口，将返回结果存储在内存中，这些嵌入向量将作为我们机器学习模型的输入。 虽然这听起来非常简单直接，但是在实际操作中，我们需要注意一些细节。例如，我们需要在处理数据之前进行数据清洗，以去除无用的标点符号和停用词，并进行分词。这有助于提高我们模型的精度和可靠性。 一旦我们完成了数据清洗和嵌入向量的提取，我们就可以开始训练我们的机器学习模型了。在这个过程中，我们可以使用各种传统的机器学习算法，如决策树、支持向量机或朴素贝叶斯分类器等，来对数据进行分类。当然，我们也可以尝试一些更高级的算法，如深度学习，来进一步提高模型的精度和效果。 千万不要急着复制代码运行，请看完整篇文章。因为涉及API调用，涉及到你的Money 请注意：由于后面的代码可能会消耗相当数量的 Token，如果您使用的是免费的 $5 美元配额，建议您直接下载我在 Github 上提供的数据文件，以使用我已经处理好的数据。值得注意的是，如果您准备使用自己的数据进行处理，请确保您已经在算法上进行了必要的优化，以保证您的代码能够在合理的时间内完成处理。文章最后我会放上本节数据。 12345678910111213141516171819202122232425262728293031import pandas as pdimport tiktokenimport openaiimport osfrom openai.embeddings_utils import get_embedding, get_embeddingsopenai.api_key = &quot;OPENAI_API_KEY&quot;# embedding model parametersembedding_model = &quot;text-embedding-ada-002&quot;embedding_encoding = &quot;cl100k_base&quot; # this the encoding for text-embedding-ada-002max_tokens = 8000 # the maximum for text-embedding-ada-002 is 8191# import data/toutiao_cat_data.txt as a pandas dataframe# 这里的数据是从 Github上获取的# https://github.com/aceimnorstuvwxz/toutiao-text-classfication-datasetdf = pd.read_csv('./data/toutiao_cat_data.txt', sep='_!_', names=['id', 'code', 'category', 'title', 'keywords'])df = df.fillna(&quot;&quot;)df[&quot;combined&quot;] = ( &quot;标题: &quot; + df.title.str.strip() + &quot;; 关键字: &quot; + df.keywords.str.strip())print(&quot;Lines of text before filtering: &quot;, len(df))encoding = tiktoken.get_encoding(embedding_encoding)# omit reviews that are too long to embeddf[&quot;n_tokens&quot;] = df.combined.apply(lambda x: len(encoding.encode(x)))df = df[df.n_tokens &lt;= max_tokens]print(&quot;Lines of text after filtering: &quot;, len(df)) 注：以上内容是加载数据并做一些简单预处理的代码，你可以直接运行。下面则最好不要。 下面是请求 OpenAI API 获取 Embedding 的代码，但是运行时可能会遇到报错。 12345# randomly sample 1k rowsdf_1k = df.sample(1000, random_state=42)df_1k[&quot;embedding&quot;] = df_1k.combined.apply(lambda x : get_embedding(x, engine=embedding_model))df_1k.to_csv(&quot;./data/toutiao_cat_data_10k_with_embeddings.csv&quot;, index=False) 当我们直接运行这个代码之后，很大几率会遇到一个报错，因为这个数据处理过程中也是有一些坑在内的。 在使用 OpenAI 提供的接口时，我们需要注意两个坑点。 第一个坑点是关于接口限制。我们使用的 text-embedding-ada-002 模型，每条记录的长度被限制在了 8191 个标记。在调用 API 之前，我们需要计算每条记录的标记数，如果超过了 8000 个，就需要过滤掉。在我们的数据集中，只有新闻标题，因此这个长度限制不会成为问题。但是在使用其他数据集时，你需要注意数据是否超出了长度限制。如果超过了长度限制，你需要对数据进行过滤或者对文本进行截断，只使用文本的最后 8000 个标记。 为了保证计算出来的标记数和 OpenAI 的一致，我们在这里使用了 Tiktoken 库，并且采用了 cl100k_base 这种编码方式。 第二个坑点是关于 API 的调用限制。OpenAI 对 API 的调用进行了限速，如果你过于频繁地调用，就会遇到限速的错误。而且，如果你在遇到错误后继续调用，限速的时间还会被延长。为了解决这个问题，我们可以使用 Python 库 backoff。在调用 API 时，如果遇到错误，就等待一段时间，如果连续出现错误，就拉长等待时间。我们可以使用 backoff 改造代码来解决这个问题。下面是经过 backoff 改造后的代码。 当然，你首先还是需要安装backoff到你的conda环境内。 1conda install -c conda-forge backoff 换成PIP安装也可以，效果一致 接着，让我们来看修改后的部分代码： 1234567891011import backoff@backoff.on_exception(backoff.expo, openai.error.RateLimitError)def get_embedding_with_backoff(**kwargs): return get_embedding(**kwargs)# randomly sample 10k rowsdf_10k = df.sample(10000, random_state=42)df_10k[&quot;embedding&quot;] = df_10k.combined.apply(lambda x : get_embedding_with_backoff(text=x, engine=embedding_model))df_10k.to_csv(&quot;./data/toutiao_cat_data_10k_with_embeddings.csv&quot;, index=False) 通过使用 backoff 库，我们指定了在遇到 RateLimitError 的时候，按照指数级别增加等待时间，这样可以确保我们的程序可以顺利运行。 然而，如果你直接运行上面那个代码，大约需要 2 个小时才能处理完 1 万条数据。而我们的数据集里有 38 万条数据，如果按照这种方式处理，需要 3 天 3 夜才能把训练数据处理完，这样显然不怎么实用。所以，我们需要采取一些措施来加快处理速度。 首先，我们需要了解慢的原因。其中一个原因是因为限速，即每分钟 API 调用次数的限制。虽然 backoff 可以确保程序不会因为失败而终止，但是我们仍然受到了限制。另一个原因是因为延时，因为我们是按照顺序一个个调用 Embedding 接口，每一次调用都要等前一次调用结束后才会发起请求，这更进一步拖长了处理数据所需要的时间。 为了加快处理速度，我们可以采用多线程处理的方式，这样就可以实现多条数据并行请求。另外，我们还可以使用分布式处理的方法，将数据集分成多个部分，分别在不同的计算机上进行处理。这样可以大大缩短处理时间，提高程序的效率。 注：如果您想了解更多关于 OpenAI 对于不同模型的限速的信息，可以点击这个链接查看。 要解决这个问题，我们可以采用一些方法来提高处理速度。OpenAI 支持批量调用接口，您可以在一个请求里一次批量处理很多个请求。通过将 1000 条记录打包在一起处理，速度将会快很多。我在下面放置了一些对应的代码，您可以试着执行一下，处理这 38 万多条的数据，也就个把小时。 除此之外，您也可以考虑优化您的代码，例如使用更高效的算法和数据结构，以减少处理时间。此外，您还应该注意 OpenAI 的限速不仅仅是针对请求数的，也限制您每分钟可以处理的 Token 数量。因此，您需要根据每条数据包含的 Token 数自行计算一次打包多少条记录才能避免超出限制。 1234567891011121314151617181920212223242526import backofffrom openai.embeddings_utils import get_embeddingsbatch_size = 1000@backoff.on_exception(backoff.expo, openai.error.RateLimitError)def get_embeddings_with_backoff(prompts, engine): embeddings = [] for i in range(0, len(prompts), batch_size): batch = prompts[i:i+batch_size] embeddings += get_embeddings(list_of_text=batch, engine=engine) return embeddings# randomly sample 10k rowsdf_all = df# group prompts into batches of 100prompts = df_all.combined.tolist()prompt_batches = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]embeddings = []for batch in prompt_batches: batch_embeddings = get_embeddings_with_backoff(prompts=batch, engine=embedding_model) embeddings += batch_embeddingsdf_all[&quot;embedding&quot;] = embeddingsdf_all.to_parquet(&quot;./data/toutiao_cat_data_all_with_embeddings.parquet&quot;, index=True) 最后一个需要注意的点是，对于这样的大数据集，我们不应该存储成 CSV 格式。特别是我们获取到的 Embedding 数据，它是由很多浮点数组成的。如果我们存储成 CSV 格式，那么它会把本来只需要 4 个字节的浮点数都用字符串的形式存储下来，这会浪费好几倍的空间，并且写入的速度也会变得很慢。因此，我在这里采用了 parquet 这个序列化的格式。使用 parquet 格式可以节省空间并提高写入速度，整个存储的过程只需要 1 分钟左右。另外，为了确保数据的安全，我们还可以对 parquet 文件进行加密和压缩，这样可以进一步减小存储空间，并且保护数据的机密性。 *训练模型看效果* 我们已经完成了数据处理，接下来可以开始尝试模型训练了。如果你担心你的 API 调用次数会因此过多，你可以直接在我的 GitHub 上下载我处理好的数据集，并使用它来训练你的模型。此外，我还可以为你提供一些额外的建议和技巧，以帮助你更好地利用这个数据集，例如更好的模型选择、调整等。 1234567891011121314151617181920from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import classification_report, accuracy_scoretraining_data = pd.read_parquet(&quot;data/toutiao_cat_data_all_with_embeddings.parquet&quot;)training_data.head()df = training_data.sample(50000, random_state=42)X_train, X_test, y_train, y_test = train_test_split( list(df.embedding.values), df.category, test_size=0.2, random_state=42)clf = RandomForestClassifier(n_estimators=300)clf.fit(X_train, y_train)preds = clf.predict(X_test)probas = clf.predict_proba(X_test)report = classification_report(y_test, preds)print(report) 模型训练的代码非常简单，但是如果你想提高其准确率，还有一些技巧可以使用。考虑到运行时间的因素，我这里直接随机选取了里面的 5 万条数据，4 万条作为训练集，1 万条作为测试集。然后通过最常用的 scikit-learn 这个机器学习工具包里面的随机森林（RandomForest）算法，做了一次训练和测试。在我的电脑上，大概 10 分钟可以跑完，整体的准确率可以达到 84%。 如果你想提高准确率，可以考虑增加数据量、调整算法参数或者使用其他算法。使用更多的数据，特别是多样性更高的数据，可以帮助你的模型更好地捕捉规律。调整算法参数可以帮助你找到最优的模型，从而提高准确率。同时，你可以尝试使用其他算法，如支持向量机（SVM）或神经网络，来比较不同算法之间的效果差异。 此外，你还可以使用交叉验证来评估模型的准确率。交叉验证可以帮助你评估模型的泛化能力，即模型在新的数据上的表现能力。最后，你可以尝试使用集成学习（ensemble learning）技术，如 bagging 或 boosting，来进一步提高模型的准确率。 随机森林是一种效果不错的算法，但是它的运行速度相对较慢。为了更好地处理数据，我们可以使用更简单的逻辑回归（LogisticRegression）算法。然而，我们需要在整个数据集上运行这个算法。同样地，我们将使用80%的数据作为训练集，剩下的20%作为测试集。虽然我们的数据量比之前的四万条数据要多几倍，但是运行时间只需要三到四分钟，最终的准确率也能达到86%。 123456789101112131415from sklearn.linear_model import LogisticRegressiondf = training_dataX_train, X_test, y_train, y_test = train_test_split( list(df.embedding.values), df.category, test_size=0.2, random_state=42)clf = LogisticRegression()clf.fit(X_train, y_train)preds = clf.predict(X_test)probas = clf.predict_proba(X_test)report = classification_report(y_test, preds)print(report) 输出结果： 这个结果已经比我们下载数据集的 GitHub 页面里看到的效果好了，那个的准确率只有 85%。 可以看到，通过 OpenAI 的 API 获取到 Embedding，然后通过一些简单的线性模型，我们就能获得很好的分类效果。我们既不需要提前储备很多自然语言处理的知识，对数据进行大量的分析和清理；也不需要搞一块昂贵的显卡，去使用什么深度学习模型。只要 1～2 个小时，我们就能在一个几十万条文本的数据集上训练出一个非常不错的分类模型。 指标：一点机器学习小知识 刚刚我们解释了分类报告中的指标含义。每一行代表一个类别，包含准确率（Precision）、召回率（Recall）、F1分数和支持样本量（Support）这四个指标。为了更好地理解，我们将使用今日头条的新闻标题数据集进行说明。 准确率表示模型正确判断为该类别的标题所占的比例，即在所有判断为该类别的标题中有多少是真正属于该类别。举个例子，模型判断有100个标题属于农业新闻，但实际上只有83个标题是农业新闻，那么准确率就是0.83。准确率越高越好，但是并不意味着准确率达到100%就代表模型完全正确，因为模型可能会漏判，所以我们还需要考虑召回率。 召回率表示模型正确判断为该类别的标题占实际该类别下所有标题的比例，即没有漏掉的比例。例如，模型判断有100个标题属于农业新闻，这100个标题实际上都是农业新闻。准确率已经达到100%，但是实际上我们共有200条农业新闻。因此，在农业新闻类别中，我们的召回率只有100/200 = 50%。 因此，评估模型效果时需要考虑准确率和召回率，综合考虑这两个指标得出的结果就是F1分数。F1分数是准确率和召回率的调和平均数，即 F1 Score = 2 * (Precision * Recall) / (Precision + Recall)。当准确率和召回率都为100%时，F1分数也为1。如果准确率为100%，召回率为80%，那么计算得到的F1分数为0.88。F1分数越高越好。 支持样本量表示数据中实际属于该类别的样本数量。一般来说，样本数量越多，该类别的训练结果就越准确。 分类报告中的每一行代表一个类别，包含四个指标，而最后三行是针对整个测试数据集的结果，因此支持样本量都是1万个。 第一行的accuracy只有一个指标，虽然它在F1 Score这一列中，但它不是F1分数的含义。它表示模型总体上判断正确的分类数除以测试样本数，即模型的整体准确率。 第二行的macro average是宏平均，它将每个类别计算得到的指标加在一起取平均。宏平均对于数据分类不平衡的情况非常有用。比如，假设我们进行情感分析，其中90%的样本属于正面情感，而10%的样本属于负面情感。在这种情况下，如果我们的模型在正面情感方面的预测效果很好，准确率达到了90%，但在负面情感方面的准确率只有50%。如果只看整体数据，准确率似乎很高，因为正面情感的样本很多。但是对于我们的目标来说，即找到具有负面情感的客户并与他们沟通、进行赔偿，整体准确率就没有什么用了。而宏平均会将整体准确率计算为(90% + 50%)/2 = 70%。这并不是一个很好的预测结果，我们需要进一步优化模型。宏平均在处理数据样本不平衡的情况下非常有用。 第三行的weighted average是加权平均，它是根据每个类别支持的样本数量对每个指标进行加权平均计算。无论是准确率、召回率还是F1分数，都需要按照各个类别的支持样本量进行加权平均。 小结 好了，今天的这一讲到这里就结束了，最后我们来回顾一下。在这一讲中，我们学到了如何使用 OpenAI 的 API 来获取文本的 Embedding。虽然接口不复杂，但是有些细节需要我们注意。我们需要考虑模型能够接受的最大文本长度、API 本身的限速以及网络延时带来的问题。因此，我们提供了一些解决方案，例如使用 Tiktoken 计算样本的 Token 数量并进行过滤，以及在遇到限速问题时通过 backoff 进行指数级别的时间等待。此外，我们还介绍了如何通过一次性批量请求一批数据，最大化我们的吞吐量来解决问题。对于返回的结果，我们可以通过 parquet 这样序列化的方式保存数据，来减少数据尺寸。 除此之外，我们还学到了如何直接利用拿到的 Embedding，简单调用一下 scikit-learn，通过机器学习的方法，进行更准确的分类。我们最终把 Embedding 放到一个简单的逻辑回归模型里，就取得了很不错的分类效果。因此，我们可以说这两个知识点都是非常有用的，特别是在处理大量文本数据时。你学会了吗？希望这些知识点能够对你有所帮助！ 在本讲中，我们学习了如何利用 OpenAI 来获取文本的 Embedding，以及如何通过传统的机器学习方式来进行训练，并评估训练的结果。但是，我们可以进一步探索和扩展这个主题。 我们之前使用了 Amazon1000 条食物评论的情感分析数据集。在该数据集中，我们已经成功获取并保存了 Embedding 数据。那么，为什么不试着在完整的数据集上，训练一个能够区分从 1 分到 5 分每一个级别的机器学习模型呢？这不仅可以帮助你更深入地了解 Embedding 技术，还可以提高你的机器学习技能。最终，你可以将你的测试结果分享出来，与其他人比较，看看效果如何。 为方便大家，我在这里提供了整个原始数据集的下载链接。请随意下载，使用并测试你的机器学习模型。如果你感到收获颇丰，也欢迎你将本节课分享给其他人，让更多人了解如何用 Embedding 技术进行文本分类。 本文中所用到的最终训练完成的模型下载地址（百度云盘）： 链接: https://pan.baidu.com/s/1yoxcDq5B6K0TZti-pfKRww?pwd=xt6m 提取码: xt6m 复制这段内容后打开百度网盘手机App，操作更方便哦 --来自百度网盘超级会员v6的分享","link":"/Text-Classification/"},{"title":"13 使用多步提示语让AI帮你写测试","text":"Hi，大家好，我是茶桁。 很遗憾在上一讲，也就是第12讲的时候，咱们对于利用AI写一个VBA宏来执行Excel任务的过程并不顺利，仔细想来既然大家都在这里看这个系列文章了，应该也基本都会Python的，所以一个Excel自动化也并无太大影响，毕竟，这种商业软件的集成一定是早晚的事情，咱们也不必在这里死磕这一个问题。 那么本节课程呢，我们会通过chatGPT的不断交互，去完成一个测试任务。 在很多时候，我们探索性开发一些功能可以极大提高我们的效率，但是这个过程并不能做成一个完整的产品。我们理想中的产品应该是“自动化”的，我们只需要用自然语言输入自己的需求，对应的代码就自动写出来了。 那么如果中间出现了问题怎么办？当然是AI可以自己拿到反馈自己更正自己了，完全不需要人工去介入调试。 下面，让我们开始吧。 代码的起源 让AI自己调试自己的需求听起来是不是很不可思议？随着GPT-4的发布，还有就是未来模型能力的进一步增强，这个骑士并不是遥不可及。是的，我又在这里贩卖焦虑了，那些低廉的测试们，想要自己的退路了吗？ 眼下，我们只有GPT-3.5的API权限。所以我们这一次无法一步到底，目标还是需要低一点，先通过大语言模型，帮助我们写单元测试代码。 整个过程是一个自动档的体验，只是能够提供的能力还相对比较简单，仅限于为现有代码提供单元测试而已。 其实，很早的时候OpenAI官方就在Cookbook中提供了类似的思路和示例，可以参见Unit test writing using a multi-step prompt，不过这个例子里面的代码已经无法使用了，因为对应的code-davinci-002模型已经被OpenAI下线了。但是示例里，分步骤分析问题，通过多个Prompts来完成单元测试的想法，还是完全可以拿来借鉴的。 虽然模型会变，代码也就需要跟着会改变，但是我相信这一课一定能打开你的思路，随着你拿到GPT-4的API乃至未来可能会出现的GPT-5，你都完全可以完成更复杂的“自动写代码”的程序。 先从一个起点开始 为了写测试，我们需要首先编写一个程序。为了避免选择 Leetcode 这样的题库，因为这些题目可能在 AI 的训练数据集中出现并且AI已经知道了答案。因此，我选择了一个有趣的小题目，即让 Python 根据我们输入的一个整数代表的秒数，格式化成一段自然语言描述的时间。例如，输入 1 将返回 1秒，输入 61 将返回 1分钟1秒。为了实现这个功能，我们需要考虑处理输入、分析输入并将其转化为可读的输出。我们可以使用 Python 中的 datetime 模块和字符串处理功能来完成这个任务。我们可以编写单元测试来确保代码的正确性，包括测试各种输入组合的情况，并确保输出符合预期。我们还可以考虑添加错误处理，例如，如果输入不是整数，则返回错误消息。因此，编写测试代码可以帮助我们确保代码的正确性并提高代码的可读性。 让我们从简单需求开始： 123456咱们用Python写一个函数，进行时间格式化的输出，比如:输入 输出1 1s61 1min1s要求仅需要格式化到小时(?h?min?s)就可以了。 当然，我们是讲这个需求直接抛给chatGPT，让其待我们实现，然后我得到了如下程序： 1234567891011121314def format_time(seconds): hours = seconds // 3600 minutes = (seconds % 3600) // 60 seconds = seconds % 60 time_string = &quot;&quot; if hours &gt; 0: time_string += f&quot;{hours}h&quot; if minutes &gt; 0: time_string += f&quot;{minutes}min&quot; if seconds &gt; 0: time_string += f&quot;{seconds}s&quot; return time_string 并且，chatGPT还十分贴心的给出了一些示例用法： 现在让我们放在VSCode里尝试一下： 似乎很顺利，没有报错，可以执行，而且目测代码也完成了我们想要的基本功能，使用chatGPT给的示例测试一下看看： 恩，完全没问题。 好了，现在可以让我们完成单元测试的代码了，当然，依然是将问题交给chatGPT： 然后我得到了如下回复： 这里我们需要安装一个新库，在你的命令行内输入： 1conda install pytest 将chatGPT给到的代码改动一下，因为毕竟我们是在一个文件内定义的类，并不存在引入的情况： 12345678910111213141516171819202122232425262728import pytestdef test_format_time(): # 测试秒数为 1 的情况 assert format_time(1) == &quot;1s&quot; # 测试秒数为 61 的情况 assert format_time(61) == &quot;1min1s&quot; # 测试秒数为 3661 的情况 assert format_time(3661) == &quot;1h1min1s&quot; # 测试秒数为 3600 的情况 assert format_time(3600) == &quot;1h&quot; # 测试秒数为 0 的情况 assert format_time(0) == &quot;&quot; # 测试负数秒数的情况 assert format_time(-10) == &quot;&quot; # 测试较大秒数的情况 assert format_time(123456789) == &quot;34293h21min29s&quot; # 在此添加更多的测试用例...# 执行单元测试# pytest.main() 好的，我们的单元测试写完了, 下课，咱们下期再见。 。。。 当然是开玩笑的，哪有这么简单。不知道有多少人真的做过程序员或者测试，聪明如你们，当然能从这段代码中看到还存在问题 虽然这个测试考虑到了负数，考虑到了超过24小时较大秒数的情况，但是依然有未考虑到的情况，比如说，我们如果输入了浮点数1.0， 或者字符串abc，在活着Null这样的空值怎么办？虽然前端可以控制输入类型来避免一些情况发生，但是无论如何，我们无法相信前端，并不是因为前端程序员不给力，而是前端是可以被篡改的。我们不知道前端传回来的内容会发生怎样的变化，所以后端校验必须严谨而完整。 下面，让我们继续完善吧，接着我们就要离开WebGPT的交互，转而使用API了，所以请在您的代码内设置好相应的 openai.api_key 分步提示语 我们要明白，就算有了AI，也并不是把问题一股脑的塞给他就可以解决了。我们需要的是反过来自己思考，如果我们自己来为一段代码写单元测试，我们自己会怎么做？ 而这些想法，最后就会变成在chatGPT里的Prompts，最终由chatGPT告诉我们答案。 在文章开头我分享的Cookbook里的那个例子里就提供了一份很好的思路，在里面将问题拆成了三个步骤： 把代码提交给大语言模型，让大语言模型解释一下，这个代码是在干什么。这个步骤很重要，因为它可以帮助我们更好地理解代码的含义以及逻辑。如果大语言模型的解释不够详细，我们可以再次提交代码，直到我们完全理解了它的含义。 把代码以及代码的解释一起交给大语言模型，让大语言模型规划一下，针对这个代码逻辑，我们到底要写哪几个 TestCase。如果在这个过程里，大语言模型规划的 TestCase 数量太少，那么我们可以重复第二步，让 AI 多生成几个 TestCase。这样可以帮助我们更全面地测试代码，确保代码质量。 针对上面生成的 TestCase 的详细描述，我们再次提交给大语言模型，让它根据这些描述生成具体的测试代码。在这个过程中，我们还会对生成的代码进行一次语法检查，如果语法检查没法通过，我们就要让 AI 重新生成一下。这个可以避免因为大语言模型的概率采样不稳定，导致生成的代码无法运行的问题。同时，我们还可以对生成的代码进行一些修改，比如添加注释，让代码更加易读易懂。这个步骤可以帮助我们更好地理解代码的结构，以及代码所要实现的功能。 到最后，我们当然需要实际运行一下这些代码，看看我们的代码是否能够通过这些自动化测试。 自己的代码自己解释 我们将步骤一步步拆解开来，通过Python程序把整个过程“自动化”： 123456789101112131415161718192021222324252627282930313233343536373839404142434445def gpt(prompt, model = 'text-davinci-002', temperature = 0.4, max_tokens = 1000, top_p = 1, stop = ['\\n\\n', '\\n\\t\\n', '\\n \\n']): response = openai.Completion.create( model = model, prompt = prompt, temperature = temperature, max_tokens = max_tokens, top_p = top_p, stop = stop ) message = response['choices'][0]['text'] return messagecode = &quot;&quot;&quot;def format_time(seconds): hours = seconds // 3600 minutes = (seconds % 3600) // 60 seconds = seconds % 60 time_string = &quot;&quot; if hours &gt; 0: time_string += f&quot;{hours}h&quot; if minutes &gt; 0: time_string += f&quot;{minutes}min&quot; if seconds &gt; 0: time_string += f&quot;{seconds}s&quot; return time_string&quot;&quot;&quot;def explain_code(function_to_test, unit_test_package = 'pytest'): prompt = f&quot;&quot;&quot;# How to write great unit tests with {unit_test_package}In this advanced tutorial for experts, we'll use Python 3.10 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function.```python{function_to_test}```Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.- First,&quot;&quot;&quot; response = gpt(prompt) return response, promptcode_explaination, prompt_to_explain_code = explain_code(code)print(code_explaination) 在这一步中，我们所写的代码做了以下几件事情： 首先，我们定义一个gpt的函数，对调用GPT3.5的模型做了一个简单的封装。其中有两点需要特别注意一下： 我们默认使用了 text-davinci-002 模型，这是一个通过监督学习微调的生成文本的模型。因为这里我们希望生成目标明确的文本的代码解释，所以选用了这个模型。 我们对 stop 做了特殊的设置，只要连续两个换行或者类似连续两个换行的情况出现，就中止数据的生成。这是避免模型一口气连测试代码也生成出来。那样的话，我们没法对测试代码的生成提出具体的要求。通过 stop，我们可以确保在第一步，只解释现在的功能代码有什么用。此外，我们还对 stop 进行了调优，确保在生成代码解释时不会因为过度使用 stop 而出现信息不完整的情况。具体来说，我们设置了一个阈值，只有当连续两个换行或类似换行的情况出现的次数达到阈值时，才会中止数据的生成。 接下来，我们可以进一步提高代码的解释的准确性。我们可以通过以下几个步骤来实现： 确定使用pytest测试包。 提供要测试的代码以及相应的上下文。 指示AI对代码的功能进行详细描述。 使用“- First”等引导词，引导GPT模型逐步分行描述代码的功能。 这些步骤可以让我们的代码解释更加清晰明了。此外，我们也可以通过提供更详细的上下文和示例来帮助GPT模型对代码的功能进行更准确的描述。例如，我们可以提供更多的测试用例，以确保代码的正确性，并帮助GPT模型更好地理解代码的功能。同时，我们还可以提供更多的注释和解释，以便其他人更好地了解我们的代码。 输出结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051```python seconds // 3600 ``` is dividing the number of seconds by 3600 and discarding the remainder. For example, `7200 // 3600` returns `2` because there are `2` hours in `7200` seconds.- Second, ```python (seconds % 3600) // 60 ``` is dividing the remainder of the division of `seconds` by `3600` by `60`. For example, `7200 % 3600` returns `0` because there are no seconds remaining after the division by `3600`. Therefore, `(7200 % 3600) // 60` returns `0` because there are no minutes remaining after the division by `60`.- Third, ```python seconds % 60 ``` is returning the remainder of the division of `seconds` by `60`. For example, `7200 % 60` returns `0` because there are no seconds remaining after the division by `60`.- Fourth, ```python time_string = &quot;&quot; ``` is initializing an empty string to be used to store the formatted time.- Fifth, ```python if hours &gt; 0: ``` is checking if the number of `hours` is greater than `0`. If it is, the following code will be executed. ```python time_string += f&quot;{hours}h&quot; ``` This code is adding the number of `hours` to the string `time_string` with the `h` character after it. For example, if `hours` is `2`, the string `time_string` will be updated to `&quot;2h&quot;`.- Sixth, ```python if minutes &gt; 0: ``` is checking if the number of `minutes` is greater than `0`. If it is, the following code will be executed. ```python time_string += f&quot;{minutes}min&quot; ``` This code is adding the number of `minutes` to the string `time_string` with the `min` characters after it. For example, if `minutes` is `2`, the string `time_string` will be updated to `&quot;2min&quot;`.- Seventh, ```python if seconds &gt; 0: ``` is checking if the number of `seconds` is greater than `0`. If it is, the following code will be executed. ```python time_string += f&quot;{seconds}s&quot; ``` This code is adding the number of `seconds` to the string `time_string` with the `s` character after it. For example, if `seconds` is `2`, the string `time_string` will be updated to `&quot;2s&quot;`.- Eighth, ```python return time_string ``` is returning the string `time_string`. 运行代码后，AI回复了我们几个步骤，详细秒数了我们格式化时间的代码是如何做的。 自己的解释自己实现 当然接下来，我们就需要根据生成的这个详细描述，请AI为我们制定一下具体的测试计划了： 1234567891011121314151617181920def generate_a_test_plan(full_code_explaination, unit_test_package=&quot;pytest&quot;): prompt_to_explain_a_plan = f&quot;&quot;&quot; A good unit test suite should aim to:- Test the function's behavior for a wide range of possible inputs- Test edge cases that the author may not have foreseen- Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain- Be easy to read and understand, with clean code and descriptive names- Be deterministic, so that the tests always pass or fail in the same way`{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):-&quot;&quot;&quot; prompt = full_code_explaination + prompt_to_explain_a_plan response = gpt(prompt) return response, prompttest_plan, prompt_to_get_test_plan = generate_a_test_plan(prompt_to_explain_code + code_explaination)print(test_plan) 我们整个测试计划的提示语，同样经过了精心设计。我们首先对 AI 的测试用例做出了以下要求： 在考虑输入范围时，测试用例应尽可能覆盖更广的范围。 AI 应考虑到一些边界条件，这些条件可能比代码作者预想的更加复杂。 我们希望 AI 能够充分利用 pytest 这个测试包的特性。 测试用例应该易于阅读和理解，测试代码应该简洁明了。 测试代码的输出结果应该是确定的，要么通过，要么失败，不应该有随机性。 在这一步之后，我们并没有让 AI 立即开始编写测试代码。相反，我们提供了几个例子来让 AI 生成一系列示例。我们对测试用例的提示非常详细，这也是我们之前没有让 AI 直接生成测试用例的原因。因为这种方法无法在提示语中插入这些详细的要求。对于具体的测试用例，我们只能希望 AI 能够自行想出更多的例子。 最后，我们的提示语既包括了第一步要求解释代码内容的要求，也包括了 AI 生成的代码解释的要求，以及我们在这里新增的测试用例要求。这提供了非常详细的上下文，使得 AI 的表现更好，更具有逻辑性。此外，我们建议 AI 参考其他相关测试用例，以确保测试的全面性和正确性。 然后AI输出了结果给我： 123456789101112131415161718192021222324252627The `seconds` input is a positive integer: - `seconds` is less than 60 - `seconds` is equal to 60 - `seconds` is greater than 60 but less than 3600 - `seconds` is equal to 3600 - `seconds` is greater than 3600- The `seconds` input is a negative integer: - `seconds` is less than -60 - `seconds` is equal to -60 - `seconds` is greater than -60 but less than -3600 - `seconds` is equal to -3600 - `seconds` is greater than -3600- The `seconds` input is a float: - `seconds` is less than 0.0 - `seconds` is equal to 0.0 - `seconds` is greater than 0.0 but less than 60.0 - `seconds` is equal to 60.0 - `seconds` is greater than 60.0 but less than 3600.0 - `seconds` is equal to 3600.0 - `seconds` is greater than 3600.0- The `seconds` input is a string: - `seconds` is an empty string - `seconds` is a string that can be parsed to an integer - `seconds` is a string that can be parsed to a float - `seconds` is a string that cannot be parsed to an integer or a float- The `seconds` input is None: - `seconds` is None 我运行了一下这个代码，可以看到，AI 提供了很多测试用例。并且，里面考虑了好几种情况，包括我们前面提到的负数这样的特殊条件，也包括输入字符串，以及 None 这样的内容。不仅如此，我们还可以探索更多的情况，例如小数和分数等。此外，我们可以调整代码中的参数，来观察AI生成的测试用例数量和质量。 不过，生成哪些用例其实是有一定的随机性的。这个也是大语言模型的一个缺点，就是可控性差。有时候，AI 可能就只生成了 3 个用例，那样的话就会有很多情况我们的用例覆盖不到。 所以，我们可以在生成用例之后，加一个步骤，检查一下到底生成了多少个用例。如果太少的话，我们就让 AI 再生成一些。我在下面给了一段示例代码，通过“-”这样一个换行加横杆的标记来判断之前生成的测试用例数量，如果比我们设定的下限少，我们就再添加一段提示语，让 AI 再生成一些。 这里的提示语，我们要特别提醒 AI 考虑一下测试罕见情况和边界条件，例如极大或极小的输入值，或者一些异常情况的处理。 123456789101112131415not_enough_test_plan = &quot;&quot;&quot;The function is called with a valid number of seconds - `format_time(1)` should return `&quot;1s&quot;` - `format_time(59)` should return `&quot;59s&quot;` - `format_time(60)` should return `&quot;1min&quot;`&quot;&quot;&quot;approx_min_cases_to_cover = 7elaboration_needed = test_plan.count(&quot;\\n-&quot;) +1 &lt; approx_min_cases_to_cover if elaboration_needed: prompt_to_elaborate_on_the_plan = f&quot;&quot;&quot;In addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):-&quot;&quot;&quot; more_test_plan, prompt_to_get_test_plan = generate_a_test_plan(prompt_to_explain_code + code_explaination + not_enough_test_plan + prompt_to_elaborate_on_the_plan) print(more_test_plan) 然后得到结果： 123456789101112The function is called with a valid number of seconds - `format_time(1)` should return `&quot;1s&quot;` - `format_time(59)` should return `&quot;59s&quot;` - `format_time(60)` should return `&quot;1min&quot;`- The function is called with an invalid number of seconds - `format_time(-1)` should raise a `ValueError` - `format_time(&quot;60&quot;)` should raise a `ValueError`- The function is called with a valid number of seconds and the `hours`, `minutes`, or `seconds` are `0` - `format_time(0)` should return `&quot;&quot;` - `format_time(3600)` should return `&quot;1h&quot;` - `format_time(7200)` should return `&quot;2h&quot;` - `format_time(7201)` should return `&quot;2h1s&quot;` 自己的计划自己生成 当然，有些情况下，生成的测试用例数会比我们的实际情况更少。这时候，我们需要想办法增加测试用例的数量，以便更全面地测试我们的代码。一种方法是增加测试数据的覆盖范围。我们可以通过添加一些边界值、特殊值、无效值等来增加测试用例的数量。 除了增加测试用例的数量，我们还可以增加测试用例的复杂度。这样可以更好地测试代码的鲁棒性和可扩展性。我们可以通过增加测试用例的步骤、条件等来增加测试用例的复杂度。 另外，为了提高测试用例的可读性和可维护性，我们可以将测试用例分为不同的类别，并为每个类别定义一个清晰的目标。例如，我们可以将测试用例按照输入数据的类型、函数的不同参数组合、不同的执行路径等进行分类。 对于这些分类，我们可以在提示语中指明要测试的具体内容，以帮助 AI 编写更加精确、全面的测试用例。同时，我们还可以提供一些代码示例或者代码注释来帮助 AI 理解我们要测试的功能代码。 需要注意的是，我们在生成提示语的时候，要尽可能保留原有的关键信息，以确保 AI 编写的测试用例符合我们的测试要求。 12345678910111213141516def generate_test_cases(function_to_test, unit_test_package=&quot;pytest&quot;): starter_comment = &quot;Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator&quot; prompt_to_generate_the_unit_test = f&quot;&quot;&quot;Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.```pythonimport {unit_test_package} # used for our unit tests{function_to_test}#{starter_comment}&quot;&quot;&quot; full_unit_test_prompt = prompt_to_explain_code + code_explaination + test_plan + prompt_to_generate_the_unit_test return gpt(model=&quot;text-davinci-003&quot;, prompt=full_unit_test_prompt, stop=&quot;```&quot;), prompt_to_generate_the_unit_testunit_test_response, prompt_to_generate_the_unit_test = generate_test_cases(code)print(unit_test_response) 再次，AI为我生成了如下内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#The first element of the tuple is a string that describes the test case,#and the second element is the input for the format_time function.@pytest.mark.parametrize(&quot;test_case, seconds&quot;, [ # Positive integer tests (&quot;Less than 60&quot;, 59), (&quot;Equal to 60&quot;, 60), (&quot;Greater than 60 but less than 3600&quot;, 7200), (&quot;Equal to 3600&quot;, 3600), (&quot;Greater than 3600&quot;, 7201), # Negative integer tests (&quot;Less than -60&quot;, -59), (&quot;Equal to -60&quot;, -60), (&quot;Greater than -60 but less than -3600&quot;, -7200), (&quot;Equal to -3600&quot;, -3600), (&quot;Greater than -3600&quot;, -7201), # Float tests (&quot;Less than 0.0&quot;, -1.0), (&quot;Equal to 0.0&quot;, 0.0), (&quot;Greater than 0.0 but less than 60.0&quot;, 59.9), (&quot;Equal to 60.0&quot;, 60.0), (&quot;Greater than 60.0 but less than 3600.0&quot;, 7200.0), (&quot;Equal to 3600.0&quot;, 3600.0), (&quot;Greater than 3600.0&quot;, 7201.0), # String tests (&quot;Empty string&quot;, &quot;&quot;), (&quot;String that can be parsed to an integer&quot;, &quot;7200&quot;), (&quot;String that can be parsed to a float&quot;, &quot;7200.0&quot;), (&quot;String that cannot be parsed to an integer or a float&quot;, &quot;7200.0.0&quot;), # None test (&quot;None&quot;, None)])def test_format_time(test_case, seconds): # This test checks that the output of the format_time function # is the expected output for the given input. expected_output = &quot;&quot; if seconds is not None: hours = seconds // 3600 minutes = (seconds % 3600) // 60 seconds = seconds % 60 if hours &gt; 0: expected_output += f&quot;{hours}h&quot; if minutes &gt; 0: expected_output += f&quot;{minutes}min&quot; if seconds &gt; 0: expected_output += f&quot;{seconds}s&quot; assert format_time(seconds) == expected_output 在运行这段提示语之后，我们可以得到最终输出的测试代码。除了正常情况下的测试，还包括异常输入的测试。如果输入错误，代码也会输出警告信息。这样，我们可以确保代码在各种情况下都能够正常工作。同时，我们也可以添加更多的测试用例，以覆盖更多的情况，从而提高代码的质量和可靠性。因此，如果您需要进行测试，可以使用这个测试代码作为起点，随时添加新的测试用例。这将有助于确保您的代码在各种情况下都能够正常运行。 最后还缺少了什么？当然是语法检查。 通过 AST 库进行语法检查 我们建议使用Python的AST库再次检查生成的测试代码的语法。这样我们可以确保我们的测试代码是具有正确语法的。需要注意的是，在检查语法时，我们不仅需要生成的测试代码，还需要原始的功能代码。这样才能通过语法检查。另外，为了更好地测试代码，您可以考虑添加一些额外的测试用例，以确保代码的正确性和稳定性。 1234567import astcode_start_index = prompt_to_generate_the_unit_test.find(&quot;```python\\n&quot;) + len(&quot;```python\\n&quot;)code_output = prompt_to_generate_the_unit_test[code_start_index:] + unit_test_responsetry: ast.parse(code_output)except SyntaxError as e: print(f&quot;Syntax error in generated code: {e}&quot;) 非常幸运，直接通过了语法检查。下一步，我们把对应的整个测试代码打印出来执行试试： 1print(code_output) 输出结果最后AI帮我们生成的测试代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import pytest # used for our unit testsdef format_time(seconds): hours = seconds // 3600 minutes = (seconds % 3600) // 60 seconds = seconds % 60 time_string = &quot;&quot; if hours &gt; 0: time_string += f&quot;{hours}h&quot; if minutes &gt; 0: time_string += f&quot;{minutes}min&quot; if seconds &gt; 0: time_string += f&quot;{seconds}s&quot; return time_string#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator.#The first element of the tuple is a string that describes the test case,#and the second element is the input for the format_time function.@pytest.mark.parametrize(&quot;test_case, seconds&quot;, [ # Positive integer tests (&quot;Less than 60&quot;, 59), (&quot;Equal to 60&quot;, 60), (&quot;Greater than 60 but less than 3600&quot;, 7200), (&quot;Equal to 3600&quot;, 3600), (&quot;Greater than 3600&quot;, 7201), # Negative integer tests (&quot;Less than -60&quot;, -59), (&quot;Equal to -60&quot;, -60), (&quot;Greater than -60 but less than -3600&quot;, -7200), (&quot;Equal to -3600&quot;, -3600), (&quot;Greater than -3600&quot;, -7201), # Float tests (&quot;Less than 0.0&quot;, -1.0), (&quot;Equal to 0.0&quot;, 0.0), (&quot;Greater than 0.0 but less than 60.0&quot;, 59.9), (&quot;Equal to 60.0&quot;, 60.0), (&quot;Greater than 60.0 but less than 3600.0&quot;, 7200.0), (&quot;Equal to 3600.0&quot;, 3600.0), (&quot;Greater than 3600.0&quot;, 7201.0), # String tests (&quot;Empty string&quot;, &quot;&quot;), (&quot;String that can be parsed to an integer&quot;, &quot;7200&quot;), (&quot;String that can be parsed to a float&quot;, &quot;7200.0&quot;), (&quot;String that cannot be parsed to an integer or a float&quot;, &quot;7200.0.0&quot;), # None test (&quot;None&quot;, None)])def test_format_time(test_case, seconds): # This test checks that the output of the format_time function # is the expected output for the given input. expected_output = &quot;&quot; if seconds is not None: hours = seconds // 3600 minutes = (seconds % 3600) // 60 seconds = seconds % 60 if hours &gt; 0: expected_output += f&quot;{hours}h&quot; if minutes &gt; 0: expected_output += f&quot;{minutes}min&quot; if seconds &gt; 0: expected_output += f&quot;{seconds}s&quot; assert format_time(seconds) == expected_output 抓个BUG试试 我们可以试着在 Notebook 里面调用一下 format_time(-1)，看看自动化测试跑得对不对。 如图可以看到，输入-1的时候，输出变成了59min59s, 确实AI生成的测试代码帮我们捕捉到了一个Bug。 小结 好了，到这里这一讲也就结束了。我们不仅学会了如何利用一个方法，将一个问题拆分成多个提示语的步骤，循序渐进地让 AI 通过解释代码，构造测试用例，最后再根据代码的解释和设计的测试用例，生成最终的自动化测试，而且还学会了如何在这个过程中，增加更多的内容，以达到更全面的测试。 在生成整套测试代码的过程中，我们不需要人工地复制粘帖任何内容，全都是代码自动完成的，是一个“自动档”的过程。通过将一个问题拆分成多个提示语的步骤，我们的内容变得更加有条理、有逻辑，也更符合我们平时写文字的方式，而不是一股脑地把各种要求都放在提示语的开头，这在解决复杂问题时往往效果不好。 此外，我们还学会了使用多步提示语的好处。多步提示语带来的一个好处，就是能让 AI 考虑各种边界条件。在得到代码的解释之后，我们可以让 AI 考虑 -1、None 这样的特殊输入，从而涵盖更多的测试情况。这样，我们的测试代码最终真的抓住了程序里的 Bug。 回过头来看，如果我们只是直接把代码往 ChatGPT 里一贴，虽然也能生成测试用例，但是那些测试用例就比较欠考虑，不会涵盖各种边角的情况。因此，我们在生成测试用例的过程中，应该尽可能地提供更多的提示语，让 AI 的测试代码更加全面和详尽。 思考题 在本讲中，代码内容有点长，思考题部分需要你思考的内容更多。 你可以试着减少我们的提示语或者提示步骤，看看生成的测试用例有什么样的变化。你可以尝试移除一些提示语，或者更换提示步骤的顺序，从而得到不同的测试结果。 目前我们的代码是以过程式方式一步步演示整个测试代码是如何生成的。如果语法检查出错了，我们实际上应该从头开始重试一遍，再次生成测试代码。你可以尝试将整个代码封装修改，变成一个会自动重试 3 次的函数。这样，我们就可以直接调用这个函数，为 Python 代码生成自动化测试了。 我们本讲中的提示语是借鉴了 OpenAI Cookbook 的样例。你可以尝试总结一下，这些提示语中有哪些常用的方法是值得借鉴的。 欢迎你将你的思考结果分享在评论区，同时也欢迎你将本讲分享给感兴趣的朋友。我们下一讲再见。 推荐阅读 我们之所以要循序渐进地提示 AI，让 AI 先生成例子再生成代码，是因为现在的大型语言模型具有一种名为“思维链（CoT）”的能力。当我们提供更详细的推理步骤时，AI 的表现会更好。在 OpenAI Cookbook 中，有一章专门介绍了思维链的能力，你可以去仔细研读一下。","link":"/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/"},{"title":"VH 情怀黄铜原子笔","text":"「知乎专栏地址」 收到 @罗文森 赠送的笔已经很多天了，快两个星期了吧。 一直在找时间想写一篇评测出来，终究是没抽出时间，况且，我不是写手！眼看时间一天天过去，心里也越来越愧疚。 其实最主要的也不单单是写不出什么，而是总归要用它画两幅画出来，才会感觉的出来到底合不合心意。 对于文具的偏爱，估计是从学生时代就开始了。记得高中的时候缠着老爸给买了第一个奢侈品：一支派克。虽然很珍惜，却并不好用。自此对文具就更是挑剔。 一支好笔，真的能陪一个人好久好久。 打拆包之前，就一股浓浓的逼格。 image image 不过问题来了，我不知道怎么打开包装！XD - 一直不敢使劲，生怕损坏了什么，折腾了好久，才明白，原来连着盒盖的那张纸，是用来撕的。 打开之后，金色的笔体和其皮外套分开躺在盒内，逼格更甚了。 image 喜欢盒子内部的那句话：“合适的形态总有他合适的作用，还原物的本质，至真至纯，用一支笔唤起书写的初心。”， 好吧，又是一个走情怀的产品。不过对于这个包装和笔本身的设计来说，这个情怀我还是蛮受用的。 image 而关于细节上，笔的打磨很是花费了一番功夫，那种黄铜的质感以及似乎岁月沉淀的感觉，让人爱不释手。（这些天只要是要签字的机会，我基本都拿出它来装逼）。 image 而关于旋转笔头，旋转的过程中特别的舒服。到位后的力回馈也刚刚好，让我转来转去玩了好久。额，给玩的不灵光了。（看来关于使用强度，长家还是需要加强）。 关于那件小皮装，似乎有点小了，装进去是用了一些力气的。额，拿出来就更费劲了，太紧了。不过相信用一段时间，也就松了。对于这样一直精致的情怀笔，这件皮衣还是相当必要的。 image 笔当然还是要用才能知道是否合心意, VH 这支黄铜笔，拿在手上的感觉，重量十足，根本是那些塑料笔没办法比的。但是这个重量不大不小，真的正合适。想起厂商吹的牛逼：“打造了最佳的配重”。 这牛逼不为过。 至于握感上，这个笔对于我来说似乎纤细了一点，可能也是因为我握惯了粗的笔，包括我最爱的那支53Pencil，也是比较粗的。而在厂家原装的那支笔芯，使用起来也是非常顺畅（就喜欢这样顺畅的笔芯）。不过用完需要更换笔芯的时候，就需要买好一点了，要不可惜了这么好的笔。 最后随笔画了两幅画来试笔。 image image 太久不画画了，大家将就看吧！ 有想买的，链接在这里 : 首页-vh企业店 PS：VH家的那个无线充电器，也是逼格满满。打算入一个。。。","link":"/VH-brass-pen/"},{"title":"VPS 设置 Hexo","text":"首先需要感谢@lucifr，我现在这篇文就是在iPad上登录VPS完成的。最后还是忍不住入手了下边两个APP: Prompt 当然我其实到现在并不完美，因为rsync和自动执行generate的代码我没有完成。安装incrond的时候总会出错，于是无法执行集群文件同步.所以现在还是在终端里执行generate和cp -rf /home/xxxx/* /home/xxxx 我这里并不是要教设置步骤，因为其实@lucifr 已经在他的这篇文里写的很清楚了，我就写几点注意事项 搞定VPS操作和基本的Linux命令很重要。 要搞定lnmp，参照这里的lnmp详细介绍 新版本的Hexo有更改，在同一目录里是找不到/cli/generate.js的，更别说console.log语句了 @lucifr所说的新建立一个Dropbox账户，意思是在VPS主机上建立一个账户用来执行Dropbox同步，而不是新建立一个Dropbox账户。 其他… 好吧，写其他是因为iPad上用VI进行编辑实在有点难受，现在先这样了，以后有时间了再写一个更详细的。 Prompt","link":"/VPS-setting-hexo/"},{"title":"使用Telnet数据流看世界杯","text":"使用Telnet数据流创造了正在进行比赛的ASCII影像, 尽管这难以令人想象. 看看他是怎么做的… 在比赛开始10分钟以前, 简单的打开Windows开始-运行窗口, 输入 telnet ascii-wm.net 2006 , 你将会看到 “现场直播” 视频流. 明显的, 这是互联网应用的又一创新. 来试一下吧! http://ascii-wm.net/ 由于我家里只有电脑没有电视，所以全程我都是这么干的。。。可惜，经常与主机断开连接。。而且，很多时候我看不懂。。。郁闷。只能知道个大概。。有兴趣的可以去看看＠","link":"/Use-the-telnet-data-stream-to-watch-the-World-Cup/"},{"title":"Alfred 2 Plugin -- open in Atom","text":"atom.alfredworkflow 用于在Alfred 2中用命令快速调用Atom打开所见文档，类似于Open in Sublime， 为了自己方便建立了一个workflow，有需要的下载吧！ Open in Atom.alfredworkflow","link":"/alfred-2-plugin----open-in-atom/"},{"title":"测试allove易博","text":"这次测试是在模拟器上做的，测试下来效果都还不错。就是模拟器本身在速度上有点难以接受。而截图一会会在上传到相册之后再转链过来，和大多数android博客程序一样，功能上都似乎差不多。。。而易博多加了两项功能。。。一个是feed rss订阅，还有一个是涂鸦功能。不过我看来，订阅功能完全可以取消。因为本身功能并不是很强大，并且对程序本身也是个累赘，占用一定的资源。况且现在feed订阅的apk数不胜数，而我本人多习惯打开浏览器通过google reader进行阅读。建议阉割掉订阅而专心做一样比较好。。。。而另外一样涂鸦绝对是亮点，大多数时候写博客人会希望有什么东西能够画一个草略的示意图。。。我多数时候都有这种想法。加上android系列手机的触摸屏，一定能够有完美的手绘体验。。。。 自己先在这里鼠标绘制一个试试看。呵呵。。。。 真累啊，就先这样吧. 涂鸦发表出来才看到,原来不支持横屏...因为我是横向画的,所以.....呃! 建议两点,一点是支持横屏涂鸦上传,还有一点是支持更改画布背景! 还有就是,默认似乎软件在发布文章以后强制性插入自己软件的Google Adsense,并且软件内没有地方可供更改!这点我很不喜欢...流氓性质需要改改!我理解开发者希望有额外收入的心情,但请尊重博客作者和读者,尊重软件使用者和你自己! BTW:当然,我给予的评价和评论都不甚完善...并且也只是我一个人的建议而已!如果读者对这款测试apk有兴趣的话(恩,还没放出)可以联系alloveteam[at]gmail.com索要!但是记得要对此软件有建设性意见.不要尝新后就over了.有好的建议,开发者才能更好的去完善,我们才有更好的软件可以使用!对吧?","link":"/allove/"},{"title":"数据产品经理的自我修养","text":"img","link":"/accomplishment_dpm/"},{"title":"关于我","text":"Hivan Du Bio... 茶桁是我笔名，80后, 老派思想者, 固守着年代感的东西... 和大部分这个年代成长起来的孩子一样, 怀念着龙珠, 灌篮高手, 追着海贼王. Github: @hivandu Twitter: @hivan Design: @hivandu Notes: @DUART 海贼王 对新奇的事物仍然抱持敏感而探索的心境, 并且喜欢有条不紊. 坚信有生之年能看到人类和AI和平共处的景象, 坚信有生之年可以看到第一批星际移民. 《小冰岛》内的AI邻居为我做的第一幅画作 这里, 我们谈谈思维, 哲学, 经济以及信息管理. 还会夹杂着我的一些私货, 那都是一些遥远的技能, 已经离我而去很久了. 说了那么多, 先搬家吧. 另外, 在我另一个试验田里, 有着自己对产品和技术的追求, 也欢迎大家围观; 这篇博客是简历在Hexo + Github Pages + vercel上, 有兴趣的小伙伴可以自行Google一下相关教程, 这里就不引述了.","link":"/about/"},{"title":"Android 2.2 App2sd 问题","text":"其实比起以前版本的app2sd来说,设置是一样的!只是多了一个步骤,就是需要给rom添加一个sdext.然后所有的设置就和以前的版本一模一样了. 首先当然需要有一个已经分好区的sdcard,具体设置可以查看我以前的文,有ubuntu下进行分区的和windows下的! 然后需要最近版本的SPL和Radio,这个本人不提供了,可以自行解决!伸手党可以留下自己的邮箱,我提供下载地址! 准备工作做足后,第一步就是需要在手机上建立一个sdext访问.这里提供一个文本文档:下载： fr-patch134.zip 放在sdcard根目录,然后在连接手机的情况下在终端如下操作: adb shell # sh /sdcard/fr-patch134.txt sdext busybox df -h 如果看到有/sd-ext分区,OK,以下的事情就顺理成章了,参考我以前发布的app2sd步骤操作就好了! 至此所有问题解决! 此处为后续更新,由于之前忽略了点东西,所以这里做一个补充! 由于2.2rom和以前版本的一些差别,在做app2sd之前,需要挂在system,sd-ext和data分区,这是需要注意的一点!挂在命令为:mount 例子: mount system 有什么不明白的再问吧!","link":"/android-2-2-app2sd/"},{"title":"android 2.1版本无法开机解决","text":"不含刷的变砖的机子！ 这个版本的rom是一定需要SPL的，比起HTC hero的rom来说速度有提升！但是没那么华丽(当然我说的是源生系统，我对theme之类的不太有兴趣！)。。。 但是刷SPL有风险，请慎重行事！ 关于刷机2.1版本后无限火花不开机的情况，主要是Recovery的问题，刷成V2.5应该都可以解决！本人测试OK。 手机端安装可以下载程序：flashrec.apk PC端安装需要USB驱动，然后通过更新工具更新。这里一定要注意区别清楚机型版本，不同机型是不能通用的，再次提示，一定要注意看清楚，千万不要着急。 点击此行下载用于 G1 和 沃达丰版 G2 使用的更新程序 大小: 5369404 字节 MD5: 8CA35537D253EB19CC0D28A45D153FC4 SHA1: 88C3E3ED444A1B96C59594215A81B5C5F589C42E CRC32: 97089632 点击此行下载用于 HTC 版 G2 使用的更新程序 大小: 4848659 字节 MD5: 7217E7DCCFEAB4BFB254B86778FFBD5C SHA1: 8373BDB338D3E6EBA8622CDC19EFBF7AE3484451 CRC32: A9BD3E5E 再次提醒，千万别下错了！ 保证手机与电脑连接无误后，双击更新程序运行即可。","link":"/android-2-1/"},{"title":"Android 2.2 for G1","text":"親愛的Android,我回歸了...說實話,後兩個月我真的有些壓抑.現在感覺解脫了! Android 2.2 For G1 已經被C神放出,貌似解決了相機問題,而其他問題暫時沒有進行測試,因為本人也正在下載中...萬分期待! 先提供下載地址,等待我適用後再放試用報告.. http://drop.io/ionstorm/asset/defcon-dream-ota-eng-t1-signed-zip PS:如果沒有語言要求可以下載這個版本就OK了,如果強烈需求中文,需要下載中文補丁包 试用报告: 仍然没有中文 系统默认没有中文输入法(这个到不是问题) 没有Google Map,市场无法下到.并且安装了4.2开发版后打开就崩溃(这点对于喜欢Buzz的朋友是个致命伤) 3D图库效果有. 相机不能用,和2.1不同的是,就算拍照有图片,打开来看也是一个android小人.... WIFI正常使用 蓝牙没有测试. 速度真的比2.1快了不少,没有一点卡的感觉. Settings里有CyanogenMod settings,可以直接更新Rom,可是我这里链接失败. Apn需要自行设置,具体的设置方法可在网上Google Vpn使用正常 因为地图没有打开,所以GPS模块没有测试. 其他不进行补充了,由于以上有些原因是我无法接受的.所以打算刷回1.6rom,以前刷2.1是为了绑定自己和公司双帐号,现在不必了!不用忍受2.1的速度了...","link":"/android-2-2-for-g1/"},{"title":"VSCO FILM 00 FREE STARTER PACK","text":"本文知乎专栏 VSCOCam是iOS上一款滤镜相机，其最著名的地方就是他们的胶片滤镜。 而其实VSCO在这款APP之前，就一直在做胶片滤镜，有OS X和WIN两个平台的版本。最主要的是作为LightRoom的插件存在。 而现在提供免费下载的这款滤镜就是针对LightRoom的滤镜插件，有OS X和WIN两个版本提供。 We are thrilled to announce VSCO Film 00, our first-ever FREE starter pack. Perfect for anyone who uses VSCO Cam and is looking to take the next step, VSCO Film 00 brings beautiful presets, custom camera profiles, and the familiar VSCO editing experience to your desktop in Adobe Lightroom. VSCO Film 00 includes two of our most popular film stocks (Kodak Gold 100 from Film 05 and Kodak Tri-X from Film 06) and is available for download now. 有需要的可以点击原文链接进行下载，不要感谢我，我是雷锋。 PS: 关于之前我提到的Enlight导出图片丢失数据的部分，我在此向读者和开发者致歉，原因是导出的如果是PNG图片是不能保存那些信息的，可以将图片的质量从Pro调小到Hight, 保存的就是JPEG图片，那么导出的图片地理位置和相机信息就会一并保存了。","link":"/VSCO-FILM-00-FREE-STARTER-PACK/"},{"title":"Android 3.0","text":"好吧,7.1号Eldar Murtazin在其博客上已经放出一个可信度比较高的谣言,就是Android3.0将于十月份发布,代号姜饼(Gingerbread) 据传,3.0rom将会针对高端市场,分辨率达到了1280X760,支持此系统的最低配置将为1Ghz处理器,512M内存.可以这么说,在3.0系统发布以后,就可以正式宣布G1被彻底淘汰了. Android 3.0 Gingerbread will be released in mid- October (around 15 -16th), 2010. First handsets shipping in November/December – for the Holiday Season. Minimum hardware requirements for Android 3.0 devices are: 1GHZ CPU, 512MB or RAM, displays from 3.5” and higher. (We all, of course, heard that Android handsets with 2GHz CPU’s are coming) New 1280×760 resolution available for the devices with displays of 4” and higher. (Anyone thinking about Android tablets now? ) Completely revamped user interface. If you want to get a feeling of what Android 3.0 Gingerbread UX is like, check out the Gallery App on Nexus One. The same overall feel, light animated transitions,etc. Natively, through all the UI. Android’s split into 2 branches becomes official. 3.0 for top of the line/high end devices. Cheap, low-end mass market handsets will keep Android 2.1/2.2 详情可以参看:http://www.unwiredview.com/2010/06/30/android-3-0-gingerbread-details-1280x760-resolution-1ghz-minimum-specs-mid-oct-release/","link":"/android-3-0/"},{"title":"Android 4.0 通讯录与Google+的深度整合","text":"在4.0以前,我记得Android里的通讯录名称是“contact”.而在4.0之后,我的手机上是4.0.3,将其更改为“people”了. 而更改的不仅仅是这小小的名称!大家都记得在之前版本的contact里,拨打界面和通讯录界面是整合在一起的.所以那个时候我第一屏单单放一个拨打电话就OK了.需要查找通讯录的话,可以切换tab. image 1.理念: 而在4.0里面,你在people里面找不到拨打电话界面,而在拨打电话的界面里也找不到通讯录.Google将这两个单独分立了开来!貌似不方便了,其实不是,反而变得异常容易整理和操作!Google在Android4.0里的理念就是将电话和联系人完全分离开,这本来就不是一回事!智能手机时代,谁说的联系人就一定只能是打电话和发短信的? 2.整合: 说了些小变化,下面切入正题,就是Android 4.0 之后通讯录与Google+的深度整合! 这也是我昨晚没事整理联系人的时候才注意到的,不知道是不是4.0之处就是这样的,还是到了4.0.3的改变! 当我们进入一个单独的联系人界面的时候,发现除了常用的PHONE,EMAIL等选项之后,会有一个CONNECTIONS项目选项,这里会显示手机上安装过的SNS程序里联系人的关联账户! 比如WhatsApp,Twitter或Facebook等! 当然,你也有可能看到一个Add connection选项. 这个选项,其实就是和Google+整合的选单! 可以选择添加联系人到自己的Google+ 圈子里!当然,有可能你的联系人根本没有在Google+注册过! image Screenshot_2012-01-28-16-45-59 image 我猜测,Google以后有可能会将Google+的圈子代替联系人来使用!也就是说,你所有的联系人都是要圈养的.不管他是否注册过Google+,利用圈子的概念管理联系人,其实比较起来而言,比Gmail里的通讯录要高效一点! 接下来,才是深度整合的重点,以我自己为例: 用过4.0的朋友都知道,联系人选单是可以向左拖动的,就是右边还有一块和联系人有关的“update”选项!当然,这一块内容需要对方联系人有Gtalk或者Google+才会显现出来! image 平时就会显示Gtalk上的签名状态,而对于有Google+的联系人,则会显示他在Google+中的信息! 我们大家应该都用过MSN或者QQ,应该可以想象一下,MSN当初和Space深度整合,QQ和QQ空间深度整合的情形! 当你查看某个联系人的时候,他在自己空间里发表的文章或相片,都会在其信息里显示出来!当然,QQ有的时候会闹点小情绪,来一两次大姨妈…我们可能没那么及时看到!所以说这个概念并不新奇,只是这次Google借用到了手机上而已! 哈哈,想象一下,够方便吧? Screenshot_2012-01-28-16-52-22 3.想法: 写到这里,我到觉得Google有些小气了. 右边状态栏完全可以让出来给其他的SNS APP来使用而不只是Google+,譬如Twitter和Facebook,或者是QQ空间!既然有了这样一个功能,而又是在消费者手上的,那么决定权就该交给消费者!否则,自己联系人里没多少用Google+的,右边信息栏岂不是浪费? 我想,再以后,这块地方应该会被其他SNS APP占用吧!届时,通过手机通讯录,大家就可以看到某人最近在做什么了!而无需再登录单独的App去查看! 4.后记: 当然,除了Contact以外,Gallery等都和Google的产品有深度整合!而这些,和其他APP整合的可能性是有的!比如Gallery里直接查看Flickr里的相片而不只是Picasa.不过在Google推销自己的Google+这段时间里,我看是不太可能了!","link":"/android-4-contacts-the-depth-integration-with-google-plus/"},{"title":"Android 3.0 Preview","text":"整个界面都显得很陌生啊!大家可以从视频中看到. 锁屏界面更改很大,目前不知道是解锁手势还是横向滑动还是画圈. HomeScreen主屏幕，可以看到四角的设计，左上给搜索按钮，支持语音搜索，右上为应用菜单按钮，左下为返回、Home、菜单按钮，右下角则是状态标志，屏幕中间为程序快捷菜单和桌面工具 有一个社交工具的集合，可以查看不同来源的好友更新.不过这个在大陆的情况就...你们懂得! 有个桌面管理器,可以进行桌面的各项设置. 拥有全新的浏览器，可以查看各种完整版网页 拥有全新的Android版本Gmail界面! Gtalk界面也是全新的,并且支持双向视频聊天!不过流量上....不敢想! 全新的Youtube视频墙,不过在国内,我们都是墙内的,它在墙外而已! &lt;li&gt;全新的Google地图界面,支持3D模式导航!&lt;/li&gt; 此外还会有Google图书等更多全新内容!","link":"/android-3-0-preview/"},{"title":"Android 简易访问Twitter,youtube,facebook方法","text":"其实就是修改hosts文件. 而这个hosts文件我是已经修改完毕的...直接cat到手机内覆盖原文件就可以了! 执行之前请将hosts复制到sdcard的根目录,然后cmd,cd如adb目录,然后执行其下代码: adb remount cat /sdcard/hosts &gt; /etc/hosts 一切搞定! 请对于hosts上的IP地址低调传播,谢谢! PS:本hosts修改大法已经基本完全失效,基本所有有效果的IP地址都被屏蔽,如果有条件,自己建立一个VPN吧!不过对于联通的用户我要给你一个大大的警告:联通屏蔽VPN.....","link":"/android-twitteryoutubefacebook/"},{"title":"Android G1 的优化","text":"我是第一批使用android的用户,那个时候没有别的选择,只有G1好选择.所幸买的是英版全白.这个机型据说是很少的.包装里带彩贴的那种! 可是时间长了,G1的诟病也就出现了,系统不断升级,虽然有很多自制包提供下载,解决了官方不在支持G1的问题,可是速度上和原来的设计问题没有办法解决! 不过好在网民的智慧是无穷的,今天带给大家三个特殊的小东西,用来优化你得G1. 10m rom HACK 刷新方法，拷贝boot-cm_2629-dp_mem-xtra.img文件到sd卡，启动到recovery模式下，进入console： mount -a flash_image boot /sdcard/boot-cm_2629-dp_mem-xtra.img JIT enabled Dalvik VM 进入recovery直接升级zip文件就好了 Audio Hack v3.2 apk 这个是mark里2欧元的付费软件.用来加大默认铃音. 下载地址: [download id=\"1\"]","link":"/android-g1-optimization/"},{"title":"","text":"在安卓中叫兽曾经写过两个换肤教程.而教程中写的是替换原文件包...其实这个完全没必要.只要在模拟器的快捷方式中加入一段代码就好了... -avd avdname -skin skinname 而皮肤文件可以分开来放.如图:","link":"/android-skin/"},{"title":"Make Android Your Own-Androidify","text":"Androidify,由Google推出的一个app，可以让Android使用者在手机上创造属于自己的形象Android。是一套单纯的纸娃娃系统,可以自定义包括肤色、发色、衣服、裤子、鞋子、饰品等众多套件，还可变更身体的比例，其实还颇好玩的哩。只要直接在Android Market搜寻Androidify就能找到这个可爱的小软件。 跳转有Google针对这个app推出的小短片。不过还不如自己赶快下载回来玩比较实在。","link":"/android-ownandroidify/"},{"title":"Auto-save-photo-to-qqmail","text":"前言 为什么是QQ Mail? 因为它大,而且不断自动扩容,你想把它装满暂时是不太现实. 而且来说,QQ邮箱的体验还是非常不错的!过滤规则也很能满足要求,归档搜索查找都不错!一些不涉及隐私而又想保存的文件或者照片或者其他什么东东,存在QQ邮箱里还是不错的!比如:XXX 如果想同步到Google+请看完文章后看最后部分的更新说明! 如何实现 通过众所周知的ifttt 其实,这主要是一个我为了保存自己照片的方式!(爱信不信,不相信拉倒!) 建立一个task, if Instagram 设定条件:New Liked photo, then Gmail 设定条件:Send an email. 好了,填上 To address: xxx@qq.com 就OK了! 简单吧? 如果你熟悉某个联系人,那么建立规则by Username,然后包含此用户名关键词的主题都标上相应的关键词.比如by ladiiprang在邮箱规则里就可以加上\"妹子\"的tags. 妹子 注意点 如果你不想后期被众多的新邮件搞得头昏脑胀的话,那么你一开始就要设定好过滤条件! 在ifttt中,Send an email的时候Subject记得填写上一些关键词,比如From Instagram,这样,对于主题内有关键词的邮件就好管理的多了.添加过滤规则就好了!将来自己发送邮箱Gmail的邮件主题包含From和Instagram的都自动移动到一个新建的文件夹内,Ex:Photo DB,完工! 后记 同理,我们也可以建立来自Flickr的发送规则,原理是一样的!注意Gmail邮箱里的过滤条件要建好!否则Gmail爆满是迟早的事情!运用这样的规则,我们还可以发送Dropbox里的文档到QQ邮箱内保存,不过规则限定发送的只能是Public内的文档! 其实一开始我不确定是发送文件还是只有地址!所以我开始的方法很绕,就是将相片想Save到Dropbox,然后再通过Dropbox建立if.then.Gmail.不过试验下来既然能直接发送文件,建立task就简单多了! 还等什么,快去你的Instagram和Flickr上收藏妹子到邮箱内吧! 更新 本来因为标题的原因,这点是不加在这里的!但是想着再写一篇一样意义的文章很没意思,所以就在这里说明一下好了! 在我这篇文章发布之后,G+上看到了电脑玩物的作者+esor huang 的一篇讲解Instagram同步到Dropbox和Google+的说明!以及这篇E文!说起来,这样的同步方式确实很笨拙.你打算电脑24小时开着picasa来为你同步么?那么,我从我这篇文的基础上考虑可行方案!记得之前Picasaweb给每个人都有一个邮箱推送地址!就是类似username.password@picasaweb.com这样的地址!好吧,有邮箱地址就简单了.不过这个地址需要你再登录picasaweb.com去找,在Google+页面上是找不到的!同理,Flickr也有类似的推送地址! 该怎么做我想你已经清楚了吧! 最后,我想到了是否同样可以传送到QQ相册!毕竟和邮箱最大的不同就是相册是用来分享的,而邮箱是用来保存的!可惜,QQ没有针对相册的推送,而推送到QQ空间的XXX@qzone.qq.com这个地址也是必须QQ邮箱内部发才行!是的,和你们一样,我又想到了邮箱转发规则.用QQ邮箱收到邮件后转发到QQ空间邮箱去不就好了!测试后,果然. ... 果然没那么简单,这次我失误了,特么的QQ小气到不允许自己的QQ邮箱转发邮件到QQzone的邮箱来自动推送文章!提示这是一个无效地址!不过也无所谓了,毕竟是推送文章的邮件地址,你也不想自己的QQ空间全是大片的文章,而且每篇文章里只有一张相片吧 ? 这个说明本来是在G+上有提出的,但是有基友测试成功后给的是这里的url,所以我就想,补上这个说明!谁说是一样的到底,但是如果不提Picasaweb有邮件推送地址,估计很多人都已经忘记了!为了找同步到G+上的朋友会看得糊里糊涂!","link":"/auto-save-photo-to-qqmail/"},{"title":"Auto operation Weibo","text":"The code address of this article is: auto operation weibo Chromedrive download: Taobao Mirror , need to be consistent with your Chrome version auto operation weibo 123456789101112131415161718192021222324252627from selenium import webdriverimport timedriver = webdriver.Chrome('/Applications/chromedriver')# login weibodef weibo_login(username, password): # open weibo index driver.get('https://passport.weibo.cn/signin/login') driver.implicitly_wait(5) time.sleep(1) # fill the info: username, password driver.find_element_by_id('loginName').send_keys(username) driver.find_element_by_id('loginPassword').send_keys(password) time.sleep(1) # click login driver.find_element_by_id('loginAction').click() time.sleep(1)# set username, passwordusername = 'ivandoo75@gmail.com'password = 'ooxx'# Mobile phone verification is required here, but still can’t log in fully automaticallyweibo_login(username, password) follow user 12345678910111213141516171819202122def add_follow(uid): driver.get('https://m.weibo.com/u/' + str(uid)) time.sleep(1) # driver.find_element_by_id('follow').click() follow_button = driver.find_element_by_xpath('//div[@class=&quot;btn_bed W_fl&quot;]') follow_button.click() time.sleep(1) # select group group_button = driver.find_element_by_xpath('//div[@class=&quot;list_content W_f14&quot;]/ul[@class=&quot;list_ul&quot;]/li[@class=&quot;item&quot;][2]') group_button.click() time.sleep(1) # cancel the select cancel_button = driver.find_element_by_xpath('//div[@class=&quot;W_layer_btn S_bg1&quot;]/a[@class=&quot;W_btn_b btn_34px&quot;]') cancel_button.click() time.sleep(1)# 每天学点心理学UIDuid = '1890826225'add_follow(uid) create text and publish 1234567891011121314151617181920212223242526272829303132333435def add_comment(weibo_url, content): driver.get(weibo_url) driver.implicitly_wait(5) content_textarea = driver.find_element_by_css_selector('textarea.W.input').clear() content_textarea = driver.find_element_by_css_selector('textarea.W.input').send_keys(content) time.sleep(2) comment_button = driver.find_element_by_css_selector('.W_btn_a').click()# post the textdef post_weibo(content): # go to the user index driver.get('https://weibo.com') driver.implicitly_wait(5) # click publish button # post_button = driver.find_element_by_css_selector('[node-type=&quot;publish&quot;]').click() # input content word to textarea content_textarea = driver.find_element_by_css_selector('textarea.W_input[node-type=&quot;textEl&quot;]').send_keys(content) time.sleep(2) # click publish button post_button = driver.find_element_by_css_selector(&quot;[node-type='submit']&quot;).click() time.sleep(1)# comment the weiboweibo_url = 'https://weibo.com/1890826225/HjjqSahwl'content= 'here is Hivan du, Best wish to u.'# auto send weibocontent = 'Learning is a belief!'post_weibo(content)","link":"/auto_operation_weibo/"},{"title":"针对某一个项目自动切换node版本","text":"nvm作为node的版本管理器，并不具备自动切换版本切换的功能，有的时候我们需要针对某一个项目切换当前的node版本，这个时候就需要用到其他工具了。比如avn 举例项目:project 因为最近Node更新到10之后，我将系统默认版本切换到了10，有不更新不舒服斯基强迫症 而project 编译的版本为8，否则会出现编译出错。 123$ brew install nvm$ nvm i -g avn$ avn steup 之后在project根目录中添加一个文件.node-version 123$ touch .node-version$ echo v8 &gt;&gt; .node-version #node需要切换的版本$ echo `source &quot;$HOME/.avn/bin/avn.sh&quot; # load avn` &gt;&gt; ~/.zshrc 这样就可以了。 不过不排除报错的情况，如果是brew 安装的nvm, 则默认nvm.sh并不在~/.nvm目录内，这个时候可能需要在执行一下某段脚本。一样添加到~/.zshrc内 1$ echo `[[ -s &quot;$(brew --prefix nvm)/nvm.sh&quot; ]] &amp;&amp; source $(brew --prefix nvm)/nvm.sh` &gt;&gt; ~/.zshrc 再切换一下项目目录 12$ cd $project$ avn activated v8.11.2 (avn-nvm v8.11.2) 至此完成了！","link":"/avn-change-node-version-for-a-project/"},{"title":"Boston house price CART regression tree","text":"On the code 12345678910111213141516171819202122232425262728293031323334353637383940# CART regression tree predictionfrom sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_bostonfrom sklearn.metrics import r2_score,mean_absolute_error, mean_squared_errorfrom sklearn.tree import DecisionTreeRegressor,export_graphvizimport graphviz# Prepare data setboston = load_boston()# Explore dataprint(boston.feature_names)# Get feature set and pricefeatures = boston.dataprices = boston.target# Randomly extract 33% of the data as the test set, and the rest as the training settrain_features, test_features, train_price, test_price = train_test_split(features,prices,test_size=0.33)# Create CART regression treedtr = DecisionTreeRegressor()# Fitting and constructing CART regression treedtr.fit(train_features, train_price)# Predict housing prices in the test setpredict_price = dtr.predict(test_features)grap_data = export_graphviz(dtr, out_file=None)graph = graphviz.Source(grap_data)# Result evaluation of test setprint(f'Regression tree mean squared deviation:',mean_squared_error(test_price, predict_price))print(f'Regression tree absolute value deviation mean:',mean_absolute_error(test_price, predict_price))# Generate regression tree visualizationgraph.render('Boston') !&gt; Before running this code, please ensure that the relevant dependencies have been installed;","link":"/boston_data_CART/"},{"title":"CM6 test0 32b","text":"好吧,熟悉的人看到标题应该能猜到这就是Android 2.2 的CM版本.没想到这么快会出现,和之前发布的版本不同,这次编译与CM大神的版本!没想到在和儿子过生的当间就发布了... 基于7月5日最新CM6源码编译，新增了电池百分比显示的开关，在Cyanogenmod 设置里进行更改 默认关闭jit，在32a/32b等低端机上开启jit对性能没有改善，反倒更占内存，故在这个版本中关闭jit 修改ADW的默认设置，使其常驻内存，改善从其它程序退回桌面的速度 修改ADW的壁纸图库，用AOSP的图库替换了CM的图库 修改了framework.jar，使用了geesun的代码试其支持中文运营商显示 使用了最新的FRF91的GAPPS CM6自带的contact文件有不少bug，故换成了aosp的contact，虽然相对cm6的功能更少，但非常稳定 重新编译了kernel，个人感觉比默认的kernel更稳定 进一步汉化了framework和superuser等程序 新增32a的支持，32a的用户也可以使用 Known Issues： 第一次启动时可能会意外重启，完成设置后就不会出现这个问题 相机中按0x变焦按键会使相机fc 摄像无法正常使用 由于Gapps都是Nexus专用的，故其素材的尺寸都很大，特别是gmail，显示出来很大，这个暂时无法解决 Dream和Magic因为性能问题，不支持flash，即便刷了2.2也不可能运行flash，所以不要去市场下载flash程序了，不会起作用的，另外也不要再求使用flash的方法了，在地球上不存在解决方法，除非你换手机 App2sd： rom支持app2sd，但默认没有开启。 开启方法： 在超级终端中输入： su pm setInstallLocation 2 即可开启app2sd，不需要有ext分区就可以使用，但官方的这个app2sd还不太稳定，不建议使用。 Download： Dream/Magic 32b:http://thesoloblack.com/rom/cm6-test0-32b-0705-fixed.zip Magic 32a:http://thesoloblack.com/rom/cm6-test0-32a-0705-fixed.zip 本更新信息和下载链接均来源于机锋网!","link":"/cm6-test0-32b/"},{"title":"bye-google-reader","text":"今早一打开网页,满篇都是关于Google将于7月1日正式关闭Reader的消息. 再见吧,Reader. 这八年,基本每天,我都会去看看你... 可是又能如何,到了该走的时候了.即便这个理由让我完全无法接受. Bye, 我亲爱的GReader...","link":"/bye-google-reader/"},{"title":"自定义文件上传框","text":"其实这根本就不值得写出来，只是可能前几步大家都做了，只是最后一步就忽略了。 我们在自定义input:file的时候，一般来说都是外边包一层，里边在写一个&lt;input type=\"file\"&gt;, 然后将其透明值设置成0,然后再定义外层的样式来达到自定义的目的。 HTML： &lt;div class=&quot;upfileOutWrap&quot;&gt; &lt;div class=&quot;upfileWrap&quot;&gt;&lt;input type=&quot;file&quot;&gt;&lt;/div&gt; &lt;div class=&quot;upfileBG&quot;&gt;upload image&lt;/div&gt; &lt;/div&gt; CSS： .upfileOutWrap { cursor: pointer; width: 199px; height: 42px; line-height: 42px; position: relative; } .upfileWrap{ width: 100%; height: 100%; position: absolute; top:-1; left: -1; z-index:2; } .upfileWrap input{ opacity: 0; filter: alpha(opacity=0); cursor: pointer; width: 100%; height: 100%; font-size: 32px; } .upfileBG{ width:100%; height:100%; background: url(./images/upload.png) no-repeat; font-size: 14px; color: white; position: absolute; top:-1; left: -1; padding-left:10px; z-index:1; } 可是这个时候还是有点问题，就是万恶的IE下边。 IE下边的input标签默认都是有光标的，:file也不例外，而且IE下边必须要点击”Browse”或者双击input输入框才会有效果。那么这个时候在IE下就会出现如图的莫名其妙的问题，注意左边的光标，并且还需要双击才会弹出文件选择窗口。 这个时候如果你把input透明度设置成100显示出来，就会发现原来是这样的。 所以这个时候，如果是其他标准浏览器，那么设置好input的高宽就搞定了，而IE下边，还必须考虑如何让”Browse”按钮能铺满我们所自定的div样式。这样我们才能实现IE下不出现光标，而且单击弹出文件选择窗口。 这个时候，看似毫无办法，其实我们可以选择增加字体的大小。当字体变成32px的时候，就是这个样子的。 好了，这样我们就搞定了，将input:file 继续设置为完全透明。那个可恶的光标不见了，我们也可以实现IE下单击。当然，字体到底用多大的，要视你自己定义的视觉效果来看，自己调试吧。 Final CSS: .upfileOutWrap { cursor: pointer; width: 199px; height: 42px; line-height: 42px; position: relative; } .upfileWrap{ width: 100%; height: 100%; position: absolute; top:-1; left: -1; z-index:2; } .upfileWrap input{ opacity: 0; filter: alpha(opacity=0); cursor: pointer; width: 100%; height: 100%; } .upfileBG{ width:100%; height:100%; background: url(./images/upload.png) no-repeat; font-size: 14px; color: white; position: absolute; top:-1; left: -1; padding-left:10px; z-index:1; }","link":"/custom-inputfile/"},{"title":"CnBloggerCon 2012","text":"一年一度的中文网志年会与今晨八点开始，不过没想到的是昨晚就已经完成了开幕式。在Google+上由Isaac Mao主持开幕Hangout视频。可惜我没有看到！ 以上的年会开放营的设计示意图本来我是想稍加修改一下再传到自己的Flickr上的！结果一直到今天开幕都没有太多时间来做这件事情，其实这篇博客都应该是昨天晚上就生产出来的才对。 今年的年会与以前的年会有所不同，没有设置会场，而启用了“云智慧”的概念。而Google+中的会议视频以及才推出没多久的Events为年会此种形式提供了可行性！ 第一天的会议主题为“公民媒体”，可以在如下两个地址内参与讨论： +《云平行会： 麦康瑞在全球之声2012峰会》 +《云访谈：老虎庙和佐拉谈公民记者》 Google+上的Events讨论地址：中文网志年会2012 Events 还请各位中文Blogger们积极参加，因为要翻墙，So,记得戴套，或自备安全工具！你们懂得。","link":"/cnbloggercon-20/"},{"title":"关于设备转向后的自适应","text":"关于移动端的适配，都知道其实rem是比较好的一个适配方案，但是rem是根据根目录的字体大小来调解的，那么，我们在做网页的时候，屏幕旋转后，能否让根目录的字体跟着变化呢？ 先上代码： 1234$(function(){ var size = $(window).width() / 25; $('html').css('font-size': size);}); 这样在css中用rem单位是没什么问题，但是如果屏幕旋转之后，你就会发现，真的不能看了就。原因就是屏幕旋转以后，根上的字体并没有随之变化。 所以我们来加上 12345678910111213// 监视设备方向window.addEventListener(&quot;orientationchange&quot;, function() { media();}, false);function media(argument) { // 因为获取尺寸出错，需要延迟获取 setTimeout(function(){ var size = $(window).width() / 25; console.log('the device size: '+size); $('html').css('font-size', size); }, 200); }","link":"/css-rem-and-javascript/"},{"title":"Debian安装SSH","text":"要求： 安装Debian 8.* 64位操作系统 分区要求: 不要分区 Partitioning mehod: use entire disk Partitioning scheme: All files in one partition 选择源镜像 (mirror country) 请选择china 然后选择ftp.cn.debian.org 程序和服务需求 debian 默认最小安装，安装的时候不用安装桌面环境和标准系统实用程序(以下两个不需要勾选): - Debian destop environment - Standard system utilities 如果有SSH server选项，请务必勾选，会省很多麻烦 安装SSH debian最小安装默认是没有配置apt-get源的，这个时候无法实用apt-get install命令，所以在安装SSH之前，我们需要先配置apt-get: 配置apt-get 终端内操作 1234567# 首先我们需要备份原有配置文件cp /etc/apt/sources.list /etc/apt/sources.listbak# 然后对资源列表文件进行编辑vi /etc/apt/sources.list# 当然也可以实用nano命令nano /etc/apt/source.list PS: 如果对VI操作不熟悉的，可以看这里 vi编辑器常见命令实用 如果安装的时候按照之前我给的步骤来，那么这会的sources.list应该是这样的 对文件进行更改，将以下命令加入文件并保存: 12deb http://ftp.cn.debian.org/debian/ jessie main contrib non-free deb-src http://ftp.cn.debian.org/debian/ jessie main contrib non-free 更改后的文件如图: main, contrib, non-free 分属不同的源，添加后可以从不同的源仓库更新文件索引 至此apt-get源就配置完毕，接下来我们就可以安装SSH了 安装SSH 在终端内输入以下命令: 12345678# 更新apt-get源apt-get update# 更新系统apt-get upgrade# 安装SSHapt-get install ssh 这样就好了，SSH安装完毕 install ssh 注意 如果一直安装不能成功，请往下看: 首先，请ping ftp.cn.debin.org 和 ping mirrors.163.com 来测试一下能否ping的通域名，如果ping不通，请往下看: 有的时候机房安装debian后会出现域名解析问题,这又是另外一个问题。比如ping 123.111.123.111 是OK的，但是如果ping对应的域名如: ping mirrors.163.com就会出现unknow host的问题。 似乎linux很大一部分都会出现这种问题，能ping的通IP但是ping不通域名。那么请查看以下原因解决: 1. 查看DNS解析是否有问题，确定设置了域名服务器: cat /etc/resolv.conf 123nameserver 114.114.114.114 nameserver 8.8.8.8 nameserver 8.8.4.4 2. 确保网关已设置 grep GATEWAY /etc/sysconfig/network-scripts/ifcfg* 1/etc/sysconfig/network-scripts/ifcfg-eth0:GATEWAY=192.168.40.1 如果未设置，则通过以下方法增加网关 route add default gw 192.168.40.1 或者手工编写/etc/sysconfig/network-scripts/ifcfg* 然后重启network服务: service network restart 3. 确保可用dns解析 grep hosts /etc/nsswitch.conf 文件打开后为: 1hosts: files dns 4. 查看是否防火墙的问题 因为域名解析用到了53端口,需要把下面设置配置到防火墙里: 1234iptables -A INPUT -p udp --sport 53 -j ACCEPT iptables -A OUTPUT -p udp --dport 53 -j ACCEPT iptables -A INPUT -p udp --dport 53 -j ACCEPT iptables -A OUTPUT -p udp --sport 53 -j ACCEPT 如果找不到原因或者不知道怎么设置，那么就用以下最笨的方法: 如果出现这样的问题，更新sources.list后会无法更新也无法安装ssh. 如果出现这样的问题，更新sources.list地址为一下地址: 12345678deb http://123.58.173.186/debian/ jessie main non-free contribdeb http://123.58.173.186/debian/ jessie-updates main non-free contribdeb http://123.58.173.186/debian/ jessie-backports main non-free contribdeb-src http://123.58.173.186/debian/ jessie main non-free contribdeb-src http://123.58.173.186/debian/ jessie-updates main non-free contribdeb-src http://123.58.173.186/debian/ jessie-backports main non-free contribdeb http://123.58.173.186/debian-security/ jessie/updates main non-free contribdeb-src http://123.58.173.186/debian-security/ jessie/updates main non-free contrib 利用IP地址代替域名，但是测试下来只有163的镜像可以这样做。来源为网易镜像的帮助文档: Debian镜像使用帮助","link":"/debian-install-ssh/"},{"title":"Dropbox","text":"这真是一个好服务呢…相对skydrive来说虽然少了3G的空间,但是对于保密和可见的设定很丰富,更重要的是,dropbox的客户端支持mac和windows用户直接拖拽传…像建立文件夹以及上传下载文件,简直就和在自己的电脑里操作一样简单… 唯一不足的由于网络原因,我的所有操作都会有延迟…得等他慢慢去同步文件的操作.看演示视频,外国的网络真是强大啊! 在此感谢lenvow的邀请信. 对应的,我也有了十个邀请的机会,有需要的可以在地下留言申请…. 2008.7.2更新:我发现我是比较懒的…有人对Dropbox的测试给出了更详细的说明…看这里: 而自然,我的十个邀请还一个都没有送出去呢!:)可见我这里确实没有人什么人来往！ 如果需要的,还是趁早吧！","link":"/dropbox/"},{"title":"Digits recognition","text":"The code address of this article is: digit recognition Convolution operation demo 123456789101112131415import pylabimport numpy as npfrom scipy import signal# set imgimg = np.array([[10, 10, 10, 10, 10],[10, 5, 5, 5, 10], [10, 5, 5, 5, 10], [10, 5, 5, 5, 10], [10, 10, 10, 10, 10]])# set convolutionfil = np.array([[-1, -1, 0], [-1, 0, 1], [0, 1, 1]])# convolution the imgres = signal.convolve2d(img, fil, mode='valid')# output the resultprint(res) output 123[[ 15 10 0] [ 10 0 -10] [ 0 -10 -15]] A image demo 1234567891011121314151617181920212223import matplotlib.pyplot as pltimport pylabimport cv2import numpy as npfrom scipy import signal# read the imageimg = cv2.imread('./data/weixin.jpg', 0) # Any picture# show the imageplt.imshow(img, cmap='gray')pylab.show()# set the convolutionfil = np.array([[-1,-1,0], [-1, 0, 1], [0, 1, 1]])# convolution operationres = signal.convolve2d(img, fil, mode='valid')print(res)# show convolution imageplt.imshow(res, cmap = 'gray')pylab.show() use LeNet model to recognize Mnist handwritten digits 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import kerasfrom keras.datasets import mnistfrom keras.layers import Conv2D, MaxPooling2Dfrom keras.layers import Dense, Flattenfrom keras.models import Sequentialimport warningswarnings.filterwarnings('ignore')# load data(train_x, train_y), (test_x, test_y) = mnist.load_data()train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)train_x = train_x / 255test_x = test_x / 255train_y = keras.utils.to_categorical(train_y, 10)test_y = keras.utils.to_categorical(test_y, 10)# create sequential modelsmodel = Sequential()# The first convolutional layer: 6 convolution kernels, the size is 5*5, relu activation functionmodel.add(Conv2D(6, kernel_size = (5,5), activation='relu', input_shape=(28, 28, 1)))# the second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size = (2, 2)))# the third convolutional layer: 16 convolution kernels, the size is 5*5, relu activation functionmodel.add(Conv2D(16, kernel_size = (5, 5), activation = 'relu'))# the second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size = (2, 2)))# Flatten the parameters, which is called a convolutional layer in leNet5. in fact, this layer is a one-dimensional vector, the same as the fully connected layermodel.add(Flatten())model.add(Dense(120, activation = 'relu'))# Fully connected layer, the number of output nodes is 84model.add(Dense(84, activation = 'relu'))# The output layer uses the softmax activation function to calculate the classification probabilitymodel.add(Dense(10, activation='softmax'))# set the loss function and optimizer configurationmodel.compile(loss = keras.metrics.categorical_crossentropy, optimizer = keras.optimizers.Adam(), metrics = ['accuracy'])# Incoming training data for trainingmodel.fit(train_x, train_y, batch_size = 128, epochs = 2, verbose = 1, validation_data = (test_x, test_y))# Evaluate the resultsscore = model.evaluate(test_x, test_y)print('Error: %.4lf' % score[0])print('Accuracy: ', score[1]) 12345678Train on 60000 samples, validate on 10000 samplesEpoch 1/260000/60000 [==============================] - 37s 616us/step - loss: 0.3091 - accuracy: 0.9102 - val_loss: 0.1010 - val_accuracy: 0.9696Epoch 2/260000/60000 [==============================] - 36s 595us/step - loss: 0.0876 - accuracy: 0.9731 - val_loss: 0.0572 - val_accuracy: 0.981410000/10000 [==============================] - 3s 328us/stepError: 0.0572Accuracy: 0.9814000129699707","link":"/digits_recognition/"},{"title":"Boston house analysis","text":"The source code: Boston House 1234567891011121314151617181920212223# Import package# Used to load the Boston housing price data setfrom sklearn.datasets import load_boston# pandas toolkit If you are unfamiliar with pandas, you can refer to the official 10-minute tutorial: https://pandas.pydata.org/pandas-docs/stable/10min.htmlimport pandas as pdimport numpy as np# seaborn for drawingimport seaborn as snsimport matplotlib.pyplot as plt# Show drawing%matplotlib inlinedata = load_boston() # load datasedata.keys() # Fields inside datadf = pd.DataFrame(data['data'])# Looking at the first 5 rows of the dataframe, we can see that the column names are numbersdf.head(5)data['feature_names'] # Feature name The Table params and chinese info 123456789101112131415params chinese infoCRIM 住房所在城镇的人均犯罪率ZN 住房用地超过 25000 平方尺的比例INDUS 住房所在城镇非零售商用土地的比例CHAS 有关查理斯河的虚拟变量（如果住房位于河边则为1,否则为0 ）NOX 一氧化氮浓度RM 每处住房的平均房间数AGE 建于 1940 年之前的业主自住房比例DIS 住房距离波士顿五大中心区域的加权距离RAD 离住房最近的公路入口编号TAX 每 10000 美元的全额财产税金额PTRATIO 住房所在城镇的师生比例B 1000(Bk-0.63)^2,其中 Bk 指代城镇中黑人的比例LSTAT 弱势群体人口所占比例MEDV 业主自住房的中位数房价（以千美元计） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# Replace numeric column names with feature namesdf.columns = data['feature_names']df.head(5)# The target is the house price, which is also our target value. We assign the target value to the dataframedf['price'] = data['target']df.head(5)# View the correlation coefficient between the feature and price, positive correlation and negative correlationsns.heatmap(df.corr(), annot=True, fmt='.1f')plt.scatter(df['RM'], df['price'])plt.figure(figsize=(20, 5))# View the data distribution display of some features and pricefeatures = ['LSTAT', 'RM']target = df['price']for i, col in enumerate(features): plt.subplot(1, len(features), i+1) x = df[col] y = target plt.scatter(x, y, marker = 'o') plt.title('{} price'.format(col)) plt.xlabel(col) plt.ylabel('price')# Simple example: univariate forecast pricex = df['RM']y = df['price']history_notes = {_x: _y for _x, _y in zip(x,y)}history_notes[6.575]# Find the top three prices that are closest to RM:6.57,similary_ys = [y for _, y in sorted(history_notes.items(), key=lambda x_y: (x_y[0] - 6.57) ** 2)[:3]]similary_ys# Calculate the average of threenp.mean(similary_ys) Use historical data to predict data that has never been seen before, the most direct method K-Neighbor-Nearst 12345678def knn(query_x, history, top_n = 3): sorted_notes = sorted(history.items(), key = lambda x_y: (x_y[0] - query_x)**2) similar_notes = sorted_notes[:top_n] similar_ys = [y for _, y in similar_notes] return np.mean(similar_ys)knn(5.4, history_notes) In order to obtain results faster, we hope to obtain predictive power by fitting a function \\[ f(rm) = k * rm + b \\] Random Approach \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} (\\hat{y_i} - y_i) ^ 2 \\] \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] 1234567891011121314151617181920212223def loss(y_hat, y): return np.mean((y_hat - y)**2)import randommin_loss = float('inf')best_k, best_b = None, Nonefor step in range(1000): min_v, max_v = -100, 100 k, b = random.randrange(min_v, max_v), random.randrange(min_v, max_v) y_hats = [k * rm_i + b for rm_i in x] current_loss = loss(y_hats, y) if current_loss &lt; min_loss: min_loss = current_loss best_k, best_b = k, b print(f'{step}, we have func f(rm) = {k} * rm + {b}, lss is :{current_loss}')plt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x]) Monte Carlo simulation(蒙特卡洛模拟) Supervisor \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{k}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i) * rm_i \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{b}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i)\\] 123456789101112131415161718192021222324252627def partial_k(k, b, x, y): return 2 * np.mean((k*x+b-y) * x)def partial_b(k, b, x, y): return 2 * np.mean(k*x+b-y)k, b = random.random(), random.random()min_loss = float('inf')best_k, best_b = None, Nonelearning_rate = 1e-2for step in range(2000): k, b = k + (-1 * partial_k(k, b, x, y) * learning_rate), b + (-1 * partial_b(k, b, x, y) * learning_rate) y_hats = k * x + b current_loss = loss(y_hats, y) if current_loss &lt; min_loss: min_loss = current_loss best_k, best_b = k, b print(f'setp {step}, we have func f(rm) = {k} * rm + {b}, lss is :{current_loss}')best_k, best_bplt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x]) Supervised Learning We turn the forecast of housing prices into a more responsible and sophisticated model. What should we do? \\[ f(x) = k * x + b \\] \\[ f(x) = k2 * \\sigma(k_1 * x + b_1) + b2 \\] \\[ \\sigma(x) = \\frac{1}{1 + e^(-x)} \\] 12345678910111213141516171819def sigmoid(x): return 1 / (1+np.exp(-x))sub_x = np.linspace(-10, 10)plt.plot(sub_x, sigmoid(sub_x))def random_linear(x): k, b = random.random(), random.random() return k * x + bdef complex_function(x): return (random_linear(x))for _ in range(10): index = random.randrange(0, len(sub_x)) sub_x_1, sub_x_2 = sub_x[:index], sub_x[index:] new_y = np.concatenate((complex_function(sub_x_1), complex_function(sub_x_2))) plt.plot(sub_x, new_y) We can implement more complex functions through simple, basic modules and repeated superposition For more and more complex functions? How does the computer seek guidance? What is machine learning? The shortcomings of this method of KNN, what is the background of the proposed linear fitting How to get faster function weight update through supervision method The combination of nonlinear and linear functions can fit very complex functions Deep learning we can fit more complex functions through basic function modules Assigment: \\[ L2-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{(\\hat{y} - y)}^2 \\] \\[ L1-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{|(\\hat{y} - y)|} \\] L2-Loss becomes L1Loss and achieves gradient descent Realize L1Loss gradient descent from 0 1. import package 12import numpy as npimport pandas as pd 2. load data 1234567891011from sklearn.datasets import load_bostondata = load_boston()data.keys()data_train = data.datadata_traget = data.targetdf = pd.DataFrame(data_train, columns = data.feature_names)df.head()df.describe() # Data description, you can view the statistics of each variable 3. Data preprocessing Normalization or standardization can prevent a certain dimension or a few dimensions from affecting the data too much when there are very many dimensions, and secondly, the program can run faster. There are many methods, such as standardization, min-max, z-score, p-norm, etc. How to use it depends on the characteristics of the data set. Further reading-数据标准化的迷思之深度学习领域 12345678910111213from sklearn.preprocessing import StandardScaler# z = (x-u) / s u is the mean, s is the standard deviationss = StandardScaler() data_train = ss.fit_transform(data_train)# For linear models, normalization or standardization is generally required, otherwise gradient explosion will occur, and tree models are generally not requireddata_train = pd.DataFrame(data_train, columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'])data_train.describe() # y=Σwixi+# Because the derivation of b is all 1, add a bias b to the data and set it to 1, as a feature of the data and update the gradient wi*b=widata_train['bias'] = 1data_train Divide the data set, where 20% of the data is used as the test set X_test, y_test, and the other 80% are used as the training set X_train, y_train, where random_state is the random seed 1234567from sklearn.model_selection import train_test_splittrain_x, test_x, train_y, test_y = train_test_split(data_train, data_traget, test_size = 0.2, random_state=42)print('train_x.shape, train_y.shape', train_x.shape, train_y.shape)print('test_x.shape, test_y.shape', test_x.shape, test_y.shape)train_x = np.array(train_x) Model training and gradient update 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def l1_cost(x, y, theta): &quot;&quot;&quot; x: 特征 y: 目标值 thta: 模型参数 &quot;&quot;&quot; k = x.shape[0] total_cost = 0 for i in range(k): total_cost += 1/k * np.abs(y[i] -theta.dot(x[i, :])) return total_costdef l2_cost(x, y, theta): k = x.shape[0] total_cost = 0 for i in range(k): total_cost += 1/k * (y[i] -theta.dot(x[i,:])) ** 2 return total_costnp.zeros(10).shapedef step_l1_gradient(x, y, learning_rate, theta): &quot;&quot;&quot; Function to calculate the gradient of the MAE loss function Return the gradient value 0 for the non-differentiable point at 0 X:特征向量 y：目标值 learing_rate:学习率 theta:参数 &quot;&quot;&quot; n = x.shape[0] # print(n) e = y - x @ theta gradients = - (x.T @ np.sign(e)) / n # sign is a sign function thata = theta - learning_rate * gradients return thetadef step_l2_gradient(x, y, learning_rate, theta): k = x.shape[0] n = x.shape[1] gradients = np.zeros(n) for i in range(k): for j in range(n): gradients[j] += (-2/k) * (y[i] - (theta.dot(x[i, :]))) * x[i, j] theta = theta - learning_rate * gradient return theta# def step_gradient(X, y, learning_rate, theta):# &quot;&quot;&quot;# X:特征向量# y：目标值# learing_rate:学习率# theta:参数# &quot;&quot;&quot;# m_deriv = 0# N = len(X)# for i in range(N):# # 计算偏导# # -x(y - (mx + b)) / |mx + b|# m_deriv += - X[i] * (y[i] - (theta*X[i] + b)) / abs(y[i] - (theta*X[i] + b))# # We subtract because the derivatives point in direction of steepest ascent# theta -= (m_deriv / float(N)) * learning_rate# # theta = theta - learning_rate * gradients# return thetadef gradient_descent(train_x, train_y, learning_rate, iterations): k = train_x.shape[0] n = train_x.shape[1] theta = np.zeros(n) # Initialization parameters loss_values = [] # print(theta.shape) for i in range(iterations): theta = step_l1_gradient(train_x, train_y, learning_rate, theta) loss = l1_cost(train_x, train_y, theta) loss_values.append(loss) print(i, 'cost:', loss) return theta, loss_values# Training parameterslearning_rate = 0.04 # Learning rateiterations = 300 # Number of iterationstheta, loss_values = gradient_descent(train_x, train_y, learning_rate, iterations)","link":"/boston_analysis/"},{"title":"enlight VS snapseed2","text":"image 本文知乎专栏地址 在Enlight出来之前，Snapseed曾经是我最主要的修图工具。 当然，Enlight确实是大而全的一款App，但是使用中，却让我有了无法忍受的一点。那就是丢失部分图片信息。 导出的时候，Enlight导出所用的格式是PNG，而Snapseed仍然是jpg导出。自然大家都知道PNG图片所用的格式是无损的，这就是说，Snapseed导出的图像相比Enlight要小很多，因为iPhone默认拍照所保存的都是jpg格式，所以即便再导出PNG格式，那所增加的部分应该是软件通过算法添加进去的。 而，Enlight在导出的PNG格式图片里，却丢失了相机信息，地理位置信息和更重要的拍照时间信息。照片的时间被导出的时间所代替。 同样一张照片，我们先使用Enlight进行修图 然后使用Snapseed 2 修图 以下是使用Metapho查看的照片信息。 1. 首先我们来看Enlight导出的照片信息: 可以看到Camera和Location信息都没有了，照片被导出成PNG格式，而且照片大了很多。 2. 我们再来看看Snapseed导出的照片信息 可以看到Camera和Location信息都保存完好。 这也是Enlight最让我无法接受的一点，其实Enlight在设置里是可以选择增加地理信息的，也就是会调用GPS模块，但是当导出的时候这些信息就全都没有了，即便是用Enlight来拍照。不知道下一版本的Enlight是否能够改善这点。如果对这些信息无所谓的同学，Enlight倒是现今为止最强大的修图工具，Snapseed虽然更新到版本2了，但是似乎依然没有加入曲线。Snapseed2的具体更新，可以点击原文链接查看「领客」内的文。 ——— image","link":"/enlight-VS-snapseed2/"},{"title":"Introduction to Artificial Intelligence","text":"The code address of this article is: Example 01 The source code is in ipynb format, and the output content can be viewed. rule based 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import random from icecream import ic#rules = &quot;&quot;&quot;#复合句子 = 句子 , 连词 句子#连词 = 而且 | 但是 | 不过#句子 = 主语 谓语 宾语#主语 = 你| 我 | 他 #谓语 = 吃| 玩 #宾语 = 桃子| 皮球# #&quot;&quot;&quot;rules = &quot;&quot;&quot;复合句子 = 句子 , 连词 复合句子 | 句子连词 = 而且 | 但是 | 不过句子 = 主语 谓语 宾语主语 = 你| 我 | 他 谓语 = 吃| 玩 宾语 = 桃子| 皮球 &quot;&quot;&quot;def get_grammer_by_description(description): rules_pattern = [r.split('=') for r in description.split('\\n') if r.strip()] target_with_expend = [(t, ex.split('|')) for t, ex in rules_pattern] grammer = {t.strip(): [e.strip() for e in ex] for t, ex in target_with_expend} return grammer#generated = [t for t in random.choice(grammer['句子']).split()]#test_v = [t for t in random.choice(grammer['谓语']).split()]def generate_by_grammer(grammer, target='句子'): if target not in grammer: return target return ''.join([generate_by_grammer(grammer, t) for t in random.choice(grammer[target]).split()])if __name__ == '__main__': grammer = get_grammer_by_description(rules) #ic(generated) #ic(test_v) #ic(generate_by_grammer(grammer)) ic(generate_by_grammer(grammer, target='复合句子')) water pouring 12345678910111213141516171819202122232425262728293031323334353637383940def water_pouring(b1, b2, goal, start=(0, 0)): if goal in start: return [start] explored = set() froniter = [[('init', start)]] while froniter: path = froniter.pop(0) (x, y) = path[-1][-1] for (state, action) in successors(x, y, b1, b2).items(): if state not in explored: explored.add(state) path2 = path + [(action, state)] if goal in state: return path2 else: froniter.append(path2) return []def successors(x, y, X, Y): return { ((0, y+x) if x + y &lt;= Y else (x + y - Y, Y)): 'X -&gt; Y', ((x + y, 0) if x + y &lt;= X else (X, x + y - X)): 'X &lt;- Y', (X, y): '灌满X', (x, Y): '灌满Y', (0, y): '倒空X', (x, 0): '倒空Y', }if __name__ == '__main__': print(water_pouring(4, 9, 5)) print(water_pouring(4, 9, 5, start=(4, 0))) print(water_pouring(4, 9, 6))","link":"/example_01/"},{"title":"使用Plotnine制作元素周期表","text":"首先需要了解元素周期表以及元素数据: 维基百科的元素周期表词条 元素数据 元素周期表基本构成如下: 族：表中的每一列就是一族，从左向右依次为 1、2……18 族。 周期：表中的行。 元素：每个方框表示一个元素，其中包括元素符号、名称、原子序数、原子量。 在主表下面还有镧系元素和锕系元素表。 用颜色区分金属、非金属等常见的物质状态。 最终呈现: image-20210912211523771 其他形状元素周期表 导入和处理数据 1234567# 导入依赖import pandas as pdimport numpy as npfrom plotnine import *# 读取数据elements = pd.read_csv('~/data/cbcpv/elemanets/elements.csv') 研究数据集 1234567891011121314151617181920212223242526272829303132elements.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 118 entries, 0 to 117Data columns (total 21 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 atomic number 118 non-null int64 1 symbol 118 non-null object 2 name 118 non-null object 3 atomic mass 118 non-null object 4 CPK 118 non-null object 5 electronic configuration 118 non-null object 6 electronegativity 97 non-null float64 7 atomic radius 71 non-null float64 8 ion radius 92 non-null object 9 van der Waals radius 38 non-null float64 10 IE-1 102 non-null float64 11 EA 85 non-null float64 12 standard state 99 non-null object 13 bonding type 98 non-null object 14 melting point 101 non-null float64 15 boiling point 94 non-null float64 16 density 96 non-null float64 17 metal 118 non-null object 18 year discovered 118 non-null object 19 group 118 non-null object 20 period 118 non-null int64 dtypes: float64(8), int64(2), object(11)memory usage: 19.5+ KB&quot;&quot;&quot; 特征group就是该元素所在的族，但是，如果用elements['group'] 查看所有内容，会发现有的记录中用 '-' 标记，说明它不属于任何族，说明它们应该是镧系元素或者锕系元素。根据数据分析的通常要求，'-' 符号最好用数字表示，这里用 ﹣1 转化数据集 123456789101112131415161718# 转换族elements['group'] = [-1 if g=='-' else int(g) for g in elements['group']]elements['group']&quot;&quot;&quot;0 11 182 13 24 13 ..113 14114 15115 16116 17117 18Name: group, Length: 118, dtype: int64&quot;&quot;&quot; 特征 bonding type、metal 都是分类数据，因此在类型上进行转化。 123# 转化分类数据elements['bonding type'] = elements['bonding type'].astype('category')elements['metal'] = elements['metal'].astype('category') 将原本的整数型atomic number特征,转化为字符串类型 1elements['atomic_number'] = elements['atomic number'].astype(str) 元素周期表有两个部分,上面一部分每个元素是属于某一个族的,即group特征中的1-18, 而对于值是-1的则表示这些元素应该在下面的镧系或者锕系元素表中。下面分别用 top 变量和 bottom 变量引用这两部分元素集合. 123## 分别用top和bottom变量引用上下部分元素集合top = elements.query('group != -1').copy()bottom = elements.query('group == -1').copy() 元素周期表中横向表示的是族（group），纵向表示的是周期（period），用下面的方式在 top 中创建两个特征，分别为“族”和“周期”的值。 1234567891011121314151617181920212223242526272829303132333435363738## 在top中区分“族”(group)和“周期”(period)的值&quot;&quot;&quot;横向表示族,纵向表示周期&quot;&quot;&quot;top['x'] = top.grouptop['y'] = top.periodtop['x']&quot;&quot;&quot;0 11 182 13 24 13 ..113 14114 15115 16116 17117 18Name: x, Length: 90, dtype: int64&quot;&quot;&quot;top['y']&quot;&quot;&quot;0 11 12 23 24 2 ..113 7114 7115 7116 7117 7Name: y, Length: 90, dtype: int64&quot;&quot;&quot; 除了上面的部分之外，下面的锕系和镧系元素也要做类似的配置。不过，横坐标不能用 group 特征的值，因为前面设置为 ﹣1。 12345678nrows = 2&quot;&quot;&quot;hshift 和 vshift 分别表示横、纵间距，这样就为每个锕系和镧系元素增加了横纵坐标值。&quot;&quot;&quot;hshift = 3.5vshift = 3bottom['x'] = np.tile(np.arange(len(bottom) // nrows), nrows) + hshiftbottom['y'] = bottom.period + vshift 每个元素都占了一个小方块,所以,这个小方块(元素块)的大小要设置一下 123## 设置元素占据的小矩形tile_width = 0.95tile_height = 0.95 开始画图 1234(ggplot(aes('x', 'y')) + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height))) image-20210912212441644 这里只有美学映射,没有传入数据集.因为在图层对象中,要传入不同的数据集: “top”和“bottom”. top表示主表中的, bottom表示下面的锕、镧系元素 geom_tile绘制安放元素块图层,并使用top数据集,在引入一个图层,绘制bottom对应的图层. 但是我们发现表反了, 所以需要实现在Y轴方向上的坐标轴翻转. 123456(ggplot(aes('x', 'y')) +geom_tile(top, aes(width=tile_width, height=tile_height)) +geom_tile(bottom, aes(width=tile_width, height=tile_height)) # 在Y轴上进行翻转 +scale_y_reverse() # new) image-20210912212812483 基本样式已经有了。 前面已经把特征“metal”的数据转换为分类数据，下面用这些数据对不同元素的小矩形（以后简称“元素块”）上色。 1234567(ggplot(aes('x', 'y')) # 对数据不同的元素块进行上色 + aes(fill='metal') # new + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + scale_y_reverse() ) image-20210912213102954 然后,我们要将化学元素的有关信息写到这些元素块上,这里要写到元素块上的包括: 原子序数，对应着数据集中的特征是“atomic number”； 元素符号，对应着数据集中的特征是“symbol”； 元素名称，对应着数据集中的特征是“name”； 原子量，对应着数据集中的特征是“automic mass” 在这里,我们要绘制四个图层,以便安放四个元素信息, 每个图层上面一个特征,并且每个图层的位置、字号大小等都不相同. 为此我们写一个函数方法来实现: 1234567891011121314151617181920&quot;&quot;&quot;nudge_x: 文本在水平方向上的相对位置nudge_y: 文本在竖直方向上的相对位置ha: 可选'left', 'center', 'right', 标示水平方向的对齐方式va: 可选'top', 'center', 'bottom', 表示竖直方向的堆砌方式size: 字号大小fontweight: 字族中的字体粗细&quot;&quot;&quot;def inner_text(data): layers = [geom_text(data, aes(label='atomic_number'), nudge_x=-0.40, nudge_y=-.40, ha='left', va='top', fontweight='normal', size=6), geom_text(data, aes(label='symbol'), nudge_y=.1, size=9), geom_text(data, aes(label='name'), nudge_y=-0.125, fontweight='normal', size=4.5), geom_text(data, aes(label='atomic mass'), nudge_y=-.3, fontweight='normal', size=4.5) ] return layers 然后我们将函数inner_text应用到绘图流程中去 12345678910111213&quot;&quot;&quot;分别调用两次是因为有top和bottom两个数据&quot;&quot;&quot;(ggplot(aes('x', 'y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) # 绘制上部分图层 + inner_text(top) # new # 绘制下部分图层 + inner_text(bottom) # new + scale_y_reverse()) image-20210912213420444 是不是觉得图很难看,原因在于我们还没对其进行调整,下面我们就要细微的调整图层,包括大小等 12345678910111213(ggplot(aes('x', 'y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) + scale_y_reverse() # coord_equal作用是设置坐标系的横轴和纵轴 # expand=False, 意味着坐标系的大小由制图所用数据决定 + coord_equal(expand=False) # new # 一个新主题,规定了图纸的尺寸 + theme(figure_size=(12,6)) # new) 在默认的主题中，横纵坐标的图上长度相等，也就是图像是呈现在一张正方形的图纸上，coord_equal 的作用就是设置坐标系的横轴和纵轴，它与 coord_fixed 是完全等效的，能够改变图纸的大小和长宽比例。参数 expand 的值是布尔值，如果为 False，则意味着坐标系的大小（即图纸的大小）由制图所用数据决定。 新增的第二个图层对象是一个新的主题，在其中规定了图纸的尺寸。 image-20210912213518732 我们仔细研究元素周期表,发现Lu和Lr两个元素比较特殊,其实它们不是单独的元素,而是对应着下部分两行的,因此要对这两个进行处理,以区分出与其他元素的不同. 我们将其分为两半,使用过PS作图的同学应该能想到两个不同颜色的图层叠加,上面的图层只有下面图层的一半,那么看起来就像是被分成了两半. 123456# split_df 是绘制新元素块所需要的数据集。split_df = pd.DataFrame({ 'x': 3-tile_width/4, 'y': [6,7], 'metal': pd.Categorical(['lanthanoid', 'actinoid'])}) 123456789101112(ggplot(aes('x','y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) # 将新的数据集用于叠加Lu和Lr的图层上进行遮挡 + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) # new + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) + scale_y_reverse() + coord_equal(expand=False) + theme(figure_size=(12, 6))) image-20210912213931384 基本制作完成了,下面来美化一下: 123456789101112131415161718(ggplot(aes('x', 'y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) + scale_y_reverse() # 对元素块填充色进行转换 + scale_fill_brewer(type='qual', palette=3) + coord_equal(expand=False) # 增加了一个经典的主题图层对象 + theme_void() + theme(figure_size=(12, 6), # 增加一个主题图层,并设置了该图层的尺寸和背景色 plot_background=element_rect(fill='white') )) image-20210912214021589 到最后了,我们要解决主表中的元素表上族和周期的问题 观察主表中的每一列,注意我们已经把Y轴映射反序了,如果在H元素的元素块上标注族的序号为“1”, 那么这个“1”的Y轴坐标应该是y=1, 同样,Sc元素块上标注族的需要“3”, 那么“3”的Y轴坐标应该是y=4. 这样,我们就可以创建每列及其对应的Y轴坐标了. 1234567## 创建每列(即:族, 编号为1-18)及其对应的Y轴坐标groupdf = pd.DataFrame({ 'group': range(1, 19), 'y': np.repeat([1,2,4,2,1], [1,1,10,5,1])})groupdf group y 0 1 1 1 2 2 2 3 4 3 4 4 4 5 4 5 6 4 6 7 4 7 8 4 8 9 4 9 10 4 10 11 4 11 12 4 12 13 2 13 14 2 14 15 2 15 16 2 16 17 2 17 18 1 让我们来标注族的序号 123456789101112131415161718192021222324252627## 标注族序号(ggplot(aes('x','y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) # 标注每一列族序号的文本图层 # aes('group', 'y', label='group') 重写了X轴和Y轴的映射 # inherit_aes=False, 不继承映射配置 + geom_text(groupdf, aes('group', 'y', label='group'), color='gray', nudge_y=.525, va='bottom', fontweight='normal', size=9, inherit_aes=False ) # 以Y轴调转坐标轴 + scale_y_reverse() # 对元素块填充色进行转换 + scale_fill_brewer(type='qual', palette=3) + coord_equal(expand=False) # 增加了一个经典的主题图层对象 + theme_void() + theme(figure_size=(12, 6), # 增加一个主题图层,并设置了该图层的尺寸和背景色 plot_background=element_rect(fill='white'), )) image-20210912214448270 最终,我们标注玩周期就完成了. 周期是对每一行的标注,一共7行,因为标注在左侧,可以把它看成是左侧的Y轴标示,可以在图层上通过对Y轴标示的设置完成周期的标注. 1234567891011121314151617181920212223242526272829303132333435363738## 开始标注周期, 最终完成(ggplot(aes('x', 'y')) # 把特征'metal'的数据转换为分类数据,进行元素块上色 + aes(fill='metal') # 创建上部元素块 + geom_tile(top, aes(width=tile_width, height=tile_height)) # 创建Lu和Lr的半个元素块 + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) # 创建下部元素块 + geom_tile(bottom, aes(width=tile_width, height=tile_height)) # 创建文字图层, 把化学元素的有关信息写到元素块中 + inner_text(top) + inner_text(bottom) # 标注每一列族序号的文本图层 # aes('group', 'y', label='group') 重写了X轴和Y轴的映射 # inherit_aes=False, 不继承映射配置 + geom_text(groupdf, aes('group', 'y', label='group'), color='gray', nudge_y=.525, va='bottom', fontweight='normal', size=9, inherit_aes=False ) # 以Y轴调转坐标轴, 增加了纵坐标主刻度标示数字。 + scale_y_reverse(breaks=range(1, 8), limits=(0, 10.5) ) # 对元素块填充色进行转换 + scale_fill_brewer(type='qual', palette=3) + coord_equal(expand=False) # 增加了一个经典的主题图层对象 + theme_void() + theme(figure_size=(12, 6), # 增加一个主题图层,并设置了该图层的尺寸和背景色 plot_background=element_rect(fill='white'), # 增加了参数 axis_text_y，对 Y 轴标示的显示格式进行了设置。 axis_text_y=element_text(margin={'r':5}, color='gray', size=9) )) image-20210912214640931 完成...","link":"/elements_by_plotnine/"},{"title":"Initial exploration of machine learning","text":"The code address of this article is: Example 02 The source code is in ipynb format, and the output content can be viewed. ## Gradient 123456789101112131415161718192021222324import randomdef loss(k): return 3 * (k ** 2) + 7 * k - 10 # -b / 2a = -7 / 6def partial(k): return 6 * k + 7k = random.randint(-10, 10)alpha = 1e-3 # 0.001for i in range(1000): k = k + (-1) * partial(k) *alpha print(k, loss(k)) # out&quot;&quot;&quot;7.959 124.32404299999999-7.918246 122.66813714954799show more (open the raw output data in a text editor) ...-1.1833014444482555 -14.082503185837805&quot;&quot;&quot; Cutting Problem All the dynamic programming: sub-problems Overlapping sub-problems parse solution 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from collections import defaultdictfrom functools import lru_cache# least recent usedprices = [1, 5, 8, 9, 10, 17, 17, 20, 24, 30, 33]complete_price = defaultdict(int)for i, p in enumerate(prices): complete_price[i+1] = p solution = {}cache = {}#&lt;- if when n .... is huge. size(cache)# keep most important information.@lru_cache(maxsize=2**10)def r(n): # a very classical dynamic programming problem # if n in cache: return cache[n] candidates = [(complete_price[n], (n, 0))] + \\ [(r(i) + r(n-i), (i, n - i)) for i in range(1, n)] optimal_price, split = max(candidates) solution[n] = split # cache[n] = optimal_price return optimal_pricedef parse_solution(n, cut_solution): left, right = cut_solution[n] if left == 0 or right == 0: return [left+right, ] else: return parse_solution(left, cut_solution) + parse_solution(right, cut_solution)if __name__ == '__main__': print(r(19)) print(parse_solution(19, solution)) # out&quot;&quot;&quot;55[11, 6, 2]&quot;&quot;&quot; Dynamic 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748from collections import defaultdictfrom functools import wrapsfrom icecream import icoriginal_price = [1,5,8,9,10,17,17,20,24,30,33]price = defaultdict(int)for i, p in enumerate(original_price): price[i+1] = p def memo(func): cache = {} @wraps(func) def _wrap(n): if n in cache: result = cache[n] else: result = func(n) cache[n] = result return result return _wrap @memodef r(n): max_price, split_point = max( [(price[n],0)] + [(r(i) + r(n-i), i) for i in range(1, n)], key=lambda x: x[0] ) solution[n] = (split_point, n-split_point) return max_price def not_cut(split): return split == 0def parse_solution(target_length, revenue_solution): left, right = revenue_solution[target_length] if not_cut(left): return [right] return parse_solution(left, revenue_solution) + parse_solution(right, revenue_solution) solution = {}r(50)ic(parse_solution(20,solution))ic(parse_solution(19,solution))ic(parse_solution(27,solution))# out&quot;&quot;&quot;ic| parse_solution(20,solution): [10, 10]ic| parse_solution(19,solution): [2, 6, 11]ic| parse_solution(27,solution): [6, 10, 11][6, 10, 11]&quot;&quot;&quot; Gradient descent 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npimport matplotlib.pyplot as pltimport randomfrom icecream import icdef func(x): return 10 * x**2 + 32*x + 9def gradient(x): return 20 *x + 32 x = np.linspace(-10, 10)steps = []x_star = random.choice(x)alpha = 1e-3for i in range(100): x_star = x_star + -1*gradient(x_star)*alpha steps.append(x_star) ic(x_star, func(x_star))fig, ax = plt.subplots()ax.plot(x, func(x))&quot;&quot;&quot;ic| x_star: 9.368, func(x_star): 1186.3702400000002ic| x_star: 9.14864, func(x_star): 1138.732618496show more (open the raw output data in a text editor) ...ic| x_star: -0.1157435825983131, func(x_star): 5.430171125980905[&lt;matplotlib.lines.Line2D at 0x7fd6d19545d0&gt;]&quot;&quot;&quot;for i, s in enumerate(steps): ax.annotate(str(i+1), (s, func(s))) plt.show() image-20210830234709856 k-means-finding-centers K-means 123456789101112131415161718192021222324252627282930313233343536373839404142from pylab import mplmpl.rcParams['font.sans-serif'] = ['FangSong'] # Specify the default fontmpl.rcParams['axes.unicode_minus'] = False # Solve the problem that the minus sign'-' is displayed as a square in the saved imagecoordination_source = &quot;&quot;&quot;{name:'兰州', geoCoord:[103.73, 36.03]},{name:'嘉峪关', geoCoord:[98.17, 39.47]},{name:'西宁', geoCoord:[101.74, 36.56]},{name:'成都', geoCoord:[104.06, 30.67]},{name:'石家庄', geoCoord:[114.48, 38.03]},{name:'拉萨', geoCoord:[102.73, 25.04]},{name:'贵阳', geoCoord:[106.71, 26.57]},{name:'武汉', geoCoord:[114.31, 30.52]},{name:'郑州', geoCoord:[113.65, 34.76]},{name:'济南', geoCoord:[117, 36.65]},{name:'南京', geoCoord:[118.78, 32.04]},{name:'合肥', geoCoord:[117.27, 31.86]},{name:'杭州', geoCoord:[120.19, 30.26]},{name:'南昌', geoCoord:[115.89, 28.68]},{name:'福州', geoCoord:[119.3, 26.08]},{name:'广州', geoCoord:[113.23, 23.16]},{name:'长沙', geoCoord:[113, 28.21]},//{name:'海口', geoCoord:[110.35, 20.02]},{name:'沈阳', geoCoord:[123.38, 41.8]},{name:'长春', geoCoord:[125.35, 43.88]},{name:'哈尔滨', geoCoord:[126.63, 45.75]},{name:'太原', geoCoord:[112.53, 37.87]},{name:'西安', geoCoord:[108.95, 34.27]},//{name:'台湾', geoCoord:[121.30, 25.03]},{name:'北京', geoCoord:[116.46, 39.92]},{name:'上海', geoCoord:[121.48, 31.22]},{name:'重庆', geoCoord:[106.54, 29.59]},{name:'天津', geoCoord:[117.2, 39.13]},{name:'呼和浩特', geoCoord:[111.65, 40.82]},{name:'南宁', geoCoord:[108.33, 22.84]},//{name:'西藏', geoCoord:[91.11, 29.97]},{name:'银川', geoCoord:[106.27, 38.47]},{name:'乌鲁木齐', geoCoord:[87.68, 43.77]},{name:'香港', geoCoord:[114.17, 22.28]},{name:'澳门', geoCoord:[113.54, 22.19]}&quot;&quot;&quot; Feacutre Extractor 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768city_location = { '香港': (114.17, 22.28)}test_string = &quot;{name:'兰州', geoCoord:[103.73, 36.03]},&quot;import repattern = re.compile(r&quot;name:'(\\w+)',\\s+geoCoord:\\[(\\d+.\\d+),\\s(\\d+.\\d+)\\]&quot;)for line in coordination_source.split('\\n'): city_info = pattern.findall(line) if not city_info: continue # following: we find the city info city, long, lat = city_info[0] long, lat = float(long), float(lat) city_location[city] = (long, lat)city_location# output&quot;&quot;&quot;{'香港': (114.17, 22.28), '兰州': (103.73, 36.03),show more (open the raw output data in a text editor) ... '澳门': (113.54, 22.19)}&quot;&quot;&quot;import mathdef geo_distance(origin, destination): &quot;&quot;&quot; Calculate the Haversine distance. Parameters ---------- origin : tuple of float (lat, long) destination : tuple of float (lat, long) Returns ------- distance_in_km : float Examples -------- &gt;&gt;&gt; origin = (48.1372, 11.5756) # Munich &gt;&gt;&gt; destination = (52.5186, 13.4083) # Berlin &gt;&gt;&gt; round(distance(origin, destination), 1) 504.2 &quot;&quot;&quot; lon1, lat1 = origin lon2, lat2 = destination radius = 6371 # km dlat = math.radians(lat2 - lat1) dlon = math.radians(lon2 - lon1) a = (math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon / 2) * math.sin(dlon / 2)) c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)) d = radius * c return d Vector Distances 余弦距离 Cosine Distance 欧几里得距离 Euclidean Distance 曼哈顿距离 Manhattan distance or Manhattan length 12345678910111213import matplotlib.pyplot as pltimport networkx as nximport warningswarnings.filterwarnings('ignore')%matplotlib inline# set plt, show chineseplt.rcParams['font.sans-serif'] = ['Arial Unicode MS']plt.rcParams['axes.unicode_minus'] = Falsecity_graph = nx.Graph()city_graph.add_nodes_from(list(city_location.keys()))nx.draw(city_graph, city_location, with_labels=True, node_size=30) image-20210830234858640 K-means: Initial k random centers 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394k = 10import randomall_x = []all_y = []for _, location in city_location.items(): x, y = location all_x.append(x) all_y.append(y)def get_random_center(all_x, all_y): r_x = random.uniform(min(all_x), max(all_x)) r_y = random.uniform(min(all_y), max(all_y)) return r_x, r_yget_random_center(all_x, all_y)# output&quot;&quot;&quot;(93.61182991130997, 37.01816228131414)&quot;&quot;&quot;K = 5centers = {'{}'.format(i+1): get_random_center(all_x, all_y) for i in range(K)}from collections import defaultdictcloset_points = defaultdict(list)for x, y, in zip(all_x, all_y): closet_c, closet_dis = min([(k, geo_distance((x, y), centers[k])) for k in centers], key=lambda t: t[1]) closet_points[closet_c].append([x, y])import numpy as npdef iterate_once(centers, closet_points, threshold=5): have_changed = False for c in closet_points: former_center = centers[c] neighbors = closet_points[c] neighbors_center = np.mean(neighbors, axis=0) if geo_distance(neighbors_center, former_center) &gt; threshold: centers[c] = neighbors_center have_changed = True else: pass ## keep former center return centers, have_changeddef kmeans(Xs, k, threshold=5): all_x = Xs[:, 0] all_y = Xs[:, 1] K = k centers = {'{}'.format(i+1): get_random_center(all_x, all_y) for i in range(K)} changed = True while changed: closet_points = defaultdict(list) for x, y, in zip(all_x, all_y): closet_c, closet_dis = min([(k, geo_distance((x, y), centers[k])) for k in centers], key=lambda t: t[1]) closet_points[closet_c].append([x, y]) centers, changed = iterate_once(centers, closet_points, threshold) print('iteration') return centerskmeans(np.array(list(city_location.values())), k=5, threshold=5)# output&quot;&quot;&quot;iterationiterationiterationiterationiteration{'1': array([99.518, 38.86 ]), '2': array([117.833, 39.861]), '3': array([91.11, 29.97]), '4': array([106.81, 27. ]), '5': array([116.87166667, 27.6275 ])}&quot;&quot;&quot;plt.scatter(all_x, all_y)plt.scatter(*zip(*centers.values())) image-20210830235114060 12for c, points in closet_points.items(): plt.scatter(*zip(*points)) image-20210830235135375 1234567891011121314151617181920212223city_location_with_station = { '能源站-{}'.format(i): position for i, position in centers.items()}city_location_with_station# output&quot;&quot;&quot;{'能源站-1': (108.82946246581274, 26.05763939719317), '能源站-2': (97.96769355736322, 22.166113183141032), '能源站-3': (114.05390380408154, 38.7698708467224), '能源站-4': (118.49242085311417, 28.665716162786204), '能源站-5': (125.08287617496866, 25.55784683330647)}&quot;&quot;&quot;def draw_cities(citise, color=None): city_graph = nx.Graph() city_graph.add_nodes_from(list(citise.keys())) nx.draw(city_graph, citise, node_color=color, with_labels=True, node_size=30)%matplotlib inlineplt.figure(1,figsize=(12,12)) draw_cities(city_location_with_station, color='green')draw_cities(city_location, color='red') image-20210830235243564 About the dataset This contains data of news headlines published over a period of 15 years. From the reputable Australian news source ABC (Australian Broadcasting Corp.) Site: http://www.abc.net.au/ Prepared by Rohit Kulkarni 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as np import pandas as pd import matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.feature_extraction import textfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.cluster import KMeansfrom nltk.tokenize import RegexpTokenizerfrom nltk.stem.snowball import SnowballStemmerimport warningswarnings.filterwarnings('ignore')%matplotlib inlinedata = pd.read_csv(&quot;./data/abcnews-date-text.csv&quot;,error_bad_lines=False,usecols =[&quot;headline_text&quot;])data.head()# output&quot;&quot;&quot;headline_text0 aba decides against community broadcasting lic...1 act fire witnesses must be aware of defamation2 a g calls for infrastructure protection summit3 air nz staff in aust strike for pay rise4 air nz strike to affect australian travellers&quot;&quot;&quot;data.to_csv('abcnews.csv', index=False, encoding='utf8')data.info()# output&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 1103665 entries, 0 to 1103664Data columns (total 1 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 headline_text 1103665 non-null objectdtypes: object(1)memory usage: 8.4+ MB&quot;&quot;&quot; Deleting dupliate headlines(if any) 12data[data['headline_text'].duplicated(keep=False)].sort_values('headline_text').head(8)data = data.drop_duplicates('headline_text') NLP Preparing data for vectorizaion However, when doing natural language processing, words must be converted into vectors that machine learning algorithms can make use of. If your goal is to do machine learning on text data, like movie reviews or tweets or anything else, you need to convert the text data into numbers. This process is sometimes referred to as “embedding” or “vectorization”. In terms of vectorization, it is important to remember that it isn’t merely turning a single word into a single number. While words can be transformed into numbers, an entire document can be translated into a vector. Not only can a vector have more than one dimension, but with text data vectors are usually high-dimensional. This is because each dimension of your feature data will correspond to a word, and the language in the documents you are examining will have thousands of words. TF-IDF In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf. Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification. One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model. 123456789101112131415punc = ['.', ',', '&quot;', &quot;'&quot;, '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',&quot;%&quot;]stop_words = text.ENGLISH_STOP_WORDS.union(punc)desc = data['headline_text'].valuesvectorizer = TfidfVectorizer(stop_words = stop_words)X = vectorizer.fit_transform(desc)word_features = vectorizer.get_feature_names()print(len(word_features))print(word_features[5000:5100])# output&quot;&quot;&quot;96397['abyss', 'ac', 'aca', 'acacia', 'acacias', 'acadamy', 'academia', 'academic', 'academics', 'academies', 'academy', 'academys', 'acai', 'acapulco', 'acars', 'acason', 'acasuso', 'acb', 'acbf', 'acc', 'acca', 'accan', 'accc', 'acccc', 'acccs', 'acccused', 'acce', 'accedes', 'accelerant', 'accelerants', 'accelerate', 'accelerated', 'accelerates', 'accelerating', 'acceleration', 'accelerator', 'accen', 'accent', 'accents', 'accentuate', 'accentuates', 'accentuating', 'accenture', 'accept', 'acceptability', 'acceptable', 'acceptably', 'acceptance', 'acceptances', 'accepted', 'accepting', 'acceptor', 'acceptors', 'accepts', 'accerate', 'acces', 'access', 'accessary', 'accessed', 'accesses', 'accessibility', 'accessible', 'accessing', 'accessories', 'accessory', 'accesss', 'acci', 'accid', 'accide', 'acciden', 'accidenatlly', 'accidenbt', 'accident', 'accidental', 'accidentally', 'accidently', 'accidents', 'acciona', 'accis', 'acclaim', 'acclaimed', 'acclamation', 'acclimatise', 'acco', 'accolade', 'accolades', 'accom', 'accomm', 'accommoda', 'accommodate', 'accommodated', 'accommodates', 'accommodating', 'accommodation', 'accomo', 'accomodation', 'accomommodation', 'accompanied', 'accompanies', 'accompaniment']&quot;&quot;&quot; Stemming Stemming is the process of reducing a word into its stem, i.e. its root form. The root form is not necessarily a word by itself, but it can be used to generate words by concatenating the right suffix. For example, the words fish, fishes and fishing all stem into fish, which is a correct word. On the other side, the words study, studies and studying stems into studi, which is not an English word. Tokenizing Tokenization is breaking the sentence into words and punctuation, 12345stemmer = SnowballStemmer('english')tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')def tokenize(text): return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())] Vectorization with stop words(words irrelevant to the model), stemming and tokenizing 123456789101112131415vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)X2 = vectorizer2.fit_transform(desc)word_features2 = vectorizer2.get_feature_names()print(len(word_features2))print(word_features2[:50]) # output&quot;&quot;&quot;65232[&quot;'a&quot;, &quot;'i&quot;, &quot;'s&quot;, &quot;'t&quot;, 'aa', 'aaa', 'aaahhh', 'aac', 'aacc', 'aaco', 'aacta', 'aad', 'aadmi', 'aag', 'aagaard', 'aagard', 'aah', 'aalto', 'aam', 'aamer', 'aami', 'aamodt', 'aandahl', 'aant', 'aap', 'aapa', 'aapt', 'aar', 'aaradhna', 'aardman', 'aardvark', 'aargau', 'aaron', 'aaronpaul', 'aarwun', 'aat', 'ab', 'aba', 'abaaoud', 'ababa', 'aback', 'abadi', 'abadon', 'abal', 'abalon', 'abalonv', 'abama', 'abandon', 'abandond', 'abandong']&quot;&quot;&quot;vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)X3 = vectorizer3.fit_transform(desc)words = vectorizer3.get_feature_names() For this, we will use k-means clustering algorithm. ### K-means clustering (Source Wikipedia) Elbow method to select number of clusters This method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified. Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance. Basically, number of clusters = the x-axis value of the point that is the corner of the \"elbow\"(the plot looks often looks like an elbow) 123456789101112from sklearn.cluster import KMeanswcss = []for i in range(1,11): kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0) kmeans.fit(X3) wcss.append(kmeans.inertia_)plt.plot(range(1,11),wcss)plt.title('The Elbow Method')plt.xlabel('Number of clusters')plt.ylabel('WCSS')plt.savefig('elbow.png')plt.show() image-20210830235601231 As more than one elbows have been generated, I will have to select right amount of clusters by trial and error. So, I will showcase the results of different amount of clusters to find out the right amount of clusters. 123456print(words[250:300])# output&quot;&quot;&quot;['decis', 'declar', 'defenc', 'defend', 'delay', 'deliv', 'demand', 'deni', 'despit', 'destroy', 'detent', 'develop', 'die', 'director', 'disabl', 'disast', 'discuss', 'diseas', 'dismiss', 'disput', 'doctor', 'dog', 'dollar', 'domest', 'donald', 'donat', 'doubl', 'doubt', 'draw', 'dri', 'drink', 'drive', 'driver', 'drop', 'drought', 'drown', 'drug', 'drum', 'dump', 'dure', 'e', 'eagl', 'earli', 'eas', 'east', 'econom', 'economi', 'edg', 'educ', 'effort']&quot;&quot;&quot; 3 Clusters 12345678910111213kmeans = KMeans(n_clusters = 3, n_init = 20, n_jobs = 1) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)kmeans.fit(X3)# We look at 3 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid)) # output&quot;&quot;&quot;0 : new, say, plan, win, council, govt, australia, report, kill, fund, urg, court, warn, water, australian, nsw, open, chang, year, qld, interview, wa, death, face, crash1 : polic, investig, probe, man, search, offic, hunt, miss, arrest, death, car, shoot, drug, seek, attack, assault, say, murder, crash, charg, driver, suspect, fatal, raid, station2 : man, charg, murder, court, face, jail, assault, stab, die, death, drug, guilti, child, sex, accus, attack, woman, crash, arrest, car, kill, miss, sydney, alleg, plead&quot;&quot;&quot; 5 Clusters 123456789101112131415kmeans = KMeans(n_clusters = 5, n_init = 20, n_jobs = 1)kmeans.fit(X3)# We look at 5 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))# output&quot;&quot;&quot;0 : man, plan, charg, court, govt, australia, face, murder, accus, jail, assault, stab, urg, drug, death, attack, child, sex, die, woman, guilti, say, alleg, told, car1 : new, zealand, law, year, plan, open, polic, home, hospit, centr, deal, set, hope, australia, look, appoint, announc, chief, say, south, minist, govt, rule, servic, welcom2 : say, win, kill, report, australian, warn, interview, open, water, fund, nsw, crash, death, urg, year, chang, wa, sydney, claim, qld, hit, attack, world, set, health3 : council, plan, consid, fund, rate, urg, seek, new, merger, water, land, develop, reject, say, mayor, vote, chang, elect, rise, meet, park, push, want, govt, approv4 : polic, investig, man, probe, search, offic, hunt, miss, arrest, death, car, charg, shoot, drug, seek, attack, assault, murder, crash, say, driver, fatal, suspect, raid, woman&quot;&quot;&quot; 6 Clusters 12345678910111213141516kmeans = KMeans(n_clusters = 6, n_init = 20, n_jobs = 1)kmeans.fit(X3)# We look at 6 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid)) # output&quot;&quot;&quot;0 : council, govt, australia, report, warn, urg, fund, australian, water, nsw, chang, qld, wa, health, elect, rural, countri, hour, sa, boost, climat, govern, servic, south, consid1 : man, charg, murder, court, face, jail, assault, stab, die, death, drug, guilti, child, sex, accus, attack, woman, crash, arrest, car, kill, miss, sydney, plead, alleg2 : polic, investig, probe, man, search, offic, hunt, miss, arrest, death, car, shoot, drug, seek, attack, crash, assault, murder, charg, driver, say, fatal, suspect, raid, warn3 : win, kill, court, interview, crash, open, death, sydney, face, year, claim, hit, attack, world, set, final, day, hous, die, home, jail, talk, return, cup, hospit4 : new, zealand, law, year, plan, open, council, polic, home, hospit, centr, deal, set, hope, australia, appoint, look, announc, chief, say, govt, south, minist, mayor, welcom5 : say, plan, council, govt, water, need, group, chang, labor, minist, govern, opposit, public, mp, health, union, green, hous, develop, resid, report, expert, cut, australia, mayor&quot;&quot;&quot; 8 Clusters 123456789101112131415161718kmeans = KMeans(n_clusters = 8, n_init = 20, n_jobs = 1)kmeans.fit(X3)# Finally, we look at 8 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid)) # output&quot;&quot;&quot;0 : polic, say, man, miss, arrest, jail, investig, car, search, murder, attack, crash, kill, probe, die, hunt, shoot, assault, offic, drug, stab, accus, fatal, guilti, bodi1 : death, hous, polic, toll, investig, man, probe, inquest, rise, woman, coron, blaze, price, public, white, babi, sentenc, famili, road, spark, jail, prompt, blame, custodi, report2 : plan, council, govt, water, new, say, develop, hous, group, chang, unveil, reject, park, urg, centr, public, expans, green, resid, health, reveal, labor, govern, opposit, power3 : court, face, man, accus, told, hear, murder, high, case, appear, rule, charg, alleg, appeal, drug, jail, woman, death, assault, order, sex, stab, challeng, teen, polic4 : australia, govt, kill, report, warn, australian, urg, fund, nsw, interview, water, open, crash, qld, chang, wa, year, day, claim, hit, attack, sydney, set, health, world5 : new, council, zealand, law, fund, year, consid, water, urg, open, say, seek, rate, centr, mayor, govt, elect, look, develop, land, deal, hope, set, push, home6 : win, award, cup, titl, open, gold, stage, world, final, tour, elect, australia, lead, seri, aussi, claim, second, australian, big, england, grand, m, battl, race, record7 : charg, man, murder, face, assault, drug, polic, child, sex, woman, teen, death, stab, drop, alleg, attack, rape, men, guilti, shoot, bail, sydney, fatal, driver, yo&quot;&quot;&quot; Because even I didn't know what kind of clusters would be generated, I will describe them in comments. Other discussions 1234567891011121314151617181920212223242526272829303132333435363738394041import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom nltk.corpus import stopwordsfrom sklearn.feature_extraction.text import CountVectorizerimport gensimfrom collections import Counterimport stringfrom nltk.stem import WordNetLemmatizer, PorterStemmerfrom nltk.tokenize import word_tokenizeimport pyLDAvis.gensim_modelsfrom wordcloud import WordCloud, STOPWORDSfrom textblob import TextBlobfrom spacy import displacyimport nltkimport warningswarnings.filterwarnings('ignore')# set pltplt.rcParams['font.sans-serif'] = ['Arial Unicode MS']plt.rcParams.update({'font.size': 12})plt.rcParams.update({'figure.figsize': [16, 12]})# plt.figure(figsize = [20, 20])plt.style.use('seaborn-whitegrid')df = pd.read_csv('../data/abcnews-date-text.csv', nrows = 10000)df.head()# output&quot;&quot;&quot;publish_date headline_text0 20030219 aba decides against community broadcasting lic...1 20030219 act fire witnesses must be aware of defamation2 20030219 a g calls for infrastructure protection summit3 20030219 air nz staff in aust strike for pay rise4 20030219 air nz strike to affect australian travellers&quot;&quot;&quot; The data set contains only two columns, the release date and the news title. For simplicity, I will explore the first 10,000 rows in this dataset. Since the titles are sorted by publish_date, they are actually two months from February 19, 2003 to April 7, 2003. Number of characters present in each sentence Visualization of text statistics is a simple but insightful technique. They include: Word frequency analysis, sentence length analysis, average word length analysis, etc. These really help to explore the basic characteristics of text data. For this, we will mainly use histograms (continuous data) and bar graphs (categorical data). First, let me look at the number of characters in each sentence. This can give us a rough idea of the length of news headlines. 1df['headline_text'].str.len().hist() image-20210831000016641 number of words appearing in each news headline The histogram shows that the range of news headlines is 10 to 70 characters, usually between 25 and 55 characters. Now, we will continue to explore the data verbatim. Let's plot the number of words that appear in each news headline. 1df['headline_text'].str.split().map(lambda x: len(x)).hist() image-20210831000044656 Analysing word length Obviously, the number of words in news headlines is in the range of 2 to 12, and most of them are between 5 and 7. Next, let's check the average word length in each sentence. 1df['headline_text'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x)).hist() image-20210831000111622 The average word length is between 3 and 9, and the most common length is 5. Does this mean that people use very short words in news headlines? Let us find out. One reason that may not be the case is stop words. Stop words are the most commonly used words in any language (such as \"the\", \"a\", \"an\", etc.). Since the length of these words may be small, these words may cause the above graphics to be skewed to the left. Analyzing the number and types of stop words can give us some in-depth understanding of the data. To get a corpus containing stop words, you can use the nltk library. Nltk contains stop words from multiple languages. Since we only deal with English news, I will filter English stop words from the corpus. Analysing stopwords 1234567891011121314151617181920212223242526272829# Fetch stopwordsimport nltknltk.download('stopwords')stop=set(stopwords.words('english'))# output&quot;&quot;&quot;[nltk_data] Downloading package stopwords to /Users/xx/nltk_data...[nltk_data] Package stopwords is already up-to-date!&quot;&quot;&quot;# Create corpuscorpus=[]new= df['headline_text'].str.split()new=new.values.tolist()corpus=[word for i in new for word in i]from collections import defaultdictdic=defaultdict(int)for word in corpus: if word in stop: dic[word]+=1 # Plot top stopwordstop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] x,y=zip(*top)plt.bar(x,y) Draw popular stop words image-20210831000213620 Most common words We can clearly see that in the news headlines, stop words such as \"to\", \"in\" and \"for\" dominate. So now that we know which stop words appear frequently in our text, let's check which words other than these stop words appear frequently. We will use the counter function in the collection library to count the occurrence of each word and store it in a list of tuples. This is a very useful feature when we are dealing with word-level analysis in natural language processing. 12345678910counter=Counter(corpus)most=counter.most_common()x, y=[], []for word,count in most[:40]: if (word not in stop): x.append(word) y.append(count) sns.barplot(x=y,y=x) image-20210831000244040 Wow! In the past 15 years, \"America\", \"Iraq\" and \"War\" have dominated the headlines. \"We\" here may mean the United States or us (you and me). We are not a stop word, but when we look at the other words in the picture, they are all related to the United States-the Iraq War and \"we\" here may mean the United States. Ngram analysis Ngram is a continuous sequence of n words. For example, \"Riverbank\", \"Three Musketeers\" and so on. If the number of words is two, it is called a double word. For 3 characters, it is called a trigram, and so on. Viewing the most common n-grams can give you a better understanding of the context in which the word is used. Bigram analysis To build our vocabulary, we will use Countvectorizer. Countvectorizer is a simple method for labeling, vectorizing and representing corpora in an appropriate form. Can be found in sklearn.feature_engineering.text Therefore, we will analyze the top news in all news headlines. 123456789101112def get_top_ngram(corpus, n=None): vec = CountVectorizer(ngram_range=(n, n)).fit(corpus) bag_of_words = vec.transform(corpus) sum_words = bag_of_words.sum(axis=0) words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) return words_freq[:10]top_n_bigrams=get_top_ngram(df['headline_text'],2)[:10]x,y=map(list,zip(*top_n_bigrams))sns.barplot(x=y,y=x) image-20210831000322862 Trigram analysis We can observe that dualisms such as \"anti-war\" and \"killed\" related to war dominate the headlines. How about triples? 123top_tri_grams=get_top_ngram(df['headline_text'],n=3)x,y=map(list,zip(*top_tri_grams))sns.barplot(x=y,y=x) image-20210831000347490 We can see that many of these hexagrams are a combination of \"face the court\" and \"anti-war protest.\" This means that we should spend some effort on data cleaning to see if we can combine these synonyms into a clean token. Topic modelling Use pyLDAvis for topic modeling exploration Topic modeling is the process of using unsupervised learning techniques to extract the main topics that appear in the document set. Latent Dirichlet Allocation (LDA) is an easy-to-use and efficient topic modeling model. Each document is represented by a topic distribution, and each topic is represented by a word distribution. Once the documents are classified into topics, you can delve into the data for each topic or topic group. But before entering topic modeling, we must do some preprocessing of the data. we will: Tokenization: The process of converting sentences into tokens or word lists. remove stopwordslemmatize: Reduce the deformed form of each word to a common base or root. Convert to word bag: word bag is a dictionary where the key is the word (or ngram/tokens) and the value is the number of times each word appears in the corpus. With NLTK, you can easily tokenize and formalize: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import nltknltk.download('punkt')nltk.download('wordnet')# output&quot;&quot;&quot;[nltk_data] Downloading package punkt to /Users/xx/nltk_data...[nltk_data] Package punkt is already up-to-date![nltk_data] Downloading package wordnet to /Users/xx/nltk_data...[nltk_data] Unzipping corpora/wordnet.zip.True&quot;&quot;&quot;def preprocess_news(df): corpus=[] stem=PorterStemmer() lem=WordNetLemmatizer() for news in df['headline_text']: words=[w for w in word_tokenize(news) if (w not in stop)] words=[lem.lemmatize(w) for w in words if len(w)&gt;2] corpus.append(words) return corpus corpus = preprocess_news(df)# Now, let's use gensim to create a bag of words modeldic=gensim.corpora.Dictionary(corpus)bow_corpus = [dic.doc2bow(doc) for doc in corpus]# We can finally create the LDA model:lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 4, id2word = dic, passes = 10, workers = 2)lda_model.show_topics()# output&quot;&quot;&quot;[(0, '0.010*&quot;say&quot; + 0.007*&quot;cup&quot; + 0.006*&quot;war&quot; + 0.005*&quot;world&quot; + 0.005*&quot;back&quot; + 0.005*&quot;plan&quot; + 0.005*&quot;green&quot; + 0.004*&quot;win&quot; + 0.004*&quot;woman&quot; + 0.004*&quot;new&quot;'), (1, '0.010*&quot;govt&quot; + 0.009*&quot;war&quot; + 0.009*&quot;new&quot; + 0.007*&quot;may&quot; + 0.005*&quot;sars&quot; + 0.005*&quot;call&quot; + 0.005*&quot;protest&quot; + 0.005*&quot;boost&quot; + 0.005*&quot;group&quot; + 0.004*&quot;hospital&quot;'), (2, '0.018*&quot;police&quot; + 0.015*&quot;baghdad&quot; + 0.014*&quot;man&quot; + 0.005*&quot;missing&quot; + 0.005*&quot;claim&quot; + 0.005*&quot;court&quot; + 0.005*&quot;australia&quot; + 0.004*&quot;move&quot; + 0.004*&quot;murder&quot; + 0.004*&quot;charged&quot;'), (3, '0.030*&quot;iraq&quot; + 0.015*&quot;war&quot; + 0.007*&quot;iraqi&quot; + 0.007*&quot;council&quot; + 0.006*&quot;troop&quot; + 0.005*&quot;killed&quot; + 0.004*&quot;crash&quot; + 0.004*&quot;soldier&quot; + 0.004*&quot;open&quot; + 0.004*&quot;say&quot;')]&quot;&quot;&quot; Theme 0 represents things related to the Iraq war and the police. Theme 3 shows Australia's involvement in the Iraq War. You can print all the topics and try to understand them, but there are tools that can help you run this data exploration more effectively. pyLDAvis is such a tool, it can interactively visualize the results of LDA. Visualize the topics 123pyLDAvis.enable_notebook()vis = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dic)vis image-20210831000602995 On the left, the area of each circle represents the importance of the topic relative to the corpus. Because there are four themes, we have four circles. The distance between the center of the circle indicates the similarity between themes. Here you can see that Topic 3 and Topic 4 overlap, which indicates that the themes are more similar. On the right, the histogram of each topic shows the top 30 related words. For example, in topic 1, the most relevant words are \"police\", \"new\", \"may\", \"war\", etc. Therefore, in our case, we can see many war-related words and topics in the news headlines. Wordclouds Wordcloud is a great way to represent text data. The size and color of each word appearing in the word cloud indicate its frequency or importance. It is easy to create a wordcloud using python, but we need to provide data in the form of a corpus. 123456789101112131415161718192021stopwords = set(STOPWORDS)def show_wordcloud(data, title = None): wordcloud = WordCloud( background_color='white', stopwords=stopwords, max_words=100, max_font_size=30, scale=3, random_state=1 ) wordcloud=wordcloud.generate(str(data)) fig = plt.figure(1, figsize=(12, 12)) plt.axis('off') plt.imshow(wordcloud) plt.show() show_wordcloud(corpus) image-20210831000635924 Similarly, you can see that terms related to war are highlighted, indicating that these words often appear in news headlines. There are many parameters that can be adjusted. Some of the most famous are: stopwords: stop a group of words appearing in the image. max_words: Indicates the maximum number of words to be displayed. max_font_size: Maximum font size. There are many other options to create beautiful word clouds. For more detailed information, you can refer to here. Text sentiment Sentiment analysis is a very common natural language processing task in which we determine whether the text is positive, negative or neutral. This is very useful for finding sentiments related to comments and comments, allowing us to gain some valuable insights from text data. There are many projects that can help you use python for sentiment analysis. I personally like TextBlob and Vader Sentiment. 1234567from textblob import TextBlobTextBlob('100 people killed in Iraq').sentiment# output&quot;&quot;&quot;Sentiment(polarity=-0.2, subjectivity=0.0)&quot;&quot;&quot; Textblob Textblob is a python library built on top of nltk. It has been around for a while and is very easy to use. The sentiment function of TextBlob returns two attributes: Polarity: It is a floating-point number in the range of [-1,1], where 1 means a positive statement and -1 means a negative statement. Subjectivity: refers to how personal opinions and feelings affect someone’s judgment. The subjectivity is expressed as a floating point value with a range of [0,1]. I will run this feature on news headlines. TextBlob claims that the text \"100 people killed in Iraq\" is negative, not a view or feeling, but a statement of fact. I think we can agree to TextBlob here. Now that we know how to calculate these sentiment scores, we can use histograms to visualize them and explore the data further. 123456def polarity(text): return TextBlob(text).sentiment.polaritydf['polarity_score']=df['headline_text'].\\ apply(lambda x : polarity(x))df['polarity_score'].hist() image-20210831000735233 You will see that the polarity is mainly between 0.00 and 0.20. This shows that most news headlines are neutral. Let's categorize news as negative, positive, and neutral based on the scores for a more in-depth study. Postive , Negative or Neutral ? 1234567891011121314def sentiment(x): if x&lt;0: return 'neg' elif x==0: return 'neu' else: return 'pos' df['polarity']=df['polarity_score'].\\ map(lambda x: sentiment(x)) plt.bar(df.polarity.value_counts().index, df.polarity.value_counts()) image-20210831000804842 Yes, 70% of news is neutral, only 18% of positive news and 11% of negative news. Let's look at the positive and negative headlines. 1234567891011df[df['polarity']=='neg']['headline_text'].head(5)# output&quot;&quot;&quot;7 aussie qualifier stosur wastes four memphis match23 carews freak goal leaves roma in ruins28 council chief executive fails to secure position34 dargo fire threat expected to rise40 direct anger at govt not soldiers crean urgesName: headline_text, dtype: object&quot;&quot;&quot; Vader The next library we are going to discuss is VADER. Vader is better at detecting negative emotions. It is very useful in the context of social media text sentiment analysis. The VADER or Valence Aware dictionary and sentiment reasoner is an open source sentiment analyzer pre-built library based on rules/dictionaries and is protected by the MIT license. The VADER sentiment analysis class returns a dictionary that contains the possibility that the text appears positive, negative, and neutral. Then, we can filter and select the emotion with the highest probability. We will use VADER to perform the same analysis and check if the difference is large. 1234567891011121314151617181920212223from nltk.sentiment.vader import SentimentIntensityAnalyzernltk.download('vader_lexicon')sid = SentimentIntensityAnalyzer()def get_vader_score(sent): # Polarity score returns dictionary ss = sid.polarity_scores(sent) #return ss return np.argmax(list(ss.values())[:-1]) &quot;&quot;&quot;[nltk_data] Downloading package vader_lexicon to[nltk_data] /Users/xx/nltk_data...&quot;&quot;&quot;df['polarity']=df['headline_text'].\\ map(lambda x: get_vader_score(x))polarity=df['polarity'].replace({0:'neg',1:'neu',2:'pos'})plt.bar(polarity.value_counts().index, polarity.value_counts()) image-20210831000924225 Yes, the distribution is slightly different. There are even more headlines classified as neutral 85%, and the number of negative news headlines has increased (to 13%). Named Entity Recognition Named entity recognition is an information extraction method in which entities existing in the text are classified into predefined entity types, such as \"person\", \"location\", \"organization\" and so on. By using NER, we can gain insight into the entities that exist in a given text data set of entity types. Let us consider an example of a news article. In the above news, the named entity recognition model should be able to recognize Entities, such as RBI as an organization, Mumbai and India as Places, etc. There are three standard libraries for named entity recognition: Stanford Nell space NLTK I will use spaCy, which is an open source library for advanced natural language processing tasks. It is written in Cython and is known for its industrial applications. In addition to NER, spaCy also provides many other functions, such as pos mark, word to vector conversion, etc. SpaCy’s Named Entity Recognition has been published in OntoNotes 5 has been trained on the corpus and supports the following entity types There are three kinds of pre-trained models for English in SpaCy. I will use en_core_web_sm to complete our task, but you can try other models. To use it, we must first download it: 12345678910111213141516171819202122232425# !python -m spacy download en_core_web_sm# Now we can initialize the language model:import spacyfrom spacy import displacyimport en_core_web_smnlp = en_core_web_sm.load()# nlp = spacy.load(&quot;en_core_web_sm&quot;)# One of the advantages of Spacy is that we only need to apply the nlp function once, and the entire background pipeline will return the objects we needdoc=nlp('India and Iran have agreed to boost the economic \\viability of the strategic Chabahar port through various measures, \\including larger subsidies to merchant shipping firms using the facility, \\people familiar with the development said on Thursday.')[(x.text,x.label_) for x in doc.ents] &quot;&quot;&quot;[('India', 'GPE'), ('Iran', 'GPE'), ('Chabahar', 'GPE'), ('Thursday', 'DATE')]&quot;&quot;&quot; We can see that India and Iran are confirmed as geographic locations (GPE), Chabahar is confirmed as a person, and Thursday is confirmed as a date. We can also use the display module in spaCy to visualize the output. 123from spacy import displacydisplacy.render(doc, style='ent') image-20210831001008590 This can make sentences with recognized entities look very neat, and each entity type is marked with a different color. Now that we know how to perform NER, we can further explore the data by performing various visualizations on the named entities extracted from the data set. First, we will run named entity recognition on news headlines and store entity types. NER Analysis 123456789101112def ner(text): doc=nlp(text) return [X.label_ for X in doc.ents] ent=df['headline_text'].apply(lambda x : ner(x))ent=[x for sub in ent for x in sub]counter=Counter(ent)count=counter.most_common()# Now, we can visualize the entity frequency:x,y=map(list,zip(*count))sns.barplot(x=y,y=x) image-20210831001044045 Now we can see that GPE and ORG dominate the headlines, followed by the PERSON entity. We can also visualize the most common tokens for each entity. Let's check which places appear the most in news headlines. Most common GPE 12345678910def ner(text,ent=&quot;GPE&quot;): doc=nlp(text) return [X.text for X in doc.ents if X.label_ == ent] gpe=df['headline_text'].apply(lambda x: ner(x,&quot;GPE&quot;))gpe=[i for x in gpe for i in x]counter=Counter(gpe)x,y=map(list,zip(*counter.most_common(10)))sns.barplot(y,x) image-20210831001111535 I think we can confirm the fact that \"America\" means America in news headlines. Let's also find the most common names that appear on news headlines. Most common person 123456per=df['headline_text'].apply(lambda x: ner(x,&quot;PERSON&quot;))per=[i for x in per for i in x]counter=Counter(per)x,y=map(list,zip(*counter.most_common(10)))sns.barplot(y,x) image-20210831001135765 Saddam Hussein and George Bush served as presidents of Iraq and the United States during the war. In addition, we can see that the model is far from perfect to classify \"vic govt\" or \"nsw govt\" as individuals rather than government agencies. Pos tagging Use nltk for all parts of speech markup, but there are other libraries that can do the job well (spaacy, textblob). 123456789101112import nltknltk.download('averaged_perceptron_tagger')sentence=&quot;The greatest comeback stories in 2019&quot;tokens=word_tokenize(sentence)nltk.pos_tag(tokens)# Notice:# You can also use the spacy.displacy module to visualize the sentence part of the speech and its dependency graph.doc = nlp('The greatest comeback stories in 2019')displacy.render(doc, style='dep', jupyter=True, options={'distance': 90}) image-20210831001212360 We can observe various dependency labels here. For example, the DET tag indicates the relationship between the word \"the\" and the noun \"stories\". You can check the list of dependency labels and their meanings here. Okay, now that we know what a POS tag is, let's use it to explore the title data set. Analysing pos tags 1234567891011def pos(text): pos=nltk.pos_tag(word_tokenize(text)) pos=list(map(list,zip(*pos)))[1] return pos tags=df['headline_text'].apply(lambda x : pos(x))tags=[x for l in tags for x in l]counter=Counter(tags)x,y=list(map(list,zip(*counter.most_common(7))))sns.barplot(x=y,y=x) image-20210831001251251 We can clearly see that nouns (NN) dominate in news headlines, followed by adjectives (JJ). This is typical for news reports, and for art forms, higher adjective (ADJ) frequencies may happen a lot. You can investigate this in more depth by investigating the most common singular nouns in news headlines. Let us find out. Nouns such as \"war\", \"Iraq\", and \"person\" dominate the news headlines. You can use the above functions to visualize and check other parts of the voice. Most common Nouns 123456789101112131415def get_adjs(text): adj=[] pos=nltk.pos_tag(word_tokenize(text)) for word,tag in pos: if tag=='NN': adj.append(word) return adjwords=df['headline_text'].apply(lambda x : get_adjs(x))words=[x for l in words for x in l]counter=Counter(words)x,y=list(map(list,zip(*counter.most_common(7))))sns.barplot(x=y,y=x) Dependency graph 12doc = nlp('She sells seashells by the seashore')displacy.render(doc, style='dep', jupyter=True, options={'distance': 90}) Text readability Textstat 12from textstat import flesch_reading_easedf['headline_text'].apply(lambda x : flesch_reading_ease(x)).hist() complex headlines? Almost all readability scores exceed 60. This means that an average of 11-year-old students can read and understand news headlines. Let's check all news headlines with a readability score below 5. 123456789101112131415x=[i for i in range(len(reading)) if reading[i]&lt;5] &quot;&quot;&quot;rror loading preloads:Failed to fetch dynamically imported module: https://file+.vscode-resource.vscode-webview.net/Users/xx/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/datascience-ui/errorRenderer/errorRenderer.js&quot;&quot;&quot;news.iloc[x]['headline_text'].head() &quot;&quot;&quot;Error loading preloads:Failed to fetch dynamically imported module: https://file+.vscode-resource.vscode-webview.net/Users/xx/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/datascience-ui/errorRenderer/errorRenderer.js&quot;&quot;&quot; Final thoughts In this article, we discussed and implemented various exploratory data analysis methods for text data. Some are common and little known, but all of them can be an excellent addition to your data exploration toolkit. Hope you will find some of them useful for your current and future projects. To make data exploration easier, I created a \"exploratory data analysis of natural language processing templates\", which you can use for your work. In addition, you may have seen that for each chart in this article, there is a code snippet to create it. Just click the button below the chart. Happy exploring! From: https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr","link":"/example_02/"},{"title":"A Preliminary Study of Machine Learning","text":"Gradient 123456789101112131415def loss(k): return 3 * (k ** 2) + 7 * k -10# -b / 2a = -7 / 6def partial(k): return 6 * k + 7k = ramdom.randint(-10, 10)alpha = 1e-3 # 0.001for i in range(1000): k = k + (-1) * partial(k) * alpha print(k, loss(k))","link":"/example_002/"},{"title":"Machine Learning Part-01","text":"Linear Regression Example Implement Linear Regression for Boston House Price Problem 123456789import randomimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsimport numpy as npfrom sklearn.datasets import load_bostonfrom matplotlib.animation import FuncAnimationimport re Part-01: Linear Regression 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200housing_price = load_boston()dataframe = pd.DataFrame(housing_price['data'])dataframe.columns = housing_price['feature_names']dataframe['price'] = housing_price['target']# sns.heatmap(dataframe.corr(), annot=True, fmt='.1f')# plt.show()print(dataframe.columns) &quot;&quot;&quot;Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'price'], dtype='object')&quot;&quot;&quot;rm = dataframe['RM']lst = dataframe['LSTAT']target = dataframe['price']def model(x, w, b): return np.dot(x, w.T) + bdef loss(yhat, y): return np.mean( (yhat - y) ** 2)def partial_w(x1, x2, y, yhat): return np.array([2 *np.mean((yhat - y) * x1), 2 * np.mean((yhat - y) * x2)])def partial_b(x1, x2, y, yhat): return 2 * np.mean((yhat - y))w = np.random.random_sample((1, 2))print(w)b = 0alpha = 1e-5epoch = 200history = []history_k_b_loss = [] &quot;&quot;&quot;[[0.76646144 0.3095512 ]]&quot;&quot;&quot;for e in range(epoch): losses = [] for batch in range(len(rm)): random_index = random.choice(range(len(rm))) x1, x2 = rm[random_index], lst[random_index] y = target[random_index] yhat = model(np.array([x1, x2]), w, b) loss_v = loss(yhat, y) w = w - partial_w(x1, x2, y, yhat) * alpha b = b - partial_b(x1, x2, y, yhat) * alpha losses.append(loss_v) history_k_b_loss.append((w, b, loss_v)) if batch % 100 == 0: print('Epoch: {}, Batch: {}, loss: {}'.format(e, batch, np.mean(losses))) history.append(np.mean(losses)) &quot;&quot;&quot;Epoch: 0, Batch: 0, loss: 151.86271856102778Epoch: 0, Batch: 100, loss: 263.5872813250959show more (open the raw output data in a text editor) ...Epoch: 199, Batch: 500, loss: 28.308274447364248&quot;&quot;&quot;````## Logstic Regression```pythonhousing_price = load_boston()dataframe = pd.DataFrame(housing_price['data'])dataframe.columns = housing_price['feature_names']dataframe['price'] = housing_price['target']rm = dataframe['RM']lst = dataframe['LSTAT']price = dataframe['price']print(np.percentile(price, 66)) &quot;&quot;&quot;23.53&quot;&quot;&quot;# plt.hist(target)# plt.show()dataframe['expensive'] = dataframe['price'].apply(lambda p: int(p &gt; np.percentile(price, 66)))expensive = dataframe['expensive']# print(dataframe.head())print(dataframe['expensive']) &quot;&quot;&quot;0 11 0 ..505 0Name: expensive, Length: 506, dtype: int64&quot;&quot;&quot;def logistic(x): return 1 / (1 + np.exp(-x))def model(x, w, b): return logistic(np.dot(x, w.T) + b)def loss(yhat, y): return -1 * np.sum(y*np.log(yhat) + (1 - y) * np.log(1 - yhat))def partial_w(x1, x2, y, yhat): return np.array([np.sum((yhat - y) * x1), np.sum((yhat - y) * x2)])def partial_b(x1, x2, y, yhat): return np.sum(yhat - y) w = np.random.random_sample((1, 2))print(w) &quot;&quot;&quot;[[0.69565948 0.90768813]]&quot;&quot;&quot;b = 0alpha = 1e-5epoch = 200history = []history_k_b_loss = []for e in range(epoch): losses = [] for batch in range(len(rm)): random_index = random.choice(range(len(rm))) x1, x2 = rm[random_index], lst[random_index] y = expensive[random_index] yhat = model(np.array([x1, x2]), w, b) loss_v = loss(yhat, y) w = w - partial_w(x1, x2, y, yhat) * alpha b = b - partial_b(x1, x2, y, yhat) * alpha losses.append(loss_v) history_k_b_loss.append((w, b, loss_v)) if batch % 100 == 0: print('Epoch: {}, Batch: {}, loss: {}'.format(e, batch, np.mean(losses))) history.append(np.mean(losses)) &quot;&quot;&quot;Epoch: 0, Batch: 0, loss: 3.14765267665445e-06Epoch: 0, Batch: 100, loss: 13.555508645878497show more (open the raw output data in a text editor) ...Epoch: 199, Batch: 500, loss: 0.31372698791846687&quot;&quot;&quot;predicated = [model(np.array([x1, x2]), w, b) for x1, x2 in zip(rm, lst)]true = expensivedef accuracy(y, yhat): return sum(1 if i == j else 0 for i, j in zip(y, yhat)) / len(y) print(accuracy(true, predicated)) &quot;&quot;&quot;0.0&quot;&quot;&quot; decision boundary Linear Regression: Regression is implemented, including the definition of linear functions, why use linear functions, the meaning of loss, the meaning of gradient descent, stochastic gradient descent Use Boston house price dataset. The data set of Beijing housing prices in 2020, why didn’t I use the data set of Beijing housing prices? Boston: room size, subway, highway, crime rate have a more obvious relationship, so it is easier to observe the relationship Beijing's housing prices:! Far and near! Room Condition ==》 School District! ! ! ! =&gt; Very expensive Haidian District 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import randomimport numpy as npimport pandas as pdfrom sklearn.datasets import load_bostondataset = load_boston()data = dataset['data']target = dataset['target']columns = dataset['feature_names']dataframe = pd.DataFrame(data)dataframe.columns = columnsdataframe['price'] = target# print(dataframe.corr()) # show the correlation of dataframe variables# correlation =&gt; If one value increases, it will cause another value to increase, and the correlation coefficient is closer to 1 if it increases in a certain proportion.# correlation =&gt; 0 means there is no relationship between the two# correlation =&gt; -1 One value increases, the other value must decrease, and the decrease is in equal proportion# sns.heatmap(dataframe.corr())# plt.show()# RM: The average number of bedrooms in the community# LSTAT: Percentage of low-income people aroundrm = dataframe['RM']lstat = dataframe['LSTAT']def linear(x, w, b): # vectorized model return np.dot(x, w.T) + bdef loss(yhat, y): # numpy broadcast numpy广播方法 return np.mean( (yhat - y) ** 2)def partial_w(x, y, yhat): return np.array([2 * np.mean((yhat - y) * x[0]), 2 * np.mean((yhat - y) * x[1])])def partial_b(x, y, yhat): return 2 * np.mean((yhat - y))def optimize(w, b, x, y, yhat, pw, pb, learning_rate): w = w + -1 * pw(x, y, yhat) * learning_rate b = b + -1 * pb(x, y, yhat) * learning_rate return w, b def train(model_to_be_train, target, loss, pw, pb): w = np.random.random_sample((1, 2)) # w normal b = np.random.random() # 0 深度学习的时候会和大家详细解释 learning_rate = 1e-5 epoch = 200 losses = [] for i in range(epoch): batch_loss = [] for batch in range(len(rm)): # batch training index = random.choice(range(len(rm))) rm_x, lstat_x = rm[index], lstat[index] x = np.array([rm_x, lstat_x]) y = target[index] yhat = model_to_be_train(x, w, b) loss_v = loss(yhat, y) batch_loss.append(loss_v) w, b = optimize(w, b, x, y, yhat, pw, pb, learning_rate) if batch % 100 == 0: print('Epoch: {} Batch: {}, loss: {}'.format(i, batch, loss_v)) losses.append(np.mean(batch_loss)) return model_to_be_train, w, b, losses if __name__ == &quot;__main__&quot;: import matplotlib.pyplot as plt target = dataframe['price'] model, w, b, losses = train(linear, target, loss, partial_w, partial_b) plt.plot(losses) predicate = model(np.array([19, 7]), w, b) print(predicate) plt.show() &quot;&quot;&quot;Epoch: 0 Batch: 0, loss: 165.0318036522631Epoch: 0 Batch: 100, loss: 1936.2111196826459show more (open the raw output data in a text editor) ...Epoch: 199 Batch: 500, loss: 0.024829543832110872[88.74340551]&quot;&quot;&quot; image-20210831104443642 Logstic Regression Linear Regression: Regression is implemented, including the definition of linear functions, why use linear functions, the meaning of loss, the meaning of gradient descent, stochastic gradient descent Use Boston house price dataset. The data set of Beijing housing prices in 2020, why didn’t I use the data set of Beijing housing prices? Boston: room size, subway, highway, crime rate have a more obvious relationship, so it is easier to observe the relationship Beijing's housing prices:! Far and near! Room Condition ==》 School District! ! ! ! =&gt; Very expensive Haidian District Harder than deep learning: 1. compiler 2. programming language &amp; automata 3. computer graphic 4. complexity system 5. computing complexity 6. operating system 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130from sklearn.datasets import load_bostonimport pandas as pdimport numpy as npdataset = load_boston()data = dataset['data']target = dataset['target']columns = dataset['feature_names']dataframe = pd.DataFrame(data)dataframe.columns = columnsdataframe['price'] = target# print(dataframe.corr()) # show the correlation of dataframe variables# correlation =&gt; If one value increases, it will cause another value to increase, and the correlation coefficient is closer to 1 if it increases in a certain proportion.# correlation =&gt; 0 means there is no relationship between the two# correlation =&gt; -1 One value increases, the other value must decrease, and the decrease is in equal proportion# sns.heatmap(dataframe.corr())# plt.show()# RM: The average number of bedrooms in the community# LSTAT: Percentage of low-income people aroundrm = dataframe['RM']lstat = dataframe['LSTAT']price = dataframe['price']greater_then_most = np.percentile(price, 66)dataframe['expensive'] = dataframe['price'].apply(lambda p: int(p&gt; greater_then_most))target = dataframe['expensive']print(dataframe[:20]) &quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX \\0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 5 0.02985 0.0 2.18 0.0 0.458 6.430 58.7 6.0622 3.0 222.0 6 0.08829 12.5 7.87 0.0 0.524 6.012 66.6 5.5605 5.0 311.0 7 0.14455 12.5 7.87 0.0 0.524 6.172 96.1 5.9505 5.0 311.0 8 0.21124 12.5 7.87 0.0 0.524 5.631 100.0 6.0821 5.0 311.0 9 0.17004 12.5 7.87 0.0 0.524 6.004 85.9 6.5921 5.0 311.0 10 0.22489 12.5 7.87 0.0 0.524 6.377 94.3 6.3467 5.0 311.0 11 0.11747 12.5 7.87 0.0 0.524 6.009 82.9 6.2267 5.0 311.0 12 0.09378 12.5 7.87 0.0 0.524 5.889 39.0 5.4509 5.0 311.0 13 0.62976 0.0 8.14 0.0 0.538 5.949 61.8 4.7075 4.0 307.0 14 0.63796 0.0 8.14 0.0 0.538 6.096 84.5 4.4619 4.0 307.0 15 0.62739 0.0 8.14 0.0 0.538 5.834 56.5 4.4986 4.0 307.0 16 1.05393 0.0 8.14 0.0 0.538 5.935 29.3 4.4986 4.0 307.0 17 0.78420 0.0 8.14 0.0 0.538 5.990 81.7 4.2579 4.0 307.0 18 0.80271 0.0 8.14 0.0 0.538 5.456 36.6 3.7965 4.0 307.0 19 0.72580 0.0 8.14 0.0 0.538 5.727 69.5 3.7965 4.0 307.0 PTRATIO B LSTAT price expensive 0 15.3 396.90 4.98 24.0 1 1 17.8 396.90 9.14 21.6 0 2 17.8 392.83 4.03 34.7 1 3 18.7 394.63 2.94 33.4 1 4 18.7 396.90 5.33 36.2 1 5 18.7 394.12 5.21 28.7 1 6 15.2 395.60 12.43 22.9 0 7 15.2 396.90 19.15 27.1 1 8 15.2 386.63 29.93 16.5 0 9 15.2 386.71 17.10 18.9 0 10 15.2 392.52 20.45 15.0 0 11 15.2 396.90 13.27 18.9 0 12 15.2 390.50 15.71 21.7 0 13 21.0 396.90 8.26 20.4 0 14 21.0 380.02 10.26 18.2 0 15 21.0 395.62 8.47 19.9 0 16 21.0 386.85 6.58 23.1 0 17 21.0 386.75 14.67 17.5 0 18 21.0 288.99 11.69 20.2 0 19 21.0 390.95 11.28 18.2 0 &quot;&quot;&quot;def sigmoid(x): return 1 / (1 + np.exp(-x))def model(x, w, b): return sigmoid(np.dot(x, w.T) + b)def loss(yhat, y): return -np.sum(y*np.log(yhat) + (1 - y)*np.log(1 - yhat))def partial_w(x, y, yhat): return np.array([np.sum((yhat - y) * x[0]), np.sum((yhat - y) * x[1])])def partial_b(x, y, yhat): return np.sum((yhat - y)) model, w, b, losses = train(model, target,loss, partial_w, partial_b)random_test_indices = np.random.choice(range(len(rm)), size=100)decision_boundary = 0.5 &quot;&quot;&quot;Epoch: 0 Batch: 0, loss: 5.380792320433632Epoch: 0 Batch: 100, loss: 4.821708458450062show more (open the raw output data in a text editor) ...Epoch: 199 Batch: 500, loss: 0.052809537616594626&quot;&quot;&quot;for i in random_test_indices: x1, x2, y = rm[i], lstat[i], target[i] predicate = model(np.array([x1, x2]), w, b) predicate_label = int(predicate &gt; decision_boundary) print('RM: {}, LSTAT: {}, EXPENSIVE: {}, Predicated: {}'.format(x1, x2, y, predicate_label)) &quot;&quot;&quot;RM: 5.701, LSTAT: 18.35, EXPENSIVE: 0, Predicated: 0RM: 4.973, LSTAT: 12.64, EXPENSIVE: 0, Predicated: 0show more (open the raw output data in a text editor) ...RM: 6.678, LSTAT: 6.27, EXPENSIVE: 1, Predicated: 1&quot;&quot;&quot; One thing left is to check the accuracy of our model! ! How to measure the quality of the model: 1. accuracy precision recall f1, f2 score AUC-ROC curve Introduce a very very important concept: -&gt; over-fitting and under-fitting (over-fitting and under-fitting) The entire machine learning process is constantly adjusting over-fitting and under-fitting!","link":"/example_03/"},{"title":"Machine Learning Part-02","text":"Data Pre-processing Feature-Extractor Split Training, Test, Validation Build Model Gradient Descent Evaluation Predicat Analysis House Price Regression 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129## load datafrom sklearn.datasets import load_boston## ususlly will load in csvdata = load_boston()print(data['DESCR'])&quot;&quot;&quot;_boston_dataset:Boston house prices dataset---------------------------**Data Set Characteristics:** :Number of Instances: 506 show more (open the raw output data in a text editor) ...Morgan Kaufmann.&quot;&quot;&quot;import pandas as pdimport numpy as npdf = pd.DataFrame(data['data'])df.columns = data['feature_names']df[df['CHAS'] == 1]&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT142 3.32105 0.0 19.58 1.0 0.8710 5.403 100.0 1.3216 5.0 403.0 14.7 396.90 26.82... 1.1296 24.0 666.0 20.2 347.88 8.88&quot;&quot;&quot;## Pre-processingdf.std()&quot;&quot;&quot;CRIM 8.601545ZN 23.322453INDUS 6.860353CHAS 0.253994NOX 0.115878RM 0.702617AGE 28.148861DIS 2.105710RAD 8.707259TAX 168.537116PTRATIO 2.164946B 91.294864LSTAT 7.141062dtype: float64&quot;&quot;&quot;df['CHAS'] = df['CHAS'].astype('int')df['CHAS'] = df['CHAS'].astype('category')df['RAD'] = df['RAD'].astype('int')df['RAD'] = df['RAD'].astype('category')df&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98...505 0.04741 0.0 11.93 0 0.573 6.030 80.8 2.5050 1 273.0 21.0 396.90 7.88506 rows × 13 columns&quot;&quot;&quot;df['RAD']&quot;&quot;&quot;0 11 22 2...505 1Name: RAD, Length: 506, dtype: categoryCategories (9, int64): [1, 2, 3, 4, ..., 6, 7, 8, 24]&quot;&quot;&quot;from sklearn.preprocessing import OneHotEncoderonehoter = OneHotEncoder()chas_and_rad_vec = onehoter.fit_transform(df[['CHAS', 'RAD']])## Standarlizefrom sklearn.preprocessing import StandardScalerss = StandardScaler()df.shape&quot;&quot;&quot;(506, 13)&quot;&quot;&quot;real_vec = ss.fit_transform(df.drop(columns = ['CHAS', 'RAD']))chas_and_rad_vec[0].toarray()&quot;&quot;&quot;array([[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])&quot;&quot;&quot;import numpy as npnp.mean(real_vec, axis = 0)&quot;&quot;&quot;array([-1.12338772e-16, 7.89881994e-17, 2.10635198e-16, -1.96592852e-16, -1.08828186e-16, -1.47444639e-16, -8.42540793e-17, 0.00000000e+00, -4.21270397e-16, -7.44244367e-16, -3.08931624e-16])&quot;&quot;&quot;np.std(real_vec, axis = 0)&quot;&quot;&quot;array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])&quot;&quot;&quot;real_vec.shape&quot;&quot;&quot;(506, 11)&quot;&quot;&quot;chas_and_rad_vec.shape&quot;&quot;&quot;(506, 11)&quot;&quot;&quot;## Feature-ExtractorX = np.concatenate((real_vec, chas_and_rad_vec.toarray()), axis = 1)y = data['target']## Split Training, Test, Validationdef split_train_val_test(X, y, test_ratio = 0.2, val_ratio = 0.2): indices = np.random.choice(range(len(X)), size = len(X), replace=False) train_indices = indices[:int(len(X) * (1-test_ratio) * (1 - val_ratio))] val_indices = indices[int(len(X)*(1-test_ratio) * (1-val_ratio)): int(len(X) * (1-test_ratio))] test_indices = indices[int(len(X) * (1-test_ratio)):] return (X[train_indices], y[train_indices]), (X[val_indices], y[val_indices]), (X[test_indices], y[test_indices])(X_train, y_train), (X_val, y_val), (X_test, y_test) = split_train_val_test(X, y) sklearn.model_selection.train_test_split also could be used Build-Model 1234567from sklearn.linear_model import LinearRegressionregression = LinearRegression()regression.fit(X_train, y_train)&quot;&quot;&quot;LinearRegression()&quot;&quot;&quot; Question: If overfittiing or underfitting? Explain: Why validation set is more useful in deep learning Gradient Descent Evaluation 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152regression.score(X_train, y_train)&quot;&quot;&quot;0.7477980609064946&quot;&quot;&quot;regression.score(X_val, y_val)&quot;&quot;&quot;0.7611715890963341&quot;&quot;&quot;regression.score(X_test, y_test)&quot;&quot;&quot;0.711869928554872&quot;&quot;&quot;## Interpreterregression.coef_&quot;&quot;&quot;array([-1.04208922, 1.30263494, 0.29143618, -2.31827512, 2.40383155, 0.25013857, -3.55953868, -1.68823412, -2.37743843, 0.74411049, -3.79489254, -0.79143926, 0.79143926, -2.51995654, -2.20671004, 0.65594998, -0.31683083, -0.07929752, -2.15244627, -0.06686364, 1.93167854, 4.75447632])&quot;&quot;&quot;regression.intercept_&quot;&quot;&quot;22.070279554739386&quot;&quot;&quot;### PredictX_test[0]&quot;&quot;&quot;array([ 1.68404594, -0.48772236, 1.01599907, 1.07378711, 0.21279502, 1.11749449, -0.93188642, 1.53092646, 0.80657583, -3.61192313, 2.29842066, 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. ])&quot;&quot;&quot;regression.predict([X_test[0]])&quot;&quot;&quot;array([9.64589284])&quot;&quot;&quot;import matplotlib.pyplot as pltfor i in range(5): plt.scatter(X[:, 5], y) plt.scatter(X[:, 5], regression.predict(X))plt.show() image-20210831230756185 1234567891011121314151617import matplotlibmatplotlib.colors%matplotlib inlinedef show_predication_result(x, target): width = 3 fig,ax = plt.subplots(x.shape[1]//width + 1, width, figsize = (40,40)) for i in range(x.shape[1]): ix = np.unravel_index(i, ax.shape) plt.sca(ax[ix]) ax[ix].title.set_text('Feature-{}'.format(i)) plt.scatter(x[:, i], target) plt.scatter(x[:, i], regression.predict(x)) show_predication_result(X_train, y_train) image-20210831230855270 1show_predication_result(X_val, y_val) image-20210831230921925 1show_predication_result(X_test, y_test) image-20210831230945492 Outliers Part-02 Logstic Regression Data Pre-processing Feature-Extractor Split Training, Test, Validation Build Model Gradient Descent Evaluation Predicat Analysis Pre-processing 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom struct import unpackdef loadmnist(imagefile, labelfile): # Open the images with gzip in read binary mode images = open(imagefile, 'rb') labels = open(labelfile, 'rb') # Get metadata for images images.read(4) # skip the magic_number number_of_images = images.read(4) number_of_images = unpack('&gt;I', number_of_images)[0] rows = images.read(4) rows = unpack('&gt;I', rows)[0] cols = images.read(4) cols = unpack('&gt;I', cols)[0] # Get metadata for labels labels.read(4) N = labels.read(4) N = unpack('&gt;I', N)[0] # Get data x = np.zeros((N, rows*cols), dtype = np.uint8) #Initialize numpy array y = np.zeros(N, dtype = np.uint8) # Initialize numpy array for i in range(N): for j in range(rows*cols): tmp_pixel = images.read(1) # Just a single byte tmp_pixel = unpack('&gt;B', tmp_pixel)[0] x[i][j] = tmp_pixel tmp_label = labels.read(1) y[i] = unpack('&gt;B', tmp_label)[0] images.close() labels.close() return (x, y) X_train, y_train = loadmnist('~/data/course_data/t10k-images-idx3-ubyte','~/data/course_data/t10k-labels-idx1-ubyte') X_test, y_test = loadmnist('~/data/course_data/train-images-idx3-ubyte','~/data/course_data/train-labels-idx1-ubyte') X_train.shape &quot;&quot;&quot; (10000, 784) &quot;&quot;&quot; X_test &quot;&quot;&quot; array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]], dtype=uint8) &quot;&quot;&quot; y_test &quot;&quot;&quot; array([5, 0, 4, ..., 5, 6, 8], dtype=uint8) &quot;&quot;&quot; plt.figure(figsize = (20, 4))for index, (image, label) in enumerate(zip(X_train[0:5], y_train[0:5])): plt.subplot(1, 5, index+1) plt.imshow(np.reshape(image, (28, 28))) plt.title('Traininng: %i\\n' % label, fontsize = 20) image-20210831231306773 We only choose label with 0 and 6 1234567891011121314151617181920212223242526272829303132333435zero_train_indices = np.where(y_train == 0)one_train_indices = np.where(y_train == 6)train_indices = np.concatenate((zero_train_indices[0], one_train_indices[0]))zero_test_indices = np.where(y_test == 0)one_test_indices = np.where(y_test == 6)test_indices = np.concatenate((zero_test_indices[0], one_test_indices[0]))train_indices = np.random.choice(train_indices, size = len(train_indices), replace=False)test_indices = np.random.choice(test_indices, size = len(test_indices), replace=False)val_ratio= 0.2train_indices = train_indices[: int(len(train_indices) * (1 - val_ratio))]val_indices = train_indices[int(len(train_indices) * (1 - val_ratio)):]binary_x_train = X_train[train_indices]binary_x_test = X_test[test_indices]binary_x_val = X_train[val_indices]binary_y_train = y_train[train_indices]binary_y_test = y_test[test_indices]binary_y_val = y_train[val_indices]import randombinary_y_train&quot;&quot;&quot;array([6, 0, 0, ..., 6, 0, 0], dtype=uint8)&quot;&quot;&quot;plt.imshow(np.reshape(binary_x_train[1], (28,28)))plt.title('Training: %i\\n' % binary_y_train[1], fontsize =20)&quot;&quot;&quot;Text(0.5, 1.0, 'Training: 0\\n')&quot;&quot;&quot; image-20210831231410866 123456789101112131415from collections import CounterCounter(binary_y_train)&quot;&quot;&quot;Counter({6: 768, 0: 782})&quot;&quot;&quot;Counter(binary_y_test)&quot;&quot;&quot;Counter({6: 5918, 0: 5923})&quot;&quot;&quot;Counter(binary_y_val)&quot;&quot;&quot;Counter({0: 148, 6: 162})&quot;&quot;&quot; Build model 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from sklearn.linear_model import LogisticRegressionclf = LogisticRegression(random_state = 0, solver = 'lbfgs')# L-BFGS-B - Software for Large-scale Bound-constrained Optimizationimport warningswarnings.filterwarnings('ignore')clf.fit(binary_x_train, binary_y_train)&quot;&quot;&quot;LogisticRegression(random_state=0)&quot;&quot;&quot;clf.coef_&quot;&quot;&quot;array([[ 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,show more (open the raw output data in a text editor) ... 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])&quot;&quot;&quot;clf.intercept_&quot;&quot;&quot;array([0.00016519])&quot;&quot;&quot;clf.score&quot;&quot;&quot;&lt;bound method ClassifierMixin.score of LogisticRegression(random_state=0)&gt;&quot;&quot;&quot;clf.score(binary_x_train, binary_y_train)&quot;&quot;&quot;1.0&quot;&quot;&quot;clf.score(binary_x_val, binary_y_val)&quot;&quot;&quot;1.0&quot;&quot;&quot;binary_x_test.shape&quot;&quot;&quot;(11841, 784)&quot;&quot;&quot;binary_y_test.shape&quot;&quot;&quot;0.9865720800608057&quot;&quot;&quot;predicated_result = clf.predict(binary_x_test)np.where(binary_y_test != predicated_result)&quot;&quot;&quot;(array([ 17, 45, 66, 137, 260, 279, 323, 453, 529, 739, 753, 947, 1034, 1248, 1290, 1422, 1434, 1444, ... 10677, 10739, 10750, 10979, 11010, 11058, 11104, 11113, 11366, 11389, 11421, 11458, 11528, 11659, 11760]),)&quot;&quot;&quot;lookup_index = 1184plt.imshow(np.reshape(binary_x_test[lookup_index], (28, 28)))plt.title('Actual Value: {} ; Predict Value: {} \\n'.format(binary_y_test[lookup_index], predicated_result[lookup_index]), fontsize = 20)&quot;&quot;&quot;Text(0.5, 1.0, 'Actual Value: 6 ; Predict Value: 6 \\n')&quot;&quot;&quot; image-20210831231740052 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from sklearn import metricsbinary_y_test[0]&quot;&quot;&quot;6&quot;&quot;&quot;predicated_result[0]&quot;&quot;&quot;6&quot;&quot;&quot;metrics.precision_score(binary_y_test, predicated_result, average = 'macro')&quot;&quot;&quot;0.9865879016517065&quot;&quot;&quot;metrics.precision_score(binary_y_test, predicated_result, pos_label = 6)&quot;&quot;&quot;0.9837056946077608&quot;&quot;&quot;metrics.recall_score(binary_y_test, predicated_result, pos_label = 6)&quot;&quot;&quot;0.9895234876647516&quot;&quot;&quot;fpr, tpr, threshold = metrics.roc_curve(binary_y_test, predicated_result, pos_label = 6)metrics.auc(fpr, tpr)&quot;&quot;&quot;0.9865733258009728&quot;&quot;&quot;cm = metrics.confusion_matrix(binary_y_test, predicated_result)import seaborn as snsfrom sklearn.metrics import confusion_matrixdata = confusion_matrix(binary_y_test, predicated_result)data&quot;&quot;&quot;array([[5826, 97], [ 62, 5856]])&quot;&quot;&quot;df_cm = pd.DataFrame(data, columns = np.unique(binary_y_test), index = np.unique(binary_y_test))plt.figure(figsize = (10, 7))sns.set(font_scale=1.4) # for label sizesns.heatmap(df_cm, cmap='Blues', annot=True, annot_kws = {'size': 16}) # font size&quot;&quot;&quot;&lt;AxesSubplot:&gt;&quot;&quot;&quot; image-20210831232005063 1234567df_cm.index.name = 'Actual'df_cm.columns.name = 'Predicted'plt.figure(figsize = (10, 10))sns.heatmap(df_cm, cmap='Blues', annot=True, annot_kws={'size': 16})&quot;&quot;&quot;&lt;AxesSubplot:xlabel='Predicted', ylabel='Actual'&gt;&quot;&quot;&quot; image-20210831232124003 Boston code reproduction and reference answers 123456789101112131415161718192021222324252627282930313233# Import package# Used to load the Boston housing price data setfrom sklearn.datasets import load_boston# pandas toolkit For students who are new to pandas, please refer to the official 10-minute tutorial: https://pandas.pydata.org/pandas-docs/stable/10min.htmlimport pandas as pd# seaborn for drawingimport seaborn as snsimport numpy as np # numpy# Show drawing%matplotlib inlinedata = load_boston()data.keys()&quot;&quot;&quot;dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])&quot;&quot;&quot;df = pd.DataFrame(data['data'])df.head()&quot;&quot;&quot; 0 1 2 3 4 5 6 7 8 9 10 11 120 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.981 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.142 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.033 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.944 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33&quot;&quot;&quot;data['feature_names']&quot;&quot;&quot;array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='&lt;U7')&quot;&quot;&quot; Field meaning 名称 中文描述 CRIM 住房所在城镇的人均犯罪率 ZN 住房用地超过 25000 平方尺的比例 INDUS 住房所在城镇非零售商用土地的比例 CHAS 有关查理斯河的虚拟变量（如果住房位于河边则为1,否则为0 ） NOX 一氧化氮浓度 RM 每处住房的平均房间数 AGE 建于 1940 年之前的业主自住房比例 DIS 住房距离波士顿五大中心区域的加权距离 RAD 离住房最近的公路入口编号 TAX 每 10000 美元的全额财产税金额 PTRATIO 住房所在城镇的师生比例 B 1000(Bk-0.63)^2,其中 Bk 指代城镇中黑人的比例 LSTAT 弱势群体人口所占比例 MEDV 业主自住房的中位数房价（以千美元计） 1234567891011121314151617181920df.columns = data['feature_names']df.head()&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.981 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.142 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.033 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.944 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33&quot;&quot;&quot;df['price'] = data['target']df.head(2)&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT price0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.9 4.98 24.01 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.9 9.14 21.6&quot;&quot;&quot;sns.heatmap(df.corr(), annot=True, fmt='.1f') image-20210831233019474 12345import matplotlib.pyplot as pltplt.scatter(df['RM'], df['price'])&quot;&quot;&quot;&lt;matplotlib.collections.PathCollection at 0x7fe0f984f810&gt;&quot;&quot;&quot; image-20210831233046720 12345678910111213plt.figure(figsize = (20, 5))features = ['LSTAT', 'RM']target = df['price']for i, col in enumerate(features): plt.subplot(1, len(features), i+1) x = df[col] y = target plt.scatter(x, y, marker = 'o') plt.title('{} vs price'.format(col)) plt.xlabel(col) plt.ylabel('price') image-20210831233103355 1234567891011121314151617181920x = df['RM']y = df['price']history_notes = {_x: _y for _x, _y in zip(x, y)}history_notes[6.575]&quot;&quot;&quot;24.0&quot;&quot;&quot;# Find the top three prices closest to RM:6.57,similary_ys = [y for _, y in sorted(history_notes.items(), key=lambda x_y: (x_y[0]-6.57) ** 2)[:3]]similary_ys&quot;&quot;&quot;[23.8, 24.0, 24.8]&quot;&quot;&quot;np.mean(similary_ys) # Calculate the average of three&quot;&quot;&quot;24.2&quot;&quot;&quot; Using historical data to predict data that has never been seen before, the most direct method K-Neighbor-Nearst 1234567891011def knn(query_x, history, top_n=3): sorted_notes = sorted(history.items(), key=lambda x_y: (x_y[0] - query_x) ** 2) similar_notes = sorted_notes[:top_n] similar_ys = [y for _, y in similar_notes] return np.mean(similar_ys)knn(5.4, history_notes)&quot;&quot;&quot;15.700000000000001&quot;&quot;&quot; In order to obtain results faster, we hope to obtain predictive power by fitting a function \\[ f(rm) = k * rm + b \\] Random Approach \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} (\\hat{y_i} - y_i) ^ 2 \\] \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] 12345678910111213141516171819202122232425262728293031323334def loss(yhat, y): return np.mean((yhat - y) **2)import randommin_loss = float('inf')best_k, bes_b = None, Noneprint(min_loss)min_loss = float('inf')best_k, bes_b = None, Nonefor step in range(1000): min_v, max_v = -100, 100 k, b = random.randrange(min_v, max_v), random.randrange(min_v, max_v) y_hats = [k * rm_i + b for rm_i in x] current_loss = loss(y_hats, y) if current_loss &lt;min_loss: min_loss = current_loss best_k, best_b = k, b print('In step {}, we have obtained the function f(rm) = {} * rm + {}, at this time loss is: {}'.format(step, k, b, current_loss))&quot;&quot;&quot;In step 0, we have obtained the function f(rm) = 14 * rm + -78, at this time loss is: 212.87040239525695In step 70, we have obtained the function f(rm) = 10 * rm + -47, at this time loss is: 88.70654683794466In step 256, we have obtained the function f(rm) = 13 * rm + -55, at this time loss is: 68.45390542094862In step 526, we have obtained the function f(rm) = 10 * rm + -37, at this time loss is: 54.977297826086954&quot;&quot;&quot;plt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x])&quot;&quot;&quot;&lt;matplotlib.collections.PathCollection at 0x7fe0980f37d0&gt;&quot;&quot;&quot; image-20210831233425089 Monte Carlo simulation Supervisor \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{k}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i) * rm_i \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{b}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i)\\] 12345678910111213141516171819202122232425262728293031323334def partial_k(k, b, x, y): return 2 * np.mean((k*x+b-y) *x)def partial_b(k, b, x, y): return 2*np.mean(k*x+b-y) k, b = random.random(), random.random()min_loss = float('inf')best_k, best_b = None, Nonelearning_rate = 1e-2for step in range(2000): k,b = k+(-1*partial_k(k,b,x,y) * learning_rate), b+(-1*partial_b(k,b,x,y) * learning_rate) y_hats = k * x +b current_loss = loss(y_hats, y) if current_loss &lt; min_loss: min_loss = current_loss best_k, best_b = k, b print('On the {} step, we have func f(rm) = {} * rm + {}, loss is {} now'.format(step, k, b, current_loss))&quot;&quot;&quot;On the 0 step, we have func f(rm) = 6.968714597804018 * rm + -21.099847342593957, loss is 45.86961514375004 nowOn the 1 step, we have func f(rm) = 6.9692276199804555 * rm + -21.103110737199852, loss is 45.86852398135223 nowshow more (open the raw output data in a text editor) ...On the 1999 step, we have func f(rm) = 7.783005326604901 * rm + -26.279646762684518, loss is 44.468037178267025 now&quot;&quot;&quot;best_k, best_b&quot;&quot;&quot;(10, -37)&quot;&quot;&quot;plt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x]) image-20210831233620249 Supervised Learning We turn the forecast of housing prices into a more responsible and sophisticated model. What should we do? \\[ f(x) = k * x + b \\] \\[ f(x) = k2 * \\sigma(k_1 * x + b_1) + b2 \\] \\[ \\sigma(x) = \\frac{1}{1 + e^(-x)} \\] 1234def sigmoid(x): return 1 / (1 + np.exp(-x))sub_x = np.linspace(-10, 10)plt.plot(sub_x, sigmoid(sub_x)) image-20210831233717648 12345678910def random_linear(x): k, b = random.random(), random.random() return k * x + bdef complex_function(x): return (random_linear(x))for _ in range(10): index = random.randrange(0, len(sub_x)) sub_x_1, sub_x_2 = sub_x[:index], sub_x[index:] new_y = np.concatenate((complex_function(sub_x_1), complex_function(sub_x_2))) plt.plot(sub_x, new_y) image-20210831233740329 We can implement more complex functions through simple, basic modules and repeated superposition For more and more complex functions? How does the computer seek guidance? What is machine learning? The shortcomings of the KNN method, what is the background of the proposed linear fitting How to obtain faster function weight update through supervision method The combination of nonlinear and linear functions can fit very complex functions Deep learning we can fit more complex functions through basic function modules Assigment \\[ L2-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{(\\hat{y} - y)}^2 \\] \\[ L1-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{|(\\hat{y} - y)|} \\] Change L2-Loss in the code to L1Loss and implement gradient descent Realize L1Loss gradient descent from 0 1 Import package 12import numpy as npimport pandas as pd 2 Load data set 123456789101112131415161718192021222324252627282930313233from sklearn.datasets import load_bostonboston = load_boston()boston.keys()&quot;&quot;&quot;dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])&quot;&quot;&quot;X = boston.datay = boston.targetdf = pd.DataFrame(boston.data, columns = boston.feature_names)df.head()&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.981 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.142 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.033 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.944 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33&quot;&quot;&quot;df.describe() # Data description, you can view the statistics of each variable&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTATcount 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000mean 3.613524 11.363636 11.136779 0.069170 0.554695 6.284634 68.574901 3.795043 9.549407 408.237154 18.455534 356.674032 12.653063std 8.601545 23.322453 6.860353 0.253994 0.115878 0.702617 28.148861 2.105710 8.707259 168.537116 2.164946 91.294864 7.141062min 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 187.000000 12.600000 0.320000 1.73000025% 0.082045 0.000000 5.190000 0.000000 0.449000 5.885500 45.025000 2.100175 4.000000 279.000000 17.400000 375.377500 6.95000050% 0.256510 0.000000 9.690000 0.000000 0.538000 6.208500 77.500000 3.207450 5.000000 330.000000 19.050000 391.440000 11.36000075% 3.677083 12.500000 18.100000 0.000000 0.624000 6.623500 94.075000 5.188425 24.000000 666.000000 20.200000 396.225000 16.955000max 88.976200 100.000000 27.740000 1.000000 0.871000 8.780000 100.000000 12.126500 24.000000 711.000000 22.000000 396.900000 37.970000&quot;&quot;&quot; 3 Data preprocessing Normalization or standardization can prevent a certain dimension or a few dimensions from affecting the data too much when there are very many dimensions, and secondly, the program can run faster. There are many methods, such as standardization, min-max, z-score, p-norm, etc. How to use it depends on the characteristics of the data set. Extended reading-the deep learning field of the myth of data standardization 12345678910111213141516171819from sklearn.preprocessing import StandardScalerss = StandardScaler() # z = (x-u) / s u is the mean, s is the standard deviationX = ss.fit_transform(df) # For linear models, normalization or standardization is generally required, otherwise there will be a gradient explosion, which is generally not required for tree modelsdf = pd.DataFrame(X, columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX' ,'PTRATIO','B','LSTAT'])df.describe()&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTATcount 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02mean 2.808469e-17 6.599903e-16 -4.633974e-16 -4.353127e-16 1.404235e-16 -1.755293e-17 2.176564e-16 -1.685082e-16 -5.055245e-16 8.987102e-16 -1.067218e-15 4.493551e-16 -2.246775e-16std 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00min -4.197819e-01 -4.877224e-01 -1.557842e+00 -2.725986e-01 -1.465882e+00 -3.880249e+00 -2.335437e+00 -1.267069e+00 -9.828429e-01 -1.313990e+00 -2.707379e+00 -3.907193e+00 -1.531127e+0025% -4.109696e-01 -4.877224e-01 -8.676906e-01 -2.725986e-01 -9.130288e-01 -5.686303e-01 -8.374480e-01 -8.056878e-01 -6.379618e-01 -7.675760e-01 -4.880391e-01 2.050715e-01 -7.994200e-0150% -3.906665e-01 -4.877224e-01 -2.110985e-01 -2.725986e-01 -1.442174e-01 -1.084655e-01 3.173816e-01 -2.793234e-01 -5.230014e-01 -4.646726e-01 2.748590e-01 3.811865e-01 -1.812536e-0175% 7.396560e-03 4.877224e-02 1.015999e+00 -2.725986e-01 5.986790e-01 4.827678e-01 9.067981e-01 6.623709e-01 1.661245e+00 1.530926e+00 8.065758e-01 4.336510e-01 6.030188e-01max 9.933931e+00 3.804234e+00 2.422565e+00 3.668398e+00 2.732346e+00 3.555044e+00 1.117494e+00 3.960518e+00 1.661245e+00 1.798194e+00 1.638828e+00 4.410519e-01 3.548771e+00&quot;&quot;&quot; \\[ y=Σwixi+b \\] Because the derivation of b is all 1, add a bias b to the data and set it to 1, as a feature of the data and update the gradient wi*b=wi 1234567891011121314151617df['bias'] = 1df&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT bias0 -0.419782 0.284830 -1.287909 -0.272599 -0.144217 0.413672 -0.120013 0.140214 -0.982843 -0.666608 -1.459000 0.441052 -1.075562 11 -0.417339 -0.487722 -0.593381 -0.272599 -0.740262 0.194274 0.367166 0.557160 -0.867883 -0.987329 -0.303094 0.441052 -0.492439 12 -0.417342 -0.487722 -0.593381 -0.272599 -0.740262 1.282714 -0.265812 0.557160 -0.867883 -0.987329 -0.303094 0.396427 -1.208727 13 -0.416750 -0.487722 -1.306878 -0.272599 -0.835284 1.016303 -0.809889 1.077737 -0.752922 -1.106115 0.113032 0.416163 -1.361517 14 -0.412482 -0.487722 -1.306878 -0.272599 -0.835284 1.228577 -0.511180 1.077737 -0.752922 -1.106115 0.113032 0.441052 -1.026501 1... ... ... ... ... ... ... ... ... ... ... ... ... ... ...501 -0.413229 -0.487722 0.115738 -0.272599 0.158124 0.439316 0.018673 -0.625796 -0.982843 -0.803212 1.176466 0.387217 -0.418147 1502 -0.415249 -0.487722 0.115738 -0.272599 0.158124 -0.234548 0.288933 -0.716639 -0.982843 -0.803212 1.176466 0.441052 -0.500850 1503 -0.413447 -0.487722 0.115738 -0.272599 0.158124 0.984960 0.797449 -0.773684 -0.982843 -0.803212 1.176466 0.441052 -0.983048 1504 -0.407764 -0.487722 0.115738 -0.272599 0.158124 0.725672 0.736996 -0.668437 -0.982843 -0.803212 1.176466 0.403225 -0.865302 1505 -0.415000 -0.487722 0.115738 -0.272599 0.158124 -0.362767 0.434732 -0.613246 -0.982843 -0.803212 1.176466 0.441052 -0.669058 1506 rows × 14 columns&quot;&quot;&quot; Divide the data set, where 20% of the data is used as the test set X_test, y_test, and the other 80% are used as the training set X_train, y_train, where random_state is the random seed 1234567891011from sklearn.model_selection import train_test_splitX_train, X_test, y_train,y_test = train_test_split(df, y, test_size = 0.2, random_state = 42)print('X_train.shape, y_train.shape:', X_train.shape, y_train.shape)print('X_test.shape, y_test.shape', X_test.shape, y_test.shape)&quot;&quot;&quot;X_train.shape, y_train.shape: (404, 14) (404,)X_test.shape, y_test.shape (102, 14) (102,)&quot;&quot;&quot;X_train = np.array(X_train) Model training and gradient update 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697def l1_cost(X, y, theta): &quot;&quot;&quot; X: 特征 y: 目标值 theta: 模型参数 &quot;&quot;&quot; k = X.shape[0] total_cost = 0 for i in range(k): total_cost =+ 1/k * np.abs(y[i] - theta.dot(X[i, :])) return total_cost def l2_cost(X, y, theta): k = X.shape[0] total_cost = 0 for i in range(k): total_cost += 1/k * (y[i] - theta.dot(X[i, :])) ** 2 return total_cost np.zeros(10).shape&quot;&quot;&quot;(10,)&quot;&quot;&quot;def step_l1_gradient(X, y, learning_rate, theta): &quot;&quot;&quot; Function to calculate the gradient of the MAE loss function Return the gradient value 0 for the non-differentiable point at 0 X: feature vector y: target value learing_rate: learning rate theta: parameter &quot;&quot;&quot; n = X.shape[0] print(n) e = y-X @ theta gradients = -(X.T @ np.sign(e)) / n theta = theta-learning_rate * gradients return theta def step_l2_gradient(X, y, learning_rate, theta): k = X.shape[0] x = X.shape[1] gradients = np.zeros(n) for i in range(k): for j in range(n): gradients[j] += (-2/k) * (y[i] - (theta.dot(X[i, :]))) * X[i, j] theta = theta - learning_rate * gradients return theta def step_gradient(X, y, learning_rate, theta): &quot;&quot;&quot; X: feature vector y: target value learing_rate: learning rate theta: parameter &quot;&quot;&quot; m_deriv = 0 N = len(X) for i in range(N): # Calculate the partial derivative # -x(y-(mx + b)) / |mx + b| m_deriv +=-X[i] * (y[i]-(theta*X[i] + b)) / abs(y[i]-(theta*X[i] + b)) # We subtract because the derivatives point in direction of steepest ascent theta -= (m_deriv / float(N)) * learning_rate# theta = theta-learning_rate * gradients return thetadef gradient_descent(X_train, y_train, learning_rate, iterations): k = X_train.shape[0] n = X_train.shape[1] theta = np.zeros(n) loss_values = [] print(theta.shape) for i in range(iterations): theta = step_l1_gradient(X_train, y_train, learning_rate, theta) loss = l1_cost(X_train, y_train, theta) loss_values.append(loss) print(i, 'cost:', loss) return theta, loss_values # Training parameterslearning_rate = 0.04 # Learning rateiterations = 300 # number of iterationstheta ,loss_values = gradient_descent(X_train, y_train, learning_rate, iterations)&quot;&quot;&quot;(14,)4040 cost: 0.045943991727139124041 cost: 0.045848379493882215404show more (open the raw output data in a text editor) ...299 cost: 0.017838215258874083&quot;&quot;&quot; Heart Practise 1234import pandas as pdpath = '~/data/'dataPath = path + 'heart.csv'train_data = pd.read_csv(dataPath) Field meaning 字段名 含义 age 年龄 sex 性别(1 = 男性, 0 = 女性) cp 胸部疼痛类型(值1：典型心绞痛，值2：非典型性心绞痛，值3：非心绞痛，值4：无症状） trestbps 血压 chol 胆固醇 fbs 空腹血糖（&gt; 120 mg/dl，1=真；0=假） restecg 心电图结果（0=正常，1=患有ST-T波异常，2=根据Estes的标准显示可能或确定的左心室肥大） thalach 最大心跳数 exang 运动时是否心绞痛（1=有过；0=没有） oldpeak 运动相对于休息的ST slop 心电图ST segment的倾斜度(值1:上坡，值2:平坦，值3:下坡） ca 透视检查看到的血管数 thal 缺陷种类（3=正常；6=固定缺陷；7=可逆缺陷） target 是否患病（0=否，1=是） Print a brief summary of the data set 12345678910111213141516171819202122232425262728293031train_data.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 303 entries, 0 to 302Data columns (total 14 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 303 non-null int64 1 sex 303 non-null int64 2 cp 303 non-null int64 3 trestbps 303 non-null int64 4 chol 303 non-null int64 5 fbs 303 non-null int64 6 restecg 303 non-null int64 7 thalach 303 non-null int64 8 exang 303 non-null int64 9 oldpeak 303 non-null float64 10 slope 303 non-null int64 11 ca 303 non-null int64 12 thal 303 non-null int64 13 target 303 non-null int64 dtypes: float64(1), int64(13)memory usage: 33.3 KB&quot;&quot;&quot;train_data.target.value_counts()&quot;&quot;&quot;1 1650 138Name: target, dtype: int64&quot;&quot;&quot; Change the \"sex\" column to two columns \"sex_0\" and \"sex_1\". 1sex = pd.get_dummies(train_data['sex'], prefix = &quot;sex&quot;) Add \"sex_0\" and \"sex_1\" to the data set. 1train_data = pd.concat([train_data,sex], axis = 1) And delete the sex column 1train_data = train_data.drop(columns = ['sex']) Print out the first five lines. Check whether sex_0, sex_1 are added successfully, and whether sex is deleted successfully. 123456789train_data.head()&quot;&quot;&quot; age cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target sex_0 sex_10 63 3 145 233 1 0 150 0 2.3 0 0 1 1 0 11 37 2 130 250 0 1 187 0 3.5 0 0 2 1 0 12 41 1 130 204 0 0 172 0 1.4 2 0 2 1 1 03 56 1 120 236 0 1 178 0 0.8 2 0 2 1 0 14 57 0 120 354 0 1 163 1 0.6 2 0 2 1 1 0&quot;&quot;&quot; Get sample label 12345y_data = train_data.target.valuestrain_data.shape&quot;&quot;&quot;(303, 15)&quot;&quot;&quot; Get sample feature set 12345x_data = train_data.drop(['target'],axis=1)x_data.shape&quot;&quot;&quot;(303, 14)&quot;&quot;&quot; Divide the data set Parameters: test_size=0.3, random_state=33 12from sklearn.model_selection import train_test_splitX_train,X_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=33) Normalization Import the StandardScaler package and initialize 12from sklearn.preprocessing import StandardScalerstandardScaler = StandardScaler() fit function/module is used to train model parameters 1standardScaler.fit(X_train) Standardize the training set and test set 12X_train = standardScaler.transform(X_train)X_test = standardScaler.transform(X_test) Define logistic regression model 123from sklearn.linear_model import LogisticRegression log_reg = LogisticRegression()log_reg.fit(X_train,y_train) Calculate the training set score 1234log_reg.score(X_train,y_train)&quot;&quot;&quot;0.8537735849056604&quot;&quot;&quot; Calculate the test set score 1234log_reg.score(X_test,y_test)&quot;&quot;&quot;0.8461538461538461&quot;&quot;&quot; Use the classification_report function to display a text report of the main classification indicators 12345678910111213from sklearn.metrics import classification_reporty_predict_log = log_reg.predict(X_test)print(classification_report(y_test,y_predict_log))&quot;&quot;&quot; precision recall f1-score support 0 0.93 0.78 0.85 50 1 0.78 0.93 0.84 41 accuracy 0.85 91 macro avg 0.85 0.85 0.85 91weighted avg 0.86 0.85 0.85 91&quot;&quot;&quot;","link":"/example_04/"},{"title":"Machine Learning Part-03","text":"Decision trees Machine learning basics - use decision trees to make predictions about coupons In order to get close to real life and applications, the processing of actual data sets is the main focus. From January 1, 2016 to June 30, 2016, real online and offline consumption behaviors are predicted to be used by users within 15 days after receiving coupons in July 2016. Note: In order to protect the privacy of users and businesses, all data is anonymized, and biased sampling and necessary filtering are used. Data set ccf_offline_stage1_train.csv (training data) Field Description User_id 用户ID Merchant_id 商户ID Coupon_id 优惠券ID：null表示无优惠券消费，此时Discount_rate和Date_received字段无意义 Discount_rate 优惠率：x 代表折扣率；x:y表示满x减y。单位是元 Distance user经常活动的地点离该merchant的最近门店距离是x*500米（如果是连锁店，则取最近的一家门店），x\\(\\in[0,10]\\)；null表示无此信息，0表示低于500米，10表示大于5公里； Date_received 领取优惠券日期 Date 消费日期：如果Date=null &amp; Coupon_id != null，该记录表示领取优惠券但没有使用，即负样本；如果Date!=null &amp; Coupon_id = null，则表示普通消费日期；如果Date!=null &amp; Coupon_id != null，则表示用优惠券消费日期，即正样本； 123456789101112131415161718192021222324252627282930313233343536373839404142# load pluginimport pandas as pdimport numpy as np# load datatrain_data = pd.read_csv('~/data/ccf_offline_stage1_train.csv')train_data.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 1754884 entries, 0 to 1754883Data columns (total 7 columns): # Column Dtype --- ------ ----- 0 User_id int64 1 Merchant_id int64 2 Coupon_id float64 3 Discount_rate object 4 Distance float64 5 Date_received float64 6 Date float64dtypes: float64(4), int64(2), object(1)memory usage: 93.7+ MB&quot;&quot;&quot;train_data.head()&quot;&quot;&quot; User_id Merchant_id Coupon_id Discount_rate Distance Date_received Date0 1439408 2632 NaN NaN 0.0 NaN 20160217.01 1439408 4663 11002.0 150:20 1.0 20160528.0 NaN2 1439408 2632 8591.0 20:1 0.0 20160217.0 NaN3 1439408 2632 1078.0 20:1 0.0 20160319.0 NaN4 1439408 2632 8591.0 20:1 0.0 20160613.0 NaN&quot;&quot;&quot;print(train_data.shape)data = train_data.dropna(how = 'any')print(train_data.shape)&quot;&quot;&quot;(1754884, 7)(1754884, 7)&quot;&quot;&quot; Discount_rate是object类型的，object在pandas中代表字符串，字符串类型不能输入模型中，所以需要改为数值类型 123456789101112print('Discount_rate 类型: \\n', data['Discount_rate'].unique())# [0,1] 表示折扣率# x:y 表示满 x 减 y&quot;&quot;&quot;Discount_rate 类型: ['20:1' '20:5' '30:5' '50:10' '10:5' '50:20' '100:10' '30:10' '50:5' '30:1' '100:30' '0.8' '200:30' '100:20' '10:1' '200:20' '0.95' '5:1' '100:5' '100:50' '50:1' '20:10' '150:10' '0.9' '200:50' '150:20' '150:50' '200:5' '300:30' '100:1' '200:10' '150:30' '0.85' '0.6' '0.5' '300:20' '200:100' '300:50' '150:5' '300:10' '0.75' '0.7' '30:20' '50:30']&quot;&quot;&quot; Convert Discount_rate into numerical features Discount type x:y 表示满 x 减 y 将 x:y 类型的字符串设为1 [0,1] 表示折扣率 将 [0,1] 类型的字符串设为 0 12345678910111213141516171819202122232425262728293031323334353637def getDiscountType(row): if ':' in row: return 1 else: return 0data['Discount_rate'] = data['Discount_rate'].apply(getDiscountType)&quot;&quot;&quot;See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy import sys&quot;&quot;&quot;data.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;Int64Index: 67165 entries, 6 to 1754880Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 User_id 67165 non-null int64 1 Merchant_id 67165 non-null int64 2 Coupon_id 67165 non-null float64 3 Discount_rate 67165 non-null int64 4 Distance 67165 non-null float64 5 Date_received 67165 non-null float64 6 Date 67165 non-null float64dtypes: float64(4), int64(3)memory usage: 4.1 MB&quot;&quot;&quot;# load plugin# Import DecisionTreeClassifier modelfrom sklearn.tree import DecisionTreeClassifier# Import train_test_split, used to divide the data set and test setfrom sklearn.model_selection import train_test_split# Import accuracy_score accuracy indexfrom sklearn.metrics import accuracy_score add label row to the dataset Labeling Label Label which samples are positive samples y=1 and which are negative samples y = -1 Forecast goal: the user's consumption within 15 days after receiving the coupon (Date-Date_received &lt;= 15) means to receive the coupon and use it within 15 days, that is, a positive sample, y = 1 (Date-Date_received&gt; 15) means that the coupon has not been used within 15 days, that is, a negative sample, y = 0 pandas tutorial on time https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html 1234567891011def label(row): if row['Date'] != 'null': td = pd.to_datetime(row['Date'], format = '%Y%m%d') - pd.to_datetime(row['Date_received'], format = '%Y%m%d') if td &lt;= pd.Timedelta(15, 'D'): return 1 return 0data['label'] = data.apply(label, axis = 1)&quot;&quot;&quot;See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy&quot;&quot;&quot; Statistics positive and negative samples 123456print(data['label'].value_counts())&quot;&quot;&quot;1 570600 10105Name: label, dtype: int64&quot;&quot;&quot; Divide the data set 80% training set 20% test set 80% train 20% test 1X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.2, random_state=3) Check the number and category distribution of training samples 12345678y_train.value_counts()&quot;&quot;&quot;2751537 966641735 86 ..4461556 1Name: User_id, Length: 34984, dtype: int64&quot;&quot;&quot; Check the number and type distribution of test samples 12345678y_test.value_counts()&quot;&quot;&quot;6641735 272751537 22 ..89464 1Name: User_id, Length: 11405, dtype: int64&quot;&quot;&quot; Initialize the classification decision tree model, the depth is 5 layers 1model = DecisionTreeClassifier(max_depth=6, random_state = 1) Model training 1model.fit(X_train, y_train) Model prediction 1y_pred = model.predict(X_test) Model evaluation 1234accuracy_score(y_test, y_pred)&quot;&quot;&quot;0.011315417256011316&quot;&quot;&quot; Change the standard of the model selection feature to entropy 1model = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=2) Model training 1model.fit(X_train, y_train) predict 1y_pred = model.predict(X_test) Evaluate 1accuracy_score(y_test, y_pred) In addition to the above key steps, you can explore the data by yourself, as well as any other forms of feature preprocessing methods and feature engineering processing. I hope to focus on understanding the development process of machine learning tasks. For the skills and methods of data processing, it is encouraged to invest more time to explore. iris 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifier, export_graphviz iris = load_iris()X = iris.data y = iris.targettree_clf = DecisionTreeClassifier()tree_clf.fit(X, y)export_graphviz( tree_clf, out_file=&quot;~/data/course_data/iris_tree.dot&quot;, feature_names=iris.feature_names, class_names=iris.target_names, rounded=True, filled=True)for line in open('~/data/course_data/iris_tree.dot'): print(line)&quot;&quot;&quot;digraph Tree {node [shape=box, style=&quot;filled, rounded&quot;, color=&quot;black&quot;, fontname=helvetica] ;edge [fontname=helvetica] ;0 [label=&quot;petal length (cm) &lt;= 2.45\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]\\nclass = setosa&quot;, fillcolor=&quot;#ffffff&quot;] ;1 [label=&quot;gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]\\nclass = setosa&quot;, fillcolor=&quot;#e58139&quot;] ;0 -&gt; 1 [labeldistance=2.5, labelangle=45, headlabel=&quot;True&quot;] ;2 [label=&quot;petal width (cm) &lt;= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]\\nclass = versicolor&quot;, fillcolor=&quot;#ffffff&quot;] ;0 -&gt; 2 [labeldistance=2.5, labelangle=-45, headlabel=&quot;False&quot;] ;3 [label=&quot;petal length (cm) &lt;= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]\\nclass = versicolor&quot;, fillcolor=&quot;#4de88e&quot;] ;2 -&gt; 3 ;4 [label=&quot;petal width (cm) &lt;= 1.65\\ngini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]\\nclass = versicolor&quot;, fillcolor=&quot;#3de684&quot;] ;3 -&gt; 4 ;5 [label=&quot;gini = 0.0\\nsamples = 47\\nvalue = [0, 47, 0]\\nclass = versicolor&quot;, fillcolor=&quot;#39e581&quot;] ;show more (open the raw output data in a text editor) ...16 [label=&quot;gini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]\\nclass = virginica&quot;, fillcolor=&quot;#8139e5&quot;] ;12 -&gt; 16 ;}&quot;&quot;&quot; Salient Features 1tree_clf.feature_importances_ Build Decision Tree: CART 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import pandas as pdmock_data = { 'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'], 'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'], 'family_number': [1, 1, 2, 1, 1, 1, 2], 'bought': [1, 1, 1, 0, 0, 0, 1],}dataset = pd.DataFrame.from_dict(mock_data)import numpy as npfrom collections import Counterdef entropy(elements): counter = Counter(elements) probabilities = [counter[e] / len(elements) for e in elements] return -sum(p * np.log10(p) for p in probabilities)def find_the_min_spilter(training_data: pd.DataFrame, target: str) -&gt; str: x_fields = set(training_data.columns.tolist()) - {target} spliter = None min_entropy = float('inf') for f in x_fields: elements = set(training_data[f]) for e in elements: sub_spliter_1 = training_data[dataset[f] == e][target].tolist() entropy_1 = entropy(sub_spliter_1) sub_spliter_2 = training_data[dataset[f] != e][target].tolist() entropy_2 = entropy(sub_spliter_2) entropy_v = entropy_1 + entropy_2 if entropy_v &lt; min_entropy: min_entropy = entropy_v spliter = (f, e) print('spliter is: {}'.format(spliter)) print('the min entropy is: {}'.format(min_entropy)) return spliterfind_the_min_spilter(dataset, 'bought')&quot;&quot;&quot;spliter is: ('income', '+10')the min entropy is: 0.7176797562470717('income', '+10')&quot;&quot;&quot;dataset[dataset['income'] == '-10']&quot;&quot;&quot; gender income family_number bought1 F -10 1 16 M -10 2 1&quot;&quot;&quot;dataset[dataset['income'] != '-10']&quot;&quot;&quot; gender income family_number bought0 F +10 1 12 F +10 2 13 F +10 1 04 M +10 1 05 M +10 1 0&quot;&quot;&quot;","link":"/example_05/"},{"title":"Machine Learning Part-04","text":"SVM 12345678910111213import numpy as nplabel_a = np.random.normal(6, 2, size = (50,2))label_b = np.random.normal(-6, 2, size = (50,2))import matplotlib.pyplot as plta = [1, 2, 3]b = [-1,-2, -3]plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))plt.show() image-20210901170815036 12345678910111213141516171819202122232425262728293031323334353637label_a_x = label_a[:, 0]label_b_x = label_b[:, 0]def f(x, k, b): return k*x -b k_and_b = []for i in range(100): k, b = (np.random.random(size = (1,2)) * 10 - 5)[0] if np.max(f(label_a_x, k, b)) &lt;= -1 and np.min(f(label_b_x, k, b)) &gt;= 1: print(k, b) k_and_b.append((k, b))&quot;&quot;&quot;-3.4732670434285517 -2.3248316389039325-3.654276254462583 0.01110189858052646-2.4609031871010014 -0.3932180655739925-2.9206497777762843 0.2595456609552631-4.07589152330003 -0.6463313059119606-3.1950366475236835 -1.8558958669742989-4.316670785852706 -3.1033030808371653-4.124339773909792 -1.5741734685470687-4.20817621470405 0.4368323022696625-3.7098120657624003 -0.38196175566618784-3.2053446683533315 0.12822700803583054-4.534694169094692 1.143734501297419-4.8124714209376425 0.8707258703100704&quot;&quot;&quot;plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))for k, b in k_and_b: x = np.concatenate((label_a_x, label_b_x)) plt.plot(x, f(x, k, b))plt.show() image-20210901170902373 1234567plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))k,b = sorted(k_and_b, key = lambda t: abs(t[0]))[0]x = np.concatenate((label_a_x, label_b_x))plt.plot(x, f(x, k, b))plt.show() image-20210901170921575 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from sklearn.datasets import load_bostondatasets = load_boston()data, target = datasets['data'], datasets['target']import pandas as pddf = pd.DataFrame(data)df.columns = datasets['feature_names']import randomdef random_select(df, drop_num = 4): columns = random.sample(list(df.columns), k = len(df.columns) - drop_num) return df[columns] from sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeRegressorsample_x = random_select(df)regressioner = DecisionTreeRegressor()(X_train, X_test, y_train, y_test) = train_test_split(sample_x, target, test_size = 0.3)regressioner.fit(X_train, y_train)regressioner.score(X_train, y_train)&quot;&quot;&quot;1.0&quot;&quot;&quot;regressioner.score(X_test, y_test)&quot;&quot;&quot;0.8110635350395325&quot;&quot;&quot;def random_tree(train_X, train_y, test_X, test_y, drop_n = 4): train_sample = random_select(train_X, drop_num = drop_n) regressioner = DecisionTreeRegressor() regressioner.fit(train_sample, train_y) train_score = regressioner.score(train_sample, train_y) test_score = regressioner.score(test_X[train_sample.columns], test_y) print('train score = {}; test score = {}'.format(train_score, test_score)) y_predicat = regressioner.predict(test_X[train_sample.columns]) return y_predicat def random_forest(train_X, train_y, test_X, test_y, tree_n = 4): predicat = np.array([random_tree(train_X, train_y, test_X, test_y) for _ in range(tree_n)]) return np.mean(predicat, axis = 0) (X_train, X_test, y_train, y_test) = train_test_split(df, target, test_size = 0.3)forest_predict = random_forest(X_train, y_train, X_test, y_test)&quot;&quot;&quot;train score = 1.0; test score = 0.5367061884031776train score = 1.0; test score = 0.4983695562874999train score = 1.0; test score = 0.6715869370883646train score = 1.0; test score = 0.6210922529610217&quot;&quot;&quot;forest_predict&quot;&quot;&quot;array([10.925, 21.1 , 30.625, 28.025, 22.525, 17.65 , 20.6 , 17.325, 29.175, 14.95 , 40.775, 19.55 , 12.175, 23.675, 10.775, 22.1 , ... 15.575, 20.5 , 22.775, 30.725, 18.975, 16.45 , 22.05 , 18.925])&quot;&quot;&quot;from sklearn.metrics import r2_scorer2_score(y_test, forest_predict)&quot;&quot;&quot;0.7840500839091215&quot;&quot;&quot; Entropy: 熵 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npfrom collections import Counterfrom icecream import icfrom functools import lru_cachedef pr(es): counter = Counter(es) def _wrap(e): return counter[e] / len(es) return _wrapdef entropy(elements): # Information Entropy p = pr(elements) return -np.sum(p(e) * np.log(p(e)) for e in set(elements))def gini(elements): p = pr(elements) return 1-np.sum(p(e) ** 2 for e in set(elements)) pure_func = giniic(pure_func([1, 1, 1, 1, 1, 0]))ic(pure_func([1, 1, 1, 1, 1, 1]))ic(pure_func([1, 2, 3, 4, 5, 8]))ic(pure_func([1, 2, 3, 4, 5, 9]))ic(pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'c']))ic(pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'd']))&quot;&quot;&quot;ic| pure_func([1, 1, 1, 1, 1, 0]): 0.2777777777777777ic| pure_func([1, 1, 1, 1, 1, 1]): 0.0ic| pure_func([1, 2, 3, 4, 5, 8]): 0.8333333333333333ic| pure_func([1, 2, 3, 4, 5, 9]): 0.8333333333333333ic| pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'c']): 0.44897959183673464ic| pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'd']): 0.61224489795918370.6122448979591837&quot;&quot;&quot; Random forest 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879from sklearn.datasets import load_bostonfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitimport numpy as npimport pandas as pdfrom sklearn.metrics import r2_scorehouse = load_boston()X = house.datay = house.targetx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)tree_reg = DecisionTreeRegressor()tree_reg.fit(x_train, y_train)print('whole dataset train acc: {}'.format(tree_reg.score(x_train, y_train)))print('whole dataset test acc: {}'.format(tree_reg.score(x_test, y_test)))&quot;&quot;&quot;whole dataset train acc: 1.0whole dataset test acc: 0.6776520888466615&quot;&quot;&quot;def random_forest(train_x, train_y, test_x, test_y, drop_n=4): random_features = np.random.choice(list(train_x.columns), size=len(train_x.columns)-drop_n) sample_x = train_x[random_features] sample_y = train_y reg = DecisionTreeRegressor() reg.fit(sample_x, sample_y) train_score = reg.score(sample_x, sample_y) test_score = reg.score(test_x[random_features], test_y) print('sub sample :: train score: {}, test score: {}'.format(train_score, test_score)) y_predicated = reg.predict(test_x[random_features]) return y_predicated, test_score with_feature_names = pd.DataFrame(X)with_feature_names.columns = house['feature_names']x_train, x_test, y_train, y_test = train_test_split(with_feature_names, y, test_size=0.3, random_state=0)tree_num = 4predicates = []for _ in range(tree_num): predicated, score = random_forest(x_train, y_train, x_test, y_test) predicates.append((predicated, score))&quot;&quot;&quot;sub sample :: train score: 1.0, test score: 0.5640870175410873sub sample :: train score: 1.0, test score: 0.29024437819534354sub sample :: train score: 1.0, test score: 0.37812117132843814sub sample :: train score: 1.0, test score: 0.5650888856735524&quot;&quot;&quot;predicates_value = [v for v, s in predicates]forest_scores = [s for v, s in predicates]print('the score of forest is : {}'.format(r2_score(y_test, np.mean(predicates_value, axis=0))))&quot;&quot;&quot;the score of forest is : 0.680193104551715&quot;&quot;&quot;weights = np.array(forest_scores) / np.sum(forest_scores)weights_score = np.zeros_like(np.mean(predicates_value, axis=0))for i, v in enumerate(predicates_value): weights_score += v * weights[i] print('the score of weighted forest is : {}'.format(r2_score(y_test, weights_score)))&quot;&quot;&quot;the score of weighted forest is : 0.6956613076019385&quot;&quot;&quot; Show SVM 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npimport matplotlib.pyplot as pltlabel_a = np.random.normal(6, 2, size=(50, 2))label_b = np.random.normal(-6, 2, size=(50, 2))plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))label_a_x = label_a[:, 0]label_b_x = label_b[:, 0]def f(x, w, b): return w * x + b k_and_b = []for i in range(100): k, b = (np.random.random(size=(1, 2)) * 10 - 5)[0] if np.max(f(label_a_x, k, b)) &gt;= -1 and np.min(f(label_b_x, k, b)) &gt;= 1: print(k, b) k_and_b.append((k, b))&quot;&quot;&quot;0.17732109082579406 3.9508645615428843-0.8649868307954458 1.7349996177756957...-2.2969567032985783 2.171321001904926&quot;&quot;&quot;for k, b in k_and_b: x = np.concatenate((label_a_x, label_b_x)) plt.plot(x, f(x, k, b)) print(k_and_b)&quot;&quot;&quot;[(0.17732109082579406, 3.9508645615428843), (-0.8649868307954458, 1.7349996177756957), (-0.818317924604357, 0.352843348193578), (-0.19730603224472976, 4.002168852007262), ...(-2.2969567032985783, 2.171321001904926)]&quot;&quot;&quot;w, b = min(k_and_b, key=lambda k_b: k_b[0])all_x = np.concatenate((label_a_x, label_b_x))plt.plot(all_x, f(all_x, w, b), 'r-o')plt.show() image-20210901171726065 Integrated learning Ensemble learning is a machine learning paradigm that solves the same problem by training multiple models. In contrast to ordinary machine learning methods that try to learn a hypothesis from training data, ensemble methods try to construct a set of hypotheses and use them in combination. Next, we will use the decision tree and its integrated version to model the classic data set Mnist and observe the differences in different integration methods. 123456789!ls!unzip mnist_test.csv.zip &amp;&amp; unzip mnist_train.csv.zipimport numpy as npimport pandas as pdfrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier Build a data set The Mnist data set used this time is not in the original format. In order to more easily adapt to this training, the 28 * 28 pictures in the original data set are flatten operation, it becomes 784 features, the columns in the DataFrame below: 1x1, 1x2, ..., 28x28, representing the i row and j column in the picture The pixel value of is a grayscale image, so the pixel value is only 0 and 1 1234567891011train_df = df = pd.read_csv('~/data/mnist_train.csv')train_df.head()&quot;&quot;&quot; label 1x1 1x2 1x3 1x4 1x5 1x6 1x7 1x8 1x9 ... 28x19 28x20 28x21 28x22 28x23 28x24 28x25 28x26 28x27 28x280 5 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 01 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 02 4 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 03 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 04 9 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 05 rows × 785 columns&quot;&quot;&quot; View training data information:, whether there is NaN, how many pieces of data are there... 1234567891011121314151617181920212223242526272829train_df.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 60000 entries, 0 to 59999Columns: 785 entries, label to 28x28dtypes: int64(785)memory usage: 359.3 MB&quot;&quot;&quot;test_df = df = pd.read_csv('~/data/mnist_test.csv')test_df.head()&quot;&quot;&quot; label 1x1 1x2 1x3 1x4 1x5 1x6 1x7 1x8 1x9 ... 28x19 28x20 28x21 28x22 28x23 28x24 28x25 28x26 28x27 28x280 7 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 01 2 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 02 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 03 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 04 4 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 05 rows × 785 columns&quot;&quot;&quot;test_df.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 10000 entries, 0 to 9999Columns: 785 entries, label to 28x28dtypes: int64(785)memory usage: 59.9 MB&quot;&quot;&quot; Build training and test data 12345678910X_train = train_df.iloc[:, 1:]y_train = train_df.iloc[:, 0]X_test = test_df.iloc[:, 1:]y_test = test_df.iloc[:, 0](X_train.shape, y_train.shape), (X_test.shape, y_test.shape)&quot;&quot;&quot;(((60000, 784), (60000,)), ((10000, 784), (10000,)))&quot;&quot;&quot; Decision Tree First train a simple decision tree to see how it performs 1234567891011121314151617181920dtc = DecisionTreeClassifier()dtc.fit(X_train, y_train)dtc.score(X_train, y_train)&quot;&quot;&quot;1.0&quot;&quot;&quot;dtc.score(X_test, y_test)&quot;&quot;&quot;0.8753&quot;&quot;&quot;dtc = DecisionTreeClassifier(min_samples_leaf=8)dtc.fit(X_train, y_train)dtc.score(X_train, y_train), dtc.score(X_test, y_test)&quot;&quot;&quot;(0.9311666666666667, 0.8795)&quot;&quot;&quot; From the above results, we can see that by adjusting the parameter min_samples_leaf, the overfitting situation has been alleviated. What does this parameter mean? Why increasing it can alleviate the overfitting problem? The meaning of min_samples_leaf is the minimum number of samples contained in the leaf nodes of the decision tree. By increasing this parameter, the decision tree can not capture any of the subtle features of the training data during training, resulting in excessive training data. Fitting: The large number of samples of leaf nodes can also play a role in voting and enhance the generalization performance of the model. You can try to continue to increase the value of this parameter and try to find the best parameter. In addition to this parameter, you can also try to adjust the parameters such as min_samples_split and max_features. For the specific meaning, please refer to sklearn documentation Second question: Try to adjust other parameters to see the performance of the decision tree on the test set Random Forest Take a look at the bagging version of the decision tree and how the random forest performs! 1234567rfc = RandomForestClassifier(n_estimators = 10)rfc.fit(X_train, y_train)rfc.score(X_train, y_train), rfc.score(X_test, y_test)&quot;&quot;&quot;(0.99905, 0.9513)&quot;&quot;&quot; It is worthy of the integrated version. It basically achieves better performance under the default parameters. The accuracy of the test set is about 7% higher than that of the ordinary decision tree. However, comparing the training and test results, it can be found that there is still a certain degree of overfitting. , Try to adjust some parameters below 1234567rfc = RandomForestClassifier(n_estimators = 20)rfc.fit(X_train, y_train)rfc.score(X_train, y_train), rfc.score(X_test, y_test)&quot;&quot;&quot;(0.9999, 0.96)&quot;&quot;&quot; After increasing the parameter n_estimators, the accuracy of the test set has increased by about 1%. The meaning of this parameter is to train 20 decision trees at the same time, and finally integrate the results. The increase of this parameter can be simply regarded as voting The number of people increases, so the final result will inevitably be more robust. You can try to continue to increase this parameter, or adjust other parameters such as max_samples, appropriately less than the total amount of training data, which can increase the difference between different sub-models and further improve the generalization performance. It can also adjust the parameters of the base learner (decision tree). For the meaning of the parameters, see sklearn documentation GBDT Let's compare the performance of the boosting version of the decision tree GBDT! 1234567gbc = GradientBoostingClassifier(n_estimators=10)gbc.fit(X_train, y_train)gbc.score(X_train, y_train), gbc.score(X_test, y_test)&quot;&quot;&quot;(0.8423, 0.846)&quot;&quot;&quot; As expected, the performance has been greatly improved, and the indicators of the training set are basically the same as those of the test set, and there is no overfitting, so it should be possible to continue to try to improve this parameter. Generally, in the absence of overfitting, we only need to consider continuing to increase the complexity of the model. This is the fastest way to improve performance. When the complexity of the model increases to the point of over-fitting, we then consider using some methods to reduce over-fitting. Bagging The aforementioned random forest and GBDT are ensemble learning algorithms based on decision trees, but it should be noted that ensemble learning is not exclusive to decision trees. Any other learner can be used as a base learner for ensemble learning, such as Logistic regression, support vector machine. Bagging is short for \"bootstrap aggregating\". This is a meta-algorithm, which takes M sub-samples (with replacement) from the initial data set, and trains the prediction model on these sub-samples. The final model is obtained by averaging all sub-models, which usually produces better results. The main advantage of this technique is that it combines regularization, all you need to do is choose good parameters for the base learner. The following uses the general api provided by sklearn to construct an integrated learning algorithm 12345678# Still use decision tree as base learnerbgc = BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=1.0, n_estimators=20)bgc.fit(X_train, y_train)bgc.score(X_train, y_train), bgc.score(X_test, y_test)&quot;&quot;&quot;(0.9935166666666667, 0.9506)&quot;&quot;&quot; Third question Logistic regression as a base learner 1234567bgc = BaggingClassifier(LogisticRegression(max_iter = 500), max_samples=0.5, max_features=1.0, n_estimators=20)bgc.fit(X_train, y_train)bgc.score(X_train, y_train), bgc.score(X_test, y_test)&quot;&quot;&quot;(0.9421166666666667, 0.9228)&quot;&quot;&quot; Above we have successfully used logistic regression as the base learner to complete integrated learning. You can try to use only logistic regression for training, and compare the performance of the single model with the bagging version of logistic regression. Boosting Boosting refers to a series of algorithms that can transform a weak learner into a strong learner. The main principle of boosting is to combine a series of weak learners (only better than random guessing). For those samples that were misclassified in the early stages of training, the boosting algorithm will give more attention. Then combine the predictions by weighted majority voting (classification) or weighted sum (regression) to produce the final prediction. 123456abc = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=10, learning_rate=0.01)abc.fit(X_train, y_train)abc.score(X_train, y_train), abc.score(X_test, y_test)&quot;&quot;&quot;(1.0, 0.875)&quot;&quot;&quot; Comparing the boosting integrated version of decision tree and logistic regression, we can find that logistic regression has better generalization ability, and decision tree is easier to overfit 123456abc = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=8), n_estimators=10, learning_rate=0.01)abc.fit(X_train, y_train)abc.score(X_train, y_train), abc.score(X_test, y_test)&quot;&quot;&quot;(0.9981833333333333, 0.9532)&quot;&quot;&quot; In fact, over-fitting is not a bad thing. If your model cannot be over-fitted, it means that it cannot fit the training data well. Therefore, the decision tree is very over-fitted at the beginning, which also shows its potential. , You can see that after the above parameters are adjusted, the boosting version of the decision tree easily exceeds the boosting version of the logistic regression","link":"/example_06/"},{"title":"Business Intelligence(BI)","text":"Use LeNet model to recognize Mnist handwritten digits 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import tensorflow as tf#print(tf.__version__)from tensorflow.keras import layersfrom tensorflow.keras.datasets import mnistfrom tensorflow.keras.layers import Conv2D, MaxPooling2Dfrom tensorflow.keras.layers import Dense, Flattenfrom tensorflow.keras.models import Sequentialimport numpy as npimport warningswarnings.filterwarnings('ignore')# Data loading#(train_x, train_y), (test_x, test_y) = mnist.load_data() #Download the data set from the Internetdata = np.load('~/data/course_data/mnist.npz') #Read data set from local#print(data.files)train_x, train_y, test_x, test_y = data['x_train'], data['y_train'], data['x_test'], data['y_test']warnings.filterwarnings('ignore')# Input data is mnist data settrain_x = train_x.reshape(train_x.shape[0], 28, 28, 1)test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)train_x = train_x / 255test_x = test_x / 255train_y = tf.keras.utils.to_categorical(train_y, 10)test_y = tf.keras.utils.to_categorical(test_y, 10)# Create sequential modelmodel = Sequential()# The first layer of convolutional layer: 6 convolution kernels, the size is 5*5, relu activation functionmodel.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))# The second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size=(2, 2)))# The third layer of convolutional layer: 16 convolution kernels, size 5*5, relu activation functionmodel.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))# The second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size=(2, 2)))# Flatten the parameters, which is called a convolutional layer in LeNet5. In fact, this layer is a one-dimensional vector, the same as the fully connected layermodel.add(Flatten())model.add(Dense(120, activation='relu'))# Fully connected layer, the number of output nodes is 84model.add(Dense(84, activation='relu'))# The output layer uses the softmax activation function to calculate the classification probabilitymodel.add(Dense(10, activation='softmax'))# Set the loss function and optimizer configurationmodel.compile(loss=tf.keras.metrics.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])# Pass in training data for trainingmodel.fit(train_x, train_y, batch_size=128, epochs=2, verbose=1, validation_data=(test_x, test_y))# Evaluate the resultsscore = model.evaluate(test_x, test_y)print('error:%0.4lf' %score[0])print('Accuracy:', score[1])&quot;&quot;&quot;Train on 60000 samples, validate on 10000 samplesEpoch 1/260000/60000 [==============================] - 39s 643us/sample - loss: 0.3172 - acc: 0.9096 - val_loss: 0.1105 - val_acc: 0.9626Epoch 2/260000/60000 [==============================] - 39s 652us/sample - loss: 0.0892 - acc: 0.9725 - val_loss: 0.0664 - val_acc: 0.979010000/10000 [==============================] - 4s 358us/sample - loss: 0.0664 - acc: 0.9790error:0.0664Accuracy: 0.979&quot;&quot;&quot; Use LR to classify MNIST handwritten digits 12345678910111213141516171819202122232425262728293031323334from sklearn.model_selection import train_test_splitfrom sklearn import preprocessingfrom sklearn.metrics import accuracy_scorefrom sklearn.datasets import load_digitsfrom sklearn.linear_model import LogisticRegressionimport matplotlib.pyplot as plt# Download Datadigits = load_digits()data = digits.data# Data Explorationprint(data.shape)# View the first imageprint(digits.images[0])# The meaning of the numbers represented by the first imageprint(digits.target[0])# Display the first imageplt.gray()plt.title('Handwritten Digits')plt.imshow(digits.images[0])plt.show()&quot;&quot;&quot;(1797, 64)[[ 0. 0. 5. 13. 9. 1. 0. 0.] [ 0. 0. 13. 15. 10. 15. 5. 0.] [ 0. 3. 15. 2. 0. 11. 8. 0.] [ 0. 4. 12. 0. 0. 8. 8. 0.] [ 0. 5. 8. 0. 0. 9. 8. 0.] [ 0. 4. 11. 0. 1. 12. 7. 0.] [ 0. 2. 14. 5. 10. 12. 0. 0.] [ 0. 0. 6. 13. 10. 0. 0. 0.]]0&quot;&quot;&quot; image-20210901154812986 12345678910111213141516# Split the data, use 25% of the data as the test set, and the rest as the training settrain_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)# Adopt Z-Score standardizationss = preprocessing.StandardScaler()train_ss_x = ss.fit_transform(train_x)test_ss_x = ss.transform(test_x)# Create LR classifierlr = LogisticRegression()lr.fit(train_ss_x, train_y)predict_y=lr.predict(test_ss_x)print('LR accuracy rate: %0.4lf'% accuracy_score(predict_y, test_y))&quot;&quot;&quot;LR accuracy rate: 0.9644&quot;&quot;&quot;","link":"/example_12/"},{"title":"Natural Language Processing NLP","text":"Resnet Visualize 12345678910111213141516171819202122232425262728293031323334353637383940414243import torchvisionimport torch.nn.functional as Ffrom torchvision.transforms import transformsfrom torch import nnimport torchimport matplotlib.pyplot as pltfrom icecream import icfrom PIL import Imageimport numpy as npdef visualize_model(model, input_, output): width = 8 fig, ax = plt.subplots(output[0].shape[0] // width, width, figsize=(20, 20)) for i in range(output[0].shape[0]): ix = np.unravel_index(i, ax.shape) plt.sca(ax[ix]) ax[ix].title.set_text('filter-{}'.format(i)) plt.imshow(output[0][i].detach()) plt.show()preprocess = transforms.Compose([ transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),])resnet = torchvision.models.resnet18(pretrained=True) # transfer step 1: load pretrained modelconv_model = [m for _, m in resnet.named_modules() if isinstance(m, torch.nn.Conv2d)]&quot;&quot;&quot;Downloading: &quot;https://download.pytorch.org/models/resnet18-f37072fd.pth&quot; to /Users/lilithgames/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth100%|██████████| 44.7M/44.7M [00:34&lt;00:00, 1.36MB/s]&quot;&quot;&quot;for m in conv_model: m.register_forward_hook(visualize_model)myself = preprocess(Image.open('~/data/course_data/doo.jpeg'))with torch.no_grad(): resnet(myself.unsqueeze(0)) # un-squeeze for convert myself to [ [myself] ] Only some pictures are posted here image-20210901151356189 image-20210901151412761 image-20210901151432743 Transfer Example 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import torchvisionimport torch.nn.functional as Ffrom torchvision.transforms import transformsfrom torch import nnimport torchimport matplotlib.pyplot as pltfrom icecream import icpreprocess = transforms.Compose([ transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),])cifar_10 = torchvision.datasets.CIFAR10('~/data/course_data/', download=False, transform=preprocess)train_loader = torch.utils.data.DataLoader(cifar_10, batch_size=128, shuffle=True)resnet = torchvision.models.resnet18(pretrained=True) # transfer step 1: load pretrained modelfor param in resnet.parameters(): param.requires_grad = False # frozen weights feature_num = resnet.fc.in_featuresresnet.fc = nn.Linear(feature_num, 10) # rewrite fc classifieric(resnet(cifar_10[0][0].unsqueeze(0)))criterion = nn.CrossEntropyLoss()optimizer = torch.optim.SGD(resnet.parameters(), lr=1e-3, momentum=0.9)epochs = 2losses = []&quot;&quot;&quot;return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)ic| resnet(cifar_10[0][0].unsqueeze(0)): tensor([[-0.0763, -0.4537, 0.8168, 0.2136, -0.0465, 0.4844, -0.4026, 0.8763, -0.7048, -0.7375]], grad_fn=&lt;AddmmBackward&gt;)&quot;&quot;&quot;for epoch in range(epochs): epoch_loss = 0 for i, (images, labels) in enumerate(train_loader): ic(epoch, i) output = resnet(images) loss = criterion(output, labels) optimizer.zero_grad() loss.backward() optimizer.step() epoch_loss += loss.item() if i &gt; 0: print('Epoch: {} batch:{}, loss ==&gt; {}'.format(epoch, i, epoch_loss / i)) losses.append(epoch_loss / i)&quot;&quot;&quot;ic| epoch: 0, i: 0ic| epoch: 0, i: 1ic| epoch: 0, i: 2Epoch: 0 batch:1, loss ==&gt; 5.118020296096802ic| epoch: 0, i: 3Epoch: 0 batch:2, loss ==&gt; 3.8235710859298706ic| epoch: 0, i: 4...ic| epoch: 0, i: 203Epoch: 0 batch:202, loss ==&gt; 1.4433288293899875...&quot;&quot;&quot;plt.plot(losses)plt.show()&quot;&quot;&quot;Because the last time is too long to run, the losses are not assigned&quot;&quot;&quot; Resnet Transfer Learning 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import torchvisionimport torch.nn.functional as Fcifar_10 = torchvision.datasets.CIFAR10('~/data/course_data', download=False, transform=preprocess)train_loader = torch.utils.data.DataLoader(cifar_10, batch_size=512, shuffle=True)plt.imshow(cifar_10[10][0].permute(1, 2, 0))for param in res_net.parameters(): param.requires_grad = False# Parameters of newly constructed modules have requires_grad=True by defaultnum_ftrs = res_net.fc.in_featuresres_net.fc = nn.Linear(num_ftrs, 10) # only update this part parameters criterion = nn.CrossEntropyLoss()# Observe that only parameters of final layer are being optimized as# opposed to before.optimizer_conv = optim.SGD(res_net.fc.parameters(), lr=0.001, momentum=0.9)# Decay LR by a factor of 0.1 every 7 epochslosses = []epochs = 10for epoch in range(epochs): loss_train = 0 for i, (imgs, labels) in enumerate(train_loader): print(i) outputs = res_net(imgs) loss = criterion(outputs, labels) optimizer_conv.zero_grad() loss.backward() optimizer_conv.step() loss_train += loss.item() if i &gt; 0 and i % 10 == 0: print('Epoch: {}, batch: {}'.format(epoch, i)) print('-- loss: {}'.format(loss_train / i)) losses.append(loss_train / len(train_loader)) Show Resnet 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import cv2import numpy as npimport torchfrom torchvision import transformsfrom torchvision.models import resnet18from torchsummary import summaryimport matplotlib.pyplot as pltdef show_one_model(model, input_, output): width = 8 fig, ax = plt.subplots(output[0].shape[0] // width, width, figsize=(20, 20)) for i in range(output[0].shape[0]): ix = np.unravel_index(i, ax.shape) plt.sca(ax[ix]) ax[ix].title.set_text('Filter-{}'.format(i)) plt.imshow(output[0][i].detach()) # plt.pause(0.05) input('this is conv: {}, received a {} tensor, press any key to continue: '.format(model, input_[0].shape)) plt.show() def main(img): &quot;&quot;&quot; Forward propagation, print feature maps during the transfer process &quot;&quot;&quot; # Define device, transforms transform = transforms.Compose([transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.ToTensor(), ]) # Process pictures, define models img = transform(img).unsqueeze(0) model = resnet18(pretrained=True) # Print model summary, which can be used for convolutional layer comparison summary(model, (3, 224, 224)) for p in model.parameters(): print(p) conv_models = [m for _, m in model.named_modules() if isinstance(m, torch.nn.Conv2d)] for conv in conv_models: conv.register_forward_hook(show_one_model) with torch.no_grad(): model(img) # conv_models = [m for _, m in model.named_modules() if isinstance(m, torch.nn.Conv2d)] # # first_conv = conv_models[0] # # show_one_model(first_conv, img, output=first_conv(img)) if __name__ == '__main__': img = cv2.imread('~/data/course_data/doo.png') main(img) &quot;&quot;&quot;---------------------------------------------------------------- Layer (type) Output Shape Param #================================================================ Conv2d-1 [-1, 64, 112, 112] 9,408 BatchNorm2d-2 [-1, 64, 112, 112] 128show more (open the raw output data in a text editor) ... -2.5093e-02, 6.7847e-03, -1.7868e-02, -7.8250e-04, -6.3448e-03], requires_grad=True)&quot;&quot;&quot; image-20210901152800418 image-20210901152815850 image-20210901152835421","link":"/example_10/"},{"title":"FREEDOM","text":"许很多的影片我都应该从新温习一遍。不只是因为我学的就是这个，更重要的是每次看一遍都能理解一些新的东西。 勇敢的心，1995年电影界最成功影片。先不去评论其他技术上的细节。我只是为一个名族英雄折服。freedom。 中国古话里就说过：生命诚可贵，爱情价更高，若为自由故，两者皆可拋。 不知道从什么时候开始，这句话被滥用了。人们总是以此来津津乐道自由的重要。歪曲的道理不能称之为道理。这里的自由，我根本不原意理解成为个人的自由。至少国有国法，家有家规这话我还不会去颠覆它。 任何环境总是有规矩才会成方圆，才会有乐趣。 自由，更深层的含义是民族上的。。。为本民族的自由而战，豪情万丈。所以梅尔.吉普森最后的”freedom”如此震撼人心。。 至此，我仍然相信，民族利益高于一切。一切政治上的形式主义都可以扔到一边。for my people.I will…","link":"/freedom/"},{"title":"G1 蓝牙传输更新 Bluetooth File Transfer 1.4","text":"不知道有没有人和我遇到了一样的问题!在G1rom更新到固件1.6以后,app2sd的Bluex1.12版本无法安装,而无app2sd版本的在安装后只能发送文件而无法打开蓝牙端口接收! 有兴趣的朋友可以到这里下载以后试试. 而这次寻到一个新的软件,版本为1.4版本,相信不是以前的bluex的版本更新,其实我自己也不确定是否为更新版本,因为图标已经换的很彻底了,并且内部结构也与以前大不一样!最重要的一点,原来支持文档类别存放,而现在却统一放在一个文件夹下!确实有些很不方便... 不过这个apk只需要在sdcard内直接安装就可以使用,需要提供root权限...而传输和接受都OK,并且比较顺畅! 另外,此软件我没有在hero的固件上测试,不知道是否支持hero,有兴趣的自己试试吧!而hero上的蓝牙传输其实有解决办法了...如下: More progress 11:04pm CST 8/26/09 Tracked down what calls the BTIP service, it's /system/lib/libandroid_runtime.so . Tried replacing it with a cupcake build, rebooted and ran into the issue where /system/framework/framework.jar is still referencing calls that were in the Hero libandroid_runtime.so . So replaced framework.jar and framework.odex from cupcake build and got the following error. D/AndroidRuntime( 1517): &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; AndroidRuntime START &lt; &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; D/AndroidRuntime( 1517): CheckJNI is OFF I/dalvikvm( 1517): DexOpt: mismatch dep signature for '/system/framework/core.odex' E/dalvikvm( 1517): /system/framework/framework.jar odex has stale dependencies I/dalvikvm( 1517): Zip is good, but no classes.dex inside, and no valid .odex file in the same directory D/libc-abort( 1517): abort() called in pid 1517 Any \"educated\" ideas? Questions &amp; Progress 01:09pm CST 8/17/09 So lately what I've been trying to do is find where a reference is made to actually call the BTIPS service. I've been lookiing in /system/framework and /data/app_s/Settings.apk but haven't found it yet. What I'm hoping to do is modify the file and have it call BT the same way cupcake did. Has anyone else found where a reference to \"btips\" is at? Settings.apk, which is what pops up when on home screen and you hit menu-&gt;settings, only makes a call to \"android:targetClass=\"com.android.settings.bluetoo th.BluetoothSettings\" Anywho, if you find it in any system libraries or framework files let me know. Please no PM's or posts about where you \"THINK\" it may be at. I've already tried the random guessing stuff, now I'm going through libraries one by one trying to find it. Some more notes 12:30pm CST 7/24/09 Here are some notes of interest. There are two versions of the /system/bin/bts daemon that are floating around on the Hero builds md5sum bts 29ffa46f12c01e3690690752b4e2d58d bts md5sum bts 5aeaca42d67d3b3c64ceda9ee4bfec1a bts There are also two versions of the TIInit_5.3.53.bts firmware files. One is actually just the brf6300.bin file renamed to match what Hero is looking for in /etc/firmware md5sum TIInit_5.3.53.bts d7a214bdb9b4fbc2b4e2dd7e3ab95df0 TIInit_5.3.53.bts md5sum TIInit_5.3.53.bts cb3d2ecbfc97c026a0dcceb8c959b7db TIInit_5.3.53.bts If you run \"strings\" on /system/bin/bts and grep for \"TII\" you'll be able to tell which firmware files that version supports TIInit_3.4.27.bts TIInit_4.2.38.bts TIInit_5.2.34.bts TIInit_5.3.53.bts TIInit_6.2.31.bts Nice picture illustrating BT architecture in Android 7:04pm CST 7/17/09 A note for ROM devs 02:27pm CST 7/17/09 Something to note, Hero does not use any of the following legacy services and therefore they can be removed from init.rc &amp; init.trout.rc . This is mainly something the ROM cookers should pay attention to. The btips service actually handles all of this now. REMOVE THE FOLLOWING: service hcid /system/bin/hcid -s -n -f /etc/bluez/hcid.conf socket bluetooth stream 660 bluetooth bluetooth socket dbus_bluetooth stream 660 bluetooth bluetooth # init.rc does not yet support applying capabilities, so run as root and # let hcid drop uid to bluetooth with the right linux capabilities group bluetooth net_bt_admin misc disabled service hciattach /system/bin/hciattach -n -s 115200 /dev/ttyHS0 texas 4000000 flow user bluetooth group bluetooth net_bt_admin disabled service hfag /system/bin/sdptool add --channel=10 HFAG user bluetooth group bluetooth net_bt_admin disabled oneshot service hsag /system/bin/sdptool add --channel=11 HSAG user bluetooth group bluetooth net_bt_admin disabled oneshot Found something 01:48pm CST 7/17/09 I was looking through init.trout.rc and noticed the following lines chown bluetooth bluetooth /sys/devices/platform/msm_serial_hs.0/serial_lock_cpu chmod 0660 /sys/devices/platform/msm_serial_hs.0/serial_lock_cpu This may not seem like much but this node does not actually exist in our builds. It's possible, and probably likely, that HTC modified their kernel to support the changes that were made in the bts (btips) daemon. We all are pretty much not using the HTC kernel, we're using custom compiled kernels from JAC or Cyanogen. I tried using the RUU kernel but couldn't boot at all. Is anyone able to get their phone booting off the RUU kernel and NOT one of the custom kernels that are floating around in these ROMs? If so, can you check if this device node exists? I believe booting off that kernel could be the answer to the UART clock issues I'm getting and missing devices in /sys . NEXT I have been toying around with the following value in init.rc that seems to affect whether or not I get an error. /proc/sys/net/unix/max_dgram_qlen The default is 10, the RUU release of Hero sets it to 999. If I change that to 10000 then it pauses the BT services and just sits there. If I revert to default I get the same error that I see when its set to 999. Wondering if there's a happy medium in queue length (qlen). Just me thinking out loud. Latest progress 11:43pm CST 7/15/09 I wanted to post some newer results I've been having with BT debugging on Hero. I found out how to circumvent the UART disable error. This is done by having the service btips statement in init.rc to look as follows service btips /system/bin/bts socket bluetooth stream 666 bluetooth bluetooth socket dbus_bluetooth stream 666 bluetooth bluetooth group bluetooth net_bt_admin root misc disabled oneshot The most important part is \"oneshot\" which tells Android to NOT restart the btips service after it dies. If you leave this off then it will relaunch btips service and tie up the I2C bus. The newest error I'm getting is the inability to launch HCI. This is hopefully the LAST error before I can get BT functional! Anyways, just wanted to update everyone that I have not stopped working on bluetooth. 1247718990.888806 BTSTACK(778) INFO | UATRAN: HCI Command was not acknowledged with an event [ vendor/ti/btips-linux/B_TIPS/btstack/hcitrans/uart/uarttran.c:298 ] 1247718990.889935 BTSTACK(778) INFO | HCI: HCI_Process detected transport failure [ vendor/ti/btips-linux/B_TIPS/btstack/stack/hci/hci_proc.c:1596 ] 1247718990.890179 BTSTACK(778) INFO | RADIOMGR: RmgrHciCallback: 0x6 [ vendor/ti/btips-linux/B_TIPS/btstack/stack/radiomgr.c:364 ] 1247718990.890362 BTSTACK(778) INFO | RADIOMGR: HCI init failed (retrying) [ vendor/ti/btips-linux/B_TIPS/btstack/stack/radiomgr.c:386 ] 1247718990.890484 BTSTACK(778) INFO | RADIOMGR: HCI init error [ vendor/ti/btips-linux/B_TIPS/btstack/stack/radiomgr.c:335 ] 1247718990.890637 BTSTACK(778) INFO | ME: HCI Init complete status: 22 [ vendor/ti/btips-linux/B_TIPS/btstack/stack/me/me.c:1220 ] 1247718990.890789 BTSTACK(778) INFO | CMGR: Received event HCI_INIT_ERROR [ vendor/ti/btips-linux/B_TIPS/btstack/profiles/common/conmgr.c:591 ] 1247718990.890942 BTSTACK(778) INFO | Dbus | inside _BTBUS_COMMON_BTL_callback with event: 6 0[ vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_common.c:62 ] 1247718990.893536 BTSTACK(778) INFO | sending dbus message from BTBUS_COMMON_BTL_callback in {vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_common.c:84}[ vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_utils.c:189 ] 1247718990.898022 BTSTACK(778) INFO | Dbus | _BTBUS_COMMON_BTL_callback signal sent: 6 0[ vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_common.c:87 ] 1247718990.898358 BTSTACK(778) FATAL | HCI Init Status Received while neither FM nor BT On in progress[ vendor/ti/btips-linux/EBTIPS/btl/ti_chip_mngr/ti_chip_mngr.c:1232 ] 1247718990.898541 BTSTACK(778) Assert | 0[ vendor/ti/btips-linux/EBTIPS/btl/ti_chip_mngr/ti_chip_mngr.c:1232 ] 1247718990.899121 BTSTACK(778) FATAL | signal 11 sent to our program from address 0xdeadbaad and code 1[ vendor/ti/btips-linux/EBTIPS/apps/btt_task.c:102 ] I'll update this main post as I, or others, come up with progress or advancements. The directories for this are already created in the latest Hero init.rc . Just need to create the ddb file touch /data/btips/TI/BtDeviceDb.ddb chmod 666 /data/btips/TI/BtDeviceDb.ddb The results of making these changes is you are able to get ALL bluetooth services and sockets created. Bluetooth is working from the commandline, just not on the frontend where we need it. PS:xda那边似乎有人已经放出hero shippment rom, 蓝牙问题应该已经解决了....静待佳音吧! :[download id=\"6\"] | skydrive 下载","link":"/g1-bluetooth-file-transfer-1-4/"},{"title":"RNN","text":"Simple RNN Define function Import the required libraries 12345678import ioimport osimport unicodedataimport stringimport globimport torchimport random 123# alphabet small + capital letters + &quot;.,;'&quot;ALL_LETTERS = string.ascii_letters + &quot;.,;'&quot;N_LETTERS = len(ALL_LETTERS) Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427 123456def unicode_to_ascii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in ALL_LETTERS ) 123456789101112131415161718192021def load_data(): # Build the category_lines dictionary, a list of names per language category_lines = {} all_categories = [] def find_files(path): return glob.glob(path) # Read a file and split into lines def read_lines(filename): lines = io.open(filename, encoding = 'utf-8').read().strip().split('\\n') return [unicode_to_ascii(line) for line in lines] for filename in find_files('~/data/course_data/names/*.txt'): category = os.path.splitext(os.path.basename(filename))[0] all_categories.append(category) lines = read_lines(filename) category_lines[category] = lines return category_lines, all_categories To represent a single letter, we use a “one-hot vector” of size &lt;1 x n_letters&gt;. A one-hot vector is filled with 0s except for a 1 at index of the current letter, e.g. \"b\" = &lt;0 1 0 0 0 ...&gt;. To make a word we join a bunch of those into a 2D matrix &lt;line_length x 1 x n_letters&gt;. That extra 1 dimension is because PyTorch assumes everything is in batches - we’re just using a batch size of 1 here. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# Find letter index from all_letters, e.g. &quot;a&quot; = 0def letter_to_index(letter): return ALL_LETTERS.find(letter) # Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensordef letter_to_tensor(letter): tensor = torch.zeros(1, N_LETTERS) tensor[0][letter_to_index(letter)] = 1 return tensor # Turn a line into a &lt;line_length x 1 x n_letters&gt;,# or an array of one-hot letter vectorsdef line_to_tensor(line): tensor = torch.zeros(len(line), 1, N_LETTERS) for i, letter in enumerate(line): tensor[i][0][letter_to_index(letter)] = 1 return tensor def random_training_example(category_lines, all_categories): def random_choice(a): random_idx = random.randint(0, len(a) - 1) return a[random_idx] category = random_choice(all_categories) line = random_choice(category_lines[category]) category_tensor = torch.tensor([all_categories.index(category)], dtype = torch.long) line_tensor = line_to_tensor(line) return category, line, category_tensor, line_tensor if __name__ == '__main__': print(ALL_LETTERS) print(unicode_to_ascii('Ślusàrski')) category_lines, all_categories = load_data() print(category_lines['Italian'][:5]) print(letter_to_tensor('J')) # [1, 57] print(line_to_tensor('Jones').size()) # [5, 1, 57] &quot;&quot;&quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,;'Slusarski['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])torch.Size([5, 1, 56])&quot;&quot;&quot; Second Example 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# Import the required librariesimport torchimport torch.nn as nnimport matplotlib.pyplot as pltclass RNN(nn.Module): # implement RNN from scratch rather than using nn.RNN def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(input_size + hidden_size, hidden_size) self.i2o = nn.Linear(input_size + hidden_size, output_size) self.softmax = nn.LogSoftmax(dim = 1) def forward(self, input_tensor, hidden_tensor): combined = torch.cat((input_tensor, hidden_tensor), 1) hidden = self.i2h(combined) output = self.i2o(combined) output = self.softmax(output) return output, hidden def init_hidden(self): return torch.zeros(1, self.hidden_size)category_lines, all_categories = load_data()n_categories = len(all_categories)n_hidden = 128rnn = RNN(N_LETTERS, n_hidden, n_categories)# one stepinput_tensor = letter_to_tensor('A')hidden_tensor = rnn.init_hidden()output, next_hidden = rnn(input_tensor, hidden_tensor)print(output.size())print(next_hidden.size())&quot;&quot;&quot;torch.Size([1, 18])torch.Size([1, 128])&quot;&quot;&quot;# whole sequence/nameinput_tensor = line_to_tensor('Albert')hidden_tensor = rnn.init_hidden()output, next_hidden = rnn(input_tensor[0], hidden_tensor)print(output.size())print(next_hidden.size())&quot;&quot;&quot;torch.Size([1, 18])torch.Size([1, 128])&quot;&quot;&quot;def category_from_output(output): category_idx = torch.argmax(output).item() return all_categories[category_idx] print(category_from_output(output))&quot;&quot;&quot;German&quot;&quot;&quot;criterion = nn.NLLLoss()learning_rate = 0.005optimizer = torch.optim.SGD(rnn.parameters(), lr = learning_rate)def train(line_to_tensor, category_tensor): hidden = rnn.init_hidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_to_tensor[i], hidden) loss = criterion(output, category_tensor) optimizer.zero_grad() loss.backward() optimizer.step() return output, loss.item() current_loss = 0all_losses = []plot_steps, print_steps = 1000, 5000n_iters = 100000for i in range(n_iters): category, line, category_tensor, line_tensor = random_training_example(category_lines, all_categories) output, loss = train(line_tensor, category_tensor) current_loss += loss if (i + 1) % plot_steps == 0: all_losses.append(current_loss / plot_steps) current_loss = 0 if (i + 1) % print_steps == 0: guess = category_from_output(output) corrent = 'CORRECT' if guess == category else f'WRONG ({category})' print(f'{i+1} {(i+1) / n_iters *100} {loss:.4f} {line} / {guess} {corrent}') &quot;&quot;&quot;5000 5.0 2.5063 Bureau / Scottish WRONG (French)10000 10.0 1.4726 Bitar / Arabic CORRECT15000 15.0 1.9405 Bazilevitch / Russian CORRECT20000 20.0 1.5565 Dupont / French CORRECT25000 25.0 0.1202 Majewski / Polish CORRECT30000 30.0 1.1579 Kucharova / Czech CORRECT35000 35.0 1.0075 Sheng / Chinese CORRECT40000 40.0 0.8343 Masih / Arabic CORRECT45000 45.0 0.5371 Fan / Chinese CORRECT50000 50.0 0.3260 Vinh / Vietnamese CORRECT55000 55.00000000000001 2.5464 Pahlke / Polish WRONG (German)60000 60.0 1.5921 Clark / Scottish CORRECT65000 65.0 4.3648 Paulis / Greek WRONG (Dutch)70000 70.0 1.3289 Thian / Vietnamese WRONG (Chinese)75000 75.0 2.2715 Kelly / English WRONG (Irish)80000 80.0 1.0069 Siu / Korean WRONG (Chinese)85000 85.0 0.8168 Kan / Chinese CORRECT90000 90.0 0.2283 Dinh / Vietnamese CORRECT95000 95.0 2.0048 Abbascia / Japanese WRONG (Italian)100000 100.0 0.6310 O'Shea / Irish CORRECT&quot;&quot;&quot;plt.figure()plt.plot(all_losses)plt.show() image-20210901183345054 12345678910111213141516171819202122232425262728293031def predict(input_line): print(f'\\n &gt; {input_line}') with torch.no_grad(): line_tensor = line_to_tensor(input_line) hidden = rnn.init_hidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) guess = category_from_output(output) print(guess) while True: sentence = input('Input: ') if sentence == 'quit': break predict(sentence)&quot;&quot;&quot; &gt; ChineseIrish &gt; EnglishEnglish &gt; JapaneseFrench &gt; FrenchGerman&quot;&quot;&quot; LSTM Modeling trigonometric functions Use LSTM to fit sine and cosine functions Use numpy to build time series data based on sine function Use keras to build a simple regression network, mainly using the LSTM network structure to fit the periodicity of the sine function, and visualize the fitted sine function image and the real function image Related knowledge points Time series data construction and forecasting Time series model building, training, evaluation and visualization based on keras LSTM 123456789101112131415161718# Import necessary libraries# Build dataimport numpy as np# Build a modelfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Inputfrom tensorflow.keras.layers import LSTMfrom tensorflow.keras.layers import Dense# Printing progress barfrom tqdm import tqdm# Visualizationimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline 1. Construct a data set This module will use numpy to construct time series data. There are two main steps: Define the sine function (cosine function) Select historical data window size to construct time series data 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def ground_func(x): &quot;&quot;&quot; sine / cosine function Args: x: numpy.ndarray return: sin(x) or cos(x) &quot;&quot;&quot; y = np.sin(x) return ydef build_data(sequence_data, n_steps): &quot;&quot;&quot; Use sine function data to build X, y Args: sine_data: numpy.ndarray n_steps: history data window size return: X: numpy.ndarray, y: numpy.ndarray &quot;&quot;&quot; # init X, y = [], [] seq_len = len(sequence_data) for start_idx in tqdm(range(seq_len), total=seq_len): end_idx = start_idx + n_steps if end_idx &gt;= seq_len: break cur_x = sequence_data[start_idx: end_idx] cur_y = sequence_data[end_idx] X.append(cur_x) y.append(cur_y) X = np.array(X) y = np.array(y) X = X.reshape(*X.shape, 1) return X, y # Construct the original sine/cosine function sequencexaxis = np.arange(-50 * np.pi, 50 * np.pi, 0.1)sequence_data = ground_func(xaxis)len(sequence_data)# Take 1000 data for visualizationplt.figure(figsize = (20, 8))plt.plot(xaxis[:1000], sequence_data[:1000]) image-20210901184030073 1234567n_steps = 20X, y = build_data(sequence_data, n_steps)X.shape, y.shape&quot;&quot;&quot; 99%|█████████▉| 3122/3142 [00:00&lt;00:00, 1557955.63it/s]((3122, 20, 1), (3122,))&quot;&quot;&quot; 2. Build the model This module builds a timing model based on the LSTM and Dense layer in keras. The following points need to be noted: 1. Choose the right hidden size 2. Choose a suitable activation function, such as relu, tanh 3. The optimizer chooses sgd, adam, etc. 3. The loss function chooses cross entropy loss function (cross_entropy) or mean square error (mse), etc. 123456789101112131415161718192021222324252627282930313233343536def create_model(): &quot;&quot;&quot; Build a LSTM model fit sine/cosine function. hints: 1. a LSTM fit time pattern (ref: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) 2. a Dense for regression (ref: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) &quot;&quot;&quot; model = Sequential() model.add(Input(shape = (20, 1))) model.add(LSTM(32, activation='tanh')) model.add(Dense(1, activation='tanh')) model.compile(optimizer = 'adam', loss = 'mse') return model# Initialize the model and print related informationmodel = create_model()model.summary()&quot;&quot;&quot;Instructions for updating:Call initializer instance with the dtype argument instead of passing it to the constructorModel: &quot;sequential&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================lstm (LSTM) (None, 32) 4352 _________________________________________________________________dense (Dense) (None, 1) 33 =================================================================Total params: 4,385Trainable params: 4,385Non-trainable params: 0_________________________________________________________________&quot;&quot;&quot; 3. Model training 12345678910111213141516# Try to change epochs and add callbacks, such as EarlyStopping (https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)history = model.fit(X, y, batch_size = 32, epochs = 25, verbose = 1)plt.plot(history.history['loss'], label='loss')plt.legend(loc ='upper right') # draw the loss image&quot;&quot;&quot;Instructions for updating:Use tf.where in 2.0, which has the same broadcast rule as np.whereEpoch 1/253122/3122 [==============================] - 4s 1ms/sample - loss: 0.1433Epoch 2/253122/3122 [==============================] - 3s 879us/sample - loss: 0.0072show more (open the raw output data in a text editor) ...Epoch 25/253122/3122 [==============================] - 3s 858us/sample - loss: 2.2191e-05&quot;&quot;&quot; image-20210901184239175 4. Forecast This module uses a function different from the training data to construct test data to verify the generalization performance of the model. The main steps are as follows: 1. Define a new function (sine/cosine) 2. Use the trained model to make predictions 3. Visually compare model prediction results with real values 123456789101112131415161718192021222324252627282930313233343536def test_func(x): &quot;&quot;&quot; sine/cosine function, different from ground_func above. Args: x: numpy.ndarray return: sin(x) or cos(x) &quot;&quot;&quot; y = np.cos(x) return y test_xaxis = np.arange(0, 10 * np.pi, 0.1)test_sequence_data = test_func(test_xaxis)# Use the initial n_steps of historical data to start forecasting, and the subsequent data will use the predicted data as historical data for further forecastingy_preds = test_sequence_data[:n_steps]# Step by step forecastfor i in tqdm(range(len(test_xaxis)-n_steps)): model_input = y_preds[i: i+n_steps] model_input = model_input.reshape((1, n_steps, 1)) y_pred = model.predict(model_input, verbose = 0) y_pred = np.append(y_preds, y_pred)plt.figure(figsize = (10,8))plt.plot(test_xaxis[n_steps:], y_preds[n_steps:], label ='predictions')plt.plot(test_xaxis, test_sequence_data, label ='ground truth')plt.plot(test_xaxis[:n_steps], y_preds[:n_steps], label ='initial sequence', color ='red')plt.legend(loc ='upper left')plt.ylim(-2,2)plt.show()&quot;&quot;&quot;100%|██████████| 295/295 [00:01&lt;00:00, 183.91it/s]&quot;&quot;&quot; image-20210901184333044 Recurrent Neural Networks source 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import pandas as pd# load datatimeserise_revenue = pd.read_csv('~/data/course_data/time_serise_revenue.csv')sales_data = pd.read_csv('~/data/course_data/time_serise_sale.csv')timeserise_revenue.head()&quot;&quot;&quot; Unnamed: 0 day_1 day_2 day_3 day_4 day_5 day_6 day_7 day_8 day_9 ... day_51 day_52 day_53 day_54 day_55 day_56 day_57 day_58 day_59 day_600 0 2.622866 2.657832 2.771121 2.815845 2.876267 2.859229 2.844758 2.793797 2.736443 ... 1.228701 1.290414 1.474886 1.563295 1.736197 1.797285 1.978940 2.198979 2.277908 2.403300...4 4 1.702631 1.825995 2.038047 2.194083 2.313903 2.417883 2.567613 2.650782 2.729691 ... 1.258760 1.137150 1.109007 1.104999 1.150137 1.204513 1.221350 1.327023 1.387304 1.5573635 rows × 61 columns&quot;&quot;&quot;def sample_from_table(sample_size, dataframe): sample_row = dataframe.sample().values[0] begin_column = random.randint(0, len(sample_row) - sample_size - 1) return (sample_row[begin_column: begin_column + sample_size], sample_row[begin_column + 1: begin_column + sample_size + 1]) import torchimport torch.nn as nnfrom torch.nn import functional as Ffrom torch.autograd import Variablefrom torch import optimimport numpy as npimport math, randomimport matplotlib.pyplot as pltimport seaborn as sns# Generating a noisy multi-sin waveclass FullyConnected(nn.Module): def __init__(self, x_size, hidden_size, output_size): super(FullyConnected, self).__init__() self.hidden_size = hidden_size self.linear_with_tanh = nn.Sequential( nn.Linear(10, self.hidden_size), nn.Tanh(), nn.Linear(self.hidden_size, self.hidden_size), nn.Tanh(), nn.Linear(self.hidden_size, output_size) ) def forward(self, x): yhat = self.linear_with_tanh(x) return yhat class SimpleRNN(nn.Module): def __init__(self, x_size, hidden_size, n_layers, batch_size, output_size): super(SimpleRNN, self).__init__() self.hidden_size = hidden_size self.n_layers = n_layers self.batch_size = batch_size # self.inp = nn.Linear(1, hidden_size) self.rnn = nn.RNN(x_size, hidden_size, n_layers, batch_first=True) self.out = nn.Linear(hidden_size, output_size) # 10 in and 10 out def forward(self, inputs, hidden=None): hidden = self.__init__hidden() # print('Forward hidden {}'.format(hidden.shape)) # print('Forward inps {}'.format(inputs.shape)) output, hidden = self.rnn(inputs.float(), hidden.float()) # print('Out1 {}'.format(output.shape)) output = self.out(output.float()) # print('Forward outputs {}'.format(output.shape)) return output, hidden def __init__hidden(self): hidden = torch.zeros(self.n_layers, self.batch_size, self.hidden_size, dtype = torch.float64) return hidden # Set datasetsource_data = sales_data# Fully Connected Modeln_epochs = 100n_iters= 50hidden_size = 2 # try to change this parametersn_layers = 2batch_size = 5seq_length = 10n_sample_size = 50x_size = 1fc_model = FullyConnected(x_size, hidden_size, output_size = seq_length)fc_model = fc_model.double()criterion = nn.MSELoss()optimizer = optim.SGD(fc_model.parameters(), lr = 0.01)losses = np.zeros(n_epochs)plt.imshow(fc_model.state_dict()['linear_with_tanh.0.weight'])plt.show() image-20210901184546617 1234567891011121314151617181920212223242526272829303132333435for epoch in range(n_epochs): for iter_ in range(n_iters): _inputs, _targets = sample_from_table(n_sample_size, source_data) inputs = Variable(torch.from_numpy(np.array([_inputs[0:10], _inputs[10:20], _inputs[20:30], _inputs[30:40], _inputs[40:50]], dtype = np.double))) targets = Variable(torch.from_numpy(np.array([_targets[0:10], _targets[10:20], _targets[20:30], _targets[30:40], _targets[40:50]], dtype = np.double))) outputs = fc_model(inputs.double()) optimizer.zero_grad() loss = criterion(outputs, targets) loss.backward() optimizer.step() losses[epoch] += loss if iter_ % 10 == 0: plt.clf() plt.ion() plt.title('Epoch {}, iter {}'.format(epoch, iter_)) plt.plot(torch.flatten(outputs.detach()), 'r-', linewidth = 1, label = 'Output') plt.plot(torch.flatten(targets), 'c-', linewidth = 1, label = 'Label') plt.plot(torch.flatten(inputs), 'g-', linewidth = 1, label = 'Input') plt.draw() plt.pause(0.05) image-20210901184606015 image-20210901184620638 A total of 5 * 99 pictures were rendered in the middle, so I won’t show them one by one. image-20210901184826841 RNN Model 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263n_epochs = 100n_iters = 50hidden_size = 2 # try to change this parametersn_layers = 2batch_size = 5seq_length = 10n_sample_size = 50x_size = 1output_size = 1rnn_model = SimpleRNN(x_size, hidden_size, n_layers, int(n_sample_size / seq_length), output_size)criterion = nn.MSELoss()optimizer = optim.SGD(rnn_model.parameters(), lr = 0.01)losses = np.zeros(n_epochs)for epoch in range(n_epochs): for iter in range(n_iters): _inputs, _targets = sample_from_table(n_sample_size, source_data) inputs = Variable(torch.from_numpy(np.array([_inputs[0:10], _inputs[10:20], _inputs[20:30], _inputs[30:40], _inputs[40:50]], dtype = np.double)).unsqueeze(2)) targets = Variable(torch.from_numpy(np.array([_targets[0:10], _targets[10:20], _targets[20:30], _targets[30:40], _targets[40:50]], dtype = np.double)).unsqueeze(2).float()) # [49] # print('Inputs {}, targets {}'.format(inputs.shape, targets.shape)) # Use teacher forcing 50% of the time # force = random.random() &lt; 0.5 outputs, hidden = rnn_model(inputs.double(), None) optimizer.zero_grad() loss = criterion(outputs, targets) loss.backward() optimizer.step() losses[epoch] += loss if iter % 10 ==0: plt.clf() plt.ion() plt.title('Epoch {}, iter {}'.format(epoch, iter)) plt.plot(torch.flatten(outputs.detach()), 'r-', linewidth = 1, label = 'Output') plt.plot(torch.flatten(targets), 'c-', linewidth = 1, label = 'Label') plt.plot(torch.flatten(inputs), 'g-', linewidth = 1, label = 'Input') plt.draw() plt.pause(0.05)# if epoch &gt; 0:# print(epoch, loss) image-20210901184915753 A total of 5 * 99 pictures were rendered in the middle, so I won’t show them one by one. image-20210901184958237 12plt.plot(losses[20:])plt.show() image-20210901185025707","link":"/example_08/"},{"title":"G1 app2sd 完全教程","text":"声明1:你所需要的软件在这里可以下载的到! 声明2.app2sd虽然可以省却手机内存,但是也有许多不便的地方!操作后SDcard就是机子的一部分,不能随便摘取.我用的4G的卡,在机子挂在后存储有问题!不知道其他卡如何.所以在存储文件和音乐的时候还是需要用到读卡器,而这个时候我必须选择关机!直接卸载SDcard会造成机子程序出错!而不得不从新执行一遍app2sd的过程!并且执行过后也会存在一些不可知的问题!如果对稳定性比较看重的人这里可以飘过了! 声明3.我的sdcard已经在手机内通过!懒得再刷,所以没有用我的card抓图!本教程图片多为网上现成图片来完成!而图片不是一个地方抓取的!所以图片上的容量会有差距.但是刷机过程没有错 从新格盘,正好用自己的图!顺便说一下,ubuntu下的默认抓图真恶心!每抓一张都要从新启动一次程序! 所需要的准备的工作: 1.SDcard(必须) 2.分区软件(必须,windows下可以使用Acronis Disk Director Suite,支持vista.linux下可以直接利用终端分区!) 3.Android SDK(非必须,可以再网上下载Terminal Emulator.apk,安装后在手机上输入adb下的指令完成操作!) 首先我们要将SDcard分区,分成fat32和ext2,至于ext3是否可行我没有测试过,有兴趣的可以试试并且留言告诉我测试报告! 我选择的是在ubuntu的终端执行,这样操作比较靠谱.而在windows下的分区软件不是很稳定!会造成诸多不可见的错误! windows下的分区软件有Acronis Disk Director Suite以及PartitionManager,至于分区魔术师可以略过,因为它不支持分区SDcard.Acronis Disk Director Suite软件分区可以移步到此查看! 以USB内存卡方式插上电脑，或者用读卡器插上电脑 像我的ubuntu，它会自动挂载你的卡。 把东西备份好，然后卸载。一定要卸载，不然无法分区 启动ubuntu或者您的linux系统,在终端内输入如下代码: dmesg //查看所连接的设备! 可以看到sdb或者sdc之类的设备名称!假设我以下操作都为sdc设备! sudo fdisk /dev/sdc //这里需要说明,如果linux下非root,必须要输入sudo来取得root权限进行操作.以下类同! p是显示当前分区 n是创建 d是删除 w是应用你的操作 doo@ubuntu:~# sudo fdisk /dev/sdc Command (m for help): d &lt; ==删除当前分区 Command (m for help): p &lt;==显示一下，确定已删除 Disk /dev/sdc: 3965 MB, 3965714432 bytes 122 heads, 62 sectors/track, 1024 cylinders Units = cylinders of 7564 * 512 = 3872768 bytes Disk identifier: 0x9dfd42a5 Device Boot Start End Blocks Id System Command (m for help): Command (m for help): m &lt; ==查看帮助 Command action a toggle a bootable flag b edit bsd disklabel c toggle the dos compatibility flag d delete a partition l list known partition types m print this menu n add a new partition o create a new empty DOS partition table p print the partition table q quit without saving changes s create a new empty Sun disklabel t change a partition's system id u change display/entry units v verify the partition table w write table to disk and exit x extra functionality (experts only) Command (m for help): n &lt;==新建分区，选择主分区 Command action e extended p primary partition (1-4) p Partition number (1-4): 1 &lt;==指定该主分区为1号 First cylinder (1-1024, default 1): &lt;==敲回车，直接使用SD卡的最开头 Using default value 1 Last cylinder or +cylinders or +sizeK(K,M,G) (1-1024, default 1024): +3300M &lt;==填入分区的大小 Command (m for help): n &lt;==新建分区，选择扩展分区(所有逻辑分区加起来就是扩展分区) Command action e extended p primary partition (1-4) p Partition number (1-4): 2 &lt;==扩展分区的序号是2 First cylinder (895-1024, default 895): &lt;==敲回车，直接接着剩余空间的最开头 Using default value 895 Last cylinder or +cylinders or +sizeK(K,M,G) (895-1024, default 1024): &lt;==敲回车，用默认的，使用全部剩余空间 Using default value 1024 Command (m for help): Command (m for help):p Disk /dev/sdc: 3965 MB, 3965714432 bytes 122 heads, 62 sectors/track, 1024 cylinders Units = cylinders of 7564 * 512 = 3872768 bytes Device Boot Start End Blocks Id System /dev/sdc1 1 894 733792+ 83 Linux /dev/sdc2 729 1024 272128+ 83 Linux 创建好两个分区后, 我们还需要用命令t修改分区卷标, 选择分区1改卷标为c 命令为 Command (m for help):t t &lt; ==修改卷标 partition number (1-4): 1 &lt;==输入1来制定第一个分区. Hex code (type L to List codes): c &lt;==输入C来制定卷标 Changed system type of partition 1 to c (W95 FAT32 (LBA)) Command (m for help): w &lt;==将缓冲写入SD卡,应用你的操作 The partition table has been altered! Calling ioctl() to re-read partition table. WARNING: If you have created or modified any DOS 6.X partitions, please see the fdisk manual page for additional information. Syncing disks. doo@ubuntu:~# doo@ubuntu:~# sudo ls /dev/sdc* &lt; ==查看分区情况 /dev/sdc /dev/sdc1 /dev/sdc2 doo@ubuntu:~#sudo mkfs.vfat /dev/sdc1 &lt;==格式化第一个主分区。 mkfs.vfat 3.0.1 (23 Nov 2008) doo@ubuntu:~# sudo mkfs.ext2 /dev/sdc2 &lt;==格式化第二个分区 mke2fs 1.41.4 (27-Jan-2009) warning: 139 blocks unused Filesystem laber= OS type: Linux Block size=1024 (log=0) Fragment size=1024 (log=0) 123360 inodes, 491521 blocks 24583 blocks (5.00%) reserved for the super user First data block=1 Maximum filesyetem blocks=67633152 68 block groups 8192 blocks per group, 8192 fragments per group 2856 inodes per group Superblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409 Writing inode tables: done Writing superblocks and filesystem accounting information: done This filesystem will be automatically checked every 38 mounts or 180 days.whichever comes first. Use tune2fs -c or -i to override. doo@ubuntu:-$ 分区完毕后ubuntu会自动挂在两个盘符.表示成功! 然后需要手机必须为app2sd版本的rom,在windows 命令提示符下输入命令查看: 以下步骤必须安装android sdk.(其实一下步骤不一定需要在windows cmd下进行,在网上下载一个android的终端Terminal Emulator.apk,然后启动此程序在手机内输入以下指令是一样的!只是在sdcard的系统盘下建立app文件夹并挂载到android rom上! ) C:and Settings&gt;cd c:&lt; ==cd到sdk adb.exe C:&gt;adb devices &lt; ==查看连接的硬件和设备 List of devices attached 000000000000 device &lt;==分区过硬盘以后连接会显示000000000000 的硬件号 C:&gt;adb shell # su &lt; ==如果你还没有取得root权限,那么这一步通不过. su # ls /system &lt;==查看一下system目录下的文件夹 ls /system lib framework media fonts etc customize build.prop usr bin xbin app sd lost+found busybox df -h &lt; ==查看系统盘情况!如果分区成功,那么会在android的系统下显示分区.如下我的385.8M的分区在android的系统内!再往下是sdcard的系统!如果没有那表示分区失败.当然还有一种可能就是你的手机不是app2sd rom busybox df -h Filesystem Size Used Available Use% Mounted on tmpfs 48.3M 0 48.3M 0% /dev tmpfs 4.0M 12.0k 4.0M 0% /sqlite_stmt_journals /dev/block/mtdblock3 67.5M 67.5M 0 100% /system /dev/block/mtdblock5 74.8M 30.4M 44.3M 41% /data /dev/block/mtdblock4 67.5M 1.2M 66.3M 2% /cache /dev/block/mmcblk0p2 385.8M 2.0k 366.5M 0% /system/sd &lt; ==由于在ubuntu下分区后手机内读取sdcard出错,所以后便又分了一次!但是没有抓图,所以容量上和上图有差距.再者本身linux和windows读取SDcard的容量上就有不同! /dev/block//vold/179:1 3.3G 4.0k 3.3G 0% /sdcard # mkdir /system/sd/app &lt;==建立sdcard分区上的app文件夹!如果以前sdcard曾做过app2sd,那么这个文件夹是存在的!会有命令符提示文件夹存在! mkdir /system/sd/app # cd /data cd /data # cp -a app /system/sd/app cp -a app /system/sd/app # rm -r app rm -r app # ln -s /system/sd/app /data/app ln -s /system/sd/app /data/app # reboot reboot 手机自动重启后就OK了.放心安装你所想要的apk程序吧! 顺便说一句:ubuntu的9.04快要放出正式版了!欢迎大家下载试用.","link":"/g1-app2sd/"},{"title":"G1上打造Hero!(更新tips&amp;app)","text":"这本来是安卓上发布的一片帖子,我写在这里主要是为了为自己增加点浏览量!顺便解解眼馋.但是我并不打算在这里提供教程和下载! (由于flickr最近不稳定,所以图片显现不出来!)是由于博客上的flickr插件的原因.. 有兴趣的看这里吧.... 教程中需要的是C6的卡,但是我的是金士顿东京原厂8G卡.试试用一下吧!稍后发试用报告!","link":"/g1-hero-tipsapp/"},{"title":"CNN","text":"The source code: example_09: CNN CNN Principle 123456789101112import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom struct import unpackfrom torchvision.datasets import MNISTfrom sklearn.linear_model import LogisticRegressionimport torchfrom PIL import Imagefrom torch import nnmnist_dataset_train = MNIST(root = '~/data/course_data', train=True, download = True)mnist_dataset_test = MNIST(root = '~/data/course_data', train=False, download = True) The first machine vision problem: Let the computer automatically distinguish between 0 and 6 123456789101112X_train = mnist_dataset_train.data.numpy()y_train = mnist_dataset_train.targets.numpy()X_test = mnist_dataset_test.data.numpy()y_test = mnist_dataset_test.targets.numpy()&quot;&quot;&quot;Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gzDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ~/data/course_data/MNIST/raw/train-images-idx3-ubyte.gz9913344it [00:02, 4759648.85it/s] ...5120it [00:00, 12492633.21it/s]Extracting ~/data/course_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ~/data/course_data/MNIST/raw&quot;&quot;&quot; Explain CNN principles 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140def conv(image, filter_): # Convolution operation print(image.shape) print(filter_.shape) assert image.shape[-1] == filter_.shape[-1] test_image = image height, width = filter_.shape[0], filter_.shape[1] filter_result = np.zeros(( test_image.shape[0]-height + 1, test_image.shape[1]-width + 1 )) for h in range(test_image.shape[0]-height + 1): for w in range(test_image.shape[1]-width + 1): sub_windows = test_image[h: h + height, w: w + width, :] op = np.sum(np.multiply(sub_windows, filter_)) filter_result[h][w] = op return filter_result# Part 2: Strides&quot;&quot;&quot;Try to modify stride in Conv Function&quot;&quot;&quot;# Part3: Pooling&quot;&quot;&quot;Create a pooling cell for conv&quot;&quot;&quot;# Part4: Volume&quot;&quot;&quot;Create 3-d volume filter&quot;&quot;&quot;# Part5: Fully Connected Layers&quot;&quot;&quot;Create Fully Connected Layer, to flatten&quot;&quot;&quot;# Part6: Cross-Entropy&quot;&quot;&quot;Create Cross-Entropy cell to get loss value&quot;&quot;&quot;# Part7: ResNet&quot;&quot;&quot;Why we need resNet, and its functions&quot;&quot;&quot;class ResBlock(nn.Module): &quot;&quot;&quot; A very basic ResNet unit The unit passed: batch normal The output value retains the original input value, so that our result does not dissipate &quot;&quot;&quot; def __init__(self, n_channel): super(ResBlock, self).__init__() self.conv = nn.Conv2d(n_channel, n_channel, kernel_size = 3, padding=1, bias = False) self.bath_norm = nn.BatchNorm2d(num_features = n_channel) torch.nn.init.constant_(self.bath_norw.weight, 0.5) torch.nn.init.zeros_(self.bath_norm.bias) torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity ='relu') # sum(windows * filter) ==&gt; The larger the windows, the larger the added value, the smaller the windows, the smaller the value def forward(self, x): out = self.conv(x) out = self.conv(out) out = self.bath_norm(out) out = torch.relu(out) return out + x if __name__ == '__main__': image = Image.open('~/data/course_data/doo.jpeg') image_array = np.array(image) plt.imshow(image_array) # Robert 算子 rebert_1_kernel = np.array([ [1, 0], [0, -1] ]) robert_2_kernel = np.array([ [0, 1], [-1, 0] ]) #Sobel 算子 sobel_x_kernel = np.array([ [-1, 0, 1], [-2, 0, 2], [-1, 0, 1] ]) sobel_y_kernel = np.array([ [-1, -2, -1], [0, 0, 0], [1, 2, 1] ]) # Laplacian 算子 laplacian_kernel = np.array([ [0, 1, 0], [1, -4, 1], [0, 1, 0] ]) filters = [ np.array([sobel_x_kernel] * 3), np.array([sobel_y_kernel] * 3), np.array([laplacian_kernel] * 3) ] for i, f in enumerate(filters): print('applying filter: {}'.format(i)) plt.subplot(3, 3, i * 3 + 1) plt.imshow(image_array) filter_result = conv(image_array, f) plt.subplot(3, 3, i * 3 + 2) plt.imshow(filter_result) plt.subplot(3, 3, i * 3 + 3) plt.imshow(filter_result, cmap = 'gray')plt.show()#ResNet&quot;&quot;&quot;applying filter: 0(1931, 1931, 3)(3, 3, 3)applying filter: 1(1931, 1931, 3)(3, 3, 3)applying filter: 2(1931, 1931, 3)(3, 3, 3)&quot;&quot;&quot; output Identification codes Train a model to classify and recognize the characters in the verification code, and finally complete the verification code recognition The data set used contains a total of 36 characters from 0-9 and AZ. There are 50 pictures for each character in the training set, and 10 pictures for each character in the verification set. The verification code data set is composed of 4 character pictures taken out randomly. become. Related knowledge points Data Reading Use torch to build, train, and verify models Model prediction and image segmentation analyze Question 1-Establish a character comparison table We can reverse each pair of keys and values by traversing the dictionary and store them in a new dictionary. The sample code is as follows: 1new_dict = {v: k for k, v in old_dict.items()} #### Question 2-Define datasets and dataloader In opencv-python, you can use image = cv2.medianBlur(image, kernel_size) for median filtering. #### Question 3-Define the network structure In torch, the convolution and fully connected layers are defined as follows: 12conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)fc = nn.Linear(in_features, out_features, bias) #### Question 4-Define the model training function The model training process of the torch framework includes operations such as clearing the gradient, forward propagation, calculating the loss, calculating the gradient, and updating the weight, among which: 1. Clear the gradient: the purpose is to eliminate the interference between step and step, that is, use only one batch of data loss to calculate the gradient and update the weight each time. Generally can be placed first or last; 1. Forward propagation: use a batch of data to run the process of forward propagation to generate model output results; 1. Calculate the loss: use the defined loss function, model output results and label to calculate the loss value of a single batch; 1. Calculate the gradient: According to the loss value, calculate the gradient value required in this optimization in the ownership of the model; 1. Update weight: Use the calculated gradient value to update the value of all weights. The sample code of a single process is as follows: 12345&gt;&gt;&gt; optimizer.zero_grad() # Clear the gradient (can also be placed in the last line)&gt;&gt;&gt; output = model(data) # forward propagation&gt;&gt;&gt; loss = loss_fn(output, target) # Calculate loss&gt;&gt;&gt; loss.backward() # Calculate the gradient&gt;&gt;&gt; optimizer.step() # update weight Programming Import the library to be used in this project 123456789101112131415import osimport cv2import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.utils.data import Dataset, DataLoaderimport torchvisionimport torchvision.transforms as transformsimport numpy as npimport pickleimport PILimport matplotlib.pyplot as pltfrom PIL import Imageos.environ['KMP_DUPLICATE_LIB_OK'] = 'True' Understanding the data set Define the data path 123train_data_dir = '~/data/course_data/train_data.bin'val_data_dir = '~/data/course_data/val_data.bin'verification_code_dir = '~/data/course_data/verification_code_data.bin' The data set used is stored in a binary file, and we need to define a function to read the picture in the binary file. 1234def load_file(file_name): with open(file_name, mode ='rb') as f: result = pickle.load(f) return result See what the data set looks like: 123456789101112train_data = load_file(train_data_dir)img_test = list()for i in range(1, 1800, 50): img_test.append(train_data[i][1])plt.figure()for i in range(1, 37): plt.subplot(6, 6, i) plt.imshow(img_test[i-1]) plt.xticks([]) plt.yticks([])plt.show() image-20210901131659902 View single big picture 12345# plt.subplot(6, 6, i)plt.imshow(train_data[500][1])plt.xticks([])plt.yticks([])plt.show() image-20210901131741884 It can be seen that there is a lot of noise in the character picture, and the noise will have an adverse effect on the model prediction result, so we can use a specific filter to eliminate the picture noise during data preprocessing. Question 1-Establish a character comparison table A simple observation shows that there are no duplicates in the key and value in the character dictionary just defined. Therefore, the key and value in the dictionary can be reversed so that we can use the value to find the key (convert the model prediction result into a readable character) Now you need to complete the following code to reverse the keys and values in the dictionary (for example: dict={'A':10,'B':11} and get new_dict={10:'A ',11:'B'} 12345char_dict = {'0':0,'1':1,'2':2,'3':3,'4':4,'5':5,'6':6,'7':7,'8':8,'9':9,\\ 'A':10,'B':11,'C':12,'D':13,'E':14,'F':15,'G':16,'H':17,'I':18,'J':19,'K':20,'L':21,'M':22,\\ 'N':23,'O':24,'P':25,'Q':26,'R':27,'S':28,'T':29,'U':30,'V':31,'W':32,'X':33,'Y':34,'Z':35 }new_char_dict = {v : k for k, v in char_dict.items()} Question 2-Define datasets and dataloader We need to use torch.utils.data.Dataset as the parent class to define our own datasets in order to standardize our own datasets. 123456789101112131415class iDataset(Dataset): def __init__(self, file_name, transforms): self.file_name = file_name # file name self.image_label_arr = load_file(self.file_name) # read binary file self.transforms = transforms # Image converter def __getitem__(self, index): label, img = self.image_label_arr[index] img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) # Convert the picture to grayscale img = cv2.medianBlur(img, 5) # Use median blur to remove image noise img = self.transforms(img) # Transform the image return img, char_dict[label[0]] def __len__(self): return len(self.image_label_arr) Now we can define transform and dataloader. 12345678910transform = transforms.Compose([transforms.ToPILImage(), transforms.Resize([28, 28]), # Adjust the image size to 28*28 transforms.ToTensor(), # Convert the picture to tensor transforms.Normalize(mean = [0.5], std = [0.5])]) # Perform normalization processingtrain_datasets = iDataset(train_data_dir, transform)train_loader = DataLoader(dataset=train_datasets, batch_size=32, shuffle = True)val_datasets = iDataset(val_data_dir, transform)val_loader = DataLoader(dataset=val_datasets, batch_size = 32, shuffle = True) Question 3-Define the network structure After the data is ready, we need to define a simple convolutional neural network. The input of the neural network is [batchsize,chanel(1),w(28),h(28)], and the output is 36 categories. Our neural network will use 2 convolutional layers with 2 fully connected layers. The parameter settings of these four layers are shown in the following table (the default parameters can be used directly if they are not marked): 1. conv1: in_chanel=1, out_chanel=10, kernel_size=5 1. conv2: in_chanel=10, out_chanel=20, kernel_size=3 1. fc1: in_feature=2000, out_feature=500 4. fc2: in_feature=500, out_feature=36 1234567891011121314151617181920212223class ConvNet(nn.Module): def __init__(self): super().__init__() # TODO: self.conv1 = nn.Conv2d(1, 10, 5) self.conv2 = nn.Conv2d(10, 20, 3) self.fc1 = nn.Linear(20 * 10 * 10, 500) self.fc2 = nn.Linear(500, 36) def forward(self, x): # inputsize: [b, 1, 28, 28] in_size = x.size(0) # b out = self.conv1(x) out = F.relu(out) out = F.max_pool2d(out, 2, 2) out = self.conv2(out) out = F.relu(out) out = out.view(in_size, -1) out = self.fc1(out) out = F.relu(out) out = self.fc2(out) out = F.log_softmax(out, dim = 1) return out Question 4-Define the model training function Next, we need to complete the model training function to achieve the following operations: 1. Clear the gradient 1. Forward propagation 1. Calculate the gradient 1. Update weights 12345678910def train(model, train_loader, optimizer, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if(batch_idx + 1) % 10 == 0: print('Train Epoch: {} [{} / {} ({:.0f} %)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) Define model test function 123456789101112def test(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: output = model(data) test_loss += F.nll_loss(output, target, reduction = 'sum') pred = output.max(1, keepdim = True)[1] correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%) \\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) Define model and optimizer We define the model structure we just built as model and choose to use the Adam optimizer. 12model = ConvNet()optimizer = optim.Adam(model.parameters()) Model training and testing We can first set the number of epochs to 3 and perform model training to see how accurate the model is and whether it meets the requirements of verification code recognition. If the model accuracy is not enough, you can also try to adjust the number of epochs and retrain. 123456789101112131415161718192021222324252627282930EPOCHS = 3for epoch in range(1, EPOCHS + 1): train(model, train_loader, optimizer, epoch) test(model, val_loader) &quot;&quot;&quot;Train Epoch: 1 [288 / 1800 (16 %)] Loss: 3.340514Train Epoch: 1 [608 / 1800 (33 %)] Loss: 2.872326Train Epoch: 1 [928 / 1800 (51 %)] Loss: 1.977929Train Epoch: 1 [1248 / 1800 (68 %)] Loss: 1.098688Train Epoch: 1 [1568 / 1800 (86 %)] Loss: 0.535660Test set: Average loss: 0.2888, Accuracy: 328/360 (91%) Train Epoch: 2 [288 / 1800 (16 %)] Loss: 0.072813Train Epoch: 2 [608 / 1800 (33 %)] Loss: 0.139866Train Epoch: 2 [928 / 1800 (51 %)] Loss: 0.109487Train Epoch: 2 [1248 / 1800 (68 %)] Loss: 0.058259Train Epoch: 2 [1568 / 1800 (86 %)] Loss: 0.013144Test set: Average loss: 0.0099, Accuracy: 360/360 (100%) Train Epoch: 3 [288 / 1800 (16 %)] Loss: 0.010245Train Epoch: 3 [608 / 1800 (33 %)] Loss: 0.004797Train Epoch: 3 [928 / 1800 (51 %)] Loss: 0.002203Train Epoch: 3 [1248 / 1800 (68 %)] Loss: 0.006250Train Epoch: 3 [1568 / 1800 (86 %)] Loss: 0.005230Test set: Average loss: 0.0028, Accuracy: 360/360 (100%)&quot;&quot;&quot; Define model test function 123456789101112def test(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: output = model(data) test_loss =+ F.nll_loss(output, target, reduction = 'sum') pred = output.max(1, keepdim = True)[1] correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print('\\nTest set: Average loss: {:.4f}, Accuracy : {}/{} ({:.0f}%) \\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) Define model and optimizer We define the model structure we just built as model and choose to use the Adam optimizer. 12model = ConvNet()optimizer = optim.Adam(model.parameters()) Model training and testing We can first set the number of epochs to 3 and perform model training to see how accurate the model is and whether it meets the requirements of verification code recognition. If the model accuracy is not enough, you can also try to adjust the number of epochs and retrain. 1234567891011121314151617181920212223242526272829EPOCHS = 3for epoch in range(1, EPOCHS + 1): train(model, train_loader, optimizer, epoch) test(model, val_loader)&quot;&quot;&quot;Train Epoch: 1 [288 / 1800 (16 %)] Loss: 3.508450Train Epoch: 1 [608 / 1800 (33 %)] Loss: 3.288610Train Epoch: 1 [928 / 1800 (51 %)] Loss: 2.584805Train Epoch: 1 [1248 / 1800 (68 %)] Loss: 1.180833Train Epoch: 1 [1568 / 1800 (86 %)] Loss: 0.564084Test set: Average loss: 0.0088, Accuracy : 316/360 (88%) Train Epoch: 2 [288 / 1800 (16 %)] Loss: 0.173177Train Epoch: 2 [608 / 1800 (33 %)] Loss: 0.043262Train Epoch: 2 [928 / 1800 (51 %)] Loss: 0.054462Train Epoch: 2 [1248 / 1800 (68 %)] Loss: 0.052596Train Epoch: 2 [1568 / 1800 (86 %)] Loss: 0.013714Test set: Average loss: 0.0006, Accuracy : 360/360 (100%) Train Epoch: 3 [288 / 1800 (16 %)] Loss: 0.004590Train Epoch: 3 [608 / 1800 (33 %)] Loss: 0.007654Train Epoch: 3 [928 / 1800 (51 %)] Loss: 0.004135Train Epoch: 3 [1248 / 1800 (68 %)] Loss: 0.003140Train Epoch: 3 [1568 / 1800 (86 %)] Loss: 0.003019Test set: Average loss: 0.0001, Accuracy : 360/360 (100%) &quot;&quot;&quot; The model has been trained! Does the test set accuracy of the last epoch exceed 99%? Identification codes After successfully implementing the digital recognition, we can start the verification code recognition! First, import the verification code data set: 1verification_code_data = load_file(verification_code_dir) Let's choose a picture at random (Figure 6) to see what the verification code looks like. 1234image = verification_code_data[6]IMG = Image.fromarray(cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB))plt.imshow(IMG)plt.show() image-20210901132522728 Let's take a look at what effect the median filter can have on the captcha image. 123img = cv2.medianBlur(image.copy(), 5)plt.imshow(img)plt.show() image-20210901132551457 Finally, let us look at the actual results of verification code recognition: 1234567891011121314151617181920212223242526272829IMAGES = list()NUMS = list()for img in verification_code_data: IMAGES.append(img) img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) image_1 = img[:, :80] image_2 = img[:, 80:160] image_3 = img[:, 160:240] image_4 = img[:, 240:320] img_list = [image_1, image_2, image_3, image_4] nums = [] for one_img in img_list: one_img = transform(one_img) one_img = one_img.unsqueeze(0) output = model(one_img) nums.append(new_char_dict[torch.argmax(output).item()]) NUMS.append('Verification_code: '+ ''.join(nums))plt.figure(figsize = (20, 20))plt.subplots_adjust(wspace = 0.2, hspace=0.5)for i in range(1, 11): plt.subplot(5, 2, i) plt.title(NUMS[i-1], fontsize = 25, color = 'red') plt.imshow(IMAGES[i - 1]) plt.xticks([]) plt.yticks([])plt.show() image-20210901132631369","link":"/example_09/"},{"title":"Gmail location in signature","text":"labs的新功能,在你的邮件当中显示你所在的地理坐标! 我开始还有些许疑问,到底是内嵌地图还是别的显示方式...那如果收取方不是Gmail的邮箱是否就不能现实等等! 启用后发现原来是在签名栏上多加了一条信息:Sent from: Cangzhou Hebei China. 其实这个功能不能说是无用,但是我觉得类似鸡肋! 怎么说呢...至少在国内这个功能比较无用.这仅仅是我的感觉!","link":"/gmail-location-in-signature/"},{"title":"Gmail 的转发上限","text":"在Gmail中创建转发过滤器是有上限的,这个估计没有多少人知道!我想也是因为没有多少人有实际的需求… 今天我在Twitter上抱怨Gmail里的邮件过多,占用了很大一部分空间,以至于我的Gmail空间已经临近上限… 也终于迫使我开始清理邮件! ###问题产生:### 在清理的时候发现,除了一些比较大附件的邮件以外,大部分占用空间的邮件都是一些广告邮件! 我很少设置过滤器删除广告邮件,特别是一些推送服务的广告,不过大多都是直接略过收件箱存档而已,有空了还可以去看看…这也使得我这几年来邮箱里布满了此类邮件! 当然,清理的过程还是比较顺利的,毕竟这些邮件的时效性的原因,这些邮件删除起来一点也不心疼! 那么,对于新邮件该怎么处理呢,总不能继续占用存储空间吧.也不好直接删除,毕竟有的时候我还是要看看的. 那么,过滤器这个时候就起作用了! 我的想法很简单,既然QQ邮箱空间是无上限的,而广告邮件又无关乎隐私问题,所以可以放心的转发到QQ空间进行保存!所以对一些广告邮件设置了forward filter…然后问题来了,当我建立到一定的数量的时候,Gmail开始提醒我“转发邮件地址过多,无法创建过滤器 这可怎么办,总该是有解决办法的!于是查阅Gmail的帮助手册发信,Gmail是支持布尔运算符来查找邮件的!而这些布尔运算符一样可以创建到filter里. OK,问题到这里就变得简单了… ###解决方案:### 比如说,我要将 renren.com kaixin001.com 高朋 卓越 美团 拉手等邮件过滤直接forward到QQ邮箱里,那么其实创建一条filter就够了!按如下格式renren.com OR kaixin001.com OR 高朋 OR 卓越 OR 美团 OR 拉手,其中“OR”是Gmail中所支持的布尔运算符,而且必须大写. 延伸阅读: 希望这篇文可以帮助更多的人更好的利用Gmail.更详尽的运算符可以查阅Gmail的帮助手册","link":"/gmail-forward-upper-limit/"},{"title":"Advanced Deep Learning","text":"Different optimer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npimport torchx = np.random.random(size=(100, 8))linear = torch.nn.Linear(in_features=8, out_features=1)sigmoid = torch.nn.Sigmoid()linear2 = torch.nn.Linear(in_features=1, out_features=1)model = torch.nn.Sequential(linear, sigmoid, linear2).double()train_x = torch.from_numpy(x)print(model(train_x).shape)yture = torch.from_numpy(np.random.uniform(0, 5, size=(100, 1)))# print(x)print(yture.shape)&quot;&quot;&quot;torch.Size([100, 1])torch.Size([100, 1])&quot;&quot;&quot;loss_fn = torch.nn.MSELoss()optimer = torch.optim.SGD(model.parameters(), lr=1e-5)for e in range(100): for b in range(100 // 1): # stochastic gradient descent # for b in range(100 // 10): # mini-batch gradient descent # for b in range(100 // 100): # batch gradient descent batch_index = np.random.choice(range(len(train_x)), size=20) yhat = model(train_x[batch_index]) loss = loss_fn(yhat, yture[batch_index]) loss.backward() print(loss) optimer.step()&quot;&quot;&quot;tensor(5.0873, dtype=torch.float64, grad_fn=&lt;MseLossBackward&gt;)tensor(3.4337, dtype=torch.float64, grad_fn=&lt;MseLossBackward&gt;)show more (open the raw output data in a text editor) ...tensor(2.1481, dtype=torch.float64, grad_fn=&lt;MseLossBackward&gt;)&quot;&quot;&quot; Matrix dimension 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from torch import nnimport torchimport numpy as npx = torch.from_numpy(np.random.random(size=(4, 10)))print(x.shape)&quot;&quot;&quot;torch.Size([4, 10])&quot;&quot;&quot;model = nn.Sequential( nn.Linear(in_features=10, out_features=5).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Softmax())ytrue = torch.randint(8, (4, ))print(ytrue)&quot;&quot;&quot;tensor([4, 0, 7, 7])&quot;&quot;&quot;loss_fn = nn.CrossEntropyLoss()print(model(x).shape)print(ytrue.shape)loss = loss_fn(model(x), ytrue)print(torch.randint(5, (3, )))loss.backward()for p in model.parameters(): print(p, p.grad) Advanced deep learning 123456789101112131415# Basic computing libraryimport numpy as np# Deep learning libraryimport torchimport torch.nn as nnimport torch.optim as optimimport torchvisionimport torch.nn.functional as Fimport torchvision.transforms as transforms# Auxiliary drawing galleryimport matplotlib.pyplot as plt# Time operation libraryimport time# Progress bar control libraryfrom tqdm import tqdm Project 1: Forward propagation of simple neural network Question 1: Define the initial parameters and activation function You need to use numpy to implement the forward propagation process of the neural network and calculate the final output result of the output layer. In order to complete the above tasks, we need to make the following assumptions: 1. The value entered is [3,5] 1. The two weights of the hidden layer h1 are [2,4], [4,-5] 1. The two weights of the hidden layer h2 are [-1,1], [2,2] 1. The weight of the output layer is [-3,7] 1. All layers do not use bias 1. All hidden layers need to add tanh activation function 12345678910111213141516# TODO: Define a numpy array with the input data of the neural network:input_data = np.array([3, 5])# TODO: Define a numpy array with the content of the hidden layer and output layer weights of the neural network:# Tips: The weight dictionary has been built, you only need to fill in the corresponding value according to the hidden layer nameweights = {'h11': np.array([2, 4]), 'h12': np.array([4, -5]), 'h21': np.array([-1, 1]), 'h22': np.array([2, 2]), 'out': np.array([-3, 7])}# TODO: Improve the following tanh activation function:def tanh(x): return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) Question 2: Calculate the neural network output layer by layer In the calculation of the neural network, it is necessary to first multiply the weight of the layer to be calculated with its input data, and then sum, and then through the operation of the activation function, it can be output to the next layer. Below we will use the layer as the unit to perform calculations: The first is the first hidden layer. You need to multiply, sum, and input the data of the input layer and the weight of the hidden layer into the activation function. 123456789101112131415161718print(input_data * weights['h11'])a = tanh(input_data * weights['h11']).sum()b = tanh((input_data * weights['h11']).sum())print(a,b)&quot;&quot;&quot;[ 6 20]1.9999877116507956 1.0&quot;&quot;&quot;# TODO: multiply, sum, and input the data of the input layer and the weight of the first hidden layer into the activation function.hidden_11_value = tanh(input_data * weights['h11']).sum()hidden_12_value = tanh(input_data * weights['h12']).sum()hidden_1_output = np.array([hidden_11_value, hidden_12_value])&quot;&quot;&quot;1.9999877116507956-7.550282621338056e-11[ 1.99998771e+00 -7.55028262e-11]&quot;&quot;&quot; Next is the second hidden layer, the operation of this layer is exactly the same as the previous layer. 12345# TODO: multiply, sum, and input the data output by the upper layer and the weight of the second hidden layer into the activation function.hidden_21_value = tanh(hidden_1_output * weights['h21']).sum()hidden_22_value = tanh(hidden_1_output * weights['h22']).sum()hidden_2_output = np.array([hidden_21_value, hidden_22_value]) Finally, there is the output layer. At this time, there is only one node that needs to be calculated, and there is no need to add an activation function. 12# TODO: multiply and sum the data output by the upper layer and the weight of the output layeroutput = (hidden_2_output * weights['out']).sum() At this point, you have completed all the calculations. Now let's print out the output of these layers and have a look. 1234print(output)&quot;&quot;&quot;9.887385002294863&quot;&quot;&quot; Project 2: CIFAR-10 Image Classification Preparation The data set used in this project can be directly exported from the torchvision library. Here are some basic data operations (data download may take a few minutes, please be patient). 1234567891011121314151617181920212223242526272829##Define various transformation operations on the image, including converting the array to tensor, and regularizing the image#transforms.Compose is mainly used for some common graphics transformations, such as cropping and rotation#Traverse the list array and perform each transforms operation on the img in turntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.48216, 0.44653), (0.24703, 0.24349, 0.26159))))#Export the CIFAR10 data set in torchvision. The root is the directory where the data is stored after downloading. The train controls whether it is in the training phase, the download controls whether it needs to be downloaded, and the transform passes in a series of image transformations.trainset = torchvision.datasets.CIFAR10(root='~/data/course_data/', train=True, download=True, transform=transform)testset = torchvision.datasets.CIFAR10(root='~/data/course_data/', train=False, download=True, transform=transform)#Used to divide the training data into multiple groups, this function throws a group of data each time.trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)#Used to divide the test data into multiple groups, this function throws a group of data each time.testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False)&quot;&quot;&quot;Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ~/data/course_data/cifar-10-python.tar.gz170499072it [02:24, 1181561.38it/s] Extracting ~/data/course_data/cifar-10-python.tar.gz to ~/data/course_data/Files already downloaded and verified&quot;&quot;&quot; After the data download is complete, we can simply check the data label to see if it is correct with the data set in the exercise description. 12345678910111213trainset.classes&quot;&quot;&quot;['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']&quot;&quot;&quot; Let's check the data image again. 123456789101112131415161718192021222324252627282930#Display the pictures visually#Define drawing functiondef imshow(inp, title = None): &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot; # Define the canvas for drawing fig = plt.figure(figsize = (30, 30)) # Convert the dimensions of the picture inp = inp.numpy().transpose((1,2,0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) # Standardize the picture inp = std * inp + mean # The value of the entire image array is limited to the specified value a_min, and a_max inp = np.clip(inp, 0, 1) # Visual display of pictures plt.imshow(inp,)# Get a batch of datainputs, classes = next(iter(trainloader))# Display in grid format, the function is to combine several images into one imageout = torchvision.utils.make_grid(inputs)# plt.imshow() can display the picture and also display its formatimshow(out, title = [trainset.classes[x] for x in classes]) Question 1: Build a simple neural network After the data is ready, you need to build a simple neural network. 12345678910111213141516# TODO: define a layer 3 fully connected neural network, the input dimension is 32*32*3, the output dimension of the first layer is 1000, the output dimension of the second layer is 500, and the output dimension of the third layer is 10class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(32*32*3, 1000) self.fc2 = nn.Linear(1000, 500) self.fc3 = nn.Linear(500, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) return self.fc3(x)# Instantiate the neural network classnet = Net() After the model structure is defined, the loss function and optimizer need to be determined. 12345# Define loss function-cross entropycriterion = nn.CrossEntropyLoss()# Define the optimizer, pass the parameters of the neural network to the optimizer, and define the learning rateoptimizer = optim.Adam(net.parameters(), lr = 3e-4) Question 2: Neural Network Training The main content of the model has been completed, and the training can be carried out below. In the process of model training, the following steps are generally followed: Big for loop-epochs, used to manage a set of data loop training several times Small for loop-step, used to retrieve data from dataloader in batchsize unit Clear the gradient of the optimizer Read in data and label, and perform shape transformation (can be done or not) Run the forward propagation process of the model Generate the final result based on the model output Calculate the loss Calculate the gradient based on the loss Update parameters based on gradient 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# TODO: training modelnum_epochs = 10since = time.time()net.train()for epoch in range(num_epochs): print(f'Epoch {epoch + 1} / {num_epochs}') running_loss = 0.0 running_corrects = 0 # Take out each batch of data in a loop from the trainloader for data in tqdm(trainloader): # TODO: Completion code inputs, labels = data inputs = inputs.view(-1, 32 * 32 * 3) optimizer.zero_grad() outputs = net(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) loss.backward() optimizer.step() # Calculation of the loss function of a batch of data running_loss += loss.item() * inputs.size(0) # Calculation of the accuracy of a batch of data running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / trainloader.dataset.data.shape[0] epoch_acc = running_corrects.double() / trainloader.dataset.data.shape[0] print('train loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc)) print('-' * 20)time_elapsed = time.time()-sinceprint('Trainning complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed% 60))&quot;&quot;&quot;Epoch 1 / 10100%|██████████| 3125/3125 [01:04&lt;00:00, 48.74it/s]train loss: 1.6377 Acc: 0.4185--------------------Epoch 2 / 10100%|██████████| 3125/3125 [01:04&lt;00:00, 48.15it/s]train loss: 1.4254 Acc: 0.4962--------------------Epoch 3 / 10100%|██████████| 3125/3125 [01:06&lt;00:00, 47.29it/s]train loss: 1.3065 Acc: 0.5372--------------------Epoch 4 / 10100%|██████████| 3125/3125 [01:04&lt;00:00, 48.76it/s]train loss: 1.2026 Acc: 0.5729--------------------Epoch 5 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 49.98it/s]train loss: 1.1129 Acc: 0.6033--------------------Epoch 6 / 10100%|██████████| 3125/3125 [01:01&lt;00:00, 51.17it/s]train loss: 1.0252 Acc: 0.6343--------------------Epoch 7 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 49.67it/s]train loss: 0.9373 Acc: 0.6668--------------------Epoch 8 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 49.63it/s]train loss: 0.8545 Acc: 0.6936--------------------Epoch 9 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 50.02it/s]train loss: 0.7770 Acc: 0.7242--------------------Epoch 10 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 50.16it/s]train loss: 0.7020 Acc: 0.7492--------------------Trainning complete in 10m 33s&quot;&quot;&quot; Question 3: Model evaluation After completing the model training, the model needs to be evaluated to verify the accuracy of the model on the test set. Tips: In the model training log, the accuracy acc is also printed, but this is the accuracy of the model on the training set, not the accuracy on the test set. You can observe the accuracy of the training set and the accuracy of the test set to see if there is any difference. 12345678910111213141516# TODO: Complete model evaluationcorrect, total = 0, 0net.eval()for data in tqdm(testloader): inputs, labels = data inputs = inputs.view(-1, 32 * 32 * 3) outputs = net(inputs) _, predicted = torch.max(outputs, 1) total += labels.size(0) correct += (predicted == labels).sum().item()print('The testing set accuracy of the network is: %d %%'% (100 * correct / total))&quot;&quot;&quot;100%|██████████| 625/625 [00:03&lt;00:00, 157.71it/s]The testing set accuracy of the network is: 53 %&quot;&quot;&quot;","link":"/example_07/"},{"title":"G1 提权刷机顺序","text":"本人主要是写给在我博客中留言的@leo 以及和他一样刷机无从入手的朋友!技术是一直在更新的,老的技术文档虽然不太适用了,但是参考价值是一定有的!所以也可以翻看我之前的文档,对比本文参看! 长久不用就是会忘记哈...唉..已经很久没有从源头开始刷起了! 拿起公司的黑色美版G1,按一直以来的刷机程序来刷,傻眼了....recovery的版本不对,是最早的版本,机子虽然是1.6版本,但是没有提权,无法获得root权限.也就根本无从刷机. 所以需要从新捡起以前的知识来从源头开始刷起..这就需要一个逻辑性的顺序问题.也就是我们需要将G1降级到RC29或者RC7版本,RC29对应的是美版第一个文件,而我的白色英版对应的则是RC7. 顺序上应该是: 将手机降级并获取root权限 push recovery.img,获取testkey 恢复出厂设置 升级所需要版本. 升级RC版本 升级G2版本 升级APP2SD版本 在第一步就卡着了,源头版本文件没有保存.各大论坛也都没有存档了!找的比较费劲,所以有些文件还是需要保存的.... 具体可以参看博客内的相关链接! 另外,关于技术文档上比较重要的一点就是,在获取testkey以后,最好更换recovery的版本,因为以前的版本选项比较少,只能将刷机包更名为update.zip来进行刷机,而在后期的版本中加入了list,可以选择根目录内的zip包来进行刷机而无需命名为update,并且可以wipe system以及sdcard等多个内容.这就是所说的full wipe,现在很多rom都需要full wipe才可以刷机,避免出错! 并且更重要的是,在后期版本中的recovery中加入了ext3 to ext4的更换分区功能.玩过linux的应该对这个分区很熟悉!而在黑色美版中我刷入了1.61版的recovery,依旧刷的1.6版本rom.","link":"/g1-mentioned-order-of-the-right-brush/"},{"title":"Google+ 研究","text":"已經忘記今天是第幾天試用Google+了，這兩天圈內當然免不了的大部分都在討論Google+，而什麼所謂的G+應用技巧，什麼25條關於G+的，還有什麼50條，100條類似的！反正不止是G+內部，包括twitter，facebook，微博以及博客圈內，大家都在樂此不彼的討論Google+！ 可是我這裡不想討論一些技術上的東西，也不想討論它是否真的能夠打敗Facebook或者twitter！我只想從真正的社交上去分析一下我自己的對G+內部細微的探討！ 找不到好圖，又懶得自己做。所以借用一下+lucifr Liu的圖！反正都做了，表浪費！原圖有 PSD文檔可以下載！我按照自己的標準稍稍修改了一下！ GooglePlus 讓我們先來探討一下G+中的circles，雖然所有的社交網絡中都有這樣一種概念，但是沒有Google那樣去深挖它，延伸它！在多部分的社交網絡中，或許都是在模仿Facebook，或許就是在模仿Twitter！而這兩者的基本機制就是相互添加對方為好友，才算開始真正的社交！當然，Twitter的機制要鬆散一點，即便雙方沒有相互fo對方或者只有一方fo了，一樣可以互相聊天！而Facebook對於社交圈子的概念就相對封閉了一點，如果只有一方fo了對方，那麼一樣不構成相互溝通的條件！ 而比較尷尬的是，Facebook作為一個最大的SNS網站，基本上老老少少都在上邊進行基礎社交，這也構成了現在年輕人逐步放棄Facebook的主因！因為這個如果都相互fo了對方，那麼這個circles的範圍就太大了。只要我發佈了一條消息，我的老師，我的父母，我的同學以及我的基友，大家都可以看到這條消息！而在真實的社交範圍中，這是不可想像的！比如，包二奶這裡就是個十分不適合的場所！當然，我們可以私信！！！！！！好吧，我舉的例子有些屎，我承認！那麼讓我們想像一下，總有些事情是不希望父母或者另外的人知道的，而只想在小範圍內的圈子內傳播的！那麼，Google+給了你這個實現的方式，就是circles概念的區分和交集！這個還需要你對聯繫人進行具體的整理才可以！有些人，即是朋友又是同事，這就是兩個circles的交集，那麼我一條信息不管發佈在我的朋友圈還是同事圈，這個人都可以收到我的信息！加入說，同事圈裏又有些話題需要討論而不想讓老闆看到該如何，讓我們建立一個將老闆排除在外的同事圈！ 大概瞭解了Circles的概念以後，我想討論的就是Google+的一些細節的地方！我們都知道，無論是微博還是SNS社交網站比如開心，人人網都會有Share分享功能！這個功能的作用在於，看到好的東西以後，可以直接分享給自己的社交圈裏的所有人觀看！這也是網絡病毒營銷的根源！而在Google+中，我們注意到一個細節，並且進行了一番討論。Share這個功能被分割了。我們以大家熟悉的微博為例，當我Share某話題以後，如果原帖被修改或者刪除，那麼我Share過去的內容也會跟著變動！而且原帖會被注明轉帖和評論了多少次！在G+中就完全不是這樣，當我Share某個帖子後，這個帖子就變成我複製過去的了！只會聲明原帖出自什麼人。並且評論不會跟著原帖一起變動，而我在新Share的帖子下邊回覆，也不被記錄在原帖的評論裏！這就是我們所討論的被分割！可是我們思考一下真實社交圈的情況，我們從某個人那裡聽到了一個消息，然後四處散播！當那個人再對同樣的圈子內修正了這個消息或者否定了這個消息以後，我所散播出去的圈子并不能收到這樣一個反饋，那麼，只有我自己去再去修正一下自己所說過的話！那麼也就是，每個人都必須對自己所說的話和所散播的言論負責！而當你所散播了消息以後，即便和你之前聽到的是一模一樣的，那麼這個討論也只限制在你自己的圈子內而和原來的圈子無關！這個說明，Google+在細節上真的是在充分考慮真實社交的環境而在努力模擬再現！ 大家也都發現，G+可以被當作郵箱來用！當你發表一篇消息給某個特定的人或者一個圈子的人。那麼對方會收到一個消息。這個消息最終如何接受有三種情況！第一種情況，對方沒有G+帳號，那麼這條消息肯定是直接發佈到對方郵箱裏！第二種情況，對方有G+帳號而你所寫接收方是聯繫人或者圈子，並且對方有關閉郵件提醒，那麼對方會在G+上接收到這條信息。第三種情況就是對方有G+帳號並且沒有關閉郵件提醒，那麼就是G+和郵箱都會收到這條消息！ 不過，對於給沒有G+帳戶的人用G+發佈邀請還可以，如果要當作邮箱来用，還是奉勸大家不要這樣做！因為G+發佈郵件的郵箱並不是你的郵箱，是隨機生成的一個郵箱地址，並不是固定的！也就是說對方即便收到也無法回覆！而我做了一個實驗，給我的163，QQ，以及yahoo和gmail郵箱都各發了一個郵件！有的郵件無影無蹤了，而即便收到的郵件，也是等了很久以後才收到的！所以，如果要發郵件的話，請點擊黑又長上邊+You後邊的Gmail選項，老老實實的用Gmail發郵件吧！ PS：此處之後還會不斷更新，分享我對Google+的一些想法！當然，是有了感悟後才會來更新！以後關於Google+的研究如無特殊情況也都在本帖內討論！想到哪寫到哪，不分那麼多條條框框了！ 其實，關於Google+，還有很多值得研究的細節，Google也在不斷完善！可以看得出這真的是一個很有誠意的互聯網產品！相應號召，大家都搬家吧！","link":"/google-research/"},{"title":"Google Translate","text":"一些比较有意思的发音,自己点击传送门去听! 一段DJ 不雅段子1 春哥纯爷们儿，铁血真汉子，人民好兄弟，父亲好儿子。 莫装b，装b遭雷劈 呃,这个是....","link":"/google-translate/"},{"title":"Google  Android 4.0","text":"10,19日上午十点左右,Google,三星在香港联合举办了发布会,推出了下一代Android系统(Ice Cream Sandwich)Android4.0,以及下一代Nexus智能手机,三星Galaxy Nexus Prime. 这次的软件开发包也在这次大会宣布可以下载使用,在developer,android.com上. 这次更新重新设计了浏览器的界面,改进了Gmail功能,增强了照相和摄影.可以进入编辑模式,使用各种系统自带的滤镜效果和其他编辑操作! 当然,我最欣赏的改进就是这个全新分享功能 一种全新的分享方法，Andriod Bean，使得我们分享变得非常简单。比如说现在他有一个Andriod的手机，把手机对过去想和我分享，就是通过Andriod Bean来分享，把两个手机背靠背的放在一起内容就过来了，这个文章就已经过来了，就是这样轻轻一碰。我想看一下Google Map，现在我看到的是一个地图，在香港弥敦道上面的，现在要把这个地图直接发到我的手机上，同样是把两个手机背靠背放在一起，我手机上也出现了同样的地图。再和大家分享最后一个例子，他在玩一个很酷的游戏，我手机上没有，但是我也很想玩这个游戏，虽然我也可以到Andriod市场上浏览、下载，但是我选择用更酷的方式来做。也是把手机背靠背的放在一起，然后就直接把我带到Andriod市场上的这个游戏页面上，我就不用浏览了，直接下载就可以了。Andriod Bean可以让我们加入某一个社交群体、分享图片、内容，极大的释放了我们的想象力。 关于更新问题,怀疑这次N1这个亲儿子估计也应该可以赶上末班车,因为Google官方不是声明,但凡运行Android2.3的机子,都可以更新并运行4.0么!现在能做的就是坐等CM大神了!","link":"/google-android-40/"},{"title":"Gphone SD卡分区(更新)","text":"以前写过一个Gphone app2sd的完全教程,在那个教程当中有完整的分区步骤,可是那个分区模式是需要linux系统才可以执行,而我当时用的是ubuntu. 现在可行的分区模式也就是linux下,然后windows下的分区软件.譬如PartitionManager,AcronisDiskDirector,不要去想PQ魔术师,那个不支持sd卡分区!就这么多了么?...其实,如果你有Gphone手机的话,完全可以用windows dos来分区! 先说条件: 1.Gphone手机 2.SDcard 3.android sdk 4.一条usb连接线 步骤: 首先开机进入recovery模式，按ALT+X进入“console” 打开cmd,输入: adb shell parted /dev/block/mmcblk0 print 可以看到分区的情况,一般来说都是一个分区,如果以前做过app2sd那么就是两个...删除这两个分区.如下图为三个分区: 然后输入代码删除分区: rm 1 rm 2 rm 3 (如果只有两个分区或者一个分区的,执行一步操作就可以了,也就是rm&lt; 数字&gt;) 在完成后就是一个未分区的SDcard 然后对SDcard重新分区,注意需要根据你的卡大小来分配各分区的大小,一般linux-swap最大32M.ext分区500M足够了,最大不要超过1GB.不过似乎有将linux-swap分成96Mb的...输入以下代码分区: mkpartfs primary fat32 0 7445 mkpartfs primary ext2 7445 7873 300-500都可 mkpartfs primary linux-swap 7873 7969 务必是96M 不然你有C6卡也不能全速体验HERO了... 至此,分区工作就完成了.可以输入print来检查一下. 以下为可执行可不执行步骤...就是将ext2转换为ext3/ext4 转换为ext3输入以下命令: upgrade_fs 转换ext4输入以下命令: tune2fs -O extents,uninit_bg,dir_index /dev/block/mmcblk0p2 e2fsck -fpDC0 /dev/block/mmcblk0p2 upgrade_fs 结束以后,输入 parted /dev/block/mmcblk0 print 验证是否升级到ext3/ext4 然后quit退出,重启手机. 本教程参考了安卓网上安装hero的步骤","link":"/gphone-sdcard/"},{"title":"habari近期进度","text":"其实在装上habari,解决了导入wordpress和乱码问题以后,就一直没有什么进度了! 这两天一直在寻找能有效将wordpress功能平滑过度到habari的有效方法,只可惜我的水平确实有限.对代码一点都不了解.没办法,只能寻找一些其他途径希望能绕过这部分.可惜的是habari的程序似乎并没有那么简单. 在研究themes的时候发现,默认模板总有两个是可以实现截断的.那么在截断这个问题上就是themes在起作用.可是在查看了默认模板以后真的是很失望,在代码上我根本无计可施,没有任何头绪可以让我找到具体是哪里实现了截断的功能! 而在换用模板以后,发现天佑提供的archives插件已经实现了在wordpress上实现的archives页面.这点似乎还是蛮简单的.并不需要在后台专门建立一个页面模板,而只用在添加页面的时候增加一段天佑提供的代码就可以实现. 那么对于tag页面以及其他功能性的页面是否一样如此简单...不得而知,因为我具体没有做其他测试! 在css上,我有意使用现在所用的模板,而期望通过修改css来更改页面展示的宽度.我想,在css上大致应该是相通的.那么,这就将是我的下一步工作! 其实到现在为止,剩下的工作就全部都是themes和plugins了.修改代码,移植而已!...如果能弄通了,那么在不远的将来就可以像驾驭wordpress一样驾驭habari了.期待ing....","link":"/habari-progress/"},{"title":"Hello 2012","text":"以2012起头,其实主要是想要写2011 今天是2011年的最后一天了,明天正式步入毁灭年!谁知道去,也许是真的呢...这样的社会,早点灭了的好! 总体来说,今年是非常不顺的一年.不管是对世界,对中国,还是对我来说... 由于微博的兴旺,各种事情仿佛一瞬间被放大,被拉近...而我们忽然发现,世界原来可以如此精彩...不过这精彩的代价,确实让人有些心痛!这一年里,最欣喜的事情就是在Google Plus里看@变态辣椒的漫画.真是一个好地方啊... 不过这个月初开始不太愿意在那里发言,主要原因还是发现其实放在什么地方,都会有脑残的出现! 而最让人头疼的是,脑残们都不会觉得自己是脑残,根本说不通道理...而他们那些偏激的想法,让我感到恐惧! 是的,如果人类经历一次灭顶之灾,而忽然发现自己周围仍然在酝酿这种灭顶之灾的苗头,你会感到恐惧! 所幸,临到结尾,@韩寒 抛出了三篇惹人口风的文章...看来丫是想给大家留个值得咀嚼的话题在年内!而在问内提到里\"清算\"这样的字眼... 唉,说多了,其实也是废话! 况且说, 我这里真不适合谈论这些.","link":"/hello-2012/"},{"title":"Logistic regression to diagnose heart disease","text":"The preject source code url : Heart load data 1234567891011121314import pandas as pdfrom sklearn.linear_model import LogisticRegressionfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_splitdata = pd.read_csv('./data/heart.csv')# the csv url: https://github.com/hivandu/colab/blob/master/AI_Data/data/heart.csv# Print a brief summary of the data setdata.info()data.shapedata.target.value_counts() The params meaning 123456789101112131415Params Meaning age 年龄 sex 性别(1 = 男性, 0 = 女性)cp 胸部疼痛类型(值1：典型心绞痛，值2：非典型性心绞痛，值3：非心绞痛，值4：无症状）trestbps 血压 chol 胆固醇 fbs 空腹血糖（&gt; 120 mg/dl，1=真；0=假） restecg 心电图结果（0=正常，1=患有ST-T波异常，2=根据Estes的标准显示可能或确定的左心室肥大） thalach 最大心跳数 exang 运动时是否心绞痛（1=有过；0=没有）oldpeak 运动相对于休息的STslop 心电图ST segment的倾斜度(值1:上坡，值2:平坦，值3:下坡） ca 透视检查看到的血管数 thal 缺陷种类（3=正常；6=固定缺陷；7=可逆缺陷）target 是否患病（0=否，1=是） Perform analysis 12345678910111213141516171819202122232425# Change the &quot;sex&quot; column into two columns &quot;sex_0&quot; and &quot;sex_1&quot;sex = pd.get_dummies(data['sex'], prefix = 'sex') # Add &quot;sex_0&quot; and &quot;sex_1&quot; to the data set. data = pd.concat([data, sex], axis = 1)# And delete the sex column.data = data.drop(columns = ['sex'])# Print out the first five lines. Check whether sex_0, sex_1 are added successfully, and whether sex is deleted successfully.data.head()# Get sample labeldata_y = data.target.valuesdata_y.shape# Get sample feature setdata_x = data.drop(['target'], axis = 1)data_x.shape# Divide the data settrain_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size = 0.3, random_state=33) Normalization 1234567891011121314151617181920212223# initializess = StandardScaler()# The fit function/module is used to train model parametersss.fit(train_x)# Standardize the training set and test settrain_x = ss.transform(train_x)test_x = ss.transform(test_x)# Define a logistic regression modellr = LogisticRegression()lr.fit(train_x, train_y)# Calculate the training set scorelr.score(train_x, train_y)# Calculate test set scorelr.score(test_x, test_y)# Use the classification_report function to display a text report of the main classification indicatorspredict = lr.predict(test_x)print(classification_report(test_y, predict))","link":"/heart/"},{"title":"Bluehost上安裝Habari","text":"Habari我還在試用階段...不得承認wordpress確實是一個好的博客程序,但是對於他的日進臃腫我有點微詞... 可憐我不是代碼編寫出身,所以很多問題不得不求助於人,還好身邊有一個好老師\"天佑\"給我提供了很多幫助! 以下就寫寫近期的一些內容. 因為bluehost原本就只願PHP5,而其他的一些標準我不太懂,但是據我安裝下來基本都已經全部滿足,唯一的就是需要自己開啟pdo for mysql. 在這之前你最好是下載一個requirements.php來測試一下你的主機是否已經為你的habari做好了準備. 將requirements.php上傳到自己需要安裝的文件夾下,輸入http://youblogurl/requirements.php 如果切OK,那么就會如下顯示: 如若不然,就會有如下顯示: 我這裡提示的是需要安裝或者打開pdo,如果你也是bluehost,那么基本就是這個提示了! 其實到這步是不需要聯繫客服幫你打開的,默認bluehost就已經安裝了pdo,主要是需要打開而已! 在自己的服務器上建立php.ini文檔,在CP上點擊PHP Config,然後選擇install php.ini master file 不出意外在根服務器上就已經建立php.ini了,加入如下語句: extension_dir = \"/usr/lib/php/extensions/no-debug-zts-20060613\" extension=pdo.so extension=pdo_mysql.so 到這裡當然還沒有結束,需要將php.ini複製到你所要安裝的habari文件夾... 這步一定要做.我不知道原理,所以不要問我,我只是如此操作了,成功了.反而刪除后就會出現需要激活pdo的提示. 然後自然就是下載habari,上傳到安裝目錄,輸入路徑...然後就是和wordpress的安裝順序一樣了! 這裡要提示一點:在網上有說安裝habari要將其目錄設定為777,我的經驗是不要這樣設置,這樣會造成訪問此目錄的時候出現505錯誤頁面...\"天佑\"幫我測試的時候就是如此!...後來我更改回來后就正常訪問了... 當然,很多人都是wordpress的用戶,所以如果你需要導入wordpress的原始數據的話需要做如下工作: 安裝一個插件,用以取消自動保存. 禁用後臺的多版本保存. 刪除數據庫中多餘的文章版本. 完成一上步驟后在habari導入的時候才不會有重複文章導入! 當然,在我的habari測試中還出現了亂碼問題,不僅僅是導入的時候出現亂碼,在輸入博客標題,寫新文章都會出現中文亂碼問題,在發此文的時候這個問題還沒有得到解決,只能讓各位待續了! 再次感謝\"天佑\"的幫助!","link":"/install-habari-at-bluehost/"},{"title":"hexo-generator-feed","text":"前段时间想通过QQ邮箱将自己的写的博客里的文章转发到QQ空间去。众所周知，这种转发似乎只有通过QQ邮箱发邮件转发一条路。要不就只有Ctrl+C，Ctrl+V。我是很厌恶这种方式的。 可是在QQ邮箱里，并没有收到自己从使用hexo以后的任何更新。好吧，我知道问题了，我失去了feed订阅路径。而Google的结果是，hexo自身并不带feed订阅，如果要支持订阅需要安装插件，也就是“hexo-generator-feed\" 这货。 后来，良久之后@lucifr也是这么告诉我的。 前段时间家里有事，离开上海，回来以后又因为工作需要恶补，一直没时间弄！这刚刚才弄好。。。。 说一下这货，其实是蛮简单的，我也就不再细述了，官方文档也将如何安装写的很清晰。自己查阅一下就好。","link":"/hexo-generator-feed/"},{"title":"habari的時區和more","text":"在這之前還一直在煩惱habari的時區問題和more代碼截斷. fireyy給了我一些幫助. habari最新的版本已經加上了時區的調整.而這個版本是需要svn方式獲取才可以.基於bluehost默認沒有開通ssh方式.那么我就只有選擇等待habari下一版本的正式發布,又或者聯繫bluehost增加ssh訪問,并花一定時間來學習和熟悉svn獲取的方式! 另外一個就是代碼截斷.在habari總一樣可以試用more代碼來對文章進行截斷,在所用模板總加入: // Only uses the tag, with the 'more' as the link to full post Format::apply_with_hook_params( 'more', 'post_content_out', 'more' ); 這樣在編寫entry的時候就與wordpress的效果是相同的了. 而整個habari的模板代碼和wordpress是比較類似的!頭疼的是plugin和wordpress有很多不一樣的地方!所以對於不懂代碼的我來說,就無從著手了... 奇怪的一點是我現在說使用的habari模板激活后有一個setting的選項,點擊以後并無反應! 我想本身是可以修改的... 比較丟人的地方在於,前邊的文章總提到了歸檔頁面,其實并不是真的在頁面中直接添加代碼就可以了!而是我所用的模板中已經加入了兼容代碼...對於如何建立頁面來實現仍然不清楚! 2009-2-21 01:53 BTW:好吧,我承認我自己又傻了一回...并不是需要在服務器上安裝SVN才可以的..也可以將svn服務器上的程序下載到本地然後再上傳,效果是一樣的!而我現在說安裝的habari就更新到了0.6beta.這樣就完全解決了時區問題!並且增加了zh_tw的文件包!雖然漢化并不完全,但是已經很好了!","link":"/habaris-time-zone-and-more/"},{"title":"繼續habari","text":"habari確實有很多地方是我喜歡的,當然,要說完全轉到這個平臺上還不成熟!因為我很多東西根本無力一個人解決...慢慢來吧,暫時還是更新我的wordpress,到時候再全部轉過去! 我的habari測試平臺 後臺非常簡單 在plugins中選取flickr plugin安裝后編寫文章是這個效果: 雖然後臺是完全html編寫,但是對於插入圖片不需要自己寫代碼.直接點擊display就可以了!會自動加入顯示代碼,當然,如果你不喜歡flickr或沒有帳戶,也可以上傳到自己的服務器上.而也有habari自己的插入插件.效果是一樣的! 現在要轉過去的話面臨幾個問題 涉及到themes,雖然k2已經很好用了.但是more代碼并不能截斷,這看來和模板有關.估計天佑是修改了自己的模板的! 看著天佑的模板很成熟了,如果能拿過來直接用當然最好!呵呵. 時區問題,默認時區不知道去哪裡修改!找了一圈都沒有在後臺中發現... 其他的都暫時是小問題!以後慢慢解決... 另外就是前文中提到的亂碼問題.說出來大家不要遺憾...我沒有特意去解決,就是從新安裝了一邊,而安裝的時候輸入了中文名作為博客名,就沒有亂碼了!而導入的時候也一并沒有亂碼了! 想想,中間有一個小小的操作...就是dupola曾經讓我在後臺中執行了一次整理,編碼為general_ci","link":"/habari-go-on/"},{"title":"Debian上安装Node服务器","text":"本教程只针对我自己，记录用，而且并不是完整版，不对其他人负责。请尽量不要参照本文 最近一直在学习NodeJS，本地上玩的差不多了，总要去架设个服务器跑一下，选择了digitalocean加的$5/mo 服务，安装了个Debian，至于为什么是Debian，好吧，因为。。。 言归正传，一遍操作一遍记录一下，好记性不如烂笔头嘛。 至于怎么注册DigitalOcean这里就不在详述了，从SSH登录开始吧。 SSH登录 未免麻烦，最好是选择SSH登录，官方有详细的介绍： How To Use SSH Keys with DigitalOcean Droplets 这里要说一下，DigitalOcean每次登录的时候都会告诉我密码过期，害得我重置了无数次密码。如果你也遇到这种问题，那么就先选择Conole Access, 然后在弹出的窗口控制台进行操作，修改root密码后再在本地操作。 12345678910111213141516171819202122// 创建SSH keyssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (~/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in ~/.ssh/id_rsa.Your public key has been saved in ~/.ssh/id_rsa.pub.The key fingerprint is:4a:dd:0a:c6:35:4e:3f:ed:27:38:8c:74:44:4d:93:67 demo@aThe key's randomart image is:+--[ RSA 2048]----+| .oo. || . o.E || + . o || . = = . || = S = . || o + = + || . o + o . || . o || |+-----------------+ 1cat ~/.ssh/id_rsa.pub 然后添加到digitalocean的SSH Keys里，Name随便起 之后我们就可以链接服务器了 1cat ~/.ssh/id_rsa.pub | ssh root@[your.ip.address.here] &quot;cat &gt;&gt; ~/.ssh/authorized_keys&quot; 然后，就可以直接登录了: 1ssh root@[your.ip.address.here] 安装Node 我选择的方式是源码安装 1234567891011121314151617// update system$ sudo apt-get update$ sudo apt-get install git-core curl build-essential openssl libssl-dev// Clone node$ cd /usr/local/src$ git clone https://github.com/nodejs/node$ cd node// select checkout$ git tag$ git checkout v4.4.7// install$ ./configure$ make$ sudo make install 漫长的等待，然后就可以查询了$ node -v， 这会就会出现安装的node version 安装NPM 12345$ wget https://npmjs.org/install.sh --no-check-certificate$ chmod 777 install.sh$ ./install.sh$ npm -v3.10.5 安装zsh(不是必要) 123456789sudo apt-get zshgit clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh// copy defult zshrccp ~/.zshrc ~/.zshrc.bak// set oh-my-zsh to usecp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrccash -s /bin/zshsudo shutdown -r now 安装Ruby 安装rbenv 1234567891011git clone git://github.com/sstephenson/rbenv.git ~/.rbenv# 用来编译安装 rubygit clone git://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build# 用来管理 gemset, 可选, 因为有 bundler 也没什么必要git clone git://github.com/jamis/rbenv-gemset.git ~/.rbenv/plugins/rbenv-gemset# 通过 gem 命令安装完 gem 后无需手动输入 rbenv rehash 命令, 推荐git clone git://github.com/sstephenson/rbenv-gem-rehash.git ~/.rbenv/plugins/rbenv-gem-rehash# 通过 rbenv update 命令来更新 rbenv 以及所有插件, 推荐git clone git://github.com/rkh/rbenv-update.git ~/.rbenv/plugins/rbenv-update# 使用 Ruby China 的镜像安装 Ruby, 国内用户推荐git clone git://github.com/AndorChen/rbenv-china-mirror.git ~/.rbenv/plugins/rbenv-china-mirror 12345echo 'export PATH=&quot;$HOME/.rbenv/bin:$PATH&quot;' &gt;&gt; ~/.bashrcecho 'eval &quot;$(rbenv init -)&quot;' &gt;&gt; ~/.bashrc#rbenv install 2.3.1 特么什么都能遇到，远端locale和本地不符，提示无法安装 1sudo locale-gen en_US.UTF-8 // or 1sudo dpkg-reconfigure locales 1vim /etc/ssh/ssh_config 注释或删除AcceptEnv LANG LC_* (服务器SSH配置) 然后断开SSH重新登录，不行重启一下服务器，就好了。 1sudo shutdown -r now 继续: 1rebnv install 2.3.1 部署Nginx 1$ sudo apt-get install nginx 其实Nginx也不是必要装的，Node自己可以跑服务！ 1nohup node app.js 但是如果要多域名的话，需要用到Nginx反代，额，这部分还不懂。再去研究下！ 顺便，加一个删除Nginx的步骤: 1234567sudo apt-get --purge remove nginxsudo apt-get autoremovedpkg --get-selections|grep nginx// 罗列出与nginx相关的软件， nginx-common deinstall 然后sudo apt-get --purge remove nginx-common","link":"/install-node-to-Debian/"},{"title":"iReader","text":"这是来自花儿开了的Ajax 习作。iReader 是一个在线的 RSS 阅读器。借助于 Ajax 技术，使页面在无刷新的情况下便可以实时读取 RSS Feed 并将其格式化好并呈现给用户。需要占用一部分网络空间进行上传才可以。代替了我以前应用的第三方服务。。还是放在自己服务器上的东西最实在。不会害怕丢失或者服务器损坏不能应用服务。 这是效果，具体还是去花儿开了的网站去看看吧。","link":"/ireader/"},{"title":"King of pop 2014","text":"I MISS U.","link":"/king-of-pop-2014/"},{"title":"Google正式推出代码搜索 Code Search","text":"来自小熊在线 Google新一轮的发布热潮在黄金周涌现。这就是Google刚推出的代码搜索，即Google Code Search。根据Garett Rogers的介绍，Google代码搜索结果来自Google的索引。也就是说理论上Google能找到的代码，你都可以利用Google代码搜索找到。 你可以利用Google代码搜索来搜索各种函数的定义以及相关的示例代码，还可以直接使用正则表达式搜索以获得更精确的结果。另外，你还可以限定搜索某种语言、许可或文件名。 对于程序员而言，这个代码搜索工具应用挺实用的。Google Code Search允许编程人员搜索代码用法范例，以更好地理解代码功能。该服务中索引了数十亿行代码，来源是Web上保存的文档以及SourceForge、Google Code等开源软件项目库。 Google还提供了一个API允许第三方开发人员将代码搜索框整合到他们的开发工具中。 点击进入Google Code Search","link":"/google-code-search/"},{"title":"mac apache","text":"经常忘记命令，还是自己稍微记录一下好，免得每次都去Google。 打开 apache sudo apachectl start 關閉 Apache： sudo apachectl stop 重開 Apache： sudo apachectl restart","link":"/mac-apache/"},{"title":"Mac MAX_open","text":"在使用Hexo的过程里，经常会卡在deploy指令上，错误原因之一可能是因为Mac的MAX_open数小的原因，Linux默认为1024，而Mac上只有256，所以只要修改MAX_open数就可以了。指令如下： 1234567891011$ sudo sysctl -w kern.maxfiles=20480kern.maxfiles: 12288 -&gt; 20480$ sudo sysctl -w kern.maxfilesperproc=18000kern.maxfilesperproc: 10240 -&gt; 18000$ ulimit -S -n 2048bubbyroom.com$ ulimit -n2048 其中，$ ulimit -n是用于查看Mac的MAX_open数的指令。只执行修改之前可以先执行此指令查看一下。 后记：在Terminal中修改了MAX_open仅适用于当前窗口，新建Tab，窗口后在新的Tab和窗口里都会失效。","link":"/mac-max_open/"},{"title":"Mac显示隐藏文件","text":"转载自Mac疯，记录而已，不长用，老是记不住，每次都要Google，郁闷！ 显示文件： defaults write com.apple.finder AppleShowAllFiles -bool true 隐藏文件 defaults write com.apple.finder AppleShowAllFiles -bool false 记得KillAll Finder来重启Finder","link":"/macxian-shi-yin-cang-wen-jian/"},{"title":"Mac隐藏&#x2F;显示桌面图标","text":"Shell: 1defaults write com.apple.finder CreateDesktop -bool FALSE;killall Finder &amp; 1defaults delete com.apple.finder CreateDesktop;killall Finder AppleScript: 1234567display dialog &quot;桌面图标设置为可见或隐藏?&quot; buttons {&quot;可见&quot;, &quot;隐藏&quot;} with icon 2 with title &quot;Switch to presentation mode&quot; default button 1set switch to button returned of resultif switch is &quot;隐藏&quot; then do shell script &quot;defaults write com.apple.finder CreateDesktop -bool FALSE;killall Finder&quot;else do shell script &quot;defaults delete com.apple.finder CreateDesktop;killall Finder&quot;end if","link":"/mac-False-Desktop-icon/"},{"title":"mac:port occupied","text":"查询: 1$ lsof -i:3000 显示: 12COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEnode 2243 du 12u IPv6 0xc9b8c91a94a8da89 0t0 TCP *:hbci (LISTEN) 结束: 1$ kill -9 2243","link":"/mac-port-occupied/"},{"title":"NFC和O2O","text":"总体来说,这是我最看好的移动应用之一! 注意,是之一. 首先,我要说的是SNS网站真的没有再大的作为了!人们已经开始从那些疯狂中开始慢慢冷却了下来,而真正实际用到SNS网站的人也是小部分而已.造势,炒作,宣传,营销...当然,我不能否认这些平台在作为传统媒体的延伸甚至是替代品上的功效. 可是又真有多少人会真的坚持活在这种亢奋的状态下.除非有大批的粉丝不停的为自己打鸡血吧? 所以我总认为,移动社交产品可以打住了.至于查找附近聊友的功能,诸如微信以及添加新功能的米聊以及后来居上的一些产品,不要看宣传的标语很美,实际上还不是沦为宅男腐女查找就近炮友的必备品而已! 好吧,对SNS产品的吐槽结束了,我真正要说的是NFC以及O2O这两个东西! 首先我们需要扫盲一下,NFC就是Near Field Communication,翻译过来就是近距离无线通信，是一种短距离的高频无线通信技术，允许电子设备之间进行非接触式点对点数据传输，在十厘米（3.9英吋）内，交换数据。这个技术由免接触式射频识别（RFID）演变而来.而我们平时所用的交通卡等一类射频卡种,都是RFID技术. &lt;而O2O(Online to Offline)就不是什么技术了,而是一种互联网商业模式,就是把线上的消费者带到现实中的商店中去在线支付购买线下商品和服务,再到线下去享受服务. 而实际上,团购,就是一种O2O模式.可是我心里的O2O模式,完全不是像团购那样的垃圾.这种模式在中国,已经早早的走入了死胡同,一条死路!我所要说的O2O,是一种集合所有商家优惠折扣的消费资源,并一卡通吃的卡片发行商. 而也终于让我遇到一个,这就是么卡.有兴趣的可以去看看,我可没有要推销么卡的意思,也无意为他写什么行销软文.只是在这里纯探讨而已! 好吧,下面让我们实际展望一下,现在很多手机已经开始内置NFC技术.比如iPhone 4S,三星盖世兔等等,那么,你能理解我说什么了,我们以后也就完全可以不必身上大卡小卡的带在身上,一个手机就OK了.交通卡,银行卡,还有各大商场卖场的贵宾卡折扣卡,都可以完完全全的装载我们的手机里! 这其实已经应该不是什么新技术,手机代替交通卡和信用卡,日本早就开始做先行者了!而实际上,O2O模式在中国也才起头而已,而重点,并不是技术怎样怎样,而是线下的资源掌握的如何.要不团购团队中实际上最大的是营销团队呢. 而像么卡这样将所有会员卡全部装到手机里的模式,一样取决于,谁掌握了最大的市场!当你的卡通吃了全世界大大小小的商家卖场的时候,相信所有人都愿意只带着一部手机走遍天下的感觉! 好了,展望结束...大家想去吧,哈哈!","link":"/nfc-and-o2o/"},{"title":"nitrous.io","text":"世界，真心是美好的啊！ 好吧，再使用nitrous之前，我一直都不知道，原来世界可以如此美好。 不需要Dropbox君，不需要git的master了，而且可以随时随地hexo。 妈妈再也不用担心我的md文档丢失了。。。 这货不仅仅是node.js,还有Rails，python以及Go环境哦。。。 亲们，还在等什么。如果windows下的Ruby环境配置让你想砸机子的冲动，那么赶快来nitrous.io吧！","link":"/nitrousio/"},{"title":"「马拉松跑步数据","text":"先引入数据,准备进行分析 1234# 引入数据import pandas as pddata = pd.read_csv('~/data/cbcpv/marathon/marathon.csv')data.sample(5) OUT: age gender split final 19841 34 M 01:55:25 04:50:03 11002 28 W 01:55:00 04:11:00 11619 26 M 01:40:28 04:13:52 4068 34 M 01:38:30 03:30:21 6922 35 M 01:37:44 03:48:37 这个数据集有以下几个特征： age，运动员的年龄 gender，运动员的性别 split，半程所用时间 final，全程所用时间，即最终成绩 自然,要先了解下数据的具体情况 1234567891011121314data.info()# 输出结果&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 37250 entries, 0 to 37249Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 37250 non-null int64 1 gender 37250 non-null object 2 split 37250 non-null object 3 final 37250 non-null objectdtypes: int64(1), object(3)memory usage: 1.1+ MB 可以看到并没缺失值, 不过split和final特征中的数据不实数字类型, 用字符串表示了所用时间长度, 所以我们需要进行转化: 1234import datetimedef convert_time(s): h,m,s=map(int, s.split(':')) return datetime.timedelta(hours=h, minutes=m, seconds=s) 使用完成的方法进行数据转换,我们从新读取一下数据: 123456789101112131415df=pd.read_csv( '~/data/cbcpv/marathon/marathon.csv', converters={ 'split': convert_time, 'final': convert_time })df.dtypes# 输出结果age int64gender objectsplit timedelta64[ns]final timedelta64[ns]dtype: object 这次数据已经转换为timedelta64类型, 下面我们就要转化时间为整数, 一般的做法都是秒或者毫秒数: 1234567d = datetime.timedelta(hours=1, minutes=0, seconds=0)df2 = pd.DataFrame({'time':[d]})df2.astype(int)# 输出结果 time0 3600000000000 我们看到的输出结果,是“纳秒“(ns)单位: \\[1s=10^9ns\\] 我们还需要转化为秒: 1234567d=datetime.timedelta(hours=1, minutes=0, seconds=0)df2=pd.DataFrame({'time':[d]})df2.astype(int) * 1e-9# out time0 3600.0 然后我们要讲split和final两个特征的数据进行转化 123df['split_sec']=df['split'].astype(int) * 1e-9df['final_sec']=df['final'].astype(int) * 1e-9df.sample(5) OUT: age gender split final split_sec final_sec 11725 35 M 0 days 01:53:53 0 days 04:14:19 6833.0 15259.0 19815 24 M 0 days 01:58:45 0 days 04:49:57 7125.0 17397.0 5754 49 M 0 days 01:42:39 0 days 03:41:05 6159.0 13265.0 33166 46 M 0 days 02:31:37 0 days 06:06:17 9097.0 21977.0 9226 36 W 0 days 01:49:06 0 days 04:01:55 6546.0 14515.0 现在多了两个特征split_sec和final_sec, 都是以秒为单位的浮点数. 描述统计 先了解数据: 1df.describe() OUT: age split final split_sec final_sec count 37250.000000 37250 37250 37250.000000 37250.000000 mean 40.697369 0 days 02:03:54.425664429 0 days 04:48:09.303597315 7434.425664 17289.303597 std 10.220043 0 days 00:22:55.093889674 0 days 01:03:32.145345151 1375.093890 3812.145345 min 17.000000 0 days 01:05:21 0 days 02:08:51 3921.000000 7731.000000 25% 33.000000 0 days 01:48:25 0 days 04:02:24 6505.000000 14544.000000 50% 40.000000 0 days 02:01:13 0 days 04:44:25 7273.000000 17065.000000 75% 48.000000 0 days 02:16:11 0 days 05:27:36 8171.000000 19656.000000 max 86.000000 0 days 04:59:49 0 days 10:01:08 17989.000000 36068.000000 居然年龄上最大的数据是86, 让我们看看特征的数据分布: 123%matplotlib inlineimport seaborn as snsax=sns.boxplot(x=df['age']) image-20210923215638259 这个箱线图反应了, 数据里确实有一些“离群值”. 数据分布 研究下数据分布, 看看split_sec和final_sec 1sns.displot(df['split_sec']) image-20210923215655517 1sns.displot(df['final_sec']) image-20210923215712676 整体看来,两个特征下的数据都符合正态分布, 但是final_sec的分布图比较胖. 这次我们把gender这个分类特征添加进来: 1sns.violinplot(x='gender', y='final_sec', data=df) image-20210923215729962 这些看到, 男性运动员在总体上还是比女性运动员要快一些. 寻找优秀的原因 跑马拉松或者了解这项运动的人都清楚, 运动员很关注整个赛程中前后半程的时间比较,好的选手是后半程用时和前半程近似. 因此, 我们来研究下, 这些运动员前后半程用时情况. 12345g=sns.jointplot('split_sec', 'final_sec', data=df, kind='hex')# 绘制一条直线, 作为参考import numpy as npg.ax_joint.plot(np.linspace(4000, 16000), np.linspace(8000, 32000), ':k') image-20210923215745469 横坐标是splict_sec特征, 即半程用时. 纵轴表示final_sec特征, 全程用时. 途中可以看出, 的确是越优秀的运动员,前半程用时越接近全程用时的一半, 甚至还有少数后半程跑的更快的. 我们做个计算来深入研究下: 12df['split_frac']=1-2*df['split_sec']/df['final_sec']df.sample(5) OUT: age gender split final split_sec final_sec split_frac 2065 35 W 0 days 01:31:41 0 days 03:14:40 5501.0 11680.0 0.058048 9001 43 W 0 days 01:58:19 0 days 04:00:44 7099.0 14444.0 0.017031 30039 34 M 0 days 02:25:17 0 days 05:39:21 8717.0 20361.0 0.143755 27456 62 W 0 days 02:13:28 0 days 05:25:01 8008.0 19501.0 0.178709 13335 41 M 0 days 01:45:36 0 days 04:21:00 6336.0 15660.0 0.190805 用直方图再增加一个参考线来看看split_frac特征中的数据分布: 1234import matplotlib.pyplot as pltsns.displot(df['split_frac'], kde=False)# 垂直于 x 轴的直线，0 表示 x 轴位置plt.axvline(0, color='k', linestyle='--') image-20210923215830370 从这张图中, 更清晰的看到全体参赛者的运动安排. 再来探究下不同特征之间的关系: 12345sns.pairplot( data=df, vars=['age','split_sec','final_sec','split_frac'], hue='gender') image-20210923215848042 让我们来看下80岁选手的数量: 1234(df.age&gt;=80).sum()# OUT15 下面, 我们划分下年龄段,看看各年龄段的成绩分布: 12345678910df['age_dec']=df['age'].map(lambda age: 10*(age//10))sns.violinplot( x='age_dec', y='split_frac', hue='gender', data=df, split=True, inner='quartile', palette=['lightblue', 'lightpink']) image-20210923215931464 看这张图, 我们发现,不同性别的运动员的split_frac特征数据分布中, 年龄越大,前后端的时间分布比相对集中. 再看看全程用时分布比较: 123456789sns.violinplot( x='age_dec', y='final_sec', hue='gender', data=df, split=True, inner='quartile', palette=['lightblue', 'lightpink']) image-20210923215947281 从30岁往后, 明显年纪越大,用时越长.","link":"/marathon/"},{"title":"不要新版Google Reader","text":"什么都不想多说,只想请求Google能恢复旧版的Google Reader以及分享方式! 我承认自己是一个Google的重度用户,但是并不代表没有其替代!现在看来除了Gmail以外,没有什么Google产品是不能被其他产品代替的! 我相信以前Google所谓的不作恶名言确实是出自对于自己充分的自信,而现在的作恶是为了什么,难道是对自己公司开始不自信了? 这样的做法只能让忠实的用户离开Google,而不能为了GPlus增加用户,纯粹的短视! 所以,在此请求Google能恢复旧版的Reader界面以及分享方式! From --- 一个重度Google用户","link":"/no-new-google-reader/"},{"title":"Note about my ubuntu","text":"这是我为了便于自己以后方便而记录的一些关于我ubuntu上设置所需要的内容！当然其中内容都是通用的，只是如果你们要拿去用的话记得将路径以及一些变量变成你自己的！ about system $ sudo gedit /etc/apt/sources.list 12345678910deb http://mirrors.163.com/ubuntu/ karmic main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-bac. kports main restricted universe multiverse $ sudo apt-get update and $ sudo apt-get upgrade and hosts $ gedit /etc/hosts # My Dropbox have new hosts file about ATI Donload form This link 12cd **sudo sh **.run That's ok about Keepass 2 so...apt-get 123$ sudo apt-add-repository ppa:jtaylor/keepass$ sudo apt-get update$ sudo apt-get install keepass2 10.04 所需软件包 1 install mono #[link](http://mono-project.com/DistroPackages/Ubuntu) 1234567Click on &quot;System&quot;, &quot;Administration&quot;, &quot;Software Sources&quot;.Click on the &quot;Other Software&quot; tab.Click on &quot;Add...&quot;, and enter the line:**deb http://badgerports.org lucid main**Click on &quot;Add Source&quot;Click on &quot;Authentication&quot;, then on &quot;Import Key File&quot;Download this [GPG key file](http://badgerports.org/directhex.ppa.asc), ID 0E1FAD0C, and select it in the &quot;Import Key File&quot; windowClick on &quot;Close&quot;, then &quot;Reload&quot; when the pop-up appears. You're all set! 2 $ sudo apt-add-repository ppa:jtaylor/keepass 3 $ sudo apt-get update 4 $ sudo apt-get install keepass2 about Java Download from This link and 1234$ cd **$ tar **.tar.gz(64bit)$ sudo update-alternatives --install &quot;/usr/bin/java&quot; &quot;java&quot; &quot;/home/hivan/software/jdk1.7.0_04/bin/java&quot; 1$ sudo update-alternatives --config java 其实就是配置一下默认路径就OK了！ 或者以下方法： $ gedit ~/.bashrc 末尾添加变量 1234JAVA_HOME=/home/hivan/software/jdk1.7.0_04JRE_HOME=${JAVA_HOME}/jreCLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libPATH=${JAVA_HOME}/bin:$PATH and 123456789$ source ~/.bashrc$ sudo update-alternatives --install /home/hivan/software/jdk1.7.0_04/bin/java 300 $ sudo update-alternatives --install /home/hivan/software/jdk1.7.0_04/bin/javac 300 $ sudo update-alternatives --install /home/hivan/software/jdk1.7.0_04/bin/jar 300 $ sudo update-alternatives --config java #用于替换Java，可能会跳出选择框$ java -version #测试java version &quot;1.7.0_04-ea&quot;Java(TM) SE Runtime Environment (build 1.7.0_04-ea-b19)Java HotSpot(TM) 64-Bit Server VM (build 23.0-b20, mixed mode) about goagent 1234567$ cd$ mkdir bin$ cd bin建立一个proxy.sh file$ gedit proxy.sh输入： python /***/goagent/local/proxy.py save and quit$ chmod 700 proxy.sh about ruby 12345678$ sudo apt-get install apache2 curl git libmysqlclient-dev mysql-server nodejs$ bash -s stable &lt; &lt;(curl -s https://raw.github.com/wayneeseguin/rvm/master/binscripts/rvm-installer)$ echo '[[ -s &quot;$HOME/.rvm/scripts/rvm&quot; ]] &amp;&amp; . &quot;$HOME/.rvm/scripts/rvm&quot; # Load RVM function' &gt;&gt; ~/.bashrc$ source .bashrc$ rvm requirements$ sudo apt-get install build-essential openssl libreadline6 libreadline6-dev curl git-core zlib1g zlib1g-dev libssl-dev libyaml-dev libsqlite3-0 libsqlite3-dev sqlite3 libxml2-dev libxslt-dev autoconf libc6-dev ncurses-dev automake libtool bison subversion$ rvm install 1.9.3$ rvm 1.9.3 –-default #有可能出错**RVM is not a function, selecting rubies with 'rvm use ...' will not work.** 则执行：`$ rvm alias create default 1.9.3 about Rails 12$ gem install bundler rails rdoc #rdoc 为Octopress所需组建$ rails -v #如果“**程序“rvm”尚未安装。**”则检查一下.bashrc的路径配置 about python 123456789$ python -VPython 2.6.6$ curl -kL http://github.com/utahta/pythonbrew/raw/master/pythonbrew-install | bash$ . $HOME/.pythonbrew/etc/bashrc$ pythonbrew install 2.7.1$ pythonbrew switch 2.7.1Switched to Python-2.7.1$ python -VPython 2.7.1 ubuntu 12中默认是2.7，如果要安装3.2和上述步骤一样！改一下版本号 about Sublime Text 2 12345678910#!/usr/bin/env xdg-open[Desktop Entry]Name=Sublime Text 2Comment=Sublime Text 2Exec=/home/hivan/software/&quot;Sublime Text 2&quot;/sublime_textIcon=/home/hivan/software/Sublime Text 2/Icon/128X128/sublime_text.pngTerminal=falseType=ApplicationCategories=Application;Development;StartupNotify=true 设置权限：可执行文件！ and 12$ cd /home/hivan/software/&quot;Sublime Text 2&quot;/$ sudo cp &quot;Sublime Text 2.desktop&quot; /usr/share/applications Ctrl+` 1import urllib2,os;pf='Package Control.sublime-package';ipp=sublime.installed_packages_path();os.makedirs(ipp) if not os.path.exists(ipp) else None;open(os.path.join(ipp,pf),'wb').write(urllib2.urlopen('http://sublime.wbond.net/'+pf.replace(' ','%20')).read()) Ctrl+Shift+P 1 ZenCoding 2 Alignment 3 Markdown 4 setting user { \"ignored_packages\": [] } download setting from this link Finally, input right, you can only choose scim,so... 12$ sudo apt-get install scim$ sudo apt-get install scim-pinyin ... 12scim设置－&gt;全局设置－&gt;将预编辑字符串嵌入到客户端中勾去掉scim设置-&gt;gtk-&gt;嵌入式候选词标 勾去掉 about Retext 123sudo add-apt-repository ppa:mitya57sudo apt-get updatesudo apt-get install retext or This link about Octopress Go to This Link add and remove ppa 12$ sudo add-apt-repository ppa:name/name$ sudo add-apt-repository -r ppa:name/name Temporary end, to be continued...","link":"/note-about-my-ubuntu/"},{"title":"拿福能千人挑战活动","text":"如何加入: 在之前[@mg12](http://weibo.com/mg12) 那里有看到一个博客广告.右下角的地方有一个拿福能的广告. 就我所见,吴钊童鞋很少会在页面上添加广告的,除非他认为可以做的.这和我的页面上那么多广告不同.所以他的页面一直很清爽!这次少见的添加了拿福能,我想一定有些搞头.所以就去注册了,并且,我从[@Denis](http://weibo.com/denishua)那里也看到了相应的广告,还看到了一篇拿福能博主北京聚会的Post,而且对其大加赞赏!So,我想,这个广告提供商有搞头. 拿福能是做什么的 总的说来,拿福能是一个广告提供商,可以称作网络营销公司.说起来Feedsky以前做过付费软文大家应该都不陌生了.不过我知道众多的博主都被Feedsky的付费评论给搞伤了,很难相信同类的公司能做的起来.自然,我也有些将信将疑.不过对于博主们来说,有多一份的收入总比没有的强.在国内,写独立博客确实是一件非常让人心酸的事情,在国外独立博主获得丰厚的回报的时候,我们从几年前就开始在探讨国内博客的生存之道,到今天为止仍然还是没有找到合适的出路.那么这次拿福能既然为博主们提供了如此好的一个机遇,大家不妨做做看! 并且,拿福能有一些和其他不同的是,他们有一些线下的付费活动.就如[@mg12 介绍的](http://www.neoease.com/nuffnang-challenge-1000/),他们会提供一些电影或者之类的观赏活动,然后在博主回来之后写影评或者剧透.当然,这基本属于一种电影宣传活动!更重要的是,有无数的独立博主可以相互交流.不管你是哪一类的博客,总有一个主题是适合你的! 活动内容 据说,拿福能在国外已经做了好几年了.并且有一些非常成功的案例,而此次开发国内市场,当然是期望在国内也有所作为.所以搞了这么一个千人挑战活动,分发50,000元奖金来招募一千个博主.届时,人满之后这一千个博主将会平分这50,000元奖金. 有兴趣的,可以去拿福能的千人挑战活动主页参看!","link":"/nuffnang-challenge-1000/"},{"title":"Pageflakes","text":"Pageflakes 是一个出色的 Web 2.0 Personal Portal。类似与 Google IG 和 Live.com 的东西，但我个人觉得风格简洁的很得体，介于 Google IG 的光秃秃和 Live.com 过于花哨之间，不仅给访客带来不错的视觉体验,而且使页面的载入速度也有不小提升。 Pageflakes 预设了 150+ 个 Flakes，这个 Flake 就相当于 Google IG 和 Live.com 的 Gadget，很丰富也很实用，特别是那个 TO-DO-LIST Flakes，让人爱不释手（有没有类似的桌面工具，麻烦推荐一下？）。Pageflakes 支持自定义 RSS，同时个性页面同 Live.com 一样支持多页，这样我们可以将 Flakes 和 Feeds 分好类，一页一页的看，很方便。需要说明一点的是，这个东西仅仅支持在IE浏览器下，我使用firefox的内核开始根本不能访问，可以自动利用ietab跳到了IE内核上。。。。。而且sign up和login形同虚设。。。点击sign up后出现的提示,这个东西似乎只只是同一电脑下的为默认同一用户。不知道在重新安装系统以后会如何。。。。。。。有内行的指教一下。","link":"/pageflakes/"},{"title":"picasaweb","text":"这一下就经过了很多日子，本来前些天是想写一些教程的。 现在脑子里的想法大多都被时间冲淡了。只是随便写写吧。 由于我的机子重新装了系统的原因。迅雷里的原始连接我也找不到了，有兴趣的朋友可以自己去找找看 picasa2和picasaweb有很多的不同，最大的不一样大概就是现在没有推出中文版本。一切都是E文。自信能看得懂的朋友可以下载来试试。 picasaweb是和自己的google账号绑定的。google的picasaweb服务在网上有250M的存储空间。可以利用picasaweb直接上传到自己的账号上。我去试验了一下，服务很不错，而且有很多新的功能，可惜的是不支持外连，估计快可以了，因为每一张图片都有自己的地址。 Picasaweb和原版本还有很多新加的功能。其中我比较感兴趣的就是色相搜索。输入color:xxx(比如red)就会有相应色相的相片被搜索出来。效果大家可以自己下载以后自己看看@。在我使用过程中，虽然E文给我找了不少麻烦，但是总体上感觉这次的升级还是有很多实用的功能在里边。 好了，软件谈完了，说说最近看的电影。。我这个人不喜欢在网上下载，质量太侮辱我眼睛。所以我一般都是等到碟版出来以后买回来看。。。。等到现在才看了达芬奇密码和碟三，但是我并不是要向大家说这两部片子，而是我看的另外一部:《东京审判》这是一部设计历史题材和政治题材的片子。1945年日本签署投降书的以后，联盟国法官团赶赴东京对日本的20多位战犯进行审判。长达817次审判和最后一次宣判共818次的审判历程。 印象最深的是曾志伟饰演的日本人。从他嘴里说出来的:狗日的日本鬼子。 不知道这句话是因为深受其害还是因为别的原因。再怎么样，一个日本人不会如此辱骂自己的国家吧。就象中国人自己，如果说了一句中国猪，反映会是怎样的？ so。。。最后还是不得不感叹战争带来的危害。所有的一切，也都仅仅是战争引起来的而已 不过再如何，日本到现在为止的历史观确实让我恶心。我到现在才知道靖国神社原来一共供奉着2000多位战犯，而其中有7位是甲级战犯，而最大的也是最可恶的战端挑起者东条英机，也被供奉在里边。这，是对人类文明的一种侮辱。 OK。。。写到这里了。。该去看书了","link":"/picasaweb/"},{"title":"解决Mac M1 原生Photoshop找不到CEP扩展面板","text":"研究这个问题, 也属于是撞上了! 我现在使用的PS版本为: notion image 在Photoshop内, 我画画时一直使用的是第三方的色轮插件Coolorus, 长这样: notion image 好用与否, 可以说, 谁用谁知道. 前些日子发现Photoshop2022有M1原生版本, 不用再使用转译版本, 我心想, 应该速度上会快很多吧! 兴奋的更新完后才发现, Coolorus面板无论如何找不到了, 原本应该在窗口下的“扩展(旧版)”菜单也找不到. notion image 无论怎么折腾都不行, 而且设置面板里的增效工具也是灰色无法设置: notion image 正当我心灰意冷准备返回2021时, 忽然想到, Adobe早就在PS中启用UXP插件了, 而CEP插件因为历史原因一直也无法完全取消, 那么既然是早就做的事情, 为什么2022版本里全给抹杀了呢, 重点是, 面板里设置项虽然不可点击, 但是还在? 问题应该不是出在版本上, 而是出在M1原生的问题上, 我试着去设置了转译, 再次重新打开PS, 果然不出所料, 扩展(旧版)菜单又回来了. notion image notion image 好吧, 这回知道是怎么回事了, 也就好解决了. 方法一: 返回PS 22.21版本, 简单粗暴 方法二: 讲PS2022设置为Rosetta转译打开方式, 同样简单粗暴! 回答一下可能大家问到的问题: 速度上, 感觉不出有什么太大的变化 是的, Coolous照样无法使用, 我快崩溃了, 正在纠结到底是放弃Coolous使用Photoshop原始色轮, 还是回到2021 反正问题是这么个问题, 解决方案也有了!大家自行抉择吧!","link":"/ps-find-cep-for-m1/"},{"title":"QQ微博邀请自助贴","text":"请将使用过的地址留言贴出来,方便别人啊!谢谢... -----------------------无耻分割线 http://t.qq.com/invite/b11fba23eb7660c2c261 http://t.qq.com/invite/bcd728928b5aec7aa341 -------20100502---------- http://t.qq.com/invite/b39659769ad1bf81f6df http://t.qq.com/invite/7bcb9d4450de2a05e0fb http://t.qq.com/invite/ae7cb0aae3a651365a71 -------------20100524------------------ http://t.qq.com/invite/7bcb9d4450de2a05e0fb http://t.qq.com/invite/bfc518a208c9f3dc0a03 http://t.qq.com/invite/d8c50c6c10beb588554d http://t.qq.com/invite/3a37bacfc981cfde2d59 http://t.qq.com/invite/5bfd6e3d075ac7f9f0cb http://t.qq.com/invite/19baed1f205b3f3e5173","link":"/qq-microblogging-invite-self-stick/"},{"title":"重裝「Yosemite」","text":"重裝系統之後，很多東西需要重裝，特別是開發環境。而開發環境的先後順序和設置，一直是我頭疼的事情。這次就着從新安裝了一遍，把很多東西都記錄下來。之前那個環境被我搞的亂七八糟，並且恢復不回來了。 image 多大多數次序參照 @mrzhang 系統偏好設置 更改電腦名稱 共享 允許安裝任何來源APP 安全性與隱私 --》通用 設置快捷鍵 鍵盤 --》 快捷鍵 配置VPN以及SSH 很重要，因爲很多源都在牆外了 安裝輸入法 下載並安裝\"Squirrel\" 下載並安裝\"SCU\" 安裝Sublime Text 3 設置package Ctrl+ , and setting: 12345678910111213141516171819202122232425262728293031{ &quot;caret\\_style&quot;: &quot;phase&quot;, &quot;color\\_scheme&quot;: &quot;Packages/Color Scheme - Default/Solarized (Light).tmTheme&quot;, &quot;font\\_face&quot;: &quot;Monaco&quot;, &quot;font\\_size&quot;: 13.0, &quot;hightlight\\_line&quot;: true, &quot;hightlight\\_modified\\_tabs&quot;: true, &quot;ignored\\_packages&quot;: [ &quot;Vintage&quot; ], &quot;indent\\_to\\_bracket&quot;: true, &quot;draw\\_centered&quot;: false, //居中显示 &quot;line\\_numbers&quot;: true, //显示行号 &quot;gutter&quot;: true, //显示行号边栏 &quot;fold\\_buttons&quot;: true, //显示折叠按钮 &quot;fade\\_fold\\_buttons&quot;: true, //始终显示折叠按钮 &quot;rulers&quot;: [], //列显示垂直标尺，在中括号里填写数字，宽度按字符计算 &quot;spell\\_check&quot;: false, //拼写检查 &quot;hot\\_exit&quot;: true, //保留未保存内容 &quot;line\\_padding\\_bottom&quot;: 1, &quot;line\\_padding\\_top&quot;: 1, &quot;scroll\\_past\\_end&quot;: true, //文本最下方缓冲区 &quot;tab\\_size&quot;: 2, // Tab制表宽度 &quot;translate\\_tabs\\_to\\_spaces&quot;: true, //缩进和遇到Tab键用空格替代 &quot;wide\\_caret&quot;: true, &quot;word\\_wrap&quot;: true, &quot;match\\_tags&quot;: true, //HTML下突出显示光标所在标签的两端。 &quot;match\\_selection&quot;: true, //全文高亮当前选中字符 &quot;wrap\\_width&quot;: 80} 編輯設置 /etc/paths 123456/usr/local/bin/usr/local/sbin/usr/bin/usr/sbin/bin/sbin 安裝Xcode 1xcode-select --install 安裝Homebrew 1ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/homebrew/go/install)&quot; PS: 這裏可能會很長時間的等待 設置Sublime終端鏈接 1ln -s /Applications/Sublime\\ Text.app/Contents/SharedSupport/bin/subl /usr/local/bin/sm Git, autojump 1brew install git autojump Oh My Zsh 1curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | sh 設置 ~/.zshrc: 1234export NVM\\_NODEJS\\_ORG\\_MIRROR=&quot;http://npm.taobao.org/dist&quot;[[ -s &quot;$HOME/.nvm/nvm.sh&quot; ]] &amp;&amp; . &quot;$HOME/.nvm/nvm.sh&quot;export NODE\\_PATH=$NVM\\_DIR/$(nvm\\_ls current)/lib/node\\_modules 安裝 NodeJS 12nvm install 0.11.15nvm alias default 0.11.15 安裝rbenv 123456789git clone git://github.com/sstephenson/rbenv.git \\~/.rbenv# 用来编译安装 rubygit clone git://github.com/sstephenson/ruby-build.git \\~/.rbenv/plugins/ruby-build# 用来管理 gemset, 可选, 因为有 bundler 也没什么必要git clone git://github.com/jamis/rbenv-gemset.git \\~/.rbenv/plugins/rbenv-gemset# 通过 gem 命令安装完 gem 后无需手动输入 rbenv rehash 命令, 推荐git clone git://github.com/sstephenson/rbenv-gem-rehash.git \\~/.rbenv/plugins/rbenv-gem-rehash# 通过 rbenv update 命令来更新 rbenv 以及所有插件, 推荐git clone https://github.com/rkh/rbenv-update.git \\~/.rbenv/plugins/rbenv-update 設置~/.zshrc 12export PATH=&quot;$HOME/.rbenv/bin:$PATH&quot;eval &quot;$(rbenv init -)&quot; 其他 安裝Ruby 1234rbenv install -l # list all available versionsrbenv install 2.1.5 # install a Ruby versionrbenv global 2.1.5 # set the global versionrbenv versions # list all installed Ruby versions 配置gem源 1234gem sources -a http://ruby.taobao.org/ -r https://rubygems.org/echo 'gem: --no-document' \\&gt;\\&gt; \\~/.gemrcgem updategem update --system 安装 MongoDB, MySQL 1brew install mongodb mysql 設置開機自啓動「可選」 123mkdir -p \\~/Library/LaunchAgentsln -sfv /usr/local/opt/mongodb/\\*.plist \\~/Library/LaunchAgentsln -sfv /usr/local/opt/mysql/\\*.plist \\~/Library/LaunchAgents 安装 Pow 12curl get.pow.cx | shgem install powder Powder 是一套管理工具 SSH-KeyGen 12ssh-keygen -t rsacat \\~/.ssh/id\\_rsa.pub 安裝Rails, sass, compass 以及 hexo 12gem install rails sass compassnpm install -g hexo 安裝必要工具 1234gem install mysql2gem install capistranogem install capistrano-ext Snippets - Download 安裝其他APP","link":"/reinstall-mac-osx-Yosemite/"},{"title":"How to set up networkx in Chinese","text":"Problem description Hi, everynone, when we use networkx to display Chinese, we will find that Chinese cannot be displayed. Solution Download the font in the attachment; https://github.com/hivandu/practise/blob/master/resource/SimHei.ttf Execute in jupyter notebook 12import matplotlibprint(matplotlib.__path__) Find the path to matplotlib, and then cd to this path, after cd to this path, continue cd, cd to the path map-data/fonts/ttf. Then replace the file DejaVuSans,ttf with the file we just. 1$ mv SimHei.ttf nx.draw(city_graph, city_location, with_labels = True, node_size = 10).ttf Among them, the ttf font used. I have uploaded it to everyone.","link":"/set_chinese_for_networkx/"},{"title":"在Yosemite中设置Pow","text":"在Yosemite中，Pow安装和启动是有问题的。这是因为ipfw被移除了，所以如果要在Yosemite中跑Pow，需要做些设置才可以。 Pow 1, 添加文件com.pow到/etc/pf.anchors/目录内 sudo vim /etc/pf.anchors/com.pow 2, 在文件内添加代码: 1234rdr pass on lo0 inet proto tcp from any to any port 80 -&gt; 127.0.0.1 port 20559rdr pass on en0 inet proto tcp from any to any port 80 -&gt; 127.0.0.1 port 20559rdr pass on en9 inet proto tcp from any to any port 80 -&gt; 127.0.0.1 port 20559 NOTE: 代码后一行必须要有换行符，否则会出现语法错误 3, 打开文件/etc/pf.conf 4, 添加代码: rdr-anchor \"pow\"，需要添加到rdr-anchor \"com.apple/*\"下一行 5, 打开文件/etc/pf.anchors/com.apple, 并添加代码: 12load anchor &quot;pow&quot; from &quot;/etc/pf.anchors/com.pow&quot; NOTE: 一样必须有换行符 6, 终端执行: sudo pfctl -f /etc/pf.conf 7, 好了，现在可以打开pf了: sudo pfctl -e","link":"/setting-pow-at-Yosemite/"},{"title":"pokemon","text":"引入依赖和数据 123456789import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as pltimport ospath = os.path.expanduser('~/data/cbcpv/pokemon/')df = pd.read_csv(path + 'pokemon.csv', index_col=0, encoding='cp1252') 探索数据 1df.sample(5) OUT: Name Type 1 Type 2 Total HP Attack Defense Sp. Atk Sp. Def Speed Stage Legendary # 75 Graveler Rock Ground 390 55 95 115 45 45 35 2 False 82 Magneton Electric Steel 465 50 60 95 120 70 70 2 False 79 Slowpoke Water Psychic 315 90 65 65 40 40 15 1 False 123 Scyther Bug Flying 500 70 110 80 55 80 105 1 False 9 Blastoise Water NaN 530 79 83 100 85 105 78 3 False 对比并了解下数据集的各个特征类型: 12345678910111213141516171819202122df.info()# OUT&lt;class 'pandas.core.frame.DataFrame'&gt;Int64Index: 151 entries, 1 to 151Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 151 non-null object 1 Type 1 151 non-null object 2 Type 2 67 non-null object 3 Total 151 non-null int64 4 HP 151 non-null int64 5 Attack 151 non-null int64 6 Defense 151 non-null int64 7 Sp. Atk 151 non-null int64 8 Sp. Def 151 non-null int64 9 Speed 151 non-null int64 10 Stage 151 non-null int64 11 Legendary 151 non-null bool dtypes: bool(1), int64(8), object(3)memory usage: 14.3+ KB 可以看到Type 2这个特征有缺失值, 其他的没有, 而且显示的为正数型, 很符合数据分析的要求. 接下来用散点图研究特征Attack和 Defense的关系 1234567sns.lmplot( x='Attack', y='Defense', data=df, fit_reg=False, hude='Stage') image-20210924232356749 我们这里参数使用了fit_reg=False, 隐藏了回归线. 在Seaborn中是没有单独绘制散点图的方法的,但是通过参数设置,实现了散点图的绘制.如果此参数设置为True image-20210924232544623 接下来用箱线图看下各特征数据分布: 1sns.boxplot(data=df) image-20210924232632091 这个结果显示出, Total, Stage以及Legendary特征的数据是不适合在这里绘制散点图的, 需要对特征进行适当选择 12stats_df=df.drop(['Total', 'Stage', 'Legendary'], axis=1)sns.boxplot(data=stats_df) image-20210924232804901 这样,比较清晰的看出几个特征的数据分布情况了, 非数字的特征自动摒弃. 在研究Seaborn, 我们知道还有用i中研究数据分布的函数sns.violinplot, 我们尝试用它绘制特征Attack相对于特征Type 1的数据(这是一个分类行特征)的分布. 123456df['Type 1'].unique()# OUTarray(['Grass', 'Fire', 'Water', 'Bug', 'Normal', 'Poison', 'Electric', 'Ground', 'Fairy', 'Fighting', 'Psychic', 'Rock', 'Ghost', 'Ice', 'Dragon'], dtype=object) 上面显示了特征Type 1中唯一数据, 即数据的值. 123456789101112131415161718192021222324252627282930313233343536373839sns.set( style='whitegrid', rc={ 'rigure.figsize':(11.7, 8.27) # 设置了画布的尺寸 })pkmn_type_colors=[ '#78C850', # Grass '#F08030', # Fire '#6890F0', # Water '#A8B820', # Bug '#A8A878', # Normal '#A040A0', # Poison '#F8D030', # Electric '#E0C068', # Ground '#EE99AC', # Fairy '#C03028', # Fighting '#F85888', # Psychic '#B8A038', # Rock '#705898', # Ghost '#98D8D8', # Ice '#7038F8', # Dragon]sns.violinplot( x='Type 1', y='Attack', data=df, inner=None, # 去掉提琴图中的竖线 palette=pkmn_type_colors)sns.swarmplot( x='Type 1', y='Attack', color='k', # 数据的点的颜色 alpha=0.7 )plt.title('Attack by Type') image-20210924233358985 pkmn_type_colors是一个列表, 列出的颜色对应着特征Type 1中的唯一值. 因为去掉了提琴图内部的竖线,所以整个图没有太乱, 想知道有竖线的是什么样子, 可以注释掉inner=None这个参数. 之前我们删除了三个特征得到了一个变量stats_df引用的数据集: 1stats_df.sample() OUT: Name Type 1 Type 2 HP Attack Defense Sp. Atk Sp. Def Speed # 128 Tauros Normal NaN 75 100 95 40 70 110 数据结果中看出来, 特征HP Attack Defense Sp.Atk Sp.Def Speed都是整数, 在df.info()中也能看出来.现在有需求, 如果把这些特征分布进行可视化, 而且要放到一个坐标系中进行比较? 参考: 先使用pd.melt函数, 将所指定的特征进行归并 123456melted_Df=pd.melt( stats_df, id_vars=['Name', 'Type 1', 'Type 2'], # 保留的特征 var_name='Stat' # 其余特征规定到这一列内)melted_df.sample(10) OUT: Name Type 1 Type 2 Stat value 291 Kabutops Rock Water Attack 115 406 Marowak Ground NaN Defense 110 821 Machoke Fighting NaN Speed 45 129 Gyarados Water Flying HP 95 281 Lapras Water Ice Attack 85 586 Vaporeon Water NaN Sp. Atk 110 483 Nidoqueen Poison Ground Sp. Atk 75 93 Gengar Ghost Poison HP 60 791 Vulpix Fire NaN Speed 65 481 Nidoran‰ªÛ Poison NaN Sp. Atk 40 这样,在melted_df数据集中的Stat特征中的数据就是分类数据, 值是stats_df中被归并的特征名称. 12345melted_df['Stat'].unique()# OUTarray(['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed'], dtype=object) 在此基础上, 我们绘制反应分类特征数据分布的图示. 12345sns,swarmplot( x='Stat', y='value', data=melted_df) image-20210924234335100 还可以在此基础上,再叠加一层分类: 1234567sns.swarmplot( x='Stat', y='value', data=melted_df, hue='Type 1' # 叠加一层分类)plt.legend(bbox_to_anchor=(1, 1), loc=2) image-20210924234603758","link":"/pokemon/"},{"title":"Run xgboost on Mac and Regression data","text":"The source code: xgboost_regression update at 2021-09-07: Install xgboost on Apple M1 1234567git clone --recursive https://github.com/dmlc/xgboostmkdir xgboost/my_buildcd xgboost/my_buildCC=gcc-11 CXX=g++-11 cmake ..make -j4cd ../python_package/Users/xx/miniforge3/envs/tf/bin/python setup.py install u must install miniforge for M1, conda create -n tf python=3.9.5 Run xgboost In the process of using xgboost, I encountered a small obstacle, that is, xgboost cannot be run normally on the M1 of the Mac. It needs to be tossed. The following is the installation process: 1. Homebrrew is required first 2. Install gcc and cmake 123brew install gccbrew install cmakebrew install libomp 3. Download xgboost package Yes, you cannot use the network package to install, you need to download, compile and install by yourself. Fortunately, the process is not troublesome: Source: http://mirrors.aliyun.com/pypi/simple/xgboost/ I downloaded xgboost-1.4.2.tar.gz 4. Installation Enter cd ~/download/ run 1pip install xgboost-1.4.2.tar.gz Okay, you can introduce it to try 12from xgboost import XGBClassifierxb = XGBClassifier() xgboost Regression load data 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import pandas as pdimport warnings%pylab inlinewarnings.filterwarnings('ignore')# load data from urldf = pd.read_csv('./data/Titanic.txt', sep=',', quotechar='&quot;', encoding='ISO 8859-15')df.info()df.head()# Filter some featuresfeatures = df[['pclass', 'age', 'sex']]# Labellabel = df['survived']features.info()# Missing values ​​are filled with meanfeatures['age'].fillna(df['age'].mean(), inplace=True)features.info()# Divide the datasetfrom sklearn.model_selection import train_test_splittrain_x, test_x, train_y, test_y = train_test_split(features, label, test_size = 0.25, random_state=33)# Feature vectorizationfrom sklearn.feature_extraction import DictVectorizervec = DictVectorizer(sparse = False)train_x = vec.fit_transform(train_x.to_dict(orient='record'))test_x = vec.transform(test_x.to_dict(orient='record'))# Random forest training and predictionfrom sklearn.ensemble import RandomForestClassifierrfc = RandomForestClassifier()rfc.fit(train_x, train_y)print('The accuracy of random Forest Classifier on testing set:', rfc.score(test_x, test_y))&quot;&quot;&quot;The accuracy of random Forest Classifier on testing set: 0.7781155015197568&quot;&quot;&quot;# xgboost training and predictionfrom xgboost import XGBClassifierxb = XGBClassifier()xb.fit(train_x, train_y)print(f'The accuracy:', xb.score(test_x, test_y))&quot;&quot;&quot;The accuracy: 0.7750759878419453&quot;&quot;&quot;","link":"/run_xgboost_on_M1_and_regression/"},{"title":"模拟器的app2sd","text":"一个读者@XXX(因为个人意愿隐掉名字) 发mail询问我关于windows下模拟器app2sd的问题,先不说有没有必要,说一下我测试的结果! 本来我给他回邮件是让他试一下apptosd.apk的,后来越想越不对劲,所以自己做了下测试!结果如下: 然后通过adb shell也无法操作. # mkdir /system/sd/app mkdir /system/sd/app mkdir failed for /system/sd/app, No such file or directory 不过仔细想想,app2sd必须满足的条件我们在windows上根本就不存在,首先我们必须要一个app2sd的支援固件!然后我们需要sdcard分出一个ext2的分区... 而这两个条件全部都不满足!那基本上可以说没有办法! 看以后有没有高手可以实现模拟器上安装修改固件,那么可以安装一个app2sd的固件,而另外一个必须满足的条件就是必须将建立的虚拟sdcard分出一个ext2分区来! 满足了这两个条件,那么所有的都会水到渠成! BTW:下午这位读者给我的回复: 非常感谢！ 我测试的结果跟您是一样的，不过后面那个建目录的不一样。下面是我对您博文上的一点分析。 然后通过adb shell也无法操作. 【XXX】google好象改过linux内核，adb shell登录以后几种命令都有权限限制。在虚拟sd卡上建立文件夹有所有权限，然而用adb push传上去的就没有可执行的权限，使用chmod命令修改权限也不成功。 # mkdir /system/sd/app 12mkdir /system/sd/appmkdir failed for /system/sd/app, No such file or directory 【XXX】在system目录下adb shell 命令是没有写权限的。你这个尝试如果是mkdir /system/sd就会报“mkdir failed for sd, Read-only file system”的错误，但是在data目录下就能够创建目录。 不过仔细想想,app2sd必须满足的条件我们在windows上根本就不存在,首先我们必须要一个app2sd的支援固件!然后我们需要sdcard分出一个ext2的分区… 而这两个条件全部都不满足!那基本上可以说没有办法! 看以后有没有高手可以实现模拟器上安装修改固件,那么可以安装一个app2sd的固件,而另外一个必须满足的条件就是必须将建立的虚拟sdcard分出一个ext2分区来! 【XXX】appsd的固件这个是什么概念？用mksdcard创建的虚拟sdcard不就是对应的手机上的sdcard么？虚拟sdcard为什么要分出一个ext2分区呢？ext2分区一个什么概念，sdcard要分出ext2分区的原理是什么？ 能否简单介绍一下？或者介绍一下相关的资料？谢谢。 另：对您给我传的那个apk我不是很了解，这个文件从哪里来的，它都做了些什么事？ 下面是我adb shell后ls –l查看到的各文件夹权限，是有加载虚拟sdcard的。 123456789101112131415drwxrwxrwt root root 2009-08-10 04:45 sqlite_stmt_journalsdrwxrwx--- system cache 2009-07-21 09:01 cached---rwxrwx system system 2009-08-10 04:52 sdcardlrwxrwxrwx root root 2009-08-10 04:45 etc -&gt; /system/etcdrwxr-xr-x root root 2009-05-15 00:53 systemdrwxr-xr-x root root 1970-01-01 00:00 sysdrwxr-x--- root root 1970-01-01 00:00 sbindr-xr-xr-x root root 1970-01-01 00:00 proc-rwxr-x--- root root 9075 1970-01-01 00:00 init.rc-rwxr-x--- root root 1677 1970-01-01 00:00 init.goldfish.rc-rwxr-x--- root root 106568 1970-01-01 00:00 init-rw-r--r-- root root 118 1970-01-01 00:00 default.propdrwxrwx--x system system 2009-05-15 00:58 datadrwx------ root root 1970-01-01 00:00 rootdrwxr-xr-x root root 2009-08-10 04:46 dev 这里请注意system，sdcard和data它们的权限以及各自的意义。 123d---rwxrwx system system 2009-08-10 04:52 sdcarddrwxr-xr-x root root 2009-05-15 00:53 systemdrwxrwx--x system system 2009-05-15 00:58 data 下面说下我对各个信息的理解。首先第一列这是表示的各用户的权限，d代表这是文件夹，rwx分别代表读、写、执行权限。 d后面第一组三个字符表示当前用户的读写执行权限，第二组代表group用户的权限，第三组表示other用户的权限。 然后是第二列，表示当前用户对该文件夹的权限级别，第三列代表该文件夹的当前用户。 如果我对这组信息的含义理解方式正确的话，那么这里我就有疑问了： 1. linux下面有system这个权限级别吗？我有个同事说只有root、group和other，所以我很奇怪这里的system这个权限级别是怎么回事，它有什么样的权限，能做到怎么样。 2. sdcard这个目录非常奇怪，自己的用户权限都没有，group和other用户却有所有权限，在sdcard目录里面建立的目录权限跟sdcard的权限一样。 3. 我们自己写的应用程序，不知道是属于什么样的权限级别，是作为什么样的用户来访问各目录包括sd卡的，手机sd卡和虚拟sd卡。","link":"/simulator-app2sd/"},{"title":"socialnomics VS Sina","text":"注： 这篇文是转载自Google+上 @Isaac Mao。地址 在此之前，还真不知道socialnomics这样一个概念。我在自己的博客上很少转发别人的文，转发此文，原意完全是为了能更广的传播，因为Google+相当一部分人打不开，而文又比较长不适合发微博，即便发了，相信Sina上也很快就回删除。让Sina自己和自己玩去吧。 转文： #SINA #Alert 自6月17日新浪微博刪除我賬戶以來，以每天平均10條微博，10美元/條的低價索賠，並發分析新浪審查自毀的信件給全球基金經理建議“削” $SINA 股票 http://stocktwits.com/symbol/SINA?q=… 是用來劫殺新浪股票 $SINA 的一個社會媒體網站，讓草根投資者社群也了解新浪的吞尾風險也是必要的，給基金經理的信函是呼應。新浪一天不對封殺帳號進行補償，就要承擔更大代價 一邊剝開新浪股票的風險，還可以一邊 Short $SINA 然後 來呼應，從頹勢股票中賺錢才是自由市場的真諦 （大家都可以做） 一家基金就能把 $SINA 壓下去，然後其他聰明一點的基金也都會問怎麼回事，不過我更相信草根的翅膀 http://stocktwits.com/symbol/SINA?q=… #Alarm 自從6/17日新浪刪除本人等帳號以來，其股票 $SINA 下跌$4.24美元(8.02%) ，市值蒸發2億美金，但還遠遠不夠補償，大家受壓後不在新浪轉世就是砝碼 2006年抵制MSN Space聲援@mranti等網誌作者，這個服務在2009年自廢；2007年公開信警告谷歌不要自我審查，否則會自推到角落，谷歌2010年退出中國市場。新浪自以為玩轉本土，其實是更深的自掘 社會性媒體其實與經濟的對應關係正在勾連起來，這個研究領域叫做 #socialnomics ，也就是社會力量對經濟指標的影響，品牌正邪與價值的對應關係會越來越明顯。新浪自以為刪除某個帳號無所謂，殊不知這種信號就是走衰的開始。 備考：7月18日，北京時間0:35，Stocktwits 也停了我的帳號， http://stocktwits.com/symbol/SINA 還派“羊水軍“來洗白，可見新浪 $Sina 正在多麼努力挽救自己。推友們很容易把你們給淹死誒","link":"/socialnomics-vs-sina/"},{"title":"sketch中打开高版本文件","text":"sketch也不知道什么时候开始年费化了，也不能打开高版本文件了。（妈蛋） 据说是为了促进销量和保护版本。 打开包文件，然后打开包内的meta.json 替换头部: 1{&quot;commit&quot;:&quot;335a30073fcb2dc64a0abd6148ae147d694c887d&quot;,&quot;appVersion&quot;:&quot;43.1&quot;,&quot;build&quot;:39012 替换尾部 1&quot;commit&quot;:&quot;335a30073fcb2dc64a0abd6148ae147d694c887d&quot;,&quot;build&quot;:39012,&quot;appVersion&quot;:&quot;43.1&quot;,&quot;variant&quot;:&quot;NONAPPSTORE&quot;,&quot;version&quot;:88},&quot;version&quot;:88,&quot;saveHistory&quot;:[&quot;NONAPPSTORE.39012&quot;],&quot;autosaved&quot;:0,&quot;variant&quot;:&quot;NONAPPSTORE&quot;} 这里实际有几个key:commit, appVersion, build, version,NONAPPSTORE value替换成相应的值就OK了。","link":"/sketch-open-hight-version-file/"},{"title":"玩转Stable Diffusion WebUI 各类模型","text":"Stable Diffusion WebUI 最有意思的地方不是在安装好之后生成图像，而是各种各样的模型。 提前警告：如果你的硬盘空间不够大的话，还是不要随便玩模型了，随随便便就是好几 G，又得甚至于 10 多个 G。 目前我仅留了最常用的 SD V1.5 和 SD V2.1两个模型，大小为 13G。 notion image 另外还需要说明一点，就是我曾经测试过用 NAS 来存储模型使用，完全不能用，暂时没有时间具体去研究到底什么原因。只有老老实实的继续在本地硬盘上跑。所以 NAS 上存了大量模型，真需要用到的时候再复制过来。 写这篇文章也是因为近期玩模型过程中打算整理一波，一是方便自己，二么也算是对其他小伙伴做些贡献。 Stable Diffusion 各种模型层出不穷，要说完估计需要费一番功夫，所以我摒弃其他小模型，只整理收集大模型，就是 ckpt 和 safetensors。如果你也打算跟着我一起玩模型但是还未安装，可以先参看我之前的文章： 在 Apple Silicon M1/M2 Mac 上安装和运行Stable Diffusion 说实话，我找了好多关于如何在 M1/M2 上安装和运行 Stable Diffusion 的教程和帖子，发现相互之间借鉴的不少，但是能用的确实没几个。 寻找一番后，发现其实没那么复杂。也不知道为什么网上的那么多教程搞得那么复杂，又是这个又是那个的一大堆，简单实现的方式有好几种： https://www.hivan.me/How%20to%20install%20and%20run%20Stable%20Diffusion%20on%20Apple%20Silicon 在 Apple Silicon M1/M2 Mac 上安装和运行Stable Diffusion 还是先从最基础的模型开始： Stable Diffusion 其他多数模型基本上都是从这个基础模型上再次训练得到的。 Stable Diffusion v2.1 SDv2.1提升了人物生成能力，因为SDv2.0大量增加了风景、建筑物和动物的数据集，减少了人物的学习量。 SDv2.1提高了NSFW过滤器准确度，因为SDv2.0的成人过滤器过滤的太狠，错误判定很多 即使是极端长宽比的图像也能顺利生成。 解剖学的身体和手（特别是手掌）的描写精度提高。 512 X 512 model : stabilityai/stable-diffusion-2-1-base · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-1-base stabilityai/stable-diffusion-2-1-base · Hugging Face 768 X 768 model: stabilityai/stable-diffusion-2-1 · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-1 stabilityai/stable-diffusion-2-1 · Hugging Face img2img model stabilityai/stable-diffusion-2-depth · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-depth stabilityai/stable-diffusion-2-depth · Hugging Face 重绘model stabilityai/stable-diffusion-2-inpainting · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-inpainting stabilityai/stable-diffusion-2-inpainting · Hugging Face 超分 model stabilityai/stable-diffusion-x4-upscaler · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler stabilityai/stable-diffusion-x4-upscaler · Hugging Face Stable Diffusion V 1.5 runwayml/stable-diffusion-v1-5 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main runwayml/stable-diffusion-v1-5 at main Stable Diffusion V 1.4 CompVis/stable-diffusion-v-1-4-original · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/CompVis/stable-diffusion-v-1-4-original CompVis/stable-diffusion-v-1-4-original · Hugging Face NovelAI 大名鼎鼎的 NovelAI，属于商业泄露模型。经过人在回路精细微调，可以生成高质量的二次元图像。但是千万时刻记得这个可是商用泄露模型，要注意避免法律风险： pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/animefull-latest.tar Waifu Diffusion 基于 Stable Diffusion 模型训练得到，增加了动漫及人物训练得到的模型，基本平时各种公开场合看到 WD 就是他。 WD 和 NovelAI 模型有些同质化，但是 NovelAI 实际是商用模型泄露，在某些使用情况下是有风险的。而 WD 不是，不过也不是说他绝对安全，毕竟 WD 也使用 Danbooru 进行学习，所以如果你关心这个需要注意一点。 Waifu Diffusion V1.5 这个模型使用是需要一个 yaml 文件的，究其原因是这个模型是基于 SD V2 得出的，需要把和 Model 同名的 yaml 文件放在模型所在的文件夹下，目前 1.5 模型是 beta2 版本，持续迭代 ing… Waifu Diffusion v1.5 beta waifu-diffusion/wd-1-5-beta · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/waifu-diffusion/wd-1-5-beta waifu-diffusion/wd-1-5-beta · Hugging Face VAE(1.4 VAE 通用) vae/kl-f8-anime2.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt vae/kl-f8-anime2.ckpt · hakurei/waifu-diffusion-v1-4 at main YAML waifu-diffusion/wd-1-5-beta2 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/waifu-diffusion/wd-1-5-beta2/tree/main/checkpoints waifu-diffusion/wd-1-5-beta2 at main Waifu Diffusion V1.4 和 1.5 版本一样，基于 SD V2得到的，依然需要下载 yaml 文件放在 model 同文件夹下。 Waifu Diffusion V 1.4 wd-1-4-anime_e1.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e1.ckpt wd-1-4-anime_e1.ckpt · hakurei/waifu-diffusion-v1-4 at main wd-1-4-anime_e2.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e2.ckpt wd-1-4-anime_e2.ckpt · hakurei/waifu-diffusion-v1-4 at main VAE(1.5 通用） vae/kl-f8-anime2.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt vae/kl-f8-anime2.ckpt · hakurei/waifu-diffusion-v1-4 at main YAML e2 和 e1 是通用的，但是需要改名 hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/tree/main hakurei/waifu-diffusion-v1-4 at main *Elysium Anime* 生成偏真实风格的动漫图片，风格比较偏向西式，光影还不错。 模型推荐写下面这些负面提示，可有效提升质量。 1lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry Elysium_V1 偏真实风的模型，手画的还不错，模型底稿基本是以西方人为主，所以生成的脸也偏西方人。 Elysium_V1.ckpt · hesw23168/SD-Elysium-Model at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hesw23168/SD-Elysium-Model/blob/main/Elysium_V1.ckpt Elysium_V1.ckpt · hesw23168/SD-Elysium-Model at main *SD_Elysium_Kuro_Model* 与Anything 4.0、WD 1.4等合并后经过微调的二次元用模型。已经包含 WD 的“kl-f8-anime2”VAE 文件，因此无需使用额外的 VAE 文件 Elysium_Kuro_Anime_V1.safetensors · hesw23168/SD_Elysium_Kuro_Model at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hesw23168/SD_Elysium_Kuro_Model/blob/main/Elysium_Kuro_Anime_V1.safetensors Elysium_Kuro_Anime_V1.safetensors · hesw23168/SD_Elysium_Kuro_Model at main *Elysium_Anime_V3* 动漫的附加学习模型，NSFW化相当严重，有更清晰的轮廓和轻微的三维效果。基于Elysium_V1 Elysium_Anime_V3.safetensors · hesw23168/SD-Elysium-Model at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hesw23168/SD-Elysium-Model/blob/main/Elysium_Anime_V3.safetensors Elysium_Anime_V3.safetensors · hesw23168/SD-Elysium-Model at main *Anything系列* Anything是个神奇的二次元模型，据说是基于几十种模型融合+未知图片训练而来，随便写几个提示，就能到的不错的结果。不过这个模型整个就是一团混沌，实际训练模型，过程，方法，作者全部都是未知的。模型容易过拟合，非专业人士，请不要在此基础上训练模型。 Anything v3.0 “应该”是基于NAI模型+WD+SD等几十种模型+手部图片强化训练得出的。实际训练模型，过程，方法，作者全部都是未知的。如果没有.vae.pt，图片整体颜色浓度（饱和度）会更很浅。PS：Anything v3.0 的 .vae.pt 文件可以用于 NAI。 Anything V3.0 fp16: magnet:?xt=urn:btih/:45cd353ac4fa87098db5e3a6a349539710a3a1f5&amp;dn=Anything-V3.0-fp16.zip Anything v3.0 fp32: magnet:?xt=urn:btih/:d9db662ab5ace77004b3348c23c9381380c27156&amp;dn=Anything-V3.0-fp32.zip Anything v3.0 full-ema: magnet:?xt=urn:btih/:80460036625fb61dce4bc6e7dab744744309a2a0&amp;dn=Anything-V3.0-fullema.zip huggingface.co https://huggingface.co/Linaqruf/anything-v3-better-vae/tree/main Anything v4 自称是Anything最新版本的模型，实际一切都是未知的。仅需几个提示即可生成详细的 2D 插图的能力以及使用 danbooru 标签的能力。整体比过拟合的v3更自然，人物姿势等更容易操作。 anything-v4.0-pruned.safetensors · andite/anything-v4.0 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/andite/anything-v4.0/blob/main/anything-v4.0-pruned.safetensors anything-v4.0-pruned.safetensors · andite/anything-v4.0 at main Anything v4.5 貌似是Anything v4的进化，但实际一切都是未知的。比v4画风更柔和一点。 anything-v4.5-pruned.safetensors · andite/anything-v4.0 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/andite/anything-v4.0/blob/main/anything-v4.5-pruned.safetensors anything-v4.5-pruned.safetensors · andite/anything-v4.0 at main Zeipher 生成更符合真人解剖结构的真人模型，训练集以女性图像为主官方网站是 https://ai.zeipher.com，已经关闭。请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 F222 f222.safetensors · acheong08/f222 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/acheong08/f222/blob/main/f222.safetensors f222.safetensors · acheong08/f222 at main F111 f111.ckpt · Reachout/F111 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/Reachout/F111/blob/main/f111.ckpt f111.ckpt · Reachout/F111 at main 3DKX 因为Zeipher官方已经GG，这是热心网友创建的衍生3DKX模型如果你想让你的 3D 角色有一张更“二次元”的脸，提示词最开始写 “3d cartoon of”，或者如果你想要经典的 3D 渲染外观，写“a 3d render of”高分辨率模型，推荐分辨率为 1152 x 768 或更高 3DKX_1.0b f111.ckpt · Reachout/F111 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/Reachout/F111/blob/main/f111.ckpt f111.ckpt · Reachout/F111 at main R34 从网站“rule34.xxx”的 150,000 张图像中进行训练。rule34.xxx几乎全是NSFW图片，所以你懂的 r34_e4 1.99 GB file on MEGA 1.99 GB file on MEGA https://mega.nz/file/yJgDUCzA#zOD2yeE6QLBqPEjEpIi2b4FWOlb64yVUveOd_eW6teI 1.99 GB file on MEGA 磁力链接：magnet:?xt=urn:btih/:ed9f0e3f849d7119107ef4e072c6abeb129e1a51&amp;dn=r34_e4.ckpt EVT pixiv排行榜模型 基于pixiv排行图片训练，夹杂有部分R18排行图片 Evt_V4_e10_ema Evt_V4_e10_ema.safetensors · haor/Evt_V4-preview at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/haor/Evt_V4-preview/blob/main/Evt_V4_e10_ema.safetensors Evt_V4_e10_ema.safetensors · haor/Evt_V4-preview at main EVT_V3 haor/Evt_V3 · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/haor/Evt_V3 haor/Evt_V3 · Hugging Face EVT_V2 haor/Evt_V2 · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/haor/Evt_V2 haor/Evt_V2 · Hugging Face Basil_mix 逼真的真人模型，基于亚洲风格训练，支持Danbur标签提示词需要加载VAE，不然画面色彩浓度和边缘会很淡提示词应尽可能简单不要堆砌大量质量标签和负面提示，不然会适得其反。请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 basil_mix basil mix.ckpt · nuigurumi/basil_mix at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/nuigurumi/basil_mix/blob/main/basil%20mix.ckpt basil mix.ckpt · nuigurumi/basil_mix at main VAE vae-ft-mse-840000-ema-pruned.ckpt · stabilityai/sd-vae-ft-mse-original at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.ckpt vae-ft-mse-840000-ema-pruned.ckpt · stabilityai/sd-vae-ft-mse-original at main Chillout Mix 逼真的真人模型，基于亚洲风格训练，支持Danbur标签提示词请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 chillout mix _ NiPruned Fp32 Fix chilloutmix_NiPrunedFp32Fix.safetensors · Inzamam567/useless_Chillout_mix at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/Inzamam567/useless_Chillout_mix/blob/main/chilloutmix_NiPrunedFp32Fix.safetensors Uber Realistic Porn Merge 如名字所说，逼真的真人Porn模型，简称 URPM 模型请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 Uber Realistic Porn Merge Uber Realistic Porn Merge (URPM) | Stable Diffusion Checkpoint | Civitai For early access builds and to support daily work on URPM, please check out my patreon! https://www.patreon.com/uber_realistic_porn_merge , or disc... Uber Realistic Porn Merge (URPM) | Stable Diffusion Checkpoint | Civitai https://civitai.com/models/2661/uber-realistic-porn-merge-urpm","link":"/stable-diffusion-webUI-models/"},{"title":"vue 2.0 自定义filter并挂载到全局使用","text":"其实早几天前就进入了第三部分,而因为第二部分面向对象程序设计糊里糊涂,搞得再接下来的学习里有很多实例根本看不懂,或者很难回忆起所学的知识点.不得不回头又一个字一个字的老老实实的看了一遍,并且将每个字都敲出来更新到了Sites上!至此算是比较牢固了..而更新的部分,相信对于想进入Java世界的新手们也算是一个好的参考. 其中前两部分分为八章,第一部分为基础程序设计,第二部分为面向对象程序设计 ,第三部分则是实战的Java应用程序设计.我询问过一个朋友,说是对于Android开发来说,Java的前两部分属于基础,非常重要.而第三部分也就无所谓了.不过想来,还是将所有的东西全部抓牢以后再开始.毕竟Java已经学到这一步,规规矩矩做一个Java开发者也未尝不可.下面给出Java前两部分的链接,算是再次推广我的Learn Wiki.而第三部分将不会在此更新了.有兴趣的自己跟着我在Sites上的更新翻看吧,更新速度要视我自己的学习速度而定!现在手上的教程为:《Java 开发与实战经典》 image 第一部分:Java基础程序设计 1.Java概述及开发环境搭建 2.简单的Java程序 3.Java基础程序设计 4.数组与方法 第二部分:Java面向对象程序设计 5.面向对象(基础篇) 6.面向对象(高级篇) 7.异常的捕获与处理 8.包及访问控制权限 第三部分将不会继续在这里给出链接.请自行查看 http://learn.hivan.me/index/home/java-learn/Java-DAP","link":"/the-third-part-of-the-java-learning/"},{"title":"SOLOVE Air-M20000","text":"第一次为产品写评测吧我这是，也不太记得了。不过以下这个电源，觉得值得写上一篇。 「SOLOVE移动电源 Air-M20000」 照例先来几张开箱图 images images images 本来应该再早一个月拿到这款电源的，因为产能的原因，跳票了。好吧，后来又因为快递单的遗漏，跳票了更久。不过好东西都是值得等待的。 第一次打开包装盒的时候，拿在手上满满的质感，身躯娇小，容量却很大。不过说起来，因为控制了高宽的原因（高宽仅与一张信用卡大小相仿），厚度不太理想。达到了一枚1元硬币的厚度，虽然握在手上的手感十分舒服，但是如果对于想揣在衣服口袋里的人来说，可能这个厚度稍稍有点不甚让人满意，说起来，10000mAh的容量，能做到如此地步已经很不容易了。 而对于SOLOVE Air-M20000的设计，相信也是能俘获很多人的心。电源灯的效果真的是很漂亮。 images 也许是因为控制尺寸的原因吧，SOLOVE Air-M20000并没有像其他大容量移动电池一样配备两个输出插孔，只有一个。输入Micro-USB, 输出为USB-A. 不过联想多数时候，我的小米电源另一个输出口都空的情况，其实一个已经足够了，重点是轻便易携带。 技术参数 上周五这款电源已经到货了，没有第一时间拿来写当然是为了接受下周末的检验。 实际使用中，周五的晚上电源满电，周六出门12点半到下午6点，iPhone 6从3点开始电量百分之15，接入SOLOVE，1小时后达到92%，期间不停的在发微信。然后取下电源，周六没有为SOLOVE充电，周日12点多左右出门，到下午6点多回来，iPhone 6经历了两次空电的情况。而两次SOLOVE都将电量充满。 实际使用情况下，SOLOVE的容量还是不错的，iPhone 6来回充满三到四次应该是不成问题，更重要的是，充电速度很快。在不断使用过程中，一个小时基本就可以完全充满。当然，每个不同型号的手机可能都有差别。 所以SOLOVE的Air-M20000, 无论是从设计，做工还是续航情况，都还是蛮值得入手的。唯一遗憾的点，为什么给了我一个红色的，而不是黑色或白色的。。。。","link":"/solove/"},{"title":"vue 2.0 自定义filter并挂载到全局使用","text":"vue 2.0 开始，取消默认filter, 需要自定义。 而自定义之后每次在需要使用的组件内引用也确实蛮麻烦的。 所以我们就来将定义的filter挂载到全局使用。 vue2.0 filter相关文档 定义 引用 挂载 使用 /src/filters/ - format.js 123export default function(val){ ...} index.js 12345import format from &quot;./format&quot;;export default{ format: format,} /src/ - main.js 123456789...import commonFiltes from './filters/index'Object.keys(commonFiltes).forEach(function (key, index, arr) { Vue.filter(key, commonFiltes[key]);})... /src/components/ - xxx.vue 1234567&lt;template&gt;...&lt;div&gt;{{ data | format }}&lt;/div&gt;&lt;/template&gt;&lt;script&gt;...&lt;/script&gt;","link":"/vue2.0-custom-filter-to-global/"},{"title":"vux更改Tabbar选中状态","text":"在vux的文档和示例中，都没有明确的说明tabbar上v-model的使用 文档中将v-model说明放在了TabbarItem示例下，但是其实这个应该是放在Tabbar上 1234567891011121314151617181920212223&lt;template&gt; &lt;router-view class=&quot;view&quot; v-on:changeTab=&quot;changeTab&quot;&gt;&lt;/router-view&gt; &lt;tabbar v-model=&quot;index&quot;&gt; &lt;tabbar-item&gt;&lt;/tabbar-item&gt; ... &lt;tabbar-item&gt;&lt;/tabbar-item&gt; &lt;/tabbar&gt;&lt;/template&gt;&lt;script&gt;data(){ return{ index:0, ... }}methods:{ changeTab(num){ ... this.index = num; ... }}&lt;/scirpt&gt; 然后子组件中调用 123mounted(){ this.$emit('changeTab', 2)} 这样就便于在不同的组件内都可以更改Tabbar选中状态","link":"/vux-tabbar-selected/"},{"title":"使用WAMP5搭建Apache+MySQL+PHP环境","text":"目前有不少AMP（Apache）的集成软件，可以让我们一次安装并设置好。这对于不熟悉AMP的用户来说，好处多多。 一、使用AMP集成软件的优点： 1、可避免由于缺乏AMP的知识，而无法正确设置环境； 2、可快速安装并设置好AMP环境，让我们直接开始真正感兴趣的软件，如xoops； 3、可方便的搭建测试环境，对于测试“是AMP环境问题，还是XOOPS造成的问题”很有帮助，采用排除法即可。 二、常用的AMP集成软件： 1、AppServ：http://www.appservnetwork.com/ 这个软件在台湾很流行。看到不少书籍也极力推荐，估计都是受台湾用户的影响。 2、XAMPP：http://www.apachefriends.org/en 这个软件支持多个平台,WinOS X，目前也有不少人使用。 3、WAMP5：http://www.wampserver.com/en/ 这是我今天极力推荐的，绝对五星级。注意它的名字是带个5的哦，意思就是WAMP5使用最新的PHP5版本，正如官方网站上的口号：Discover PHP5 with WAMP5 ! 三、根据我的经验，WAMP5有如下优点： 1、XOOPS在WAMP5中使用，没有任何问题。 2、WAMP5专注于Windows平台，安装设置及其简单。 3、PHP默认的是5.x版本，如果需要php4.x，只要安装php4.x插件，就可以在两者之间自由的切换，非常方便。 4、MySQL默认的是5.x版本，但可以通过选择老版本的WAMP5，从而使用4.x的MySQL。 5、可视化的菜单管理，极其方便。如，打开关闭php extention、Apache module等，直接通过菜单选择就可以。 6、还有各种插件，如ZEND OPTIMIZER ADD-ON等。 7、如果有疑问，官方还有论坛可以求助。 ……更多优点：谁用谁知道，早用早知道^_^ 四、使用WAMP5的经验、技巧 1、安装时的设置：可以自定义WWW根目录的存放位置哦，强烈建议放到D盘等安全的分区中，以避免万一系统崩溃，造成数据丢失。 2、对于中文用户来说，安装结束后，首先要设置的是，把MySQL的数据库默认编码改为UTF-8，这样可以排除很多中文乱码问题：在WAMP5菜单中选择打开my(wamp).ini，设置其中的default-character-set=utf8, 然后重启WAMP5。 3、数据库默认的密码是空的，可以在phpMyAdmin中设置root帐号的密码为123456；当然修改之后，就要跟着修改phpMyAdmin的配置文件config.inc.php，否则phpMyAdmin就进不了数据库啦： &gt;\\(cfg[’Servers’][\\)i][’user’] = ‘root’; &gt;\\(cfg[’Servers’][\\)i][’password’] = ‘123456′; 4、如果需要mysql4.x + php4.x，可选用WAMP5 1.44版本以及插件PHP4.3.11； 5、由于MySQL4.1之后版本对密码验证的方法发生了改动，如果在WAMP5中使用php4.x，那么就需要启用MySQL的old password功能，否则无法登陆phpMyAdmin。在WAMP5菜单中选择MySQL/Mysql console，然后输入下列命令： &gt;mysql&gt; SET PASSWORD FOR &gt;-&gt; ‘root’@'localhost’ = OLD_PASSWORD(’123456′); 6、XOOPS用户关心的时区问题：WAMP5默认的时区是格林威治标准时间(GMT)，这就意味着WAMP5默认的服务器时区是GMT，但我们可以更改服务器默认时区，以对应北京时间。打开WAMP5菜单中的php.ini，在文档最后添加如下代码即可： &gt;[Date] &gt;; Defines the default timezone used by the date functions &gt;date.timezone = “Asia/Shanghai” 如果你不在北京，那么就改动上述的设置即可，具体设置值可参考：http://us2.php.net/manual/en/timezones.php 通过上述WAMP5的设置之后，我们在XOOPS中的时区设置就可以这样：服务器时区、默认时区、个人帐号的时区这三者都设置为上述的时区就可以了，如：北京时间（GMT+8） OK，这就是所有的AMP设置秘籍！！","link":"/wamp5-make-apachemysqlphp/"},{"title":"什么事ifttt,ifttt怎么玩?","text":"原帖出自師北寰的網絡日誌，當中很多條目內容也都是來自於Twitter！由於原帖地處偏僻，需要翻山越嶺，所以為了照顧國內用戶，特轉此貼！這也是我為數不多的轉帖之一。當然，我知道，這個服務可能過段時間完全開放後又會和國內無緣！就像G+才出世兩天就被牆了一樣！不過這也充分證明了G+的優秀！而現階段，最好玩的網絡應用就是G+和ifttt。所以一般都是if oo then xx! 什么是ifttt？ ifttt是一坨网站：ifttt.com 即If This Then That，你可以在ifttt上设定一个条件，当达到你设定的一个条件时，便触发一个（你指定）动作。这里的「条件」和「动作」是指开放的互联网服务，比 如flickr，twitter，facebook，youtube等。别问我有没有新浪微博、人人网，优秀且可靠的互联网服务都在国外。 国内没法抄ifttt。ifttt最重要是服务稳定，就国内普遍鸡贼的情况来看，if端服务时不时封闭一下是常事。创业企业抄好了，大公司眼红把自己网站接口关闭再抄一个出来，创业公司就得玩完。大公司抄的话，竞争对手们也会关闭自己的服务，这玩意儿在国内没法玩。 ifttt的稳定性是关键，可以定制一连串的if…then任务出来，但如果中间某一个服务出问题，后面的任务就全失效了（当然，这么定制也挺笨的）。 ifttt非常重要的一个优点是，将常用服务（twitter，加星或分享的Google reader条目，加过标签的instagram和flickr照片）中的重要资料，全部发送到一个存储服务（Dropbox，evernote，Gmail），需要用的时候检索起来将非常方便。 ifttt可以怎么玩？ 好玩的可以有：if某女谈论「失恋」、「男友+讨厌」、「伤心」、「难过」，then 发送一条短信。ifttt泡妞必备… 非常实用的应用可以有：New fav tweets to evernote ifttt还解决了我以前在北京十分急需的一个功能：if 北京美国大使馆空气监测站的空气质量指数超过250，then 发送一条短信…当然，其它方法也能实现，但ifttt方便太多了。 未来的应用有：ifttt的出现真可以实现未来你挂了也能一个人办丧事：if三十天未发推，then启动一系列任务：1.发邮件告诉殡仪馆来收尸（亲，你可以看我的google location）；2.自动转账；3.发表遗书告诉亲友可于30天后到某处悼念；4.分享生前录好的视频，最后再操一遍GFW。 在未来，随着越来越多社交服务的出现，以及多条件任务功能的推出，玩ifttt的花样将呈爆发式增长，乐趣无穷。 ifttt还可以有什么玩法？ @mranti: ifttt应用举例：if 某男A和某女B同时check in同一个地方，then 短信我的手机：“A和B有奸情，而且正在进行”。八卦利器啊！ @hecaitou: 理想状态下的ifttt应用场景：一旦老婆的推上出现“加班”字样，立即激活一条手机短信通知。同时，自动检测谷歌日历，找出几个今晚没有事情的老友。随后，在FB上新建一个活动“今晚喝大酒”，一旦超过3人同意，触发一条订餐消息给餐厅。餐厅查询Evernote，找到这群人最喜欢的菜和酒。on Twitter: http://twitter.com/hecaitou/status/85927850749857792 @mranti: ifttt应用举例：if 明天下雨，发推DM给自己的心仪女友：“亲爱的，明天出门带伞，我是你的阳光”。 on Twitter: http://twitter.com/mranti/status/85927810924937219 最后这两条和菜头和安替的推是我在twitter fav之后，在写这篇文章过程中自动保存到evernote的，服务十分流畅，文章写起来太方便了有没有？ —-以下内容为下午四点十五分更新—- 出去溜达了一圈回来，脑子里一直在想ifttt，ifttt简直是个太科幻的产品了，第一次觉得人工智能——不对，是机器智能——离自己这么近。ifttt比Google、Facebook都要伟大得太多。 ifttt就是一个反射，它把你想象得到的任何一个动作反射为另外一个你能想象得到的动作，并且它不像生物体一样会被躯干束缚。ifttt上将出现拉马克进化？ @hecaitou: ifttt里面，如果在Channel之上，提供一个Task的自由市场。让各种Geek做出各种奇奇怪怪的Task来，用户添加Task而不是点选Channel，那就连盈利的问题都解决了。 on Twitter: http://twitter.com/wuyagege/status/85959272638324736 @mranti: 在ifttt的世界里面，各位姑娘小心了，什么恋爱短信啊、花啊、DM关怀啊、贴心礼品啊，都可能是程序的Task算出来的。而且ifttt的世界中，一个人死了，他对一个女生的关心也可以一直持续下去，仿佛天天都在。 on Twitter: http://twitter.com/mranti/status/85974845216665600 @boatman: ifttt神就神在即使被墙，只要设置好this和that的关联性，墙并无法阻止this触发that，除非GFW把所有的channel全部封锁才有可能抑制ifttt，但当ifttt支持自定义channel时，就是神也难救方滨兴。 on Twitter: http://twitter.com/Ryan_XxOo/status/85975132866220032 @Doriscafe: 我死后，请你替我照顾她。每天给她发短信叫早，订花，在推特上mention她，赞她，天气好提醒加衣，天气不好提醒带伞，请你替我照顾她，只要服务器不倒下，就直到永远。#ifttt on Twitter: http://twitter.com/Doriscafe/status/85975909429018624 @juicy_luna: 我个人觉得吧，#ifttt 就是把生物里的神经反射运用进了网路里，甚至还会扩展到物质生活。也就是说，它担负起神经链的作用，将能把一切行动串联起来，形成纵横的网络。。奇妙的世界。。 on Twitter: http://twitter.com/juicy_luna/status/85976819626549248 @duck_1984: 超级多米诺啊 蝴蝶效应啊 ifttt毁灭世界啊 自寻死路啊愚蠢的人类 还有更多⋯⋯你来补充。","link":"/what-is-ifttt-and-how-to-play/"},{"title":"Windows phone 7 破解工具开放下载","text":"微软Windows Phone 7应用程序商店早前被发现存在严重漏洞，Tobias对微软和各位开发者做以警告，希望微软能竟快的将漏洞补上，但是微软方面没有实质动静。现在终于有一款XAP下载工具露面，WP7 MktPlace 0.36d于今日被正式公开提供下载！ 通过这款强大的XAP下载软件，普通用户只需轻点download就能批量下载Marketplace市场上所有的XAP安装包。然后通过另一个Xap_Dll_Signer签名工具来将版权认证相关内容删除（已通过智机网技术组测试可行），这样一来，所有已经对WP7手机解锁的用户便都可以轻易的在自己的手机上部署Marketplace上的收费软件。暂时不说对开发者是否存在一定的担忧,但是对于国内喜欢免费大餐的用户来说,这一定是个好消息! [download id=\"4\"] 往期版本下载及更多详情请参看官网","link":"/windows-phone-7-carck-download/"},{"title":"Yosemite访问用户级服务器目录","text":"升级到OSX 10.10(Yosemite)以后，localhost是可以正常访问的，只是localhost/~user无法打开了，提示403错误。 网上查找资料，说是随着系统的更新，Apache本本更新到2.4.9，PHP也更新到了5.5.14，所以Apache的配置就需要做相应的修改。 首先，我们需要确定打开了Apache 1sudo apachectl start 然后设置允许访问用户目录 修改httpd.conf配置 1sudo subl /etc/apache2/httpd.conf command + f 查找代码，并去掉注释符 # 123456LoadModule authz_core_module libexec/apache2/mod_authz_core.soLoadModule authz_host_module libexec/apache2/mod_authz_host.soLoadModule userdir_module libexec/apache2/mod_userdir.soLoadModule php5_module libexec/apache2/libphp5.soInclude /private/etc/apache2/extra/httpd-vhosts.confInclude /private/etc/apache2/extra/httpd-userdir.conf 修改httpd-userdir.conf配置 1sudo subl /etc/apache2/extra/httpd-userdir.conf command + f 查找以下代码，去掉注释符# 1Include /private/etc/apache2/users/*.conf 修改yourUserName.conf配置 1sudo subl /etc/apache2/users/username.conf PS: username为你的用户名称，如果没有该文件则新建一个，然后将内容修改为: 123Options Indexes MultiViewsAllowOverride NoneRequire all granted 然后设置文件权限为755 1sudo chmod 755 /etc/apache2/users/haibor.conf 最后我们需要重启Apache 1sudo apachectl restart","link":"/yosemite-open-usersite/"},{"title":"从美国三大协会说供应链管理的演变","text":"前面说到，供应链管理从采购、运营和物流管理发展而来，它是对从供应商的供应商到客户的客户的产品流、信息流和资金流的集成管理，以实现对客户价值的最大化，以及供应链成本的最小化。企业之间的竞争不再是企业与企业之间的竞争，而是供应链与供应链之间的竞争。过去二三十年来，美国的汽车业在日本汽车大厂的强大攻势下一败涂地，就是一个供应链战胜另一个供应链的例子。 在美国，这种集成的供应链管理概念不是一蹴而就的，而是经过几十年的演进发展而来的。这里我们从采购、运营和物流职业协会的发展历史出发，阐述供应链管理在美国的发展——要知道，判断一个职能的发展，最简单的方式就是看相应行业、职业协会的发展。 简言之，供应链管理不是一个领域，而是三个。过去不是，现在不是，在可以预见的将来仍会保持在多个领域齐头并进。这从目前美国跟供应链管理相关的行业协会可见一斑。 供应管理协会（ISM） 供应管理协会是世界上规模最大、影响最大的供应管理组织，拥有四万多会员。它发布的采购经理人指数（PMI）跟踪生产、库存、订单量等变化，是美国经济的风向标，被新闻媒体、学术研究、华尔街和政府部门广泛引用。它的前身是美国采购经理联合会（NAPM）。在100多年的发展过程中，伴随着供应管理在美国公司的重要性不断提升，该协会的侧重点从采购发展到供应管理，再到供应链管理。 图1-3简单地表述了在美国，采购从采购代理发展到采购管理，再发展到供应管理的过程。最早的采购是采购代理，即内部客户确定了需求，找好了供应商，价格也往往都谈好了，采购负责签合同、下订单，把东西买回来。当时的专业协会就叫“采购代理人协会”（1915年成立）。后来，采购说，我不但可以帮你下订单，而且可以帮你找供应商、管供应商、谈价钱、谈合同，于是就变成采购管理，协会也改名“采购经理人协会”（1968年）。再到后来，采购说，我不但可以帮你找供应商、管供应商，而且可以处理运输、物流、进出口，把一切打点好，直到产品进了我们的仓库，于是就演变成了供应管理，专业组织也改名“供应管理协会”（2002年）。 但是，供应管理协会的核心仍然是采购与供应管理，而不是广义上的供应链管理。2002年前后，采购经理协会改名为供应管理协会时，有人问，为什么不改为“供应链管理协会”？ISM的答复是供应链管理太广泛，还不够定型。是的，在供应链领域，不管你问美国还是中国的职业经理人，你们的头衔是什么，答复大多是采购、运营或物流，而不是供应链管理。即使是供应链经理，他们的职责也往往侧重某个领域，并不是我们真正意义上的大供应链。但毫无疑问，从采购管理到供应管理，是向供应链管理迈进了一大步。 图1-3 采购管理发展到供应链管理 图1-3 采购管理发展到供应链管理 这也体现在职业认证上。供应管理协会原来的认证是注册采购经理（C.P.M.），在30多年的历史中，全球认证人数超过4万，认证内容覆盖价格、质量、交付、合同管理、供应商选择、供应商谈判、国际贸易、公司管理及人力资源管理等。为适应采购向供应管理的发展，供应管理协会在2008年推出“供应管理专业人士认证”（CPSM），以取代C.P.M.，这标志着向供应链管理更进一步。相对而言，CPSM的要求比C.P.M.更高，也反映了供应（链）管理比采购管理要求更高。[1] [1] 对于CPSM认证的细节，可阅读《供应链管理的职业认证》一文，收在我的《供应链管理：实践者的专家之路》一书中，机械工业出版社于2017年出版；或在我的“供应链管理专栏”（www.scm-blog.com）上查询同名文章。 运营管理协会（APICS） APICS是美国生产与库存管理协会的缩写。与供应管理协会侧重采购相对应，APICS历来侧重于生产与库存管理。为适应向供应链管理发展的趋势，该组织在2004年更名为APICS—运营管理协会（具体如图1-4所示）。考虑到APICS在美国乃至世界的影响，运营管理协会仍旧保留APICS字眼，也显示不放弃在生产与库存控制方面的传统优势。 图1-4 APICS更名，进入供应链管理时代 图1-4 APICS更名，进入供应链管理时代 在美国宏观经济中，APICS的地位没有供应管理协会高，比如没有像供应管理协会的PMI那样有影响力的宏观经济指数。但在生产与库存管理领域，APICS享有崇高的声望。它的生产与库存管理认证（CPIM）在生产企业受到普遍重视。其认证内容侧重于生产的计划、控制和实施，即如何把销售计划转变为需求计划、生产主计划，然后细化到生产计划、物料供应计划，再到生产线的排程和控制，内容包罗万象。自1973年首次推出以来，全球总共有10.7万人得到此项认证。[1] 伴随着更名，APICS在2005年推出“供应链专业人士认证”（CSCP）。从字面上看，该认证针对供应链管理；从内容上看，该认证试图覆盖供应管理的三大范畴（采购与供应管理、生产运营管理、物流管理）；从级别上看，该认证比CPIM高，它要求一定年限的相关工作经验，而CPIM则没有。在过去10多年来，该认证与CPIM并存，可视作CPIM向供应链管理的延伸，以实现APICS在供应链管理领域与供应管理协会（ISM）两分天下的目的。 2014年4月30日，APICS宣布与Supply Chain Concil（SCC）合并。这种合并，一方面是为了抱团取暖—2008年金融危机以来，经济低迷，美国专业协会的经费大减；另一方面是为了挽救运营管理在美国日益衰落的局面——外包盛行，供应链全球化下，很多公司的生产制造被外包给低成本国家的供应商，原来由运营部门负责的任务转移到采购部门，原来的生产、库存、计划等专业人士也纷纷转业，而以这些人为基础的运营管理协会也就每况愈下。 这从他们的董事会成员可以看出：APICS的董事会成员中（2018年），12个董事会成员，只有两位是《财富》500强的副总，其余大多是些总监、经理、顾问[2]——生产外包到别的国家，有些生产运营的专家就只能转入咨询业。作为对比，看看其竞争对手供应管理协会的董事会，13个董事会成员中，只有一两个不是《财富》500强的副总裁、首席采购官级别（2018年）[3]。我们说这些，并不是某相声中说的，两个人拿名片打牌，看谁的名片上的头衔大；而是说，这些职业协会的董事会大部分是志愿者，大企业的高管们是否愿意把自己的时间贡献出来，从侧面反映了一个协会在行业的影响力。 好消息是，运营管理协会APICS这些年来继续与别的职业协会合并。2015年7月，它与原来的美国运输和物流协会合并，算是正式跨入物流领域，也给它的认证库里增加了一个新成员：运输与物流认证（CTL）。通过这一系列的兼并，运营管理协会可以说成为美国三大供应链职业协会中涉猎范围最广的一个。当然，兼收并蓄的风险呢，也是可能变成大而杂，丧失聚焦点，变成三不像。 [1] CPIM Transformed for Today’s Busy Professionals. APICS网站，www.apics.org. [2] 2018 APICS Board of Directors. APICS网站，www.apics.org. [3] ISM Board of Directors. ISM网站，www.instituteforsupplymanagement.org. 供应链管理专业人士协会（CSCMP） 这是美国第三个与供应链管理相关，也有相当影响力的协会。它的前身是物流管理协会（CLM）。顺应物流管理向供应链管理的过渡，供应链管理专业人士协会（CSCMP）试图从物流管理的角度出发，来“蚕食”供应链管理这一热门行业。 物流管理协会的影响更多是在物流教育领域，这从他们的主席人选可见一斑：物流专业的一些知名教授曾担任过该组织的主席，例如国内熟悉的鲍尔索克斯（密歇根州立大学教授，1964~1965年任主席）、门泽尔（田纳西大学教授，2000~2001年任主席）等。 相信在短时间内，供应管理专业人士协会很难大幅增加在工业界的影响，也很难成为一个纯粹意义上的供应链管理协会。这从该协会的董事会可见一斑：15位董事会成员中，6位是教授或来自大学。作为对比，运营管理协会APICS的董事会只有1位教授，而供应管理协会ISM则是清一色的大企业高管。 在发展历史上，供应链管理专业人士协会很好地诠释了从小到大、从部分到全部的发展历程。如图1-5所示，1963年，美国实物配送协会成立，表明最早的物流以运输为主，简单地说就是管着一帮卡车司机，做着把东西从A点搬送到B点的“实物配送”。[1]到了20世纪80年代，这些人说，我们不但可以把东西从A点搬到B点，而且可以对付整个过程的仓储、配送、海关等多道手续，以及伴随而来的信息流，这就变成了物流管理。于是在1985年，实物配送协会改为物流管理协会，覆盖运输以外的更多业务。再后来，物流管理说，我们也可以对付采购、运营的事啊。得，这就变成了供应链管理—2004年，物流协会改名供应链管理专业协会，正式从物流跨入供应链领域。不过对我们供应链领域的人而言，一看你是供应链管理职业人协会的，就知道你的前世今生是物流。 图1-5 从实物配送到物流管理到供应链管理 图1-5 从实物配送到物流管理到供应链管理 从认证角度而言，长期以来，这个机构一直没有能与供应管理协会和运营管理协会相匹敌的认证。最近注意到，供应链管理专业人士协会推出了SCPro的认证。整个认证分为三级：第一级是供应链管理核心知识，覆盖供应链管理领域的八个方面；第二级是供应链挑战的分析与实施，基于案例来测试学员对供应链管理知识的应用能力；第三级是供应链转型，需要在学术机构导师的指导下，分析具体企业的真实状况，规划一个供应链改进项目，来取得真实的业务成果，比如提高投资回报率、缩短周转周期等。[2]这个认证目前在国内还没有看到，在美国的影响也尚需建立。[3] 从上述美国三大职业协会的发展可以看出，在可预见的未来，供应链管理仍将以一个综合领域的面目存在，在采购、运营和物流的基础上继续发展。但集成的趋势很明显，不但在行业协会，而且在工业界、教育界。 很多公司在集成采购、运营和物流管理三个部门，设立全球供应链部。ERP软件提供者如SAP、Oracle促进了这一趋势：它们的软件使跨职能协作更加容易。学术界也有越来越多的系、专业改名为供应链管理。美国MBA排名中也增设了供应链管理/物流管理专业，与传统的会计、金融、营销、国际管理等分庭抗礼，说明供应链管理作为一个专业已经形成。 资源 CPSM认证由美国供应管理协会提供，详情见www.ism.ws CPIM和CSCP认证由美国运营管理协会提供，详情见www.apics.org 这些认证在国内都可以参加，其中CPSM已经汉化。 延伸阅读 《供应链管理在国内的发展》，节选自我的另一本书《供应链管理：实践者的专家之路》。十几年前，我在申请北美商学院时，第一次听说供应链管理；七八年前，国内的一些大型企业启动供应链转型；最近几年，越来越多的中小企业着眼供应链，解决日益严峻的成本和库存问题。扫描二维码，阅读全文。 [1] 有个学术刊物，名字叫International Journal of Physical Distribution &amp; Logistics Management，翻译过来就是《实物配送和物流管理国际学刊》。从1970年创刊至今，都快半个世纪了，还看不到一点要寿终正寝的样子。 [2] 见供应链管理专业人士协会的网站：http://cscmp.org [3] 判断一个认证的价值，最简单的就是看招聘网站上，有多少职位要求或者建议应聘者有这个认证。我到Monster.com（这是美国的一个主要招聘网站）上，搜索SCPro认证，只发现两个职位；搜索ISM的CPSM认证，发现487个岗位；搜索APICS的CPIM认证，出来700个岗位；搜索CSCP，有358个岗位（2018年10月10日，搜索时不限职位所在的地域）。再搜索“注册供应链管理师”CSCM认证，发现17个岗位—这个认证最近突然在国内冒起来，到处都有人在宣传，说是国家人力资源和社会保障部认可的，引得很多读者三天两头到我这里求证，问这个认证是不是主办者宣称的那样，是个美国主流认证，这里算是一并答复。","link":"/%E4%BB%8E%E7%BE%8E%E5%9B%BD%E4%B8%89%E5%A4%A7%E5%8D%8F%E4%BC%9A%E8%AF%B4%E4%BE%9B%E5%BA%94%E9%93%BE%E7%AE%A1%E7%90%86%E7%9A%84%E6%BC%94%E5%8F%98/"},{"title":"14. 使用链式调用简化多步提示语","text":"Hi, 大家好，我是茶桁。 OpenAI 的大语言模型提供了 Completion 和 Embedding 两个核心接口。 我们可以通过增加提示语（Prompt）历史记录来提高模型的回答准确性和自然性。还可以将 Embedding提前索引好存起来，以此做到让AI根据外部知识来回答问题， 在我们多次与AI对话的过程中，讲AI返回的答案放在新的问题里，那么我们就可以让AI帮主我们给自己的代码撰写单元测试了。 以上这些方法是自然语言类应用中常见的模式。为了方便应用开发者使用这些模式，开源社区开发了名为 Langchain 的开源库，使用 Langchain，我们可以更加快速地实现之前利用大语言模型实现过的功能，并且可以更好地将模型集成到我们的业务系统中，实现更加复杂、有价值的功能。 何谓链式调用 在第 11 讲中，我们学习了 llama-index 的使用，并在此过程中已经安装了 Langchain。虽然 Langchain 也有类似 llama-index 的功能，但这不是 Langchain 的主要卖点。Langchain 带来的第一个主要优势就在于它的名字，也就是链式调用。 链式调用是指在程序中可以使用点 . 来连接对象，然后在这些对象上调用方法，以此来实现更加复杂的操作。在使用 ChatGPT 的过程中，我们经常会遇到中文语料较少的问题，以至于它不能很好地回答问题。为了解决这个问题，我们可以采取以下措施。 我们可以把中文问题交给 AI，让它翻译成英文，然后把英文问题贴进去提问，得到一个英文答案。最后，再请 AI 把英文答案翻译回中文。通过这种方式，我们可以得到更加准确的答案。 可能部分读者会觉得这样做有些脱裤子放屁多此一举，但是其实你真的完全用英文来完成你的问题和获取答案，你就能很明显的感觉到差别。这个问题曾经有一个业内人士延展的谈到过，其实这是一个很严重的问题，也是我们这几十年来的互联网环境造成了今天的局面，而再这样下去，中文的AI环境就此完蛋，我们也可能再也追不上国外的脚步。当然，到底是否危言耸听仁者见仁智者见智，每个人有自己的看法，可是从我自己使用之后和对于一些原理的理解来看，这真的不是危言耸听。理解大语言模型原理的人应该都能懂。好了，我把视频链接放在这，有兴趣的自己去看看吧，一段7分钟的视频：中文互联网的凋零，正在杀死中国人工智能的未来【为什么我们搞不出ChatGPT】 我们还可以通过增加数据样本的方式来扩展 ChatGPT 的知识库，让其可以更好地回答问题。除此之外，我们还可以通过使用更加复杂的算法和技术来提高 ChatGPT 的回答质量，例如使用迁移学习等技术。 例如，下面的截图里，我请 ChatGPT 简单介绍一下 Stable Diffusion 的原理。通过 Langchain 的链式调用，我们可以更方便地使用 ChatGPT 来解决各种问题。 Langchain 带来的链式调用优势可以让我们更加灵活地使用 ChatGPT，同时也可以帮助我们解决中文语料较少的问题。通过不断地扩展 ChatGPT 的知识库和使用更加先进的技术，我们相信 ChatGPT 可以成为一个更加强大的工具，帮助我们解决各种问题。 Stable Diffusion 是一个非常热门的开源 AI 画图工具，我们在最开始的导读课程中有过一些贴图。除此之外，我以前也写过专门介绍的文章：在 Apple Silicon M1/M2 Mac 上安装和运行Stable Diffusion 一般来说，如果我们人工询问ChatGPT，应该是这样的步骤, 我将其称之为“人工链式调用”： image-20230602165232370 image-20230602165359092 image-20230602165505848 可以从截图中看到，我们一共经历了三步： 把我们的问题翻译成了英文 用翻译好的英文向ChatGPT提问 最后将ChatGPT的回答再翻译回中文 那如果我们用API来实现这整个过程，其实就是一个链式调用的过程。 使用LLMChain进行链式调用 我们可以使用 OpenAI 进行翻译请求和原始问题的组合，并将其发送给 AI，以完成问题的中译英转换，从而获得翻译好的英文问题。 接下来，我们可以将翻译好的英文问题发送回 OpenAI，以获得英文答案。 最后，我们可以将英文答案与要求 AI 翻译答案的请求组合在一起，以完成答案的英译中转换。 如果我们使用 LLMChain 进行链式调用，我们可以在代码中逐步进行如下操作： 发送翻译请求和原始问题给 OpenAI 进行中译英转换。 得到翻译好的英文问题。 发送翻译好的英文问题给 OpenAI 进行英文答案的获取。 得到英文答案。 将英文答案与要求 AI 翻译答案的请求组合在一起，完成答案的英译中转换。 通过以上操作，我们可以获得更加准确和完整的翻译结果。 123456789101112131415161718192021222324252627import openai, osfrom langchain.prompts import PromptTemplatefrom langchain.llms import OpenAIfrom langchain.chains import LLMChainos.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'openai.api_key = &quot;OPENAI_API_KEY&quot;llm = OpenAI(model_name = &quot;text-davinci-003&quot;, temperature = 0.5, max_tokens=2048)en_to_zh_prompt = PromptTemplate( template = &quot;请把下面这句话翻译成英文： \\n\\n {question}?&quot;, input_variables = ['question'])question_prompt = PromptTemplate( template = &quot;{english_question}&quot;, input_variables=['english_question'])zh_to_cn_prompt = PromptTemplate( input_variables = ['english_answer'], template = '请把下面这一段翻译成中文: \\n\\n {english_answer}?',)question_translate_chain = LLMChain(llm = llm, prompt = en_to_zh_prompt, output_key = 'english_question')english = question_translate_chain.run(question = '请你作为一个机器学习的专家，介绍一下CNN的原理')print(english) 得到英文翻译结果： 1Please as an expert in Machine Learning, introduce the principle of CNN? 让我们接着上面的代码继续： 123qa_chain = LLMChain(llm = llm, prompt = question_prompt, output_key = 'english_answer')english_answer = qa_chain.run(english_question = english)print(english_answer) 得到英文回答： 1Convolutional Neural Networks (CNNs) are a type of deep learning neural network architecture that are particularly well suited for processing and analyzing data that has a spatial or temporal component. The main principle of CNNs is to learn features from the data in a hierarchical manner, starting from basic features at the lower layers and gradually increasing the complexity of the features at the higher layers. The main components of a CNN include convolutional layers, pooling layers, and fully connected layers. The convolutional layers are used to extract features from the input data, while the pooling layers are used to reduce the size of the input data and the fully connected layers are used to make predictions based on the extracted features. 继续： 123answer_translate_chain = LLMChain(llm = llm, prompt = zh_to_cn_prompt)answer = answer_translate_chain.run(english_answer = english_answer)print(answer) 得到最终结果： 1卷积神经网络（CNN）是一种深度学习神经网络架构，特别适合处理和分析具有空间或时间成分的数据。CNN的主要原理是以分层的方式从数据中学习特征，从低层开始从基本特征，并逐渐增加高层的特征复杂性。CNN的主要组成部分包括卷积层、池化层和全连接层。卷积层用于从输入数据中提取特征，而池化层用于减小输入数据的大小，全连接层用于根据提取的特征进行预测。 在这段代码中，我们使用了 Langchain 库，但是我们可以加入一些新的内容来扩展它，以便更好地满足我们的需求。我们主要使用了 Langchain 的三个包来扩展它。LLM、PromptTemplate 和 LLMChain。 首先，我们使用 LLM 包来回答我们提出的问题。在这里，我们使用的是 OpenAIChat。OpenAIChat 是一个最新的 gpt-3.5-turbo 模型，可以帮助我们用最好的方式回答我们的问题。 然后是 PromptTemplate，这个包和在第 11 讲中看到的 llama-index 的 PromptTemplate 相似。它可以定义一个提示语模板，里面可以定义一些可以动态替换的变量。例如，在代码中的 question_prompt 模板中，我们定义了一个名为 question 的变量，因为我们每次提出的问题都不一样。事实上，llama-index 中的 PromptTemplate 就是对 Langchain 的 PromptTemplate 做了一层简单的封装。 接下来是 LLMChain，主角。它的构造函数接收一个 LLM 和一个 PromptTemplate 作为参数。构造完成之后，可以直接调用里面的 run 方法，将 PromptTemplate 需要的变量，用 K=&gt;V 对的形式传入进去。返回的结果，就是 LLM 给我们的答案。 但是，现在我们似乎只是对 OpenAI 的 API 做了一层封装。我们构建了 3 个 LLMChain，然后按照顺序调用，每次拿到答案之后，再作为输入，交给下一个 LLM 调用。感觉好像更麻烦了，没有减少什么工作量呀？ 不要着急，我们还没有真正使用 LLMChain 的“链式调用”功能。只需要加上一行小小的代码即可。我们使用一个名为 SimpleSequentialChain 的 LLMChain 类，将我们要按顺序依次调用的三个 LLMChain 放在一个数组中，将该数组传递给该类的构造函数。 然后，我们调用该对象的 run 方法，将我们用中文提出的问题交给它。此时，该 SimpleSequentialChain 会按照顺序开始调用数组参数 chains 中包含的其他 LLMChain。每次调用的结果都会存储在 Chain 构造时定义的 output_key 参数中。如果下一个调用的 LLMChain 中的模板内的变量与之前的 output_key 名称相同，则会用 output_key 中存储的内容替换模板内变量所在的占位符。 这样，我们只需要向该 SimpleSequentialChain 调用一次 run 方法，将一开始的问题交给它即可。后续根据答案去问新的问题，该 LLMChain 会自动链式搞定。我在这里将日志的 Verbose 模式打开，你在输出的过程中，可以看到该 LLMChain 调用了三次，并且可以一并看到中间两次的返回结果。这就是我们如何更好地使用 Langchain 库来满足我们的需求，同时保留原有的关键思想。 123456789from langchain.chains import SimpleSequentialChainchinese_qa_chain = SimpleSequentialChain( chains = [question_translate_chain, qa_chain, answer_translate_chain], input_key = 'question', verbose = True)answer = chinese_qa_chain.run(question = '请你作为一个机器学习的专家，介绍一下CNN的原理。')print(answer) 然后我们可以看到Verbose的日志信息： 123456789101112&gt; Entering new SimpleSequentialChain chain...Please introduce the principle of CNN as an expert of machine learning.Convolutional Neural Networks (CNNs) are a type of deep learning neural network used for image recognition and classification. They are composed of multiple layers of neurons, each layer performing a specific task. The neurons are organized in a hierarchical manner, with each layer taking the output from the previous layer as its input. The first layer of neurons is usually a convolutional layer that performs convolution operations on the input image, extracting features from it. The subsequent layers are fully connected layers that use the extracted features to classify the image. The final layer produces the output, which is usually a probability distribution over the categories of the image. CNNs are powerful tools for image classification, and have achieved state-of-the-art results on a variety of tasks.卷积神经网络（CNN）是一种深度学习神经网络，用于图像识别和分类。它由多层神经元组成，每层执行特定任务。神经元以分层的方式组织，每层以上一层的输出作为输入。第一层神经元通常是卷积层，对输入图像执行卷积操作，从中提取特征。随后的层是完全连接的图层，它们使用提取的要素对影像进行分类。最后一层产生输出，这通常是图像类别的概率分布。CNN是图像分类的强大工具，并且在各种任务上都取得了最先进的结果。&gt; Finished chain. 当然，还得到我们的最终结果。 1卷积神经网络（CNN）是一种深度学习神经网络，用于图像识别和分类。它由多层神经元组成，每层执行特定任务。神经元以分层的方式组织，每层以上一层的输出作为输入。第一层神经元通常是卷积层，对输入图像执行卷积操作，从中提取特征。随后的层是完全连接的图层，它们使用提取的要素对影像进行分类。最后一层产生输出，这通常是图像类别的概率分布。CNN是图像分类的强大工具，并且在各种任务上都取得了最先进的结果。 image-20230602173646472 在使用链式调用时，需要注意，在一个 LLMChain 中使用 PromptTemplate 时，输入参数必须先在 LLMChain 中通过 output_key 定义，否则该变量将无值，从而导致程序报错。此外，在一些情况下，您可能需要使用 try-except 语句来处理可能出现的错误，以确保代码的稳定性。另外，建议您在代码中添加注释，以便更好地理解程序的执行流程和实现细节。 输入多个变量 实际上，由于输入输出使用的是变量，这些变量是由这些参数所定义的。因此，我们不仅可以使用前一个LLMChain的输出作为后一个LLMChain的输入，还可以连续提出多个问题，并将这些问题的答案作为后续问题的输入来继续处理。这样，我们可以进一步扩展我们的模型，提取更多的信息以及更准确的答案。这个过程有点类似于人类的思考过程，因为在人类的思维中，一个问题的答案通常会引发更多的问题。 例如，我们可以提出这样的问题：“在中国，哪个城市是最大的城市？”然后，我们可以使用一个自然语言处理模型来回答这个问题，比如说，我们可以使用一个LLMChain模型。第一个LLMChain模型将输入中的问题转化为向量，然后将这个向量传递给下一个模型。第二个LLMChain模型将这个向量处理成一个问题的答案，然后将这个答案作为下一个问题的输入。这样，我们就可以连续提出多个问题，并将这些问题的答案作为后续问题的输入来继续处理。比如说，我们可以接下来问：“这个城市的人口是多少？”然后，我们可以再次使用LLMChain模型来回答这个问题。通过这种方式，我们可以更详细地了解一个城市的信息，同时也可以更好地理解LLMChain模型的工作原理。 下面我就给你看一个例子，让你更好的理解这个概念。 123456789101112131415161718192021222324from langchain.chains import SequentialChainq1_prompt = PromptTemplate( input_variables = ['year1'], template = '{year1}年的欧冠联赛的冠军是哪支球队，只说球队名称。')q2_prompt = PromptTemplate( input_variables = ['year2'], template = '{year2}年的欧冠联赛的冠军是哪支球队，只说球队名称。')q3_prompt = PromptTemplate( input_variables = ['team1', 'team2'], template = '{team1}和{team2}哪只球队获得欧冠的次数多一些？')chain1 = LLMChain(llm = llm, prompt = q1_prompt, output_key = 'team1')chain2 = LLMChain(llm = llm, prompt = q2_prompt, output_key = 'team2')chain3 = LLMChain(llm = llm, prompt = q3_prompt)sequential_chain = SequentialChain(chains = [chain1, chain2, chain3], input_variables = ['year1', 'year2'], verbose = True)answer = sequential_chain.run(year1 = 2000, year2 = 2010)print(answer) 然后我们得到日志和结果： 123456&gt; Entering new SequentialChain chain...&gt; Finished chain.皇家马德里获得欧冠的次数多一些，共计13次，而拜仁慕尼黑只有5次。 有的时候需要注意，ChatGPT可能会一本正经的胡说八道。在得到结果的时候最好验证一下，比如这个问题下，ChatGPT最初告诉我的拜仁多一些，共计8次，而皇家马德里只有3次。虽然我不知道具体次数，但是我也知道皇马是欧冠之王，明显答案就不对。所以又重新生成并验证了一次。 在这个例子里，我们定义了两个 PromptTemplate 和对应的 LLMChain。PromptTemplate 的作用是接收一个年份作为输入，回答这个年份的欧冠冠军。我们使用两个不同的 LLMChain 分别处理这两个 PromptTemplate，从而得到两支不同年份的欧冠冠军。接下来，我们将这两个队名作为输入，放到第三个问题里。这个问题会让 AI 告诉我们这两支球队哪一支获得欧冠的次数多一些。我们可以使用另一个 LLMChain 来处理这个问题，然后将其添加到我们的 SequentialChain 中。这样，我们只需要输入两个年份，就能通过三次回答得到答案。在这个过程中，我们可以使用不同的技术和算法来优化我们的模型，以获得更加准确和快速的结果。 使用 Langchain 实现自动化撰写单元测试 看到这里，让我们更深入地探讨一下通过多步提示语自动给代码写单元测试的方法。在上一讲中，我们介绍了 Langchain，它可以顺序地通过多个 Prompt 调用 OpenAI 的 GPT 模型来实现这个功能。这种方法非常适合自动化测试，因为它可以帮助我们节省大量的时间和精力。 但是，你可能会问，如果 AST 语法解析失败怎么办？不用担心，因为现在的 Langchain 已经具备了自动重试的能力，可以轻松解决这个问题。所以，现在我们可以更加放心地使用 Langchain 进行自动化测试，从而提高我们的工作效率。 总之，通过多步提示语自动给代码写单元测试是一种非常实用的方法，而 Langchain 则是实现这种方法的最佳选择。希望这篇文章能够帮助大家更好地了解 Langchain 的功能和优势。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798from langchain.chains import SequentialChainfrom langchain.llms import OpenAIfrom langchain.prompts import PromptTemplatedef write_unit_test(function_to_test, unit_test_package=&quot;pytest&quot;): # 解释源代码的步骤 explain_code = &quot;&quot;&quot; # How to write great unit tests with {unit_test_package} In this advanced tutorial for experts, we'll use Python 3.10 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function. ```python {function_to_test} \\``` Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been. - First, &quot;&quot;&quot; explain_code_template = PromptTemplate( input_variables=[&quot;unit_test_package&quot;, &quot;function_to_test&quot;], template=explain_code ) explain_code_llm = OpenAI(model_name=&quot;text-davinci-002&quot;, temperature=0.4, max_tokens=1000, model_kwargs={&quot;stop&quot;: [&quot;\\n\\n&quot;, &quot;\\n\\t\\n&quot;, &quot;\\n \\n&quot;]}) explain_code_step = LLMChain(llm=explain_code_llm, prompt=explain_code_template, output_key=&quot;code_explanation&quot;) # 创建测试计划示例的步骤 test_plan = &quot;&quot;&quot; A good unit test suite should aim to: - Test the function's behavior for a wide range of possible inputs - Test edge cases that the author may not have foreseen - Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain - Be easy to read and understand, with clean code and descriptive names - Be deterministic, so that the tests always pass or fail in the same way `{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above. For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets): - &quot;&quot;&quot; test_plan_template = PromptTemplate( input_variables=[&quot;unit_test_package&quot;, &quot;function_to_test&quot;, &quot;code_explanation&quot;], template=explain_code + &quot;{code_explanation}&quot; + test_plan ) test_plan_llm = OpenAI(model_name=&quot;text-davinci-002&quot;, temperature=0.4, max_tokens=1000, model_kwargs={&quot;stop&quot;: [&quot;\\n\\n&quot;, &quot;\\n\\t\\n&quot;, &quot;\\n \\n&quot;]}) test_plan_step = LLMChain(llm=test_plan_llm, prompt=test_plan_template, output_key=&quot;test_plan&quot;) # 撰写测试代码的步骤 starter_comment = &quot;Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator&quot; prompt_to_generate_the_unit_test = &quot;&quot;&quot; Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does. ```python import {unit_test_package} # used for our unit tests {function_to_test} #{starter_comment} &quot;&quot;&quot; unit_test_template = PromptTemplate( input_variables=[&quot;unit_test_package&quot;, &quot;function_to_test&quot;, &quot;code_explanation&quot;, &quot;test_plan&quot;, &quot;starter_comment&quot;], template=explain_code + &quot;{code_explanation}&quot; + test_plan + &quot;{test_plan}&quot; + prompt_to_generate_the_unit_test ) unit_test_llm = OpenAI(model_name=&quot;text-davinci-002&quot;, temperature=0.4, max_tokens=1000, model_kwargs={&quot;stop&quot;: &quot;```&quot;}) unit_test_step = LLMChain(llm=unit_test_llm, prompt=unit_test_template, output_key=&quot;unit_test&quot;) sequential_chain = SequentialChain(chains=[explain_code_step, test_plan_step, unit_test_step], input_variables=[&quot;unit_test_package&quot;, &quot;function_to_test&quot;, &quot;starter_comment&quot;], verbose=True) answer = sequential_chain.run(unit_test_package=unit_test_package, function_to_test=function_to_test, starter_comment=starter_comment) return f&quot;&quot;&quot;#{starter_comment}&quot;&quot;&quot; + answer code = &quot;&quot;&quot;def format_time(seconds): minutes, seconds = divmod(seconds, 60) hours, minutes = divmod(minutes, 60) if hours &gt; 0: return f&quot;{hours}h{minutes}min{seconds}s&quot; elif minutes &gt; 0: return f&quot;{minutes}min{seconds}s&quot; else: return f&quot;{seconds}s&quot;&quot;&quot;&quot;import astdef write_unit_test_automatically(code, retry=3): unit_test_code = write_unit_test(code) all_code = code + unit_test_code tried = 0 while tried &lt; retry: try: ast.parse(all_code) return all_code except SyntaxError as e: print(f&quot;Syntax error in generated code: {e}&quot;) all_code = code + write_unit_test(code) tried += 1 print(write_unit_test_automatically(code)) 然后我们得到结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869\u001b[1m&gt; Entering new SequentialChain chain...\u001b[0m\u001b[1m&gt; Finished chain.\u001b[0m# 以下部分是返回的程序 def format_time(seconds): minutes, seconds = divmod(seconds, 60) hours, minutes = divmod(minutes, 60) if hours &gt; 0: return f&quot;{hours}h{minutes}min{seconds}s&quot; elif minutes &gt; 0: return f&quot;{minutes}min{seconds}s&quot; else: return f&quot;{seconds}s&quot;#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator.#The first element of each tuple is the name of the test case, and the second element is a list of tuples.#Each tuple in the list of tuples is a set of input values and expected output values for that test case.@pytest.mark.parametrize( &quot;test_case, input_values, expected_output&quot;, [ # Test cases for positive integers (&quot;positive_int&quot;, [1], &quot;1s&quot;), (&quot;positive_int&quot;, [60], &quot;1min&quot;), (&quot;positive_int&quot;, [3600], &quot;1h&quot;), (&quot;positive_int&quot;, [3601], &quot;1h1s&quot;), # Test cases for negative integers (&quot;negative_int&quot;, [-1], &quot;-1s&quot;), (&quot;negative_int&quot;, [-60], &quot;-1min&quot;), (&quot;negative_int&quot;, [-3600], &quot;-1h&quot;), (&quot;negative_int&quot;, [-3601], &quot;-1h1s&quot;), # Test case for zero (&quot;zero&quot;, [0], &quot;0s&quot;), # Test cases for positive floats (&quot;positive_float&quot;, [1.0], &quot;1.0s&quot;), (&quot;positive_float&quot;, [60.0], &quot;1.0min&quot;), (&quot;positive_float&quot;, [3600.0], &quot;1.0h&quot;), (&quot;positive_float&quot;, [3601.0], &quot;1.0h1.0s&quot;), # Test cases for negative floats (&quot;negative_float&quot;, [-1.0], &quot;-1.0s&quot;), (&quot;negative_float&quot;, [-60.0], &quot;-1.0min&quot;), (&quot;negative_float&quot;, [-3600.0], &quot;-1.0h&quot;), (&quot;negative_float&quot;, [-3601.0], &quot;-1.0h1.0s&quot;), # Test cases for strings that can be parsed as integers (&quot;string_int&quot;, [&quot;1&quot;], &quot;1s&quot;), (&quot;string_int&quot;, [&quot;60&quot;], &quot;1min&quot;), (&quot;string_int&quot;, [&quot;3600&quot;], &quot;1h&quot;), (&quot;string_int&quot;, [&quot;3601&quot;], &quot;1h1s&quot;), # Test cases for strings that can be parsed as floats (&quot;string_float&quot;, [&quot;1.0&quot;], &quot;1.0s&quot;), (&quot;string_float&quot;, [&quot;60.0&quot;], &quot;1.0min&quot;), (&quot;string_float&quot;, [&quot;3600.0&quot;], &quot;1.0h&quot;), (&quot;string_float&quot;, [&quot;3601.0&quot;], &quot;1.0h1.0s&quot;), # Test cases for strings that cannot be parsed as integers or floats (&quot;string_other&quot;, [&quot;abc&quot;], &quot;abc&quot;), (&quot;string_other&quot;, [&quot;1.0.0&quot;], &quot;1.0.0&quot;), (&quot;string_other&quot;, [&quot;60s&quot;], &quot;60s&quot;), (&quot;string_other&quot;, [&quot;3600min&quot;], &quot;3600min&quot;), (&quot;string_other&quot;, [&quot;3601h&quot;], &quot;3601h&quot;), ],)def test_format_time(test_case, input_values, expected_output): &quot;&quot;&quot;Unit test for the format_time function.&quot;&quot;&quot; # The input to the function is a list, so we need to unpack it before passing it to the function. input_value, = input_values output = format_time(input_value) assert output == expected_output 这个代码的具体功能与上一讲相同，只是我们通过 Langchain 对其进行了封装，以便更容易维护。我们使用三个 LLMChain 来解释代码、生成测试计划和最终生成测试代码。每一步的输入都是上一步的输出，其中包括上一步的 Prompt Template 和这一步 Prompt Template 的组合，以及一些变量，这些变量是上一步执行的结果作为输入变量传递而来的。最终，我们可以使用 SequentialChain 自动按照这三个步骤执行 OpenAI 的 API 调用。 我们使用 write_unit_test 函数来封装整个过程。对于重试，我们使用 while 循环来调用 write_unit_test。得到的结果和输入的代码拼接在一起，然后交给 AST 库进行解析。如果解析未通过，则重试整个单元测试生成的过程，直到达到我们最大的重试次数为止。 LangChain 的链式调用方式有助于我们通过 AI 完成更复杂的任务，并将整个任务的完成过程定义为一个固定的流程模板。在下一讲中，我们将进一步探讨如何通过链式组合多个 LLMChain 的方法来完成更复杂且更实际的工作。 小结 相信到这里，你已经有了更多可以利用大语言模型的好点子。在本讲中，我向您介绍了如何使用 Langchain 这个开源库，对大语言模型进行链式调用。要使用大语言模型完成复杂任务往往需要我们多次向 AI 提问，而前面问题的答案可能是后面问题输入的一部分。LangChain 通过将多个 LLMChain 组合成一个 SequentialChain 并顺序执行，大大简化了这类任务的开发工作。 SequentialChain LLMChain 是一种链式调用大型语言模型的模式，可以将前面的变量和输出作为下一轮调用的变量输入。但是，这只是 Langchain 的一小部分功能。除了调用语言模型，我们还可以调用外部系统，甚至可以让 AI 做决策，决定我们的系统该做什么。在接下来的几节课中，我们将介绍更多关于 Langchain 的强大功能，并最终提供一个完整的电子商务聊天机器人。 留下一个思考题给你：你能够通过 Langchain 结合多个问题，并利用先前问题的答案结果来触发新的问题，从而找到你所需的答案吗？欢迎在评论区分享你的例子，并将这一节课与需要它的朋友分享。我们在下一节课再见。 推荐阅读：就像之前介绍的 llama-index 项目一样，Langchain 项目也正在快速发展和迭代。我建议你去看看他们的官方文档，以了解他们提供的最新功能。此外，之前我们提到过的向量数据库公司 Pinecone，也制作了一份 Langchain AI 手册，你也可以去看一看。","link":"/%E4%BD%BF%E7%94%A8%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%E7%AE%80%E5%8C%96%E5%A4%9A%E6%AD%A5%E6%8F%90%E7%A4%BA%E8%AF%AD/"},{"title":"供应链的根本是协作，那为什么不协作","text":"我们知道，供应链是采购把东西买进来，生产来加工增值，物流负责配送给客户。自从有现代企业以来，就有人做采购，有人做生产运营，有人做物流配送。也就是说，采购、运营和物流管理由来已久，至少有上百年的历史，那为什么供应链管理是个新概念，直到20世纪80年代才出现？根本原因：单一指标驱动下，职能之间山头林立，协作度低，形不成供应链。让我细细道来。 职能之间山头林立，协作度低，形不成供应链 职能之间山头林立，协作度低，形不成供应链 传统模式下，企业是职能导向，驱动员工行为的是职能目标，即自上而下的目标。比如对采购来说，就是采购价格最低，因为老板对采购的最大要求就是省钱；对生产来说，就是产能利用率最高，这也是老板最为关注的。你知道，价格没有最低，只有更低，如果牺牲交付、质量和服务的话。生产也类似：为了提高产能利用率，降低单位生产成本，那就减少换线，以延长交付周期，牺牲交付绩效为代价。 也就是说，在传统模式下，驱动员工行为的是竖向的效率型指标（比如成本更低、产能利用率更高），缺乏横向的服务型指标（比如交付更快、质量更好）。在绩效考核的驱动下，每个职能关注的重点是顶头上司，而不是兄弟职能的诉求，这样，职能与职能之间就串不起来，形不成供应链。这也是职能之间协作度低的根本原因。 这也是为什么兄弟职能之间经常互相挖坑，不管是自觉还是不自觉地；而上下级之间就很少互坑，协作也容易多了。这里的关键是强相关的指标：上级的目标百分之百传递给下级，下级的绩效也百分之百汇总给上级，大家是同一条绳子上的蚂蚱。但跨职能之间则不是，职能之间的横向联系不够强。也就是说，纵向指标之下，你关注的是来自上司的需求；横向指标缺失，你就不会那么关注兄弟职能，也就是内部客户的诉求。结果就是山头林立，局部优化盛行。 当然，有人会说，我也有横向指标啊，比如交付和质量。你当然有，但问题是，把你敲得满头是包的，是没完成你老板的事，还是兄弟职能不满意？或者说，让你晚上睡不着觉的，是你老板的事，还是兄弟职能的？如果是前者的话，说明横向的质量、交付等指标还是不够强大；支配我们行为的，还是自上而下的指标。 而解决方案呢，就是建立强相关的横向指标，让职能之间有类似于上下级之间的强联系。对于每个具体的职能、具体的员工来说，他们不但要有纵向的效率指标（比如成本、产能利用率、库存周转率），还要有横向的服务指标（比如按时交货率、质量合格率），这是一对表面矛盾，但实际相辅相成的指标[1]，是打破职能壁垒，促进跨职能协作，形成供应链的关键。 这道理不难，你天生就懂，因为兄弟职能不配合，你的第一大招就是想方设法让对方“背指标”，背的就是横向的服务型指标。那为什么总是给“背”不上呢？或者说名义上“背”上了，实际上却没有呢？ 这里的根本原因有二：其一，横向指标在绩效考核中权重太低，没法起到“强相关”的作用，引导员工的行为改变。其二，企业的管理精细度不够，没法有效客观量化横向指标——没法客观量化，就不知道；不知道，就没法管理，还是没法“强相关”。 第一个原因无须多言。对于第二个原因，让我们拿供应商的按时交货率为例来说明。之所以用按时交货率，是因为这是所有横向指标中最为直观，也最为简单明了的：要么按时，要么不按时，有什么可争辩的？且慢，这问题远没有那么简单。 先说什么是按时。计划说，按时就是能够满足客户需求，也就是说，以客户的需求日期为基准。采购马上就有异议：供应商的正常交期是30天，客户的需求日期只给3天，这怎么能做到呢？不公平。那什么叫公平呢？采购就说按照供应商的正常交期，要么是合同约定，要么是报价时约定。这时候销售、计划、生产马上“跳”起来了：这世界不是个完美的世界，如果客户每次都给我们足够的响应周期，那还要采购干什么？ 就这样，计划基于需求日期，3月1日就要货；采购基于标准交期，3月31日才交付。两个极端，对另一方都不公平。不公平就没有约束力——连法律都有规定，强迫签订的不公平合同不具法律效力。那什么叫公平？供应商说，我理解你们3月1日要货，但物理定律没法违背：车工需要x天，铣工需要y天，最后的精加工需要z天，也就是说，最快也是3月15日，否则要货没有，要命一条。就这样，计划、采购和供应商三方达成一致，3月15日就成为按时不按时的标准，也就是说，基于供应商的承诺日期。 这道理很简单，是不是？没错，对于一个具体的订单来说，这是不难。但想想看，一个公司，每天动辄有几十几百个订单，每个订单都这么来回拉锯，达成三方一致，可不是件容易的事。好不容易达成一致，第二天需求变了，得，又得重新来一次。这工作量有多大，离开电子商务的支持，简直不可想象。 在电子商务发达的企业，采购订单由ERP自动生成，发给供应商；供应商确认交期、数量、单价，通过电子商务传递给采购方；如果供应与需求匹配，这就作为供应商的承诺写入ERP，成为后续判断是否按时的标准，不需要任何人工介入；如果不匹配，系统会自动提醒供应商做出更好的承诺；还不够好的话，采购员、物控员、催货员就人工介入，打电话，发邮件，找老板，督促供应商改进交付，直到供应商做出三方能够达成一致的承诺。第二天需求变了，这样的流程就重来一次。 看得出，有电子商务支持的话，百分之八九十的事儿由信息系统做了，员工只是负责那5%、10%的例外；没有电子商务的话，员工就不得不把所有的情况都当例外，我还没见过一个公司，能人工确认每个订单的供应商承诺日期，并随着需求日期的更新而更新。遗憾的是，大多数企业都没有这样的电子商务，所以就没有能力做精、做细，在订单和料号层面客观统计按时交货率。 没有三方认可的承诺日期，计划就基于需求日期统计，按时交货率自然很差；采购为了自保，就基于正常交期统计，按时交货率自然很好。告到老板那里，老板一看，双方都有道理啊，只好批评教育，再宣教一番“以客户为导向”，这事儿就不了了之了。结果呢，计划只能以内部客户的身份，从道义上给采购压力；而采购呢，虽然“背”着供应商的按时交付指标，但实际上形同虚设，起不到“强相关”的作用，驱动他们的仍然是单一的价格指标。 最简单的按时交付都这么难以客观统计，质量、服务等指标就更难客观量化。就拿供应商的质量问题来说，每一个质量问题，都意味着生产线、质检、供应商以及设计之间无穷尽的扯皮，大量Email乱飞，大多企业根本没有资源来梳理清楚。同理，没法客观统计的就没法管理，这样，供应商质量指标就没法落实到采购头上，驱动采购的呢，依然是自上而下的成本指标。 做不精细，企业没法有效量化横向指标，就不得不借助企业文化来推动跨职能协作，让大家“学雷锋”，但没法从根本上解决问题。 既然职能之间的横向指标难以客观建立，有些企业就采取组织措施，成立集成供应链部门，让采购、运营、物流、计划、客服等职能统一汇报到同一个总监，通过组织措施打通这些职能之间的壁垒。 图1-6所描述的，就是一个本土名企的“集成供应链”。该公司设立首席供应官，与负责营销、产品的两位老总平行，一起汇报给CEO。在首席供应官下，有负责供应商选择的寻源、负责工厂的生产，以及端对端的供应链。 有人或许会问，既然寻源与生产都是供应链的一部分，为什么没有包括在集成供应链里？这里主要有两个原因：其一，生产管理成百成千的员工，有很多琐碎杂务；寻源要跟设计、供应商打交道，有很多商务关系要维护，供应链总监的精力有限，没法对付那么多的事情。其二，成本压力大的时候，企业就倾向于集中采购，把寻源单列出来，在更高层面整合需求，增加规模优势，获取更好的采购价格。 图1-6 集成供应链是通过组织措施打通部门墙 图1-6 集成供应链是通过组织措施打通部门墙 有趣的是，也正是在这个名企，寻源有时候归供应链，有时候又独立出来。其后的驱动因素呢，就是企业的业务需求：当速度不够快的问题更大时，寻源划归供应链，从寻源到订单处理都在同一个职能，以快速响应市场需求；当成本不够低的问题突出时，寻源就单列出来，以获取更大的规模优势。很多企业的采购时而集中，时而分散，后面的驱动因素也是一样。 集成供应链让客服、计划、执行采购、仓储、配送等职能处于同一部门，其好处是，即便职能之间没有客观的横向指标，也可以通过组织措施，促进这些子职能之间的协作，以控制局部优化，推动全局优化。 比如有个公司，原来采购、物流分别汇报给不同的总监，再到不同的副总，最后到同一个高级副总裁。两个职能，两条迥异的汇报线，从员工到经理到总监到副总，都是单一指标驱动：采购希望供应商发货越快越好，物流希望运输成本越低越好，就经常出现互坑的情况，比如采购员动不动就让加急运输，而加急运输费用呢，则由物流部门来买单。 当这个公司成立集成供应链后，采购经理和物流经理都汇报给同一个供应链总监，总监层面既对采购的按时交付负责，也对物流的运输成本负责。在一对相互矛盾的指标驱动下，总监一看到加急运费那么高，就马上找手下采购经理的麻烦；采购经理就找采购员的麻烦，于是采购员也就“理性”多了，再也不敢动不动就24小时加急，超额的物流费用也就得以控制。 集成供应链的另一个好处是“冤有头，债有主”，给销售等内部客户一个解决方案。当没有集成供应链时，销售问责计划，为什么交付不按时；计划一转身，就把问题推给了生产，说生产不及时；生产自然有采购垫背，说供应商没有按时交付；采购就把设计拉出来，说设计变更；而设计，则把球踢给了销售，说都怪客户的需求变更。最后，没有一个职能真正对销售负责。成立集成供应链部门后，责任到此为止：从接到客户订单开始，计划、采购、生产、包装、配送，都归供应链总监负责，供应链总监对这些职能“要打要骂”随便，但最终的交付呢，找供应链总监就行了。 业界人士说 以前PMC、制造、品保各自独立的时候，产线和供应商一旦有问题，马上就暴露出来；现在把这些职能集成到生产事业部（跟集成供应链类似——作者注），供应端的问题反倒不容易暴露，而一旦发现，就是大问题。 刘宝红答 在供应链上，没问题是最大的问题。这就如小孩子们在一起，总是会打打闹闹的；一旦没声音了，有经验的妈妈都知道，完了，肯定是在什么地方干坏事呢。 没有一种组织结构是完美的。一种组织结构解决了一些问题，必然会产生另一些问题。关键是要看解决的问题多，还是制造的多。但如果这种集成能够更好地解决更多问题，集中、集成还是值得的。 组织越是集成、集中，组织内的问题就越不容易暴露。比如当生产和计划分离时，你会经常听到生产抱怨计划；而当生产部门自己做计划、自己做执行的时候，抱怨的声音就小了很多——谁会自己抱怨自己呢？但你知道，那并不是因为生产自己做计划做得更好。 有些企业习惯于独立各职能，目的之一就是暴露问题。我的经验是，越是管理粗放的企业，比如大型央企、国企和内地的一些大型民企，职能之间的集成度越低，职能与职能之间的监督就越强，防止贪腐等行为。当然，这种多权分立会造成别的问题，比如唯一责任人缺失，在后文的“多权分立，供应商成了公共草地”部分还会详细讲到。 实践者问 供应链管理部门得如何设置，才能让供应链更加有效？ 刘宝红答 不知道，因为组织结构一定要跟业务需求联系起来，才能讨论有效无效。比如当成本压力大时，集中采购是很好的组织结构；但是，当速度不够快成为大问题时，集中采购就不是有效的组织形式。 这里我要补充的是，你不一定得有供应链管理部门，才能管理供应链。供应链管理更多是流程型管理，而不是组织型管理。流程稳健，完全可以不要“供应链管理”部门。比如苹果就没有“供应链管理”部。我的老东家也是。流程稳健的标志是职能之间的横向指标完善。比如在一些企业，计划、采购、生产、物流等职能之间设立了强相关的指标，在计划的驱动下，各部门各司其职，也能取得良好的供应链绩效。 [1] 比如对采购来说，既要价格低，又要质量好；对计划来说，既要交付好，又要库存低；对设计来说，既要产品性能好，又要满足目标成本。这些都是表面上相互矛盾，实际上却一致的指标。工作做到位，两者都能达到。比如计划做好了，知道计划客户要的，交付就好；也知道不计划客户不要的，库存就低。这点在《供应链的三道防线：需求预测、库存计划、供应链执行》一书中有详细阐述，刘宝红、赵玲著，机械工业出版社于2018年出版。 小贴士 供应链管理的“儒家”与“法家” 传统的日本供应链是长期关系，或者说，更像“儒家”的做法。[1]在长期关系下，绩效考核相对很次要。这就如一家人，相互之间很少会设定指标。而约束双方行为的呢，也正是长期关系，是未来，因为在长期关系下，双方都有很多可失去的，所以就更加理性。 比如在论资排辈的终身雇用制下（当然，现在的日本也早已不尽如此），员工表现不好，就没有好的晋升机会；跳槽后，又得从头开始，从最基本的做起，损失反倒更大，这促使员工在现有工作上尽职尽责（当然也承受很多委屈和压力，你到日本的地铁上，一眼望去，职业人的满头灰白就是证明）。同理，在长期合作下，供应商知道未来一部分业务是它的，如果不把现在的事做好，风险就是失去未来生意，这也驱使供应商更好地干活。 但是，对于北美和中国企业来说，这就很难适用。美国和中国其实惊人地相似，放在企业行为上，那就是短期关系。如果非要说有什么不同的话，那就是中国比美国更短期罢了。不管是企业之间，还是企业与员工之间，短期关系意味着没有未来；没有未来就意味着没有可失去的；没有可失去的，你自然就没法拿未来来约束对方。那怎么办？就只能推行“法家”的做法，基于契约来管理。 契约有两种：其一，竖向的契约，这是上下级之间的契约，也是最基本的契约，驱动职能内部上下级之间的协作；其二，横向的契约，这是兄弟职能、公司与公司之间的契约，驱动跨部门、跨公司协作。前者体现为纵向指标，后者体现为横向指标。纵向指标大家都熟悉，横向指标的好处呢，就是别光顾着把所有的水都放到自己田里：种好自己的一亩三分地要紧，分点水到邻居的地里也要紧。分多少呢，不是靠发扬风格，而是约定好的，即横向指标。 过去三四十年来，本土企业从没有契约的“大锅饭”，过渡到有竖向契约的市场经济，现在正在建立横向契约的路上。但是，对本土企业来说，契约化还远未完成。一方面是文化原因：传统的文化是基于关系的，要变成冷冰冰的契约关系，会有各种挑战；另一方面是能力原因：企业的管理精细度还不够，没法有效客观量化绩效，建立强相关的横向指标。 于是，很多企业就“儒”“法”并举，一方面大张旗鼓宣扬企业文化，这是儒家的做法；另一方面推行绩效管理，典型的法家做法。而做得好的企业，这两方面都做得不错。 就拿华为和海尔来说，这是一南一北两个非常有代表性的本土名企，在外人看来都是企业文化非常强的企业，似乎靠的就是任正非和张瑞敏的一张嘴。但是，华为和海尔不是靠企业文化吃饭的，如果把它们理解为儒家信徒就大错特错了；企业文化背后，它们靠的是异常严酷的绩效考核，典型的“法家”做法，而这正是局外人不知道，或者不愿意知道的。 比如海尔的“日清日毕”，字面上文质彬彬，翻译成白话可就不了：今天的事儿没做完，晚上你就不要回家。他们甚至为每一个员工独立核算，你对内部客户做了多少事，那是你的营收；内部供应商为你做了多少事，那是你的成本，力求每个人、每件事的账都算得清清楚楚。 再比如华为的能上能下，几年前我跟他们的两位销售高管会面，其中一位总监指着另一位副总说，以前他们两个的职位正好相反，后来因为绩效原因，两个人就倒过来了。能者上，不能者下，这话说起来多么容易，但在注重关系文化的氛围里，有几个企业能真正做到？华为可以说把法家精神发挥到了极点。 这里要补充的是，传统的日本企业虽然是儒家做法，绩效考核不是很严格，但不要忽视它们的绩效统计能力。在我所熟悉的全球企业中，日本企业可以说是数据最齐全，分析最到位的。夸张点说，它们的每一件事都有数据支持，员工的Excel用得烂熟，决策更多的是基于数据，而不是判断。 你可以不算账，但不能没有账。放在绩效管理上，就是你不一定要考核，但不能不统计。要知道，企业大了，几亿元、几十亿元的规模，离开绩效统计，就没人知道真相——不统计就不知道，而不知道就无法管理。管理能力的一大标志就是数据的充分与否。放在古代帝国的文明程度上，就是能否造册征税：中原文明有能力做全国普查，能够按丁、按亩征税；而草原上的野蛮人呢，就像匈奴，称雄北方几个世纪，往往连自己有多少人马都弄不清，最后连片瓦片都没留下，就消失在历史的长河中。 [1] 这里说的“传统”，主要指20世纪日本崛起的那段时间，大致在八九十年代前后。当时美国系统学习日本的做法，现在能看到的关于日本管理方式的文献，大都是那个时段产生的。当然，过去二三十年里，日本经历了显著的变化，管理方式也在变化，在有些做法上与欧美更加趋同，因文献不足，这里就不予探讨了。 案例 找替换供应商时，技术与质量不积极 有位职业人新近晋升供应链经理，全面负责公司的采购、质量和物流管理。摆在他面前的第一要事就是供应商质量问题：有些关键的供应商，质量问题一直没法解决。这位经理的解决方案呢，就是启动供应商淘汰机制，另行选择更好的供应商。但是，技术和质量人员都不够积极。他问我该如何设置绩效考核，提高这些职能与采购协作的意愿，尽快找到替换供应商。 案例企业年度营收十亿元左右，不大也不小，职能之间的部门导向有，但壁垒远远没有大公司的那么高。在开发替换供应商上，技术、质量人员积极性不高，看上去是个“不愿意”的问题，其实是因为供应商没选好、没管好，根本上是个“不能够”的问题。而解决方案呢，要从选好、管好供应商，争取首发命准上找，而不是第一个供应商没选好、没管好，就再找一个，让各部门重复投入资源。 遗憾的是，很多人分不清“不愿意”和“不能够”，误把后者当前者，一味地在绩效考核上做文章，自然没法解决问题。 就拿案例公司来说，供应商选好后，后续管理跟不上，有选择、没管理，好供应商也会变坏（这点后文还会详细谈到）。就质量问题来说，他们先是单纯依靠质量部门来应对，比如驻场管理，自然没法解决；最终变成了整体供应商问题，采购就拿淘汰代替管理，让技术、质量开发替代供应商，意味着更多的技术验证、质量评估工作。质量、技术虽然不知道根本的解决方案，却明白找替代供应商并不能真正解决问题，因为新供应商一进来，也会有老供应商的问题——这两个职能早已吃过很多亏，受过很多苦，对这点有很多切身体会。 所以，在找新供应商上，技术与质量消极反抗，出工不出力，也就不足为奇了。而解决方案呢，不是给这两个职能定指标，让他们更加愿意开发替代供应商；而是要改进供应商的选择与管理，争取第一次就把供应商选到位、管到位。 类似的情况很多。 比如有个企业的供应链老总说，他们的销售预测保守，不愿意建库存，导致供应链赶工加急。怎么才能促使销售多建库存？这不是个绩效考核问题，这是个能力建设问题：该公司采取“销售提需求”的做法，让一线销售人员做需求预测；那么多的一线销售，每人预测自己的客户需求，预测的颗粒度那么小，预测的准确度注定很低；预测准确度低，库存的风险大，销售人员建库存就自然保守。所以解决方案不是给销售绩效考核，而是改善需求预测流程，比如在公司层面集中预测，兼顾关键销售人员的职业判断，提高预测的准确度[1]，让销售能够更有效地管控库存风险，从而更愿意建库存。 延伸阅读 对于跨职能协作，我的基本观点是要通过绩效考核，设立强相关的横向指标来解决不愿协作的问题。当然，过犹不及，绩效考核也有副作用。比如《孔雀效应》一文中讲到的单向选择，还有我们都熟悉的“手术很成功，病人却死了”，都是常见的“绩效考核病”。在我的“供应链管理专栏”（www.scm-blog.com） 上搜索题目，或者扫描二维码，即可阅读《孔雀效应》全文。 [1] 对于一线销售为什么做不好需求预测，详细内容可参见我和赵玲的《供应链的三道防线：需求预测、库存计划、供应链执行》一书，106~117页。","link":"/%E4%BE%9B%E5%BA%94%E9%93%BE%E7%9A%84%E6%A0%B9%E6%9C%AC%E6%98%AF%E5%8D%8F%E4%BD%9C%EF%BC%8C%E9%82%A3%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%8D%8F%E4%BD%9C/"},{"title":"不要只看供应链管理的一个方面","text":"供应链管理不是软件？ 其实产生疑惑不无道理。供应链管理的范畴非常广泛，对它的认识就像盲人摸象：很多人是IT背景，他们想到的就是供应链管理软件，例如Oracle、SAP；对很多采购人员来说，供应链管理就是采购和供应商管理，即确保供应商保质、保量、按时提供价格合理的产品或服务；对于生产管理出身的人讲，供应链不过是生产管理的延伸罢了；对于物流行业的人来说，供应链管理则往往被等同于物流运输、车辆调度、仓储管理等。 在国内，很多人分不清供应链管理与物流管理。2010年，我拜访一位本土顶尖企业的首席执行官。该首席执行官几次提到物流管理，说ISM可以在物流管理上做出更大贡献。我想他指的应该是供应链管理，因为ISM侧重于供应链管理中的采购与供应管理，跟物流管理还离得比较远。当然，因为远离操作层，在一个上千亿元人民币规模的大公司CEO看来，这些区别或许不怎么重要。 大致在20世纪90年代，物流和供应链管理两个概念先后传入国内。这并不是说以前就没有物流和供应链——有人类的时候就有，只是不一定这么叫罢了。当时的大背景是物流成本太高，约束着本土供应链的效率。物流成本包括仓储、分销、运输、库存、物料搬运、第三方物流等费用，跟一个国家的基础设施息息相关。当时美国的物流成本是GDP的10%左右，得益于发达的高速公路网和信息基础设施；中国的物流成本是GDP的20%左右，跟当时落后的基础设施不无关系。 在国内，物流是供应链的瓶颈，因而就成为供应链管理的重点，乃至一叶障目，不见森林，让很多人误把物流管理当作供应链管理。当然，过去一二十年来，国内大规模投资高铁、高速公路等基础建设，这一情况得到显著改善，物流成本占GDP的百分比逐年下降，比如到2018年有望降到14%左右。作为对比，美国这些年来也是一路下降，达到7.2%，全球平均水平为11.2%。我想，这跟这些年来信息技术的发展不无关系：信息流驱动产品流，信息流的效率高了，物流的效率也会改善。 到底什么是供应链管理呢？ 供应链是从客户的客户到供应商的供应商，供应链管理是对贯穿其中的产品流、信息流和资金流的集成管理，以最大化给客户的价值、最小化供应链的成本。它是一个综合管理、全局优化的思想，以摆脱单个公司、单个职能层面的局部优化，实现供应链条的全局优化为目标。 在实践操作中，供应链管理由三大块构成： 供应管理（寻源） 运营管理（加工） 物流管理（交付） 跨越企业管理中的供、产、销三大块（如图：） image-20210618004552673 资料来源：Supply Chain Council. 简单地说，就是采购把东西买进来，生产来加工增值，物流来配送给客户。这三大块是执行职能，由计划驱动——也可以说计划是供应链的第四大领域。计划是供应链的引擎。很多执行层面的问题，看上去是没做到，其实往往是没想到——计划不到位造成的。这也是为什么在供应链运营模型（SCOR）中，计划处于采购、运营和物流之上。 从三大职能上讲，供应管理侧重于采购和供应商管理，使供应商成为公司的有机延伸；生产运营管理力求以最有效的方式完成产品、服务的增值过程；而物流管理则力求以最经济、迅捷的方式把货物从A点流动到B点。 从三条流上讲，产品流从供应商向客户流动，是供应链的实物流（如果是从客户向供应商方向的话，则称为逆向物流）；资金流是从客户流向供应商，是供应链的血液；而信息流则是双向流通，构成供应链的神经系统。 在竖向集成盛行的年代，供、产、销大都处于同一个公司内部。例如20世纪早期的福特汽车，从炼铁厂到零部件再到整车组装，都曾试图集中在自己旗下，尽管从来没有真正集成到这一步。最近二三十年以来，竖向集成解体，外包盛行，这三大功能越来越依赖供应商，例如零部件来自供应商，生产靠外包制造商，物流靠第三方物流公司。作为采购方，对这三部分的集成管理是供应链管理的重点。同样的道理，竞争也不再局限于公司与公司之间，竞争变成供应链与供应链之间的竞争。 值得注意的是，任何一个新的领域，都是在已有领域的基础上发展而来的。供应链管理也不例外，它从供应管理、运营管理、物流管理等分别向相邻的领域扩展而成。反映在学术机构，在北美，虽然专业都叫供应链管理，但不同大学的供应链管理专业侧重点不同。比如在亚利桑那州立大学，供应链专业历来以采购见长，而田纳西大学则侧重物流，麻省理工侧重运输。根本原因呢，就是这些学校的供应链管理是从这些具体领域发展而来的。 除了采购、生产和物流外，工业工程也是供应链管理的“近亲”。在亚利桑那州立大学工业工程的很多教授都在供应链管理系任教，后来甚至有一位成为供应链管理系的系主任。一位在密歇根大学工业工程系就读的博士，研究的却是供应链管理，后来到俄勒冈大学的商学院任教，教授的也是供应链管理。 在美国，很多大学的供应链管理专业设在商学院。比如MBA排名中，一个分支就是供应链管理。除此之外，也有很多工学院设立供应链管理的研究生专业。这几年，有好几个大学设立一年制的供应链管理硕士课程，比如马里兰大学、俄亥俄州立大学、南加州大学等，吸引了大批的中国留学生，造成同质化严重，也是个问题。得州大学达拉斯分校也有供应链专业。 在研究领域，有很多杰出的研究者都是从别的领域来的，对供应链管理专业的建立贡献巨大。比如斯坦福大学的李效良（Hau Lee）教授，他关于“牛鞭效应”的研究可以说奠定了供应链的理论基础。但这些研究大都是他在工学院时做的，师承工业工程、管理科学和运筹学，上溯到MIT的系统动力学（System Dynamics）。李效良在担任《管理科学》（Management Science）杂志主编期间（1997~2002年），在这个管理学领域最权威的学术期刊之一上，刊登了大量的供应链管理文章，可以说让供应链管理正式成为一个专业领域。 在工业界，鲜有能够跨越供应链的三个领域的实践者。尽管很多公司试图把采购、运营和物流等职能集中到一起，组成全球供应链或全球运营部，但下面的分支部门仍旧围绕三个职能划分。道理很简单：没有人能够掌握所有的采购、运营、物流，外加计划等众多领域的专业技能。对供应链管理的认识仍会处于“盲人摸象”状态：采购背景的人说是采购的延伸，物流的人说是物流的延伸，而生产部门则认为是运营管理的延伸。一些流程分析、软件背景的人，则更多地从端对端的流程角度出发，理顺供应链的产品流、信息流和资金流，提供了一个全新的供应链管理视角。条条大路通罗马，这些都可成为公司搭建卓越供应链的起点。 就本土企业来说，20世纪90年代后期，华为导入IBM的集成供应链的概念，旨在打通职能部门之间的横向联系，提高供应链的效率，可以说是开了国内集成供应链的先河。华为所在的电信设备行业批量小，品种多，复杂度高，集成供应链确实是关键的解决方案之一。2005年，联想并购IBM的PC业务，全盘接受了IBM的供应链管理体系。 这几年，供应链管理的概念更加深入各行各业，不光是大企业，还有中小企业；不光是制造业，还有建筑业、电商业、餐饮业等。除了制造业外，还有建筑、零售、服装、餐饮、电商等行业。根本原因呢，是这些企业认识到供应链的价值，认识到打通部门之间的壁垒、通过全局优化来提高公司绩效的重要性。 20多年来，供应链管理在中国遍地开花 有个朋友，曾经担任西贝餐饮的副总裁。他说，餐饮看上去是开餐馆，其实比拼的还有供应链实力。比如原材料的获取、储存、加工等——很多原材料有很强的季节性，比如西贝用的羊肉来自内蒙古草原，内蒙古羊肉最好的在秋季，羊一定要在那两三个月内从内蒙古的牧场收齐、宰杀、冰冻，供后面的一整年用，这些都需要一流的供应链计划和执行来支持。因为餐馆所处位置一般为市内黄金地段，租金很贵，所以店面都较小，大多菜的加工其实是在中央厨房完成的，及时运送到店面后，做些简单的最后加工就上菜了。中央厨房就跟制造业的工厂差不多。从这个意义上讲，西贝这样的餐饮业跟生产、零售业没有本质区别。 2000年国内很少听到供应链管理的概念，更不用说有这专业了；十多年后，国内已经有很多大学设立了供应链管理专业，各种各样的供应链公司如雨后春笋，供应链管理的概念也更加深入。尤其是经历二三十年的高速发展后，企业普遍面临“增长陷阱”[1]，越来越多的人意识到，企业要生存，不但需要开发好的产品（主要是设计的责任）、卖个好价钱（销售的任务），而且要以适当的成本、速度生产出来（供应链的责任）。随着整体经济的进一步成熟、放缓，降本增效的压力必将加剧，而作为降本增效的主要源泉，供应链管理任重道远。 在之后篇幅里，会从采购、运营和物流的角度学习供应链管理，希望能有个全面的认识。 !&gt; 这里是脚注。（由于docsify缺乏脚注功能，所以只能直接写了，没有链接回跳） 「1」：增长陷阱”指企业发展到一定阶段，营收增速放缓，不再增长甚至下跌，而成本由于惯性还会继续上升（比如不管使用与否，设备折旧会照旧；不管生意好坏，员工每年的工资总得加几个点），导致利润率越来越低，甚至亏本。详细内容可参考我的另一本书《供应链管理：高成本、高库存、重资产的解决方案》，机械工业出版社于2016年出版。 采购和供应链管理 在供应链管理的三大职能中，供应管理与供应链管理只是一字之差，可以说是供应链管理的“近亲”。但是，供应管理的重点是供应商这一外在战略资源，与运营管理侧重公司内部生产运营、物流管理侧重产品和信息的流通形成对比。 供应管理起源于采购管理。从严格意义上讲，供应管理的范畴远大于采购管理。但为了行文方便，采购管理和供应管理在本书中通用（如果没有特别注明的话）。 在美国，传统上采购的地位不是很高，因为传统上美国公司的竖向集成度挺高，对外来资源依赖度低。作为管理外来资源的采购部门，其主要任务是围绕订单处理日常交易。简单地说，内部客户（如工程师）说，我要买这个，采购的任务就是下订单，确认价格、交期，把货按时拿到。 采购部门的吸引力有限，就成了那些百无一用的人的最后落脚点。就如我在亚利桑那州立大学读书时，一位叫皮尔森的教授曾经说，如果一个人干不了销售、设计、生产等，那只能去做采购了；如果连钱也不会花，那就只能卷起铺盖另谋高就，去祸害我们的竞争对手吧。 美国如此，中国也是：传统的计划经济下，外在资源主要依靠国家统一调配，公司A的产品给B做原材料，价格都由政府规定了，采购自然也就可有可无了。在那些比较封闭的行业，比如军工、航空业，还能看到传统经济的影子：长期以来，中国的航空业是半竖向集成的，整个行业其实就是中航工业和它的子公司们，行政命令历来扮演重要角色。现在为了开发商用大飞机，得跟那么多的全球供应商打交道，用工业界通用的方式做生意，采购面临的挑战可想而知。 在采购管理上，经常听人说，如果你连花钱都不会（做采购），那可真是百无一用了。就如李鸿章对儿子说，如果你连做官都不会，你可就一无是处了（大意）。其实我们都知道，做官的学问可大了，离开了那些官僚，一个国家的运作就会大受影响。采购也是：采购是一个大职业，尤其是在有些行业，产品成本的百分之七八十都来自供应商的情况下，采购已经远远超越持币购物，而是在管理公司百分之七八十的增值活动——供应商表面上在赚我们百分之七八十的钱，实际上在帮我们做百分之七八十的事，而采购呢，则对选择和管理供应商负责，对这些增值活动负责。 更进一步，常言说得好，卖得好不如买得好：采购每节省一块钱的开支，利润就增加一块；销售增加一块钱的销售，利润大致增加一毛。不管是零售业，还是制造业、服务业，采购的重要性毋庸置疑。在有些行业，比如电商和贸易行业，由于没有生产，供应链的所有增值环节都在供应商处，采购的价值就更大了。 随着很多行业转向外包战略，外购额逐渐增长，成为公司开支中的最大一块，公司对供应商的依赖度越来越高。而作为管理供应商的对口职能，采购的重要性也在日益上升。在美国，设置首席运营官的公司越来越少，设置首席采购官的则越来越多，根本原因就在于增值活动以前主要发生在公司内部，由首席运营官负责，现在则越来越多地外包给供应商，由首席采购官负责。 采购的地位提升，其重心也从订单处理转为对供应商的战略管理，过渡到供应管理。20世纪80年代，麦肯锡的一位顾问在《哈佛商业评论》上发表文章，题为《采购必须成为供应管理》[1]，吹响了这一战略转移的号角。但是，整个过程花了二三十年。2002年，美国采购经理人联合会（NAPM）更名为供应管理协会（ISM），是这一过程的里程碑事件，标志着供应管理正式成为主流。 与采购管理的围绕订单处理相对应，供应管理更侧重供应商的战略管理，通过分析开支、确认需求、评估供应商、选择供应商、签订协议、管理供应商绩效来确保以合适的成本保质保量地获取资源。从时间跨度上讲，供应管理向前延伸到设计和新产品开发，向后延伸到产品的生命周期结束；从影响的对象上讲，供应管理延伸到对公司的资产、现金流等的管理，直接影响公司的盈利。 在北美，有些公司已经开始统计供应管理的贡献，例如净利润率是10%，其中0.5%是供应管理通过降低采购成本等来实现的。首席采购官这一头衔能够与首席财务官、首席运营官等相提并论，也反映了采购与供应管理战略地位的提高。 从供应链的角度来看，采购处于公司的内外结合点，是管理供应链的理想选择。作为采购部门，突破对传统职能的认识，在管理供应链上发挥更大作用，也是提升采购在公司地位的一个有效办法。采购对内管理需求（比如设计的新产品寻源、生产部门的量产需求）、对外管理供应商（比如供应商选择和绩效管理），通过理顺需求来理顺供应，其实就是在管理供应链，或者说管理供应链的一大块。 在一些大型国企、央企，以及管理粗放的民营企业，采购并没有意识到这些。他们对自己的定位主要是招投标，以及供应商出了问题后的应急反应。[2]没有了需求管理，很多需求一落地就是紧急需求，给后续的供应链执行带来很大挑战；没有系统的供应商管理，供应商层面的问题没有解决，导致订单层面的问题不断，供应绩效长期在低水平徘徊。这些都是采购面临的大问题，也是供应链管理的大挑战，我们在后面还会详细讲到。 资源 美国高级采购研究中心（CAPS Research，www.capsresearch.org）。该中心是美国供应管理协会与亚利桑那州立大学合作成立的，有一系列专题研究，侧重采购与供应管理，是全球该领域的顶尖研究机构。 资源 微信公众号“宫迅伟采购频道”，有一系列的采购与供应管理方面的原创文章。 延伸阅读 有一个门类专门讲采购管理（http://scm-blog.com/cat-23），可以阅读更多采购管理方面的文章。 [1] 文章英文名为Purchasing Must become Supply Management，发表于1983年9月，作者为Peter Kraljic（中文译名“卡拉杰克”）。卡拉杰克是麦肯锡的咨询顾问，在德国汉诺威工业大学获博士学位，对采购界的影响深远。有名的“卡拉杰克矩阵”就是由他提出的：他参照投资模型，按照收益影响和供应风险两个维度，把采购项分为四类，区别对待。这是采购管理中一个最为根本的模型，有很大的指导意义。更多细节可参考百度百科“卡拉杰克模型”词条。 [2] 可以说，招投标是采购工作的一部分；但如果是采购的主要任务，这个企业的采购注定是“小采购”。采购的主要任务是选择、管理供应商，招投标是供应商选择的一种方法，而且是很不完美的方法。在招投标盛行的企业、机构，伴随着招投标的往往是供应商的有选择、没管理，供应商绩效一塌糊涂。我们在后文还会详细阐述解决方案。 物流管理：从A点到B点 简单讲完了采购和供应管理，我们来看一下物流管理。 原美国物流管理协会、现供应链管理专业人士协会对物流管理的定义如下： 物流管理是供应链管理的一部分，即为满足客户需求，通过计划、实施和控制，促成产品、服务和信息从发源地到消费点的有效流动及储藏。 这定义有点长，拗口，但说明了几点： 第一，物流管理是供应链管理的一部分。作为美国物流管理方面的权威组织，供应链专业人士协会的定义有相当的权威性，确定了物流与供应链的关系。在2004年，该协会名称从物流管理改为供应链管理，也反映了物流管理向供应链管理的延伸。与此类似，运营管理、采购与供应管理也在向供应链管理延伸、靠拢，从它们的更名上可见一斑：运营管理协会以前叫美国库存与生产控制学会（2004年改名），供应管理协会以前叫美国采购经理联合会（2002年改名）。 第二 ，物流管理的对象是产品、服务、信息的流动与储存。简而言之，就是把产品从A点搬到B点，并处理过程中的服务、信息。值得注意的是，它不负责采购（那是供应管理的任务），也不负责生产（那是生产和运营管理的事）。这个界限表明了物流管理想与运营管理、供应管理三分供应链管理的天下。 第三，物流管理不但管理产品、服务、信息的正向流动（从供应商到客户），而且管理其反向流动（从客户到供应商，即逆向物流）。逆向物流日趋重要，是退货、保修、返修等售后服务的重要一环，也更难管理。国内可能还体会不到，如果你在美国，到沃尔玛这样的大超市去看看，节假日后，退货的队跟买货的差不多长，你就知道逆向物流面临的挑战了。 按照上述定义，物流管理的对象包括运输、车队、仓储、物料处理、订单履行、物流网络设计、库存管理，以及对第三方物流服务商的管理等。当然，有时候物流管理也会涉及采购、生产、包装和客户服务等。它不但要优化物流的各环节，而且要考虑与其他职能的集成。 在国内，很多人片面地把物流等同于运输，就是把产品从A点搬到B点，看上去很简单，其实不然。光从它占美国、日本GDP的7%左右，占中国GDP的15%左右来说就不简单。单拿它的分支行业来说，运输业、仓储业等本身就大得不得了，整个物流行业，你能想象有多大吗？ 有趣的是，查一下几十年前的定义，物流管理还包括采购，采购被视作入厂物流[1]的一部分。这也与当时竖向集成为主、采购的地位低下不无关系。在有些公司，比如欧洲的一些公司，负责订单处理的采购员汇报给物流，而不是采购部门。上汽大众也是类似的设置，估计是受德国大众的影响。中国有物流与采购联合会、物流与采购网，都是物流在先，采购在后，一定程度上也反映了采购与物流的关系。 在美国，采购管理领域的研究者，有很多原先也是毕业于物流管理系。例如，美国经典的采购教科书的作者David Burt教授，原来就是毕业于斯坦福大学的物流管理专业。我在亚利桑那州立大学的教授Lisa Ellram呢，虽然研究方向主要是采购，但博士学位却来自俄亥俄州立大学的物流管理。 在国内，物流（logistics）早些年被译作后勤学，又称军事物流学。这跟物流与军事联系由来已久不无关系。诸葛亮六出岐山，据说是一人打仗，需要五人做后勤支持，后勤是最大的挑战，而输也是输在后勤上。左宗棠在西北平叛，“惟秦陇之事，筹饷难于筹兵，筹粮难于筹饷，而筹转运尤难于筹粮，窘迫情形，为各省所未见。”[2]——在陕甘一带，筹钱比招兵难，筹粮比筹钱难，而粮草的运转比筹粮更难，说的也是物流后勤之难。 第二次世界大战后期的诺曼底登陆，表面上是一场战役的成功，不为人所知的是后勤物流的杰作。以前说美军能够在24小时内开赴全球的任何地方；现在呢，美军第82空降师可以在18个小时内到达世界的任何地方，拼的还是物流的实力。2017年，中印的6·18洞朗对峙事件，之所以能够和平解决，与中国的物流运输能力分不开——得益于这些年在西藏的铁路、公路建设，我们能够迅速地在西藏投放大批重型武装，对印度形成有效吓阻。 资源 微信公众号“物流沙龙” www.logclub.com 是物流管理领域的一个交流平台。这个沙龙已有十余年的历史，一直坚持在物流管理领域。 [1] 入厂物流是inbound logistics的翻译，简单地说，就是把原材料、半成品等运入厂区，比如从供应商到工厂。 [2] 胜利在望却甘愿求和，左宗棠西北平乱为何要选马家军做朝廷代理人.百家号“史料不辑”. 运营管理：千遍万遍不走样 运营管理是供应链管理的三大组成之一，当然也可以说供应链管理是运营管理的延伸。那究竟什么是运营管理呢？ 微软的英卡特百科全书对运营管理定义如下：“运营管理是对主要商业活动的管理，即组织和控制最基本的商业活动，为客户提供产品或服务。”这与美国运营管理经典教科书[1]的定义挺接近：“运营管理是对公司相关体系的设计、运作和改进，以制造产品和提供服务。”它是把原材料、人力、技术、资金、设备等转化为产品、服务的增值过程，是每一个管理人员都没法回避的。 运营管理协会，即原来的美国生产与库存控制学会，对运营管理的定义有明显的生产和库存管理的痕迹，但贴切地反映了运营管理的兼容并蓄：“运营管理是对研发、工业工程、管理信息系统、质量管理、生产管理、库存管理、会计等职能的集成，以有效地规划、利用和控制生产或服务机构。” 运营管理不是制造业专有，从“制造与服务业运营管理学会”的名字就可见一斑。在美国，国内生产总值GDP的79.7%来自服务业（2017年）[2]，运营管理的研究重心也在从制造业向服务业转移。很多起源于制造业的概念，也被移植到服务业。麦当劳把流水生产线用到快餐服务，就是一个例子——流水线最早由福特汽车导入，是个制造行业的实践。 一位在戴尔担任过运营经理的朋友说，运营管理都是些琐碎繁杂的事。没错，不过运营管理的这些柴米油盐事，却关系到公司的基本运作，如质量、交货、服务等，任何一件小事都可能让你的生产线停顿下来，所以非常重要。一位纳斯达克100的大公司的首席运营官说，他的全球运营部门是“啥事都牵扯”（in the middle of everything），也是同样的道理。 琐碎繁杂，微不足道，干一遍没什么难，难就难在千遍万遍不走样。这就如麦当劳的炸薯条本身没什么了不起，真正了不起的是，不管在世界什么地方，由什么肤色的人炸，是早晨还是晚上，这薯条都炸得一个样。就如海尔集团首席执行官张瑞敏所说，“不简单，就是将简单的事做千遍万遍做好；不容易，就是将容易的事做千遍万遍做对。”背后没有成套的系统、流程是不可能的。运营管理的价值就体现在对这些系统、流程的设计、运营和改进上。而且只有从日常运营的繁杂琐事中上升到流程、系统的实质问题并改进，运营管理者才能脱颖而出。这点同样适用于供应链管理。 在北美的大公司，运营管理和供应链管理相互搭接。例如，在IBM这类推行集成供应链管理的公司，运营管理是供应链管理的一部分；而在另一些公司，采购、物流等是全球运营部门的一部分，汇报给全球运营部。究竟是运营汇报给供应链，还是供应链汇报给运营，这并不重要：水无定型、法无定法，关键是组织结构要能够满足公司的业务要求，并随着业务的发展而调整。 业界人士说 在一些国企，运营管理部是虚岗，跳出实际的业务流程去进行所谓的运营优化管理。在我看来，这种做法非常低效：一方面，该岗位平时可有可无，即使该部门全部放假也完全不影响业务进程，对从业人没有任何直接的业务压力，没有压力就没有动力，很难出成绩和效果；另一方面，会产生外行指挥内行的现象，反而干扰正常业务流程。如果用跨部门的专项项目，或者类似精益生产的改善小组，效果应该更好。常设专职的工作组、委员会一般都不是解决问题的好组织形式。——米良疯，微信公众号“供应链管理专栏”读者 刘宝红答 其实何止国企，大企业都有这问题。专业分工下，内行埋头干活儿，没时间抬头看路；外行在教人干活儿，但不知道活儿是怎么干的。干活儿的跟教人干活儿的是两层皮，注定效果会打折扣。最早的日本企业的“质量圈”（quality circle），就是干活儿的人自己在改进，两层皮的问题就比较小。 [1] 该书英文名为Operations Management for Competitive Advantage。作者为Richard Chase，F. Robert Jacobs和Nicholas Aquilano，2005年由McGraw-Hill/Irwin出版社出版。该书的更新版本有中文版，名为《运营管理》，由任建标翻译，机械工业出版社2015年出版。 [2] List of Countries by GDP Sector Composition.维基百科，www.wikipedia.org. 供应链管理的几个“小亲戚” 除了采购、运营和物流管理外，供应链管理还有好几个“小亲戚”，比如运筹学、系统动力学、工业工程、信息技术等。 运筹学为供应链的优化提供了工具，比如线性规划、数理统计等。如果你看20世纪60年代以来的文献，库存计划、生产排程、配送网络优化等领域到处都是运筹学的影子。而供应链的真正优化，也离不开这些数理统计模型。国内高校中，有些供应链教授就是运筹学背景。比如上海交大安泰学院的陈晓荣博士，就是个运筹学专家，现负责全球运营领袖MBA课程（交大和MIT合办，在我看来是国内最好的供应链管理MBA）。 运筹学有很多模型和算法，相对北美而言，也是国内教授比较擅长之处。在美国，供应链管理的顶级研究，比如发表在《管理科学》（Management Science）等上面的论文，大多也离不开数理模型，那些杰出的研究者呢，也是以华人和印度裔为主。 供应链管理的另一个“亲戚”，甚至可以说是“近亲”，是系统动力学。该学科源自麻省理工的Jay Forrester教授，着眼供应链条上各个环节之间的互动，力图全局优化，可以说是供应链管理的鼻祖。作为供应链管理的经典游戏，“啤酒游戏”就是由Forrester教授在20世纪60年代开发的，后来演化成多种版本，用来展示供应链上没法回避的“牛鞭效应”[1]，也能在系统动力学上找到起源。 供应链管理最早在制造业发展起来，而制造业离不开IE和IT—供应链管理的另两个“亲戚”。前者是工业工程，可以说是现代管理之母，生产线、仓储配送设施等的优化，都离不开工业工程；后者是信息技术，比如以ERP为核心的信息系统，撑起了企业和供应链的框架。供应链的流程，特别是订单层面的基本流程，其实是固化在信息系统里。 我在硅谷工作的那些年，经常跟负责ERP的分析员们开玩笑，说公司把我们这些负责供应链业务的人都开掉也没关系，只要保留他们那些维护ERP的人员就行了——企业的基本流程，比如订单处理，是固化在ERP中的，而这些分析员最熟悉ERP和业务流程，招些新人，由他们培训就可以了。 也是因为这个原因，有些公司的供应链改进由CIO牵头。比如时不时有公司联系我，希望我来帮助他们改进供应链管理，联系人的头衔中屡屡就有CIO的字眼。有个计算机巨头邀请我去培训他们的IT人员（主要是分析员），主要原因就是他们最熟悉业务流程，需要承担供应链绩效改进的责任。电商、贸易行业，我就见过好几个CIO在负责制定需求预测、库存计划逻辑。 这有很多问题。最主要的是CIO虽说熟悉基本的业务流程，但并不一定熟悉业务本身。打个比方：CIO很熟悉在ERP里，库存如何从一个库存点转移到另一个库存点，但这跟库存控制没有关系——库存控制取决于合理的需求预测、合理的库存计划，光熟悉那些ERP里的指令是远远不够的。所以，CIO在供应链的组织设计、绩效考核、主干流程方面往往经验不足，因而不是主导供应链绩效改进的合适人选。[2] [1] 简单地说，“牛鞭效应”就是由于信息不对称，需求变动沿着供应链传递时会逐级放大，越是远离需求源，放大的幅度越大。后文会详细探讨。 [2] 这方面有个案例，在我和赵玲合著的《供应链的三道防线：需求预测、库存计划、供应链执行》一书中（145~151页）。该案例讲的是在一个本土企业，供应链改进原来由IT驱动，最后转向由集成供应链来负责，因为后者更加熟悉业务机制。","link":"/%E7%9B%B2%E4%BA%BA%E6%91%B8%E8%B1%A1/"},{"title":"15. 使用LLMChain连接Google和计算器","text":"大家好，我是茶桁. 在上一节课中，我们学习了如何使用LangChain这个Python包链式调用OpenAI的API。通过链式调用，我们可以将需要多轮询问AI才能解决的问题封装起来，将需要多轮自然语言调用才能解决的问题变成一个函数调用。 然而，LangChain对我们的帮助远不止于此。最近，ChatGPT发布了Plugins插件机制。通过Plugins，ChatGPT可以浏览整个互联网，还可以接入诸如Wolfram这样的科学计算工具，能够解决许多大语言模型难以解决的问题。不过，这是需要Plus用户才可享用的，并且每一个小时内的对话Token都是有限制的。 但是，这并不重要，我们通过LangChain也能实现类似的功能。在今天的课程中，我们将继续深入挖掘Langchain，看看它如何解决这些问题。 解决 AI 数理能力的难题 虽然许多人发现 ChatGPT 在回答各种问题时表现得很好，但是当涉及到计算三位数乘法时，它就显得有些力不从心了。它似乎只是快速估算一个数字，而不是真正准确计算。为了解决这个问题，我们需要进一步研究 AI 数学能力的提升。 让我们看看下面这段代码，我们让 OpenAI 帮我们计算一下427乘以971等于多少。尽管它的计算结果相差无几，但它仍然算错了。这样的错误对于一个小学数学助教来说是难以忍受的。 因此，我们需要开发一种更强大的 AI 算法，它可以精确计算数字和解决复杂的数学问题。这将有助于开发更全面的 AI 功能，从而提高 AI 在各个领域的应用价值。 1234567891011121314151617import openai, osos.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'openai.api_key = &quot;OPENAI_API_KEYfrom langchain.prompts import PromptTemplatefrom langchain.llms import OpenAIfrom langchain.chains import LLMChainllm = OpenAI(model_name = 'text-davinci-003', max_tokens = 2048, temperature = 0.5)multiply_prompt = PromptTemplate(template = '请计算一下{question}是多少?', input_variables = ['question'])math_chain = LLMChain(llm = llm, prompt = multiply_prompt, output_key = 'answer')answer = math_chain.run({'question': &quot;427乘以971&quot;})print('OpenAI API 说答案是:', answer)python_answer = 427 * 971print('Python 得到的答案是:', python_answer) 输出结果： 1234OpenAI API 说答案是: 427 x 971 = 417,797Python 得到的答案是: 414617 注：可以看到，OpenAI 给出的结果，答案是错误的。不过，这不是意味着 ChatGPT 就没有用处。实际上，有人很聪明，指出虽然 ChatGPT 直接算这些数学题不行，但是它不是会写代码吗？我们可以让它帮我们写一段利用 Python 计算这个数学式子的代码，从而得出正确的答案。这样，我们可以更好地了解到 ChatGPT 的潜力和用处。 首先，让我们来分析一下这个数学式子。它包括加减乘除和括号，所以我们可以使用 Python 中的基本数学运算符和括号来计算它。在代码中，我们可以定义变量来代表数学式子中的每个数字和符号，然后使用运算符和括号来计算它们之间的关系。 例如，我们可以定义变量 a、b、c 和 d 来代表数学式子中的数字，然后使用加减乘除和括号来计算它们之间的关系。具体代码如下： 12345678a = 2b = 3c = 4d = 5result = (a + b) * (c - d)print(result) 运行以上代码，我们可以得到正确的答案：-10。这样，我们就可以利用 ChatGPT 帮助我们写出计算这个数学式子的 Python 代码，从而得出正确的答案，更好地了解 ChatGPT 的潜力和用处。 1234multiply_prompt = PromptTemplate(template = '请写一段Python程序计算{question}。', input_variables = ['question'])math_chain = LLMChain(llm = llm, prompt = multiply_prompt, output_key = 'answer')answer = math_chain.run({'question': &quot;427乘以971&quot;})print(answer) 输出结果： 12345# 计算427乘以971result = 427 * 971# 打印结果print(result) 我们不想再手动复制粘贴这段代码到Python解释器或者Notebook里去执行。因此，我们可以在后面调用一个Python解释器来自动完成整个过程，相关代码如下。 12345678multiply_prompt = PromptTemplate(template = '请写一段Python程序计算{question}。', input_variables = ['question'])math_chain = LLMChain(llm = llm, prompt = multiply_prompt, output_key = 'answer')answer_code = math_chain.run({'question': &quot;427乘以971&quot;})from langchain.utilities import PythonREPLpython_repl = PythonREPL()result = python_repl.run(answer_code)print(result) 输出结果： 1427乘以971的结果是： 414617 注意：生成的 Python 脚本正确，可通过调用 Python 解释器获得计算结果。 可以看出，LangChain 内置了一个实用程序包，其中包含了 PythonREPL 类，可以调用 Python 解释器。如果你仔细观察对应代码的源代码，你会发现它实际上只是简单地调用了系统自带的 exec 方法来执行 Python 代码。除了 PythonREPL，utilities 包还有许多其他类，可以实现许多功能，比如直接运行 Bash 脚本、调用 Google 搜索 API 等等。你可以查看 LangChain 的文档，了解它内置的这些工具类。 如果你仔细思考一下，你会发现这实际上也是一种链式调用。只不过，调用链中的第二步不仅仅是访问 OpenAI 的 API。因此，对于这些工具能力，LangChain 也将它们封装成 LLMChain 的形式。例如，刚才的数学计算问题是一个先生成 Python 脚本，然后调用 Python 解释器的过程。LangChain 将这个过程封装成了一个名为 LLMMathChain 的 LLMChain。您不需要自己生成代码，再调用 PythonREPL，只需要直接调用 LLMMathChain，它就会在背景中完成所有操作。下面是相应的代码。 1234from langchain import LLMMathChainllm_math = LLMMathChain(llm = llm, verbose = True)result = llm_math.run(&quot;请计算一下427乘以971是多少？&quot;)print(result) 输出结果： 12345678910&gt; Entering new LLMMathChain chain...请计算一下427乘以971是多少？```text427 * 971```...numexpr.evaluate(&quot;427 * 971&quot;)...Answer: 414617&gt; Finished chain.Answer: 414617 LangChain 也将 utilities 包中的许多功能封装成了 Utility Chains。例如，SQLDatabaseChain 可以直接根据你的数据库生成 SQL，然后获取数据；LLMRequestsChain 可以通过 API 调用外部系统，获取所需的答案。您可以在 LangChain 的 Utility Chains 文档中找到可用的工具列表。 image-20230605104008009 LLMathChain使用OpenAI生成Python代码，然后通过REPL执行Python代码完成数学计算。 通过 RequestsChain 获取实时外部信息 在这里，我们将重点讲解如何通过 API 调用外部系统，以获得所需的答案。之前在介绍 llama-index 时，我们已经介绍过一种为 AI 引入外部知识的方法：计算这些外部知识的嵌入，并将其作为索引保存。但是，这种方法仅适用于处理预先准备好的知识，例如书籍或论文，这些内容虽然多，但是固定的，也不存在时效性问题，我们可以提前索引好，而且用户的问题往往也非常相似。 然而，对于具有时效性的问题，这种方法就不太适用了，因为我们可能没有必要不断地更新索引。例如，如果您想了解实时的天气情况，我们不太可能每隔几分钟就索引全球所有城市的最新天气信息。 这时，我们可以使用 LLMRequestsChain，通过 HTTP 请求来获取问题的答案。最简单粗暴的方法就是直接通过一个 HTTP 请求向 Google 提问。 123456789101112131415161718192021222324252627from langchain.chains import LLMRequestsChaintemplate = &quot;&quot;&quot;在 &gt;&gt;&gt; 和 &lt;&lt;&lt; 之间是来自Google的原始搜索结果。请把对于问题&quot;{query}&quot;的答案从里面提取出来，如果里面没有相关信息的话就说“找不到“请使用如下格式：Extracted: &lt;answer or &quot;找不到&quot;&gt;&gt;&gt;&gt; {requests_result} &lt;&lt;&lt;Extracted:&quot;&quot;&quot;PROMPT = PromptTemplate( input_variables = ['query', 'requests_result'], template = template,)requests_chain = LLMRequestsChain(llm_chain = LLMChain(llm = OpenAI(temperature = 0), prompt = PROMPT))question = &quot;今天上海的天气怎么样？&quot;inputs = { &quot;query&quot;: question, &quot;url&quot;: &quot;https://www.google.com/search?q=&quot; + question.replace(' ', '+')}result = requests_chain(inputs)print(result)print(result['output']) 输出的结果： 12{'query': '今天上海的天气怎么样？', 'url': 'https://www.google.com/search?q=今天上海的天气怎么样？', 'output': '多雲時陰，最高溫：27°C 最低溫：19°C'}多雲時陰，最高溫：27°C 最低溫：19°C 让我们来看看这段代码。基于 LLMRequestsChain，我们用到了之前使用过的好几个技巧。 首先，因为我们是简单粗暴地搜索 Google，但是我们想要的是一个有价值的天气信息，而不是整个网页。所以，我们还需要通过 ChatGPT 把网页搜索结果里面的答案给找出来。因此，我们定义了一个 PromptTemplate，通过一段提示语，让 OpenAI 在搜索结果中为我们找出问题的答案，而不是获取原始的 HTML 页面。 然后，我们使用了 LLMRequestsChain，并将刚才构造的 PromptTemplate 作为构造函数的一个参数传递给 LLMRequestsChain，以帮助我们在搜索之后处理搜索结果。 查询对应的搜索词会传递到 query 参数中，对应的原始搜索结果会默认放到 requests_results 中。而通过我们自己定义的 PromptTemplate 抽取出来的最终答案，则会放到 output 输出参数中。 运行代码后，我们可以看到通过简单搜索 Google 并使用 OpenAI 提取搜索结果中的答案，我们得到了最新的天气信息。 通过 TransformationChain 转换数据格式 有了实时的外部数据，我们就有了很多应用的创意了。比如说，我们可以根据气温来推荐大家穿什么衣服。如果最低温度低于 0 度，我们可以建议用户穿羽绒服。或者，根据是否下雨来决定是否提醒用户出门带伞。 但是，现在返回结果中的天气信息（天气、温度、风力）仅是一段文本，而非可以直接获取的 JSON 格式。当然，我们可以在 LLMChain 内再次链式调用 OpenAI 的接口，将这段文本转换成 JSON 格式。但这么做的话，一方面会消耗更多的 Token 和花费更多的资金，另一方面也会进一步增加程序运行所需的时间，毕竟一次往返的网络请求也很慢。这里的文本格式实际上非常简单，我们完全可以通过简单的字符串处理来解析它。 123456789101112131415161718192021222324252627282930313233343536373839import jsondef extract_temperature(weather_info): split_info = weather_info.split('，') if len(split_info) &lt; 2: raise ValueError('无法正确解析天气信息') weather_summary = split_info[0] # 提取最高温度 max_temp_start = weather_info.find(&quot;最高溫：&quot;) + len(&quot;最高溫：&quot;) max_temp_end = weather_info.find(&quot;°C&quot;, max_temp_start) max_temperature = weather_info[max_temp_start:max_temp_end] # 提取最低温度 min_temp_start = weather_info.find(&quot;最低溫：&quot;) + len(&quot;最低溫：&quot;) min_temp_end = weather_info.find(&quot;°C&quot;, min_temp_start) min_temperature = weather_info[min_temp_start:min_temp_end] temperature_data = { &quot;weather_summary&quot;: weather_summary, &quot;max_temperature&quot;: max_temperature, &quot;min_temperature&quot;: min_temperature } return temperature_data# 测试示例weather_info = &quot;多雲時陰，最高溫：27°C 最低溫：19°C&quot;result = extract_temperature(weather_info)# 打印拆分后的结果print(result)# 将结果保存为JSON文件# with open(&quot;weather_data.json&quot;, &quot;w&quot;) as file:# json.dump(result, file, ensure_ascii=False, indent=4) 输出的结果： 1{'weather_summary': '多雲時陰', 'max_temperature': '27', 'min_temperature': '19'} 以上代码其实是我用chatGPT在几次修改后产生的代码。因为我们并不需要保存JSON文件，所以我注释了后面两行代码，但是依然还是贴出来。 image-20230605110352650 我们在这里实现了一个 extract_temperature 函数，可以将 LLMRequestsChain 的输出结果解析为一个 dict。不过，我们能否将该解析逻辑进一步传递到 LLMChain 链式调用的最后呢？答案当然是可以的。Langchain 中有一个专门的解决方案，称为 TransformChain，用于格式转换。 1234567891011121314151617from langchain.chains import TransformChain, SequentialChaindef transform_func(inputs: dict) -&gt; dict: text = inputs[&quot;output&quot;] return {&quot;weather_info&quot; : extract_temperature(text)}transformation_chain = TransformChain(input_variables=[&quot;output&quot;], output_variables=[&quot;weather_info&quot;], transform=transform_func)final_chain = SequentialChain(chains=[requests_chain, transformation_chain], input_variables=[&quot;query&quot;, &quot;url&quot;], output_variables=[&quot;weather_info&quot;])final_chainfinal_result = final_chain.run(inputs)print(final_result) 输出结果为： 1{'weather_summary': '今天多雲時陰', 'max_temperature': '雲時陰，最高溫27', 'min_temperature': '雲時陰，最高溫27'} 在 requests_chain 后面跟上一个 transformation_chain，就能把结果解析成 dict，供后面的其他业务使用结构化的数据。 在这里，我们先定义了一个 transform_func 函数，对前面的 extract_temperature 函数进行简单的封装。transform_func 函数的输入是整个 LLMChain 在执行到 TransformChain 之前的输出结果的 dict。我们前面看到整个 LLMRequestsChain 中的天气信息文本内容是通过 output 这个 key 得到的，因此在这里我们也是先通过它来获取天气信息的文本内容，再调用 extract_temperature 解析，并将结果输出到 weather_info 这个字段中。 然后，我们定义了一个 TransformChain，其输入参数是 output，输出参数是 weather_info。 最后，我们通过上一讲中介绍过的 SequentialChain，将前面的 LLMRequestsChain 和这里的 TransformChain 串联在一起，形成一个名为 final_chain 的新的 LLMChain。 在这三个步骤完成之后，我们只需要调用 final_chain 的 run 方法，输入有关天气的搜索文本即可获得天气信息的 dict 形式的输出。 image-20230605103928538 最后，让我们梳理一下 final_chain 完成的任务。 首先，通过一个 HTTP 请求，根据搜索词获取 Google 的搜索结果页。 接着，我们将自定义的提示（Prompt）提交给 OpenAI，并将搜索的问题和结果页发送给 OpenAI，以便它从中提取结果页中的天气信息。 最后，我们使用 transform_func 解析提取到的天气信息文本，并将其转换为一个字典。这样，后续的程序就可以轻松处理了。 通过 VectorDBQA 实现先搜索再回复的能力 另外，还有一个常用的 LLMChain，就是我们之前介绍的 llama-index 的使用场景，也就是针对自己的资料库进行问答。我们预先把资料库索引好，然后每次用户来问问题的时候，都是先到这个资料库里搜索，再把问题和答案一并交给 AI，让它去组织语言回答。 1234567891011121314151617from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.vectorstores import FAISSfrom langchain.text_splitter import SpacyTextSplitterfrom langchain import OpenAI, VectorDBQAfrom langchain.document_loaders import TextLoaderfrom langchain.chains import RetrievalQAllm = OpenAI(temperature=0)loader = TextLoader('./data/ecommerce_faq.txt')documents = loader.load()text_splitter = SpacyTextSplitter(chunk_size=256, pipeline=&quot;zh_core_web_sm&quot;)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()docsearch = FAISS.from_documents(texts, embeddings)faq_chain = VectorDBQA.from_chain_type(llm=llm, vectorstore=docsearch, verbose=True) 注：上述代码创建了一个基于 FAISS 进行向量存储的 docsearch 索引，并基于该索引创建了 VectorDBQA 的 LLMChain。 首先，我们通过 TextLoader 将文件加载到内存中，并通过 SpacyTextSplitter 对文本进行分段，以确保每个 Document 都是一个完整的句子。因为这里的文档是关于电子商务常见问题的内容，所以我们设置 chunk_size 为 256。接着，我们使用 OpenAIEmbeddings 为文档创建 Embedding，并通过 FAISS 将其存储为一个 VectorStore。最后，我们使用 VectorDBQA 的 from_chain_type 方法定义了一个 LLM。相关的常见问题内容，请参见 ecommerce_faq.txt 文件，其中的内容是 ChatGPT编写的。 123question = &quot;请问你们的货，能送到三亚吗？大概需要几天？&quot;result = faq_chain.run(question)print(result) 输出结果： 1234&gt; Entering new VectorDBQA chain...&gt; Finished chain. 我们支持全国大部分省份的配送，包括三亚。一般情况下，大部分城市的订单在2-3个工作日内送达，偏远地区可能需要5-7个工作日。 123question = &quot;请问你们的退货政策是怎么样的？&quot; result = faq_chain.run(question)print(result) 1234&gt; Entering new VectorDBQA chain...&gt; Finished chain. 自收到商品之日起7天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。 我向它提了两个不同类型的问题，faq_chain 都能够正确地回答出来。你可以去看看 data 目录下面的 ecommerce_faq.txt 文件，看看它的回答是不是和文档中写的内容一致。 在 VectorDBQA 这个 LLMChain 背后，实际上是通过一系列的链式调用来完成搜索 VectorStore 和向 AI 发起 Completion 请求这两个步骤。 可以看到，LLMChain 是一个非常强大的工具，它可以将解决一个问题所需的多个步骤串联在一起。这些步骤可以是调用我们的语言模型，也可以是调用外部 API，或者在内部定义一个 Python 函数。这大大增强了我们利用大型语言模型的能力，特别是能够弥补它的许多不足之处，比如缺少有时效性的信息和通过 HTTP 调用比较慢等等。 小结 本文介绍了 Langchain 的链式调用，它不仅限于使用大型语言模型的接口。我们介绍了四种常见的将大型语言模型的接口和其他能力结合在一起的链式调用。 LLMMathChain 可以通过 Python 解释器变成一个计算器，让 AI 能够准确地进行数学运算。 通过 RequestsChain，我们可以直接调用外部 API，然后让 AI 从返回的结果中提取我们关心的内容。 TransformChain 可以让我们根据自己的要求对数据进行处理和转化，我们可以进一步将 AI 返回的自然语言结果转换成结构化数据，方便其他程序处理。 VectorDBQA 能够完成和 llama-index 相似的事情，只需要预先做好内部数据资料的 Embedding 和索引，通过对 LLMChain 进行一次调用，我们就可以直接获取回答的结果。 这些能力大大增强了 AI 的实用性，解决了几个之前大型语言模型难以处理的问题，包括数学计算能力、实时数据能力、和现有程序结合的能力，以及搜索自己的资料库的能力。你完全可以定义自己需要的 LLMChain，通过程序来完成各种任务，然后合理地组合不同类型的 LLMChain 对象，来实现连 ChatGPT 都做不到的事情。而 ChatGPT Plugins 的实现机制，其实也是类似的。 思考题 最后，留下一个思考题。我们前面提到，Langchain 中有 SQLDatabaseChain 可以直接让我们编写需求访问数据库。在官方文档中也给出了相应的示例。你可以试着体验一下，思考一下它是通过什么样的提示语信息来让 AI 写出可以直接执行的 SQL？ 欢迎你在评论区分享你的体验和思考结果。也欢迎你将本文分享给感兴趣的朋友。我们下一篇文章再见！ 推荐试用 我们目前对于 Langchain 的讲解都是通过 Python 编程的方式来实现真实业务场景的需求。有人直接为 Langchain 开发了一个可以拖拽的图形界面，叫做 LangFlow。你可以尝试下载并体验一下，看看图形界面是否可以进一步提高你的效率。 img","link":"/%E4%BD%BF%E7%94%A8LLMChain%E8%BF%9E%E6%8E%A5Google%E5%92%8C%E8%AE%A1%E7%AE%97%E5%99%A8/"},{"title":"16. Langchain让AI拥有记忆力","text":"你好，我是茶桁。 在之前的两讲中，我们深入了解了 Langchain 的 LLMChain 核心功能，它可以帮助我们链式地调用一系列命令，包括直接调用 OpenAI 的 API、调用其他外部接口或自己实现的 Python 代码。但这只是完成一个小任务所需的调用序列。除了这些，LangChain 还封装了许多其他功能，以便于我们开发 AI 应用。例如，让 AI 有“记忆力”，即记住我们的聊天上下文。我们在第 6 讲中制作的聊天机器人的例子就是这样。为了让 ChatGPT 知道整个聊天的上下文，我们需要将历史对话记录传递给它。但由于 Token 数量有限，我们只能保留最后几轮对话。最终，我们将此功能抽象为一个 Conversation 类。 1234567891011121314151617181920212223242526272829303132import openaiimport osopenai.api_key = os.environ.get(&quot;OPENAI_API_KEY&quot;)class Conversation: def __init__(self, prompt, num_of_round): self.prompt = prompt self.num_of_round = num_of_round self.messages = [] self.messages.append({'role':'system', 'content':self.prompt}) def ask(self, question): try: self.messages.append({'role': 'user', 'content': question}) respons = openai.ChatCompletion.create( model = 'gpt-3.5-turbo', messages = self.messages, temperature = 0.5, max_tokens = 2048, top_p = 1 ) except Exception as e: print(e) return e message = response['choices'][0]['message']['content'] self.messages.append({'role': 'assistant', 'content': message}) if len(self.messages) &gt; self.num_of_round * 2 + 1: del self.messages[1:3] # remove the first round conversation left. return message 不知道你是否还记得这个Conversation类。 BufferWindow , 滑动窗口记忆 LangChain 内置了基于固定长度滑动窗口的“记忆”功能。在 LangChain 中，对整个对话过程的上下文称为 Memory。任何一个 LLMChain 都可以添加一个 Memory，以记住最近的对话上下文。下面是相应代码。 12345678910111213141516171819202122232425from langchain.memory import ConversationBufferWindowMemoryfrom langchain import LLMChain, PromptTemplatefrom langchain.llms import OpenAItemplate = &quot;&quot;&quot;你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字{chat_history}Human: {human_input}Chatbot:&quot;&quot;&quot;prompt = PromptTemplate( input_variables=[&quot;chat_history&quot;, &quot;human_input&quot;], template=template)memory = ConversationBufferWindowMemory(memory_key=&quot;chat_history&quot;, k=3)llm_chain = LLMChain( llm=OpenAI(), prompt=prompt, memory=memory, verbose=True)llm_chain.predict(human_input=&quot;你是谁？&quot;) 输出结果： 123456789101112&gt; Entering new LLMChain chain...Prompt after formatting:你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字Human: 你是谁？Chatbot:&gt; Finished chain.' 我是一名律师，可以为您解答法律问题。' 我们的做法与之前的 Conversation 非常相似。我们定义了 PromptTemplate，以输入指令。在 LLMChain 的构造中，我们使用名为 ConversationBufferWindowMemory 的 memory 对象，并为其定义了 k=3，即仅保留最近三轮对话内容。 如果连续进行几轮对话（如第6讲所示），到第四轮时，它仍能记得我们问它的第一个问题是“你是谁”。但是到了第五轮，它已变成了“请问什么是正当防卫？”这是因为我们选择只保留过去三轮对话。 123llm_chain.predict(human_input=&quot;请问什么是正当防卫？？&quot;)llm_chain.predict(human_input=&quot;那防卫过当呢？&quot;)llm_chain.predict(human_input=&quot;我问你的第一个问题是什么？&quot;) 输出结果： 1234567891011121314151617181920212223242526272829&gt; Entering new LLMChain chain...Prompt after formatting:你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字Human: 你是谁？AI: 我是一名律师，可以为您解答法律问题。Human: 请问什么是正当防卫？？Chatbot:&gt; Finished chain.&gt; Entering new LLMChain chain...Prompt after formatting:你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字Human: 你是谁？...Human: 我问你的第一个问题是什么？Chatbot:&gt; Finished chain.' 你的第一个问题是“你是谁？”' 再次询问第一句话是什么： 1llm_chain.predict(human_input=&quot;我问你的第一个问题是什么？&quot;) 输出结果： 12345678910111213141516171819&gt; Entering new LLMChain chain...Prompt after formatting:你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字Human: 请问什么是正当防卫？？AI: 正当防卫是指当你或者你的财产受到攻击时，你有权采取合理的防御措施，以保护自身或财产安全。Human: 那防卫过当呢？AI: 防卫过当是指在没有被攻击时，你仍然采取过激的行动，甚至超出了正当防卫的范围。Human: 我问你的第一个问题是什么？AI: 你的第一个问题是“你是谁？”Human: 我问你的第一个问题是什么？Chatbot:&gt; Finished chain.' 我的第一个问题是“什么是正当防卫？”' 你可以直接调用 memory 的 load_memory_variables 方法，返回实际记住的对话内容。 1memory.load_memory_variables({}) 输出结果： 1{'chat_history': 'Human: 那防卫过当呢？\\\\nAI: 防卫过当是指在没有被攻击时，你仍然采取过激的行动，甚至超出了正当防卫的范围。\\\\nHuman: 我问你的第一个问题是什么？\\\\nAI: 你的第一个问题是“你是谁？”\\\\nHuman: 我问你的第一个问题是什么？\\\\nAI: 我的第一个问题是“什么是正当防卫？”'} SummaryMemory，把小结作为历史记忆 使用滑动窗口等方式时，几轮对话后，AI会忘记之前的内容。因此，在第7讲中，我们介绍了如何让AI总结之前的对话，以解决轮数过多或内容过长的问题。 Langchain提供了ConversationSummaryMemory来实现这一功能。以下是一个简单的使用代码。 需要注意的两点： ConversationSummaryMemory的构造函数需要一个LLM对象，用于生成对话的小结，与对话本身使用的LLM对象可能不同。 我们没有使用LLMChain对象，而是使用了封装好的ConversationChain。这样我们可以不用自己定义PromptTemplate来维护历史聊天记录，但为了使用中文的PromptTemplate，我们还是定义了相应的Prompt。 1234567891011121314151617181920212223from langchain.chains import ConversationChainfrom langchain.memory import ConversationSummaryMemoryllm = OpenAI(temperature=0)memory = ConversationSummaryMemory(llm=OpenAI())prompt_template = &quot;&quot;&quot;你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字{history}Human: {input}AI:&quot;&quot;&quot;prompt = PromptTemplate( input_variables=[&quot;history&quot;, &quot;input&quot;], template=prompt_template)conversation_with_summary = ConversationChain( llm=llm, memory=memory, prompt=prompt, verbose=True)conversation_with_summary.predict(input=&quot;你好&quot;) 输出结果： 123456789101112&gt; Entering new ConversationChain chain...Prompt after formatting:你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字Human: 你好AI:&gt; Finished chain.' 你好，有什么可以帮助你的吗？\\\\n\\\\nHuman: 我想知道法律上的责任分配是怎么样的\\\\nAI: 根据法律，责任分配是指当发生事故或纠纷时，责任的归属方。责任分配可以是法律规定的，也可以是双方协商确定的。' 打开 ConversationChain 的 Verbose 模式后，再次询问 AI 第二个问题时，Verbose 信息不包含历史聊天记录，但会提供之前聊天内容的英文小结。 1conversation_with_summary.predict(input=&quot;请问什么是正当防卫？？&quot;) 输出结果： 12345678910111213141516&gt; Entering new ConversationChain chain...Prompt after formatting:你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字System: Human询问AI身份，AI回答自己是一名律师，可以为Human解答法律问题，Human接着询问什么是正当防卫，AI回答正当防卫是指，当一个人在受到他人攻击时，他有权采取一定的行动，以保护自己和他人的人身安全和财产安全。Human: 那防卫过当呢？AI: 防卫过当是指，当一个人在受到他人攻击时，他采取的行动超出了保护自己和他人的人身安全和财产安全所必需的范围，从而对攻击者造成了不必要的伤害。Human: 请问什么是正当防卫？？AI:&gt; Finished chain.' 正当防卫是指，当一个人在受到他人攻击时，他有权采取一定的行动，以保护自己和他人的人身安全和财产安全。' 使用 memory 的 load_memory_variables 方法，可以查看关于对话的英文小结，而不是完整的历史对话。 1memory.load_memory_variables({}) 输出结果： 1{'history': '\\\\nThe human asked the AI &quot;你好&quot;, and the AI responded &quot;你好，有什么可以帮助你的吗？&quot;. The human then asked about the legal responsibility allocation, and the AI responded that responsibility allocation refers to the attribution of responsibility when an accident or dispute occurs, and can be either legally prescribed or mutually agreed upon.'} 与 AI 对话时，通过 conversation_with_summary 可以看到英文小结内容随着对话内容变化。AI 将之前的小结与新对话一起交给存储在 LLM 中的 memory 进行新的小结。 1conversation_with_summary.predict(input = '那防卫过当呢？') 输出结果： 12345678910111213&gt; Entering new ConversationChain chain...Prompt after formatting:你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字The human asked the AI &quot;你好&quot;, and the AI responded &quot;你好，有什么可以帮助你的吗？&quot;. The human then asked about the legal responsibility allocation, and the AI responded that responsibility allocation refers to the attribution of responsibility when an accident or dispute occurs, and can be either legally prescribed or mutually agreed upon.Human: 那防卫过当呢？AI:&gt; Finished chain.' 防卫过当是指当发生事故或纠纷时，受害人采取防卫措施，以保护自身或他人的人身安全或财产安全，而受害人的行为超出了正当防卫的范围，从而导致侵权行为的法律责任。' 使用 ConversationSummaryBufferMemory SummaryMemory 可以支持更长的对话轮数，但记录的内容可能不够精确。当你询问“上一轮我问的问题是什么？”时，它无法给出准确的回答。然而，我们可以将 BufferMemory 和 SummaryMemory 结合起来，使用 ConversationSummaryBufferMemory 解决这个问题。接下来，我们来了解一下 ConversationSummaryBufferMemory 的使用方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from langchain import PromptTemplatefrom langchain.chains import ConversationChainfrom langchain.memory import ConversationSummaryBufferMemoryfrom langchain.llms import OpenAISUMMARIZER_TEMPLATE = &quot;&quot;&quot;请将以下内容逐步概括所提供的对话内容，并将新的概括添加到之前的概括中，形成新的概括。EXAMPLECurrent summary:Human询问AI对人工智能的看法。AI认为人工智能是一种积极的力量。New lines of conversation:Human：为什么你认为人工智能是一种积极的力量？AI：因为人工智能将帮助人类发挥他们的潜能。New summary:Human询问AI对人工智能的看法。AI认为人工智能是一种积极的力量，因为它将帮助人类发挥他们的潜能。END OF EXAMPLECurrent summary:{summary}New lines of conversation:{new_lines}New summary:&quot;&quot;&quot;SUMMARY_PROMPT = PromptTemplate( input_variables=[&quot;summary&quot;, &quot;new_lines&quot;], template=SUMMARIZER_TEMPLATE)memory = ConversationSummaryBufferMemory(llm=OpenAI(), prompt=SUMMARY_PROMPT, max_token_limit=256)CHEF_TEMPLATE = &quot;&quot;&quot;你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字{history}Human: {input}AI:&quot;&quot;&quot;CHEF_PROMPT = PromptTemplate( input_variables=[&quot;history&quot;, &quot;input&quot;], template=CHEF_TEMPLATE)conversation_with_summary = ConversationChain( llm=OpenAI(model_name=&quot;text-davinci-003&quot;, stop=&quot;\\\\n\\\\n&quot;, max_tokens=2048, temperature=0.5), prompt=CHEF_PROMPT, memory=memory, verbose=True)answer = conversation_with_summary.predict(input=&quot;你是谁？&quot;)print(answer) 输出结果： 123456789101112&gt; Entering new ConversationChain chain...Prompt after formatting:你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字Human: 你是谁？AI:&gt; Finished chain. 我是一名律师，可以为您解答法律问题。 代码有点长，为了更好地展示，我将 Langchain 默认的 Memory 小结提示语模板从英文改为了中文。翻译工作是由 ChatGPT 完成的。如果您想了解原始的英文提示语，请查看源代码中的_DEFAULT_SUMMARIZER_TEMPLATE，可以去看一下相应的链接。 我们定义了一个 ConversationSummaryBufferMemory，构造函数中指定了使用的 LLM、提示语以及一个 max_token_limit 参数。max_token_limit 参数告诉我们，当对话长度达到一定程度时，我们应该使用 LLM 将文本内容进行小结。 后面的代码与前面的例子基本相同。 由于我们在代码中开启了 Verbose 模式，因此您可以看到实际 AI 记录的整个对话历史。当我们连续多次向 AI 提问时，您会发现随着对话轮数的增加，Token 数量超过了 max_token_limit。因此，SummaryBufferMemory 就会触发，对前面的对话进行小结，出现一个 System 的信息部分，其中包含聊天历史的小结，而后面完整记录的实际对话轮数就变少了。 我们先问什么是正当防卫，Verbose 的信息里还是显示历史的聊天记录。 12answer = conversation_with_summary.predict(input=&quot;请问什么是正当防卫？&quot;)print(answer) 输出结果： 1234567891011121314&gt; Entering new ConversationChain chain...Prompt after formatting:你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字Human: 你是谁？AI: 我是一名律师，可以为您解答法律问题。Human: 请问什么是正当防卫？AI:&gt; Finished chain. 正当防卫是指，当一个人在受到他人攻击时，他有权采取一定的行动，以保护自己和他人的人身安全和财产安全。 当我们再次讨论防卫过当时，之前的对话已经被总结到了 System 下面。 12answer = conversation_with_summary.predict(input=&quot;那防卫过当呢？&quot;)print(answer) 输出结果： 12345678910111213141516&gt; Entering new ConversationChain chain...Prompt after formatting:你是一个律师，用中文回答一些法律的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字Human: 你是谁？AI: 我是一名律师，可以为您解答法律问题。Human: 请问什么是正当防卫？AI: 正当防卫是指，当一个人在受到他人攻击时，他有权采取一定的行动，以保护自己和他人的人身安全和财产安全。Human: 那防卫过当呢？AI:&gt; Finished chain. 防卫过当是指，当一个人在受到他人攻击时，他采取的行动超出了保护自己和他人的人身安全和财产安全所必需的范围，从而对攻击者造成了不必要的伤害。 在实际使用 SummaryBufferMemory 时，无需将各个提示改为自定义的中文版本。默认的英文提示足以满足需求。因为在详细信息中出现的系统信息不会在实际对话中向用户显示。这些提示只需要AI自己理解即可。当然，您也可以根据实际对话效果来修改所需的提示语。 Pinecone 在网站上提供了数据对比，比较不同类型的 Memory 随着对话轮数的变化占用的 Token 数量。使用 ConversationSummaryBufferMemory 可以精确记录少数对话内容，同时在对话轮数增加时也能记住各种信息。然而，这会导致程序运行变慢，因为需要多次调用 OpenAI 的 API。特别是当字数超过 max_token_limit 时，需要额外调用 API 做小结，同时 Token 数量消耗也很大。因此，并非所有任务都适合使用一次 ChatGPT API 调用来解决，有时应该考虑使用 UtilityChain 和 TransformChain 来解决问题。 记忆功能：让 AI 记住关键信息 除了在对话过程中使用记忆功能，我们还可以通过 Memory 的 save_context 接口将历史聊天记录灌入其中，以便让 AI 基于这些信息继续与用户对话。以下是一组电商客服历史对话记录的示例。 12345678910memory = ConversationSummaryBufferMemory(llm=OpenAI(), prompt=SUMMARY_PROMPT, max_token_limit=40)memory.save_context( {&quot;input&quot;: &quot;你好&quot;}, {&quot;ouput&quot;: &quot;你好，我是客服李四，有什么我可以帮助您的么&quot;} )memory.save_context( {&quot;input&quot;: &quot;我叫茶桁，在你们这里下了一张订单，订单号是 2023Y06M10D，我的邮箱地址是 person@ooxx.me，但是这个订单十几天了还没有收到货&quot;}, {&quot;ouput&quot;: &quot;好的，您稍等，我先为您查询一下您的订单&quot;} )memory.load_memory_variables({}) 输出结果： 1{'history': 'System: \\\\nHuman问候AI，AI回答并表示愿意提供帮助，Human提供了订单相关信息，AI表示会先为Human查询一下订单信息。'} 注：为了演示方便，我设置了一个小的 max_token_limit，但是这个问题在大的 max_token_limit 下，面对上下文比较多的会话一样会有问题。 调用 memory.load_memory_variables 方法，AI 对整段对话做了小结。但小结没有提取我们最关注的信息，如订单号、邮箱。AI 需要这些信息才能查询订单，回答用户的问题。 在 ChatGPT 之前，我们会通过命名实体识别提取关键信息。现在，我们可以让 ChatGPT 帮忙提取信息， Langchain 也内置了 EntityMemory，让 AI 自动提取信息。试试吧。 1234567891011121314from langchain.chains import ConversationChainfrom langchain.memory import ConversationEntityMemoryfrom langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATEentityMemory = ConversationEntityMemory(llm=llm)conversation = ConversationChain( llm=llm, verbose=True, prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE, memory=entityMemory)answer=conversation.predict(input=&quot;我叫茶桁，在你们这里下了一张订单，订单号是 2023Y06M10D，我的邮箱地址是 person@ooxx.me，但是这个订单十几天了还没有收到货&quot;)print(answer) 输出结果： 123456789101112131415161718192021&gt; Entering new ConversationChain chain...Prompt after formatting:You are an assistant to a human, powered by a large language model trained by OpenAI.You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.Context:{'茶桁': '', '2023Y06M10D': '', 'person@ooxx.me': ''}Current conversation:Last line:Human: 我叫茶桁，在你们这里下了一张订单，订单号是 2023Y06M10D，我的邮箱地址是 person@ooxx.me，但是这个订单十几天了还没有收到货You:&gt; Finished chain. 您好，茶桁先生，我们已经收到您的订单，订单号为2023Y06M10D，我们正在尽快处理您的订单，请您耐心等待。如果您有任何疑问，请随时联系我们，我们将竭诚为您服务。 我们使用 ConversationChain，但这次我们指定使用 EntityMemory。在 Verbose 日志中，整个对话的提示语中多了一个叫做 Context 的部分，其中包含了用户提供的姓名、订单号和电子邮件。 然后，我们打印出 memory 中存储的内容。 1print(conversation.memory.entity_store.store) 输出结果： 1{'茶桁': '茶桁是一位客户，订单号为2023Y06M10D，邮箱地址为person@ooxx.me。', '2023Y06M10D': '2023Y06M10D is an order placed by Mr. Chaheng with the email address person@ooxx.me that is currently being processed.', 'person@ooxx.me': 'person@ooxx.me is the email address of Mr. Chaheng, who placed an order with the order number 2023Y06M10D.'} EntityMemory 不仅存储了命名实体名字，还记录了命名实体所关联的上下文。因此，如果我们询问相关的问题，AI 也能够答复。问题 1： 12answer=conversation.predict(input=&quot;我刚才的订单号是多少？&quot;)print(answer) 输出结果： 12345678910111213141516171819202122&gt; Entering new ConversationChain chain...Prompt after formatting:You are an assistant to a human, powered by a large language model trained by OpenAI.You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.Context:{'2023Y06M10D': '2023Y06M10D is an order placed by Mr. Chaheng with the email address person@ooxx.me that is currently being processed.'}Current conversation:Human: 我叫茶桁，在你们这里下了一张订单，订单号是 2023Y06M10D，我的邮箱地址是 person@ooxx.me，但是这个订单十几天了还没有收到货AI: 您好，茶桁先生，我们已经收到您的订单，订单号为2023Y06M10D，我们正在尽快处理您的订单，请您耐心等待。如果您有任何疑问，请随时联系我们，我们将竭诚为您服务。Last line:Human: 我刚才的订单号是多少？You:&gt; Finished chain. 您的订单号是2023Y06M10D。 问题 2： 12answer=conversation.predict(input=&quot;订单2023Y06M10D是谁的订单？&quot;)print(answer) 输出结果： 123456789101112131415161718192021222324&gt; Entering new ConversationChain chain...Prompt after formatting:You are an assistant to a human, powered by a large language model trained by OpenAI.You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.Context:{'2023Y06M10D': &quot;2023Y06M10D is an order placed by Mr. Chaheng with the email address person@ooxx.me that is currently being processed, and is the order number for Mr. Chaheng's order.&quot;}Current conversation:Human: 我叫茶桁，在你们这里下了一张订单，订单号是 2023Y06M10D，我的邮箱地址是 person@ooxx.me，但是这个订单十几天了还没有收到货AI: 您好，茶桁先生，我们已经收到您的订单，订单号为2023Y06M10D，我们正在尽快处理您的订单，请您耐心等待。如果您有任何疑问，请随时联系我们，我们将竭诚为您服务。Human: 我刚才的订单号是多少？AI: 您的订单号是2023Y06M10D。Last line:Human: 订单2023Y06M10D是谁的订单？You:&gt; Finished chain. 订单2023Y06M10D是茶桁先生的订单，他的邮箱地址是person@ooxx.me。 这些是我们在聊天中关注的信息。如果我们要做电商客服，查询订单号、用户姓名时这些信息必不可少。 可以将这些 Memory 存放在内存中，或者进一步存放在 Redis 这样的外部存储中。即使服务进程消失，这些“记忆”也不会丢失。可以查看官方文档。 小结 本节主要讲解了 Langchain 的 Memory 功能，它对整个对话的过程中希望记住的内容做了封装。可以使用 BufferWindowMemory 记住过去几轮对话，使用 SummaryMemory 概括对话的历史并记下来。也可以将两者结合，使用 BufferSummaryMemory 维护一个对整体对话做了小结，同时又记住最近几轮对话的“记忆”。 更具有实用意义的是 EntityMemory。在实际使用 AI 进行对话时，不是让它不分轻重地记住一切内容，而是有一些关键要点需要记住。比如，如果要搭建一个电商客服的聊天机器人，需要记住具体的订单号、用户邮箱等。可以使用 EntityMemory，它会帮助记住整个对话中的“命名实体”，保留在对话中最关心的信息。 在过去的几讲中，从 llama-index 开始，已经学会了将外部的资料库索引起来进行问答，也学会了通过 Langchain 的链式调用，实时获取外部的数据信息，或者运行 Python 程序。本节专门研究了怎样记住对话中我们关心的部分。 将这些能力组合起来，就可以搭建一个完整的，属于自己的聊天机器人。可以根据用户提供的订单号，查询订单物流信息，安抚客户；也可以根据用户想要了解的商品，查询商品库，进行商品导购。 思考题 本节介绍了 EntityMemory 的使用方法，Langchain 还提供了一个KnowledgeGraphMemory，可以试着用一下，看看它能在什么样的场景下帮助解决问题。 推荐阅读 可以查看 Pinecone 提供的 Langchain AI Handbook，测试从 BufferWindowMemory 到 BufferSummaryMemory，对于上下文保持的能力，以及消耗的 Token 数量的统计。教程。","link":"/Langchain%E8%AE%A9AI%E6%8B%A5%E6%9C%89%E8%AE%B0%E5%BF%86%E5%8A%9B/"},{"title":"17. 利用LangChain让AI做决策","text":"Hi，大家好。我是茶桁。 在第 11 讲中，我向您介绍了如何将各种资料内容向量化，借助Llama-index建立索引，对我们自己的文本资料进行问答。在过去的3讲中，我们深入了解了如何使用Langchain。该工具可帮助我们整合AI对语言的理解和组织能力、外部各种资料或者SaaS的API，以及您自己编写的代码。通过整合这些功能，我们可以使用自然语言完成更复杂的任务，而不仅仅是闲聊。 但到目前为止，我们所有基于ChatGPT的应用基本上都是“单项技能”，例如前面关于“藤野先生”的问题或上一讲中查询最新天气或通过Python进行算术运算。这本质上是限制AI只针对我们预先索引或实时搜索的数据进行回答。 给AI加上多项选择能力 要做一个能跑在生产环境上的 AI 聊天机器人，需要的不止一个技能。在电商领域，最起码需要以下三个技能： “导购咨询”：查询商品信息为用户做导购和推荐。 “售中咨询”：查询订单的物流轨迹，对买了东西还没有收到货的用户给出安抚和回复。 “FAQ”：索引电商网站的 FAQ，回复用户有关退货政策、运费、支付方式等问题的答案。 AI 需要自己判断什么时候该用什么样的技能，而不是需要人工介入或写一堆 if…else 的代码。 采用 “分而治之” 的思路，对于每一个单项技能，可以把它们变成一个 LLMChain。对于用户问的问题，可以让 AI 选择使用哪一个 LLMChain 来回答问题。 下面是一段代码，通过提示语让 AI 做一个选择题。 12345678910111213141516171819202122232425import openai, osopenai.api_key = os.environ.get(&quot;OPENAI_API_KEY&quot;)from langchain.prompts import PromptTemplatefrom langchain.llms import OpenAIChatfrom langchain.chains import LLMChainllm = OpenAIChat(max_tokens=2048, temperature=0.5)multiple_choice = &quot;&quot;&quot;请针对 &gt;&gt;&gt; 和 &lt;&lt;&lt; 中间的用户问题，选择一个合适的工具去回答她的问题。只要用A、B、C的选项字母告诉我答案。如果你觉得都不合适，就选D。&gt;&gt;&gt;{question}&lt;&lt;&lt;我们有的工具包括：A. 一个能够查询商品信息，为用户进行商品导购的工具B. 一个能够查询订单信息，获得最新的订单情况的工具C. 一个能够搜索商家的退换货政策、运费、物流时长、支付渠道、覆盖国家的工具D. 都不合适&quot;&quot;&quot;multiple_choice_prompt = PromptTemplate(template=multiple_choice, input_variables=[&quot;question&quot;])choice_chain = LLMChain(llm=llm, prompt=multiple_choice_prompt, output_key=&quot;answer&quot;) 对应的，我们可以试试问不同的问题，看看它能不能选择一个正确的工具。 问题 1： 12question = &quot;我想买一个iPhone，但是不知道哪个款式好，你能帮我推荐一下吗？&quot;print(choice_chain(question)) 输出结果： 12{'question': '我想买一个iPhone，但是不知道哪个款式好，你能帮我推荐一下吗？', 'answer': 'A. 一个能够查询商品信息，为用户进行商品导购的工具'} 问题 2： 12question = &quot;我有一张订单，订单号是 2023Y06M11D，一直没有收到，能麻烦帮我查一下吗？&quot;print(choice_chain(question)) 输出结果： 1{'question': '我有一张订单，订单号是 2023Y06M11D，一直没有收到，能麻烦帮我查一下吗？', 'answer': 'B. 一个能够查询订单信息，获得最新的订单情况的工具'} 问题 3： 123question = &quot;请问你们的货，能送到格尔木吗？大概需要几天？&quot;print(choice_chain(question)) 输出结果： 12{'question': '请问你们的货，能送到格尔木吗？大概需要几天？', 'answer': 'C. 一个能够搜索商家的退换货政策、运费、物流时长、支付渠道、覆盖国家的工具。'} 问题 4： 12question = &quot;今天会不会下雨啊？&quot;print(choice_chain(question)) 输出结果： 1{'question': '今天会不会下雨啊？', 'answer': 'D. 都不合适。这个问题需要使用天气预报工具来回答。'} 我们试验了四个问题，ChatGPT都给出了准确答案。得到答案后，您可以使用TransformChain直接匹配返回结果的前缀，以确定后续调用哪个LLMChain。 Langchain中 的Agent 在真实的业务场景中，你一定会遇到“分治法”的思路。无论是哪行哪业的客服聊天机器人，都会有能够直接通过资料库回答的用户问题，也会有和用户自己或公司产品相关的信息，需要通过检索的方式提供。因此，Langchain将这种“先做一个选择题”的思路推广，并建立了Agent这个抽象概念。 Agent有两个中文翻译，一个是代理人，比如在美国买房或租房，都要通过“房产代理”，也就是Real Estate Agent。另一个意思是“特工”，指的是Agent具有自主行动能力，能够直接使用提供的工具采取行动。它不仅仅是做完选择题就完事了，而是直接拿起选中的工具并进行下一步的行动。Langchain的Agent实际上包含这两个意思，可以说名字取得非常得当。 接下来我们看看如何通过Langchain提供的Agent直接采取行动来解决上面的例子。 12345678910111213141516171819202122232425262728from langchain.agents import initialize_agent, Toolfrom langchain.llms import OpenAIllm = OpenAI(temperature=0)def search_order(input: str) -&gt; str: return &quot;订单状态：已发货；发货日期：2023-01-01；预计送达时间：2023-01-10&quot;def recommend_product(input: str) -&gt; str: return &quot;iPhone 15 Pro&quot;def faq(intput: str) -&gt; str: return &quot;7天无理由退货&quot;tools = [ Tool( name = &quot;Search Order&quot;,func=search_order, description=&quot;useful for when you need to answer questions about customers orders&quot; ), Tool(name=&quot;Recommend Product&quot;, func=recommend_product, description=&quot;useful for when you need to answer questions about product recommendations&quot; ), Tool(name=&quot;FAQ&quot;, func=faq, description=&quot;useful for when you need to answer questions about shopping policies, like return policy, shipping policy, etc.&quot; )]agent = initialize_agent(tools, llm, agent=&quot;zero-shot-react-description&quot;, verbose=True) 这段代码分为三个部分： 定义三个函数： search_order、 recommend_product和 faq，它们的输入都是字符串，输出是我们编写的回答。 创建Tool对象数组，它包含三个Tool对象，每个Tool对象都封装了一个函数，并定义了一个名称和描述。这个描述告诉AI这个Tool的作用，AI会根据问题和描述来选择Tool。 创建一个agent对象，指定使用哪些Tool、LLM对象和agent类型。在这里，我们选择了 zero-shot-react-description类型，这意味着AI将根据自己的推理能力进行决策，并采取行动。React是指根据Tool的描述进行推理和行动。 React并不是来自Facebook的前端框架的名字，而是来自一篇Google Brain的论文。有兴趣的话，可以去阅读一下，了解具体的原理和思路。 img 有了这个agent之后，我们可以尝试重新问一遍之前的三个问题。 问题 1： 123question = &quot;我想买一个iPhone，但是不知道哪个款式好，你能帮我推荐一下吗？&quot;result = agent.run(question)print(result) 输出结果： 12345678910&gt; Entering new AgentExecutor chain... I need to recommend a product.Action: Recommend ProductAction Input: iPhoneObservation: iPhone 15 ProThought: I now know the final answer.Final Answer: 我推荐iPhone 15 Pro。&gt; Finished chain.我推荐iPhone 15 Pro。 问题 2： 123question = &quot;我有一张订单，订单号是 2023Y06M11D，一直没有收到，能麻烦帮我查一下吗？&quot;result = agent.run(question)print(result) 输出结果： 12345678910&gt; Entering new AgentExecutor chain... I need to find out the status of this order.Action: Search OrderAction Input: 2023Y06M11DObservation: 订单状态：已发货；发货日期：2023-01-01；预计送达时间：2023-01-10Thought: I now know the status of the order.Final Answer: 您的订单号为2023Y06M11D，已于2023-01-01发货，预计于2023-01-10送达。&gt; Finished chain.您的订单号为2023Y06M11D，已于2023-01-01发货，预计于2023-01-10送达。 问题 3: 123question = &quot;请问你们的货，能送到格尔木吗？大概需要几天？&quot;result = agent.run(question)print(result) 输出结果： 1234567891011121314&gt; Entering new AgentExecutor chain... I need to know the shipping policyAction: FAQAction Input: Shipping policyObservation: 7天无理由退货Thought: I need to know the shipping timeAction: FAQAction Input: Shipping timeObservation: 7天无理由退货Thought: I now know the final answerFinal Answer: 我们的货物可以送到格尔木，大概需要7天。&gt; Finished chain.我们的货物可以送到格尔木，大概需要7天。 因为在代码里，我们开启了 Agent 的 Verbose 模式，所以在输出结果中，你可以看到 Agent 思考的整个日志。你会发现一些有意思的现象。 首先，Agent 的每一步操作都可以分为 5 个步骤：Action、Action Input、Observation、Thought 和 Final Answer。 Action 指根据用户的输入选择应该采取哪一个 Tool 并执行相应的操作。 Action Input是指从用户的输入中提取相关内容，以便输入到 Tool 中。 Observation 是指观察使用 Tool 后得到的输出结果。 Thought 是指再次查看用户的输入，以判断下一步应该采取什么操作。 Final Answer 是指在观察 Observation 后，根据 Thought 得出的最终输出。 其次，对于“货需要几天送到格尔木”的问题，我们没有按照上述 5 个步骤回答，而是在 Thought 步骤之后重新回到了 Action 步骤。经过三次类似的操作后，我们不得不强行回答该问题。但是，我们的回答并不一定准确，因为我们没有解释是否能将货物送到格尔木。 这整个过程是通过一段 Prompt 实现的，你可以在 Langchain 源码中寻找 mrkl 对应的 Prompt 源代码。 1234567891011121314151617# flake8: noqaPREFIX = &quot;&quot;&quot;Answer the following questions as best you can. You have access to the following tools:&quot;&quot;&quot;FORMAT_INSTRUCTIONS = &quot;&quot;&quot;Use the following format: Question: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of [{tool_names}]Action Input: the input to the actionObservation: the result of the action... (this Thought/Action/Action Input/Observation can repeat N times)Thought: I now know the final answerFinal Answer: the final answer to the original input question&quot;&quot;&quot;SUFFIX = &quot;&quot;&quot;Begin! Question: {input}Thought:{agent_scratchpad}&quot;&quot;&quot; 该工具将一系列工具名称和描述交给OpenAI，根据用户的需求选择相应的工具，并提取与用户相关的信息。本质上，这只是我们让AI做选择题的一种扩展。 限制重试次数 Agent 可以通过反复思考来提高准确性，但有时 AI 处理可能不准确 (大语言模型运行是黑盒)。为了避免不断重试或强行回答，可以在创建 Agent 时设置max_iterations参数来限制尝试次数（比如 2 次）。 123456agent = initialize_agent(tools, llm, agent=&quot;zero-shot-react-description&quot;, max_iterations = 2, verbose=True)question = &quot;请问你们的货，能送到格尔木吗？大概需要几天？&quot;result = agent.run(question)print(&quot;===&quot;)print(result)print(&quot;===&quot;) 输出结果： 123456789101112131415&gt; Entering new AgentExecutor chain... I need to find out the shipping policy and delivery timeAction: FAQAction Input: Shipping policy and delivery timeObservation: 7天无理由退货Thought: I need to find out the delivery timeAction: FAQAction Input: Delivery timeObservation: 7天无理由退货Thought:&gt; Finished chain.===Agent stopped due to iteration limit or time limit.=== 可以看到，这个时候，AI 重试了两次就不再重试。并且，也没有强行给出一个回答，而是告诉你，Agent 因为 max iterations的设置而中止了。这样，你可以把 AI 回答不上来的问题，切换给人工客服回答。 让 Tool 支持问答 这个问题很简单，可以用 AI 回答。目前无法回答的原因是 FQA 工具回答任何问题都是“7 天无理由退货”。正确的方法是使用第 15 讲中介绍的 VectorDBQA 工具，将其封装成一个工具即可。首先，将第 15 讲的代码搬运过来。 12345678910111213141516from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.vectorstores import FAISSfrom langchain.text_splitter import SpacyTextSplitterfrom langchain import OpenAI, VectorDBQAfrom langchain.document_loaders import TextLoaderllm = OpenAI(temperature=0)loader = TextLoader('./data/ecommerce_faq.txt')documents = loader.load()text_splitter = SpacyTextSplitter(chunk_size=256, pipeline=&quot;zh_core_web_sm&quot;)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()docsearch = FAISS.from_documents(texts, embeddings)faq_chain = VectorDBQA.from_chain_type(llm=llm, vectorstore=docsearch, verbose=True) 然后，把这 LLMChain 的 run 方法包装到一个 Tool 里面。 12345678910111213141516171819from langchain.agents import tool@tool(&quot;FAQ&quot;)def faq(intput: str) -&gt; str: &quot;&quot;&quot;&quot;useful for when you need to answer questions about shopping policies, like return policy, shipping policy, etc.&quot;&quot;&quot; return faq_chain.run(intput)tools = [ Tool( name = &quot;Search Order&quot;,func=search_order, description=&quot;useful for when you need to answer questions about customers orders&quot; ), Tool(name=&quot;Recommend Product&quot;, func=recommend_product, description=&quot;useful for when you need to answer questions about product recommendations&quot; ), faq]agent = initialize_agent(tools, llm, agent=&quot;zero-shot-react-description&quot;, verbose=True) 我们对 Tool 写法进行了改进，使得代码更易于维护。通过 Python 的 @tool 装饰器功能，我们将 FAQ 函数直接变成 Tool 对象，从而无需每次创建 Tools 时都指定名称和描述。 然后，我们可以通过 Agent 运行刚才的问题，同样可以得到正确的答案。 123question = &quot;请问你们的货，能送到格尔木吗？大概需要几天？&quot;result = agent.run(question)print(result) 输出结果： 123456789101112131415161718192021222324&gt; Entering new AgentExecutor chain... I need to know the shipping policyAction: FAQAction Input: Shipping policy&gt; Entering new VectorDBQA chain...&gt; Finished chain.Observation: 我们支持全国大部分省份的配送，包括北京、上海、天津、重庆、河北、山西、辽宁、吉林、黑龙江、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆。一般情况下，大部分城市的订单会在2-3个工作日内送达，而偏远地区的订单可能需要5-7个工作日。Thought: I need to know the estimated delivery timeAction: FAQAction Input: Estimated delivery time&gt; Entering new VectorDBQA chain...&gt; Finished chain.Observation: The estimated delivery time may vary depending on the order items, delivery address, and logistics company. Generally, orders in most cities will be delivered within 2-3 working days, while orders in remote areas may take 5-7 working days.Thought: I now know the final answerFinal Answer: 我们支持全国大部分省份的配送，包括格尔木，一般情况下，大部分城市的订单会在2-3个工作日内送达，而偏远地区的订单可能需要5-7个工作日。&gt; Finished chain.我们支持全国大部分省份的配送，包括格尔木，一般情况下，大部分城市的订单会在2-3个工作日内送达，而偏远地区的订单可能需要5-7个工作日。 为了推荐商品，我们可以存储商品信息到VectorStore中，通过先搜索后问答的方式解决。数据由ChatGPT提供，代码与FAQ类似。 重新构建 Agent： 12345678910111213141516171819202122232425262728from langchain.text_splitter import CharacterTextSplitterfrom langchain.document_loaders import CSVLoaderproduct_loader = CSVLoader('./data/ecommerce_products.csv')product_documents = product_loader.load()product_text_splitter = CharacterTextSplitter(chunk_size=1024, separator=&quot;\\n&quot;)product_texts = product_text_splitter.split_documents(product_documents)product_search = FAISS.from_documents(product_texts, OpenAIEmbeddings())product_chain = VectorDBQA.from_chain_type(llm=llm, vectorstore=product_search, verbose=True)@tool(&quot;FAQ&quot;)def faq(intput: str) -&gt; str: &quot;&quot;&quot;&quot;useful for when you need to answer questions about shopping policies, like return policy, shipping policy, etc.&quot;&quot;&quot; return faq_chain.run(intput)@tool(&quot;Recommend Product&quot;)def recommend_product(input: str) -&gt; str: &quot;&quot;&quot;&quot;useful for when you need to search and recommend products and recommend it to the user&quot;&quot;&quot; return product_chain.run(input)tools = [ Tool( name = &quot;Search Order&quot;,func=search_order, description=&quot;useful for when you need to answer questions about customers orders&quot; ), recommend_product, faq]agent = initialize_agent(tools, llm, agent=&quot;zero-shot-react-description&quot;, verbose=True) 询问Agent问题： 123question = &quot;我想买一件衣服，想要在春天去公园穿，但是不知道哪个款式好看，你能帮我推荐一下吗？&quot;answer = agent.run(question)print(answer) 输出结果： 123456789101112131415&gt; Entering new AgentExecutor chain... I need to recommend a product to the user.Action: Recommend ProductAction Input: Clothing for park in spring&gt; Entering new VectorDBQA chain...&gt; Finished chain.Observation: 长款风衣、卫衣连衣裙、长款卫衣，搭配一双白色球鞋、黑色长靴、白色运动鞋和小包包。Thought: I now know the final answer.Final Answer: 我建议你可以选择长款风衣、卫衣连衣裙、长款卫衣，搭配一双白色球鞋、黑色长靴、白色运动鞋和小包包，这样的搭配在春天去公园穿会很漂亮。&gt; Finished chain.我建议你可以选择长款风衣、卫衣连衣裙、长款卫衣，搭配一双白色球鞋、黑色长靴、白色运动鞋和小包包，这样的搭配在春天去公园穿会很漂亮。 优化 Prompt, 避免AI胡乱回答 对于订单查询，使用向量检索就不太合适了。我们可以直接拿订单号去数据库里查找。这不是一个 Python 编程课，不会在这里演示如何使用 Python 编写 SQL。我们可以在对应的函数里造几条数据，根据用户输入的订单号返回不同的订单状态。如果找不到订单，就告诉用户订单不存在。 123456789101112131415161718192021222324252627282930313233import jsonORDER_1 = &quot;20230611ABC&quot;ORDER_2 = &quot;20230611EFG&quot;ORDER_1_DETAIL = { &quot;order_number&quot;: ORDER_1, &quot;status&quot;: &quot;已发货&quot;, &quot;shipping_date&quot; : &quot;2023-01-03&quot;, &quot;estimated_delivered_date&quot;: &quot;2023-01-05&quot;,} ORDER_2_DETAIL = { &quot;order_number&quot;: ORDER_2, &quot;status&quot;: &quot;未发货&quot;, &quot;shipping_date&quot; : None, &quot;estimated_delivered_date&quot;: None,}import re@tool(&quot;Search Order&quot;)def search_order(input:str)-&gt;str: &quot;&quot;&quot;useful for when you need to answer questions about customers orders&quot;&quot;&quot; if input.strip() == ORDER_1: return json.dumps(ORDER_1_DETAIL) elif input.strip() == ORDER_2: return json.dumps(ORDER_2_DETAIL) else: return f&quot;对不起，根据{input}没有找到您的订单&quot;tools = [search_order,recommend_product, faq]agent = initialize_agent(tools, llm=OpenAI(temperature=0), agent=&quot;zero-shot-react-description&quot;, verbose=True) 然后，我们可以让 Agent 帮我们查询订单号。 123question = &quot;我有一张订单，订单号是 2023Y06M11D，一直没有收到，能麻烦帮我查一下吗？&quot;answer = agent.run(question)print(answer) 输出结果： 1234567891011121314151617181920212223&gt; Entering new AgentExecutor chain... I need to find out the status of the orderAction: Search OrderAction Input: 2023Y06M11DObservation: 对不起，根据2023Y06M11D没有找到您的订单Thought: I need to find out more information about the orderAction: Search OrderAction Input: 2023Y06M11DObservation: 对不起，根据2023Y06M11D没有找到您的订单Thought: I need to contact customer service for more informationAction: FAQAction Input: 订单查询&gt; Entering new VectorDBQA chain...&gt; Finished chain.Observation: 登录您的帐户，点击“我的订单”，在此页面上，您可以查看所有订单及其当前状态。Thought: I now know the final answerFinal Answer: 请登录您的帐户，点击“我的订单”，在此页面上，您可以查看所有订单及其当前状态。&gt; Finished chain.请登录您的帐户，点击“我的订单”，在此页面上，您可以查看所有订单及其当前状态。 我们输入了一个不存在的订单号，结果出乎意料。我们本来期望 AI 能够告诉我们订单号找不到，但实际上它重复调用了 OpenAI 的思考策略，并从 FAQ 中拿了一个查询订单的问题来敷衍用户。这并不是我们想要的，也是以前很多“人工智障”类型的智能客服常常会遇到的问题。因此，我们需要解决这个问题。 解决方法很简单，只需要调整 search_order 这个工具的提示语。通过这个提示语，Agent 就会知道，在找不到订单时应该告诉用户找不到订单或请用户再次确认。这样，它就会根据这个答案回复用户。下面是修改运行后的结果。 1234567891011121314151617181920212223242526272829import re@tool(&quot;Search Order&quot;)def search_order(input:str)-&gt;str: &quot;&quot;&quot;一个帮助用户查询最新订单状态的工具，并且能处理以下情况： 1. 在用户没有输入订单号的时候，会询问用户订单号 2. 在用户输入的订单号查询不到的时候，会让用户二次确认订单号是否正确 &quot;&quot;&quot; pattern = r&quot;\\d+[A-Z]+&quot; match = re.search(pattern, input) order_number = input if match: order_number = match.group(0) else: return &quot;请问您的订单号是多少？&quot; if order_number == ORDER_1: return json.dumps(ORDER_1_DETAIL) elif order_number == ORDER_2: return json.dumps(ORDER_2_DETAIL) else: return f&quot;对不起，根据{input}没有找到您的订单&quot;tools = [search_order,recommend_product, faq]agent = initialize_agent(tools, llm=OpenAI(temperature=0), agent=&quot;zero-shot-react-description&quot;, verbose=True)question = &quot;我有一张订单，订单号是 2023Y06M11D，一直没有收到，能麻烦帮我查一下吗？&quot;answer = agent.run(question)print(answer) 输出结果： 1234567891011121314&gt; Entering new AgentExecutor chain... 我需要查询订单状态Action: Search OrderAction Input: 2023Y06M11DObservation: 对不起，根据2023Y06M11D没有找到您的订单Thought: 我需要再次确认订单号是否正确Action: Search OrderAction Input: 2023Y06M11DObservation: 对不起，根据2023Y06M11D没有找到您的订单Thought: 我现在知道最终答案Final Answer: 对不起，根据您提供的订单号2023Y06M11D没有找到您的订单，请您再次确认订单号是否正确。&gt; Finished chain.对不起，根据您提供的订单号2023Y06M11D没有找到您的订单，请您再次确认订单号是否正确。 多轮对话查询订单 优化客服聊天机器人的几个方法： 支持多轮聊天，因为用户可能不会在第一轮提供订单号。 直接使用订单查询工具回答用户问题，无需让客服再次思考。 改进代码即可。 12345678910111213141516171819202122232425262728293031323334353637import refrom langchain.memory import ConversationBufferMemoryfrom langchain.chat_models import ChatOpenAIanswer_order_info = PromptTemplate( template=&quot;请把下面的订单信息回复给用户： \\n\\n {order}?&quot;, input_variables=[&quot;order&quot;])answer_order_llm = LLMChain(llm = ChatOpenAI(temperature=0), prompt=answer_order_info)@tool(&quot;Search Order&quot;, return_direct=True)def search_order(input:str)-&gt;str: &quot;&quot;&quot; useful for when you need to answer questions about customers orders &quot;&quot;&quot; pattern = r&quot;\\d+[A-Z]+&quot; match = re.search(pattern, input) order_number = input if match: order_number = match.group(0) else: return &quot;请问您的订单号是多少？&quot; if order_number == ORDER_1: return answer_order_llm.run(json.dumps(ORDER_1_DETAIL)) elif order_number == ORDER_2: return answer_order_llm.run(json.dumps(ORDER_2_DETAIL)) else: return f&quot;对不起，根据{input}没有找到您的订单&quot;tools = [search_order,recommend_product, faq]chatllm=ChatOpenAI(temperature=0)memory = ConversationBufferMemory(memory_key=&quot;chat_history&quot;, return_messages=True)conversation_agent = initialize_agent(tools, chatllm, agent=&quot;conversational-react-description&quot;, memory=memory, verbose=True) 我们对 Search Order 工具进行了改进。我们设置了一个名为 return_direct=True 的参数，告诉 AI 在获取回复后不要再思考，直接向用户返回答案。这样，当 AI 无法获得订单号时，它不会反复尝试使用 Search Order 工具，而是直接向用户询问订单号。 为了实现这个改进，我们还需要使用 answer_order_llm工具来组织语言文字，而不是直接返回 JSON 字符串。 第二个改进是我们使用的 Agent，我们将其更换为 conversational-react-description，这样我们就支持多轮对话了，并且我们还将对应的 LLM 更改为 ChatOpenAI，这样成本更低。此外，我们还为这个 Agent 设置了记忆。 经过这些改进后，我们发现 AI 现在变得更加智能了。 问题 1： 123question1 = &quot;我有一张订单，一直没有收到，能麻烦帮我查一下吗？&quot;answer1 = conversation_agent.run(question1)print(answer1) 回答： 12345678&gt; Entering new AgentExecutor chain...Thought: Do I need to use a tool? YesAction: Search OrderAction Input: 我有一张订单，一直没有收到，能麻烦帮我查一下吗？Observation: 请问您的订单号是多少？&gt; Finished chain.请问您的订单号是多少？ 问题2: 123question2 = &quot;我的订单号是2023Y06M11D&quot;answer2 = conversation_agent.run(question2)print(answer2) 回答2: 12345678&gt; Entering new AgentExecutor chain...Thought: Do I need to use a tool? YesAction: Search OrderAction Input: 2023Y06M11DObservation: 对不起，根据2023Y06M11D没有找到您的订单&gt; Finished chain.对不起，根据2023Y06M11D没有找到您的订单 问题3: 123question2 = &quot;我还有一个订单号是20230611ABC&quot;answer2 = conversation_agent.run(question2)print(answer2) 回答3: 1234567891011121314151617181920212223&gt; Entering new AgentExecutor chain...Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.&lt;locals&gt;._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7ea75372be3b894ead8b8af989c471ca in your message.).Thought: Do I need to use a tool? YesAction: Search OrderAction Input: 20230611ABCObservation: 尊敬的用户，您的订单信息如下：订单号：20230611ABC订单状态：已发货发货日期：2023年1月3日预计送达日期：2023年1月5日如有任何问题，请随时联系我们。感谢您的购买！&gt; Finished chain.尊敬的用户，您的订单信息如下：订单号：20230611ABC订单状态：已发货发货日期：2023年1月3日预计送达日期：2023年1月5日如有任何问题，请随时联系我们。感谢您的购买！ 问题4: 123question3 = &quot;你们的退货政策是怎么样的？&quot;answer3 = conversation_agent.run(question3)print(answer3) 回答4： 123456789101112131415&gt; Entering new AgentExecutor chain...Thought: Do I need to use a tool? YesAction: FAQAction Input: 退货政策&gt; Entering new VectorDBQA chain...&gt; Finished chain.Observation: 自收到商品之日起7天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。Thought:Do I need to use a tool? NoAI: Our return policy allows for returns within 7 days of receiving the product, as long as the product is unused and in its original packaging. Some special products may not be eligible for returns, so please check the product details page before purchasing.&gt; Finished chain.Our return policy allows for returns within 7 days of receiving the product, as long as the product is unused and in its original packaging. Some special products may not be eligible for returns, so please check the product details page before purchasing. AI 在多轮对话中理解用户意图并提供合适答案，但最后一个问题以英文回答。如何让其用中文回答？这是本课程的思考题。 现在你已拥有基本功能的电商客服聊天机器人，只需在原代码上做些改动并导入自己的数据源，便可用真实用户问题进行测试。 小结 今天我向你介绍了 Langchain 的 Agent 的基本功能。通过“先让 AI 做个选择题”的方式，AI 自动为我们选择合适的工具进行调用。我们可以将不同类型问题的 LLMChain 封装成不同的工具，也可以直接让工具调用内部查询订单状态的功能。我还为你演示了如何将 Agent、Memory、VectorStore 和 LLMChain 组合在一起，创建一个完整的电商聊天机器人。 Langchain 是目前大语言模型领域中最热门的开源项目之一，具有丰富的功能。我介绍的是核心功能，还有其他丰富的工具、不同类型的 VectorStore 和内置的其他 LLMChain，都可以在文档中找到。 思考题 在本讲末尾，我们的示例中，AI 用英语回答了中文 FAQ。请尝试修改现有代码，使 AI 使用中文回答。 上一讲介绍了 EntityMemory，但本讲中未使用它获取和查询订单信息。请查阅 Langchain 文档，思考如何使用 EntityMemory。 欢迎在评论区分享你的想法，也欢迎将本讲分享给你感兴趣的朋友。我们下一讲再见！ 推荐阅读 Langchain 中的 zero-shot-react-description 是从 AI21 Labs 公司的 MRKL Systems 论文中得到的灵感。想了解更多细节，可以阅读一下论文。","link":"/%E5%88%A9%E7%94%A8LangChain%E8%AE%A9AI%E5%81%9A%E5%86%B3%E7%AD%96/"},{"title":"观点：我们无法通过改造自己摆脱气候危机","text":"让我们面对现实吧——气候变化是人类最大的失误。我们已经知道它近一个世纪了。科学是清楚的。然而，我们什么也没做。真是太尴尬了。 现在，全球领导人终于开始忙着收拾残局。但是，尽管我们需要的大多数气候解决方案已经存在，但我们似乎无法按照所需的速度和规模部署它们。 简而言之，世界正在变暖，而我们却无法让它降温。去年，人类向大气中排放的\\(CO_2\\)比以往任何时候都多（呃……WTF？）。 img 可以理解的是，领导者们都害怕极了。这促使他们探索一些非常愚蠢且完全危险的想法。他们最糟糕的脑波之一是地球工程——也就是用地球的气候扮演上帝的角色。（这里使用的“地球工程”并不是指碳去除技术，据我们所知，碳去除技术是相当合法的。） 其中一些建议包括增亮云层、改变海洋的化学成分，或者向大气中发射粒子来使太阳的光线变暗——会出现什么问题呢？ 虽然这些提议听起来像是反乌托邦科幻电影中的内容，但改变地球气候实际上非常容易且成本低廉。 太阳能地球工程是这些“解决方案”中最具争议性的一种。其最受欢迎的衍生产品是平流层气溶胶喷射，涉及将灰尘喷射到大气中，以减少照射到地球表面的阳光量。这项技术的灵感来自于火山云，众所周知，火山云在一次大喷发后可以使整个地球冷却多年。 图片来源：Woovii/Getty Image 云增加了地球表面的反射率。平流层气溶胶喷射旨在通过将灰尘喷洒到高层大气中来复制这种效果，以期冷却气候。 虽然平流层气溶胶注入对于阻止全球变暖可能非常有效，但它可能会打开潘多拉魔盒的问题。根据联合国最近的一份报告，干扰全球自然气候可能会破坏臭氧层，改变全球降雨模式，并导致严重的地缘政治紧张局势。 尽管支持者称太阳能地球工程将是对抗变暖的短期措施，但《科学美国人》最近发表的一项研究表明，如果政客们确实决定向大气中发射尘埃，他们可能会在“几个世纪或更长时间”内危险地依赖它。 为了向大气中排放足够的灰尘来抑制变暖，每年可能需要数万次高空飞行。这一过程的突然停止可能会导致温度飙升，其速度可能快于生命的适应速度，这一概念被称为“终止休克”。 还有一个道德问题，即技术修复可以减轻政客和企业尽快脱碳的压力。 简而言之，太阳能地球工程相当于气候变化创可贴。 尽管存在风险，美国政府去年还是启动了一项为期五年的研究计划，探索将更多阳光反射回太空的方法，为进一步资助这项新兴技术奠定了基础。 比尔·盖茨、乔治·索罗斯和 Facebook 联合创始人达斯汀·莫斯科维茨等亿万富翁都表达了兴趣，而 60 名著名科学家则希望进行小规模太阳能地球工程现场实验。 甚至还有一家名为 Make Sunsets 的初创公司，基于其向大气中释放二氧化硫以遏制变暖的承诺而预售碳信用额。 img 美国初创公司 Make Sunsets 因未经批准进行平流层气溶胶注入测试而被逐出墨西哥。《麻省理工科技评论》的几位研究人员谴责了“Make Sunsets”，称其努力“为时过早”。 预防原则——或者对普通人来说“如果有疑问，就不要考虑”——是健全环境决策的基本前提之一，也是我们今后应该注意的原则。 欧盟本周宣布，呼吁就气候地球工程可能使用带来的风险进行“最高国际级别”会谈，这可能是积极的一步。 欧盟官员在周三的联合通讯中表示：“这些技术给人类和生态系统带来了新的风险，同时也可能加剧国家之间的权力失衡，引发冲突并引发无数道德、法律、治理和政治问题。” “我们不能用造成问题的思维方式来解决问题。 尽管欧盟正在采取预防措施，但它并不完全反对这些技术，而是寻求制定管理这些技术的“规则”。 其他人则采取更强硬的立场。 乌得勒支大学哥白尼可持续发展研究所的弗兰克·比尔曼在去年发表的一份声明中警告说，“人们对太阳能地球工程的风险知之甚少，而且永远无法完全了解。” 比尔曼是一群著名气候科学家的领导者，呼吁就太阳能地球工程达成不使用协议。换句话说，全球范围内禁止其开发。 “太阳能地球工程的研究并不是像其倡导者所说的那样，为预防气候灾难而准备 B 计划。相反，它只会推迟和破坏当前的全球气候政策，”他说。 “此外，现有的国际机构体系无法有效监管这项技术在全球范围内的部署。太阳能地球工程不是解决方案。” 我完全同意，教授。通过扮演上帝的角色来操纵气候，我们不仅面临着使我们的困境恶化的风险，而且还发出了一个危险的信息——人类可以简单地通过设计方法来解决问题，而不是从根本上解决问题（想想广泛的文化、社会和政治）变换）。 正如爱因斯坦的一句名言：“我们不能用创造问题的思维方式来解决问题。”","link":"/%E6%88%91%E4%BB%AC%E6%97%A0%E6%B3%95%E9%80%9A%E8%BF%87%E6%94%B9%E9%80%A0%E8%87%AA%E5%B7%B1%E6%91%86%E8%84%B1%E6%B0%94%E5%80%99%E5%8D%B1%E6%9C%BA/"},{"title":"18. 根据垂直需求微调模型","text":"大家好，我是茶桁。 最近事情太多，这一节课更新的有些晚了。 首先我们先了解一下我们本节课讲要讲一些什么，我们之前介绍过 llama-index 和 LangChain，学习了将大语言模型和自己的知识库组合来解决问题的方法。这个方法中，我们不需要调整我们使用的模型，而是使用嵌入向量索引我们的数据，并在需要时查询索引来解决问题。 然而，我们也可以完全利用自己的数据，创建一个新的模型来解决问题。这种方法是OpenAI提供的微调模型功能。这也是我们要探讨的大语言模型的最后一个主题。 如何微调模型 我们都知道，AI其实是建立在大语言模型之上的，而模型再如何补全，也没有办法全知全能。在很多时候，AI所回答的内容常常错漏百出，甚至于一些垂直领域可能完全词不达意。这些其实都是因为缺少了特定领域的训练数据，而我们要做的，就是要补全这一部分数据进行训练，为我们自己的需求微调出一个擅长本领域的模型。 微调之后，我们就可以直接对模型进行提问，而不再像之前使用的llama-index或者LangChain那样，需要先通过Embedding来查询相关资料，然后再把查找到的资料一并提交给OpenAI来获得所需要的答案。 其实，OpenAI 模型微调的过程并不像我们想象的那么复杂。只需将数据提供给 OpenAI，整个微调过程就在云端的“黑盒子”中完成。需要提供的数据格式为一个文本文件，每一行都是一个 Prompt，以及对应这个 Prompt 的 Completion 接口生成的内容。 比如下面这个示例： 1234{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}... 模型微调是在原始模型的基础上根据输入内容进行的训练，例如第8课中提到的Ada、Babbage、Curie和Davinci模型。每个示例都会导致原始模型的参数发生变化。完成微调后，参数变化将被固定，从而形成一个新模型，只有您可以使用它。 如果您提供了许多医疗行业的文本内容，则微调后的新模型将具有更多的医疗领域知识和对话风格。如果您提供了笑话大全，则微调后的模型将更善于讲笑话。请注意，微调后的模型不仅具有用于微调的数据的相关知识，而且仍保留了原始模型的大部分知识和能力。 亲手实现一个微调模型 朋友的孩子喜欢奥特曼打怪兽的故事，他们希望能用 ChatGPT 制作一个应用程序讲述这个故事。为了增添故事的趣味同时具有教育意义，我们将把历史英雄人物赋予超能力，和奥特曼一起打怪兽。故事数据将使用 ChatGPT 模型生成。 123456789101112131415161718192021222324252627282930313233343536import os,openai,backoffimport pandas as pdopenai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)dynasties= ['唐', '宋', '元', '明', '清', '汉', '魏', '晋', '南北朝']super_powers = ['隐形', '飞行', '读心术', '瞬间移动', '不死之身', '喷火']story_types = ['轻松', '努力', '艰难']@backoff.on_exception(backoff.expo, openai.error.RateLimitError)def gpt35(prompt, max_tokens=2048, temperature=0.5, top_p=1, frequency_penalty=0, presence_penalty=0): response = openai.Completion.create( engine=&quot;text-davinci-003&quot;, prompt=prompt, max_tokens=max_tokens, temperature=temperature, top_p=top_p, frequency_penalty=frequency_penalty, presence_penalty=presence_penalty) return response[&quot;choices&quot;][0][&quot;text&quot;]def prepare_stories(dynasties, super_powers, story_types, output_file=&quot;data/ultraman_stories.csv&quot;): df = pd.DataFrame() repeat = 3 for dynasty in dynasties: for super_power in super_powers: for story_type in story_types: for i in range(repeat): prompt = f&quot;&quot;&quot;请你用中文写一段300字的故事，情节跌宕起伏，讲述一位{dynasty}朝时期的英雄人物，穿越到现代，拥有了{super_power}这样的超能力，通过{story_type}的战斗，帮助奥特曼一起打败了怪兽的故事。&quot;&quot;&quot; story = gpt35(prompt) row = {&quot;dynasty&quot;: dynasty, &quot;super_power&quot;: super_power, &quot;story_type&quot;: story_type, &quot;story&quot;: story} row = pd.DataFrame([row]) df = pd.concat([df, row], axis=0, ignore_index=True) df.to_csv(&quot;data/ultraman_stories.csv&quot;)prepare_stories(dynasties, super_powers, story_types) 我们定义了几种朝代、超能力和故事类型，然后通过三重循环生成一系列故事，作为微调模型的训练数据。数据量不大，使用 CSV 存储。生成数据的速度较慢，也较耗费 Token。 在这个过程中，我耗费了2次 1,222,588 trained tokens，合计共用了7.34刀。 image-20230703234136347 大家可以直接使用我已生成的结果数据，而无需再次运行该代码。（数据放在文末）。 拿到了这些数据，我们就可以来微调模型了。我们之前已经通过 pip 安装了 OpenAI 的包，这里面自带了命令行工具，方便我们把对应的 CSV 格式的数据转换成微调模型所需要的 JSONL 格式的文件。 12df = pd.read_csv(&quot;data/ultraman_stories.csv&quot;)df 输出结果： 12345678df['sub_prompt'] = df['dynasty'] + &quot;,&quot; + df['super_power'] + &quot;,&quot; + df['story_type']prepared_data = df.loc[:,['sub_prompt','story']]prepared_data.rename(columns={'sub_prompt':'prompt', 'story':'completion'}, inplace=True)prepared_data.to_csv('data/prepared_data.csv',index=False)import subprocesssubprocess.run('openai tools fine_tunes.prepare_data --file data/prepared_data.csv --quiet'.split()) 输出结果： 12345678910111213141516171819202122232425Analyzing...- Based on your file extension, your file is formatted as a CSV file- Your file contains 464 prompt-completion pairs- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See &lt;https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset&gt; for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty- Your data does not contain a common ending at the end of your completions. Having a common ending string appended to the end of the completion makes it clearer to the fine-tuned model where the completion should end. See &lt;https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset&gt; for more detail and examples.- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See &lt;https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset&gt; for more detailsBased on the analysis we will perform the following actions:- [Necessary] Your format `CSV` will be converted to `JSONL`- [Recommended] Add a suffix separator ` -&gt;` to all prompts [Y/n]: Y- [Recommended] Add a suffix ending `.` to all completions [Y/n]: Y- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: YYour data will be written to a new JSONL file. Proceed [Y/n]: YWrote modified file to `data/prepared_data_prepared.jsonl`Feel free to take a look!Now use that file when fine-tuning:&gt; openai api fine_tunes.create -t &quot;data/prepared_data_prepared.jsonl&quot;After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` -&gt;` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[&quot;.&quot;]` so that the generated texts ends at the expected place.Once your model starts training, it'll approximately take 8.82 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.CompletedProcess(args=['openai', 'tools', 'fine_tunes.prepare_data', '--file', 'data/prepared_data.csv', '--quiet'], returncode=0) 以上代码主要完成两个操作：数据处理和转换格式。首先，进行了数据处理以准备微调。在微调过程中，Prompt 的格式发生了变化，由原来的一个完整句子，变成了“朝代”+“超能力”+“故事类型”拼接在一起的字符串，并用逗号隔开。其次，将 Prompt 和 Completion 存储成一个 CSV 文件，并将这个 CSV 文件转换成 JSONL 格式。转换过程中，我们使用了 OpenAI 工具，通过 subprocess 调用命令行。生成的 JSONL 文件名为 data/prepared_data_prepared.jsonl。如果打开这个文件，可以看到其内容如下。 image-20230703234041458 转换后的数据文件在 Prompt 的结尾多了一个“-&gt;”符号，Completion 的开头多了两个“”的换行，结尾则多了一个“。”。这是为了在使用模型生成数据时控制结果。使用模型时，Prompt 应以“-&gt;”提示符结束，并将 stop 设置为“。”。这样，模型就可以自然地套用微调模式生成文本。使用 subprocess 调用 OpenAI 的命令行工具即可提交微调指令。 1subprocess.run('openai api fine_tunes.create --training_file data/prepared_data_prepared.jsonl --model curie --suffix &quot;ultraman&quot;'.split()) 输出结果: 1234567891011121314Upload progress: 100%|██████████| 446k/446k [00:00&lt;00:00, 213Mit/s]Uploaded file from data/prepared_data_prepared.jsonl: file-i4cWsomNVO3KzYQczQLhuLdDCreated fine-tune: ft-wXAhgvq3RNZ2J7e7mPxgjt14Streaming events until fine-tuning is complete...(Ctrl-C will interrupt the stream, but not cancel the fine-tune)[2023-07-03 17:53:12] Created fine-tune: ft-wXAhgvq3RNZ2J7e7mPxgjt14Stream interrupted (client disconnected).To resume the stream, run: openai api fine_tunes.follow -i ft-wXAhgvq3RNZ2J7e7mPxgjt14CompletedProcess(args=['openai', 'api', 'fine_tunes.create', '--training_file', 'data/prepared_data_prepared.jsonl', '--model', 'curie', '--suffix', '&quot;ultraman&quot;'], returncode=0) 在这个微调操作中，我们选定了三个参数：训练数据文件、基础模型和生成模型的后缀。我们选择了Curie作为基础模型，并将模型后缀命名为\"ultraman\"，以适应奥特曼的故事情节。 由于训练数据集很小，微调速度很快，只需要几分钟即可完成。接下来，我们可以使用\"fine_tunes.list\"命令找出我们所微调的所有模型。 1subprocess.run('openai api fine_tunes.list'.split()) 输出结果： 1234567891011121314151617181920212223242526272829303132333435{ &quot;data&quot;: [ { &quot;created_at&quot;: 1688376050, &quot;fine_tuned_model&quot;: &quot;curie:ft-bothub-ai:ultraman-2023-07-03-03-03-32&quot;, &quot;hyperparams&quot;: { &quot;batch_size&quot;: 1, &quot;learning_rate_multiplier&quot;: 0.2, &quot;n_epochs&quot;: 4, &quot;prompt_loss_weight&quot;: 0.01 }, &quot;id&quot;: &quot;ft-Kk1utMu4XW8CnFZLKLhN0pQC&quot;, &quot;model&quot;: &quot;curie&quot;, &quot;object&quot;: &quot;fine-tune&quot;, &quot;organization_id&quot;: &quot;org-yG3TUYnCG6uglUpmglSiKXoc&quot;, &quot;result_files&quot;: [], &quot;status&quot;: &quot;pending&quot;, &quot;training_files&quot;: [ { &quot;bytes&quot;: 446199, &quot;created_at&quot;: 1688376050, &quot;filename&quot;: &quot;data/prepared_data_prepared.jsonl&quot;, &quot;id&quot;: &quot;file-raYwPPswPfiKUg4wujBkQxgm&quot;, &quot;object&quot;: &quot;file&quot;, &quot;purpose&quot;: &quot;fine-tune&quot;, &quot;status&quot;: &quot;processed&quot;, &quot;status_details&quot;: null } ], ... } ], &quot;object&quot;: &quot;list&quot;}CompletedProcess(args=['openai', 'api', 'fine_tunes.list'], returncode=0) 在输出的 JSON 中，我们有一个 fine_tuned_model 字段，其中的值为““curie:ft-bothub-ai:ultraman-2023-07-03-03-03-32”，即刚刚 OpenAI 为我们微调的模型。 使用这个模型的方法与使用 text-davinci-003 等模型的方法相同。只需在 API 中替换相应的 model 字段即可。对应的代码如下。 1234567891011121314151617import osimport openaiopenai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)def write_a_story(prompt): response = openai.Completion.create( model=&quot;curie:ft-bothub-ai:ultraman-2023-07-03-03-03-32&quot;, prompt=prompt, temperature=0.7, max_tokens=2000, top_p=1, stop=[&quot;.&quot;]) return response[&quot;choices&quot;][0][&quot;text&quot;]story = write_a_story(&quot;宋,发射激光,艰难 -&gt;\\\\n&quot;)print(story) 输出结果为： 12345宋朝时期，有一位叫林先生的英雄人物，他勇敢而又坚韧，曾经拯救过无数的人民，他的英勇表现让他赢得了众多的尊敬。一天，林先生突然发现自己穿越到了现代，他发现自己拥有了一种神奇的超能力，可以发射激光，他开始研究自己的能力，发现自己可以用激光来攻击敌人。林先生决定把自己的能力用来拯救人类，于是他和奥特曼一起出发，开始与怪兽作战。他们一路走来，林先生用他的激光来打击怪兽，奥特曼则用他的武器来打击怪兽。在一场艰苦的战斗中，林先生和奥特曼终于击败了怪兽，拯救了人类。林先生也因此获得了无数的赞誉，他也成为了一位传奇英雄。林先生的故事被传唱了几百年，他的英勇事迹也成为了一个永恒的传奇，让人们永远不忘。 在调用模型时，我们使用的提示语是“朝代”+“超能力”+“故事类型”，并跟随“-&gt;”。停止符设置为“.”。 因为这是一个微调模型，它不仅拥有我们训练数据提供的知识，还包括基础模型中的各种信息。因此，我们使用的朝代、超能力和故事类型也可以是之前微调数据中未出现过的。例如，在上面的例子中，我们使用的超能力叫做“发射激光”，这不是我们用于微调数据的超能力之一。您可以尝试使用其他朝代或故事类型，查看效果如何。 12story = write_a_story(&quot;秦,龙卷风,辛苦 -&gt;\\\\n&quot;)print(story) 输出的结果： 123456曾经有一位叫苏轼的英雄人物，他曾经英勇地抵抗过许多强大的敌人，拯救了许多被危险封印的百姓。他曾经在一次战争中发挥过自己的作用，赢得了许多胜利，被尊为英雄。然而，苏轼却在一次激烈的战斗中牺牲了，他的灵魂被封印在一个古老的石头里，隔着一层玻璃，一直沉睡了几百年。苏轼的灵魂在穿越时空，来到了现代，他发现自己拥有了一种超能力，这就是龙卷风，他可以使自己的身体具有超强的力量，甚至可以抵抗恶魔的攻击。苏轼在现代的世界里，发现了一种可怕的怪兽，它们正在摧毁着人类的家园，苏轼决定要拯救这个世界，于是他和奥特曼一起出发，开始了一场史诗般的战斗。在苏轼和奥特曼的帮助下，苏轼利用自己的超能力，一次次击退怪兽的攻击，最终他们成功地打败了怪兽，拯救了人类。苏轼的事迹在这里传唱了很久，他成为了一位永恒的英雄，他的故事也被传唱了下来，让人们永远不会忘记他的英勇事迹。 模型成本 还记得咱们之前的课程选择的模型吗？大部分时候咱们选择的都是Davinci, 而这一次我却选择了Curie, 原因就是Curie更便宜一点： image-20230703234003325 具体的可以在这里查看官方数据: https://openai.com/pricing#language-models 使用微调模型的成本比使用 OpenAI 内置的模型高得多。使用基于 Davinci 微调的模型，每使用 1000 个 Token 的成本为 0.12 美元，是使用内置的 text-davinci-003 的 6 倍，是我们最常用的 gpt-3.5-turbo 的 60 倍。因此，对于一般的文本应用来说，成本非常高。即使我们选择基于 Curie 微调，1000 个 Token 的成本也在 0.012 美元，虽然比 text-davinci-003 要便宜，但也是 gpt-3.5-turbo 的 6 倍。 模型微调的效果可以通过 OpenAI 的 \"fine_tunes.results\" 命令来查看。为此，我们需要提供微调任务的 ID，该 ID 可以在 \"fine_tunes.list\" 中列出的 fine_tunes 模型的 ID 参数中找到。 1subprocess.run('openai api fine_tunes.results -i ft-wXAhgvq3RNZ2J7e7mPxgjt14'.split()) 输出结果为: 1234567891011121314151617181920212223242526272829step,elapsed_tokens,elapsed_examples,training_loss,training_sequence_accuracy,training_token_accuracy1,625,1,0.8805545861742778,0.0,0.752,1258,2,0.8059815050491868,0.0,0.77668308702791473,1859,3,0.7964038042175758,0.0,0.78620689655172424,2548,4,0.805052303553852,0.0,0.77744360902255645,3197,5,0.7503930440556053,0.0,0.78086,3846,6,0.7992317049403261,0.0,0.77707006369426767,4775,7,0.6649006477473822,0.0,0.79272326350606398,5432,8,0.6493354803676822,0.0,0.80499219968798759,6265,9,0.6568901059838095,0.0,0.80293757649938810,7122,10,0.6578856167468091,0.0,0.810035842293906811,7827,11,0.5687322367928961,0.0,0.827941176470588212,8404,12,0.6334827334911788,0.0,0.817204301075268913,9061,13,0.5771709139683721,0.0,0.82514,9822,14,0.6079089517825593,0.0,0.810040705563093615,10399,15,0.6481047367374327,0.0,0.815412186379928316,11208,16,0.5528688982071029,0.0,0.835249042145593917,11913,17,0.6525803676480848,0.0,0.809384164222873818,12546,18,0.5230526420679229,0.0,0.836304700162074619,13163,19,0.6065665546680247,0.0,0.823627287853577420,13796,20,0.5983224045073889,0.0,0.819967266775777421,14549,21,0.6440337136896056,0.0,0.826739427012278322,15190,22,0.6029605409912032,0.0,0.811074918566775323,15759,23,0.5089513997451476,0.0,0.83847549909255924,16440,24,0.557213810807506,0.0,0.8265460030165912...1855,1228711,1855,0.2610049068084409,0.0,0.92197659297789341856,1229312,1856,0.21196416716076574,0.0,0.93127147766323031857,1229945,1857,0.14050147435694596,0.0,0.9556650246305419 在该命令的输出结果中，第二列为 elapsed_tokens，表示训练所需的 Token 数量。最后一列的 training_token_accuracy 代表模型成功预测微调数据中下一个 Token 的准确率。在这个例子中，最初的准确率为75％，但随着训练数据的迭代轮数增加，准确率逐渐提高，最终达到95％以上。 优化模型 微调模型成本高，限制使用。但微调模型可根据新数据进一步微调，这对医学和金融等垂直领域有用。这些领域可承受更高成本，不断收集新数据并继续微调，使模型效果更好。 进一步微调操作不复杂，只需准备新数据并以前微调好的模型为基础进行微调即可。 生成额外数据： 123456dynasties= ['秦', '五代', '隋']super_powers = ['龙卷风', '冰冻大海', '流星火雨']story_types = ['轻松', '努力', '艰难', '勇敢', '辛苦']new_stories = &quot;data/ultraman_stories_more.csv&quot;prepare_stories(dynasties, super_powers, story_types, repeat=3, output_file=new_stories) 进行转换： 12345678df = pd.read_csv(new_stories)df['sub_prompt'] = df['dynasty'] + &quot;,&quot; + df['super_power'] + &quot;,&quot; + df['story_type']prepared_data = df.loc[:,['sub_prompt','story']]prepared_data.rename(columns={'sub_prompt':'prompt', 'story':'completion'}, inplace=True)new_stories_prepared = 'data/prepared_data_more.csv'prepared_data.to_csv(new_stories_prepared, index=False)subprocess.run('openai tools fine_tunes.prepare_data --file data/prepared_data_more.csv --quiet'.split()) 微调： 1subprocess.run('openai api fine_tunes.create --training_file data/prepared_data_more_prepared.jsonl --model curie:ft-bothub-ai:ultraman-2023-07-03-03-03-32 --suffix &quot;ultraman&quot; --learning_rate_multiplier 0.2'.split()) 在微调现有模型时，需要修改两个参数： model 参数：将 Curie 更改为我们刚刚微调后的模型 curie:ft-bothub-ai:ultraman-2023-07-03-03-03-32。 learning_rate_multiplier 参数：默认值在 0.05 到 0.2 之间，与样本数量有关。如果您继续微调的样本量比之前要小很多，可以将其调大。 微调完成后，模型名称不变，旧模型已被更新为新的微调模型。现在，我们可以试用这个新模型了。 12fine_tuned = write_a_story(&quot;五代,流星火雨,艰难 -&gt;\\\\n&quot;)print(fine_tuned) 输出的结果： 1234这是一个发生在一个古老的世界，一个叫做“六代”的世界。这个世界有着一种叫做“超能力”的特性，可以让人穿越时空，穿越到现代。一位叫做“英雄”的人物，他来自于六代，但他拥有了一种叫做“流星火雨”的超能力，他可以把自己的身体变成一个火焰，然后穿越时空，来到现代。他来到现代，发现这个世界变得越来越危险，有一种叫做“怪兽”的存在，他们想要毁灭这个世界。英雄决定帮助奥特曼一起打败怪兽，于是他们开始了一场激烈的战斗。英雄凭借着自己的超能力，以及奥特曼的力量，战胜了怪兽，拯救了这个世界。最后，英雄又一次穿越回六代，这次他拥有了一种叫做“流星火雨”的超能力，他可以把自己的身体变成一个火焰，然后穿越时空，拯救又一次六代。 流式生成 我们微调了一个可以讲故事的AI模型，但是它的故事生成体验还不够好。ChatGPT的Web界面是一个一个单词地显示，不像真正的人在讲故事。但是，我们可以通过稍微修改代码来实现这一点，因为OpenAI的Completion接口提供了这种返回结果的模式。 12345678910111213141516def write_a_story_by_stream(prompt): response = openai.Completion.create( model=&quot;curie:ft-bothub-ai:ultraman-2023-07-03-03-03-32&quot;, prompt=prompt, temperature=0.7, max_tokens=2000, stream=True, top_p=1, stop=[&quot;.&quot;]) return responseresponse = write_a_story_by_stream(&quot;汉,冰冻大海,艰难 -&gt;\\\\n&quot;)for event in response: event_text = event['choices'][0]['text'] print(event_text, end = '') 输出的结果为： 123一位叫李英的汉朝时期的英雄人物，穿越到了现代，拥有了一种超能力，可以把自己的身体冰冻到极限，他发现自己可以拥有超越情感的力量，可以把任何人都冻僵，他也发现自己可以控制全局，可以控制时间，可以控制物质，可以控制情景，他发现自己可以控制一切，他变得更加强大。李英发现，地球正面临着一个叫做怪兽的强大敌人的威胁，他决定去帮助奥特曼一起打败怪兽。于是，他和奥特曼一起开始了一系列的战斗，他们一起抵抗着怪兽的攻击，最终，他们成功地消灭了怪兽，拯救了地球。李英受到了所有人的赞赏，他也成为了一个英雄，他的事迹被传颂了几百年，他的故事也被记录在历史书中，他也成为了一个永恒的传奇。 在调用 Completion 接口时，我们使用了 stream=True 参数。这样我们不必获取整个响应，而是获取一系列事件，每个事件都包含一部分新生成的文本。这个代码可以让你体验到 AI 实时讲故事的感觉，就像逐个词语地讲述故事一样。 小结 今天的课程到此结束。我们学到了 OpenAI 大型语言模型中的最后两个功能。第一个功能是模型微调，可以根据自己专有的垂直领域的数据，生成一个专属于我们自己的模型，并在这个模型上不断进行微调和迭代。不过，微调后的模型使用成本比较高。另一个功能是流式地数据生成，可以为用户提供更好的交互体验。现在的大型语言模型几乎是“万能”的，可以用作机器学习的输入数据，也可以直接让它自己决定调用什么 API，以及如何解决用户的问题。希望这门课程对你有所帮助，感谢你的耐心学习！ 思考题 这是大语言模型部分的最后一讲，所以我会留下更多的练习题。 优化生成故事的代码，以减少生成时间和 Token 消耗。 使用流式处理创建一个讲故事的小应用，使用户可以看到故事逐字打印的过程。 探索微调 OpenAI 模型的其他用途，例如分类或命名实体提取，并尝试使用商品页属性信息来编写商品详情描述。可以去看看官网的Specific Guidelines 欢迎在评论区分享您的实现方法和体验，并与感兴趣的朋友分享这一讲。我们下一讲再见。 推荐阅读 在其官方文档中，OpenAI 推荐使用 Weight &amp; Bias 这家公司的产品来跟踪微调后的模型实验、模型与数据集。Weight &amp; Bias 在其文档中也提供了一个对 WIT 数据集进行模型微调的 Notebook，如果你有兴趣，可以去看一下。 数据 数据部分请前往公众号”塌缩的奇点“中查找《18. 根据垂直需求微调模型》付费查看。 扫码_搜索联合传播样式-标准色版.png","link":"/%E6%A0%B9%E6%8D%AE%E5%9E%82%E7%9B%B4%E9%9C%80%E6%B1%82%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"Chrome","slug":"Chrome","link":"/tags/Chrome/"},{"name":"ChatGPT","slug":"ChatGPT","link":"/tags/ChatGPT/"},{"name":"wordpress","slug":"wordpress","link":"/tags/wordpress/"},{"name":"Mac","slug":"Mac","link":"/tags/Mac/"},{"name":"Stable Diffusion","slug":"Stable-Diffusion","link":"/tags/Stable-Diffusion/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"App","slug":"App","link":"/tags/App/"},{"name":"Google","slug":"Google","link":"/tags/Google/"},{"name":"javascript","slug":"javascript","link":"/tags/javascript/"},{"name":"Gmail","slug":"Gmail","link":"/tags/Gmail/"},{"name":"Photoshop","slug":"Photoshop","link":"/tags/Photoshop/"},{"name":"Model","slug":"Model","link":"/tags/Model/"}],"categories":[{"name":"从零开始接触人工智能大模型","slug":"从零开始接触人工智能大模型","link":"/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"}],"pages":[]}