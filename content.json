{"posts":[{"title":"2023年薪酬最高的科技工作中产品经理赫然在列","text":"在最近一段时间内，互联网以及科技公司面临了很大的挑战，工作岗位迅速减少。这让我很大程度上看衰了互联网行业，并且认为目前只是开始，更大的裁员潮还没延伸到中小企业。让我们来看看数据： 就数据（美国的数据）来看，现实非常严峻，2023年迄今为止已有809家科技公司裁员211,400名员工（这个数据一直在变化，最新数据可以看这里），但是各组织仍然在快速招聘人才，以填补云技术、区块链和网络安全等新兴领域的滋味。 看出来了么？并不是互联网和技术行业不香了，而是技术正在进行一轮行业替换，岗位需求发生了大变化。虽然早几年前大家都知道这些岗位是日后的方向，可是这一下子发生了一个断层的变化，导致很大一部分人没有完成转变，科技公司也没进行缓慢的过渡，再加上整个大环境的经济压力，才导致了如今全世界范围内（中国并未逃脱）的大裁员。如果最近一连串的科技裁员让您感到惊慌，请放心，科技作为一个行业仍然健康发展。 而事实上，83% 的美国人力资源专业人士表示，在过去 12 个月里招聘候选人一直很困难，这导致了未来缺乏必要的技术专业人员的更大问题。 麦肯锡最近的一份报告发现，43%的组织目前面临技能缺口，而来自Korn Ferry的单独数据表明，到2030年可能会出现8500 万人短缺，导致同年潜在年收入总计损失 8.5 万亿美元。 好消息是，对于那些寻找新工作的人来说，机会很多。但是坏消息是，对于35岁以上的求职者，这依然是一个难以跨过去的坎，我们不得不承认一点，对于在国内的求职者们，35岁这个节点比在国外更加明显。（为我自己默哀。） 好了，说了这么多，还是要有点干货的。接下来咱们根据国外的相关数据，详细介绍五个薪酬最高的技术职位，数据来源于Payscale，可以在VentureBeat上找到数千个职位，就算国内的求职者们碰不到这些职业，我们依然可以从这些岗位的数据来分析一下目前最吃香的相关职位，为自己的转型做个有力的参考（以下内容都是基于美国当前数据所做的分析）。 1. 云计算解决方案架构师 预计2023年云计算应用将超过6000 亿美元，并将推动人工智能和Web3等新兴技术。 平均工资： 132,700 美元 如果您有云计算方面的经验，德勤正在招聘一名云解决方案架构师，负责核心业务运营（CBO）组合的工作，以帮助C-suite和项目负责人通过新兴和颠覆性技术改造他们的组织并加速任务执行。 此外，SAIC 正在聘请一名专门从事系统工程的云解决方案架构师，以协助确定技术解决方案，解决技术差距，如在其国家情报社区（NIC）业务部门、美国政府任务和信息技术部门内的蜂窝和云服务。 2. 产品经理（软件） 产品经理负责根据数据制定策略，其角色不断发展，因此是任何销售产品或服务的组织不可或缺的一部分。 由于我本身就是一名数据产品经理，这里我不得不发表一些感慨。不过我们还是得认清一个事实，产品经理的门槛在不断变高，不要认为PRD和原型就可以胜任了，我们从数据中可以看到，数据产品、策略产品以及安全相关的产品经理更容易赢得心仪的工作。 基于此，我觉得我还是的多写点数据产品经理的相关文章了。 平均工资： $102,866 如果您正在该领域寻找职位，西门子正在寻找一名高级产品经理来领导网络安全产品的产品策略的开发和执行，与保护和自动化产品/解决方案的产品管理人员密切合作，以确保无缝集成网络安全功能。 与此同时，苹果公司正在招聘一名新产品技术项目经理。在此职位中，您将需要建立矩阵管理并监督材料预测、规划、分析和报告、物流准备、预算、采购和配置管理活动。 3. 网络安全工程师 由于数据泄露和网络威胁仍然是一个大问题——网络犯罪预计每年增长 15%，到2025 年将达到每年 10.5 万亿美元——网络安全领域迫切需要拥有保护企业及其资产的技能和经验的专业人员来自恶意软件攻击。 平均工资： $99,887 国土安全部特别投资于网络安全，因此政府和军事承包商 Booz Allen Hamilton 正在美国各地招聘各种网络安全工程师职位，包括华盛顿、圣安东尼奥和埃尔塞贡多。 在这些职位上，您将需要提供国家和国际层面的网络安全解决方案。 4. 软件工程师 美国劳工统计局预测，从 2021 年到 2031 年，软件开发人员、质量保证分析师和测试人员的就业人数预计将增长 25%，新增 411,400 个就业岗位。 平均工资： $90,777 对于那些拥有丰富经验的人，诺斯罗普·格鲁曼公司正在招聘一名软件工程师/首席软件工程师，作为其企业范围数字化转型的一部分。在此职位上，您将支持工程应用和产品的生成，例如实验室电子战 (EW) 以及靶场训练和模拟系统。 经验丰富的 Aces Incorporated 也正在招聘一名软件工程师来应对美国政府最困难的挑战。 对于金融服务行业的职位，摩根大通银行正在招聘一名全栈首席软件工程师，以安全、稳定和可扩展的方式增强、构建和交付值得信赖的市场领先技术产品。 5. 区块链工程师 虽然大多数人认为区块链的唯一功能是加密货币，但该技术目前已应用于医疗保健、房地产、抵押贷款处理和游戏等各个领域，并且是一个正在增长的领域。 平均工资： 90,000美元 cyberThink Inc 正在寻找一名区块链工程师，带领技术开发人员和云工程师团队建立 AWS 区块链集成环境并管理数据接口和链码开发。 在西海岸，Third Republic 正在与一个开发团队合作，该团队为财富 500 强公司提供创新的软件开发解决方案，以聘请区块链开发人员。","link":"/2023%E5%B9%B4%E8%96%AA%E9%85%AC%E6%9C%80%E9%AB%98%E7%9A%84%E7%A7%91%E6%8A%80%E5%B7%A5%E4%BD%9C%E4%B8%AD%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E8%B5%AB%E7%84%B6%E5%9C%A8%E5%88%97/"},{"title":"纪念","text":"纪念 1234567891011121314&lt;math xmlns='http://www.w3.org/1998/Math/MathML'&gt; &lt;mn&gt;25&lt;/mn&gt; &lt;mo&gt; &amp;#x00D7;&lt;!--multiplication sign --&gt;&lt;/mo&gt; &lt;msup&gt; &lt;mrow&gt; &lt;mn&gt;2&lt;/mn&gt; &lt;/mrow&gt; &lt;mrow&gt; &lt;mn&gt;6&lt;/mn&gt; &lt;/mrow&gt; &lt;/msup&gt; &lt;mo&gt;=&lt;/mo&gt; &lt;mo&gt;?&lt;/mo&gt;&lt;/math&gt; 以上，为了纪念！","link":"/25X2de6cifang/"},{"title":"《AI秘籍》预告","text":"Hi, 大家好，我是茶桁，这里为自己做个广告，目前打算开始写一整个系列《AI秘籍》。 这一段时间内我写过一个系列《零基础学习大语言模型》（目前还没写完）。 说实话，这个系列其实原出处并不是我，严谨的说来，有涉嫌擦边“洗稿”的嫌疑，所以最后放弃了收费的想法，仅仅对一些模型，资源以及计算结果进行了补偿性的收费。不过在写这个系列的同时，我开始有了自己的一些想法，打算真正写一个属于自己的系列文章。 因为我的个人博客并没有付费阅读的功能，所以还在看平台。第一选择自然是我的微信订阅号，有想过发到少数派里，但是并不清楚少数派对我文章的审核会是什么结果，能成为专栏发出来不太有信心。 说说这个专栏本身，参照我几个自媒体平台的数据来看，Python的基础知识还是更受欢迎一点，我想大概也是更多基础不太好的小伙伴希望能入行吧。所以这次我准备从基础开始写起，总的来说分成以下几个大的篇章： 第一卷：Python 第二卷：核心知识 第三卷：核心能力培养 第四卷：NLP 第五卷：BI 第六卷：CV 第七卷：扩展 - 数学 第八卷：扩展 - 英语 大致的算了一下，可能这个系列会耗费比较长的时间和精力，也希望小伙伴们能多多支持。 在这里，放上我的公众号订阅方式： 最后，找到合适的订阅平台之后，本篇内容应该会有更新。","link":"/AI%20Cheats%20Trailer/"},{"title":"卡尔曼滤波器的非数学介绍","text":"如果你想查看的话，本文的代码可以在我的Github上查看。 卡尔曼滤波器非常巧妙。如果你从未听说过卡尔曼滤波器，那么一种非常直观（也可以说是还原）的思考方式就是将其视为一个漏斗，在这里你可以从多个嘈杂的信息源中获取信息，并将其浓缩为一个更精确的统计数据。 如果这一切听起来含糊不清，请不要担心。稍后，我们将把这句话剥离成一个更容易理解的例子，希望能进一步加深我们的直觉。要研究和推理卡尔曼滤波器，没有比数学更好的工具了。但同样，卡尔曼滤波器的基础数学具有挑战性，包含线性代数、概率论和微积分等内容。因此，并非所有人都能轻松掌握。这篇文章的目的就是希望为你提供一个易于理解的直观印象，或许能促使你自己深入研究这个问题。现在，让我们开始吧，同时牢记这一点：\"以下内容仅提供直觉，可能并不完整\"。 让我们先问一句：\"为什么卡尔曼滤波器是必要的？对于这个问题，一个简单而又故意模糊的答案是：现实生活并不完美。请看这个激励性的例子：想象一艘船在一个维度上行驶，从港口出发（x=0）并行驶一段距离。这艘船的发动机被设定为为船提供一个恒定的速度，例如 10 米/秒。 我们首先要问的问题是，在离开港口 2 秒钟后，船到底在哪里？很自然，你会说船离港口的距离是 210=20m，因为毕竟距离 = 速度 时间。在理想世界里，这的确是正确的，根本不需要卡尔曼滤波器。但在现实世界中，情况绝非如此简单。首先，可能没有足够的发动机能够产生足够的力，使每个时间点的速度始终保持在 10 米/秒。当然，你可能会在某些时候获得 10.00001 的速度，或在其他时候获得 9.99999 m/s 或介于两者之间的某个数字，但正如所说，99.99% 的完美终究还是不完美。其次，即使你说你确实拥有这样一个完美的发动机，但当你施加一个精确测量的力时，你也不可能获得预期的完美速度。波浪运动可能会让你的船稍微慢一点，或者风可能会让它加速，或者谁也不知道什么东西会以什么方式对它产生影响。 因此，仅仅通过测量你想要的位置，你永远无法确定你的船在哪里。 那么，我们是否注定永远无法真正知道自己的位置呢？不尽然！这就是传感器的用武之地。想象一下，你，水手，随身携带一个全球定位系统。这样，GPS 就能精确地告诉您在任何给定时刻的位置！事实上，你现在甚至不需要船的速度，因为无论船如何行驶，你的全球定位系统都能准确地告诉你所在的位置。问题解决了吗？就像我说的，不完全是。在现实中，传感器经常会出现错误，而且不可靠。也就是说，它们确实能告诉你你在哪里，但测量结果可能并不精确。因此，您的 GPS 可能会告诉您，3 秒钟后您距离港口 29.998 米或 30.002 米，甚至是距离港口 100 米，但这种可能性极低。此外，您也无法确保传感器永远不会出现故障。以 GPS 传感器为例。一旦你发现自己身处没有卫星覆盖的地区，它就会失灵。事实上，如果有一个传感器能保证永远不会离线，并能以任意的精确度测量出你想知道的信息，那就根本不需要卡尔曼滤波器了。 有了这些，我们现在就可以回答为什么需要卡尔曼滤波器了。而答案与我们之前已经确定的并没有什么不同。卡尔曼滤波器是一个漏斗，它能接收两个或更多不完美、不可靠的信息源，并对你想知道的信息做出更准确的估计。在这个例子中，卡尔曼滤波器会把你在任何时间的速度估计值和 GPS 估计值（如果有的话）作为输入，然后给出比这两个信息加起来更准确的估计值！事实上，如果你有更多的信息来源，比如雷达或声纳，甚至是你目前在水中看到的鱼的种类，理论上你可以将这些测量结果结合起来，从而对你的位置做出更准确的估计。 因此，现在的问题是，如果不使用这样的数学知识（摘自维基百科），我们如何理解卡尔曼滤波器的作用和原理？ 首先，我们假设船上没有一名乘客，而是有一千名乘客，每个人都有自己的 GPS 设备。现在，每位乘客都可以通过以下方式进行基于速度的估算，从而估算出自己的位置（进而估算出船的位置）： 123456from random import gaussdef new_position(last): velocity = 10 wind = gauss(0, 2) wave = gauss(0, 0.1) return last + velocity + wind + wave 注：有关高斯函数的可选但更完整的解释，请参阅下面的附录。目前，只需说明它会产生一个随机数（正数/负数），其顺序由第二个参数指示即可。 从本质上讲，这 1000 名乘客中的每一个人都是这样做的：取上一次已知的位置（在现在之前的时间），加上速度，并且知道风和水波会轻微地改变航向，再加上一些随机的估计波动。现在，如果这些乘客真的有估算风速和水速的好方法，他们就会使用它。但因为他们没有，所以只能用随机数来估计影响。实际上，现实生活中也是如此。我们不可能测量所有的东西，所以我们只能用一些简单的方法来估计它们，就像我们上面用平均值（0）和偏差参数（0.1 和 2）所做的那样。 现在我们进入卡尔曼滤波法的第二阶段，即测量。在这一阶段，所有乘客都知道，由于风噪和水噪的影响，他们对自己的状态（所处位置）只有不完全的了解，因此，他们会利用自己的传感器来改善自己的状态： 1234567891011def sensor(t): if t == 3: # oops, passing through a thunderstorm. GPS fluctuating! sensor_noise = gauss(5, 10) elif t == 6: # uh-oh, satellite unavailable! sensor_noise = gauss(-5, 10) else: sensor_noise = gauss(0, 1) return true_position[t] + sensor_noise 请记住，传感器是一种不精确的设备，也就是说，它们返回的统计数据大多是正确的，在本例中就是变量 true_position，但它们本身也有噪声，我们再次使用高斯函数随机生成的数字来模拟这种噪声。此外，我们在这里还模拟了传感器的不稳定性，即在某些情况下（t=3 和 t=6），由于某些因素传感器基本上是不可用的，而这些因素并非完全不可想象。因此，每位乘客在使用传感器时，实际上都会得到不同的测量结果。 想象一下，这艘船现在离开港口，每秒行驶这些距离： 12true_position = [0, 9, 19.2, 28, 38.1, 48.5, 57.2, 66.2, 77.5, 85, 95.2] 也就是说，船从港口出发（x=0），第一秒行驶 9 米，第二秒行驶 10.2 米，最后到达 19.2 米，以此类推。现在，乘客们的任务是利用他们所掌握的嘈杂且不可靠的测量数据，尽可能准确地预测出每一秒的不同位置。 因此，在时间 t = 1 时，乘客可以通过上述函数得到这些读数： 1# 如果 t=0 时的新位置为 0，则 t=1 时的新位置为 0new_position(0) =&gt; 9.37 (error -0.37)# t = 1s 时的传感器读数sensor(1) =&gt; 8.98 (error +0.02) 所有乘客都是如此。现在的问题是，真相到底是什么？是我们的牛顿物理知识更可靠，还是 GPS 传感器更可靠？在这种特殊情况下，由于我们已经知道船的真实位置距离 true_position 变量 9 米，答案可能是显而易见的，但情况并非总是如此。在这种情况下，为了将这两个独立的统计数据结合起来，我们实际上采用了一种非常简单的方法：取两者的平均值！在上面的例子中，我们可以得出以下结果 1combined =&gt; (9.37+8.98)/2 =&gt; 9.17 (error -0.17) 请注意，在这个例子中，综合统计量的误差比单独的速度估计值要小，但比传感器估计值要差。但问题是，我们实际上可以做得比取平均值更好。考虑一下这样的情况：你知道你的传感器实际上是最先进的，而且非常可靠。这实际上意味着你应该更倾向于传感器的数据，而不是速度更新的数据。实际上，您可以通过使用加权平均值来做到这一点。请看这段代码 123def combine(A, B, trustA, trustB): total_trust = trustA + trustB return (A * trustA + B * trustB) / total_trust 这就综合了 A 和 B 来源的两个数字，但也考虑到了您对这些来源的信任程度。因此，如果您将其称为 12combine(9.37, 8.98, 10, 1) =&gt; 9.33 (error -0.33)combine(9.37, 8.98, 1, 10) =&gt; 9.01 (error -0.01) 在第一次调用中，您对源 A（速度）的信任度远高于源 B（传感器），即 10 比 1，因此得到的答案更倾向于源 A，即更接近 9.37。这种基于信任的加权平均法是卡尔曼滤波器的核心，也是它的数据组合能力所在。 但现在，我们遇到了一个新问题。哪个来源更可信，或者如何计算可信度？是应该优先考虑速度呢？还是应该优先考虑 GPS 测量结果？决定这一点的是偏差或方差指标。想想看，什么更值得信赖？是波动剧烈的信息源还是没有波动的信息源？试想一下，你收听 10 个气象广播电台，其中 4 个告诉你会下雨，6 个告诉你会是晴天。现在想象一下，你登录 10 个天气网站，其中 9 个告诉你会下雨，1 个告诉你会是晴天。哪个消息来源更可靠？你倾向于相信大多数气象广播电台告诉你的（晴天）？还是你倾向于相信天气网站告诉你的（下雨）？理性的做法是更倾向于网站的结论，因为许多网站的结论都是一致的，即它们的方差较小，而气象广播电台，至少在这个例子中，它们的结论似乎波动很大，所以也许不应该太相信。 这样，完整的更新步骤就变成了这样： 1234567891011121314151617181920212223242526272829303132from statistics import variance# Find updated positions per passenger at t secondsdef update(t, last): velocity_updates = [] sensor_updates = [] for p in range(1000): # for each passenger # new velocity update based on last known position # for the passenger velocity_updates.append(new_position(last[p])) sensor_updates.append(sensor(t)) # Calculate trust metrics for velocity and sensor measurements # Remember that as fluctuation increases, trust decreases # And vice-versa fluctuation_velocity = variance(velocity_updates) fluctuation_sensor = variance(sensor_updates) # calculate trust trust_velocity = 1/fluctuation_velocity trust_sensor = 1/fluctuation_sensor # combine these together for each passenger combined = [] for p in range(1000): combined.append(combine(A = velocity_updates[p], B = sensor_updates[p], trustA = trust_velocity, trustB = trust_sensor)) # Sensor updates &amp; velocity updates returned for plotting purposes return sensor_updates, velocity_updates, combined 注：有关方差函数的更多信息，请参阅附录。现在，只需将其视为数字列表波动的度量。 这段代码相对简单。对于每位乘客，它都会进行基于速度的噪声测量和基于传感器的噪声测量。根据所有乘客的这些测量结果，计算出每个测量结果的信任度指标，作为方差的倒数（因为方差增加，信任度降低），然后调用包含相关信任度参数的组合方法。值得注意的是，这里的每位乘客都在为自己进行位置更新。在这些单个更新结束后，可以根据所有乘客位置的平均值推断出船只本身的实际位置。 我们使用以下代码来连接上述整个代码。 123456789101112131415161718192021# We'll do a final plot using this listplot_data = []def update_plot(t, sensor, velocity, combined_position): # add true position at this time plot_data.append({'passenger': 'true', 'type': 'true', 'time': t, 'position': true_position[t]}) # for each passengers for p in range(1000): plot_data.append({'passenger': p, 'type': 'sensor', 'time': t, 'position': sensor[p]}) plot_data.append({'passenger': p, 'type': 'velocity', 'time': t, 'position': velocity[p]}) plot_data.append({'passenger': p, 'type': 'combined', 'time': t, 'position': combined_position[p]})update_plot(0, [0]*1000, [0]*1000, [0]*1000)estimated_positions = [0]*1000 # all estimates start from 0for t in range(1, 10): # ten seconds _sensor, _velocity, estimated_positions = update(t, estimated_positions) update_plot(t, _sensor, _velocity, estimated_positions) update_plot 函数只是做一些基本的簿记工作，以存储用于绘图的瞬时统计数据。这里的主要迭代只是最底层的 for 循环，它使用乘客当前的最佳估计值，在任何给定时间持续更新位置估计值。除此以外，代码基本上不言自明。 使用 seaborn 库绘制的结果如下： 由于目前的比例尺，这有点难以解析。让我们放大这两个区域，特别是 t=0.75 至 t=1，即传感器正常工作时，以及 t=2 至 t=4 出现故障时。 注：包络线指的是不确定性。线中的包络线越宽，我们对数字的不确定性就越大。 在第一种情况下，正如您所看到的，所有 1000 名乘客的综合位置估计值比单独的速度估计值要好（绿色），虽然在第一种情况下，我们的估计值确实比我们的传感器读数要差，但在第二种情况下，我们的估计值实际上比单独的故障传感器读数要好得多！这是因为卡尔曼滤波器会自动调整不可预见的波动造成的剧烈变化，并始终为我们提供合理可靠的指标。如下图所示，一旦我们的传感器恢复正常（t=4 到 t=5），卡尔曼滤波器就会再次偏向于传感器（由于传感器读数和真实值重叠太多，所以有点难看）。 我相信你至少对卡尔曼滤波器的工作原理有了一些直观的了解。卡尔曼滤波器的实际理论基础同样引人入胜，如果你的工作需要，我鼓励你继续深入研究。与此同时，我希望这篇文章能证明，代码作为一种形式语言，能在多大程度上帮助人们对那些乍看之下令人生畏的概念产生直觉。我也希望能够通过简单的代码，向大家传授一些我认为很有吸引力的话题的更多见解。 高斯函数 这里唯一需要知道的特殊函数是正态分布函数，即 gauss(0, 0.1) 和 gauss(0,2)。简单地说，它给你一个随机数，这个数通常在 0 附近（技术上正确的说法是以 0 为中心），而得到离 0 更远的数的几率由第二个参数控制，即 2 和 0.1。 因此，如果调用 gauss(0,0.1)，得到 0.06、-0.07、-0.06、0.02、-0.23、-0.06、0.09 等数字的可能性较大，顺序不分先后。 而如果调用 gauss(0,2)，则更有可能得到 1.05、1.03、-1.06、0.32、1.29、-0.40、-1.72 等数字，同样不分先后。 直观地说，第二个参数也叫标准偏差，控制着测量值的波动程度。在上面的代码中，这意味着您通常会认为风的偏差过大（大风天？请注意下面直方图中偏差=2 和偏差=0.1 所产生的数字的频率（特别注意 x 轴）。虽然数字的范围有很大不同，但这两个直方图的形状看起来差不多。这种钟形分布被称为高斯分布、正态分布或钟形曲线分布，在自然界中经常出现。 方差 方差是衡量一致性的标准。也就是说，如果一致性好，方差就小，反之亦然。在上图中，由于 x 轴实际上是自动调整的，所以你无法完全看到方差。如果我们在相同的坐标轴限制内绘制上图中的直方图，就会得到如下结果： 注意到第一张图片有多宽了吗？这是因为其中的数字变化很大。也就是说，你会发现其中有很多 -2、2、0 和一些 4、-4。但在第二幅图中，你会发现很多 0、0.1、-0.1 等，但你会发现-2、2 等的数量会少很多。正确地说，第一个分布的方差（准确地说是 4）大于第二个分布的方差（0.01）。有关方差的更多信息，请上网查阅。","link":"/2023_8_3_Kalman/"},{"title":"新专辑《AI秘籍》，你所感兴趣的一切","text":"Hi，大家好。我时茶桁。 最近，我花了几天时间仔细思考了一下即将要开始写的专栏《AI秘籍》，再根据自己的能力大概规划了一下。目前大致已经理出了一些相关信息可以分享给大家。 专栏形式 本次专栏应该会以文章的形式先和大家见面，后续还会根据能力以原本的文章为准录制视频版本。 专栏平台 就如前一篇文章公布的内容一致，会优先发表在我的公众号上，当然目前我还在努力寻找其他的专栏平台。 我的预想是尽量能够让大家一篇一篇的购买，不需要必须购买全部专辑，这样朋友们可以根据自己具体需求来进行购买。 而目前我的百家号收费专栏也在申请之中，不知道会不会顺利申请下来。有新的平台入驻之后，我会进行通知的。 专栏内容 在规划专栏的时候，大部分时间基本都放在了规划内容上。包括目录的编排，内容取舍等等。 目前规划中的专栏打算从基础开始，到Python开发，再到一些应用基础，比如AI数学，AI英语等。而由于这些内容都会是针对AI学习的，所以并不会是那种很全面的学习资料。 比如说Python，我们不会讲的很系统，重点会放在数据结构以及数据分析和开发方面。数学等基础当然也会是一致的。 当基础篇学完之后，接下来就是重点了，会根据三个不同的AI方向来进行讲解，分别包括：BI、NLP以及CV。 基本目录如下： 第一篇： Python基础（AI方向） 第二篇：核心基础能力 第三篇：核心知识增强 第四篇：BI 基础 第五篇：CV 基础 第六篇：NLP 基础 第七篇：BI 进阶 第八篇：CV 进阶 第九篇：NLP 进阶 拓展篇1: 数学 拓展篇2: 英语 详细目录如下（进阶部分目录未完全展开）： 一些说明和后续 本专栏暂时价格上还未进行调研，反正第一篇Python部分应该会是全免费发放。毕竟Python课程网上太多了，而且同质化严重，收费感觉没太大必要。所以，咱们写的时候再慢慢想。小伙伴们也可以留言来说说大家期望是一个什么价格，我根据大家留言再结合自己的实际情况最后定价。 另外，除了Python部分之外，数学和英语部分也是免费的。说实话，我数学和英语并不是很好，这两部分我仅仅给大家一个总结和方向，反正也是独立内容，均可以去网上找相关替代的。 专栏在完成之后，会更新一些关于算法和数据库的内容，然后会考虑整篇投放到其他平台去进行完整售卖。 结尾 好了，结束语也无需说太多。让我们一起期待吧，希望在我的课程完成的那一天，各位小伙伴们能完全入门并掌握人工智能。 本次课程的所有代码都会上传到Github上，地址：","link":"/AI-cheats-information/"},{"title":"12 AI帮你写个小插件，轻松处理Excel文件","text":"开头我就要跟各位先说对不起，本来我是很想为大家把这部分实现并完成的。但是很抱歉，因为我用的Mac，而这部分代码实现起来的时候一直会如下报错： 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/AI-create-a-excel-plugin/"},{"title":"Analysis data and research report collection","text":"The purpose is to facilitate finding specific locations when doing data analysis by yourself 1. 国内咨询机构网站数据报告列表 艾瑞研究-艾瑞网 互联网行业报告 艾瑞APP指数 移动App TOP 1000 月度活跃和日活跃 艾瑞PC指数 PC TOP 1000 月度活跃和日活跃 199IT互联网数据中心 中国互联网络信息中心 数据新知 - 易观 【友盟+】数据报告 http://www.dcci.com.cn/report/index.html 北京赛诺市场研究有限责任公司 赛诺数据，智能机出货量的专业统计 数据报告-移动观象台-TalkingData Talkingdata报告 艾媒网-全球移动互联网行业数据发布平台/iiMedia Research出品 DataEye大数据移动营销综合服务商-数据报告 手游方向 ASO100 - 专业App Store数据平台丨ASO优化专家平台丨iOS榜单排行榜查询工具 电影电视行业免费报告列表页 研究娱乐行业 旅游数据报告-旅游圈旅游行业报告 小程序报告-阿拉丁统计 爱应用：一个应用所有历史版本的产品分析截图记录 Appannie，国外下载应用 卡思数据-短视频网红分析数据分析 国金证券研究所 国家宏观经济研究数据和报告 中金研报 抖音快手的热门视频和kol的 各大媒体的每天的热门排行榜 短视频行业的数据 招商证券的电商类报告电商类的行业观察，企业研究，品牌深度报告 短视频和图文内容类的行业报告-新榜 http://www.100ec.cn/zt/wmds/ 涵盖跨境电商所有报告，行业数据和研报 镝数聚-权威数据 海量聚合 提取了报告中的数据，颗粒度比较细 东方财富研报首页 东方财富研报 2. 国家机构公开数据 中国信通院-研究成果-权威发布-权威数据 中国城市轨道交通协会城市地铁线路的流量数据 国家的便民服务查询（包括5A景区list，小微企业名录，法人信用查询，出租车信息查询） 国家宏观经济数据（GDP,CPI，总人口，社会消费品零售总额，粮食产品，PPI，各地区行政规划，各地财政收支等等，分月度季度和年度）部分数据如下 国家统计局（数据多到瞠目结舌，包括年度，季度，国家，国际，年鉴，介乎涵盖所有数据指标和历史）部分数据举例 世界银行的公开数据库（有健康，农业，公共部部门，人口分布，外债，性别，教育，环境，气候变化，能源，贫困等各种公开数据） 世界数据图册（世界和地区统计资料，各国数据，地图，排名）包含的全球的国家公开的数据 国家机关部委的公开数据（包括国家发展和改革委员会，教育部，民政部，司法部，财政部，工业和信息化不，交通运输部，文化和旅游部等） 各城市开放数据（包括浙江数据开放网，青岛数据开放网，贵阳数据开放平台，成都数据公开平台，合肥数据开放平台，河南开放数据平台等） 宏观经济查询数据（包括高校财经信息库，人民网经济数据库，香港统计处，联合国统计司，世界经合组织，欧盟统计局，国际货币基金组织等） 房价数据（包括中国房价指数，房价走势，台湾房价行情，北京房价查询，深圳楼盘成交查询等，上海地铁房价，贝壳指数等） 汽车数据（包括中国汽车工业协会数据，百度汽车网，易车汽车指数，汽车渠道直通车，中国汽车流通协会数据中心，德国汽车工业协会等） 权威发布 | 中华全国商业信息中心 3. 国内互联网公司数据报告网站列表 讲座PPT-腾讯大讲堂 Tencent 腾讯-业绩报告 腾讯大数据-腾讯云数据分析出来的行业报告 百度开放服务平台-百度云数据分析出来的行业报告 百度数据研究中心 提供行业研究报告、行业分析报告-百度数据中心报告 首页-阿里研究院-阿里行业研究报告 企鹅智酷_腾讯网-腾讯出品行业报告 腾讯CDC -腾讯交互设计报告 百度用户体验中心-百度UED用户研究报告 网易用户体验设计中心-网易UED用户研究报告 网络视频数据报告-优酷指数行业报告 PP指数_PPTV聚力-PPTV指数行业报告 360研究报告_360安全中心-360应用商店等产品出品报告 4. 国外咨询机构网站数据报告列表 国外咨询机构较多，数据详实，无论是海外出海产品，海外报告中多有亚洲和中国的重点研究，相关报告和趋势分析都可以选看 Flurry-国外app行业报告 App Annie Blog-app指数报告 https://www.appannie.com/insights/ (Appnnie的行业包括，包括app 分发行业的分发量和收入） BI Intelligence-business insider的报告 Today's Articles on Digital Marketing and Media-emarker的报告 http://www.newzoo.com/category/press-releases/-newzoo侧重于手游行业报告 Gartner Press Release Archives-gartner侧重于硬件的出货量，包括智能机和PC等 IDC - Search Results-IDC的硬件出货量全球报告 Yozzo Telecom News J.P. Morgan Home-摩根投行报告 德勤中国 | 审计, 企业管理咨询, 财务咨询, 风险管理, 税务服务及行业洞察 Precisely Everywhere-comscore的互联网行业报告 Ericsson - A world of communication（Global移动行业报告） GamesIndustry.biz（Global游戏行业报告） http://adfonic.com/（Global广告行业报告） Canalys | Insight. Innovation. Impact.（Global智能机报告） Mobile, Online &amp; Digital Market Research, Data &amp; Consultancy（通信无线报告） Home | GfK Global（终端比较专业的报告） Kantar Worldpanel（主要统计Android和ios的市场份额） PwC publications（皮尤的所有用户，市场研究报告） Fiksu | Data-fueled mobile marketing（统计app用户获取成本和应用商店下载频次的监测） https://www.weforum.org/reports（世界经济论坛的报告，揭示国内外发展的大趋势） Insights - Jampp （Jampp是国外的app 的粘性和转化漏洞的网站，在insights里还有行业的app的retention等benchmark的数据，有些类似flurry的行业数据） 罗兰贝格行业评论 战略和行业评论和报告 普华永道:blog 各个行业的主要发现和行业报告 Website Traffic &amp; Mobile App Analytics （similar web 以色列的网站分析工具，可以分析任何网站，包括用户，来源，终端，分布等等，数据非常棒） CADAS（全球航空公司研究报告）：非常支持和专业 印度互联网年报 - 竺帆 | 助力中企扬帆天竺 （印度出海报告，非常详细） GSMA： 全球移动互联网经济分析报告，全部免费下载报告和数据，从2015年到现在 商业价值研究院 -IBM（行业观点报告比较多） Home - McKinsey Greater China麦肯锡 **BCG - 波士顿咨询公司波士顿 企业管理咨询公司罗兰贝格 Accenture - China埃森哲** 5. 各大公司不定期发布的报告，比如（细分方向的时候用）** 高德地图：2015年度中国主要城市交通分析报告 微信城市服务发布《2015微信政务民生白皮书》 【报告】淘宝发布 2015 中国消费趋势数据，2015 年我们为什么买单？ 互联网增长的第一本数据分析手册-Growing IO的公开手册 移动游戏运营数据分析指标白皮书（一）-Talkingdata 运营指标分析白皮书 多多大学 （多多大学也分享了很多的拼多多运营数据还提供课程，可以看） 6. 找行业内的人事打听内部一手资料 关注一些专门打听行业内部人事的信息来源 这里先推荐一家公共号：晚点LatePost（微信搜索公共号可以 关注） 这家主要是会 看一些行业内部和重要的消息 在行上约人。在行 App 如果想知道一些企业的信息，可以在在行上找到一些行家，曾经一手经营或者运营过祥光额项目和参与过竞品和行业公司的操盘，可以约出来，从信息和方法论角度获得资讯 7. 企业信息报告** 新三板在线 - 中国最大的新三板生态平台（各行各业的新三板上市公司财务数据，高管数据等） 企查查|企业查询（查询企业的产品，品牌和法人信息） 企业注册信息查询（天眼查，同企查查） SEC.gov | Home（美国上市公司年度财务报告） 巨潮资讯网—（中国上市公司季度年度财务报告） Baidu | Investors（各大上市公司季度财报，IR.XX公司.com，比如百度这个） 天眼查（可以查到各个企业的详细信息，还可以查到员工个数） 8. 爬虫网站或者APP的数据 最近研究发现，还有一个好的行业信息获取来源，就是通过站内或者App内的爬虫抓取，这个渠道获取的数据，通常可以帮助你了解行业和竞品的站内使用情况，用户喜欢的内容，用户的分布，用户的行为和喜好等等。 爬虫，简单来说是通过程序来获取网页的信息，整理成数据库，从而进行数据挖掘的得到分析结论的过程。比如你可以爬虫购物的页面，知道哪个商品的销量好，比如你可以爬虫小红书的页面，知道哪些kol收到欢迎，你还可以爬取他们的分类，知道美妆和购物的kol表现好，并且有多少个这样的kol。如果你没有对方的数据库权限（当然你肯定没有），那么从外部爬虫是最好的了解他们业务数据的方式。 通常的搜索方式 是：你要了解的网站/App+爬虫，在搜索平台比如百度搜索 这里举例一些程序员垂直的网站， CSDN网站：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索 简书 - 创作你的创作：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索， V2EX：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索 掘金：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索 9 . 业内微信群 现在发现很多好的内部报告和难以获得报告，是通过加入一些干货群，内部群来获得的。 比如做直播电商的人自己比较关注一些直播和电商带货的详细的数据和报告趋势，大家会自己组建一些干货群，只要是市面上有的报告，自己内部发现的都会往里面扔。 这个是淘宝直播的负责人赵圆圆离开淘宝后创业，同时聚集的几个群，里面关于直播的干货非常多。 其他的关于投资的，趋势，创业的类似群也很多，获取报告也很一手，大家也可以自己开发下这样的群组织。 10. 搜索引擎 搜索引擎还是可以搜到很多你个性化想要找的报告和趋势。以前我没觉得搜索引擎很很难，后来发现也需要学习和熟练使用，才能让其为自己所用。 如何使用搜索引擎 11. 各大公司的财报 通常对于上市公司来说，财报信息包含的内容是最全面的，关于用户，商业，渠道，增长，业务策略等等。所以如果想了解一个公司，如果是上市公司最好第一手先看财报后者SEC（上市报告）。 很多同学问我财报哪里找，不知道怎么看。其实每个公司都有自己的IR（投资者页面），在上面有财报的完整的pdf下载。另外，也推荐大家听听每期的企业conference call（回答财报问题），可以听下CEO对财报的解读。 这里我列举几个大公司财报的网站 wind：金融数据库，包含财报和行业信息（wind的账号可以到闲鱼租） 百度财报 PDF：Baidu | Investors | Home 百度财报解读podcast：音频可以在线听 阿里财报pdf：阿里巴巴集团 腾讯财报pdf：Tencent 腾讯 - 投资者关系 搜狐财报pdf：http://investors.sohu.com/ 拼多多pdf：Investor Relations | Pinduoduo Inc. 拼多多财报解读：音频可以在线听。 如果大家有自己想要了解的公司，在百度or google搜索：公司名字+IR ，可以 定位到他们公司的财报网站页面。在页面上找到conference call或者webcast，可以 找到他们的财报解读音频。 12. 投资机构的统计网站（创业方向选择，投融资选择的时候用） IT桔子 | IT互联网公司产品数据库及商业信息服务（IT桔子，中国创业公司投融资数据和报告） 研究院_ChinaVenture投资中国网-（投中的每个季度的行业融资报告，不定期有专项分析报告） CB Insights - Blog （CBI insights的一系列产品，包括公司的估值，独角兽公司列表等） The Downround Tracker（公司估值下降的趋势） The Complete List of Unicorn Companies（独角兽公司列表） IPO Center: IPO Market, IPO News, IPO Calendars, IPO Pricings, IPO Voting（IPO相关新闻和趋势报告） PrivCo | Private Company Financial Intelligence（美国金融数据公司，主要关注未上市公司的所有投融资资料，目前涵盖的公司包括全世界，当然也包括中国公司） 券商行业研究报告 （国内券商的行业报告，策略报告，可以筛选行业，筛选报告类型） https://pitchbook.com/news/reports（PitchBook的PE,VC，M&amp;A行业报告） 研究院_ChinaVenture投资中国网 （IPO 投融资行业报告） Dow Jones VentureSource 2Q’16 U.S. Venture Capital Report（道琼斯旗下机构Dow Jones LP Source行业投资报告） NVCA Venture Investment（美国国家风险投资协会，每个季度和年度都会出投融资行业报告） PWC-MoneyTree Home（PWC的money tree report是每个季度美国的风险投资行业报告） https://home.kpmg.com/xx/en/home/insights.html （KPMG毕马威的insights报告，一般是每个季度的创投趋势，比较细致的分析） Mattermark - Discover, Enrich, &amp; Analyze Companies（创业公司投资并购信息一站式搜索） M&amp;A, Private Equity &amp; Venture Capital Database（创业公司投资并购信息一站式搜索） DataFox | Prospect Sales Leads with Company Signals（创业公司投资并购信息一站式搜索） CrunchBase accelerates innovation by bringing together data on companies and the people behind them.（创业公司数据库） Venture Intelligence PE/VC database Stock Market Insights | Seeking Alpha （二级市场金融分析网站） Tencent Holdings Ltd -- Trefis（各个公司的revenue model的预测和key driver的趋势，这个网站简直不能再棒） 13. 本地数据库 这个世界有很多有用的信息，搜索引擎只解决了其中20%，其他80%的信息再各个角落，包括微信群，包括口口，甚至包括直播里都有，但是都不在搜索引擎。 就搜索引擎而言，现在很多人只是使用了其中的5%还不到。搜索引擎的技巧可以提升，但是其他80%的信息获取渠道更为隐蔽和无法公开获得的。 我加了很多 群，里面都是这些报告和信息和各行各业的各种信息，这些是搜索引擎提供不了的 这些冰山下的信息才决定了信息的获取的不同和优质与否。 除了上述渠道外，能找到靠谱渠道，找到合适的报告随时存储起来，等用的时候随手打开用是最好的。分享一个我最近看的收藏的精品的报告收藏夹，也希望对你们有用（随时更新） 共享下我看过的精品报告的收藏夹list 14. 怎么提炼自己获取信息的层次和获取信息的价值 找到行业信息报告知识获取信息只是其中一个层次 ，获取信息是否更有价值更直接可用，在于基本功行业信息报告的甄别和获取，积累和提炼，这是非常重要的。 但是 越往上走，越是接近信息更有价值，更新鲜，更真实有效，更直接，有大量的渠道 可以 获得更多 的信息，这些不仅是通过 行业报告获取的，还有包括自己可以控制的方法，包括爬虫，数据挖掘，信息技术 等，还包括人脉，圈子，内幕的等渠道。大家感兴趣可以到这个答案看下详情，我对每个层级的方法的解读。 哪些渠道可以获取一般人不知道的知识和信息 15. 其他（不定期更新） IBM商业价值研究院","link":"/Analysis_data_and_research_report_collection/"},{"title":"Apple M1的AI环境搭建","text":"本文环境搭建的基础是Python3.9， 因为M1为ARM架构，所以放弃了Anaconda，使用Miniforge3。包括Tensorflow, xgboost, Lightgbm, Numpy, Pandas, Matplotlib, NGBoost等。当然，因为是Python3.9， 所以有些库实在是无法使用。 Homebrew 作为Mac的包管理神器，首先当然要先从Homebrew开始。Homebrew已经支持了ARM架构，可以直接进行安装，当然，如果你电脑里以前存在X86的brew支持，请先卸载干净。 Homebrew 卸载 1/bin/bash -c &quot;$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/uninstall.sh)&quot; Install ARM Homebrew 1/bin/bash -c &quot;$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/install.sh)&quot; 执行完毕后，Homebrew安装在/opt/homebrew路径下；在安装完毕后，命令行后会提示执行命令设置环境变量，当然，以防万一，这里也提供一下： 12echo 'eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;' &gt;&gt; ~/.zprofileeval &quot;$(/opt/homebrew/bin/brew shellenv)&quot; 如果是bash shell， 则： 12echo 'eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;' &gt;&gt; ~/.bash_profileeval &quot;$(/opt/homebrew/bin/brew shellenv)&quot; 记得source ~/.zprofile Install X86 Homebrew 1arch -x86_64 /bin/bash -c &quot;$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/install.sh)&quot; X86版本的安装执行完成后命令行未提示添加环境变量。 alias 支持多版本 在终端执行： 12alias brew='arch -arm64 /opt/homebrew/bin/brew'alias ibrew='arch -x86_64 /usr/local/bin/brew' 这里可以看出两者路径区别 设置镜像 中科大源 1234567891011# brewgit -C &quot;$(brew --repo)&quot; remote set-url origin https://mirrors.ustc.edu.cn/brew.git# coregit -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git# caskgit -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.gitbrew update 清华大学源 1234567891011# brewgit -C &quot;$(brew --repo)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git# coregit -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git# caskgit -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-cask.gitbrew update 恢复默认源 1234567891011# brewgit -C &quot;$(brew --repo)&quot; remote set-url origin https://github.com/Homebrew/brew.git# coregit -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://github.com/Homebrew/homebrew-core.git# caskgit -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://github.com/Homebrew/homebrew-cask.gitbrew update 更多源 Homebrew 其他相关 设置bottles镜像 1234567# bottles for zshecho 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles/bottles' &gt;&gt; ~/.zprofilesource ~/.zprofile# bottles bashecho 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles/bottles' &gt;&gt; ~/.bash_profilesource ~/.bash_profile cask 目前cask是从GitHub上读取软件源，而GitHub Api对访问有限制，如果使用比较频繁的话，可以申请Api Token，然后在环境变量中配置到HOMEBREW_GITHUB_API_TOKEN。 12echo 'export HOMEBREW_GITHUB_API_TOKEN=yourtoken' &gt;&gt; ~/.zprofilesource ~/.zprofile Install Miniforge3 首先需要下载安装包： Download 请下载arm64(Apple Silicon)版本： 下载完成后进入到文件目录，比如我是在~/Download/内，执行： 1bash Miniforge3-MacOSX-arm64.sh 整个执行过程会有大概三次填写yes并回车确定，最后一次会询问你是否执行conda init， 会自动在~/.zshrc内添加环境变量，如果未执行的，可以将下面语句加入文件末尾： 12345678910111213141516# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;# !! Contents within this block are managed by 'conda init' !!__conda_setup=&quot;$('/Users/xx/miniforge3/bin/conda' 'shell.zsh' 'hook' 2&gt; /dev/null)&quot;if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot;else if [ -f &quot;/Users/xx/miniforge3/etc/profile.d/conda.sh&quot; ]; then . &quot;/Users/xx/miniforge3/etc/profile.d/conda.sh&quot; else export PATH=&quot;/Users/xx/miniforge3/bin:$PATH&quot; fifiunset __conda_setupconda activate tf# &lt;&lt;&lt; conda initialize &lt;&lt;&lt; 记得自行更改/Users/xx/内的用户名 等待Miniforge3安装完成，然后设置一个专供学习Tensorflow的虚拟环境 12conda create -n tf python=3.9.5conda activate tf # 将这句添加到~/.zshrc内，每次打开shell都会自动执行 关于conda切换环境的命令，建议自行Google学习一下，很有用。 Install Tensorflow 目前网上流传的Tensorflow安装基本是两个版本，一个是安装一大堆的支持和依赖，一个是使用yml文件提前准备好环境库一键完成环境创建，比如environment.yml： 1conda env create --file=environment.yml --name=tf 其实这一步也很简单，Apple为了大力推广自家的ARM，已经为大家做好了这部分准备，我们只需要安装就行了。 假设目前在tf环境内 123conda install -c apple tensorflow-depspython -m pip install tensorflow-macospython -m pip install tensorflow-metal 好了，结束！ 可以自行利用下面一段代码测试下： 123456789101112from tensorflow.keras import layersfrom tensorflow.keras import modelsmodel = models.Sequential()model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.Flatten())model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(10, activation='softmax'))model.summary() 123456789101112131415from tensorflow.keras.datasets import mnistfrom tensorflow.keras.utils import to_categorical(train_images, train_labels), (test_images, test_labels) = mnist.load_data()train_images = train_images.reshape((60000, 28, 28, 1))train_images = train_images.astype('float32') / 255test_images = test_images.reshape((10000, 28, 28, 1))test_images = test_images.astype('float32') / 255train_labels = to_categorical(train_labels)test_labels = to_categorical(test_labels)model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])model.fit(train_images, train_labels, epochs=5, batch_size=64)test_loss, test_acc = model.evaluate(test_images, test_labels)test_acc 执行过程中可以在资源管理器中看到GPU的占用： 其他 Lightgbm 1conda install Loghtgbm 一句代码解决，完全靠谱。 xgboost xgboost稍微有点麻烦，我测试了最稳妥的安装方式，还是自行编译，那这个时候我们就需要用到brew安装并设置编译环境了： 注意，我用的都是brew而非ibrew, 目前都是在ARM环境下完成操作。 123brew install gccbrew install cmakebrew install libomp 然后下载源码并执行 1234567git clone git@github.com:dmlc/xgboost.gitcd xgboostmkdir buildcd buildCC=gcc-11 CXX=g++-11 cmake ..cd ../python-package/Users/xx/miniforge3/envs/tf/bin/python setup.py install 然后就OK了。 至于其他的，Numpy在安装Tensorflow的时候就自动作为依赖安装了，Pandas, Matplotlib, NGBoost等，执行下方： 123conda install -c conda-forge pandasconda install -c conda-forge matplotlibconda install -c conda-forge ngboost 如果conda内实在没有的，再试试pip安装，再不行，就只能自行下载源码编译了。 目前在当前环境下解决不了的几个库： CatBoost Cairo -&gt; Pycairo GraphEmbedding CV2 igraph 在整个过程中，可能会遇到各种各样的问题，大家要习惯于使用Google和查阅官方文档； 参考 Tensoflow-macos Run xgboost on Mac and Regression data Accelerating TensorFlow Performance on Mac The new Apple M1 chips have accelerated TensorFlow support M1 Mac Mini Scores Higher Than My RTX 2080Ti in TensorFlow Speed Test. GPU acceleration for Apple's M1 chip? M1芯片Mac上Homebrew安装教程 Mac mini M1使用简单体验(编程、游戏、深度学习) Installing TensorFlow 2.4 on MacOS 11.0 without CUDA for both Intel and M1 based Macs 在 M1 芯片 Mac 上使用 Homebrew Apple M1终于让MacBook变的可以炼丹了 Install XGBoost and LightGBM on Apple M1 Macs Installing TensorFlow on the M1 Mac Getting Started with tensorflow-metal PluggableDevice M1芯片mac安装xgboost和lightgbm AI - Apple Silicon Mac M1 机器学习环境 (TensorFlow, JupyterLab, VSCode) M1芯片安装tensorflow 使用MacBook pro M1搭建基于ML Compute加速的TensorFlow深度学习环境 你的Mac有了专用版TensorFlow，GPU可用于训练，速度最高提升7倍 在M1的Mac上安装Tensorflow（避坑版） 在M1芯片Mac上搭建原生适配Python环境 Conda-forge Miniforge M1 mac安装PyTorch的完整步骤指南 macOS M1(AppleSilicon) 安装TensorFlow环境 傻瓜版M1配置Tensorflow-超简单近乎一键完成 environment.yml opencv-python MAC安装Opencv以及Dlib碰到的一些问题 Jupiter Widgets 启动SparkContext报错 MacBook Pro 2020 M1芯片安装xgboost xgboost Homebrew / Linuxbrew 镜像使用帮助 镜像助手 Apple Silicon Mac 安装xgboost M1芯片mac安装xgboost和lightgbm mac安装lightgbm踩坑心得，亲测有效！ MAC 上 使用lightgbm遇到image not found 解决办法总结 杂记-Macbook Pro M1芯片能玩深度学习吗？","link":"/Apple_M1_AI_environment_construction/"},{"title":"07 AI帮你做总结","text":"Hi， 我是茶桁。 在上一节中，我们介绍了如何使用最新的ChatGPT API，注册HuggingFace账户，并将我们的聊天机器人部署出去。在这个过程中，我们学习了实际的应用开发过程，使你对聊天机器人的开发有了充足的体验。在这一讲中，我们将探讨OpenAI的各种接口提供的能力，以更深入地了解这些接口。我们将分别介绍如何利用嵌入（Embedding）进行文本聚类，并使用提示语（Prompt）对文本进行总结。此外，我们还将介绍其他的接口能力，如语言模型和自然语言生成，以帮助您更好地理解和利用OpenAI的各种功能。 基于 Embedding 向量进行文本聚类文本聚类简介 文本聚类是一种自动将未标注的文本根据相似度分成几类的方法。使用 GPT 系列模型进行文本聚类非常简单，我们可以将文本转换为向量，然后使用一些简单的聚类算法，比如最简单的 K-Means 算法。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/AI-can-help-you-summarize-your-content/"},{"title":"人工神经网络","text":"神经元、如何构建网络、高级神经网络 人工神经网络是人工智能（AI）中重要而有趣的一部分。 什么是神经网络？ 神经网络是对大脑神经过程的复制。 它是在计算机上构建的大脑模拟。 神经网络，无论是生物的还是人工的，都由大量的简单单元和神经元组成，它们相互接收和传输信号。 它由细胞体和连接神经元的导线组成。 用生物学语言来说 ： 为神经元提供输入的电线称为树突。 在某些情况下，神经元会向另一个神经元发送信号，这些向外发送信号的导线被称为轴突。 轴突可能与一个或多个树突相连，这种交叉点称为突触。 这个过程会随着我们的成长而不断调整，这种 \"调整 \"被称为记忆或学习。 什么是深度学习？ 深度学习是一种机器学习技术，由相互连接的多层简单处理单元组成。 它的灵感来源于大脑处理视觉信息的方式。 为什么要开发人工神经网络？ 开发人工神经网络（ANN）的原因之一是为了帮助神经科学（研究大脑和神经系统）。 人们相信，通过绘制人脑图谱，我们可以了解意识和智力背后的秘密。 我们已经能够识别异常功能，并帮助大脑避免异常功能。 例如--解决老年痴呆症、因受伤造成的损伤和发育障碍。 开发人工神经网络（ANN）的另一个原因是为了建立更好的人工智能和机器学习技术。 因为，大脑是一个极其复杂的信息处理系统。 人工神经网络的特点 ： ANN 由许多神经元组成，可以同时处理信息。这意味着，我们可以同时处理大量数据，从而提高了效率。 神经元可以同时存储（就像内存一样）和处理信息，因此从存储器中检索数据不会有任何延迟，因而速度很快。 是的，ANN 可以快速处理信息，但很难应用于 \"传统计算机\"（单机处理），因为它一次只能完成一项任务。这就是 GPU 的用武之地。 听说过 GPU 吗？ GPU 是图形处理单元（Graphical Processing Unit）的缩写，它可以进行并行处理，而不是像传统计算机那样进行单一处理。因此，神经网络可以快速完成工作或处理信息。 构建神经网络 ： 权重在神经网络中扮演着重要角色，它通过控制每个输入，让网络从这些数据中学习，从而做出准确的预测。 但是，什么是权重？ 权重就像可调节的旋钮，决定着每个输入对最终输出的影响程度。 例如，为了找到适当的平衡（数据），我们要给输入值加上适当的权重。 因此，通过将每个输入值（神经元）与权重相乘并相加，我们就能实现 \"线性组合\"。 线性组合公式 ： 考虑到我们有 4 个输入，因此我们也需要 4 个权重来平衡它，而且还会有一个额外的固定值，称为截距（偏差）。 截距值是一个偏置值，用作基准值，这样即使输入值为零，网络也能做出预测。 计算公式： 线性组合 = [截距 + Weight 1 × Input 1 + Weight 2 × Input 2 + Weight 3 × Input 3 + Weight 4 × Input 4］ 问：请考虑以下表达式 10.0 + 5.4 × 8 + (-10.2) × 5 + (-0.1) × 22 + 101.4 × (-5) + 0.0 × 2 + 12.0 × (-3) = -543.0 (i) 表达式中的截距（偏差）项是什么？ \"10.0 \"是截距（偏差）数，因为它没有乘以任何变量。 (ii) 这里的输入是什么？ \"8,5,22,-5,2,-3 \"是输入值，因为它是乘法中的第二个数字。 实现线性组合后，再将其传递给 \"激活函数\"。 激活功能： 激活函数就像一个开关，它决定信号是否应该通过，使神经网络能够有效地学习和解决不同的问题。 为图像识别、自然语言处理等进行预测。 激活函数示例 ： 激活函数的一些示例如下 识别函数：什么也不做，只输出线性组合（与线性回归相同，不提供任何新信息，因此很少使用） 步进功能：如果线性组合值大于 0，则通过信号，否则什么也不做 Sigmoid 函数：阶跃函数的 \"软 \"版本 通过线性组合激活函数实现的神经元输出用于预测或决策。 \"感知器--人工神经网络（ANN）之母\"： Perceptron 是一种使用阶跃激活函数的简单神经元模型。 它被用作二元分类任务中的简单分类器。 由于它是第一个正式的神经网络模型，因此被称为 \"ANN 之母\"。 \"现在，让我们回到神经网络的构建上来。 网络架构由层级组成，例如 ： 输入层：由作为输入数据的神经元组成。例如，用于图像识别的图像像素值。 隐藏层：接收输入层的输出，并将自己的输出传递给下一层。 输出层：产生网络的最终结果。例如，用于人脸识别的人的概率值。 为了在这些层中进行线性组合，我们应该能够找到合适的权重。 反向传播 - 找到合适的权重 ： 在过去（20 世纪 80 年代之前），人们曾使用过感知器算法，但寻找权重需要花费大量时间。 因此，人们引入了反向传播算法。它通过层层递进和递退来确定合适的权重，从而做出准确的预测。 现在，让我们举个例子，来识别图像。 建立分类器，对图像显示的是 \"X \"还是 \"O \"进行分类 这里是一个 5 × 5 的网格，因此每幅图像由 25 个像素组成。阴影像素为 1，其他空白像素为 0。 现在，我们应该应用权重，其中在中心位置，权重假设为-1，而在近中心像素位置，权重假设为 1： 因此，在这里，如果线性组合为负数，即激活度为零，则为 \"X\"；如果为正数，则为 \"O\"。 对第一幅图像进行线性组合 ： (忽略 0 值权重，得到） -&gt; 1 × -1 = -1 因此，我们得到 \"X” 对第二幅图像进行线性组合： (忽略 0 值权重，我们得到） -&gt; 1 × 1 + 1 × 1 + 1 × 1 + 1 × 1 = 4 因此我们得到 \"O\" 到目前为止，我们已经了解了 -&gt; 多层网络（超过一层的神经网络）、非线性函数（阶跃激活函数和 Sigmoid 激活函数）、学习规则（如反向传播）。 让我们进入高级神经网络。 卷积神经网络 ： 使用感知器或线性回归可以进行图像处理，但由于需要大量权值，而且无法有效检测图像特征，因此效果和效率都不高。 因此，为了解决这些局限性，人们引入了卷积神经网络。 CNN 或卷积神经网络由卷积层组成，可以自动学习和提取图像特征，如颜色、图案、边缘等。 例如，CNN 可用于动物检测、标志检测等。 如果我们想使用传统方法检测图像或识别图像，它将使用图像中的像素位置来检测物体。因此，我们必须有一张相似的图像才能做到这一点，但对于卷积神经网络来说，这并非必要。 例如，我们有一张位于图像中心的停车标志的训练图像，然后我们会得到一张测试图像，该图像的右上角有一个停车标志。由于训练图像和测试图像的像素值和位置不同，因此无法使用感知器进行检测。不过，通过使用卷积神经网络，它可以成功检测出图像中任何位置的停车标志。 生成式人工智能（Generative AI）： 生成式人工智能是人工智能的一种，可以生成文本、图像、音频和合成数据等各种类型的内容。 它可以是.....： 监督学习法 无监督学习法 半监督学习法 判别模型用于通过标注数据的训练进行分类或预测。 生成模型用于生成新数据，如预测序列中的下一个单词。 生成对抗网络（GAN）： 其原理是让两个神经网络相互竞争。 一个网络将生成与训练数据类似的图像。 另一个网络将对生成的图像和训练图像进行分类。 这样做是为了生成逼真的图像。 上述图像由英伟达公司开发的 GAN 生成。 将人工智能应用于现实问题比解决谜题和游戏更具挑战性。在现实世界的场景中，可能出现的状态数量之多令人目不暇接，使得穷举式搜索或巧妙的启发式方法无法奏效。此外，由于我们无法控制的因素，行动的结果并不总是可以预测的，这就引入了随机性。为了解决这些复杂问题，我们需要将不确定性和概率的概念纳入算法，同时利用先进的神经网络，使我们能够有效地解决现实世界中的人工智能问题。 康康康康恐龙康。最后，你已经掌握了基本的神经网络和高级神经网络的基本知识。","link":"/Artificial-Neural-Network/"},{"title":"Finish the search problem","text":"The code address of this article is: example_01_Assignment Please read the answer below after thinking for yourself Please using the search policy to implement an agent. This agent receives two input, one is @param start station and the other is @param destination. Your agent should give the optimal route based on Beijing Subway system. Dataflow: 1. Get data from web page. Get web page source from: https://baike.baidu.com/item/%E5%8C%97%E4%BA%AC%E5%9C%B0%E9%93%81/408485 You may need @package requests https://2.python-requests.org/en/master/ page to get the response via url You may need save the page source to file system. The target of this step is to get station information of all the subway lines; You may need install @package beautiful soup https://www.crummy.com/software/BeautifulSoup/bs4/doc/ to get the url information, or just use &gt; Regular Expression to get the url. Our recommendation is that using the Regular Expression and BeautiflSoup both. You may need BFS to get all the related page url from one url. Question: Why do we use BFS to traverse web page (or someone said, build a web spider)? Can DFS do this job? which is better? 2. Preprocessing data from page source. Based on the page source gotten from url. You may need some more preprocessing of the page. the Regular Expression you may need to process the text information. You may need @package networkx, @package matplotlib to visualize data. You should build a dictionary or graph which could represent the connection information of Beijing subway routes. You may need the defaultdict, set data structures to implement this procedure. 3. Build the search agent Build the search agent based on the graph we build. for example, when you run: 1&gt;&gt;&gt; search('奥体中心', '天安门') you need get the result: 奥体中心-&gt; A -&gt; B -&gt; C -&gt; ... -&gt; 天安门 HTTP协议 超文本传输协议（HTTP，HyperText Transfer Protocol）是互联网上应用最为广泛的一种网络协议。所有的www文件都必须遵守这个标准。 HTTP用于客户端和服务器之间的通信。协议中规定了客户端应该按照什么格式给服务器发送请求，同时也约定了服务端返回的响应结果应该是什么格式。 请求访问文本或图像等信息的一端称为客户端，而提供信息响应的一端称为服务器端。 客户端告诉服务器请求访问信息的方法： - Get 获得内容 - Post 提交表单来爬取需要登录才能获得数据的网站 - put 传输文件 更多参考： HTTP请求状态 了解200 404 503 - 200 OK //客户端请求成功 - 404 Not Found //请求资源不存在，eg：输入了错误的URL - 503 Server Unavailable //服务器当前不能处理客户端的请求，一段时间后可能恢复正常。 #### Requests 纯粹HTML格式的网页通常被称为静态网页，静态网页的数据比较容易获取。 在静态网页抓取中，有一个强大的Requests库能够让你轻易地发送HTTP请求。 在终端上安装 Requests pip install requents 123456789101112# 获取响应内容import requests# get（输入你想要抓去的网页地址）r = requests.get('https://www.baidu.com/')print('文本编码：（服务器使用的文本编码）', r.encoding)print('响应状态码：（200表示成功）', r.status_code)print('字符串方式的响应体：（服务器响应的内容）', r.text) 拓展知识： Unicode和UTF-8有什么区别?(盛世唐朝回答) 正则表达式 正则表达式的思想是你在人群中寻找你的男朋友/女朋友，他/她在你心里非常有特点。 同样，从一堆文本中找到需要的内容，我们可以采用正则表达式。 正经点说，是以一定的模式来进行字符串的匹配。 掌握正则表达式需要非常多的时间，我们可以先入门，在以后的工作中遇到，可更加深入研究。 使用正则表达式有如下步骤： 寻找【待找到的信息】特点 使用符号找到特点 获得信息 12345678910111213141516171819202122232425262728293031323334353637383940# 请先运行一下、看一下有什么参数？# 请思考，找到会返回什么？没找到会返回什么？import rehelp(re.match)# 请运行之后、思考 match 与 search 的区别?m = re.search('foo', 'seafood')print(m)print(m.group())print('-------------------------')m = re.match('foo', 'seafood')print(m)#### `search`是搜索字符串中首次出现的位置# 匹配多个字符串 |m = re.match('bat|bet|bit', 'bat')print(m.group()) if m is not None else print('None')# 匹配任意单个字符 .m = re.match('.end', 'kend')print(m.group()) if m is not None else print('None')m = re.match('.end', 'end')print(m.group()) if m is not None else print('None')# 字符串集合 []m = re.match('[cr][23][dp][o2]', 'c3p2')print(m.group()) if m is not None else print('None')# [] 与 |是不同的m = re.match('c3po|r2d2', 'c3p2')print(m.group()) if m is not None else print('None') 给大家提供一个字典，供大家查询～ 字符 描述 &lt;/th&gt; 将下一个字符标记为一个特殊字符、或一个原义字符、或一个向后引用、或一个八进制转义符。例如，“n”匹配字符“n”。“”匹配一个换行符。串行“\\”匹配“&lt;/code&gt;”而“(”则匹配“(”。 &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;^&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配“&lt;code&gt;\\n&lt;/code&gt;”或“&lt;code&gt;\\r&lt;/code&gt;”之后的位置。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt; \\* &lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配前面的子表达式零次或多次。例如，zo\\*能匹配“&lt;code&gt;z&lt;/code&gt;”以及“&lt;code&gt;zoo&lt;/code&gt;”。\\* 等价于{0,}。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;+&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配前面的子表达式一次或多次。例如，“&lt;code&gt;zo+&lt;/code&gt;”能匹配“&lt;code&gt;zo&lt;/code&gt;”以及“&lt;code&gt;zoo&lt;/code&gt;”，但不能匹配“&lt;code&gt;z&lt;/code&gt;”。+等价于{1,}。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;?&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配前面的子表达式零次或一次。例如，“&lt;code&gt;do(es)?&lt;/code&gt;”可以匹配“&lt;code&gt;does&lt;/code&gt;”或“&lt;code&gt;does&lt;/code&gt;”中的“&lt;code&gt;do&lt;/code&gt;”。?等价于{0,1}。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;}&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;是一个非负整数。匹配确定的&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;次。例如，“&lt;code&gt;o{2}&lt;/code&gt;”不能匹配“&lt;code&gt;Bob&lt;/code&gt;”中的“&lt;code&gt;o&lt;/code&gt;”，但是能匹配“&lt;code&gt;food&lt;/code&gt;”中的两个o。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,}&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;是一个非负整数。至少匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;次。例如，“&lt;code&gt;o{2,}&lt;/code&gt;”不能匹配“&lt;code&gt;Bob&lt;/code&gt;”中的“&lt;code&gt;o&lt;/code&gt;”，但能匹配“&lt;code&gt;foooood&lt;/code&gt;”中的所有o。“&lt;code&gt;o{1,}&lt;/code&gt;”等价于“&lt;code&gt;o+&lt;/code&gt;”。“&lt;code&gt;o{0,}&lt;/code&gt;”则等价于“&lt;code&gt;o*&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;}&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;和&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;均为非负整数，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&amp;lt;=&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;。最少匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;次且最多匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;次。例如，“&lt;code&gt;o{1,3}&lt;/code&gt;”将匹配“&lt;code&gt;fooooood&lt;/code&gt;”中的前三个o。“&lt;code&gt;o{0,1}&lt;/code&gt;”等价于“&lt;code&gt;o?&lt;/code&gt;”。请注意在逗号和两个数之间不能有空格。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;?&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;当该字符紧跟在任何一个其他限制符（*,+,?，{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;}，{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,}，{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“&lt;code&gt;oooo&lt;/code&gt;”，“&lt;code&gt;o+?&lt;/code&gt;”将匹配单个“&lt;code&gt;o&lt;/code&gt;”，而“&lt;code&gt;o+&lt;/code&gt;”将匹配所有“&lt;code&gt;o&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;.&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配除“&lt;code&gt;\\&lt;/code&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;&lt;code&gt;n&lt;/code&gt;&lt;/span&gt;”之外的任何单个字符。要匹配包括“&lt;code&gt;\\&lt;/code&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;&lt;code&gt;n&lt;/code&gt;&lt;/span&gt;”在内的任何字符，请使用像“&lt;code&gt;(.|\\n)&lt;/code&gt;”的模式。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配pattern并获取这一匹配。所获取的匹配可以从产生的Matches集合得到，在VBScript中使用SubMatches集合，在JScript中则使用$0…$9属性。要匹配圆括号字符，请使用“&lt;code&gt;\\(&lt;/code&gt;”或“&lt;code&gt;\\)&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?:pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配pattern但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用或字符“&lt;code&gt;(|)&lt;/code&gt;”来组合一个模式的各个部分是很有用。例如“&lt;code&gt;industr(?:y|ies)&lt;/code&gt;”就是一个比“&lt;code&gt;industry|industries&lt;/code&gt;”更简略的表达式。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?=pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，“&lt;code&gt;Windows(?=95|98|NT|2000)&lt;/code&gt;”能匹配“&lt;code&gt;Windows2000&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;Windows3.1&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?!pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如“&lt;code&gt;Windows(?!95|98|NT|2000)&lt;/code&gt;”能匹配“&lt;code&gt;Windows3.1&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;Windows2000&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?&amp;lt;=pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;反向肯定预查，与正向肯定预查类拟，只是方向相反。例如，“&lt;code&gt;(?&amp;lt;=95|98|NT|2000)Windows&lt;/code&gt;”能匹配“&lt;code&gt;2000Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;3.1Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?&amp;lt;!pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;反向否定预查，与正向否定预查类拟，只是方向相反。例如“&lt;code&gt;(?&amp;lt;!95|98|NT|2000)Windows&lt;/code&gt;”能匹配“&lt;code&gt;3.1Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;2000Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;x|y&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配x或y。例如，“&lt;code&gt;z|food&lt;/code&gt;”能匹配“&lt;code&gt;z&lt;/code&gt;”或“&lt;code&gt;food&lt;/code&gt;”。“&lt;code&gt;(z|f)ood&lt;/code&gt;”则匹配“&lt;code&gt;zood&lt;/code&gt;”或“&lt;code&gt;food&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[xyz]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;字符集合。匹配所包含的任意一个字符。例如，“&lt;code&gt;[abc]&lt;/code&gt;”可以匹配“&lt;code&gt;plain&lt;/code&gt;”中的“&lt;code&gt;a&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[^xyz]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;负值字符集合。匹配未包含的任意字符。例如，“&lt;code&gt;[^abc]&lt;/code&gt;”可以匹配“&lt;code&gt;plain&lt;/code&gt;”中的“&lt;code&gt;p&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[a-z]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;字符范围。匹配指定范围内的任意字符。例如，“&lt;code&gt;[a-z]&lt;/code&gt;”可以匹配“&lt;code&gt;a&lt;/code&gt;”到“&lt;code&gt;z&lt;/code&gt;”范围内的任意小写字母字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[^a-z]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;负值字符范围。匹配任何不在指定范围内的任意字符。例如，“&lt;code&gt;[^a-z]&lt;/code&gt;”可以匹配任何不在“&lt;code&gt;a&lt;/code&gt;”到“&lt;code&gt;z&lt;/code&gt;”范围内的任意字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\b&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个单词边界，也就是指单词和空格间的位置。例如，“&lt;code&gt;er\\b&lt;/code&gt;”可以匹配“&lt;code&gt;never&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”，但不能匹配“&lt;code&gt;verb&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\B&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配非单词边界。“&lt;code&gt;er\\B&lt;/code&gt;”能匹配“&lt;code&gt;verb&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”，但不能匹配“&lt;code&gt;never&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\cx&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配由x指明的控制字符。例如，\\cM匹配一个Control-M或回车符。x的值必须为A-Z或a-z之一。否则，将c视为一个原义的“&lt;code&gt;c&lt;/code&gt;”字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\d&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个数字字符。等价于[0-9]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\D&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个非数字字符。等价于[^0-9]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\f&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个换页符。等价于\\x0c和\\cL。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\n&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个换行符。等价于\\x0a和\\cJ。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\r&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个回车符。等价于\\x0d和\\cM。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\s&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配任何空白字符，包括空格、制表符、换页符等等。等价于[ \\f\\n\\r\\t\\v]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\S&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配任何非空白字符。等价于[^ \\f\\n\\r\\t\\v]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\t&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个制表符。等价于\\x09和\\cI。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\v&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个垂直制表符。等价于\\x0b和\\cK。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\w&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配包括下划线的任何单词字符。等价于“&lt;code&gt;[A-Za-z0-9_]&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\W&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配任何非单词字符。等价于“&lt;code&gt;[^A-Za-z0-9_]&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\x&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，“&lt;code&gt;\\x41&lt;/code&gt;”匹配“&lt;code&gt;A&lt;/code&gt;”。“&lt;code&gt;\\x041&lt;/code&gt;”则等价于“&lt;code&gt;\\x04&amp;amp;1&lt;/code&gt;”。正则表达式中可以使用ASCII编码。.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;num&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;num&lt;/span&gt;，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;num&lt;/span&gt;是一个正整数。对所获取的匹配的引用。例如，“&lt;code&gt;(.)\\1&lt;/code&gt;”匹配两个连续的相同字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;标识一个八进制转义值或一个向后引用。如果\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;之前至少&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;个获取的子表达式，则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为向后引用。否则，如果&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为八进制数字（0-7），则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为一个八进制转义值。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;标识一个八进制转义值或一个向后引用。如果\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;之前至少有&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;个获得子表达式，则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;为向后引用。如果\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;之前至少有&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;个获取，则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为一个后跟文字&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;的向后引用。如果前面的条件都不满足，若&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;和&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;均为八进制数字（0-7），则\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;将匹配八进制转义值&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nml&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;如果&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为八进制数字（0-3），且&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m和l&lt;/span&gt;均为八进制数字（0-7），则匹配八进制转义值&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;l。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\u&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;是一个用四个十六进制数字表示的Unicode字符。例如，\\u00A9匹配版权符号（©）。&lt;/td&gt; &lt;/tr&gt; 12345678910111213# 匹配电子邮件地址patt = '\\w+@(\\w+\\.)?\\w+\\.com'm = re.match(patt, 'nobody@xxx.com')print(m.group()) if m is not None else print('None')# 匹配QQm = re.search('[1-9][0-9]{4,}', '这是我的QQ号781504542,第二个qq号：10054422288')print(m.group()) if m is not None else print('None')# findall() 是search的升级版，可以找到所有匹配的字符串m = re.findall('[1-9][0-9]{4,}', '这是我的QQ号781504542,第二个qq号：10054422288')print(m) if m is not None else print('None') 了解了怎么使用，下面进入实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# get the data (subway for beijing ,from amap)# 你需要用到以下的包import requestsimport reimport numpy as npr = requests.get('http://map.amap.com/service/subway?_1469083453978&amp;srhdata=1100_drw_beijing.json')r.textdef get_lines_stations_info(text): # Please write your code here pass # Traverse the text format data to form the location data structure # Dict of all line information: key: line name; value: list of site names lines_info = {} # A dict of all site information: key: site name; value: site coordinates (x, y) stations_info = {} for i in range(len(lines_list)): # Several questions you may need to think about, get &quot;Metro line name, station information list, station name, coordinates (x, y), add data to the information dict of the station, add data to the subway line dict&quot; passlines_info, stations_info = get_lines_stations_info(r.text)# According to the route information, establish the site adjacency table dictdef get_neighbor_info(lines_info): pass # Add str2 to the adjacency list of site str1 def add_neighbor_dict(info, str1, str2): # Please write code here pass return neighbor_info neighbor_info = get_neighbor_info(lines_info)neighbor_info# Draw subway mapimport networkx as nximport matplotlibimport matplotlib.pyplot as plt# If Chinese characters cannot be displayed, please refer tomatplotlib.rcParams['font.sans-serif'] = ['SimHei']# matplotlib.rcParams['font.family']='sans-serif'# You can use recursion to find all pathsdef get_path_DFS_ALL(lines_info, neighbor_info, from_station, to_station): # Recursive algorithm, essentially depth first # Traverse all paths # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Check input site name passdef get_next_station_DFS_ALL(node, neighbor_info, to_station): pass# You can also use the second algorithm: simple breadth first without heuristic functiondef get_path_BFS(lines_info, neighbor_info, from_station, to_station): # Search strategy: take the number of stations as the cost (because the ticket price is calculated by station) # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Since the cost of each layer is increased by 1, the cost of each layer is the same, and it does not matter whether it is calculated or not, so it is omitted # Check input site name pass# You can also use the third algorithm: heuristic search with path distance as the costimport pandas as pddef get_path_Astar(lines_info, neighbor_info, stations_info, from_station, to_station): # Search strategy: the straight-line distance between the stations of the route is accumulated as the cost, and the straight-line distance from the current station to the target is used as the heuristic function # Check input site name pass As much as you can to use the already implemented search agent. You just need to define the is_goal(), get_successor(), strategy() three functions. Define different policies for transfer system. Such as Shortest Path Priority（路程最短优先）, Minimum Transfer Priority(最少换乘优先), Comprehensive Priority(综合优先) Implement Continuous transfer. Based on the Agent you implemented, please add this feature: Besides the @param start and @param destination two stations, add some more stations, we called @param by_way, it means, our path should from the start and end, but also include the @param by_way stations. e.g 123451. Input: start=A, destination=B, by_way=[C] Output: [A, … .., C, …. B]2. Input: start=A, destination=B, by_way=[C, D, E] Output: [A … C … E … D … B] # based on your policy, the E station could be reached firstly. The Answer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305# get the data (subway for beijing ,from amap)import requestsimport reimport numpy as npr = requests.request('GET', url = 'http://map.amap.com/service/subway?_1469083453978&amp;srhdata=1100_drw_beijing.json')def get_lines_stations_info(text): lines_info = {} stations_info = {} pattern = re.compile('&quot;st&quot;.*?&quot;kn&quot;') lines_list = pattern.findall(text) for i in range(len(lines_list)): pattern = re.compile('&quot;ln&quot;:&quot;.*?&quot;') line_name = pattern.findall(lines_list[i])[0][6:-1] pattern = re.compile('&quot;rs&quot;.*?&quot;sp&quot;') temp_list = pattern.findall(lines_list[i]) station_name_list = [] for j in range(len(temp_list)): pattern = re.compile('&quot;n&quot;:&quot;.*?&quot;') station_name = pattern.findall(temp_list[j])[0][5:-1] station_name_list.append(station_name) pattern = re.compile('&quot;sl&quot;:&quot;.*?&quot;') position = tuple(map(float, pattern.findall(temp_list[j])[0][6:-1].split(','))) stations_info[station_name] = position lines_info[line_name] = station_name_list return lines_info, stations_infolines_info, stations_info = get_lines_stations_info(r.text)# print(stations_info)# print(lines_info)len(lines_info)def get_neighbor_info(lines_info): def add_neighbor_dict(info, str1, str2): list1 = info.get(str1) if not list1: list1 = [] list1.append(str2) info[str1] = list1 return info neighbor_info = {} for line_name, station_list in lines_info.items(): for i in range(len(station_list) -1): sta1 = station_list[i] sta2 = station_list[i+1] neighbor_info = add_neighbor_dict(neighbor_info, sta1, sta2) neighbor_info = add_neighbor_dict(neighbor_info, sta2, sta1) return neighbor_infoneighbor_info = get_neighbor_info(lines_info)print(neighbor_info)import networkx as nximport matplotlibimport matplotlib.pyplot as pltmatplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS']matplotlib.rcParams['font.size'] = 2plt.figure(figsize = (20, 20))stations_graph = nx.Graph()stations_graph.add_nodes_from(list(stations_info.keys()))nx.draw(stations_graph, stations_info, with_labels = True, node_size = 5)stations_connection_graph = nx.Graph(neighbor_info)plt.figure(figsize = (30, 30))nx.draw(stations_connection_graph, stations_info, with_labels = True, node_size = 5)# The first algorithm: recursively find all pathsdef get_path_DFS_ALL(lines_info, neighbor_info, from_station, to_station): # # Recursive algorithm, essentially depth first # Traverse all paths # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Check input site name if not neighbor_info.get(from_station): print('起始站点“%s”不存在。请正确输入！'%from_station) return None if not neighbor_info.get(to_station): print('目的站点“%s”不存在。请正确输入！'%to_station) return None path = [] this_station = from_station path.append(this_station) neighbors = neighbor_info.get(this_station) node = {'pre_station':'', 'this_station':this_station, 'neighbors':neighbors, 'path':path} return get_next_station_DFS_ALL(node, neighbor_info, to_station)def get_next_station_DFS_ALL(node, neighbor_info, to_station): neighbors = node.get('neighbors') pre_station = node.get('this_station') path = node.get('path') paths = [] for i in range(len(neighbors)): this_station = neighbors[i] if (this_station in path): # If this station is already in the path, it means a loop, and this road is unreachable return None if neighbors[i] == to_station: # Find the end, return to the path path.append(to_station) paths.append(path) return paths else: neighbors_ = neighbor_info.get(this_station).copy() neighbors_.remove(pre_station) path_ = path.copy() path_.append(this_station) new_node = {'pre_station':pre_station, 'this_station':this_station, 'neighbors':neighbors_, 'path':path_} paths_ = get_next_station_DFS_ALL(new_node, neighbor_info, to_station) if paths_: paths.extend(paths_) return pathspaths = get_path_DFS_ALL(lines_info, neighbor_info, '回龙观', '西二旗')print('共有%d种路径。'%len(paths))for item in paths: print(&quot;此路径总计%d站:&quot;%(len(item)-1)) print('-'.join(item))# The second algorithm: simple breadth first without heuristic functiondef get_path_BFS(lines_info, neighbor_info, from_station, to_station): # Search strategy: take the number of stations as the cost (because the ticket price is calculated by station) # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Since the cost of each layer is increased by 1, the cost of each layer is the same, and it does not matter whether it is calculated or not, so it is omitted # Check input site name if not neighbor_info.get(from_station): print('起始站点“%s”不存在。请正确输入！'%from_station) return None if not neighbor_info.get(to_station): print('目的站点“%s”不存在。请正确输入！'%to_station) return None # The search node is a dict, key=site name, value is a list of sites that contain passing nodes = {} nodes[from_station] = [from_station] while True: new_nodes = {} for (k,v) in nodes.items(): neighbor = neighbor_info.get(k).copy() if (len(v) &gt;= 2): # Do not go to the previous stop pre_station = v[-2] neighbor.remove(pre_station) for station in neighbor: # Traverse neighbors if station in nodes: # Skip the nodes that have been searched continue path = v.copy() path.append(station) new_nodes[station] = path if station == to_station: # Find the path, end return path nodes = new_nodes print('未能找到路径') return Nonepaths = get_path_BFS(lines_info, neighbor_info, '回龙观', '西二旗')print(&quot;路径总计%d站。&quot;%(len(paths)-1))print(&quot;-&quot;.join(paths))# Gaode Navigation is 31 stations, only 1 transfer# The result of the code is 28 stations, but there are 5 transfers# Guess Gaode's path cost is mainly time# The third algorithm: heuristic search with path distance as the costimport pandas as pddef get_path_Astar(lines_info, neighbor_info, stations_info, from_station, to_station): # Search strategy: the straight-line distance between the stations of the route is accumulated as the cost, and the straight-line distance from the current station to the target is used as the heuristic function # Check input site name if not neighbor_info.get(from_station): print('起始站点“%s”不存在。请正确输入！'%from_station) return None if not neighbor_info.get(to_station): print('目的站点“%s”不存在。请正确输入！'%to_station) return None # Calculate the straight-line distance from all nodes to the target node, spare distances = {} x,y = stations_info.get(to_station) for (k,v) in stations_info.items(): x0,y0 = stations_info.get(k) l = ((x-x0)**2 + (y-y0)**2)**0.5 distances[k] = l # Nodes that have been searched, dict # key=site name, value is the minimum cost from a known starting point to this site # 已搜索过的节点，dict searched = {} searched[from_station] = 0 # The data structure is pandas dataframe # index is the site name # g is the path taken, h is the heuristic function value (the current straight-line distance to the target) nodes = pd.DataFrame([[[from_station], 0, 0, distances.get(from_station)]], index=[from_station], columns=['path', 'cost', 'g', 'h']) count = 0 while True: if count &gt; 1000: break nodes.sort_values('cost', inplace=True) for index, node in nodes.iterrows(): count += 1 # Search for the site that is the shortest from the destination among the neighbors neighbors = neighbor_info.get(index).copy() if len(node['path']) &gt;= 2: # Do not search in the reverse direction of this path neighbors.remove(node['path'][-2]) for i in range(len(neighbors)): count += 1 neighbor = neighbors[i] g = node['g'] + get_distance(stations_info, index, neighbor) h = distances[neighbor] cost = g + h path = node['path'].copy() path.append(neighbor) if neighbor == to_station: # Find the goal, end print('共检索%d次。'%count) return path if neighbor in searched: if g &gt;= searched[neighbor]: # Explain that the search path is not optimal, ignore it continue else: searched[neighbor] = g # Modify the node information corresponding to this site# nodes.loc[neighbor, 'path'] = path # 这行总是报错# nodes.loc[neighbor, 'cost'] = cost# nodes.loc[neighbor, 'g'] = g# nodes.loc[neighbor, 'h'] = h # I don’t know how to modify the list element in df, I can only delete and add new rows nodes.drop(neighbor, axis=0, inplace=True) row = pd.DataFrame([[path, cost, g, h]], index=[neighbor], columns=['path', 'cost', 'g', 'h']) nodes = nodes.append(row) else: searched[neighbor] = g row = pd.DataFrame([[path, cost, g, h]], index=[neighbor], columns=['path', 'cost', 'g', 'h']) nodes = nodes.append(row) # All neighbors of this site have been searched, delete this node nodes.drop(index, axis=0, inplace=True) # The outer for loop only runs the first row of data, and then re-sort and then calculate continue print('未能找到路径') return Nonedef get_distance(stations_info, str1, str2): x1,y1 = stations_info.get(str1) x2,y2 = stations_info.get(str2) return ((x1-x2)**2 + (y1-y2)**2)** 0.5paths = get_path_Astar(lines_info, neighbor_info, stations_info, '回龙观', '西二旗')if paths: print(&quot;路径总计%d站。&quot;%(len(paths)-1)) print(&quot;-&quot;.join(paths))# Gaode Navigation is 31 stations, only 1 transfer# The code result is 28 stations, which is the same as the result with the number of subway stations as the cost, but the path is different (from the first traversal algorithm, you can see that there are 3 paths for 28 stations to reach the destination)# Guess Gaode's path cost is mainly time","link":"/Assignment/"},{"title":"将 Bard API 与 ChatGPT 集成：实时数据访问","text":"在人工智能领域，很少有创新能像 OpenAI 的 ChatGPT 一样激发世界的想象力。这种非凡的对话式人工智能改变了我们看待人机交互的方式，展现出一定程度的复杂性、情境意识和创造力，而这些曾经被认为是人类智能的专属领域。 ChatGPT 基于强大的 GPT-3 模型构建，能够进行引人入胜、有意义且令人印象深刻的类人对话。它可以写诗、回答复杂的问题、辅导各种科目、翻译语言，甚至模仿著名作家的写作风格。从本质上讲，它重新定义了我们认为人工智能可能实现的界限。 然而，ChatGPT 的主要缺点是它缺乏实时互联网数据访问。这意味着，虽然 ChatGPT 可以生成高度智能且上下文准确的响应，但其知识基本上被及时冻结，截止日期为 2021 年 9 月。 那么，当出现需要通过将 Google 的 Bard API 与 ChatGPT 集成来获取超出此限制的信息的问题时，会发生什么情况呢？ 以下是使用Python将 Bard API 连接到 ChatGPT 以检索实时数据的分步指南： 第 1 步：安装非官方 Bard Python 库并检索 Cookie 值（API 密钥） 我正在使用Daniel Park使用逆向工程开发的非官方 Bard 库。这个库是一个非常用户友好的Python包。其主要目的是通过 API 从 Google Bard 获取响应。使用 Bard-API，用户可以方便地将 Bard 的自然语言响应集成到他们的 Python 项目和各种应用程序中。 1pip install bardapi 您还可以直接从 Github安装最新版本： 1pip install git+https://github.com/dsdanielpark/Bard-API.git 12345from bardapi import Bardtoken = 'xxxxxxx'bard = Bard(token=token)bard.get_answer(&lt;your query&gt;)['content'] 设置您的 API 密钥 安装 Bard-API 后，使用 Bard cookie 中的 Secure-1PSID 进行身份验证。尽管非正式地称为 API KEY（Cookie 值），但请记住对其保密以确保安全访问。 访问https://bard.google.com/ 按 F12 或右键单击并“检查” 转到应用程序 → Cookie，并将您的 __Secure-1PSID Cookie 值复制到安全位置。 步骤 2：从openai.com获取 OpenAI 密钥并安装 OpenAI 库 访问 OpenAI 网站并获取您的 OpenAI API 密钥。现在安装 OpenAI 库并导入它。 1pip install openai 12import openaiopenai.api_key = &lt;Your_API_Key&gt; 步骤3：将bard请求结果连接到gpt-3.5-turbo模型并设计提示 这里的关键部分是设计将 Bard 结果集成到 ChatGPT API 函数中所需的提示。因此，我为此制定了一个方法： 123456789101112query = input(&quot;Your query&quot;)bard_result = bard.get_answer(query)['content']completion = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Act as an AI chatbot with access to the internet.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Provide a well structured and easily readable text by analyzing this: The first content below is the user's query and the second content below is the result obtained by accessing the internet with the help of google's search alogoritm. Provide the well structured and good mannered answer by processing the user's query and the result from Google search algorithm. /n&quot;+query+' /n '+ bard_result} ])final_response = completion[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]print(final_response) 将所有代码封装在一起，得出结果： 1234567891011121314151617from bardapi import Bardimport openaiopenai.api_key = &lt;Your Key&gt;token = &lt;Your Key&gt;bard = Bard(token=token)query = input(&quot;Your query: &quot;)bard_result = bard.get_answer(query)['content']completion = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Act as an AI chatbot with access to the internet.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Provide a well structured and easily readable text by analyzing this: The first content below is the user's query and the second content below is the result obtained by accessing the internet with the help of google's search alogoritm. Provide the well structured and good mannered answer by processing the user's query and the result from Google search algorithm. /n&quot;+query+' /n '+ bard_result} ])final_response = completion[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]print(final_response) 与其使用 gpt-3.5-turbo 型号，不如试试 gpt-3.5-turbo-16k 和 gpt-4-0314，效果会更好。 通过整合像谷歌的 Bard 这样的应用程序接口，ChatGPT 可以超越目前的局限，为用户提供实时、准确的上下文信息。这将大大增强其协助、教育和与用户互动的能力，为人与人工智能的互动增添一个全新的维度。此外，这还将极大地扩展 ChatGPT 的应用范围，为企业、教育工作者、研究人员和个人带来新的机遇。 我认为这是将互联网接入集成到 ChatGPT 并从 ChatGPT 获得实时见解的最简单方法。","link":"/BardAPI-ChatGPT/"},{"title":"最佳 ChatGPT Chrome 扩展程序","text":"想要轻松访问ChatGPT吗？其中一个最佳方式是通过其一系列Chrome扩展程序。这些扩展程序还为您提供更好的使用ChatGPT的方法，包括帮助您编写更好的提示以获得更好的响应，或为ChatGPT授予搜索互联网的能力，从而提供对更多最新信息的访问。 这是您现在可以使用的最佳 ChatGPT Chrome 扩展程序。 Google 聊天 GPT 嫉妒 Bing Chat 及其在您搜索时与您聊天或使用最新的 GPT-4 语言模型的能力吗？不需要。只需从网上商店获取适用于 Google 的 ChatGPT，您就可以将 ChatGPT 与 Google 搜索一起使用。事实上，只需进行一次普通的 Google 搜索，在结果旁边，您也会收到来自 ChatGPT 的回复，这有时比 Google 结果本身更有用。 Merlin 想要 ChatGPT 在您上网的任何地方响应任何内容？Merlin 将 ChatGPT 带到任何网站，因此您可以突出显示任何文本或网页，并要求 ChatGPT 对其做出响应。您可以让它为您总结一个网页，或为您提供 YouTube 视频的纲要，这样您就不需要全部观看了。 TalkBerry 为什么要在 ChatGPT 上打字，而不是直接与 ChatGPT 对话呢？使用 TalkBerry，您可以简单地与 ChatGPT 通话。只需安装扩展程序并确保您的麦克风或耳机已插入，即可开始使用。使用 TalkBerry，您可以节省大量在输入上的时间，或者将 ChatGPT 用作语言导师，让它聆听并帮助您提高发音和语言理解能力。 TweetGPT 使用 TweetGPT 可以让您的社交媒体游戏更上一层楼。TweetGPT 是 ChatGPT 的插件，利用 AI 聊天机器人工具的强大功能，制作更有趣、更尖刻、更具吸引力或更友好的推文和回复。您可以选择要发布的主题、您的情绪基调和语言，ChatGPT 将完成剩下的工作。如果您对某些措辞不满意，您甚至可以在之后编辑该消息。 GPT-EZ 如果您不喜欢 ChatGPT 界面并想将其更改为您自己的喜好，请尝试 GPT-EZ。它允许您自定义 ChatGPT 网站的 UI，包括配色方案、字体样式和其他选项。此外，它还可以让您更轻松地复制和继续与 ChatGPT 的对话，并让您更轻松地下载对话日志。 SnackPrompt 通过使用一些评价最高的提示来充分利用 AI 聊天机器人。SnackPrompt 列出并排名全球其他聊天机器人用户的最佳提示，让您可以访问一些最新和最强大的 AI 功能。 WebChatGPT ChatGPT 的最大限制之一是它无法访问最近的信息。即使您使用的是最新的 GPT-4 语言模型，它仍然只能访问 2021 年之前的信息。使用 WebChatGPT，您可以让 ChatGPT 能够在网络上搜索更多最新的信息来源。从 Chrome 网上应用店获取扩展程序，在使用 ChatGPT 时只需将其打开即可享受这一方便的功能。 YouTube Summary 喜欢 YouTube 教程，但不想看完序言？让此 ChatGPT 扩展为您总结说明。只需从 YouTube 视频页面获取转录内容，然后将其输入到插件中，您就会立即获得摘要。它还适用于文章、电子邮件或科学论文。","link":"/Best-ChatGPT-Chrome-Extension/"},{"title":"21. 尝试制作你自己的数字人进行播报","text":"Hi， 大家好。我是茶桁。 在之前的课程中，我们接触了AI进行文字回复，语音合成。 那么将这两个组合在一起，我们基本就可以制作一个智能的语音聊天机器人了。看过电影《Her》的同学都应该清楚，AI因为用了女神斯嘉丽.约翰逊的配音，吸引到了不少的观众。 不过， 我们怎么能就满足于此呢，从文字到音频，我们似乎还缺少了一点什么。是啊，谁不希望拥有一个特定的虚拟人来发出自己特定的语音。看着自己在镜头面前侃侃而谈的样子，是不是想想就兴奋？ 把这些需求都结合在一起，那就是“数字人”了，我相信各位小伙伴或多或少都已经接触过，至少在抖音上看到过其他主播的“数字人”了。但是我们不得不说，那些都是一些商业公司的成熟方案，而咱们要实现的内容肯定比不了人家，但是作为概念演示，那是完全够用了。 好了，让我开始吧。首先呢，让我们先制作一个语音聊天机器人 语音聊天机器人的制作 第一步：文本聊天 第一步是什么？当然是需要先做一个「文本聊天机器人」，还记得咱们第六讲的内容吗？接下来，咱们就需要用到第六讲中的代码逻辑，整个UI界面也还是使用Gradio来创建。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Digital-human-broadcasting/"},{"title":"ChatGPT代码解释器：如何为我节省数小时的工作","text":"创建一个交互式世界地图，显示国家人口数量，配以简短的句子描述。 2023年7月6日，OpenAI宣布Code Interpreter将在接下来的一周内向ChatGPT Plus用户开放。它可能是增强ChatGPT的能力和功能的最佳插件之一。 Code Interpreter可以运行代码，允许上传数据，这样您就可以用它来进行数据清理、分析、可视化等许多其他任务。它就像是您指尖上的数据分析师。 听起来很棒吧？接下来我们来看看。 我在一项任务中使用了它，这项任务可能需要我花费几个小时才能完成。根据您的任务和对Python库的熟悉程度，这项任务甚至可能需要更长时间。 在使用ChatGPT时，我只需要写几句话并上传两个CSV文件。 我将逐步介绍整个过程，以及Code Interpreter在理解数据、清理和预处理数据以及创建数据可视化方面的印象深刻之处。 您需要先启用它 如果您想在聊天中使用代码解释器，需要先通过设置启用它。 如何启用代码解释器 开启之后，你在接下来开启新的聊天的时候就可以使用它了。 来个具体任务 我打算创建一个世界地图可视化，其中点的大小与国家的人口成比例。 我有两个数据集，一个是国家人口，另一个是以纬度和经度表示的位置。 您可以轻松地在网上找到这些数据集。如果没有，文末我将提供数据集（以及源代码）。 让我们继续吧 由于这是一个简单的任务，我没有花费太多时间在提示工程上。这是我向ChatGPT请求的第一句话： 你可以在提示窗口左侧使用加号上传数据集。 使用+号上传数据集 首先要做的是读取数据集，这是显而易见的事情。但这还不够令人印象深刻！ 12345678# Code Interpreter 生成的代码import pandas as pd# 将CSV数据加载到pandas DataFrame中population_data = pd.read_csv('/mnt/data/country_population_historic.csv')# 显示 DataFrame 的前几行population_data.head() 然后，ChatGPT要求提供国家位置： 由于我已经拥有相关数据了，所以我要做的就只是上传它： 123456# Code Interpreter 生成的代码# 加载国家坐标数据country_coordinates = pd.read_csv('/mnt/data/country_coordinates.csv')# 显示前几行country_coordinates.head() 数据清洗和预处理 读取数据后的下一步是清洗和预处理数据。我相信你已经听过很多次，这一步通常需要最多的时间。 在创建世界地图可视化之前，有一些问题需要处理。ChatGPT检测到它们并在不到一分钟的时间内提供了解决方案。相当令人印象深刻！ 上面代码段中的代码: 12345678910# 由代码解释器生成的代码# 检查数据框中的国家名称是否一致population_countries = set(population_data['Country Name'])coordinates_countries = set(country_coordinates['name'])# 人口数据中的国家但不在坐标数据中missing_in_coordinates = population_countries - coordinates_countries# 坐标数据中的国家但人口数据中没有missing_in_population = coordinates_countries - population_countries 在坐标数据集和人口数据集中都有一些国家缺失。 ChatGPT在上面的代码片段中做了什么： 创建了Python集合，包含两个数据集中的国家名称。 分别比较这些集合，找到它们之间的差异。 ChatGPT在不到一分钟的时间内解决了这个问题： 以上片段中的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# Code Interpreter 生成的代码# 将人口数据集中的国家名称映射到坐标数据集中country_name_mapping = { 'Bahamas, The': 'Bahamas', 'Brunei Darussalam': 'Brunei', 'Cabo Verde': 'Cape Verde', 'Congo, Dem. Rep.': 'Congo [DRC]', 'Congo, Rep.': 'Congo [Republic]', &quot;Cote d'Ivoire&quot;: &quot;Côte d'Ivoire&quot;, 'Czechia': 'Czech Republic', 'Egypt, Arab Rep.': 'Egypt', 'Gambia, The': 'Gambia', 'Hong Kong SAR, China': 'Hong Kong', 'Iran, Islamic Rep.': 'Iran', &quot;Korea, Dem. People's Rep.&quot;: 'North Korea', 'Korea, Rep.': 'South Korea', 'Kyrgyz Republic': 'Kyrgyzstan', 'Lao PDR': 'Laos', 'Macao SAR, China': 'Macau', 'Micronesia, Fed. Sts.': 'Micronesia', 'Myanmar': 'Myanmar [Burma]', 'North Macedonia': 'Macedonia [FYROM]', 'Russian Federation': 'Russia', 'Sao Tome and Principe': 'São Tomé and Príncipe', 'Slovak Republic': 'Slovakia', 'Syrian Arab Republic': 'Syria', 'Turkiye': 'Turkey', 'Venezuela, RB': 'Venezuela', 'Yemen, Rep.': 'Yemen', 'Eswatini': 'Swaziland'}# 清理人口数据集population_data_clean = population_data.replace({&quot;Country Name&quot;: country_name_mapping})# 删除在坐标数据集中不存在的行population_data_clean = population_data_clean[population_data_clean['Country Name'].isin(coordinates_countries)]# 将人口数据与国家坐标数据合并merged_data = pd.merge(population_data_clean, country_coordinates, left_on='Country Name', right_on='name')# 只保留必要的列merged_data = merged_data[['Country Name', '2022', 'latitude', 'longitude']]# 重命名列以增加清晰度merged_data.columns = ['Country', 'Population', 'Latitude', 'Longitude']merged_data.head() 清理和预处理的数据 现在我们已经有了整洁和清晰的数据格式。这个步骤通常需要很多时间和人工。ChatGPT在大约一分钟内完成了这一步。 创建可视化 ChatGPT首先尝试使用Matplotlib库进行创建。然而，由于没有世界地图图像而失败： 然后，我只是要求它使用 Plotly，这就是我一开始想到的。 以上代码段中的代码： 12345678910111213141516171819202122232425262728293031# 代码由代码解释器生成import plotly.graph_objects as go# 创建散点地理图fig = go.Figure(data=go.Scattergeo( lon = merged_data['Longitude'], lat = merged_data['Latitude'], text = merged_data['Country'] + ': ' + merged_data['Population'].astype(str), mode = 'markers', marker = dict( size = (merged_data['Population'] / 1000000) ** 0.5, # Take square root to better visualize differences sizemode = 'diameter', sizeref = 1, sizemin = 4, color = merged_data['Population'], colorscale = 'Viridis', colorbar_title = &quot;Millions&quot;, )))# 更新布局fig.update_layout( title_text = '2022 World Population', geo = dict( showframe = False, showcoastlines = True, projection_type = 'equirectangular' ))fig.show() 它不能在浏览器中显示交互式世界地图，但成功编写了生成它的代码。我所要做的就是将代码复制粘贴到Jupyter笔记本中。 这是最终结果： 互动世界地图，显示国家人口 最后的话 我们所做的事情： 读取数据集 清洗、预处理和合并它们 创建互动数据可视化 我们所要做的只是写两个句子（并告诉 ChatGPT 使用 Plotly）。我认为这非常令人印象深刻！ 数据 本文已经结束。 文章最后，我将提供数据以及一个jupyter notebook内容，和往常一样，数据将付费查看，以获取一些成本。有想要的朋友可以去我公众号内搜索本文购买：","link":"/ChatGPT%E4%BB%A3%E7%A0%81%E8%A7%A3%E9%87%8A%E5%99%A8/"},{"title":"01 进入AI大门，学会与其交谈","text":"不用问我都知道，你们一定是被ChatGPT的火热出圈导致的开始关注人工智能，也是由于此才看到我这篇文章。 放心，大家想要的我一定会给予，既然大家都想先认识ChatGPT，那么我们就从这个主题开始。 接下来，我们学学如何利用openAI的API来和其沟通。在整个使用过程中，我们都使用的是GPT-3.5的大预言模型。 在本课程中，我们将回答许多问题，例如，OpenAI 的 API 能够实现哪些神奇的事情？OpenAI 的产品被称为已经离通用人工智能（AGI）不远了，它们长什么样子？GPT-3 这样的模型与之前基于深度学习的自然语言处理解决方案有什么不同？我们将通过逐步解释这些问题，使您深入了解这个令人兴奋的领域。 无论您是否是一名程序员，您都可以从本课程中学习如何使用 AI 技术，尤其是大型语言模型，为您的项目和业务提供价值。 基础工作 创建帐号和API Key 了开始学习本课程，您需要先注册一个可以使用 OpenAI 的 API 的账号。您可以通过注册入口进行注册。目前，OpenAI 尚未向中国大陆和香港地区开放，因此您需要自己寻找适当的解决方案进行注册。如果您有更好的解决方案，也欢迎在评论区分享。 注册账号后，您需要点击右上角的账号，然后进入 \"View API Keys\" 页面管理 API Keys。 您可以点击下方的 \"+Create new secret key\" 来创建一个新的 API Key。 您需要将此 API Key 存储在一个安全的位置，因为在后续的 OpenAI 接口调用中，需要使用此 API Key。 储存API Key留用这方便，我使用的是1Password，开了家庭版，很好用。 目前，OpenAI 为所有免费注册的用户提供了 5 美元（原来是 18 美元）的免费 API 额度，足够您体验 API 的功能并完成本课程的学习。如果您想在实际产品中进一步使用此 API，则需要考虑升级到付费账户。 注：在本文完成之时，我发现免费账户已经无法使用免费的API配额了，不仅如此，因为API配额和ChatGPT Plus是两个支付系统，所以Plus并不等同于API 配额，你需要绑定一张信用卡用于支付你的使用量。 未绑卡 已绑卡 关于绑卡这个事，可以自己在网上搜索看，办法总比问题多。不要找我，虽然我有渠道，但是我的渠道很贵，到时候说我骗人钱我可说不清楚。 搭建环境 既然是开发API应用，那必然是需要开发环境的。如果你自己会，那就最好不过了，如果不是太熟悉，可以参考一下我这篇文章： 这篇文章详细的介绍了在Mac内如何搭建AI环境，包括Tensorflow的安装等。 基本上，我们现在需要的是3.10 的Python环境，还有Conda（我习惯用这个），然后在本地安装好Jupyter lab, 如下： 123conda create --name gpt python=3.10conda activate gptconda install -c conda-forge jupyterlab ipywidgets openai 这一段命令的意思是创建一个名为 gpt的python 3.10的开发环境，然后切换到这个环境里，再安装必要的包。 在后面的使用过程中，当然你可以选择jupyter notebook, 也可以和我一样，使用VSCode。 当然，你也可以选择Colab，其实这也是一个Jupyterlab，如果你不想本地搭建环境，那就直接使用Colab吧，不过注意一点，需要科学上网。就算你本地有环境，我还是建议你有些事后使用Colab，能用到一些免费的GPU资源，我的M1没有好的显卡支持，很多时候还是需要上Colab。 使用时候，记得要安装openAI的库，并且设置自己的API Key： 12!pip install openai%env OPENAI_API_KEY=&quot;这里输入你的API Key&quot; 测试一下 让我们现在开始依次写完这段代码，虽然截图内已经有了，但是还是让我们一步步来执行起来，这一段代码，并不是出自我之手，而且直接借鉴的徐文浩的代码： 123456789101112131415161718192021222324252627282930313233import openaiimport json# 设定API Key和模型openai.api_key = &quot;输入你自己的代码&quot;COMPLETION_MODEL = &quot;text-davinci-003&quot;# 设定关键词和描述prompt = &quot;&quot;&quot;Consideration proudct : 工厂现货PVC充气青蛙夜市地摊热卖充气玩具发光蛙儿童水上玩具1. Compose human readale product title used on Amazon in english within 20 words.2. Write 5 selling points for the products in Amazon.3. Evaluate a price range for this product in U.S.Output the result in json format with three properties called title, selling_points and price_range&quot;&quot;&quot;# 写一个调用方法def get_response(prompt): completions = openai.Completion.create ( engine=COMPLETION_MODEL, prompt=prompt, max_tokens=512, n=1, stop=None, temperature=0.0, ) message = completions.choices[0].text return message# 调用方法并打印最终结果print(get_response(prompt)) 然后我们就可以看到返回了： 1234567891011{ &quot;title&quot;: &quot;Glow-in-the-Dark Inflatable PVC Frog Night Market Hot Selling Water Toy for Kids&quot;, &quot;selling_points&quot;: [ &quot;Made of durable PVC material&quot;, &quot;Glow-in-the-dark design for night play&quot;, &quot;Inflatable design for easy storage and transport&quot;, &quot;Perfect for water play and outdoor activities&quot;, &quot;Great gift for kids&quot; ], &quot;price_range&quot;: &quot;$10 - $20&quot;} 这段代码里面，我们调用了 OpenAI 的 Completion 接口，然后向它提了一个需求，也就是为一个我在 1688 上找到的中文商品名称做三件事情。 为这个商品写一个适合在亚马逊上使用的英文标题。 给这个商品写 5 个卖点。 估计一下，这个商品在美国卖多少钱比较合适。 同时，我们告诉 OpenAI，我们希望返回的结果是 JSON 格式的，并且上面的三个事情用 title、selling_points 和 price_range 三个字段返回。 神奇的是，OpenAI 真的理解了我们的需求，返回了一个符合我们要求的 JSON 字符串给我们。在这个过程中，它完成了好几件不同的事情。 第一个是翻译，我们给的商品名称是中文的，返回的内容是英文的。 第二个是理解你的语义去生成文本，我们这里希望它写一个在亚马逊电商平台上适合人读的标题，所以在返回的英文结果里面，AI 没有保留原文里有的“工厂现货”的含义，因为那个明显不适合在亚马逊这样的平台上作为标题。下面 5 条描述也没有包含“工厂现货”这样的信息。而且，其中的第三条卖点 “Inflatable design for easy storage and transport”，也就是作为一个充气的产品易于存放和运输，这一点其实是从“充气”这个信息 AI 推理出来的，原来的中文标题里并没有这样的信息。 第三个是利用 AI 自己有的知识给商品定价，这里它为这个商品定的价格是在 10～20 美元之间。而我用 “Glow-in-the-Dark frog” 在亚马逊里搜索，搜索结果的第一行里，就有一个 16 美元发光的青蛙。 最后是根据我们的要求把我们想要的结果，通过一个 JSON 结构化地返回给我们。而且，尽管我们没有提出要求，但是 AI 还是很贴心地把 5 个卖点放在了一个数组里，方便你后续只选取其中的几个来用。返回的结果是 JSON，这样方便了我们进一步利用返回结果。比如，我们就可以把这个结果解析之后存储到数据库里，然后展现给商品运营人员。 接下来，我们再看一个其他的例子： 1234567prompt = &quot;&quot;&quot;Man Utd must win trophies, says Ten Hag ahead of League Cup final请将上面这句话的人名提取出来，并用json的方式展示出来&quot;&quot;&quot;print(get_response(prompt)) 得到输出结果： 123{ &quot;names&quot;: [&quot;Ten Hag&quot;]} 看出AI干了什么吗？其实从中文中你能知道我需要AI做什么，而他完完全全输出了我想要的。 我们这里的两个例子，其实对应着很多不同的问题，其中就包括机器翻译、文本生成、知识推理、命名实体识别等等。在传统的机器学习领域，对于其中任何一个问题，都可能需要一个独立的机器学习模型。就算把这些模型都免费提供给你，把这些独立的机器学习模型组合到一起实现上面的效果，还需要海量的工程研发工作。没有一个数十人的团队，工作量根本看不到头。然而，OpenAI 通过一个包含 1750 亿参数的大语言模型，就能理解自然的语言输入，直接完成各种不同的问题。而这个让人惊艳的表现，也是让很多人惊呼“通用人工智能（AGI）要来了”的原因。 这两个例子虽然简单，但是咱们暂时先到此为止，记得课后好好练习。 请将今天课程中提供的示例代码，在你搭建的开发环境中运行一下。 你可以去看一下 OpenAI 提供的示例，找几个你感兴趣的用途，在上面的开发环境里运行体验一下，你也可以脑洞大开，尝试一些你想用 AI 解决的问题，看看 AI 能不能给出你想要的结果。 推荐阅读 推荐阅读如果你想知道 GPT 系列大模型到底是怎么回事儿，我推荐你去看一下李沐老师讲解 GPT 系列论文的视频 GPT、GPT-2、GPT-3 论文精读，这个视频深入浅出，能够让你理解为什么现在 GPT 那么火热。","link":"/Enter-the-door-of-AI-learn-to-communicate-with-it/"},{"title":"扩展欧几里德","text":"PE -05欧几里德算法 又名「辗转相除」法 迄今为止已知的最古老的算法, 距今(2017年)2317年 用于快速计算两个数字的最大公约数 还可以用于快速求解ax + by = 1方程的一组整数解 扩展欧几里德 已知算法上推导其它算法的流程, 思想过程才是最重要的;","link":"/Extended-Euclid-algorithm/"},{"title":"Exploring the Potential and Challenges of Hybrid Machine Learning Systems in AI","text":"随着机器学习和深度学习的飞速发展，人工智能（AI）正取得飞跃性进展。然而，越来越多的研究者一致认为，AI演进的下一个阶段在于开发混合机器学习系统。这篇博客文章将探讨这个新兴领域，讨论它的潜力、挑战以及对AI未来的影响。 什么是混合机器学习系统？ 混合机器学习系统结合了两个或更多的机器学习模型或技术，创建出一个更强大、更灵活的AI解决方案。这些系统可以发挥每个组成模型的优势，同时弥补它们各自的弱点。组合机器学习模型的方法有多种，例如： 集成学习：将多个基础模型结合成一个更强大的模型。这可以通过bagging、boosting和stacking等技术来实现。 多模态学习：集成不同的数据来源（例如文本、图像和音频）以创建更丰富的数据表示，并提高整体性能。 迁移学习：利用从一个领域或任务中获得的知识，以改善另一个领域或任务中的性能。 元学习：训练模型学习如何学习，使它们能够更快地适应新任务。 混合系统的潜力 混合机器学习系统有潜力彻底改变AI，为解决复杂问题开辟新的可能性。这些系统的一些关键优势包括： 改善性能：通过结合多个模型，混合系统可以实现比任何单个模型更好的性能。对于过于复杂以至于单个模型无法有效解决的问题尤为如此。 鲁棒性：混合系统可以更好地抵御噪声、过拟合和其他影响单个模型的问题。这在现实世界中的应用中尤其重要，因为数据通常是嘈杂和不完美的。 通用性：混合系统可以处理各种问题、数据类型和任务，这使它们高度适应各种行业和应用。 可迁移性：混合系统可以更轻松地利用从一个领域或任务中获得的知识，以改善另一个领域或任务中的性能，这使它们非常适合具有有限训练数据的任务。 开发混合系统的挑战 尽管有潜力，混合机器学习系统也面临着一些挑战。其中最显著的障碍包括： 复杂性：设计和实施混合系统可能比单个模型更加复杂。研究人员和实践者需要仔细考虑如何最好地组合模型和技术，以创建一个有效的系统。 可伸缩性：混合系统的增加的复杂性可能使它们更难以扩展，无论是在计算资源方面还是处理大量数据的能力方面。 可解释性：混合系统可以更具挑战性地解释和说明，因为它们涉及多个模型和技术的交互。这可能使得理解系统如何做出决策并确保其正确运行变得更加困难。 训练和适应：与训练单个模型相比，训练混合系统可能需要更多的计算资源和时间。此外，将这些系统适应到新任务或不断变化的条件可能需要大量的工作。 总之，混合机器学习系统代表了AI未来的一个有希望的方向。通过利用多个模型和技术的优势，这些系统有潜力在性能、鲁棒性和通用性方面取得重大进展。然而，实现这个潜力需要克服与复杂性、可伸缩性、可解释性和训练相关的挑战。随着研究人员和实践者继续探索这个令人兴奋的领域，我们可以期待在各种行业和应用中看到混合机器学习系统的大量进展。","link":"/Exploring-the-Potential-and-Challenges-of-Hybrid-Machine-Learning-Systems-in-AI/"},{"title":"04 GPT-3&#x2F;4对比其他模型胜在哪？","text":"大家好，我是茶桁。 在前两节课中，我们一起体验了 OpenAI 提供的 GPT-3.5 系列模型的两个核心接口。一个是获取文本的 Embedding 向量，另一个是根据提示语生成补全的文本内容。通过这两种方法，我们可以在零样本或少样本的情况下进行情感分析任务。然而，你可能会有两个疑问。首先，Embedding 不就是将文本转换为向量吗？为什么不直接使用开源模型（如Word2Vec、Bert）而要调用 OpenAI 的 API 呢？在我们的情感分析任务中，我们进行了一些简化处理。一方面，我们排除了相对中性的评分（3分）；另一方面，我们将1分、2分和4分、5分合并，将原本需要判断5个分类的问题简化了。那么，如果我们想要准确预测多个分类，是否也能如此简单呢？ 在本节中，我们将通过代码和数据来回答第一个问题，尝试使用常见的开源模型，看看是否可以通过零样本学习的方式取得良好的效果。至于第二个问题，我们将在下节课中探讨，探索如何进一步利用 Embedding 结合机器学习算法来更好地处理情感分析问题。 什么是预训练模型？ 预训练模型是指通过在大规模文本数据上进行学习而生成的模型，能够将文本转化为语义丰富的向量表示。OpenAI 的 GPT-3 是一种超大规模的预训练模型，其英文全称为“Generative Pre-trained Transformer”，即生成式预训练 Transformer。通过预训练模型，我们可以在没有见过具体问题的情况下，利用大量可用的文本数据进行学习。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/GPT-3-VS-Other-Model/"},{"title":"系列课程：从零开始接触人工智能大模型（介绍）","text":"整个系列课程内容虽然为自己所写，但是参考了bothub 创始人徐文浩的课程《AI 大模型之美》 人工智能是计算机科学领域中最具前瞻性和影响力的技术之一。它是一种智慧型算法，能够模拟人类的思维过程，处理大量的数据和信息，从而发现隐藏在其中的规律和趋势。人工智能的应用范围非常广泛，包括语音识别、图像识别、自然语言处理、机器翻译、智能推荐、智能问答、自动驾驶等等。 人工智能的发展历程可以追溯到上个世纪五六十年代。当时，计算机科学家们开始思考如何让计算机能够像人类一样思考和行动。1956年，美国达特茅斯学院举办了一次名为“人工智能”的会议，正式提出了人工智能的概念。自此以后，人工智能的研究和应用就成为了计算机科学领域的一项重要任务。 随着科技的不断进步，人工智能技术得到了长足的发展。各种机器学习算法、深度学习算法、开源的软件包以及云平台提供的解决方案不断涌现，为企业提供了各种智能化的产品和服务。例如，通过图像识别技术，我们可以将人脸识别、车牌识别、病变识别等技术应用于安防、交通、医疗等领域；通过自然语言处理技术，我们可以实现智能客服、智能翻译、智能问答等功能，提升用户体验和效率；通过机器学习技术，我们可以实现推荐系统、广告精准投放等功能，提高市场竞争力。 然而，人工智能领域仍然存在着挑战和困难。其中一个难点就是“有多少人工就有多少智能”这个诅咒。很多“智能”都来自于大量的人工数据标注和硬编码的业务规则，导致人工智能在某些特殊情况下表现得像“人工智障”。因此，如何提高人工智能的效率和性能，仍然是人工智能领域需要解决的问题之一。 去年 12 月，我第一次尝试与 ChatGPT 进行交互。一开始我并没有对这个新的 AI 聊天机器人抱有太高的期待，毕竟以前的聊天机器人总是表现得像“人工智障”。但是，ChatGPT 证明了我的想法是错误的。在与它交流了几分钟后，我决定让它帮我写一些 Bash 脚本和 SQL 代码。我很惊讶地发现，它不仅完全理解了我的需求，还精确地写出了我需要的复杂 SQL 代码。这次体验让我对人工智能的潜力有了新的认识，也让我更加期待未来人工智能的发展。 对于复杂的需要窗口函数的 SQL，ChatGPT 写得比我快多了。 从年前到目前（5月份）为止，我一直在体验市面上大部分的人工智能工具，例如最近非常火热的ChatGPT等。这样的体验让我对人工智能的潜力有了新的认识，也让我开始思考各行业未来的前景，并提出了一些担忧。当我们看到人工智能技术不断取得突破性进展的同时，也会担心它是否会取代人类的工作，进而给社会带来不稳定的因素。然而，我相信人工智能的发展是为了更好地服务人类，而非取代人类。我们需要更多人去了解和掌握人工智能技术，这样才能更好地应对未来的挑战，发挥人工智能技术的最大价值。 基于此，我想让更多人开始接触人工智能，并且学会如何利用人工智能，更甚为学习新一代AI应用编程。因此，我想介绍一门系列课程：从零开始接触人工智能大模型。该课程将介绍人工智能的基本概念、常见应用场景以及如何利用最新的AI技术构建自己的AI应用。我们将从浅入深地讲解人工智能相关的知识，帮助每个人都能够轻松上手，并且学会如何应用到自己的领域中去。不仅仅是算法工程师和机器学习研究人员，每个工程师都可以快速学习并参与开发新一代的AI应用。我相信，学会开发新一代AI应用是每个软件开发行业从业者都值得学习的，无论是产品经理还是工程师，乃至于行业之外的业务人员，都应该拥抱新的AI浪潮。 在学习的过程中，我们将讨论人工智能的应用场景，例如语音识别、图像识别、自然语言处理、机器翻译、智能推荐、智能问答、自动驾驶等等。这将有助于我们了解人工智能技术在不同领域的应用，从而更好地把握未来的发展方向。同时，我们也会学习最新的人工智能技术，例如大模型、自监督学习等等。这些技术的出现，为人工智能的应用提供了更加广阔的空间和深度。 在这个充满挑战和机遇的时代，我们需要准备好迎接未来的挑战。学习人工智能技术，是每个软件开发行业从业者都需要掌握的技能。无论是产品经理还是工程师，乃至于行业之外的业务人员，都应该拥抱新的AI浪潮，学习开发新一代的AI应用。我相信，通过学习新一代的AI应用编程，我们能够更好地应对未来的挑战，为我们的生活和工作带来更多的便利和机遇。 学习成本那么高，给我一个理由先 这个应该是普遍的一个想法，其实对于此，我将不仅给你一个理由，而是给你三个： 1. 开发门槛降低，人人可学习AI应用开发 人人都应该学习如何开发新一代 AI 应用，因为这一轮的 AI 浪潮里，开发新的 AI 应用的门槛大大降低了。过去，AI 应用开发是一个门槛比较高的领域，需要掌握大量的机器学习和深度学习的知识，了解各种基础模型，使用各种机器学习的编程框架，以及理解在实际应用里锤炼机器学习的各种实战技巧和模型。对于没有相关经验的人来说，不花上个一两年时间，你可能很难用 AI 算法做出有价值的产品。 但是现在，随着预训练好的大型基础模型的出现，以及这些模型的能力通过开放 API 的形式提供出来，即使没有任何机器学习的理论知识，你只需要一两天时间，就能做出一个能解决实际问题的 AI 应用。比如，最近在 GitHub 上就能看到很多工程师，花上 1-2 天时间就做出来的图书翻译、人工智能语音对话的应用。 这样的开发方式，让更多的人有机会参与到 AI 应用的开发中来。无论你是产品经理、UI/UE 设计师、前端开发、后端开发还是大数据团队的人员，都可以通过学习一些基本的 AI 应用开发技能，为自己的职业生涯增添新的技能和竞争力。特别是在当前的科技革命和数字化转型浪潮下，AI 技术已经逐渐渗透到各个行业中，很多企业已经开始了 AI 落地实践，而能够掌握 AI 技术的人才也逐渐成为各个行业中的稀缺资源。因此，学习如何开发新一代 AI 应用，也是提升自己职业竞争力的一种重要途径。 最后，学习如何开发新一代 AI 应用还可以让我们更好地了解 AI 技术的本质和应用，拓宽我们的知识面和视野。AI 技术正在改变我们的生活和工作方式，了解和掌握这些技术，也有助于我们更好地适应未来的发展和变化。 学习如何开发新一代 AI 应用对于个人的职业发展和未来非常重要，因为 AI 技术已经开始在各个行业得到广泛应用。无论你从事什么行业，都可以利用 AI 技术来提高效率、降低成本、提供更好的服务，并在竞争中脱颖而出。掌握 AI 技术也可以让你在未来的就业市场上更有竞争力，拥有更广泛的职业选择。因此，学习如何开发新一代 AI 应用可以为个人的职业发展和未来打下坚实的基础。 2. 站在巨人的肩膀上 随着人工智能技术的迅猛发展，AI应用开发的范围也越来越广泛，涉及到自然语言处理、计算机视觉、语音识别等多个领域。这一轮的AI浪潮已经开始让我们看到了通用人工智能（AGI）的雏形，AI应用的覆盖领域被大大扩展了，几乎任何一个问题都有机会通过AI来解决优化。 过去，机器学习模型的应用通常局限于某一个细分领域上的进步，而且对于每一个具体问题都要单独收集数据、训练单独的机器学习模型来解决里面某一个小问题。然而，随着计算能力的提高和模型规模的增加，现在拥有海量参数的大模型已经开始成为主流。例如，2020年发布的GPT-3模型拥有1750亿个参数，可以无需任何微调，就能解决情感分析、命名实体识别、机器翻译等一系列的自然语言处理问题。同时，对于很多AI没有见过的问题，只要通过自然语言给到AI几个例子，通过\"小样本学习\"，AI就能给出正确的回答。这意味着，一个大模型可以一次性解决不同类型的很多问题。 在计算机视觉上，像2021年OpenAI发布的CLIP模型也有类似的效果。通过4亿个（图片、文本）对的训练，对于图片的分类可以任意扩展，而不需要预先标注。这样的模型使得我们对于图片的分类不再局限于预先的人工数据标注的类别，而是可以扩展到任何类别上去。 这种发展趋势使得AI应用开发的门槛逐渐降低，使得普通人也能够参与到AI应用的开发中来。无论你所在的行业和领域，都有机会通过简单的AI应用开发，提升效率和产出。同时，了解和掌握AI技术也成为了一种职业竞争力，可以帮助你更好地适应未来的工作环境。 总之，AI技术的广泛应用和快速发展已经让AI应用开发成为一个非常重要的技能。了解AI技术的人，无论是在工作中还是在日常生活中，都会受益匪浅。 3. 人工智能对我们的工作的影响 人工智能（AI）已经开始以多种方式改变我们的生活。我们已经习惯了依赖AI进行日常任务，如编写代码、翻译文本，甚至为文章生成图像。然而，AI的影响超出了我们的个人生活，它将对我们的工作产生重大影响。 随着AI的不断发展，它不可避免地将接管许多以前由人类执行的任务。公司已经在使用AI来优化产品描述、搜索算法和其他曾经是人工工作者领域的任务。这无疑会导致工作的流失，并改变许多人的工作性质。 尽管存在工作流失的可能性，但那些拥抱AI的人无疑将会获得好处。使用AI的团队和公司很可能会看到更高的效率和生产力，从而导致更大的产出和成功。无论您是产品经理、工程师、运营专家还是平面设计师，AI的出现都将从根本上改变您的工作性质。 AI将作为助手，帮助我们完成简单的基于知识的任务，甚至提供创造性的灵感。事实上，有些人将AI的发展与工业革命相比，标志着我们生活和工作方式的根本变化。虽然这可能会对一些工人造成危机，但有机会拥抱这种变化并学习在以AI为驱动的未来需求的新技能。 正如2008年App Store的发布创造了对移动应用程序开发人员的需求一样，AI革命为那些愿意学习和适应的人带来了新的机会。无论是获得机器学习、数据科学还是其他与AI相关的领域的专业知识，那些积极应对不断变化的就业市场的人无疑会蓬勃发展。 AI对我们的工作的影响不容小觑。尽管有些人可能将其视为危机，但有机会拥抱这种变化并学习在以AI为驱动的未来需求的新技能。每个人都可以决定如何应对这种变化，以及他们是否会抓住它带来的机会。 如何学习呢？ 新一代AI应用开发是一个快速发展的领域，需要不断更新自己的知识和技能。而通过实践学习是最有效的方法之一，因为它可以让你在实际解决问题的过程中学习和掌握技能。 这门课程采用实践教学的方式，让学生能够亲自动手解决一系列实际问题，如情感分析、记忆聊天机器人、图像搜索等。学生们将通过编写几行或几十行的代码来解决这些问题，并在在线Notebook的环境下进行代码运行，无需搭建复杂的开发环境。即使你是产品经理或业务方，也可以轻松地体验到新一代AI应用的开发过程，从而更好地理解和掌握其工作原理。 除了OpenAI的API外，这门课程还涵盖了语音、视觉等应用场景，包括语音识别、语音合成、AI绘画等。学生们将了解到如何使用开源模型以及如何根据自己的数据微调这些模型，从而更好地满足不同场景下的需求。 此外，这门课程还将探讨AI应用的套路和方法，例如分类、搜索、推荐、问答等问题。学生们将学习如何使用现有模型的能力来解决这些问题，并将这些方法和套路应用到现有的业务系统中，以提高应用的体验和效率。 随着课程的深入，学生们还将学习如何组合多个API、开源模型和开源库来解决复杂的真实问题。例如，如果你想实现一个电商客服，不仅需要检索知识库和问答的能力，还需要连接现有的订单和物流信息的能力。学生们将学习如何在AI应用的开发过程中将复杂的业务流程串联起来，以更好地应对实际问题。 拥抱新时代，接受“通用人工智能” 随着人工智能技术的不断进步，越来越多的人开始认识到“通用人工智能”（AGI）的重要性和可能性。如今，AGI已经不再是一个遥不可及的概念，而是一个即将到来的现实。我们可以看到，各个领域的科学家和工程师正在努力推进AGI的研究和应用，希望通过人工智能技术的创新和发展来实现这一目标。 在这个变化迅速的时代，我们需要尽快拥抱AGI。AGI可以帮助我们解决许多复杂的问题，并且能够极大地改善我们的生活质量。例如，我们可以使用AGI来开发更为智能的医疗设备，提高医疗诊断的准确性和效率；我们也可以利用AGI来优化城市交通，减少交通堵塞和污染；此外，AGI还能为我们提供更好的教育和娱乐体验，让我们的生活更加丰富多彩。 因此，我们需要尽快投入时间和精力来学习和应用AGI技术。学习AGI不仅可以让我们掌握更加先进的技能和知识，还可以激发我们的创新和热情，让我们更好地适应这个变化迅速的时代。我们需要通过学习和实践，尽快将AGI技术应用到我们的工作和生活中，让其发挥最大的价值。 最后，我希望每一个人都能积极地拥抱AGI技术，努力学习和应用这项技术，为推动人工智能技术的发展贡献自己的力量。相信在不久的将来，AGI将成为我们生活中不可或缺的一部分，让我们共同期待并努力实现这一目标。 目录 注： 这里将会是我未来所有系列教程的目录，便于大家更快找到相关章节 导读：了解AI并使用它/他/她们","link":"/Gettin_started_with_large-scale_artificial_intelligence_models_from_scratch/"},{"title":"09 使用Embedding实现语义检索","text":"Hi，我是茶桁。 过去的8讲，你已熟悉Embedding和Completion接口。Embedding适合用于机器学习中的分类、聚类等传统场景。Completion接口可以用作聊天机器人，也可以用于文案撰写、文本摘要、机器翻译等工作。 然而，很多同学可能认为这与他们的日常工作无关。实际上，我们通常在搜索、广告、推荐等业务中使用自然语言处理技术。因此，我们今天来看看如何使用OpenAI的接口来帮助这些需求。 当涉及到优化搜索结果时，OpenAI的Embedding接口可以提供有价值的功能。Embedding接口能够将文本转换为表示其语义特征的向量，这些向量可以用于比较文本之间的相似性，从而优化搜索结果的排序和相关性。 首先，使用OpenAI的嵌入接口，您可以将搜索查询和搜索结果中的文本转换为嵌入向量。通过比较查询向量与结果向量之间的相似度，您可以重新排列搜索结果，以提供更相关和有用的结果。这可以帮助用户更快地找到他们想要的信息，并提供更好的搜索体验。 其次，OpenAI的嵌入接口还可以帮助您改进搜索结果的相关性。通过将用户的上下文和历史记录与搜索查询结合起来，您可以生成更具个性化和定制化的搜索结果。使用嵌入接口，您可以将用户的上下文信息转换为嵌入向量，并与查询向量进行比较，以确定最相关的结果，并在搜索结果中突出显示这些个性化的内容。 此外，OpenAI的嵌入接口还可以用于相似性搜索和聚类分析。您可以使用嵌入向量来比较不同文本之间的相似性，并将相似的文本聚集在一起。这有助于在搜索结果中提供更多相关的选项，并帮助用户发现相关但可能未被明确搜索的内容。 下面，就让我们来一步步的实现： 生成实验数据 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Implementing-semantic-retrieval-using-Embedding/"},{"title":"2. 初识Python脚本","text":"Python的系列课程是写给零基础的小白看的，如果基础比较好的小伙伴可以暂时先不关注，等待后续课程。 Hi， 大家好，我是茶桁。 之前的课程已经给大家讲解了Python相关特性和基本语法。那么这节课呢，我们尝试着从最简单的脚本来开始认识Python。 在开始这节课之前呢，我是默认大家已经安装好了VSCode，并且配置好了Jupyter环境和Python的基本环境。如果在这一步有疑问的小伙伴，可以留言私信我。 我们在终端里输入: 12$ python -VPython 3.10.11 可以看到自己的Python版本。 这个时候，我们其实已经可以在终端里进行Python的代码编辑了，如下： 123$ python&gt;&gt;&gt; print(&quot;Hello AI Cheats&quot;)Hello AI Cheats 如下图： 我们这里需要理解一下，编写python程序的文件，称为python的脚本或程序。要求我们当前的python脚本的文件后缀名必须是.py，如果是Jupyter的文件，后缀是.ipynb print() 输出函数 print可以在程序中输出一些内容，如字符串，数字等等。 函数就是为了完成一些功能，例如： print就是为了输出数据。 变量 变量就是用一个英文字符串来记录或标记一些数据，并且这个被标记的数据是可以变化的。 比如 num = 10，就是把数据10赋值给了变量num来使用，之后就可以使用num来代替这个10的数据。 命名规范 这里我们强调一下Python的命名规范，所有在教授编程的教程中最初一定都会强调规范性。 变量的命名规范如下： 变量名可以使用字母，数字，下划线_， 不能以数字开头 严格区分大小写 不要使用中文 不要使用关键字 if else True False print 当然具体关键字并不仅仅是这些，这里列出了Python的关键字，大家可以自行查看一下，命名的时候需要进行避免。 变量的定义方式 在遵循了变量命名规范之下，我们可以有以下几种定义方式： 123456# 第一种定义方式a = 10b = 20# 第二种定义方式a,b = 30, 40 这里我们来思考一个问题，如何实现两个变量的数据相互交换呢？ 123456# 定义两个变量a = 10b = 20# 交换两个变量的值... 如果使用最普通的方式完成变量数据的交换，那么我们可以使用如下步骤： 把a变量的值 赋值给c ，此时 c变量中 就是 10 把b变量的值 赋值给a ， 此时 a变量中 就是 20 把c变量的值 赋值给b ， 此时 b变量中 就是 10 123456789# 普通方式交换数据a = 10b = 20c = aa = bb = cprint(a, b) 输出结果： 120, 10 我们还可以利用python定义比变量的语法来实现变量的数据交换 12345# 定义比变量的语法方式a = 10b = 20a,b = b,aprint(a, b) 输出结果： 120,10 Python的数据类型 数据类型就是数据的表现形式，比如 “你好” 就是一个字符串，200 就是一个数字。 在程序当中除了这种常用的字符和数字外还有很多其它的数据表现形式。 在Python中，我们可以使用type()函数来返回当前数据的数据类型： 123s = 'ilovechaheng'res = type(s)print(res) 输出结果： 1&lt;class 'str'&gt; 字符串类型 单双引号都可以定义字符串 三引号也可以定义字符串 单双引号定义的字符串不能随意换行，需要在换行时指明换行符 字符串中的引号可以互相嵌套，但是不能嵌套自己（例如不能在单引号中嵌套单引号，除非转义） 字符串中可以使用转义字符，如 .. 如果不想在字符串中实现转义字符可以在字符定义时 加 love = r'\\nihao \\shijie' 123456789# 单引号和双引号进行定义love = 'iloveyou'hello = &quot;你好 世界&quot;# 使用三引号实现大字符串的定义，一般用于大文本字符串的定义，并且大字符串，可以换行s = '''比如说这一个很长很长的文章内容。。。''' 数字类型 int 整型 float 浮点类型 complex 复数 bool 布尔类型（True，False） 12345678910111213141516171819# 数字类型 Numbervarn = 521varn = -1111varn = 3.1415926varn = 0x10 # 十六进制varn = b'001100111' # bytes# 复数varn = 5+6j # complex# 布尔类型 boolvarn = Truevarn = False# print(varn,type(varn))# 数值类型可以参与运算a = 10b = 20print(a+b) # 输出结果 30 List列表类型 列表用来表示一系列数据，例如： 需要记录一组数字或其它数据 列表中存储的数据可以是任意类型的 在需要记录多个数据时，可以使用中括号进行定义 [], 并且每个数据之间使用逗号分隔 , 例如以下数据，定义了几组数字 列表中存储的每一组数据，称为元素 列表中存储的数据，可以通过下标的方式进行获取 那么列表中元素的值可不可以存储一个列表,称为 二级列表（二维列表） 或者 多级列表 （多维列表） 关于列表中的下标，正读和反读的正负号是不一样的： 12345678# 关于列表中的下标''' 0 1 2 3 4 ['a','b',521,'pai',3.1415926] -5 -4 -3 -2 -1'''a = ['a','b',521,'pai',3.1415926]print(a[-3]) 输出结果： 1521 tuple 元组类型的定义 在定义多个数据内容时，可以选择使用List列表类型 还可以使用元组类型来定义， 元组和列表非常像，都时用于存储多个数据时使用 元组使用小括号进行定义（），列表使用中括号进行定义 元组的最大特点就是值不能被改变 123vart = (1,2,3,'a','b')# 元组的其它定义方式vart = 1,2,3 注意在定义元组时，如果元组中只有一个元素，那么需要加, 不然就不是元组类型了 Dict字典类型 字典也是用于存储一组或多组数据时使用，使用大括号 {}来定义 字典是 键值对 的存储方式 name ：admin 键和值之间使用冒号进行分隔，多组键值对之间使用逗号分隔 键必须是字符串或数字类型，值可以是任意类型 键名不能重复，值可以重复 12345678910# 比如需要记录一本书的相关数据 书名，作者，价格，。。。vard = {'title':'&lt;&lt;鬼谷子&gt;&gt;','author':'鬼谷子','price':'29.99'}# print(vard,type(vard))# {'title': '&lt;&lt;鬼谷子&gt;&gt;', 'author': '鬼谷子', 'price': '29.99'} &lt;class 'dict'&gt;# 获取字典中的值print(vard['title'])# 字典中的键不能重复使用，否则会覆盖vard = {'a':10,'b':10,'c':20,'a':'aa',1:'abcdef','2':'2222'}print(vard) 输出结果： 12&lt;&lt;鬼谷子&gt;&gt;{'a': 'aa', 'b': 10, 'c': 20, 1: 'abcdef', '2': '2222'} 在python之前的版本中，字典是无序的 set集合类型 set集合是一个 无序且元素不重复的 集合的数据类型 set集合使用 中括号或者set()方法来定义 如果需要定义一个空集合时 只能使用 set()方法,因为大括号时定义的空字典 集合主要用于运算，交集，差集，并集，对称集合 123456789101112131415161718a = {1,2,3,'a'}# 给集合添加元素# a.add('b')# 无法获取集合中的单个元素，但是可以添加和删除# a.discard('a')# print(a)# 检查当前的元素是否在集合中# print(1 in a)# 集合主要用于运算，交集，差集，并集，对称集合a = {1,2,3,'a','b'}b = {1,'a',22,33}print(a &amp; b) # 交集 {1, 'a'}print(a - b) # 差集 {'b', 2, 3} a 集合有，b集合没有的print(a | b) # 并集 {1, 2, 3, 33, 'a', 'b', 22} 两个集合，放到一起，并且去重print(a ^ b) # 对称差集 {33, 2, 3, 'b', 22} 输出结果： 1234{1, 'a'}{2, 3, 'b'}{1, 2, 3, 'a', 33, 22, 'b'}{33, 2, 3, 22, 'b'} 总结 最后，让我们来进行总结一下，关于Python的数据类型可以查看如下列表： 12345678910111213141516字符串 string数字类型 Number 整型 int 浮点 float 复数 布尔 bool列表 list元组 tuple字典 dict集合 set可变数据类型：列表，字典，集合不可不数据类型： 字符串，数字，元组容器类型数据 ： 字符串，列表，元组，集合，字典非容器类型数据： 数字，布尔类型 数据类型转换 什么是数据类型转换？ 把一个数据类型转换为另一个数据类型，例如 字符串转为数字 为什么需要数据类型转换？ 因为不同的数据类型之间不能运算 数据类型转换的形式？ 自动类型转换 强制类型转换 自动类型转换 当两个不同的值进行运算时，结果会向更高的精度进行计算：True ==&gt; 整型 ==&gt; 浮点 ==&gt; 复数 12345a = 123b = True # 在和数字运算时 True转为数字1，False转为数字 0print(a+b)print(12.5+22)print(True+3.14) 输出结果： 12312434.54.140000000000001 强制类型转换 python中的每个数据类型都有对应的方法，可以对数据类型进行转换 str()可以把所有的其它数据类型转换为字符串类型 int()字符串转数字类型时，如果字符串中是纯数字，可以转换，其它容器类型不能转为数字int类型 float()浮点类型的转换和int类型一样，不过转换的结果是浮点类型 bool() 可以把其它类型转换布尔类型的True或False 以下情况转bool的结果是 False: '',0,0.0,False,[],{},(),set() list()列表 数字类型是 非容器类型，不能转换为列表 字符串 转换为列表时 会把字符串中的每一个字符当做列表的元素 集合 可以转换为 list列表类型 元组 可以转换为 list列表类型 字典 可以转换为 list列表类型,只保留了字典中的键 tuple()元组 数字类型 非容器类型，不能转换为元组 其它容器类型的数据进行转换时，和列表一样 set()集合 数字类型 非容器类型，不能转换为 集合 字符串,列表，元组 可以转为 集合 结果是无序的 字典转换为集合时，只保留了字典的键 key dict()字典 数字类型 非容器类型，不能转换为 字典 字符串不能直接转换为 字典 列表可以转换为字典，要求是一个二级列表，并且每个二级元素只能有两个值 元组可以转换为字典，要求是一个二级元组，并且每个二级元素只能有两个值","link":"/Introduction-to-Python-scripting/"},{"title":"在 Apple Silicon M1&#x2F;M2 Mac 上安装和运行Stable Diffusion","text":"说实话，我找了好多关于如何在 M1/M2 上安装和运行 Stable Diffusion 的教程和帖子，发现相互之间借鉴的不少，但是能用的确实没几个。 寻找一番后，发现其实没那么复杂。也不知道为什么网上的那么多教程搞得那么复杂，又是这个又是那个的一大堆，简单实现的方式有好几种： 1. Diffuers 这是可以在 App Store 上直接搜索并下载的一个 App，看评分和排名似乎都不太好，开发者却是「Hugging Face」，其实在官方的 Github 上就有其下载链接：‣ 这应该是体验 Stable Diffusion最简便的方式了吧。而且还支持选择 Model， 不过有点遗憾的点是没办法调整参数。 2. DiffusionBee 这是出现的比较早的一款第三方 App，使用起来也是特别简单，直接下载安装就行了： DiffusionBee - Stable Diffusion App for AI Art DiffusionBee is the easiest way to generate AI art on your computer with Stable Diffusion. Completely free of charge. https://diffusionbee.com/download 目前不止是 MacOS，还有对应 Windows 64 Bit 的版本，而且，你可以选择下载 HQ Version 版本。官方对其说的是速度慢两倍，但是图像质量更好。 以上两个 App 第一次使用的时候都是需要下载 Model 的，之后就可以直接开心的玩耍了，相比较而言，DiffusionBee 在参数选择上要多一点。支持 Text to Image, Image To Image等。 # 安装 AUTOMATIC1111 这个方法是需要有一点动手能力了，不过相比较而言，也不是那些网站上介绍的那么繁琐。其实就只需要几步而已。 1. 下载 Homebrew 一个包管理器，不太明白的朋友不需要管那些，操作就行了 打开你的终端，不明白什么是终端直接在你的搜索框里输入”终端”，或者”Terminal”, 就能看到了。 然后直接把下面的代码粘贴进去，回车，看着他跑就行了 1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; 2. 安装一些必须的软件包 等上面步骤跑完之后，再复制下面的代码，一样粘贴进去回车看着他跑： 1brew install cmake protobuf rust python@3.10 git wget 3. 下载 AUTOMATIC1111 存储库 等上面步骤跑完，在你的终端输入一下代码并回车 1cd ~ 以上命令是为了让你进入你在 Mac 电脑上的账户主目录，就是这个地址 1/User/xx/ 然后我们接下来的操作会在你这个目录下下载一个文件夹，名字叫 「stable-diffusion-webui」，这贴下面代码到你的终端里，然后回车 1git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui 命令跑完后，你就会渐渐自己名字的目录下多了一个 stable-diffusion-webui 的目录了，然后在终端进入这个目录，操作方法和上面一样 1cd ~/stable-diffusion-webui/ 在你的访达里你也进入这个目录，然后继续进入 models/Stable-diffusion， 这里是存放 Model 的地方。现在你需要下载一个 Model 存放进去，你可以直接在这里下载 1.5 model, 当然，如果你需要 2.1 的或者其他 model，可以去点击下面的链接进去自己下载一个合适的，然后扔到目录里。 Models - Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/models?sort=downloads&amp;search=stable+diffusion 然后你的目录应该会是这样： 然后你就可以尝试着跑你的 stable diffusion 了，刚才我们在终端里进入了 ~/stable-diffusion-webui/， 假设你还在这个位置，我们就可以直接输入： 1./webui.sh 然后去干些自己的事情吧，喝杯茶，看看书。要跑一会呢，特别是你网络不好的情况。 等到终端命令全部跑完后，打开你的 Safari，输入：http://127.0.0.1:7860/ 好了，可以把玩了。 4. 其他 是的，还没结束，还有一些要说的，其实在 Mac App Store 里搜索的话，你还能看到一些其他的 App 可以直接使用，比如： 反正都是免费的，尽量多试试，找到一个自己满意的。 另外，不管你用那种方法，你都需要知道一些good prompts for Stable Diffusion, 这里有一个地方可以看些别人的例子，不过不是那么容易打开： arthub.ai https://arthub.ai/ 还有啊，自己可以多测试一些 model，下下来把玩下。 祝你玩的愉快。全文完。","link":"/How-to-install-and-run-Stable-Diffusion-on-Apple-Silicon/"},{"title":"Knock 升级 -- 快速输入管理员密码","text":"本文知乎专栏 还记得之前介绍过的Knock么？只需要敲击两下就能快速解锁Mac的app。 其实这款App还是 @Rachel 介绍给我的，当时就觉得很酷，可是用下来之后，并没有觉得有很高的实用性。 不过，这次Knock升级了，除了解锁Mac之外，还可以再你需要root权限的时候免去输入管理员密码的麻烦，你所需要的，仅仅是敲击两下你的iPhone。 好吧，现在可以为自己的Mac设定一个超级复杂的密码了。","link":"/Knock-update/"},{"title":"16. Langchain让AI拥有记忆力","text":"你好，我是茶桁。 在之前的两讲中，我们深入了解了 Langchain 的 LLMChain 核心功能，它可以帮助我们链式地调用一系列命令，包括直接调用 OpenAI 的 API、调用其他外部接口或自己实现的 Python 代码。但这只是完成一个小任务所需的调用序列。除了这些，LangChain 还封装了许多其他功能，以便于我们开发 AI 应用。例如，让 AI 有“记忆力”，即记住我们的聊天上下文。我们在第 6 讲中制作的聊天机器人的例子就是这样。为了让 ChatGPT 知道整个聊天的上下文，我们需要将历史对话记录传递给它。但由于 Token 数量有限，我们只能保留最后几轮对话。最终，我们将此功能抽象为一个 Conversation 类。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Langchain%E8%AE%A9AI%E6%8B%A5%E6%9C%89%E8%AE%B0%E5%BF%86%E5%8A%9B/"},{"title":"LLMs的实用介绍","text":"在实践中使用LLMs的3个级别 这是关于在实践中使用大型语言模型（LLMs）系列文章的第一篇。在这里，我将介绍LLMs并提出三个使用它们的级别。未来的文章将探讨LLMs的实际方面，例如如何使用OpenAI的公共API、Hugging Face Transformers Python库、如何微调LLMs以及如何从头构建LLMs 什么是LLM？ LLM 是 Large Language Model 的缩写，是人工智能和机器学习中的最新创新。这种强大的新型人工智能在2022年12月随着 ChatGPT 的发布而迅速传播开来。 对于那些生活在人工智能热潮和技术新闻周期之外的人来说，ChatGPT 是运行在名为 GPT-3 的 LLM 上的聊天界面（现在在撰写本文时已升级到 GPT-3.5 或 GPT-4）。 如果你使用过 ChatGPT，显然这不是来自 [AOL Instant Messenger]（https://en.wikipedia.org/wiki/AIM_(software)) 或你的信用卡客服的传统聊天机器人。 这个聊天机器人感觉不同。 什么使得LLM“大”？ 当我听到“大型语言模型”这个术语时，我的第一个问题是，这与“常规”语言模型有何不同？ 语言模型比大型语言模型更通用。就像所有正方形都是矩形，但并非所有矩形都是正方形一样。所有LLM都是语言模型，但不是所有语言模型都是LLM。 所以LLM是一种特殊的语言模型，但是什么使它们与众不同呢? 有2个关键属性区分LLMs与其他语言模型。一个是数量上的，另一个则是质量上的。 数量上，LLM的区别在于模型中使用的参数数量。目前的LLM大约有10-1000亿个参数[1]。 质量上，当语言模型变得“大”时，会发生一些非凡的事情。它会展示出所谓的*** emergent properties***例如零-shot学习[1]。这些是当语言模型达到足够大的规模时，似乎突然出现的特性。 零样本学习 GPT-3（以及其他LLM）的主要创新在于它能够在各种情境下进行零样本学习[2]。这意味着ChatGPT可以执行一个任务，即使它没有被明确训练过。 尽管这对我们这些高度进化的人类来说可能不是什么大不了的事情，但是这种零样本学习能力与之前的机器学习范例形成了鲜明对比。 以前，为了获得良好的性能，模型需要明确地在它所要完成的任务上进行明确的训练。这可能需要1k-1M个预标记的训练示例。 例如，如果你想让计算机进行语言翻译、情感分析和识别语法错误。每个任务都需要一个专门的模型，它需要在大量标记示例的基础上进行训练。然而，现在，LLM可以在没有明确训练的情况下完成所有这些任务。 LLM如何工作？ 训练大多数最先进的LLM所使用的核心任务是单词预测。换句话说，给定一序列单词，下一个单词的概率分布是什么？ 例如，给定序列Listen to your ____，最有可能的下一个单词可能是：heart，gut，body，parents，grandma等。这可能看起来像下面显示的概率分布。 有趣的是，这是许多（非大型）语言模型过去被训练的方式（例如GPT-1）[3]。然而，由于某种原因，当语言模型超过一定大小（例如~10B个参数）时，这些（新生的）能力，例如零-shot学习，开始出现[1]。 尽管目前还没有明确的答案，解释为什么会发生这种情况（只有推测），但明显LLM是一种强大的技术，具有无数的潜在用例。 使用LLM的3个层次 现在我们来看看如何在实践中使用这种强大的技术。虽然有无数的LLM用例，但在这里，我将它们按所需的技术知识和计算资源排序为3个层次。我们从最容易使用的开始。 一级：提示工程 使用LLM的第一级别是“提示工程”，我将其定义为“任何使用LLM的开箱即用方式”，即不更改任何模型参数。虽然许多技术倾向的个人似乎对提示工程的想法不屑一顾，但这是实际中使用LLM（在技术和经济上）最可访问的方法。 有两种主要的提示工程方式： 简单方式 和 较不简单方式。 简单方式：ChatGPT（或其他方便的LLM UI） - 这种方法的关键好处是方便。像ChatGPT这样的工具提供了一种直观，免费且无代码的使用LLM的方法（没有比这更容易的方法了）。 然而，方便通常是有代价的。在这种情况下，这种方法有两个主要缺点。第一个是缺乏功能。例如，ChatGPT不容易使用户自定义模型输入参数（例如温度或最大响应长度），这些值调节LLM输出。第二，与ChatGPT UI的交互不能轻松地自动化，因此无法应用于大规模使用情况。 虽然这些缺点可能是某些用例的杀手级应用，但如果我们将提示工程向前推进一步，这两个缺点都可以得到改善。 较不简单方式：直接与LLM交互 - 我们可以通过编程接口直接与LLM进行交互来克服ChatGPT的一些缺点。这可以通过公共API（例如OpenAI的API）或在本地运行LLM（使用像Transformers这样的库）来实现。 虽然这种提示工程方式不太方便（因为它需要编程知识和潜在的API成本），但它提供了一种可定制，灵活和可扩展的使用LLM的方法。本系列文章将讨论付费和免费的方法来进行此类提示工程。 尽管提示工程（如此定义）可以处理大多数潜在的LLM应用程序，但依赖通用模型可能会导致特定用例的次优性能。对于这些情况，我们可以进入使用LLM的下一个级别。 等级 2：模型微调 使用 LLM 的第二个等级是模型微调，我定义为对现有 LLM 进行微调以用于特定用例，通过改变至少一个（内部）模型参数，即权重和偏差。在此类别中，我还将在此处将迁移学习即使用现有 LLM 的某些部分来开发另一个模型。 微调通常包括两个步骤。步骤 1：获得预先训练的 LLM。步骤 2：基于给定的特定任务更新模型参数（通常是数千个）高质量标记的示例。 模型参数是定义 LLM 对输入文本的内部表示的。因此，通过针对特定任务调整这些参数，内部表示变得针对微调任务进行了优化（或者至少是这样的想法）。 这是一种强大的模型开发方法，因为相对较少的示例和计算资源可以产生出色的模型性能。 然而，缺点是它需要比提示工程更多的技术专业知识和计算资源。在未来的一篇文章中，我将尝试通过审查微调技术并共享示例 Python 代码来缓解这种缺点。 虽然提示工程和模型微调可能可以处理 LLM 应用程序的 99％，但有时必须走得更远。 等级 3：构建自己的 LLM 在实践中使用 LLM 的第三种最终方法是构建自己的。在模型参数方面，这是您从头开始制定所有模型参数的地方。 LLM 主要是其训练数据的产物。因此，对于某些应用程序，可能需要策划自定义的高质量文本语料库进行模型训练，例如医学研究语料库，用于开发临床应用程序。 这种方法最大的优点是您可以完全自定义 LLM 以适用于您的特定用例。这是终极的灵活性。但是，通常情况下，灵活性的代价是方便性。 由于LLM 性能的关键是规模，因此从头开始构建 LLM 需要巨大的计算资源和技术专业知识。换句话说，这不会是一个个人周末项目，而是一个完整的团队工作数月甚至数年，预算达到 7-8F。 尽管如此，在我未来文章中，我希望探讨从头开始开发 LLM 的流行技术。 最后让我们来总结一下： 虽然LLM现在被吹得足够大，但它们是AI领域的一项强大创新。在这里，我提供了有关LLMs是什么以及如何在实践中使用它们的入门指南。日后我希望写一些文章提供初学者指南，帮助大家启动下一个LLM用例。 资源 链接：「个人博客」 社交：「推特」|「微博」| 「领英」|「油管」 之后我会出一些AI相关的具体视频教程，目前还未找到合适的平台托管，敬请期待。关注我，我会第一时间通知到家。 在我的公众号内的文章大部分是免费阅读的（除非有实际成本支出），如果您觉得对您有帮助，可以给我赞赏一下以表支持。 引用 [1] 大型语言模型调查。 arXiv:2303.18223 [cs.CL] [2] GPT-3论文。 arXiv:2005.14165 [cs.CL] [3] Radford，A.，&amp; Narasimhan，K。（2018）。通过生成式预训练改善语言理解。 （GPT-1论文）","link":"/LLMs%E7%9A%84%E5%AE%9E%E7%94%A8%E4%BB%8B%E7%BB%8D/"},{"title":"02 大语言模型做情感分析","text":"上一节中，我们介绍了大型语言模型的接口非常简单，仅提供了Complete和Embedding两个接口。但这样看似简单的接口，实际上可以解决很多自然语言处理问题。例如，情感分析、文本分类、文章聚类、摘要生成、搜索等问题，都可以使用大型语言模型解决。接下来的几节课中，我们将介绍如何使用这两个简单的API来解决传统的自然语言处理问题。本节我们将从最常见的自然语言处理问题“情感分析”开始介绍，看看如何使用大型语言模型。 传统的二分类方法：朴素贝叶斯与逻辑回归 朴素贝叶斯与逻辑回归可以用来解决“情感分析”问题。这些算法的基本思想是，根据给定的标记数据，学习一个分类器，用来将新的输入数据进行分类。对于情感分析问题，分类器的目标是将一段文字分为正面或负面情感。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Large-language-model-for-sentiment-analysis/"},{"title":"Foundation of Artificial Intelligence - Lecture 1","text":"Algorithm --&gt; Data Structure No obvious solution ==&gt; Algorithm engineers do it If there is a clear implementation path ==&gt; the person who develops the project will do it What's the Algorithm? {Ace of hearts, 10 of spades, 3 of spades, 9 of hearts, 9 clubs, 4 of diamonds, J} First: Hearts&gt; Diamonds&gt; Spades&gt; Clubs Second: Numbers are arranged from small to large Some people put the colors together first Some people arrange the size first, and extract the colors one by one \\[ 1024 --&gt; 10^3 --&gt; 1k \\] \\[ 1024 * 1024 --&gt; 10^6 --&gt; 1M \\] \\[ 1024 * 1024 * 1024 --&gt; 10^9 --&gt; 1G \\] 123456struction-0 00011101struction-1 00011111 struction-2 00011100struction-3 00011101struction-4 00011100struction-5 00011001 2.6G Hz 12345def fac(n): # return n! if n == 1: return 1 # 返回操作 else: return n * fac(n-1) # 乘法操作 + 返回操作 + 函数调用 12345678910fac(1)&gt; 1fac(100)&gt; 93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000fac_100 = &quot;&quot;&quot;93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000&quot;&quot;&quot;len(fac_100)&gt; 158 12345?? N --&gt; fac(n)# 乘法操作 + 返回操作 + 函数调用?? (N - 1)--&gt; fac(n-1)?? N == 100 fac(N) ??? 99 1234Object ` N --&gt; fac(n)` not found.Object ` (N - 1)--&gt; fac(n-1)` not found.Object ` N == 100 fac(N)` not found.Object `? 99` not found. \\[ Time(N) - Time(N-1) = constant \\] \\[ Time(N-1) - Time(N-2) = constant \\] \\[ Time(N-2) - Time(N-3) = constant \\] \\[ Time(2) - Time(1) = constant \\] \\[ Time(N) - Time(1) == (N-1)constant \\] \\[ Time(N) == (N-1)constant + Time(1) \\] \\[ Time(N) == N * constant + (Time(1) - constant) \\]","link":"/Lecture_1/"},{"title":"M1安装Homebrew(ARM)","text":"?&gt; 详情可见作者说明 安装 ARM版本Homebrew必须安装在/opt/homebrew路径下 123cd /optsudo mkdir homebrewsudo curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C homebrew 如果不进行sudo授权，则会报错； 环境变量 本人使用zsh, 所以编辑文件~/.zshrc. 添加如下内容： 12path=('/opt/homebrew/bin' $path) export PATH ?&gt; 如果是使用bash，请修改~/.bashrc 在终端内执行: 1source ~/.zshrc 现在可以试试执行brew install graphviz试试看能否正常安装回归树可视化模块； 软件包和迁徙 软件包依然需要使用X86版Homebrew 1arch -x86_64 启用一个X86模式中端，之后运行的命令都在X86模式下运行，再次安装Homebrew 1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; !&gt; 注意：要将ARM版本Homebrew环境变量设置到最前面，此时两个版本共存时会有限启动ARM版本，需要运行X86版本时，需要手动输入完整路径arch -x86_64 /usr/local/bin/brew 可以在配置文件中设置alias 12abrew='/opt/homebrew/bin/brew' # ARM Homebrewibrew='arch -x86_64 /usr/local/bin/brew' # X86 Homebrew 如果对已有软件包做迁徙，则： 1ibrew bundle dump 此时在目录下就得到一个名为Brewfile的备份文件，导入内容并安装 1abrew bundle --file /path/to/Brewfile !&gt; 执行之前需要编辑Brewfile文件，将cask和mas开头的记录删除掉；","link":"/M1_install_homebrew/"},{"title":"5. 模块化编程","text":"HI, 大家好。我是茶桁。 上一节中我们学习了Python基本的流程控制，并且预告了这一节的内容，就是将要学习「模块化编程」。那什么是模块化编程呢？按照维基百科的说法： 模块化编程（英语：modular programming），是强调将计算机程序的功能分离成独立的、可相互改变的“模块)”（module）的软件设计技术，它使得每个模块都包含着执行预期功能的一个唯一方面（aspect）所必需的所有东西。 说的简单一点，就是把程序进行封装（函数封装、面向对象、文件...） OK，话不多说，让我们开始吧。 函数 什么是函数？ 函数的英文单词为function, 我们将其翻译过来，就是“函数，功能”。 其实，函数就是一个具有特定功能的代码块。 函数的作用 函数的目的是封装，将特定功能的代码块进行封装，其目的是提高代码的重用性，从而提高开发效率，并且降低了后期的维护成本。 函数的定义和使用 函数的定义其实非常简单，我们用代码来写一下： 12345# 定义函数[基本结构]def 函数名([参数列表]): 当前行数的具体功能的代码 当前行数的具体功能的代码 ... 当然，函数在写完之后并不会自动执行，只是把函数定义了而已。如果想要使用定义完成的函数，需要用语法来进行函数的调用。 那么函数该如何调用呢？如下： 1函数名() 示例： 12345678# 函数的定义格式def love(): print('i') print('love') print('u')# 函数的调用love() 当前程序运行输出结果： 123iloveu 以上代码可以得到函数的第一个特征：函数定义后，不调用不执行。还记得咱们上节课强调的流程控制吗？代码最基本流程顺序是自上而下的，所以，这个时候我们如果调用放在上方，例如： 12345678# 函数的调用love()# 函数的定义格式def love(): print('i') print('love') print('u') 此时因为love()调用的时候函数还未被定义，所以会执行报错： 1NameError: name 'love' is not defined 所以我们需要注意：不能在函数定义前调用函数。 另外，我们需要注意，函数的调用不受次数的影响，比如，我们定义好函数后，这个时候在后面调用三次： 123love()love()love() 那执行后的结果应该是连着打印了三次结果。 和变量一样，函数的命名也是要遵守命名规范的： 字母数字下划线，不能以数字开头 严格区分大小写，且不能使用关键字 命名最好有意义，且不要使用中文 现在我们想想，在love()函数被定义后，我们再来定义一个同名的函数会怎么样？ 我们尝试一下，在刚才定义好的函数下方重复写一个同名的函数： 1234567891011121314# 函数的定义格式def love(): print('i') print('love') print('u')def love(): print('u') print(&quot;don't&quot;) print('love') print('me')# 函数的调用love() 直接结果： 1234udon'tloveme 那，我们得到了实验结果：同样的函数名被再次定义之后，冲突的函数会被覆盖。 所以，最后我们总结一下函数的特征及注意事项： 123456781. 函数定义后，不调用不执行2. 不能在函数定义前调用函数3. 函数的调用不受次数影响4. 函数的命名要遵守命名规范 - 字母数字下划线，不能以数字开头 - 严格区分大小写，不能使用关键字 - 命名最好有意义，且不要使用中文5. 函数名不要冲突，冲突后会被覆盖 函数的参数 在定义函数的时候，我们需要注意给函数的参数。可以在参数列表的位置进行定义，这个称为形参。如果一个函数有形参，那么在调用的时候必须传递参数（实参）。实参将值传递给实参的过程，本质上就是变量赋值操作。 函数参数概念及分类 带有参数的函数，该如何定义？ 在定义函数时，在小括号内可以定义形参（形式上的参数） 12345def love(w): print(f'i love {w}')# 调用带有形参的函数时，需要传递参数（实参）love('马户') 执行结果为： 1i love 马户 在这整个函数中，小括号内的w就是形参，在调用的时候的马户就是实参，在调用过程中将值传给了形参w。 那么，如果我在调用的时候没有传递实参，就会直接报错： 123love()TypeError: love() missing 1 required positional argument: 'w' 形参可以是多个，这就是定义带有多个参数的函数： 1234def love(m, n): print(f'{m} love {n}')love('i', 'u') 执行结果： 1i love u 如果形参是多个的话，那么有多少个形参就必须传递几个实参。并且参数都是按照顺序进行传递的。 如果少传一个参数，则同样会被错。 那，能不能多传呢？也不行，如果多传了参数，一样会报错。 至此，我们可以做如下总结： 函数参数：调用时需要传递的数据 函数参数的大类分为形参和实参 形参意思：函数定义时的参数 实参意思：函数调用时的参数 形实关系：函数调用时，形参和实参个数需要一一对应 函数中的参数类型 在确定了什么是形参和实参之后，我们来看看，这两种参数都有哪些类型。 函数参数在类型上，包括： 普通参数 默认参数 收集参数 命名关键字参数 关键字参数收集 普通参数 先来说说普通参数，其实就是位置参数，也叫顺序参数，也是必须传递的参数。 1234def love(m, n): print(f'{m} love {n}')love('i', 'u') 这段代码中，m, n就是普通参数，必须传递。 默认参数 有些函数在定义的时候，行参上就已经定义了默认值，那么这种就叫做默认参数。 在调用函数的时候，默认参数是可以不传值的。当传值之后，默认值就会被改变： 1234def func(x, y=20): print(x, y)func(2) 这段代码中的行参y就是默认参数，我们在调用函数func()只写了一个实参，也就是只传了一个值给函数。这个时候执行结果为： 12 20 我们修改一下，传两个值进去看看结果： 1234def func(x, y=20): print(x, y)func(2, 100) 执行结果： 12 100 可以看到，本来我们定义的行参y的默认值被改变了。 在定义默认参数的时候需要注意，函数中的默认参数只能全部定义在普通参数的后面，否则在调用函数的时候就会报错，比如以下这些情况： 1234567891011121314151617# 第1种错误情况def func(x=100, y=200, z): print(x, y, z)func(100,200,300)# 第2种错误情况def func(x=100, y, z=200): print(x, y, z)func(100, 200, 300)# 第3种错误情况def func(x, y=100, z): print(x, y, z)func(300,200,50) 收集参数 收集参数就是专门收集在函数调用时传递的多余的实参，或者我们可以理解为，不确定需要传递多少个实参，直接用一个行参来接收。 比如，我们现在有个需求就是需要计算用户输入的数字总和，我们按前面那个函数的定义方式为： 1234def func(x, y z=100): print(x+y+z)func(20,30) 这个函数中，我们输入2个值或者3个值都可以，但是当我们只输入一个值或者三个以上的时候，程序就会报错了。 那么有没有什么办法，不管用户输入多少个数字，我们都可以进行相加计算呢？ 1234567def func(x=&quot;+&quot;, *args): if x == '+': print('加法运算', args) else: print('减法运算', args)func(&quot;-&quot;, 2, 3, 4, 5, 6, 7, 8) 这段代码执行结果为： 1减法运算 (2, 3, 4, 5, 6, 7, 8) 虽然中间的运算代码我没有写，但是大家可以看到，已经可以接受不固定的多个参数了。 这个*args就是我们的收集参数。 在定义函数的时候，如果需要收集参数，那么这个形参前面需要加一个*号，例如*args。这里需要注意一点，*args并不是固定写法，你可以随意定义一个，只要前面有*号就可以了。比如下面这样： 1234567def func(x=&quot;+&quot;, *y): if x == '+': print('加法运算', y) else: print('减法运算', y)func(&quot;-&quot;, 2, 3, 4, 5, 6, 7, 8) 一样可以执行并得到一样的结果，这个时候，我们的*y就是收集参数。 收集参数也有两类，一种是普通的收集参数：专门用于收集多余的普通参数，形成一个新的元组。 1语法： 参数前面加*, 例如：*args 还有一种是关键字收集参数：用于专门收集多余关键字实参，形成一个新的字典： 1语法：参数前面加**, 例如：**kwargs 现在我们已经理解了普通的收集参数，那么在学习关键字收集参数之前，我们先来学习一下命名关键字参数 命名关键字参数 命名关键字是放在*号后面的参数，调用的时候强制必须传入制定参数名才能进行调用。 1234def func(a, b, c=3, *args, name): print(a, b, c, &quot;\\n&quot;, *args, &quot;\\n&quot;, name)func(1, 2, 3, 4, 5, 6, 7, 8, name='茶桁') 这段代码执行结果： 1231 2 3 4 5 6 7 8 茶桁 我们特意在中间加了换行字符来清晰的辨别*args和name。 如果在这段代码中我稍微变一下，在执行函数的时候，实参里不标明name可以吗？ 1234def func(a, b, c=3, *args, name): print(a, b, c, &quot;\\n&quot;, *args, &quot;\\n&quot;, name)func(1, 2, 3, 4, 5, 6, 7, 8, '茶桁') 执行之后我们收到了报错： 1TypeError: func() missing 1 required keyword-only argument: 'name' 这段报错明显告诉我们，确实了一个必须的关键字参数name。 那为什么会出现这种报错呢？这是因为在关键字参数之前，我们使用了*args来进行收集参数，那么无论你写多少，这些值都会被*args接收变成元组，那么后面的name自然就无法接受到值了。 让我们再来做一个实验，给命名关键字参数加上一个默认值，那么我们就能明显的看出问题： 1234def func(a, b, c=3, *args, name='_茶桁'): print(a, b, c, &quot;\\n&quot;, *args, &quot;\\n&quot;, name)func(1, 2, 3, 4, 5, 6, 7, 8, '茶桁') 这段代码执行结果： 1231 2 3 4 5 6 7 8 茶桁 _茶桁 可以看到，name给了默认值之后不再出现报错，而我们的实参也并未传到name里，而是全部被*args接收了。最后打印出了name的默认值_茶桁。 利用命名参数的这种定义参数名称接收值的特点，我们就可以打乱之前普通参数传值的顺序性，比如： 12345def func(x, y): print(x, &quot;\\t&quot;, y)func(2, 3)func(y = 2, x = 3) 执行结果为： 122 33 2 还是最开始的普通参数的写法，但是最后执行函数的时候，我们给实参指定了名称，这样传参顺序就没那么重要了。 所以，我们总结一下： 关键字参数定义在收集参数后面 关键字参数必须通过形参的名字来进行传递 关键字参数收集 前面我们在讲收集参数的结尾处提到了关键字参数收集，形式为**kwargs。 1234567def func(a, b, c=3, *args, name, age, **kwargs): print(a, b, c) print(args) # 普通收集参数，会把多余的参数收集为元组 print(name, age) print(kwargs) # 关键字参数收集，会把多余的关键字参数收集为字典func(1, 2, 4, 112, 123, 321, 541, 231, name=&quot;茶桁&quot;, age=18, sex='male', height=185, x='x', y='y') 执行结果： 12341 2 4(112, 123, 321, 541, 231)茶桁 18{'sex': 'male', 'height': 185, 'x': 'x', 'y': 'y'} 从执行结果上我们可以看到，在name和age之后的所有参数都被传递到了**kwargs里，然后作为字典打印了出来。 在声明这个函数和执行函数的时候需要注意，这些参数都是有顺序的，如果在执行函数的时候再多传一个非关键字参数，这个时候程序就会报错，如果是关键字参数，则照样会被**kwargs接收。 在我们介绍完这些参数之后，我们最后再说明一下： 形参声明的位置顺序：普通参数 -&gt; 默认参数 -&gt; 收集参数 -&gt; 命名关键字参数 -&gt; 关键字收集参数 def func(a, b, c=1, *args, d, **kw) 这段声明中，a,b为普通参数，c是默认参数，args是收集参数，d是命名关键字参数，kw是关键字收集参数 极少情况下会同时出现上面五种形参，一般情况下为：普通参数，收集参数，关键字收集参数 所有参数的摆放问题: 实参：普通实参在前，关键字参数在后 形参：关键字收集参数一定在最后出现，收集参数推荐在普通参数之后使用。 推荐顺序：普通形参、收集参数、关键字收集参数 函数的返回值 一个函数除了可以完成一定功能之外，还可以用来安需要返回一些内容。在函数中，使用return关键字来制定返回数据，可以返回任意类型的数据。 函数的返回值会把数据返回到调用的地方，可以使用变量进行接收，或者作其他处理。 函数可以分为两类： 执行过程函数：函数体内完成一定的功能即可，没有返回值 具有返回值的函数：函数体内完成一定的功能，并且返回一个结果到函数调用处。 比如： 12def func(a, b): print(f'{a} love {b}') 以上函数就是一个没有返回值的函数，这个函数只是为了完成打印这句话的功能。 那么有返回值的函数是什么样子？ 如果需要在函数中制定返回内容，我们需要使用return关键字。 12345678# 有返回值的函数def func(a, b): res = f'{a} love {b}' # 可以在函数体内，使用return返回内容 return resr = func('老鼠', '布丁')print(r) 执行结果为： 1老鼠 love 布丁 在这段代码中，我们在func()的函数体内最后利用关键字return返回了任意内容，并且使用变量r接收了这个返回值，最后讲r打印了出来。 在调用func()这个函数的时候，函数中的返回值会返回到函数调用处。 我们再来研究一下return这个关键字，我们在return的前后都加上一段打印代码，看看会发生什么。 12345678def func(a, b): res = f'{a} love {b}' print('这是return前') return res print('这是return后')r = func('老鼠','布丁')print(r) 执行结果： 12这是return前老鼠 love 布丁 看到结果我们可以清楚，return之后的的代码并未继续执行，也就是说，我们如果要在函数体内执行其他任务，必须放在return之前执行，否则根本不会执行。那么我们可以得出结论：return必须放在一个定义函数的最后面。 Tips: 其实，即便没有return关键字或者returen之后没有任何内容，也有返回值，只是返回的是None值。 None是一个特殊的数据，表示什么都没有。查询类型可以看到返回 &lt;class ‘NoneType’&gt; 变量的作用域 作用域就是当前起作用，可用的范围区域。也就是变量的有效范围。 变量按作用域可以分为： 局部变量： 在函数内部可以使用的变量 全局变量：在函数内外都可以使用的变量 局部变量 让我们尝试下，如果函数内定义的变量在函数外使用会如何： 1234def func(): a = 20 print(a) 执行结果： 1NameError: name 'a' is not defined 被告知a并未被定义。 可以看到，函数内定义的变量，在函数外连获取都做不到。这种在函数内定义的这个变量a，就是局部变量，它在函数外不能使用。 再让我们来看看将变量定义在函数外会是怎样的一种情况： 12345num = 10def func(): print(num)func() 执行结果： 110 在func函数内，我们获取到了变量num的值并打印。那说明，在函数内我们可以获取函数外部的变量。 我们继续在继续看： 123456num = 10def func(): num += 20 print(num)func() 执行后报错： 1UnboundLocalError: local variable 'num' referenced before assignment 这样我们可以看到，变量num在函数内虽然可以使用，但是无法进行更改。 那在函数外定义的所有变量都是如此吗？再让我们试试看： 123456items = [1, 2, 3, 4, 5]def func(): items[0] = 20 print(items)func() 执行结果: 1[20, 2, 3, 4, 5] 由此我们看出，并不是所有的变量都不可在函数内进行更改。 其实，变量是分为两种的： 可变数据类型：在函数外定义的变量，在函数内可以使用并改变 不可变数据类型：在函数外定义的变量，在函数内只可以访问而无法改变 可变数据类型有列表和字典，其他的都是不可变数据类型。 全局变量 之前我们介绍的都是局部变量，那怎样定义全局变量呢？ 在函数内使用global直接定义的变量，就是全局变量，函数内外都可以直接使用。 在函数外定义的变量，在函数内使用global关键字进行声明，那么也是全局变量。 例如： 1234567num = 20def func(): global num num += 10 print(num)func() 这个时候我们可以得到执行结果： 130 那有小伙伴就问了，如果我在函数外直接使用global定义全局变量可以吗？让我们来试试看就知道了： 1234567global numnum = 20def func(): num += 10 print(num)func() 执行之后得到报错： 1UnboundLocalError: local variable 'num' referenced before assignment 这样我们就得到了结果：不可以。 在整个程序中，我们可以使用两个函数方法来获取数据： globals()用来获取全局数据，locals()用来获取当前作用域的数据 讲到这里，我们再来多看一组代码： 12345678# 函数的作用域def outer(): print('this is outer func...') def inner(): print('this is inner func...')outer()inner() 这段代码执行结果为： 123this is outer func...NameError: name 'inner' is not defined 正常执行了outer()内的打印，然后又报了一个错误，提示inner函数未定义。 说明，不只是变量有作用域，函数一样也有作用域。要想inner函数内的打印也起作用，我们需要在函数内就调用执行inner()。 1234567# 函数的作用域def outer(): print('this is outer func...') def inner(): print('this is inner func...') inner() # 在函数内执行outer() 这样，我们执行的结果就是： 12this is outer func...this is inner func... 如果我们在外层函数中定义一个局部变量，能在内层函数中使用吗？ 12345678910# 函数的作用域def outer(): a = 2 print('this is outer func...') def inner(): a += 1 print('this is inner func...') print(a) inner()outer() 执行之后得到报错： 1UnboundLocalError: local variable 'a' referenced before assignment 说明并不可以。 nonlocal关键字 那么，到底有没有什么办法在内函数中使用上一层函数中的局部变量呢？答案是有办法。 在内函数中如果想要使用外层函数的变量，那么需要使用nonlocal关键字，可以引用上一层函数中定义的局部变量。 123456789101112# 定义一个外层函数def outer(): # 外函数的局部变量 num = 10 # 内函数, 局部函数, 在函数的内部定义的函数 def inner(): # nonlocal 关键字在局部函数中使用 nonlocal num # 可以引用上一层函数中定义的局部变量 num += 1 print(num) inner()outer() 执行后返回结果： 111 至此，我们通过使用nonlocal关键字，成功拿到了外层函数定义的变量num并使用。最后打印出使用的结果。 这里我们要注意，nonlocal虽然可以引用上一层函数中定义的局部变量，但是这并不代表提升为了全局变量。 既然我们有了global关键字可以提升变量为全局变量，为什么还需要一个nonlocal关键字呢？是不是有点多此一举？ 这两者的功能上并不相同。global关键字修饰变量后标识该变量是全局变量，对该变量进行修改就是修改全局变量，而nonlocal关键字修饰变量后标识该变量是上一级函数中的局部变量，如果上一级函数中不存在该局部变量，nonlocal位置会发生错误（最上层的函数使用nonlocal修饰变量必定会报错）。 关于函数的文档 我们在一个未定义任何变量和函数的空白文档中打印一下全局数据： 1print(globals()) 执行结果： 1234567891011121314151617181920212223242526272829{ '__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': &lt;module 'builtins' (built-in)&gt;, '__builtins__': &lt;module 'builtins' (built-in)&gt;, '_ih': ['', 'print(globals())'], '_oh': {}, '_dh': [PosixPath('/Users/xx/git/AI_Cheats/Python'), PosixPath('/Users/xx/git/AI_Cheats/Python')], 'In': ['', 'print(globals())'], 'Out': {}, 'get_ipython': &lt;bound method InteractiveShell.get_ipython of &lt;ipykernel.zmqshell.ZMQInteractiveShell object at 0x105be29e0&gt;&gt;, 'exit': &lt;IPython.core.autocall.ZMQExitAutocall object at 0x105be3280&gt;, 'quit': &lt;IPython.core.autocall.ZMQExitAutocall object at 0x105be3280&gt;, 'open': &lt;function open at 0x10488e710&gt;, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/Users/xx/git/AI_Cheats/Python/globals.ipynb', '_i': '', '_ii': '', '_iii': '', '_i1': 'print(globals())'} 我们来看这个打印出来的json 类似于__name__这种前后有__ __的数据，称之为“魔术变量”。我们并未定义，但是已经存在了。 如果脚本作为主程序，那么__name__值是__main__，如果是当作一个模块在另外一个脚本中引用去使用，那么值就是当前文件的命名。 __doc__当前脚本的文档说明，在当前脚本当中的第一个三引号注释就是当前脚本的说明文档。比如，我们在这个空白的文档中写一段三引号注释 12345&quot;&quot;&quot;这里是整个文档的说明部分。&quot;&quot;&quot;def func(): pass 然后我们直接将doc打印出来查看： 1print(__doc__) 可以看到输出内容： 1这里是整个文档的说明部分。 这种文档其实不止适用于python文件，对于定义的函数依然适用。比如，我们执行定义了一个函数，并在函数内部用三引号进行注释： 12345678910def func(): &quot;&quot;&quot; 这里是让你写一写函数的文档说明的。 需要说明当前函数的作用， 如果当前函数还有形参，那么也需要对形参进行一一说明。 name: 这个是一个name参数，用于接收姓名 age: 这个参数是表示年龄 :return: 此处说明当前函数的返回值 &quot;&quot;&quot; pass 这个时候，我们在下方执行： 1print(func.__doc__) 可以看到我们在注释内定义的说明文档被打印出来了： 这样，我们不仅可以在自己写函数的时候在上方清晰的写明当前函数的作用及参数，我们还可以使用此方法，查找其他人所写的函数的一些说明。 在我们平时写代码的时候，养成一个好习惯是非常有必要的。 总结一下： print(__name__): 获取当前脚本的文件名 print(__doc__): 获取当前脚本的说明文档 print(func.__doc__)： 获取当前函数的说明文档 练习：函数封装 我们上一讲中的练习中，我们打印了乘法表，矩形图形，还计算了12生肖。这里我们就将乘法表来封装成函数，实现我们上节课留的其中一道思考题：反向打印。 我们先将打印乘法表封装起来： 123456789# 定义函数，打印九九乘法表def multiply_table(): &quot;&quot;&quot; 当前函数的功能是打印出乘法表 &quot;&quot;&quot; for x in range(1, 10): for y in range(1, x+1): print(f'{x}X{y}={x*y}', end=&quot; &quot;) print() 这样，我们在其他地方执行multiply_table()函数的时候，就可以直接打印出乘法表。 现在让我们给这个函数多加一些功能： 123456789101112131415# 定义函数，打印九九乘法表def multiply_table(i=0): &quot;&quot;&quot; 当前函数的功能是打印出乘法表 i=0; i 这个参数可以用来控制正向输出和方向输出，0的时候正向，1的时候反向,默认为0 &quot;&quot;&quot; if i: rs = range(9, 0, -1) else: rs = range(1, 10) for x in rs: for y in range(1, x+1): print(f'{x}X{y}={x*y}', end=&quot; &quot;) print() 我们执行函数的时候，输入1来试试看： 1multiply_table(1) 输出结果： 1234567899X1=9 9X2=18 9X3=27 9X4=36 9X5=45 9X6=54 9X7=63 9X8=72 9X9=81 8X1=8 8X2=16 8X3=24 8X4=32 8X5=40 8X6=48 8X7=56 8X8=64 7X1=7 7X2=14 7X3=21 7X4=28 7X5=35 7X6=42 7X7=49 6X1=6 6X2=12 6X3=18 6X4=24 6X5=30 6X6=36 5X1=5 5X2=10 5X3=15 5X4=20 5X5=25 4X1=4 4X2=8 4X3=12 4X4=16 3X1=3 3X2=6 3X3=9 2X1=2 2X2=4 1X1=1 可以看到，我们的控制结果被成功打印出来。至此，这个有一些小功能的九九乘法表就封装完成了。 那么这一节课就不留思考题了，大家课后熟练掌握一下封装函数和变量的作用域，我们下节课来学习一些高阶函数， 好，下课。","link":"/Modular-programming/"},{"title":"03 提示语，做个聊天机器人","text":"大家好，我是茶桁。 在本次讲座中，我们将研究 OpenAI 提供的 Completion API 接口。你可能已经体验过与 ChatGPT 进行聊天的乐趣，或是利用它帮我们编写各种文本材料的便利。现在，我们可以从这个需求出发，进一步探索 Completion API 的具体用法，以及如何利用它来生成更加优质的文本。 AI 客服 在我了解人工智能和自然语言生成之前，我听说过智能客服，然而我并没有亲身体验过。我想象中，智能客服的回答应该是按照固定的模板进行生成的，这样的缺点就是每次回答都会是一样的。虽然可以设计多个模板来轮换着表达相同的意思，但是最多也就是三四个模板，整体的体验可能会比较呆板。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Let-s-Build-a-Chatbot/"},{"title":"Power for Mac","text":"很多人都会使用\"Pow\"来进行本地静态页面开发环境。对于其配置确实简单到有爱。 不过Yosemite上\"Pow\"都不能正常工作，之前我参照官方的办法写了一篇如何在Yosemite上设置\"Pow\"的方法。有兴趣的可以参看我原文:http://hivan.me/setting-pow-at-Yosemite/ 不过一直用的好好的\"Pow\"近期又开始出现 404 错误，短暂的解决无果以后，我开始寻找一些快速能解决的办法，便遇到了\"Power\"，和\"Pow\"一样，都是建立快速开发环境，并且经过测试，在Yosemite 10.10.1下正常工作。 使用\"Pow\"来做开发环境的，可以暂时用这样一个替代方案，毕竟开发中没有时间去多做研究了，暂时不知道两者的区别，不过\".dev\"正常访问已经没有问题。 Power 项目地址: https://github.com/HackPlan/power/","link":"/Power-for-Mac/"},{"title":"1. Python的特性和语法","text":"千里之行始于足下。 大家好，我是茶桁，这里是我们《AI秘籍》的第一节，让我们先从Python来开始好好的打好基础。 第一堂课，我们先从最基础的Python特性开始，当然，还有一些基本语法。 上来就开始讲特性和语法，说明我们将会遗弃惯用的“环境搭建”等更基础的内容，那些内容网上已经很丰富了，一查一大堆，并且相对来说内容都比较独立，所以希望还不太会搭建开发环境的同学可以自己去搜索看看。或者，其实从我这篇《Apple M1的AI环境搭建》也能完全搭建起一个完整的Python开发环境。（Windows和Linux的同学就只能在网上搜索一下看看了。） 总体来说，Python语言的使用率越来越高，它不仅可以用于GUI开发、Web开发，还能进行数据预处理、数据探索性分析（EDA），更是进行数据挖掘、机器学习、深度学习等任务的首选语言。 基于Python的包也越来越丰富，各种优秀的库层出不穷。根据\"Tiobe编程语言排行榜\"的最新统计，Python结束了自己攀爬的势头，已经开始长期占据榜首。Python的发展势头让人们看到了它在各个领域都有着广阔的应用前景。 得益于Python语言的简洁语法和高效开发能力，使得集成系统变得非常方便。 与此同时，Python相关的就业机会也非常丰富，待遇也相当优渥。 因此，无论从易用性、就业机会还是薪酬待遇来看，Python都是IT从业者必备的编程语言之一。 课程说明 本课程所使用语言为Python3 本课程将会有一些案例，用于辅助学习和理解知识点。 基本所有案例均使用Jupyter Notebook做演示 一些项目会用到软件工程和设计模式的思想。 本课程无任何文学色彩，重点在于简单通俗易懂。 接下来，让我们真实开始吧。 Python的两大特性 一句话总结，就是Python是一门动态的、强类型语言。 动态语言 在了解“动态语言”之前，我们先来了解一下“类型检查”。 类型检查是验证变量或表达式的数据类型是否符合语言规定的类型约束的过程。它用于确保程序在运行时不会出现类型错误，例如将一个字符串与一个整数相加或将一个数字与一个布尔值进行比较。类型检查旨在捕捉潜在的类型不匹配错误，并在编译时或运行时提供相应的警告或错误信息。 如果类型检查发生在程序运行阶段（运行时），则称为\"动态类型语言\"（dynamically typed languages）。常见的动态语言包括： Python JavaScript Ruby PHP Lua Perl Shell脚本 有动态语言，则必然会有其相对的“静态语言”，常见的“静态类语言”包含： C C++ Java C# Swift Kotlin Rust TypeScript 当然，这些都只是一部分常见的动态语言和静态类型语言，还有许多其他编程语言也属于这两个类别。在实际开发中，选择使用动态语言还是静态类型语言取决于项目的需求、开发团队的喜好和项目的规模等因素。每种类型的语言都有其独特的特点和适用场景，因此选择合适的语言是非常重要的。 强类型语言 强类型语言（Strongly Typed Language）要求在编程过程中严格定义和遵守数据类型规则。在强类型语言中，变量必须明确地声明其数据类型，并且在运行时不能隐式地改变数据类型。这意味着变量在使用之前必须进行类型转换，以确保数据的一致性和安全性。 在强类型语言中，编译器或解释器会对数据类型进行严格的检查，如果发现不符合类型规则的操作，就会报错并拒绝执行。这样可以防止一些潜在的类型错误，确保程序的稳定性和正确性。 强类型语言的主要特点包括： 静态类型检查：在编译时或解释时进行类型检查，检查数据类型是否匹配，避免类型不匹配的错误。 显式类型转换：在进行类型转换时，必须显式地指定转换的方式，例如强制类型转换。 不支持隐式类型转换：强类型语言不允许在不明确声明的情况下将一个数据类型隐式地转换为另一个数据类型。 这么说可能并不直接，我们来拿个示例，我们输入两行代码： 12a = 5a = a + 's' 然后我们会看到程序抛出TypeError异常： 这个异常意思是不支持int变量和str变量相加。 常见的强类型语言包括： Java C++ C# Python Swift Kotlin TypeScript Rust Pascal Ada Delphi 一样的，对应的就是弱类型语言。弱类型语言容易与其他类型混合计算，其代表是JavaScript。（有一说一，我还挺喜欢JS的这个特性的） 弱类型语言包括： JavaScript PHP Perl Ruby Tcl Bash AWK MATLAB (在一些操作上可以被视为弱类型) 当然，对于这个划分其实并不是所有人都一致的，有些人还是会把Python归结为弱类型语言，而通常意义上，大家会把C++划分到弱类型。这里我们不去争论，仅仅记住动态/静态、弱类型/强类型的区别就行了。 基本语法 基本语法里，我们介绍一下Python的命名规则、缩进原则、特殊关键字和特殊运算符四个方面。 命名规则 Python的变量命名规则包括以下几条： 允许包括英文、数字以及下划线（_），不能以数字开头。 名称区分大小写，例如\"myVar\"和\"myvar\"是两个不同的变量。 以单下划线（_）开头的变量通常用作受保护的变量，表示应该将其视为私有，不建议直接访问。虽然Python没有严格的访问控制，但这是一种约定俗成的做法。 以双下划线（__）开头和结尾的变量是Python中的特殊标识符，具有特殊的意义，如类的私有成员或专用标识符。 Python的变量命名习惯一般遵守蛇形命名法（snake case）： 一般变量命名使用小写字母，多个单词之间用下划线连接，例如：book_id、book_store_count。 类名首字母大写，如Python内置模块collections.abc中的Iterable类，我们自定义的Book类等。 类方法名也使用小写字母，多个单词之间用下划线连接，例如：get_store_count()。 与Java的命名方法不同，Java通常使用驼峰命名法（camel case）来命名变量和方法名，其中第一个单词首字母小写，后续单词首字母大写，例如：myVar、getStoreCount()。而Python则更倾向于使用蛇形命名法。这是因为Python社区普遍认可了蛇形命名法，使得代码在风格上更加一致和易读。 缩进原则 Python最具特色的特点之一是使用缩进来表示代码的逻辑层次，而不是像Java和C++中使用{}。Python的缩进层级结构非常重要，它代表着代码的逻辑结构。 通常情况下，Python的缩进为4个空格字符。例如，在定义一个Book类并重写__add__方法来计算两本书的库存量时，代码如下所示： 12345678910111213141516171819class Book(object): # 定义类的参数 def __init__(self, b_id, b_name, b_store_count): self.b_id = b_id self.b_name = b_name self.b_store_count = b_store_count # 重写加法操作 def __add__(self, book): return self.b_store_count + book.b_store_count # 创建两个Book类的实例python_intro_book = Book(1, '金瓶梅', 100)ml_intro_book = Book(2, '玉蒲团', 200)# 求两本书的总销量sales_cnt = python_intro_book + ml_intro_bookprint(sales_cnt) 上述代码定义了一个Book类，包括初始化方法__init__和重写的加法操作__add__。通过这种缩进结构，我们可以清晰地看到代码的层次结构和逻辑。 在Python编码中，缩进格式、行间空行数、变量和等号之间的空格等都遵循PEP8（Python Enhancement Proposal 8）规范。可以使用autopep8包来自动实现PEP8规范，保持代码的规范和易读性。 特殊关键字 Python有35个关键字，这些关键字具有特殊的含义，不能用于自定义变量名，否则会引起语法错误。以下是Python的关键字列表： 1234567False await else import passNone break except in raiseTrue class finally is returnand continue for lambda tryas def from nonlocal whileassert del global not withasync elif if or yield 这些关键字在Python编程中扮演着重要的角色。其中，True和False用于表示布尔值的真和假，类似于Java中的true和false；None表示空值，类似于Java中的null；Python使用not表示逻辑反操作，而Java使用!；Python使用and表示两个条件同时满足，Java使用&amp;&amp;；Python使用or表示两个条件满足其一，Java使用||；Python使用elif，而Java使用else if。 还有一些比较特殊的关键字，例如： del用于删除可迭代对象中的某个元素； def用于定义函数； 带有yield的关键字用于定义生成器（generator）函数； global和nonlocal是在Python函数式编程的闭包场景中使用的。 pass关键字用于占位，当你在定义函数或类时暂时不想添加具体的实现时，可以使用pass关键字。 这些关键字的具体用法将在后续文章中更详细地介绍。在此，先对它们有一个整体的认识即可。 特殊运算符 Python的运算符包括： 123+ - * ** / // % @&lt;&lt; &gt;&gt; &amp; | ^ ~ :=&lt; &gt; &lt;= &gt;= == != 大部分运算符在其他编程语言中也是常见的，不过这里重点介绍三个比较特殊的运算符：//、**和:=。 //运算符用于两个数值相除并向下取整，类似于Python的math.floor()功能： 12print(5 // 2) # 输出: 2print(5 // 4.5) # 输出: 1.0 **运算符用于进行幂运算： 1print(2 ** 3) # 输出: 8 :=运算符是在Python 3.8版本中引入的，被形象地称为“海象运算符”。它可以在表达式中同时为变量赋值和比较： 123n = len(a)if n &gt; 10: print(f&quot;{n}大于10&quot;) 可以用“海象运算符”简化为： 12if (n := len(a)) &gt; 10: print(f&quot;{n}大于10&quot;) Python的比较运算符还支持链式比较，使得编码更加方便： 123i = 3print(1 &lt; i &lt; 3) # 输出: Falseprint(1 &lt; i &lt;= 3) # 输出: True 此外，运算符@用于装饰器功能，本专栏会深入解释它的本质，并提供相关案例，帮助你学会使用装饰器。 总结 在本文中，我们一起学习了Python这门功能强大的编程语言。Python的两大特性是动态语言和强类型语言。 动态语言意味着在运行时执行类型检查，而不是在编译时。这使得Python更加灵活和易于使用，允许我们在代码中动态创建和修改变量。Python的动态特性使其成为进行数据预处理、数据探索性分析、数据挖掘、机器学习和深度学习等任务的首选语言。 另一方面，强类型语言意味着变量的类型在声明时就已经确定，并且不能进行隐式类型转换。这确保了代码的稳定性和安全性，帮助我们避免一些常见的错误。 在Python的基本语法方面，我们学习了变量命名规则，缩进原则，特殊关键字和特殊运算符。Python的命名规则允许使用英文、数字和下划线，但不能以数字开头，并且区分大小写。特殊关键字包括Python的35个关键字，如if、else、for、while等等，它们有着特定的含义和用途。特殊运算符中，//用于整数除法，**用于幂运算，:=是在Python 3.8版本中引入的“海象运算符”，使得在表达式中同时为变量赋值和比较变得更加方便。 通过学习Python的特性和基本语法，我们已经具备了编写简单到复杂的程序的基础知识。Python的易用性、丰富的库和社区支持，使其成为一个优秀的编程语言，适用于各种应用场景。无论是初学者还是有经验的开发者，Python都是一个值得深入学习和探索的语言。 希望本文能够为读者提供了一个对Python的初步认识，并激发了你继续学习和研究的兴趣。在接下来的学习过程中，我们可以更深入地了解Python的各种功能和应用领域，并用Python来解决更复杂的问题。 好了，我是茶桁，咱们下节见。","link":"/Python-features-and-syntax/"},{"title":"3. Python3 运算符","text":"Hi，大家好。我是茶桁。 前两节我们学习了基本的Python特性和语法，并且认识了一些基本的Python脚本。今天，我们来学习一下Python的运算符，而我们选择的版本为Python3。 什么是运算符 为了能让我们的学习顺利进行下去，首先我们需要先弄明白：什么是运算符。 这里举一个简单的栗子：\\(4 + 5 = 9\\), 在这个简单的数学计算栗子中，4和5倍称为操作数，+就被成为是运算符, 最后9就是它的运算结果。 到这里，我们对于运算符应该有了一个基本的认知，那么Python语言都支持哪些运算符呢？如下列表： 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 接下来，就让我们来一个个的学习Python的运算符 算术运算符 运算符 描述 实例 + 加 - 两个对象相加 a + b 输出结果 31 - 减 - 得到负数或是一个数减去另一个数 a - b 输出结果 -11 * 乘 - 两个数相乘或是返回一个被重复若干次的字符串 a * b 输出结果 210 / 除 - x 除以 y b / a 输出结果 2.1 % 取模 - 返回除法的余数 b % a 输出结果 1 ** 幂 - 返回x的y次幂 a**b 为10的21次方 // 取整除 - 向下取接近除数的整数 &gt;&gt;&gt; 9//2 4 &gt;&gt;&gt; -9//2 -5 1234567891011# 算术运算符a = 10b = 21print(&quot;a+b=&quot;, a+b)print(&quot;a-b=&quot;, a-b)print(&quot;a*b=&quot;, a*b)print(&quot;b/a=&quot;, b/a)print(&quot;b%a=&quot;, b%a)print(&quot;a**b=&quot;, a**b)print(9//2)print(-9//2) 输出结果： 12345678a+b= 31a-b= -11a*b= 210b/a= 2.1b%a= 1a**b= 10000000000000000000004-5 比较运算符 运算符 描述 实例 == 等于 - 比较对象是否相等 (a == b) 返回 False。 != 不等于 - 比较两个对象是否不相等 (a != b) 返回 True。 &gt; 大于 - 返回x是否大于y (a &gt; b) 返回 False。 &lt; 小于 - 返回x是否小于y。所有比较运算符返回1表示真，返回0表示假。这分别与特殊的变量True和False等价。注意，这些变量名的大写。 (a &lt; b) 返回 True。 &gt;= 大于等于 - 返回x是否大于等于y。 (a &gt;= b) 返回 False。 &lt;= 小于等于 - 返回x是否小于等于y。 (a &lt;= b) 返回 True。 1234567# 比较运算符print(&quot;a==b:&quot;, a==b)print(&quot;a!=b:&quot;, a!=b)print(&quot;a&gt;b:&quot;, a&gt;b)print(&quot;a&lt;b:&quot;, a&lt;b)print(&quot;a&gt;=b:&quot;, a&gt;=b)print(&quot;a&lt;=b:&quot;, a&lt;=b) 输出结果： 123456a==b: Falsea!=b: Truea&gt;b: Falsea&lt;b: Truea&gt;=b: Falsea&lt;=b: True 赋值运算符 运算符 描述 实例 = 简单的赋值运算符 c = a + b 将 a + b 的运算结果赋值为 c += 加法赋值运算符 c += a 等效于 c = c + a -= 减法赋值运算符 c -= a 等效于 c = c - a *= 乘法赋值运算符 c = a 等效于 c = c a /= 除法赋值运算符 c /= a 等效于 c = c / a %= 取模赋值运算符 c %= a 等效于 c = c % a **= 幂赋值运算符 c = a 等效于 c = c a //= 取整除赋值运算符 c //= a 等效于 c = c // a 1234567891011121314151617# 赋值运算符c = a+bprint(c)c+=aprint(c)c-=aprint(c)c*=aprint(c)c/=aprint(c)c%=aprint(c)c=aprint(c)c//=aprint(c) 输出结果： 1234567831413131031.01.0101 位运算符 按位运算符是把数字看作二进制来进行计算的。bin()函数可以把数字转为二进制。 Python中的按位运算法则如下： 下表中变量 a 为 60，b 为 13二进制格式如下： 12345678910111213a = 0011 1100b = 0000 1101-----------------a&amp;b = 0000 1100a|b = 0011 1101a^b = 0011 0001~a = 1100 0011 运算符 描述 实例 &amp; 按位与运算符：参与运算的两个值,如果两个相应位都为1,则该位的结果为1,否则为0 (a &amp; b) 输出结果 12 ，二进制解释： 0000 1100 | 按位或运算符：只要对应的二个二进位有一个为1时，结果位就为1。 (a | b) 输出结果 61 ，二进制解释： 0011 1101 ^ 按位异或运算符：当两对应的二进位相异时，结果为1 (a ^ b) 输出结果 49 ，二进制解释： 0011 0001 ~ 按位取反运算符：对数据的每个二进制位取反,即把1变为0,把0变为1。~x 类似于 -x-1 (~a ) 输出结果 -61 ，二进制解释： 1100 0011， 在一个有符号二进制数的补码形式。 &lt;&lt; 左移动运算符：运算数的各二进位全部左移若干位，由\"&lt;&lt;\"右边的数指定移动的位数，高位丢弃，低位补0。 a &lt;&lt; 2 输出结果 240 ，二进制解释： 1111 0000 &gt;&gt; 右移动运算符：把\"&gt;&gt;\"左边的运算数的各二进位全部右移若干位，\"&gt;&gt;\"右边的数指定移动的位数 a &gt;&gt; 2 输出结果 15 ，二进制解释： 0000 1111 1234567891011# 位运算符print(bin(20))a = 60b = 13print(&quot;a = &quot;, bin(a), &quot;, b = &quot;, bin(b))print(&quot;a&amp;b =&quot;,bin(a&amp;b))print(&quot;a|b =&quot;,bin(a|b))print(&quot;a^b =&quot;,bin(a^b))print(&quot;~a =&quot;,bin(~a))print(&quot;a&lt;&lt;2 = &quot;,bin(a&lt;&lt;2))print(&quot;a&gt;&gt;2 = &quot;,bin(a&gt;&gt;2)) 输出结果： 123456780b10100a = 0b111100 , b = 0b1101a&amp;b = 0b1100a|b = 0b111101a^b = 0b110001~a = -0b111101a&lt;&lt;2 = 0b11110000a&gt;&gt;2 = 0b1111 逻辑运算符 Python语言支持逻辑运算符，以下假设变量 a 为 10, b为 20: 运算符 逻辑表达式 描述 实例 and x and y 布尔\"与\" - 如果 x 为 False，x and y 返回 False，否则它返回 y 的计算值。 (a and b) 返回 20。 or x or y 布尔\"或\" - 如果 x 是 True，它返回 x 的值，否则它返回 y 的计算值。 (a or b) 返回 10。 not not x 布尔\"非\" - 如果 x 为 True，返回 False 。如果 x 为 False，它返回 True。 not(a and b) 返回 False 123456# 逻辑运算符a = 10b = 20print(&quot;a and b = &quot;, a and b)print(&quot;a or b = &quot;, a or b)print(&quot;not(a and b) = &quot;, not(a and b)) 输出结果： 123a and b = 20a or b = 10not(a and b) = False 成员运算符 除了以上的一些运算符之外，Python还支持成员运算符，测试实例中包含了一系列的成员，包括字符串，列表或元组。 运算符 描述 实例 in 如果在指定的序列中找到值返回 True，否则返回 False。 x 在 y 序列中 , 如果 x 在 y 序列中返回 True。 not in 如果在指定的序列中没有找到值返回 True，否则返回 False。 x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 12345# 成员运算符x = [0, 1, 2, 3, 4, 5, 6, 7]y = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]print(&quot;x in y :&quot;, x in y)print(&quot;x not in y :&quot;, x not in y) 输出结果： 12x in y : Falsex not in y : True 身份运算符 身份运算符用于比较两个对象的存储单元 运算符 描述 实例 is is 是判断两个标识符是不是引用自一个对象 x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False is not is not 是判断两个标识符是不是引用自不同对象 x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 注： id() 函数用于获取对象内存地址。 1234567891011# 身份运算符x = 10y = xprint(&quot;x is y:&quot;, x is y)x = 10y = 10print(&quot;x is y:&quot;, x is y)print(&quot;id(x) == id(y)&quot;, id(x) == id(y))print(&quot;x is not y:&quot;, x is not y)id(x) 输出结果： 123456x is y: Truex is y: Trueid(x) == id(y) Truex is not y: False4312793616 is 与 == 区别： is 用于判断两个变量引用对象是否为同一个， == 用于判断引用变量的值是否相等。 运算符优先级 以下表格列出了从最高到最低优先级的所有运算符： 运算符 描述 ** 指数 (最高优先级) ~ + - 按位翻转, 一元加号和减号 (最后两个的方法名为 +@ 和 -@) * / % // 乘，除，取模和取整除 + - 加法减法 &gt;&gt; &lt;&lt; 右移，左移运算符 &amp; 位 'AND' ^ | 位运算符 &lt;= &lt; &gt; &gt;= 比较运算符 == != 等于运算符 = %= /= //= -= += *= **= 赋值运算符 is is not 身份运算符 in not in 成员运算符 not and or 逻辑运算符 注意：Pyhton3 已不支持 &lt;&gt; 运算符，可以使用 != 代替 本教程相关代码可在此查看 ​","link":"/Python-operators/"},{"title":"4. Python的流程控制","text":"Hi，大家好。我是茶桁。 在前面几节课的基础之上，我们今天开始尝试在Python中控制流程。这中间，让我们来做一些实际的练习。 Python语句的分类 让我们先了解一下Python语句的分类。 在Python中，可分为单行代码和代码块/组, 顾名思义，单行代码就是一行的Python代码，而代码块是以冒号作为开始，用缩进划分相同作用域，这样的结构称之为代码块，是一个整体。 12345678# 单行代码a = 123# 代码块if a == 123: print('True')else: print('False') 以上代码中输出结果为： 1True 在输入代码块的时候，我们要注意使用缩进。在其他语言中代码块可能是{}，但是在Python中严格遵守的缩进规则就是代码块。缩进可以是一个Tab距离或者四个空格，可是注意绝对不能混合使用，必须自使用一种方式缩进。 流程控制的分类 什么是流程？流程就是计算机执行代码时候的顺序。 流程可以被分为以下几类： 顺序结构 分支结构/选择结构 循环结构 顺序结构 顺序结构是系统的默认程序结构，自上而下进行执行。 分支结构 分支结构可以让代码走向不同的方向，不同的分支区间。 分支结构中又包含了单向分支，双分支和多分支以及巢状分支。 单向分支 单向分支就是在条件满足之后，执行后续任务。条件不满足的情况下，则不执行。 比如： 123456if 条件表达式： 一条python代码 a = Trueif a: print(&quot;True&quot;) 执行结果： 1True 一个经典案例： 程序员下班前女朋友打电话：下班路上买十个包子回来，如果看到卖西瓜的买一个 123456baozi = 10mxg = Falseif mxg: baozi = 1 print(&quot;买 %s 个包子&quot; %(baozi)) 输出结果： 1买 10 个包子 正常情况下，我们是直接买了10个包子回家，那如果我们看到了卖西瓜的呢？那么这段代码中等于是我们重新赋值了mxg, 就变成： 12345678910baozi = 10mxg = False# 走在路上看到了卖西瓜的，重新赋值mxg = Trueif mxg: baozi = 1 print(&quot;买 %s 个包子&quot; %(baozi)) 输出结果： 1买 1 个包子 双分支 双分支就是在单向分支的基础之上，又多了一个“否则”的选项，当条件不满足的时候执行其他操作。 123456789101112if 条件表达式: 一条python代码else: 另外一条python代码 person = 'girl'if person == 'girl': # 真区间 print(&quot;上前搭讪：美女，能加个微信吗？&quot;)else: # 假区间 print(&quot;直接走开。&quot;) 执行结果： 1上前搭讪：美女，能加个微信吗？ 以上就是一个双向的流程控制，这里面的含义为：表达式成立则执行真区间，如果不成立则执行假区间。 多分支 多分支就是在双分支的基础之上再增加其他可能出现的判断条件，用于执行更多的其他操作。 1234567891011121314if 条件表达式： 一条python代码 ...elif 条件表达式： 一条python代码 ...elif 条件表达式： 一条python代码 ......else: 一条python代码 ... 这段代码中的elif就是可能出现的不同条件，示例如下: 1234567891011score = 59if score &gt;= 90 and score &lt;= 100: print(&quot;奖励一个手机&quot;)elif score &gt;= 80 and score &lt; 90: print(&quot;今晚吃一顿好的奖励一下&quot;)elif score &gt;= 70 and score &lt; 80: print(&quot;鼓励：下次努力加油。&quot;)elif score &gt;= 60 and score &lt; 70: print(&quot;盯紧复习，争取下次进步。&quot;)else: print(&quot;奖励一顿‘竹笋炒肉’&quot;) 执行结果： 1奖励一顿‘竹笋炒肉’ 可以看到以上代码中，是从上到下依次进行判断条件，当所有条件都没有满足的时候，最后走到了else区间。 这就是多分支，需要判断多个表达式的结果，会自行其中符合条件的一个。 巢状分支 巢状分支，也就是嵌套分支。也就是if条件语句的嵌套： 12345678if 条件表达式： 代码语句 if 条件表达式: 代码语句 else: 代码语句else： 代码语句 示例： 1234567891011121314age = 25height = 177sex = 'male'if sex == 'male': # 可以往后判断 if age &gt;= 22 and age &lt;= 35: # 年龄比较合适 if height &gt;= 175: print(&quot;处一下试试...&quot;) else: print(&quot;拉到...&quot;)else: print('当闺蜜吧。') 输出结果： 1处一下试试... 在嵌套分支中我们需要注意，3 ～ 5层嵌套就是极限了，不要再往后嵌套。如果这个层数无法解决你的问题，那么可以重新梳理一下逻辑。基本大部分时候都是逻辑上有问题了。 分支 练习：十二生肖 当用户输入一个四位数的年份，计算当前这个年份对应的生肖： 申猴 酉鸡 戌狗 亥猪 子鼠 丑牛 寅虎 卯兔 辰龙 已蛇 午马 未羊 我们先来做一个用户输入的操作 123# 获取用户输入的年份year = input(&quot;请输入四位数的年份: &quot;)print(year, type(year)) 添加一个type()函数是为了验证用户输入之后的数据类型，当我们输入2023之后，可以看到输出结果为： 12023 &lt;class 'str'&gt; 证明虽然我们输入的是数字，但是被转成了字符串，那这个时候，我们就需要处理一下了： 123# 获取用户输入的年份year = int(input(&quot;请输入四位数的年份: &quot;))print(year, type(year)) 输出结果： 12023 &lt;class 'int'&gt; 这下就对了。 原本我们是需要讲位数，以及范围都控制在合理的数据内的。因为时间关系，在这整个示例中，我就不再去做更多的验证判断了。 123456789101112131415161718192021222324252627282930313233343536# 获取用户输入的年份year = int(input(&quot;请输入四位数的年份: &quot;))#print(year%12)num = year % 12&quot;&quot;&quot;申猴 酉鸡 戌狗 亥猪 子鼠 丑牛 寅虎 卯兔 辰龙 巳蛇 午马 未羊&quot;&quot;&quot;if num == 0: print(f'{year}年是 ==&gt; 申猴')elif num == 1: print(f'{year}年是 ==&gt; 酉鸡')elif num == 2: print(f'{year}年是 ==&gt; 戌狗')elif num == 3: print(f'{year}年是 ==&gt; 亥猪')elif num == 4: print(f'{year}年是 ==&gt; 子鼠')elif num == 5: print(f'{year}年是 ==&gt; 丑牛')elif num == 6: print(f'{year}年是 ==&gt; 寅虎')elif num == 7: print(f'{year}年是 ==&gt; 卯兔')elif num == 8: print(f'{year}年是 ==&gt; 辰龙')elif num == 9: print(f'{year}年是 ==&gt; 巳蛇')elif num == 10: print(f'{year}年是 ==&gt; 午马')elif num == 11: print(f'{year}年是 ==&gt; 未羊')else: print(&quot;您为输入正常的年份&quot;) 当我输入2023的时候，程序输出结果： 12023年是 ==&gt; 卯兔 程序是正常运行了（排除我没做特殊处理可能会出现的BUG），但是我们看这段代码，已经不能用丑陋来形容了。 让我们再改动一下，还记得咱们第二节课程中所学的list吗？这段代码中我们去判断的num是不是和list的下标是一模一样？OK，让我们利用下标来重新写一下这段代码： 123456# 获取用户输入的年份year = int(input('请输入四位数的年份：'))# 定义十二生肖 列表items = ['申猴', '酉鸡', '戌狗', '亥猪', '子鼠', '丑牛', '寅虎', '卯兔', '辰龙', '巳蛇', '午马', '未羊']print(f'{year}年是%s年' %(items[year % 12])) 这段代码输出结果为： 12023年是卯兔年 是不是比起第一段代码来优雅多了？ 循环结构 在完成了分支结构之后，我们来看一下循环结构。循环结构非常重要，必须熟练掌握。 为什么我们需要循环呢？先来看一段代码： 123456print(1)print(2)print(3)print(4)print(5).... 这段代码中，我们重复做了很多次打印的工作。这种事情，其实完全没必要重复去做，交给循环就可以了。 目前在Python中常用的循环有两个，while循环和for...in循环。 while 循环 12345while 条件表达式: 代码内容 代码内容 代码内容 ... 在while循环中，我们通常都会写带有条件变化的循环 1234num = 1while num &lt;=10: print(f'num为{num}') num += 1 输出结果： 12345678910num为1num为2num为3num为4num为5num为6num为7num为8num为9num为10 在这样一段代码中，在进入循环的时候，判断了一下当前条件是否成立。我们先设定了num的值为1，满足进入循环的条件，所以就进入了循环体，然后输出了num的值。 之后，每循环一次我们都对num做一次+1的处理. 也就是更改了变量。当变量更改后，会重新走到循环体的开始去判断条件。在循环11次之后，num就变成了11，不符合进入循环的条件了，循环自然被终止。也可以说，更该变量也是在朝着循环结束的方向在前进。 那么如果我们没有设定这个num的条件变化呢，自然就是无限的循环下去，最终导致内存溢出。 for循环 通常来说，for循环是用来遍历一个容器类型的数据。 1234for 自定义变量 in 容器数据: 代码内容,可以使用自定义变量 代码内容,可以使用自定义变量 代码内容,可以使用自定义变量 使用for...in循环遍历容器类型数据，那么中间的自定义变量就是当前容器类型中的每一个元素。 示例： 123n = '123456789'for i in n: print(i) 输出结果： 123456789123456789 在整个for...in循环体内，我们经常使用range()函数来迭代输出一个范围，比如： 12for i in range(0, 10): print(i) 输出结果为： 123456789100123456789 可以看到我们输出了从0开始，一直到9结束， 一共输出了10个数字。 从结果中，我们大致可以猜到range()函数中(a, b)的含义为：从a开始循环输出，输出到b（不包含b）为止, 比如，我们将刚才的数字改为range(1,8)，那么我们最后输出的内容就会是： 12345671234567 其他流程控制语句 在循环体中，我们还经常应用一些其他的控制语句，用于程序的正常执行和中止。这其中包括 continue语句， 用于跳过当前这一次循环 break语句，用于结束或者跳出 pass语句， 用于占位 12345678num = 1while num &lt;= 10: num += 1 # 判断当前的num是否为偶数 if num % 2 == 0: continue # 跳过本次循环，执行下一次循环 print(num) 输出结果： 12345357911 可以从结果中看到，每次num为偶数的时候，打印并未执行，被跳过了。 让我们来更改一下这一段代码： 123456789num = 1while num &lt;= 10: num += 1 # 判断当前的num是否为偶数 if num % 2 == 0: continue # 跳过本次循环，执行下一次循环 if num == 7: break # 跳出并结束循环，不再继续执行。 print(num) 输出结果为： 1235 结果中我们可以看到，代码只输出到了5。我们来剖析一下整个代码，当代码为5的时候，print()函数还是正常执行了一次，然后再进来的时候，num在最前方+1变为了6，执行了continue，跳出了本次循环。再进入循环之后，num +1 变成了7，这个时候进入了第二个if判断，直接执行了break语句，跳出并结束了整个循环。这样，print()函数这无法再继续执行下去了。 特殊语句 exit() quit() 这两个特殊语句，均是用于结束程序的执行，exit()和quit()之后的代码不会执行。在单纯的循环结构中的作用与break很像，但是完全不能混为一谈。这两个语句是用于结束并退出当前python解释器的，而break仅用于结束当前的循环体。 练习 打印矩形 让我们循环出十行十列 ★ ☆ ，隔一行换色，再做一个隔一列换色。 在最开始，我们先思考一下，十行十列，那就是完成100次打印。我们先把这部分实现一下： 输出结果因为占篇幅，我就不写了，大家自行执行就可以了。 1234num = 0while num &lt; 100: print('☆', end = &quot; &quot;) num += 1 在这之后，我们需要考虑一下，既然是十行十列，那说明我们每隔10个就需要一次换行： 1234567num = 0while num &lt; 100: print('☆', end = &quot; &quot;) # 判断是否需要换行 if num % 10 == 9: print('\\n') num += 1 现在打印出了十行☆, 每一行十个。第一步我们已经实现了，那么现在，让我们来尝试一下隔一行打印一个不同的。思考一下，其实就是奇偶数的问题，想明白之后，接下来就好办了： 123456789101112# 隔列换色num = 0while num &lt; 100: # 判断当前是基数还是偶数 if num % 2 == 0: print('☆', end = &quot; &quot;) else: print('★', end = &quot; &quot;) # 判断是否需要换行 if num % 10 == 9: print('\\n') num += 1 隔列换色实现之后，我们再来考虑一下隔行换色，让我们从隔列换色上找一点灵感。既然隔列换色是奇偶数的问题，那么隔行换色，是不是就是每一行的奇偶数问题？ 那么我们如何对行数做判断呢？其实很简单，我们只要对当前数字做取整数操作：num // 10，然后获取到的整数再来取余就行了。 那么我们就可以这样来实现： 123456789101112# 隔行换色num = 0while num &lt; 100: # 以当前行数为基数，对2取余，判断奇偶 if num // 10 % 2 == 0: print('☆ ', end = &quot; &quot;) else: print('★', end = &quot; &quot;) # 判断是否需要换行 if num % 10 == 9: print('\\n') num += 1 大家可以执行去操作一下试试，建议使用Jupyter Notebook，这种实验性的代码块，很方便得到结果。 打印乘法口诀表 这也是Python教学中经常被拿来进行教学的一个经典案例，和上一个练习一样，我们一边分析，便来完善代码。 整个代码中，我们用到了刚才学到的for...in循环以及range()函数。 首先我们利用range()函数，输出1到9，每输出一个换一次行： 1234# 乘法口诀表for x in range(1, 10): # 换行 print() 然后我们在每一行内再做一次循环，输出每一行的序列, 当然还是从1开始。 12345678# 乘法口诀表for x in range(1, 10): # 第二层循环，内循环 # 内循环负责当前行的函数，第一行 1列 2行 2列....9行 9列 for y in range(1, 10): print(f'{x}x{y}={x*y}', end=&quot; &quot;) # 换行 print() 这里需要注意，就乘法表而言，我们最大列不能大于这一行的被乘数, 那么我们range()需要调整一下： 12345678# 乘法口诀表for x in range(1, 10): # 第二层循环，内循环 # 内循环负责当前行的函数，第一行 1列 2行 2列....9行 9列 for y in range(1, x+1): print(f'{x}x{y}={x*y}', end=&quot; &quot;) # 换行 print() 斐波那契数列 再来让我们多做一个练习，斐波那契数列。 在做这个练习之前，首先我们需要了解什么是斐波那契数列。我这里应用一下维基百科的解释： 斐波那契数所形成的数列称为斐波那契数列。这个数列是由意大利数学家斐波那契在他的《算盘书》中提出。在数学上，斐波那契数是以递归的方法来定义： \\(F_0=0\\) \\(F_1=1\\) \\(F_n=F_{n-1}+F_{n-2}(n&gt;=2)\\) 用文字来说，就是斐波那契数列由0和1开始，之后的斐波那契数就是由之前的两数相加而得出。首几个斐波那契数是：1、 1、 2、 3、 5、 8、 13、 21、 34、 55、 89、 144、 233、 377、 610、 987…… 了解之后，让我们来分析一下： 10, 1, 1, 2, 3, 5, 8, 13... 第0项如果是0，那么第一项是1， 第二项也是1， 之后的第三项开始，每一项都是前面两个数的和。 因为这个数列是一个无限递归下去的数列，我们不能无限的计算下去，所以需要先知道自己计算多少项： 12# 获取用户输入的数据num = int(input('你需要计算多少项？')) 之前我们分析得到，第三项开始，每一项是前面两个数的和，那么，我们需要定义两个变量用来承载相加的两个数，再多定义一个初始值，用于判断是否执行循环： 1234num = int(input('你需要计算多少项？'))n1 = 0n2 = 1count = 2 然后，让我们开始进入正题，需要先判断用户输入的数字是否正整数，我们先不搞那么复杂，只需要简单判断一下是否大于等于0，然后再判断用户输入是否为1， 因为如果是只输出1项，那么就不需要计算了，直接输出n1就好了： 1234567891011num = int(input('你需要计算多少项？'))n1 = 0n2 = 1count = 2# 从之后的数字开始计算if num &lt;= 0: print('请输入一个正整数。')elif num == 1: print(f'斐波那契数列: {n1}')else: pass # 占位 然后，让我们正式进入循环计算, 现在n1为第一项，n2就是第二项，直接输出就可以了 1234567891011num = int(input('你需要计算多少项？'))n1 = 0n2 = 1count = 2# 从之后的数字开始计算if num &lt;= 0: print('请输入一个正整数。')elif num == 1: print(f'斐波那契数列: {n1}')else: print(f'斐波那契数列: {n1}, {n2}', end = &quot;, &quot;) 之后，我们去判断count是否小于用户输入的数字，如果小于，就进入循环。然后再循环内定义一个变量n3， 用来承载相加之后得到的结果，作为当前项输出。再讲n1, n2重新赋值。不要忘了给count加值。 1234567891011121314151617num = int(input('你需要计算多少项？'))n1 = 0n2 = 1count = 2# 从之后的数字开始计算if num &lt;= 0: print('请输入一个正整数。')elif num == 1: print(f'斐波那契数列: {n1}')else: print(f'斐波那契数列: {n1}, {n2}', end = &quot;, &quot;) while count &lt; num: n3 = n1 + n2 print(n3, end = &quot;, &quot;) # 更新数据 n1, n2 = n2, n3 count += 1 当我们输入9的时候，输出结果： 1斐波那契数列: 0, 1, 1, 2, 3, 5, 8, 13, 21, 百钱买百鸡 让我们先来说明一下这个题目： 123一共有100块钱，需要买100只鸡公鸡 3元钱一只，母鸡1元钱一只，小鸡5毛钱一只。要求计算，100块钱买100只鸡，一共有多少种方案 在这个题目里，我们可以计算如果只买一种，这公鸡可以有33只，母鸡有100只，小鸡这可以买200只。 这里面我们可以思考一下，这里我们一共需要3个变量和2个常量，变量为公鸡，母鸡以及小鸡；2个常量为100块钱和总共100只鸡。 先让我们从循环体来开始写： 12345678910num = 0for gj in range(1, 34): for mj in range(1, 101): for xj in range(1, 201): # 判断是否为100只，是否话费100元 if gj + mj + xj == 100 and gj*3 + mj + xj*0.5 == 100: print(f'公鸡{gj}只，母鸡{mj}只，小鸡{xj}只， 共花费{gj*3 + mj + xj*0.5}元') num += 1print(num) 这里，我们是计算了三只都买的情况，那么其实还有一种额外的情况，就是我们一开始说的，100块钱都买母鸡的情况，也正好是100块钱100只鸡。所以，我们的num要从1开始计数 12345678910num = 1for gj in range(1, 34): for mj in range(1, 101): for xj in range(1, 201): # 判断是否为100只，是否话费100元 if gj + mj + xj == 100 and gj*3 + mj + xj*0.5 == 100: print(f'公鸡{gj}只，母鸡{mj}只，小鸡{xj}只， 共花费{gj*3 + mj + xj*0.5}元') num += 1print(num) 输出结果（只看num）： 120 也就是说，我们目前一共有20种组合方案。具体有哪些方案，有兴趣的小伙伴可以执行我所写的代码，会打印出来。 虽然解决问题了，可是这并不是最好的写法。 看看这团垃圾的效率：第一层需要计算34次， 第一层每次计算，第二层都要计算100次，第二层每跑一遍，第三层需要计算200次.... 这简直就是一堆米田共。当我们加上一个计数变量稍微统计一下到底计算了多少次 1234567count = 0... for xj in range(1, 201): count += 1 # 判断是否为100只，是否话费100元...print(count) 可以得到最后结果为： 1660000 是不是很恐怖？让我们改动一下代码，优化性能： 让我们来思考一下，100只鸡这个总数是不是固定不变的？那么公鸡，母鸡的计算得到之后，是不是小鸡的数量就得到了。还有必要在进入一次循环吗？肯定没必要了对不对？所以我们这样改动： 12345678910111213count = 0num = 1for gj in range(1, 34): for mj in range(1, 101): xj = 100 - gj - mj count += 1 # 判断是否为100只，是否话费100元 if gj + mj + xj == 100 and gj*3 + mj + xj*0.5 == 100: print(f'公鸡{gj}只，母鸡{mj}只，小鸡{xj}只， 共花费{gj*3 + mj + xj*0.5}元') num += 1print(f'一共有{num}种组合方式。')print(f'当前一共计算了{count}次') 最后得到的计算结果： 12一共有20种组合方式。当前一共计算了3300次 从660000次一下下降到了3300次，这个性能的提示是很大的了。 所以，很多问题我们不要只追求解决，要善于多思考。 那么至此，我们这节课也就结束了。让我们最后放几个思考题： 思考题 对于我们打印矩阵，完成了隔行上色和隔列上色的问题，我们思考一下该如何解决三角形和菱形； 对于乘法表，思考下我们如何完成反向打印。 解决了思考题的小伙伴，可以在评论区留言。期待看到大家的想法。我是茶桁，咱们下次见，下一节课，我们进入「模块化编程」，开始学习函数。","link":"/Python-process-control/"},{"title":"06 快速建立一个AI应用","text":"Hi，我是茶桁。 在过去的两讲中，我们已经使用 OpenAI 提供的 Embedding 接口完成了文本分类的功能。现在，我们回到 Completion 接口，这一讲将带你更深入地了解该接口的使用。除此之外，我们还将快速搭建一个有界面的聊天机器人，这将让你更好地理解 Completion 接口的应用场景。在这个过程中，你将第一次使用 HuggingFace 这个平台，它是目前最流行的深度学习模型社区。通过 HuggingFace，你可以下载到最新的开源模型，查看其他人提供的示例代码，并参与到社区的交流中。 价廉高质的ChatGPT 我们在第三讲里介绍了Completion接口，并且通过它实现了一个聊天机器人的功能。在那个时候，我们采用的是自己将整个对话拼接起来，将整个上下文都发送给 OpenAI 的 Completion API 的方式。不过，在 3 月 2 日，因为 ChatGPT 的火热，OpenAI 放出了一个直接可以进行对话聊天的接口。这个接口叫做 ChatCompletion，对应的模型叫做 gpt-3.5-turbo，不但用起来更容易了，速度还快，而且价格也是我们之前使用的 text-davinci-003 的十分之一，可谓是物美价廉了。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Quickly-build-an-AI-application/"},{"title":"08 改写和审核","text":"Hi, 我是茶桁。 我们已经介绍了 OpenAI 的主要接口。这是基础知识系列的最后一讲，我们将讨论 OpenAI GPT 系列模型的其他接口。你可能不会经常使用其中一些接口，但了解它们不会有任何坏处，说不定你会在某些需求中用到它们。 在这篇文章中，我们将一起探讨 OpenAI 为文本改写和内容审核提供的功能，以及 GPT 系列模型的种类、区别和应用场景。 文本改写教程 我猜你可能已经用过许多基于 AI 大型语言模型的产品了。其中很常见的一类应用是写作助手，比如 Notion AI。它可以帮助你在文章中选择一段内容，并让 AI 帮你修改它，例如缩短文本或改变语气等。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Rewriting-and-Reviewing/"},{"title":"AI ability practice","text":"Including core capabilities, BI and algorithm related, Code warehouse: 【AI Basic](https://github.com/hivandu/practise/tree/master/AI-basic)","link":"/README/"},{"title":"SQL练习1","text":"-- UPPER 是转换大写的函数 1SELECT emp_name,salary * 12, UPPER(email) FROM employee; -- 使用别名 1234SELECT e.emp_name AS &quot;姓名&quot;, salary * 12 AS &quot;12月的工资&quot;, UPPER(email) &quot;电子邮箱&quot;FROM employee AS e; -- 无表查询 1234567SELECT 1+1;-- Oracle 实现，dual只有一个字段且只包含一行数据SELECT 1+1FROM dual;SELECT * FROM employee WHERE emp_name = '刘备'; -- 比较运算符 123select * from employee where sex &lt;&gt; '男'select * from employee where salary BETWEEN 5000 and 7000select * from employee where emp_name IN('刘备') -- 一个时间段之后入职 1234567select emp_name, hire_date from employee where hire_date &gt; DATE '2018-01-01'SELECT emp_name,hire_date,manager from employee where manager IS NULLSELECT emp_name, sex, salary from employee where sex = '女' AND salary &gt; 10000SELECT emp_name, sex, salary FROM employee WHERE emp_name = '刘备' OR emp_name = '张飞' OR emp_name = '赵云' -- 短路运算 short-circuit evaluation 12345select 'AND' FROM employee WHERE 1 = 0 AND 1/0 = 1;SELECT 'OR' FROM employee where 1 = 1 OR 1/0 = 1;-- NOTselect emp_id,emp_name FROM employee WHERE emp_name NOT IN('刘备','张飞','赵云') -- 运算符优先级 123SELECT emp_name,dept_id,bonusFROM employeeWHERE (dept_id = 2 OR dept_id = 3) AND bonus IS NOT NULL; -- 去处重复值 12SELECT DISTINCT SEX FROM employeeSELECT DISTINCTROW sex from employee / 查找 2018 年 1 月 1 日之后入职，月薪小于 5000，并且奖金小于 1000（包括没有奖金）的员工。 / 12345SELECT emp_id,emp_name,salary,hire_date,bonus FROM employee WHERE hire_date &gt; '2018-01-01' AND salary &lt; 5000 AND (bonus &lt; 1000 OR bonus IS NULL) -- LIKE运算符 1234567SELECT emp_id,emp_name,sexFROM employeeWHERE emp_name LIKE '赵%'SELECT emp_name,emailFROM employeeWHERE email NOT LIKE 'dengzh_@shuguo.com'; -- 转义字符 1234567891011CREATE TABLE t_like(c1 VARCHAR(20))INSERT INTO t_like(c1) VALUES ('进度:25% 已完成')INSERT INTO t_like(c1) VALUES ('日期:2019年5月25日')SELECT c1FROM t_likeWHERE c1 LIKE &quot;%25\\%%&quot;SELECT c1FROM t_likeWHERE c1 LIKE &quot;%25#%%&quot; ESCAPE '#' -- 大小写匹配 123SELECT emp_name,emailFROM employeeWHERE email LIKE 'M%' -- 正则表达式 判断邮箱 1234567891011/*以字母或者数字开头；后面是一个或者多个字母、数组或特殊字符（ . _ - ）；然后是一个 @ 字符；之后包含一个或者多个字母、数组或特殊字符（ . - ）；最后是域名，即 . 以及 2 到 4 个字母。^[a-zA-Z0-9]+[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}$*/SELECT email FROM t_regexpWHERE REGEXP_LIKE (email, BINARY '^[a-z0-9]+[a-z0-9._-]+@[a-z0-9.-]+\\\\.[a-z]{2,4}$','i'); -- 降序排序 1234select emp_name,salary,hire_datefrom employeewhere dept_id = 4ORDER BY salary DESC; -- 多列排序(工资，入职先后) 1234select emp_name,salary,hire_dateFROM employeewhere dept_id = 4ORDER BY salary DESC, hire_date; -- 按照SELECT顺序 1234SELECT emp_name, salary, hire_dateFROM employeeWHERE dept_id = 4ORDER BY 2 desc, 3 中文排序 -- CONVERT 是一个函数，用于转换数据的字符集编码。以下转换为中文GBK字符集 1234SELECT emp_namefrom employeeWHERE dept_id = 4ORDER BY CONVERT(1 USING GBK) -- 空值排序 1234select emp_name,bonusfrom employeewhere dept_id = 2ORDER BY 2 -- 其他空值排序方法 12345-- COALESCE函数将控制转换为一个指定的值SELECT emp_name, COALESCE(bonus,0) AS bonusFROM employeewhere dept_id = 2ORDER BY COALESCE(bonus,0); / 第六节练习: 查询所有的员工信息，按照员工的总收入（年薪加奖金）从高到低进行排序，总收入相同再按照姓名的拼音顺序排序。 / 1234SELECT emp_name, salary, bonus, (salary+bonus) as sumFROM employeeWHERE dept_id &gt;= 0ORDER BY (salary+bonus) DESC, CONVERT(1 USING GBK) -- TopN 排行榜 123456789101112131415161718-- 标准FETCH语法,此语法MySQL不支持，Oracle, PostgreSQL支持SELECT emp_name, salaryFROM employeeORDER BY salary DESCOFFSET 0 ROWSFETCH FIRST 5 ROWS ONLY;-- LIMIT实现TOPN排行榜SELECT emp_name, salaryFROM employeeORDER BY salary DESCLIMIT 5 OFFSET 0;-- 第二种写法SELECT emp_name,salaryFROM employeeORDER BY salary DESCLIMIT 0,5 -- SQL 实现分页查询 1234SELECT emp_name, salaryFROM employeeORDER BY salary DESCLIMIT 10,5 -- 员工排名第3高 123456789select emp_name,salary FROM employeeWHERE salary = ( select salary from employee ORDER BY salary DESC LIMIT 2,1)SELECT emp_name,salary FROM employeeORDER BY salary desc limit 5 offset 10; / 练习：使用LIMIT和OFFSET找出员工表中月薪排名第三高的所有员工 / 123456select emp_name, salary FROM employeewhere salary = ( select salary FROM employee ORDER BY salary desc limit 1 offset 2)","link":"/SQL_ext_1/"},{"title":"SVM-based Text Classification in Practice","text":"The source code: SVM-based Text Classification in Practice 'cnews.train.txt' data cannot be uploaded because it is too large, so it needs to be decompressed and imported after compression. Use SVM to implement a simple text classification based on bag of words and support vector machine. import data 1234# importimport codecsimport osimport jieba Chinese news data is prepared as a sample data set. The number of training data is 50,000 and the number of test data is 10,000. All data is divided into 10 categories: sports, finance, real estate, home furnishing, education, technology, fashion, current affairs, games and entertainment . From the training text, you can load the code, view the data format and samples: 1234567891011data_train = './data/cnews.train.txt' # training data file name data_test = './data/cnews.test.txt' # test data file namevocab = './data/cnews.vocab.txt' # dictionarywith codecs.open(data_train, 'r', 'utf-8') as f: lines = f.readlines()# print sample contentlabel, content = lines[0].strip('\\r\\n').split('\\t')content Take the first item of the training data as an example to segment the loaded news data. Here I use the word segmentation function of LTP, you can also use jieba, and the segmentation results are displayed separated by \"/\" symbols. 123# print word segment resultssegment = jieba.cut(content)print('/'.join(segment)) To sort out the above logic a bit, implement a class to load training and test data and perform word segmentation. 123456789101112131415161718192021222324# cut datadef process_line(idx, line): data = tuple(line.strip('\\r\\n').split('\\t')) if not len(data)==2: return None content_segged = list(jieba.cut(data[1])) if idx % 1000 == 0: print('line number: {}'.format(idx)) return (data[0], content_segged) # data loading methoddef load_data(file): with codecs.open(file, 'r', 'utf-8') as f: lines = f.readlines() data_records = [process_line(idx, line) for idx, line in enumerate(lines)] data_records = [data for data in data_records if data is not None] return data_records# load and process training datatrain_data = load_data(data_train)print('first training data: label {} segment {}'.format(train_data[0][0], '/'.join(train_data[0][1])))# load and process testing datatest_data = load_data(data_test)print('first testing data: label {} segment {}'.format(test_data[0][0], '/'.join(test_data[0][1]))) After spending some time on word segmentation, you can start building a dictionary. The dictionary is built from the training set and sorted by word frequency. 12345678910111213141516171819202122def build_vocab(train_data, thresh): vocab = {'&lt;UNK&gt;': 0} word_count = {} # word frequency for idx, data in enumerate(train_data): content = data[1] for word in content: if word in word_count: word_count[word] += 1 else: word_count[word] = 1 word_list = [(k, v) for k, v in word_count.items()] print('word list length: {}'.format(len(word_list))) word_list.sort(key = lambda x : x[1], reverse = True) # sorted by word frequency word_list_filtered = [word for word in word_list if word[1] &gt; thresh] print('word list length after filtering: {}'.format(len(word_list_filtered))) # construct vocab for word in word_list_filtered: vocab[word[0]] = len(vocab) print('vocab size: {}'.format(len(vocab))) # vocab size is word list size +1 due to unk token return vocabvocab = build_vocab(train_data, 1) In addition, according to category, we know that the label itself also has a \"dictionary\": 12345678910def build_label_vocab(cate_file): label_vocab = {} with codecs.open(cate_file, 'r', 'utf-8') as f: for lines in f: line = lines.strip().split('\\t') label_vocab[line[0]] = int(line[1]) return label_vocablabel_vocab = build_label_vocab('./data/cnews.category.txt')print(f'label vocab: {label_vocab}') Next, construct the id-based training and test sets, because we only consider the bag of words, so the order of words is excluded. Constructed to look like libsvm can eat. Note that because the bag of word model 12345678910111213141516171819202122def construct_trainable_matrix(corpus, vocab, label_vocab, out_file): records = [] for idx, data in enumerate(corpus): if idx % 1000 == 0: print('process {} data'.format(idx)) label = str(label_vocab[data[0]]) # label id token_dict = {} for token in data[1]: token_id = vocab.get(token, 0) if token_id in token_dict: token_dict[token_id] += 1 else: token_dict[token_id] = 1 feature = [str(int(k) + 1) + ':' + str(v) for k,v in token_dict.items()] feature_text = ' '.join(feature) records.append(label + ' ' + feature_text) with open(out_file, 'w') as f: f.write('\\n'.join(records))construct_trainable_matrix(train_data, vocab, label_vocab, './data/train.svm.txt')construct_trainable_matrix(test_data, vocab, label_vocab, './data/test.svm.txt') Training process The remaining core model is simple: use libsvm to train the support vector machine, let your svm eat the training and test files you have processed, and then use the existing method of libsvm to train, we can change different parameter settings . The documentation of libsvm can be viewed here, where the \"-s, -t, -c\" parameters are more important, and they decide what you choose Svm, your choice of kernel function, and your penalty coefficient. 1234567891011121314from libsvm import svmfrom libsvm.svmutil import svm_read_problem,svm_train,svm_predict,svm_save_model,svm_load_model# train svmtrain_label, train_feature = svm_read_problem('./data/train.svm.txt')print(train_label[0], train_feature[0])model=svm_train(train_label,train_feature,'-s 0 -c 5 -t 0 -g 0.5 -e 0.1')# predicttest_label, test_feature = svm_read_problem('./data/test.svm.txt')print(test_label[0], test_feature[0])p_labs, p_acc, p_vals = svm_predict(test_label, test_feature, model)print('accuracy: {}'.format(p_acc)) After a period of training, we can observe the experimental results. You can change different svm types, penalty coefficients, and kernel functions to optimize the results.","link":"/SVM-based_text_classification_in_practice/"},{"title":"11 用好开源模型节约成本","text":"Hi， 大家好，我是茶桁。 直奔主题，我们来谈谈成本这件事。 大家应该都知道，ChatGPT对免费用户是有5美元的API调用额度的，说是这么说，可是那是以前，现在新注册的小伙伴应该都发现自己的API Key根本无法调用API，原因是这个免费额度似乎已经失效了。而我可以直接说，在我从第一节到第10节的课程中所用到的金额，已经超过这个数目了。也就是说，我这10节课API调用成本就已经超过了40元人民币。 看到这大家大概能理解我这个课程为什么改为付费课程了吧？ 对于 ChatCompletion 的接口来说，为了更好地使用它，我们需要传入更多的上下文信息，以便更准确地进行文本生成。不过要注意的是，实际消耗的 Token 数量可能比我们感觉的要多。此外，除了费用之外，数据安全也是我们需要考虑的一个问题。由于每个国家的数据监管要求不同，不是所有的数据都适合通过 OpenAI 的 API 来处理。因此，我们需要寻找一个除 OpenAI 以外的解决方案。幸运的是，有一些开源的大语言模型可以帮助我们解决这个问题。通过利用这些开源的模型，中小型公司也可以轻松地获得更准确、更安全的文本生成服务。 在 Colab 中使用 GPU 在本课中，我们需要使用一些开源模型。但是，并不是所有人的电脑都配备了强劲的 NVIDIA GPU。因此，我建议您使用 Colab 运行相应的笔记本，并注意将运行环境设置为 GPU。 如下图，选择 代码执行程序-&gt;更改运行时类型,然后在硬件加速器上选择 GPU 就可以了。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Save-costs-with-an-open-source-model/"},{"title":"「泰坦尼克」生存预测","text":"最好的学习就是输出,所以虽然这个预测很多人做过了,我还是在这里再做一遍,纯粹是为了自己学习. 前言 这次预测使用的是Sklearn中的决策树模型: 1clf = DecisionTreeClassifier(criterion='entropy') 其中criterion是标准,决定了构造分类树是采用ID3分类树还是CART分类树,对应的取值分别是entropy和gini entropy: 基于信息熵,也就是ID3算法, 实际结果与C4.5相差不大; gini: 默认参数,基于基尼系数. CART算法是基于基尼系数做属性划分的,所以criterion=gini时, 实际上执行的是CART算法. 其完整参数: 123456DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best') 参数代表的含义如下表: 参数表 作用 criterion 在基于特征划分数据集合时，选择特征的标准。默认是 gini,也可以是entropyo splitter 在构造树时，选择属性特征的原则，可以是best或者 random。默认是best,best代表在所有的特征中选择最 好的，random代表在部分特征中选择最好的。 max_depth 决策树的最大深度，我们可以控制决策树的深度来防止 决策树过拟合 max_features 在划分数据集时考虑的最多的特征值数量。为int或float类型。其中int值是每次split时最大特征数；float值是百 分数，即特征数=max_features * n_featureso min_samples_split 当节点的样本数少于min_samples_split时，不再继续分 裂。默认值为2 min_samples_leaf 叶子节点需要的最少样本数。如果某叶子节点数目小于 这个阈值，则会和兄弟节点一起被剪枝。 min_samples_leaf的取值可以是int或float类型。 int类型：代矗小样本数； float类型：表示一个百分比，这是最小样本数 =min_samples_leaf乘以样本数量，并向上取整。 max_leaf_nodes 最大叶子节点数。int类型，默认为None。 默认情况下是不设置最大叶子节点数，特征不多时，不 用设置。特征多时，可以通过设置最大叶子节点数，防 止过拟合。 min_impurity_decrease 节点划分最小不纯度。float类型，默认值为0。 节点的不纯度必须大于这个阈值，否则该节点不再生成 子节点。通过设置，可以限制决策树的增长。 minjmpurity_split 信息増益的阀值。信息増益必须大于这个阀值，否则不 分裂。 class_weight 类别权重。默认为None,也可以是diet或balanced。 diet类型：指定样本各类别的权重，权重大的类别在决策 树构造的时候会进行偏倚。 balanced:算法自己计算权重，样本量少的类别所对应 的样本权重会更高。 presort bool类型，默认是false,表示在拟合前，是否对数据进 行排序来加快树的构建。当数据集较小时，使用 presort=true会加快分类器构造速度。当数据集庞大 时，presort=true会导致整个分类非常缓慢。 在构造决策树分类器后,我们可以使用fit方法让他分类器进行拟合, 使用predict方法对新数据进行预测, 得到预测的分类结果, 也可以使用score方法得到分类器的准确率. fit、predict和score方法的作用如下表: 方法表 作用 fit(features, labels) 通过特征矩阵, 分类表示,让分类器进行拟合 predict(features) 返回预测结果 score(features, labels) 返回准确率 本次数据集一共两个,一个是train.csv, 用于训练, 包含特征信息和存活与否的标签, 一个是test.csv, 测试数据集, 只包含特征信息. 训练集中,包括了以下字段: 字段 描述 Passengerld 乘客编号 Survived 是否幸存 Pclass 船票等级 Name 乘客姓名 Sex 乗客性别 SibSp 亲戚数虽（兄妹、配偶数） Parch 亲戚数虽（父母、子女数） Ticket 船票号码 Fare 船票价格 Cabin 船舱 Embarked 登陆港口 流程 整个流程可以划分为三个阶段: 获取数据 准备阶段 数据探索 数据清洗 特征选择 分类阶段 决策树模型 模型评估&amp;预测 决策树可视化 获取数据 这一步还包含了引入所需依赖 123456789101112# 引入依赖import pandas as pdfrom sklearn.feature_extraction import DictVectorizerfrom sklearn.tree import DecisionTreeClassifierimport os# 准备工作path = os.path.expanduser('~/data/python/Titanic_Data/')# 获取数据train_data = pd.read_csv(path + 'train.csv')test_data = pd.read_csv(path + 'test.csv') 准备阶段 对数据进行探索,分析数据质量,并对数据进行清洗,然后通过特征选择对数据进行降维, 以便于之后进行分类运算; 数据探索 123456train_data.info() # 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度train_data.describe() # 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等train_data.describe(include=['O']) #查看字符串类型 (非数字) 的整体情况train_head(5) # 查看前几行数据 (默认是前 5 行)train_tail(5) # 查看后几行数据 (默认是最后 5 行)train_sample(5) # 查看随机几行数据 (默认是随机1行) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 运行结果&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId 891 non-null int64Survived 891 non-null int64Pclass 891 non-null int64Name 891 non-null objectSex 891 non-null objectAge 714 non-null float64SibSp 891 non-null int64Parch 891 non-null int64Ticket 891 non-null objectFare 891 non-null float64Cabin 204 non-null objectEmbarked 889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.6+ KBNone------------------------------ PassengerId Survived ... Parch Farecount 891.000000 891.000000 ... 891.000000 891.000000mean 446.000000 0.383838 ... 0.381594 32.204208std 257.353842 0.486592 ... 0.806057 49.693429min 1.000000 0.000000 ... 0.000000 0.00000025% 223.500000 0.000000 ... 0.000000 7.91040050% 446.000000 0.000000 ... 0.000000 14.45420075% 668.500000 1.000000 ... 0.000000 31.000000max 891.000000 1.000000 ... 6.000000 512.329200[8 rows x 7 columns]------------------------------ Name Sex ... Cabin Embarkedcount 891 891 ... 204 889unique 891 2 ... 147 3top Peter, Mrs. Catherine (Catherine Rizk) male ... B96 B98 Sfreq 1 577 ... 4 644[4 rows x 5 columns]------------------------------ PassengerId Survived Pclass ... Fare Cabin Embarked0 1 0 3 ... 7.2500 NaN S1 2 1 1 ... 71.2833 C85 C2 3 1 3 ... 7.9250 NaN S3 4 1 1 ... 53.1000 C123 S4 5 0 3 ... 8.0500 NaN S[5 rows x 12 columns]------------------------------ PassengerId Survived Pclass ... Fare Cabin Embarked886 887 0 2 ... 13.00 NaN S887 888 1 1 ... 30.00 B42 S888 889 0 3 ... 23.45 NaN S889 890 1 1 ... 30.00 C148 C890 891 0 3 ... 7.75 NaN Q[5 rows x 12 columns]------------------------------ PassengerId Survived Pclass ... Fare Cabin Embarked619 620 0 2 ... 10.5000 NaN S330 331 1 3 ... 23.2500 NaN Q647 648 1 1 ... 35.5000 A26 C716 717 1 1 ... 227.5250 C45 C860 861 0 3 ... 14.1083 NaN S[5 rows x 12 columns] 数据清洗 探索之后, 我们发现Age、Cabin这两个字段的数据有缺失. 其中, Cabin为船舱, 有大量的缺失值, 在训练集和测试集中的缺失率分别为77%和78%, 无法补齐, Age可以获取平均值进行补齐, 而Embarked是登陆港口, 这个字段也有少量(2个)缺失值, 可以使用最大数据进行补齐. 12345train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)test_data['Age'].fillna(test_data['Age'].mean(), inplace=True)train_data['Embarked'].fillna(train_data['Embarked'].value_counts().idxmax(), inplace=True)test_data['Embarked'].fillna(test_data['Embarked'].value_counts().idxmax(), inplace=True) 分类阶段 特征选择 需要选择有用的字段作为特征,这一步其实很重要: 123456789# 特征选择train_data.columns# 从上一句的结果中选择特征字段features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Parch', 'Embarked']train_features = train_data[features]test_features = test_data[features]train_labels = train_data['Survived'] 这中间有一些事字符串字段, 是不适合进行后续运算的, 需要在这里转变为数值类型,比如Sex字段, 男女两种取值需要转变成0和1 再比如Embarked有S, C, Q三种可能, 我们可以改成Embarked=S, Embarked=C, Embarked=Q三个字段,然后用数值0和1来表示, 其中sklearn特征选择中的DictVectorizer类(上面已引入依赖), 可以处理符号化的对象, 将符号转变为0/1进行表示: 12dvec=DictVectorizer(sparse=False)train_features=dvec.fit_transform(train_features.to_dict(orient='record')) fit_transform这个函数可以讲特征向量转化为特征值矩阵, 我们查看下: 1dvec.feature_names_ 123# 运行结果:['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp'] 我们讲Embarked转化为三列 (['Embarked=C', 'Embarked=Q', 'Embarked=S']), Sex变为了两列 ([Sex=female', 'Sex=male']) 决策树模型 1234# 构造ID3决策树clf=DecisionTreeClassifier(criterion='entropy')# 决策树训练clf.fit(train_features, train_labels) 模型预测 &amp; 评估 我们首先得到测试集的特征值矩阵, 然后使用训练好的决策树clf进行预测, 得到预测结果: 123test_features=dvec.transform(test_features.to_dict(orient='record'))# 决策树预测pred_labels=clf.predict(test_features) 模型评估中,决策树提供了score函数直接得到准确率,但是我们并不知道真实的预测结果,所以无法用预测值和真实的预测结果做比较, 需要使用训练机中的数据进行模型评估, 可以使用决策树自带的score函数计算: 123# 得到决策树准确率acc_decision_tree=round(clf.score(train_features, train_labels), 6)acc_decision_tree 12# 运行结果0.982043 其实,以上准确率评估并不准确,因为我们用训练集做了训练,再用训练集做准确率评估, 并不能代表决策树分类器的准确率. 要统计决策树分类器的准确率, 可以使用K折交叉验证, cross_val_score 函数中的参数 cv 代表对原始数据划分成多少份，也就是我们的 K 值，一般建议 K 值取 10，因此我们可以设置 CV=10 1234import numpy as npfrom sklearn.model_selection import cross_val_score# 使用K折交叉验证, 统计决策树准确率np.mean(cross_val_score(clf, train_features, train_labels, cv=10)) 12# 输出结果0.7778901373283394","link":"/Titanic/"},{"title":"导读：了解AI并使用它&#x2F;他&#x2F;她们","text":"如果你想开始学习AI应用开发，那么在学习之前，有一些学前提醒需要注意。在当今AI爆发的时代，学习AI应用开发需要的学习方法和策略也发生了变化。本课程的目标是通过多尝试、多体验、多做头脑风暴的学习方法，帮助学生在短时间内掌握AI应用开发的基本技能。我们并不会传授过于深奥的数学和理论知识，而是会通过简单易学的API，让学生能够快速上手开发实用价值的AI应用。因此，在学习本课程的过程中，我们鼓励学生多尝试、多体验、多做头脑风暴，以更加轻松、快速地掌握AI应用开发的技能。如果你不知道如何开始，你可以使用Jupyter或者Golab这样的工具来帮助自己入门。 多练习，多尝试，多交流 1. 多尝试运行和修改代码 为什么要多尝试运行和修改代码？因为这是学习AI最有效的方式之一。通过自己亲手运行代码并进行修改，可以更深入地理解算法和模型背后的原理，并从中学到许多实用的技巧。此外，尝试运行和修改代码也能够帮助你更好地掌握编程语言和工具。 而为了方便地进行代码实验，我们可以使用一些开源的工具。例如，Jupyter Notebook 是一个广泛使用的交互式笔记本工具，它支持多种编程语言，并且可以在本地运行。如果你不知道如何搭建环境， 除了本地启动Jupyter之外，你也可以直接使用微软的VSCode，可以直接调用本地Jupyter环境（推荐）。 也可以使用 Google 的 Colaboratory（简称 Colab） 这样的云端工具，只需要一个 Google 账号即可使用。 自然，这也不是全无门槛的，学会如何科学上网是必备技能。这一部分请原谅我无法教授，还是需要自行查找资料。 2. 多体验不同的AI工具 在学习 AI 应用开发的过程中，你需要了解当前市场上涌现的海量 AI 应用。通过体验这些 AI 应用，你可以更好地了解 AI 的能力和应用场景，也能够了解到当前 AI 技术的发展状况。 例如，你可以尝试使用一些人工智能工具来完成自己的工作，比如使用自然语言处理的工具来帮助你写作、使用机器学习的工具来进行数据分析、使用计算机视觉的工具来进行图像处理等等。此外，你还可以体验一些常用的 AI 应用，比如语音助手、智能家居、智能客服、智能医疗等等。通过这些体验，你可以深入了解 AI 技术在实际场景中的应用和效果，从而更好地理解 AI 技术的价值和未来发展方向。 在课程中，你还可以从我推荐中了解到一些最新的 AI 应用。你可以注册账号、下载应用，多去体验一下这些 AI 应用，这不仅能够激发你学习课程的动力，也能够打开你自己利用 AI 大模型能力的思路。 总之，多去体验各类 AI 应用能够帮助你更好地了解 AI 技术的应用和发展现状，也能够激发你对 AI 技术的兴趣和热情，从而更好地进行学习和实践。 本地搭建Stable Diffusion 这里给大家一个小小的建议，尽量不要用线上的图片生成AI去生成商业图片，会有法律隐患的。而如果是在本地架设的情况下，这种问题基本就不存在了。 3. 多交流 与周围的人以及朋友一起多做做头脑风暴，尝试寻找有趣的新产品的机会。事实上，这是一个非常好的建议。AI应用已经涌现出许多，但是有些应用只是简单地使用了现有的API，缺乏创意和创新。然而，还有很多应用具有独特的想法，有些甚至可以直接商业化。学习AI的目的在于学以致用，可以与身边对新一代AI应用有兴趣的人一起探讨，看看课程中介绍的各种方法和技巧能否用于不同的场景和角度。这才是学习这门课程的真正价值。当然，如果你对AI大模型的底层原理有兴趣，可以深入研究其中的数学原理和各种深度学习模型。现在，有能力构建大模型的人实在是太少了，而不是太多了。能够推动通用人工智能向前发展一小步，相信是所有AI从业者都梦寐以求的事情。所以，学习AI，不仅要学习知识，更要发挥创造力，发掘新的应用场景，才能真正做到学以致用。 Stable Diffution 生成的填色图，完全可以变成一项生意，生成多张图做本书：Link 使用AI工具改变你现在的工作方式 随着 ChatGPT、Whisper 和 Stable Diffusion 等强大的 AI 技术的出现，我们的学习和工作方式也需要跟着改变。现在，利用 AI 工具来改造自己的学习和工作流程已经成为一种趋势。通过将 AI 技术应用到各个方面，我们可以获得更加沉浸式的学习体验，同时也能够提高日常生活和工作的效率。在过去的几个月里，我自己也不断地研究和学习新技术，并通过 AI 工具来提高自己的效率。在这篇文章中，我想和大家分享一些我所使用的 AI 工具和优化流程。 ChatGPT，这是一种基于 GPT 技术的人工智能语言模型。我常常利用 ChatGPT 来帮助自己解决问题，比如在学习编程的过程中，我会输入一些代码，然后让 ChatGPT 来帮我检查代码的错误。ChatGPT 还可以用来进行翻译、摘要和生成文章等等。这种 AI 工具可以帮助我们更加高效地学习和工作。 Whisper，这是一种人工智能笔记工具。与传统笔记工具不同的是，Whisper 可以将我们所输入的笔记和文本转化为自然语言，并通过 AI 技术来优化笔记的布局和结构。这样一来，我们可以更加快速和方便地记录学习和工作中的重要信息，并将其整理成易于理解的形式。 Stable Diffusion，这是一种用于大规模数据处理和分析的 AI 工具。在我的研究工作中，我常常需要处理海量的数据，并对数据进行分析和建模。Stable Diffusion 的出现让我能够更加高效地处理数据，并且能够利用 AI 技术来进行数据建模和预测。 利用 AI 工具来改造学习和工作流程已经成为一种趋势。通过利用这些工具，我们可以更加高效地学习和工作，并且可以更加快速地解决问题。当然，这些工具只是 AI 技术应用的冰山一角，未来还将有更多更加强大和智能的 AI 工具出现，让我们拭目以待。 如何使用 ChatGPT 进行学习 随着 AI 技术的发展，ChatGPT 成为了许多人学习知识的“助教”。但是，有些人觉得 ChatGPT 没有多大用处，这可能是因为他们询问了过于宽泛的问题。实际上，ChatGPT 的作用是为我们提供有针对性的回答，只需询问具体问题即可。 当我们学习新知识时，我们可以请 ChatGPT 帮助我们解释我们不理解的内容。与搜索不同，ChatGPT 可以根据我们的追问提供更深入的解释，直到我们完全理解这个知识点为止。此外，ChatGPT 不仅可以解释概念，还可以解释代码。我们可以将需要解释的代码段粘贴到 ChatGPT 中，它将为我们提供详细的讲解。 此外，ChatGPT 作为一个 AI “助教”，它的知识广博、不知疲倦，极其耐心。我们不需要担心问题过于简单或产生心理压力，因为 ChatGPT 不会嫌麻烦或不耐烦。因此，与查找资料或询问他人相比，使用 ChatGPT 可以更高效地解决问题。 问题具体化，将思考的过程交给自己，而获取知识交给ChatGPT，是我最常用的方式 在 Poe 平台中，我们可以选择不同的语言模型，不仅可以使用 ChatGPT，还可以使用其他大型语言模型。这样，我们可以选择最适合自己的模型。 学会使用工具获取额外资料 在当今的数字时代，英文资料已经成为许多行业中获取第一手信息的主要来源。特别是在技术领域，大量的技术文档、API文档和博客文章都是用英文写成的。虽然英文阅读能力是每个人在学习和工作中必须掌握的技能，但对于许多人来说，英文阅读还是比较吃力的。然而，随着机器翻译技术的不断提高，人们已经越来越多地使用翻译插件，将英文资料转换为中英对照版本。 DeepL是一种在线翻译工具，它使用了深度学习技术，可以对英文文本进行准确的翻译。使用DeepL，只需将需要翻译的文本复制粘贴到工具中，它就可以快速将其翻译成目标语言。另外，DeepL还可以通过浏览器插件的形式直接嵌入到浏览器中，当您访问英文网页时，它会自动将其翻译成您的语言。 使用翻译插件可以帮助我们快速浏览英文资料，同时避免了语言障碍。当我们遇到一些翻译不准确的地方，可以快速查看英文原文，确保对资料的理解和应用。同时，翻译插件的中英对照形式也让我们能够更快速地阅读英文资料，从而提高我们的阅读效率。 除了文本资料外，现在还有越来越多的音视频资料，如播客和视频等。这些最新的资料往往只有音频或视频版，而没有文字版。但是通过语音识别和文本摘要技术，我们也可以快速将音频和视频转换为文本，并且生成一个摘要。这样，我们就可以先快速浏览一遍摘要，决定是否值得去完整地听或看。 现在市面上有许多这样的浏览器插件，例如 Glarity，可以帮助我们快速总结视频内容，再来决定是否要看。对于像约翰卡马克这样的大神的访谈，我们可以使用 ChatGPT 背后的语言模型来生成一个摘要，以快速浏览视频内容，确定是否值得花时间去看。 如何通过AI来阅读论文 在科技飞速发展的今天，不断学习新知识，跟上最新的技术进展是非常必要的。读论文是获取新知识的好方法。然而，阅读一篇论文是一项费时费力的工作，因为它通常包含大量的专业术语、公式和图表。但是，随着大型语言模型的出现，我们现在可以借助AI来阅读论文。 现在有很多工具可以帮助我们阅读论文。例如，scispace是一个网站，可以将要阅读的论文上传到其中。然后，我们可以向AI提出问题，以快速了解论文讲解了什么内容。scispace内置了许多你可能会关心的问题，并且可以直接选择回答的语言。此外，在阅读过程中，AI可以对公式、图表等内容进行详细解释，这些工具都可以大大降低阅读论文的门槛，提高掌握这些复杂知识的效率。 除了scispace之外，还有许多其他的工具可以帮助我们阅读论文。例如，ChatPDF是一个可以对PDF文件进行小结和提问的工具。将各种分析报告上传至ChatPDF中，可以快速获取所需的信息。 利用 AI 写代码 Demo：提高生产效率的新工具 GitHub Copilot 是一个利用 AI 技术帮助工程师写代码的工具。通过将需求描述给 ChatGPT，它可以快速生成可用的代码，帮助工程师节省时间和精力。使用 Copilot 写代码的体验非常棒，只需输入注释或代码的开头，Copilot 就能为你生成完整的代码。 对于一些简单的函数调用等胶水代码，Copilot 十有八九是能帮上忙的。即使有些代码不够完美，以它为基础改造比从头开始写更快。当你需要使用一些不熟悉的包时，Copilot 尤其有用。 如果你是一个工程师，安装 Copilot 并使用它写代码是提高生产效率的好方法。此外，使用 ChatGPT 和 Copilot 来帮助写 Demo 代码，可以帮助你快速实验需求，而不必费时查找文档和阅读教程。这些工具能让你更轻松地完成工作，提高你的生产力和效率。 在实际使用中，你可能需要花一些时间来熟悉和调整 Copilot 生成的代码。但是，一旦你熟悉了 Copilot，它将成为你编程工作中最有价值的助手之一。 当然，我是一个穷人，写代码并不是我的主业，所以我一般都使用ChatGPT来完成我为数不多的需求： ChatGPT和Copilot的代码正确率以及BUG率比多数工程师都要来的优秀，自然，也包括我自己。 使用AI获取灵感 现在随着人工智能技术的快速发展，越来越多的AI写作工具涌现出来，让人们在创作过程中更加得心应手。其中，最让人印象深刻的就是AI如何帮助我们获取灵感。虽然AI还没有完全替代人类的思维，但在寻找灵感方面，它们已经展现出了惊人的能力。 在实际的写作中，很多人并不依赖AI产生内容，但是当缺少灵感的时候，AI可以作为一个非常好的助手。例如，当你在围绕一个主题思考写作内容时，可以尝试使用notion.ai等工具寻找灵感。虽然这些工具的很多主意并不新颖，但它们往往可以给你带来意想不到的角度和思路。 此外，你还可以尝试通过人设的不同来让AI从另一个角度帮助你思考问题。例如，为ChatGPT设置一个不同的人设，让它模拟某个领域的专家来帮助你做头脑风暴。这个时候，虽然你只有一个人在思考，但是你却可以组织一个各路大神汇聚的团队帮助你思考问题，让你得到更多有价值的点子。 不仅如此，对于不同领域的人群，还有一些专门的AI工具可以帮助他们获取灵感。例如，Midjourney、Dall-E 2等工具可以让设计师在创作过程中更加得心应手，快速地制作出优秀的设计作品。 接受它，别被它替代 随着科技的快速发展，现代社会正在经历着巨大的变革。许多传统行业和工作岗位正在面临被自动化和数字化取代的风险。因此，我们必须积极地适应新的技术，以免被时代抛弃。尤其是人工智能的发展，不仅给我们的生活带来了诸多便利，也对我们的工作和职业规划产生了重大影响。因此，了解人工智能的基本原理和应用场景，具备使用和开发人工智能的能力，成为未来职场竞争力的关键。同时，我们也需要不断地学习和更新知识，跟上时代的步伐，不断提升自己的技能和能力，以应对快速变化的社会和市场。","link":"/Understanding_and_Utilizing_AI/"},{"title":"05 为文本分类","text":"Hi， 我是茶桁。 在前一讲中，我们观察到大型模型的确表现出良好效果。在情感分析任务中，通过使用 OpenAI API 提供的 Embedding，我们得到的结果要比能在单个机器上运行的较小模型（如T5-base）要好得多。然而，我们之前所选择的问题确实有些过于简单。我们将5个不同的评分分成了正面、负面和中性，同时还排除了相对难以判断的\"中性\"评价，因此我们判断准确率的提高相对较容易实现。但是，如果我们想要准确预测具体的分数呢？这将是我们接下来需要探索的问题。 训练机器学习模型 一种最简单的方法是利用我们获得的文本 Embedding 向量。这次我们将不再直接使用向量之间的距离，而是采用传统的机器学习方法进行分类。这样做的原因在于，仅仅使用向量之间的距离无法最大化利用已标注好的分数信息。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Text-Classification/"},{"title":"利用AI写一个『贪吃蛇游戏』","text":"在完成一些简单的任务之后，我觉得ChatGPT可以帮我完成一些更为复杂，甚至于可能我并不是特别擅长的任务，而我想到了，也许，可以帮我写一个简单的游戏。 贪多嚼不烂的道理我还是明白的，于是，我给自己提前就设定了一个目标： 游戏可以运行实现简单的规则减少BUG的存在可以开始和结束 带着这四个最朴素的愿景，让我们开始吧。 一贯的，一上来我就将问题抛给了chatGPT 1请用Python3.10 编写一个贪吃蛇的游戏，给我完整的代码。 这里我们需要注意一下，你的描述越清晰，得到的结果越有可能是你想要的。在这里，我告诉了chatGPT，我希望用Python写一个游戏，而且版本号是3.10。并且，我告诉了chatGPT，请给我完整的代码。因为如果我不这么说的话，那很有可能chatGPT会先给我一堆的步骤而不是实际的代码。 在之后，我得到了chatGPT的答复： 完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import pygameimport random# 游戏窗口的大小WINDOW_WIDTH = 800WINDOW_HEIGHT = 600# 蛇身和食物的大小BLOCK_SIZE = 20# 定义颜色WHITE = (255, 255, 255)BLACK = (0, 0, 0)RED = (255, 0, 0)# 初始化 Pygamepygame.init()# 创建游戏窗口window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))pygame.display.set_caption(&quot;贪吃蛇游戏&quot;)clock = pygame.time.Clock()def game_loop(): game_over = False # 蛇的初始位置和速度 snake_x = WINDOW_WIDTH // 2 snake_y = WINDOW_HEIGHT // 2 snake_x_change = 0 snake_y_change = 0 # 食物的初始位置 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE while not game_over: for event in pygame.event.get(): if event.type == pygame.QUIT: game_over = True elif event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT: snake_x_change = -BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_RIGHT: snake_x_change = BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_UP: snake_y_change = -BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_DOWN: snake_y_change = BLOCK_SIZE snake_x_change = 0 # 更新蛇的位置 snake_x += snake_x_change snake_y += snake_y_change # 绘制游戏窗口 window.fill(BLACK) pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.draw.rect(window, WHITE, [snake_x, snake_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.display.update() # 碰撞检测 if snake_x == food_x and snake_y == food_y: food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE clock.tick(20) # 控制游戏的帧率 pygame.quit()# 启动游戏game_loop() 从代码里我们可以看出来，文件引用了库pygame以及random, 其中，pygame需要我们单独安装一下: 1conda install pygame 建立了一个snake_v1.py的文件，并把这段代码粘贴到文件里之后，我开始尝试运行它： 1python ~/xx/game/snake_v1.py 事情果然没有想的那么简单，根本就是毫无动静。我开始询问chatGPT chatGPT给到的答案似乎没有什么意义，这些事情在开始之前我就已经做好了。在短暂的思考之后，我忽然想到，是否因为我是Mac系统，从而导致了窗口不出现，于是我继续问问题： 然后继续问： 这一次，我抓到了重点。不明白为什么刚才我提到我是Mac系统的时候他不告诉我，先不管这些，在其中添加这段代码后，游戏终于可以运行了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import pygameimport random# 游戏窗口的大小WINDOW_WIDTH = 800WINDOW_HEIGHT = 600# 蛇身和食物的大小BLOCK_SIZE = 20# 定义颜色WHITE = (255, 255, 255)BLACK = (0, 0, 0)RED = (255, 0, 0)# 初始化 Pygamepygame.init()# 创建游戏窗口window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))pygame.display.set_caption(&quot;贪吃蛇游戏&quot;)pygame.display.flip()clock = pygame.time.Clock()def game_loop(): game_over = False # 蛇的初始位置和速度 snake_x = WINDOW_WIDTH // 2 snake_y = WINDOW_HEIGHT // 2 snake_x_change = 0 snake_y_change = 0 # 食物的初始位置 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE while not game_over: for event in pygame.event.get(): if event.type == pygame.QUIT: game_over = True elif event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT: snake_x_change = -BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_RIGHT: snake_x_change = BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_UP: snake_y_change = -BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_DOWN: snake_y_change = BLOCK_SIZE snake_x_change = 0 # 更新蛇的位置 snake_x += snake_x_change snake_y += snake_y_change # 绘制游戏窗口 window.fill(BLACK) pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.draw.rect(window, WHITE, [snake_x, snake_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.display.update() # 碰撞检测 if snake_x == food_x and snake_y == food_y: food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE clock.tick(20) # 控制游戏的帧率 pygame.quit()# 启动游戏game_loop() 不知道各位看出什么问题了没有，问题如下： 动作太快了，完全是地狱级别的难度； 蛇似乎不会增长，吃了物品没有变长； 撞到墙壁后游戏虽然停止了，但是没任何提示，蛇就这么消失了； 于是，我只能让chatGPT再帮我优化代码： 当然，又有了其他问题， 蛇在撞到自己之后没有办法结束游戏 游戏无法退出 游戏无法重新开始 Game Over之后显示的是乱码 当然，有编程经验的人大概能明白，乱码的话一定是因为中文显示不正确，而目前，我也没打算支持中文。干脆用英文好了。 于是，我又开始提需求，在连续三四轮这种询问之后，终于来到了最后一次： 然后我得到了如下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137import pygameimport random# 游戏窗口的大小WINDOW_WIDTH = 800WINDOW_HEIGHT = 600# 蛇身和食物的大小BLOCK_SIZE = 20# 定义颜色WHITE = (255, 255, 255)BLACK = (0, 0, 0)RED = (255, 0, 0)# 初始化 Pygamepygame.init()# 创建游戏窗口window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))pygame.display.set_caption(&quot;贪吃蛇游戏&quot;)pygame.display.flip()clock = pygame.time.Clock()def game_loop(): game_over = False game_restart = False game_paused = False while not game_restart: if game_over: show_game_over() # 蛇的初始位置和速度 snake_x = WINDOW_WIDTH // 2 snake_y = WINDOW_HEIGHT // 2 snake_x_change = 0 snake_y_change = 0 # 蛇身的初始位置和长度 snake_body = [] snake_length = 1 # 食物的初始位置 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE score = 0 while not game_over: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() quit() elif event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT and snake_x_change != BLOCK_SIZE: snake_x_change = -BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_RIGHT and snake_x_change != -BLOCK_SIZE: snake_x_change = BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_UP and snake_y_change != BLOCK_SIZE: snake_y_change = -BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_DOWN and snake_y_change != -BLOCK_SIZE: snake_y_change = BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_SPACE: game_paused = not game_paused elif event.key == pygame.K_q: pygame.quit() quit() if game_paused: continue # 更新蛇的位置 snake_x += snake_x_change snake_y += snake_y_change # 碰撞检测 if snake_x == food_x and snake_y == food_y: # 蛇吃到了食物，增加长度和得分 snake_length += 1 score += 1 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE # 更新蛇的身体 snake_head = [snake_x, snake_y] snake_body.append(snake_head) if len(snake_body) &gt; snake_length: del snake_body[0] # 检测碰撞 if snake_head in snake_body[:-1] or snake_x &lt; 0 or snake_x &gt;= WINDOW_WIDTH or snake_y &lt; 0 or snake_y &gt;= WINDOW_HEIGHT: game_over = True # 绘制游戏窗口 window.fill(BLACK) pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE]) for body_part in snake_body: pygame.draw.rect(window, WHITE, [body_part[0], body_part[1], BLOCK_SIZE, BLOCK_SIZE]) # 显示实时分数 font = pygame.font.Font(None, 28) text = font.render(&quot;Score: &quot; + str(score), True, WHITE) window.blit(text, (10, 10)) pygame.display.update() clock.tick(10) # 控制游戏的帧率 # 重新开始游戏 game_loop()def show_game_over(): font = pygame.font.Font(None, 36) text = font.render(&quot;Game Over. Press Space to restart&quot;, True, WHITE) text_rect = text.get_rect(center=(WINDOW_WIDTH // 2, WINDOW_HEIGHT // 2)) while True: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() quit() elif event.type == pygame.KEYDOWN and event.key == pygame.K_SPACE: game_loop() window.fill(BLACK) window.blit(text, text_rect) pygame.display.update() clock.tick(10)# 启动游戏game_loop() 大家可以看看效果： 我们可以看到，这个游戏相对来说是比较完整了，速度正常了，有计分系统，蛇能正常增长，碰到自己游戏会结束，碰到墙壁后也会Game Over，并且可以通过SPACE来重新开始。并且，我还加入了按Q键的时候退出的功能。 当然，还可以继续完善，比如随着时间的流逝，速度上可以渐渐加快等等。就看你怎么想，然后其他的交给chatGPT。 通过这次的示例演示，其中重点不是教大家如何做一个贪吃蛇游戏，而是教大家如何利用chatGPT来解决你需要解决的问题。当然，我需要收回我开头说的话，chatGPT并不能帮你解决你不熟悉的问题。就比如，如果我完全不懂这其中内容的话，可能我窗口都打不开，我完全都不知道我什么时候才能解决Mac系统中不一样的部分，而也正是因为有一些简单的经验，才让我考虑的那个层面，从而针对性提问解决了问题。 所以要记住，AI并不能帮你解决你完全不懂的问题，起码，你要知道你想问什么，也要知道问题大概卡在哪里了，针对性继续提问。 最后，友情提示一下，不要用API来完成这一次次的对话，经验之谈，去买个Plus，比API交互便宜多了。你看那一串串的代码重复的给你写出来，你完全不知道会耗费多少Token。那些宝贵的Token，还是用在聊天窗无法完成的任务上比较合适。","link":"/Use-AI-to-write-a-snake-game/"},{"title":"10 利用AI索引并分析文献和图片","text":"Hi, 我是茶桁。 看到我这篇文章的读者们不知道有多少人是接触过ChatGPT或者其他人工智能产品的。 市面上目前充斥着大量的人工智能产品，从聊天，文案，脚本，音乐，绘画等方方面面都涵盖了。但是不知道有多少人遇到过以下的场景不知道该如何解决： 我需要针对一篇很长的文章（可以是论文，可以是小说）进行总结或者分析的时候，就开始无从下手。因为ChatGPT在接收长度上是有限制的，这个长度我大概测试过，如果你用的是WebGPT，那么应该中文应该是在2500字左右，多一个字都会告诉你长度超出限制。而我们一篇论文，起码来说都是5000字以上的。分两段来喂给ChatGPT当然可以，但是上下文关联有时候会遇到问题，ChatGPT也会给你胡编乱造。 有的时候我从客户那里接收到的是一张图片，也许是截图，也许就是拍的一张照片。那么，怎样利用ChatGPT去分析这张图片上的内容，然后根据我的需求给我相应的答案呢？ 以上这两点，估计是很多人遇到想解决的。而今天这篇文章，就是从这两点入手教你如何解决。 大语言模型的不足 让我们打开ChatGPT来问一些常识性的问题，这个问题对于大部分上过学的中国人来说，都能从课本上了解到： “鲁迅先生在日本学习医学的老师是谁？” 结果如下图，这个“嘉泽源之助”到底是谁呢？我也不知道，得到这个答案的时候，我还特意去Google了一下，根本找不到相关资料。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Use-AI-to-index-and-analyze-documents-and-images/"},{"title":"13 使用多步提示语让AI帮你写测试","text":"Hi，大家好，我是茶桁。 很遗憾在上一讲，也就是第12讲的时候，咱们对于利用AI写一个VBA宏来执行Excel任务的过程并不顺利，仔细想来既然大家都在这里看这个系列文章了，应该也基本都会Python的，所以一个Excel自动化也并无太大影响，毕竟，这种商业软件的集成一定是早晚的事情，咱们也不必在这里死磕这一个问题。 那么本节课程呢，我们会通过chatGPT的不断交互，去完成一个测试任务。 在很多时候，我们探索性开发一些功能可以极大提高我们的效率，但是这个过程并不能做成一个完整的产品。我们理想中的产品应该是“自动化”的，我们只需要用自然语言输入自己的需求，对应的代码就自动写出来了。 那么如果中间出现了问题怎么办？当然是AI可以自己拿到反馈自己更正自己了，完全不需要人工去介入调试。 下面，让我们开始吧。 代码的起源 让AI自己调试自己的需求听起来是不是很不可思议？随着GPT-4的发布，还有就是未来模型能力的进一步增强，这个骑士并不是遥不可及。是的，我又在这里贩卖焦虑了，那些低廉的测试们，想要自己的退路了吗？ 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/"},{"title":"使用Telnet数据流看世界杯","text":"使用Telnet数据流创造了正在进行比赛的ASCII影像, 尽管这难以令人想象. 看看他是怎么做的… 在比赛开始10分钟以前, 简单的打开Windows开始-运行窗口, 输入 telnet ascii-wm.net 2006 , 你将会看到 “现场直播” 视频流. 明显的, 这是互联网应用的又一创新. 来试一下吧! http://ascii-wm.net/ 由于我家里只有电脑没有电视，所以全程我都是这么干的。。。可惜，经常与主机断开连接。。而且，很多时候我看不懂。。。郁闷。只能知道个大概。。有兴趣的可以去看看＠","link":"/Use-the-telnet-data-stream-to-watch-the-World-Cup/"},{"title":"VH 情怀黄铜原子笔","text":"「知乎专栏地址」 收到 @罗文森 赠送的笔已经很多天了，快两个星期了吧。 一直在找时间想写一篇评测出来，终究是没抽出时间，况且，我不是写手！眼看时间一天天过去，心里也越来越愧疚。 其实最主要的也不单单是写不出什么，而是总归要用它画两幅画出来，才会感觉的出来到底合不合心意。 对于文具的偏爱，估计是从学生时代就开始了。记得高中的时候缠着老爸给买了第一个奢侈品：一支派克。虽然很珍惜，却并不好用。自此对文具就更是挑剔。 一支好笔，真的能陪一个人好久好久。 打拆包之前，就一股浓浓的逼格。 不过问题来了，我不知道怎么打开包装！XD - 一直不敢使劲，生怕损坏了什么，折腾了好久，才明白，原来连着盒盖的那张纸，是用来撕的。 打开之后，金色的笔体和其皮外套分开躺在盒内，逼格更甚了。 喜欢盒子内部的那句话：“合适的形态总有他合适的作用，还原物的本质，至真至纯，用一支笔唤起书写的初心。”， 好吧，又是一个走情怀的产品。不过对于这个包装和笔本身的设计来说，这个情怀我还是蛮受用的。 而关于细节上，笔的打磨很是花费了一番功夫，那种黄铜的质感以及似乎岁月沉淀的感觉，让人爱不释手。（这些天只要是要签字的机会，我基本都拿出它来装逼）。 而关于旋转笔头，旋转的过程中特别的舒服。到位后的力回馈也刚刚好，让我转来转去玩了好久。额，给玩的不灵光了。（看来关于使用强度，长家还是需要加强）。 关于那件小皮装，似乎有点小了，装进去是用了一些力气的。额，拿出来就更费劲了，太紧了。不过相信用一段时间，也就松了。对于这样一直精致的情怀笔，这件皮衣还是相当必要的。 笔当然还是要用才能知道是否合心意, VH 这支黄铜笔，拿在手上的感觉，重量十足，根本是那些塑料笔没办法比的。但是这个重量不大不小，真的正合适。想起厂商吹的牛逼：“打造了最佳的配重”。 这牛逼不为过。 至于握感上，这个笔对于我来说似乎纤细了一点，可能也是因为我握惯了粗的笔，包括我最爱的那支53Pencil，也是比较粗的。而在厂家原装的那支笔芯，使用起来也是非常顺畅（就喜欢这样顺畅的笔芯）。不过用完需要更换笔芯的时候，就需要买好一点了，要不可惜了这么好的笔。 最后随笔画了两幅画来试笔。 太久不画画了，大家将就看吧！ 有想买的，链接在这里 : 首页-vh企业店 PS：VH家的那个无线充电器，也是逼格满满。打算入一个。。。","link":"/VH-brass-pen/"},{"title":"VPS 设置 Hexo","text":"首先需要感谢@lucifr，我现在这篇文就是在iPad上登录VPS完成的。最后还是忍不住入手了下边两个APP: 当然我其实到现在并不完美，因为rsync和自动执行generate的代码我没有完成。安装incrond的时候总会出错，于是无法执行集群文件同步.所以现在还是在终端里执行generate和cp -rf /home/xxxx/* /home/xxxx 我这里并不是要教设置步骤，因为其实@lucifr 已经在他的这篇文里写的很清楚了，我就写几点注意事项 搞定VPS操作和基本的Linux命令很重要。 要搞定lnmp，参照这里的lnmp详细介绍 新版本的Hexo有更改，在同一目录里是找不到/cli/generate.js的，更别说console.log语句了 @lucifr所说的新建立一个Dropbox账户，意思是在VPS主机上建立一个账户用来执行Dropbox同步，而不是新建立一个Dropbox账户。 其他… 好吧，写其他是因为iPad上用VI进行编辑实在有点难受，现在先这样了，以后有时间了再写一个更详细的。","link":"/VPS-setting-hexo/"},{"title":"VSCO FILM 00 FREE STARTER PACK","text":"本文知乎专栏 VSCOCam是iOS上一款滤镜相机，其最著名的地方就是他们的胶片滤镜。 而其实VSCO在这款APP之前，就一直在做胶片滤镜，有OS X和WIN两个平台的版本。最主要的是作为LightRoom的插件存在。 而现在提供免费下载的这款滤镜就是针对LightRoom的滤镜插件，有OS X和WIN两个版本提供。 We are thrilled to announce VSCO Film 00, our first-ever FREE starter pack. Perfect for anyone who uses VSCO Cam and is looking to take the next step, VSCO Film 00 brings beautiful presets, custom camera profiles, and the familiar VSCO editing experience to your desktop in Adobe Lightroom. VSCO Film 00 includes two of our most popular film stocks (Kodak Gold 100 from Film 05 and Kodak Tri-X from Film 06) and is available for download now. 有需要的可以点击原文链接进行下载，不要感谢我，我是雷锋。 PS: 关于之前我提到的Enlight导出图片丢失数据的部分，我在此向读者和开发者致歉，原因是导出的如果是PNG图片是不能保存那些信息的，可以将图片的质量从Pro调小到Hight, 保存的就是JPEG图片，那么导出的图片地理位置和相机信息就会一并保存了。","link":"/VSCO-FILM-00-FREE-STARTER-PACK/"},{"title":"关于我","text":"茶桁是我笔名，80后, 老派思想者, 固守着年代感的东西... 和大部分这个年代成长起来的孩子一样, 怀念着龙珠, 灌篮高手, 追着海贼王. Github: @hivandu Twitter: @hivan Design: @hivandu Notes: @DUART 对新奇的事物仍然抱持敏感而探索的心境, 并且喜欢有条不紊. 坚信有生之年能看到人类和AI和平共处的景象, 坚信有生之年可以看到第一批星际移民. 这里, 我们谈谈思维, 哲学, 经济以及信息管理. 还会夹杂着我的一些私货, 那都是一些遥远的技能, 已经离我而去很久了. 说了那么多, 先搬家吧. 另外, 在我另一个试验田里, 有着自己对产品和技术的追求, 也欢迎大家围观; 这篇博客是简历在Hexo + Github Pages + vercel上, 有兴趣的小伙伴可以自行Google一下相关教程, 这里就不引述了.","link":"/about/"},{"title":"Alfred 2 Plugin -- open in Atom","text":"用于在Alfred 2中用命令快速调用Atom打开所见文档，类似于Open in Sublime， 为了自己方便建立了一个workflow，有需要的下载吧！ Open in Atom.alfredworkflow","link":"/alfred-2-plugin----open-in-atom/"},{"title":"android 2.1版本无法开机解决","text":"不含刷的变砖的机子！ 这个版本的rom是一定需要SPL的，比起HTC hero的rom来说速度有提升！但是没那么华丽(当然我说的是源生系统，我对theme之类的不太有兴趣！)。。。 但是刷SPL有风险，请慎重行事！ 关于刷机2.1版本后无限火花不开机的情况，主要是Recovery的问题，刷成V2.5应该都可以解决！本人测试OK。 手机端安装可以下载程序：flashrec.apk PC端安装需要USB驱动，然后通过更新工具更新。这里一定要注意区别清楚机型版本，不同机型是不能通用的，再次提示，一定要注意看清楚，千万不要着急。 点击此行下载用于 G1 和 沃达丰版 G2 使用的更新程序 大小: 5369404 字节 MD5: 8CA35537D253EB19CC0D28A45D153FC4 SHA1: 88C3E3ED444A1B96C59594215A81B5C5F589C42E CRC32: 97089632 点击此行下载用于 HTC 版 G2 使用的更新程序 大小: 4848659 字节 MD5: 7217E7DCCFEAB4BFB254B86778FFBD5C SHA1: 8373BDB338D3E6EBA8622CDC19EFBF7AE3484451 CRC32: A9BD3E5E 再次提醒，千万别下错了！ 保证手机与电脑连接无误后，双击更新程序运行即可。","link":"/android-2-1/"},{"title":"Android 2.2 App2sd 问题","text":"其实比起以前版本的app2sd来说,设置是一样的!只是多了一个步骤,就是需要给rom添加一个sdext.然后所有的设置就和以前的版本一模一样了. 首先当然需要有一个已经分好区的sdcard,具体设置可以查看我以前的文,有ubuntu下进行分区的和windows下的! 然后需要最近版本的SPL和Radio,这个本人不提供了,可以自行解决!伸手党可以留下自己的邮箱,我提供下载地址! 准备工作做足后,第一步就是需要在手机上建立一个sdext访问.这里提供一个文本文档:下载： fr-patch134.zip 放在sdcard根目录,然后在连接手机的情况下在终端如下操作: adb shell # sh /sdcard/fr-patch134.txt sdext busybox df -h 如果看到有/sd-ext分区,OK,以下的事情就顺理成章了,参考我以前发布的app2sd步骤操作就好了! 至此所有问题解决! 此处为后续更新,由于之前忽略了点东西,所以这里做一个补充! 由于2.2rom和以前版本的一些差别,在做app2sd之前,需要挂在system,sd-ext和data分区,这是需要注意的一点!挂在命令为:mount 例子: mount system 有什么不明白的再问吧!","link":"/android-2-2-app2sd/"},{"title":"Android 2.2 for G1","text":"親愛的Android,我回歸了...說實話,後兩個月我真的有些壓抑.現在感覺解脫了! Android 2.2 For G1 已經被C神放出,貌似解決了相機問題,而其他問題暫時沒有進行測試,因為本人也正在下載中...萬分期待! 先提供下載地址,等待我適用後再放試用報告.. http://drop.io/ionstorm/asset/defcon-dream-ota-eng-t1-signed-zip PS:如果沒有語言要求可以下載這個版本就OK了,如果強烈需求中文,需要下載中文補丁包 试用报告: 仍然没有中文 系统默认没有中文输入法(这个到不是问题) 没有Google Map,市场无法下到.并且安装了4.2开发版后打开就崩溃(这点对于喜欢Buzz的朋友是个致命伤) 3D图库效果有. 相机不能用,和2.1不同的是,就算拍照有图片,打开来看也是一个android小人.... WIFI正常使用 蓝牙没有测试. 速度真的比2.1快了不少,没有一点卡的感觉. Settings里有CyanogenMod settings,可以直接更新Rom,可是我这里链接失败. Apn需要自行设置,具体的设置方法可在网上Google Vpn使用正常 因为地图没有打开,所以GPS模块没有测试. 其他不进行补充了,由于以上有些原因是我无法接受的.所以打算刷回1.6rom,以前刷2.1是为了绑定自己和公司双帐号,现在不必了!不用忍受2.1的速度了...","link":"/android-2-2-for-g1/"},{"title":"Android 3.0 Preview","text":"整个界面都显得很陌生啊!大家可以从视频中看到. 锁屏界面更改很大,目前不知道是解锁手势还是横向滑动还是画圈. HomeScreen主屏幕，可以看到四角的设计，左上给搜索按钮，支持语音搜索，右上为应用菜单按钮，左下为返回、Home、菜单按钮，右下角则是状态标志，屏幕中间为程序快捷菜单和桌面工具 有一个社交工具的集合，可以查看不同来源的好友更新.不过这个在大陆的情况就...你们懂得! 有个桌面管理器,可以进行桌面的各项设置. 拥有全新的浏览器，可以查看各种完整版网页 拥有全新的Android版本Gmail界面! Gtalk界面也是全新的,并且支持双向视频聊天!不过流量上....不敢想! 全新的Youtube视频墙,不过在国内,我们都是墙内的,它在墙外而已! &lt;li&gt;全新的Google地图界面,支持3D模式导航!&lt;/li&gt; 此外还会有Google图书等更多全新内容!","link":"/android-3-0-preview/"},{"title":"Android 3.0","text":"好吧,7.1号Eldar Murtazin在其博客上已经放出一个可信度比较高的谣言,就是Android3.0将于十月份发布,代号姜饼(Gingerbread) 据传,3.0rom将会针对高端市场,分辨率达到了1280X760,支持此系统的最低配置将为1Ghz处理器,512M内存.可以这么说,在3.0系统发布以后,就可以正式宣布G1被彻底淘汰了. Android 3.0 Gingerbread will be released in mid- October (around 15 -16th), 2010. First handsets shipping in November/December – for the Holiday Season. Minimum hardware requirements for Android 3.0 devices are: 1GHZ CPU, 512MB or RAM, displays from 3.5” and higher. (We all, of course, heard that Android handsets with 2GHz CPU’s are coming) New 1280×760 resolution available for the devices with displays of 4” and higher. (Anyone thinking about Android tablets now? ) Completely revamped user interface. If you want to get a feeling of what Android 3.0 Gingerbread UX is like, check out the Gallery App on Nexus One. The same overall feel, light animated transitions,etc. Natively, through all the UI. Android’s split into 2 branches becomes official. 3.0 for top of the line/high end devices. Cheap, low-end mass market handsets will keep Android 2.1/2.2 详情可以参看:http://www.unwiredview.com/2010/06/30/android-3-0-gingerbread-details-1280x760-resolution-1ghz-minimum-specs-mid-oct-release/","link":"/android-3-0/"},{"title":"Android 4.0 通讯录与Google+的深度整合","text":"在4.0以前,我记得Android里的通讯录名称是“contact”.而在4.0之后,我的手机上是4.0.3,将其更改为“people”了. 而更改的不仅仅是这小小的名称!大家都记得在之前版本的contact里,拨打界面和通讯录界面是整合在一起的.所以那个时候我第一屏单单放一个拨打电话就OK了.需要查找通讯录的话,可以切换tab. 1.理念: 而在4.0里面,你在people里面找不到拨打电话界面,而在拨打电话的界面里也找不到通讯录.Google将这两个单独分立了开来!貌似不方便了,其实不是,反而变得异常容易整理和操作!Google在Android4.0里的理念就是将电话和联系人完全分离开,这本来就不是一回事!智能手机时代,谁说的联系人就一定只能是打电话和发短信的? 2.整合: 说了些小变化,下面切入正题,就是Android 4.0 之后通讯录与Google+的深度整合! 这也是我昨晚没事整理联系人的时候才注意到的,不知道是不是4.0之处就是这样的,还是到了4.0.3的改变! 当我们进入一个单独的联系人界面的时候,发现除了常用的PHONE,EMAIL等选项之后,会有一个CONNECTIONS项目选项,这里会显示手机上安装过的SNS程序里联系人的关联账户! 比如WhatsApp,Twitter或Facebook等! 当然,你也有可能看到一个Add connection选项. 这个选项,其实就是和Google+整合的选单! 可以选择添加联系人到自己的Google+ 圈子里!当然,有可能你的联系人根本没有在Google+注册过! 我猜测,Google以后有可能会将Google+的圈子代替联系人来使用!也就是说,你所有的联系人都是要圈养的.不管他是否注册过Google+,利用圈子的概念管理联系人,其实比较起来而言,比Gmail里的通讯录要高效一点! 接下来,才是深度整合的重点,以我自己为例: 用过4.0的朋友都知道,联系人选单是可以向左拖动的,就是右边还有一块和联系人有关的“update”选项!当然,这一块内容需要对方联系人有Gtalk或者Google+才会显现出来! 平时就会显示Gtalk上的签名状态,而对于有Google+的联系人,则会显示他在Google+中的信息! 我们大家应该都用过MSN或者QQ,应该可以想象一下,MSN当初和Space深度整合,QQ和QQ空间深度整合的情形! 当你查看某个联系人的时候,他在自己空间里发表的文章或相片,都会在其信息里显示出来!当然,QQ有的时候会闹点小情绪,来一两次大姨妈…我们可能没那么及时看到!所以说这个概念并不新奇,只是这次Google借用到了手机上而已! 哈哈,想象一下,够方便吧? 3.想法: 写到这里,我到觉得Google有些小气了. 右边状态栏完全可以让出来给其他的SNS APP来使用而不只是Google+,譬如Twitter和Facebook,或者是QQ空间!既然有了这样一个功能,而又是在消费者手上的,那么决定权就该交给消费者!否则,自己联系人里没多少用Google+的,右边信息栏岂不是浪费? 我想,再以后,这块地方应该会被其他SNS APP占用吧!届时,通过手机通讯录,大家就可以看到某人最近在做什么了!而无需再登录单独的App去查看! 4.后记: 当然,除了Contact以外,Gallery等都和Google的产品有深度整合!而这些,和其他APP整合的可能性是有的!比如Gallery里直接查看Flickr里的相片而不只是Picasa.不过在Google推销自己的Google+这段时间里,我看是不太可能了!","link":"/android-4-contacts-the-depth-integration-with-google-plus/"},{"title":"Android G1 的优化","text":"我是第一批使用android的用户,那个时候没有别的选择,只有G1好选择.所幸买的是英版全白.这个机型据说是很少的.包装里带彩贴的那种! 可是时间长了,G1的诟病也就出现了,系统不断升级,虽然有很多自制包提供下载,解决了官方不在支持G1的问题,可是速度上和原来的设计问题没有办法解决! 不过好在网民的智慧是无穷的,今天带给大家三个特殊的小东西,用来优化你得G1. 10m rom HACK 刷新方法，拷贝boot-cm_2629-dp_mem-xtra.img文件到sd卡，启动到recovery模式下，进入console： mount -a flash_image boot /sdcard/boot-cm_2629-dp_mem-xtra.img JIT enabled Dalvik VM 进入recovery直接升级zip文件就好了 Audio Hack v3.2 apk 这个是mark里2欧元的付费软件.用来加大默认铃音. 下载地址: [download id=\"1\"]","link":"/android-g1-optimization/"},{"title":"Make Android Your Own-Androidify","text":"Androidify,由Google推出的一个app，可以让Android使用者在手机上创造属于自己的形象Android。是一套单纯的纸娃娃系统,可以自定义包括肤色、发色、衣服、裤子、鞋子、饰品等众多套件，还可变更身体的比例，其实还颇好玩的哩。只要直接在Android Market搜寻Androidify就能找到这个可爱的小软件。 跳转有Google针对这个app推出的小短片。不过还不如自己赶快下载回来玩比较实在。","link":"/android-ownandroidify/"},{"title":"","text":"在安卓中叫兽曾经写过两个换肤教程.而教程中写的是替换原文件包...其实这个完全没必要.只要在模拟器的快捷方式中加入一段代码就好了... -avd avdname -skin skinname 而皮肤文件可以分开来放.如图:","link":"/android-skin/"},{"title":"Android 简易访问Twitter,youtube,facebook方法","text":"其实就是修改hosts文件. 而这个hosts文件我是已经修改完毕的...直接cat到手机内覆盖原文件就可以了! 执行之前请将hosts复制到sdcard的根目录,然后cmd,cd如adb目录,然后执行其下代码: adb remount cat /sdcard/hosts &gt; /etc/hosts 一切搞定! 请对于hosts上的IP地址低调传播,谢谢! PS:本hosts修改大法已经基本完全失效,基本所有有效果的IP地址都被屏蔽,如果有条件,自己建立一个VPN吧!不过对于联通的用户我要给你一个大大的警告:联通屏蔽VPN.....","link":"/android-twitteryoutubefacebook/"},{"title":"Auto-save-photo-to-qqmail","text":"前言 为什么是QQ Mail? 因为它大,而且不断自动扩容,你想把它装满暂时是不太现实. 而且来说,QQ邮箱的体验还是非常不错的!过滤规则也很能满足要求,归档搜索查找都不错!一些不涉及隐私而又想保存的文件或者照片或者其他什么东东,存在QQ邮箱里还是不错的!比如:XXX 如果想同步到Google+请看完文章后看最后部分的更新说明! 如何实现 通过众所周知的ifttt 其实,这主要是一个我为了保存自己照片的方式!(爱信不信,不相信拉倒!) 建立一个task, if Instagram 设定条件:New Liked photo, then Gmail 设定条件:Send an email. 好了,填上 To address: xxx@qq.com 就OK了! 简单吧? 如果你熟悉某个联系人,那么建立规则by Username,然后包含此用户名关键词的主题都标上相应的关键词.比如by ladiiprang在邮箱规则里就可以加上\"妹子\"的tags. 注意点 如果你不想后期被众多的新邮件搞得头昏脑胀的话,那么你一开始就要设定好过滤条件! 在ifttt中,Send an email的时候Subject记得填写上一些关键词,比如From Instagram,这样,对于主题内有关键词的邮件就好管理的多了.添加过滤规则就好了!将来自己发送邮箱Gmail的邮件主题包含From和Instagram的都自动移动到一个新建的文件夹内,Ex:Photo DB,完工! 后记 同理,我们也可以建立来自Flickr的发送规则,原理是一样的!注意Gmail邮箱里的过滤条件要建好!否则Gmail爆满是迟早的事情!运用这样的规则,我们还可以发送Dropbox里的文档到QQ邮箱内保存,不过规则限定发送的只能是Public内的文档! 其实一开始我不确定是发送文件还是只有地址!所以我开始的方法很绕,就是将相片想Save到Dropbox,然后再通过Dropbox建立if.then.Gmail.不过试验下来既然能直接发送文件,建立task就简单多了! 还等什么,快去你的Instagram和Flickr上收藏妹子到邮箱内吧! 更新 本来因为标题的原因,这点是不加在这里的!但是想着再写一篇一样意义的文章很没意思,所以就在这里说明一下好了! 在我这篇文章发布之后,G+上看到了电脑玩物的作者+esor huang 的一篇讲解Instagram同步到Dropbox和Google+的说明!以及这篇E文!说起来,这样的同步方式确实很笨拙.你打算电脑24小时开着picasa来为你同步么?那么,我从我这篇文的基础上考虑可行方案!记得之前Picasaweb给每个人都有一个邮箱推送地址!就是类似username.password@picasaweb.com这样的地址!好吧,有邮箱地址就简单了.不过这个地址需要你再登录picasaweb.com去找,在Google+页面上是找不到的!同理,Flickr也有类似的推送地址! 该怎么做我想你已经清楚了吧! 最后,我想到了是否同样可以传送到QQ相册!毕竟和邮箱最大的不同就是相册是用来分享的,而邮箱是用来保存的!可惜,QQ没有针对相册的推送,而推送到QQ空间的XXX@qzone.qq.com这个地址也是必须QQ邮箱内部发才行!是的,和你们一样,我又想到了邮箱转发规则.用QQ邮箱收到邮件后转发到QQ空间邮箱去不就好了!测试后,果然. ... 果然没那么简单,这次我失误了,特么的QQ小气到不允许自己的QQ邮箱转发邮件到QQzone的邮箱来自动推送文章!提示这是一个无效地址!不过也无所谓了,毕竟是推送文章的邮件地址,你也不想自己的QQ空间全是大片的文章,而且每篇文章里只有一张相片吧 ? 这个说明本来是在G+上有提出的,但是有基友测试成功后给的是这里的url,所以我就想,补上这个说明!谁说是一样的到底,但是如果不提Picasaweb有邮件推送地址,估计很多人都已经忘记了!为了找同步到G+上的朋友会看得糊里糊涂!","link":"/auto-save-photo-to-qqmail/"},{"title":"Auto operation Weibo","text":"The code address of this article is: auto operation weibo Chromedrive download: Taobao Mirror , need to be consistent with your Chrome version auto operation weibo 123456789101112131415161718192021222324252627from selenium import webdriverimport timedriver = webdriver.Chrome('/Applications/chromedriver')# login weibodef weibo_login(username, password): # open weibo index driver.get('https://passport.weibo.cn/signin/login') driver.implicitly_wait(5) time.sleep(1) # fill the info: username, password driver.find_element_by_id('loginName').send_keys(username) driver.find_element_by_id('loginPassword').send_keys(password) time.sleep(1) # click login driver.find_element_by_id('loginAction').click() time.sleep(1)# set username, passwordusername = 'ivandoo75@gmail.com'password = 'ooxx'# Mobile phone verification is required here, but still can’t log in fully automaticallyweibo_login(username, password) follow user 12345678910111213141516171819202122def add_follow(uid): driver.get('https://m.weibo.com/u/' + str(uid)) time.sleep(1) # driver.find_element_by_id('follow').click() follow_button = driver.find_element_by_xpath('//div[@class=&quot;btn_bed W_fl&quot;]') follow_button.click() time.sleep(1) # select group group_button = driver.find_element_by_xpath('//div[@class=&quot;list_content W_f14&quot;]/ul[@class=&quot;list_ul&quot;]/li[@class=&quot;item&quot;][2]') group_button.click() time.sleep(1) # cancel the select cancel_button = driver.find_element_by_xpath('//div[@class=&quot;W_layer_btn S_bg1&quot;]/a[@class=&quot;W_btn_b btn_34px&quot;]') cancel_button.click() time.sleep(1)# 每天学点心理学UIDuid = '1890826225'add_follow(uid) create text and publish 1234567891011121314151617181920212223242526272829303132333435def add_comment(weibo_url, content): driver.get(weibo_url) driver.implicitly_wait(5) content_textarea = driver.find_element_by_css_selector('textarea.W.input').clear() content_textarea = driver.find_element_by_css_selector('textarea.W.input').send_keys(content) time.sleep(2) comment_button = driver.find_element_by_css_selector('.W_btn_a').click()# post the textdef post_weibo(content): # go to the user index driver.get('https://weibo.com') driver.implicitly_wait(5) # click publish button # post_button = driver.find_element_by_css_selector('[node-type=&quot;publish&quot;]').click() # input content word to textarea content_textarea = driver.find_element_by_css_selector('textarea.W_input[node-type=&quot;textEl&quot;]').send_keys(content) time.sleep(2) # click publish button post_button = driver.find_element_by_css_selector(&quot;[node-type='submit']&quot;).click() time.sleep(1)# comment the weiboweibo_url = 'https://weibo.com/1890826225/HjjqSahwl'content= 'here is Hivan du, Best wish to u.'# auto send weibocontent = 'Learning is a belief!'post_weibo(content)","link":"/auto_operation_weibo/"},{"title":"针对某一个项目自动切换node版本","text":"nvm作为node的版本管理器，并不具备自动切换版本切换的功能，有的时候我们需要针对某一个项目切换当前的node版本，这个时候就需要用到其他工具了。比如avn 举例项目:project 因为最近Node更新到10之后，我将系统默认版本切换到了10，有不更新不舒服斯基强迫症 而project 编译的版本为8，否则会出现编译出错。 123$ brew install nvm$ nvm i -g avn$ avn steup 之后在project根目录中添加一个文件.node-version 123$ touch .node-version$ echo v8 &gt;&gt; .node-version #node需要切换的版本$ echo `source &quot;$HOME/.avn/bin/avn.sh&quot; # load avn` &gt;&gt; ~/.zshrc 这样就可以了。 不过不排除报错的情况，如果是brew 安装的nvm, 则默认nvm.sh并不在~/.nvm目录内，这个时候可能需要在执行一下某段脚本。一样添加到~/.zshrc内 1$ echo `[[ -s &quot;$(brew --prefix nvm)/nvm.sh&quot; ]] &amp;&amp; source $(brew --prefix nvm)/nvm.sh` &gt;&gt; ~/.zshrc 再切换一下项目目录 12$ cd $project$ avn activated v8.11.2 (avn-nvm v8.11.2) 至此完成了！","link":"/avn-change-node-version-for-a-project/"},{"title":"Boston house analysis","text":"The source code: Boston House 1234567891011121314151617181920212223# Import package# Used to load the Boston housing price data setfrom sklearn.datasets import load_boston# pandas toolkit If you are unfamiliar with pandas, you can refer to the official 10-minute tutorial: https://pandas.pydata.org/pandas-docs/stable/10min.htmlimport pandas as pdimport numpy as np# seaborn for drawingimport seaborn as snsimport matplotlib.pyplot as plt# Show drawing%matplotlib inlinedata = load_boston() # load datasedata.keys() # Fields inside datadf = pd.DataFrame(data['data'])# Looking at the first 5 rows of the dataframe, we can see that the column names are numbersdf.head(5)data['feature_names'] # Feature name The Table params and chinese info 123456789101112131415params chinese infoCRIM 住房所在城镇的人均犯罪率ZN 住房用地超过 25000 平方尺的比例INDUS 住房所在城镇非零售商用土地的比例CHAS 有关查理斯河的虚拟变量（如果住房位于河边则为1,否则为0 ）NOX 一氧化氮浓度RM 每处住房的平均房间数AGE 建于 1940 年之前的业主自住房比例DIS 住房距离波士顿五大中心区域的加权距离RAD 离住房最近的公路入口编号TAX 每 10000 美元的全额财产税金额PTRATIO 住房所在城镇的师生比例B 1000(Bk-0.63)^2,其中 Bk 指代城镇中黑人的比例LSTAT 弱势群体人口所占比例MEDV 业主自住房的中位数房价（以千美元计） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# Replace numeric column names with feature namesdf.columns = data['feature_names']df.head(5)# The target is the house price, which is also our target value. We assign the target value to the dataframedf['price'] = data['target']df.head(5)# View the correlation coefficient between the feature and price, positive correlation and negative correlationsns.heatmap(df.corr(), annot=True, fmt='.1f')plt.scatter(df['RM'], df['price'])plt.figure(figsize=(20, 5))# View the data distribution display of some features and pricefeatures = ['LSTAT', 'RM']target = df['price']for i, col in enumerate(features): plt.subplot(1, len(features), i+1) x = df[col] y = target plt.scatter(x, y, marker = 'o') plt.title('{} price'.format(col)) plt.xlabel(col) plt.ylabel('price')# Simple example: univariate forecast pricex = df['RM']y = df['price']history_notes = {_x: _y for _x, _y in zip(x,y)}history_notes[6.575]# Find the top three prices that are closest to RM:6.57,similary_ys = [y for _, y in sorted(history_notes.items(), key=lambda x_y: (x_y[0] - 6.57) ** 2)[:3]]similary_ys# Calculate the average of threenp.mean(similary_ys) Use historical data to predict data that has never been seen before, the most direct method K-Neighbor-Nearst 12345678def knn(query_x, history, top_n = 3): sorted_notes = sorted(history.items(), key = lambda x_y: (x_y[0] - query_x)**2) similar_notes = sorted_notes[:top_n] similar_ys = [y for _, y in similar_notes] return np.mean(similar_ys)knn(5.4, history_notes) In order to obtain results faster, we hope to obtain predictive power by fitting a function \\[ f(rm) = k * rm + b \\] Random Approach \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} (\\hat{y_i} - y_i) ^ 2 \\] \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] 1234567891011121314151617181920212223def loss(y_hat, y): return np.mean((y_hat - y)**2)import randommin_loss = float('inf')best_k, best_b = None, Nonefor step in range(1000): min_v, max_v = -100, 100 k, b = random.randrange(min_v, max_v), random.randrange(min_v, max_v) y_hats = [k * rm_i + b for rm_i in x] current_loss = loss(y_hats, y) if current_loss &lt; min_loss: min_loss = current_loss best_k, best_b = k, b print(f'{step}, we have func f(rm) = {k} * rm + {b}, lss is :{current_loss}')plt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x]) Monte Carlo simulation(蒙特卡洛模拟) Supervisor \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{k}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i) * rm_i \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{b}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i)\\] 123456789101112131415161718192021222324252627def partial_k(k, b, x, y): return 2 * np.mean((k*x+b-y) * x)def partial_b(k, b, x, y): return 2 * np.mean(k*x+b-y)k, b = random.random(), random.random()min_loss = float('inf')best_k, best_b = None, Nonelearning_rate = 1e-2for step in range(2000): k, b = k + (-1 * partial_k(k, b, x, y) * learning_rate), b + (-1 * partial_b(k, b, x, y) * learning_rate) y_hats = k * x + b current_loss = loss(y_hats, y) if current_loss &lt; min_loss: min_loss = current_loss best_k, best_b = k, b print(f'setp {step}, we have func f(rm) = {k} * rm + {b}, lss is :{current_loss}')best_k, best_bplt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x]) Supervised Learning We turn the forecast of housing prices into a more responsible and sophisticated model. What should we do? \\[ f(x) = k * x + b \\] \\[ f(x) = k2 * \\sigma(k_1 * x + b_1) + b2 \\] \\[ \\sigma(x) = \\frac{1}{1 + e^(-x)} \\] 12345678910111213141516171819def sigmoid(x): return 1 / (1+np.exp(-x))sub_x = np.linspace(-10, 10)plt.plot(sub_x, sigmoid(sub_x))def random_linear(x): k, b = random.random(), random.random() return k * x + bdef complex_function(x): return (random_linear(x))for _ in range(10): index = random.randrange(0, len(sub_x)) sub_x_1, sub_x_2 = sub_x[:index], sub_x[index:] new_y = np.concatenate((complex_function(sub_x_1), complex_function(sub_x_2))) plt.plot(sub_x, new_y) We can implement more complex functions through simple, basic modules and repeated superposition For more and more complex functions? How does the computer seek guidance? What is machine learning? The shortcomings of this method of KNN, what is the background of the proposed linear fitting How to get faster function weight update through supervision method The combination of nonlinear and linear functions can fit very complex functions Deep learning we can fit more complex functions through basic function modules Assigment: \\[ L2-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{(\\hat{y} - y)}^2 \\] \\[ L1-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{|(\\hat{y} - y)|} \\] L2-Loss becomes L1Loss and achieves gradient descent Realize L1Loss gradient descent from 0 1. import package 12import numpy as npimport pandas as pd 2. load data 1234567891011from sklearn.datasets import load_bostondata = load_boston()data.keys()data_train = data.datadata_traget = data.targetdf = pd.DataFrame(data_train, columns = data.feature_names)df.head()df.describe() # Data description, you can view the statistics of each variable 3. Data preprocessing Normalization or standardization can prevent a certain dimension or a few dimensions from affecting the data too much when there are very many dimensions, and secondly, the program can run faster. There are many methods, such as standardization, min-max, z-score, p-norm, etc. How to use it depends on the characteristics of the data set. Further reading-数据标准化的迷思之深度学习领域 12345678910111213from sklearn.preprocessing import StandardScaler# z = (x-u) / s u is the mean, s is the standard deviationss = StandardScaler() data_train = ss.fit_transform(data_train)# For linear models, normalization or standardization is generally required, otherwise gradient explosion will occur, and tree models are generally not requireddata_train = pd.DataFrame(data_train, columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'])data_train.describe() # y=Σwixi+# Because the derivation of b is all 1, add a bias b to the data and set it to 1, as a feature of the data and update the gradient wi*b=widata_train['bias'] = 1data_train Divide the data set, where 20% of the data is used as the test set X_test, y_test, and the other 80% are used as the training set X_train, y_train, where random_state is the random seed 1234567from sklearn.model_selection import train_test_splittrain_x, test_x, train_y, test_y = train_test_split(data_train, data_traget, test_size = 0.2, random_state=42)print('train_x.shape, train_y.shape', train_x.shape, train_y.shape)print('test_x.shape, test_y.shape', test_x.shape, test_y.shape)train_x = np.array(train_x) Model training and gradient update 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def l1_cost(x, y, theta): &quot;&quot;&quot; x: 特征 y: 目标值 thta: 模型参数 &quot;&quot;&quot; k = x.shape[0] total_cost = 0 for i in range(k): total_cost += 1/k * np.abs(y[i] -theta.dot(x[i, :])) return total_costdef l2_cost(x, y, theta): k = x.shape[0] total_cost = 0 for i in range(k): total_cost += 1/k * (y[i] -theta.dot(x[i,:])) ** 2 return total_costnp.zeros(10).shapedef step_l1_gradient(x, y, learning_rate, theta): &quot;&quot;&quot; Function to calculate the gradient of the MAE loss function Return the gradient value 0 for the non-differentiable point at 0 X:特征向量 y：目标值 learing_rate:学习率 theta:参数 &quot;&quot;&quot; n = x.shape[0] # print(n) e = y - x @ theta gradients = - (x.T @ np.sign(e)) / n # sign is a sign function thata = theta - learning_rate * gradients return thetadef step_l2_gradient(x, y, learning_rate, theta): k = x.shape[0] n = x.shape[1] gradients = np.zeros(n) for i in range(k): for j in range(n): gradients[j] += (-2/k) * (y[i] - (theta.dot(x[i, :]))) * x[i, j] theta = theta - learning_rate * gradient return theta# def step_gradient(X, y, learning_rate, theta):# &quot;&quot;&quot;# X:特征向量# y：目标值# learing_rate:学习率# theta:参数# &quot;&quot;&quot;# m_deriv = 0# N = len(X)# for i in range(N):# # 计算偏导# # -x(y - (mx + b)) / |mx + b|# m_deriv += - X[i] * (y[i] - (theta*X[i] + b)) / abs(y[i] - (theta*X[i] + b))# # We subtract because the derivatives point in direction of steepest ascent# theta -= (m_deriv / float(N)) * learning_rate# # theta = theta - learning_rate * gradients# return thetadef gradient_descent(train_x, train_y, learning_rate, iterations): k = train_x.shape[0] n = train_x.shape[1] theta = np.zeros(n) # Initialization parameters loss_values = [] # print(theta.shape) for i in range(iterations): theta = step_l1_gradient(train_x, train_y, learning_rate, theta) loss = l1_cost(train_x, train_y, theta) loss_values.append(loss) print(i, 'cost:', loss) return theta, loss_values# Training parameterslearning_rate = 0.04 # Learning rateiterations = 300 # Number of iterationstheta, loss_values = gradient_descent(train_x, train_y, learning_rate, iterations)","link":"/boston_analysis/"},{"title":"Boston house price CART regression tree","text":"On the code 12345678910111213141516171819202122232425262728293031323334353637383940# CART regression tree predictionfrom sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_bostonfrom sklearn.metrics import r2_score,mean_absolute_error, mean_squared_errorfrom sklearn.tree import DecisionTreeRegressor,export_graphvizimport graphviz# Prepare data setboston = load_boston()# Explore dataprint(boston.feature_names)# Get feature set and pricefeatures = boston.dataprices = boston.target# Randomly extract 33% of the data as the test set, and the rest as the training settrain_features, test_features, train_price, test_price = train_test_split(features,prices,test_size=0.33)# Create CART regression treedtr = DecisionTreeRegressor()# Fitting and constructing CART regression treedtr.fit(train_features, train_price)# Predict housing prices in the test setpredict_price = dtr.predict(test_features)grap_data = export_graphviz(dtr, out_file=None)graph = graphviz.Source(grap_data)# Result evaluation of test setprint(f'Regression tree mean squared deviation:',mean_squared_error(test_price, predict_price))print(f'Regression tree absolute value deviation mean:',mean_absolute_error(test_price, predict_price))# Generate regression tree visualizationgraph.render('Boston') !&gt; Before running this code, please ensure that the relevant dependencies have been installed;","link":"/boston_data_CART/"},{"title":"bye-google-reader","text":"今早一打开网页,满篇都是关于Google将于7月1日正式关闭Reader的消息. 再见吧,Reader. 这八年,基本每天,我都会去看看你... 可是又能如何,到了该走的时候了.即便这个理由让我完全无法接受. Bye, 我亲爱的GReader...","link":"/bye-google-reader/"},{"title":"CM6 test0 32b","text":"好吧,熟悉的人看到标题应该能猜到这就是Android 2.2 的CM版本.没想到这么快会出现,和之前发布的版本不同,这次编译与CM大神的版本!没想到在和儿子过生的当间就发布了... 基于7月5日最新CM6源码编译，新增了电池百分比显示的开关，在Cyanogenmod 设置里进行更改 默认关闭jit，在32a/32b等低端机上开启jit对性能没有改善，反倒更占内存，故在这个版本中关闭jit 修改ADW的默认设置，使其常驻内存，改善从其它程序退回桌面的速度 修改ADW的壁纸图库，用AOSP的图库替换了CM的图库 修改了framework.jar，使用了geesun的代码试其支持中文运营商显示 使用了最新的FRF91的GAPPS CM6自带的contact文件有不少bug，故换成了aosp的contact，虽然相对cm6的功能更少，但非常稳定 重新编译了kernel，个人感觉比默认的kernel更稳定 进一步汉化了framework和superuser等程序 新增32a的支持，32a的用户也可以使用 Known Issues： 第一次启动时可能会意外重启，完成设置后就不会出现这个问题 相机中按0x变焦按键会使相机fc 摄像无法正常使用 由于Gapps都是Nexus专用的，故其素材的尺寸都很大，特别是gmail，显示出来很大，这个暂时无法解决 Dream和Magic因为性能问题，不支持flash，即便刷了2.2也不可能运行flash，所以不要去市场下载flash程序了，不会起作用的，另外也不要再求使用flash的方法了，在地球上不存在解决方法，除非你换手机 App2sd： rom支持app2sd，但默认没有开启。 开启方法： 在超级终端中输入： su pm setInstallLocation 2 即可开启app2sd，不需要有ext分区就可以使用，但官方的这个app2sd还不太稳定，不建议使用。 Download： Dream/Magic 32b:http://thesoloblack.com/rom/cm6-test0-32b-0705-fixed.zip Magic 32a:http://thesoloblack.com/rom/cm6-test0-32a-0705-fixed.zip 本更新信息和下载链接均来源于机锋网!","link":"/cm6-test0-32b/"},{"title":"CnBloggerCon 2012","text":"一年一度的中文网志年会与今晨八点开始，不过没想到的是昨晚就已经完成了开幕式。在Google+上由Isaac Mao主持开幕Hangout视频。可惜我没有看到！ 以上的年会开放营的设计示意图本来我是想稍加修改一下再传到自己的Flickr上的！结果一直到今天开幕都没有太多时间来做这件事情，其实这篇博客都应该是昨天晚上就生产出来的才对。 今年的年会与以前的年会有所不同，没有设置会场，而启用了“云智慧”的概念。而Google+中的会议视频以及才推出没多久的Events为年会此种形式提供了可行性！ 第一天的会议主题为“公民媒体”，可以在如下两个地址内参与讨论： +《云平行会： 麦康瑞在全球之声2012峰会》 +《云访谈：老虎庙和佐拉谈公民记者》 Google+上的Events讨论地址：中文网志年会2012 Events 还请各位中文Blogger们积极参加，因为要翻墙，So,记得戴套，或自备安全工具！你们懂得。","link":"/cnbloggercon-20/"},{"title":"关于设备转向后的自适应","text":"关于移动端的适配，都知道其实rem是比较好的一个适配方案，但是rem是根据根目录的字体大小来调解的，那么，我们在做网页的时候，屏幕旋转后，能否让根目录的字体跟着变化呢？ 先上代码： 1234$(function(){ var size = $(window).width() / 25; $('html').css('font-size': size);}); 这样在css中用rem单位是没什么问题，但是如果屏幕旋转之后，你就会发现，真的不能看了就。原因就是屏幕旋转以后，根上的字体并没有随之变化。 所以我们来加上 12345678910111213// 监视设备方向window.addEventListener(&quot;orientationchange&quot;, function() { media();}, false);function media(argument) { // 因为获取尺寸出错，需要延迟获取 setTimeout(function(){ var size = $(window).width() / 25; console.log('the device size: '+size); $('html').css('font-size', size); }, 200); }","link":"/css-rem-and-javascript/"},{"title":"自定义文件上传框","text":"其实这根本就不值得写出来，只是可能前几步大家都做了，只是最后一步就忽略了。 我们在自定义input:file的时候，一般来说都是外边包一层，里边在写一个&lt;input type=\"file\"&gt;, 然后将其透明值设置成0,然后再定义外层的样式来达到自定义的目的。 HTML： &lt;div class=&quot;upfileOutWrap&quot;&gt; &lt;div class=&quot;upfileWrap&quot;&gt;&lt;input type=&quot;file&quot;&gt;&lt;/div&gt; &lt;div class=&quot;upfileBG&quot;&gt;upload image&lt;/div&gt; &lt;/div&gt; CSS： .upfileOutWrap { cursor: pointer; width: 199px; height: 42px; line-height: 42px; position: relative; } .upfileWrap{ width: 100%; height: 100%; position: absolute; top:-1; left: -1; z-index:2; } .upfileWrap input{ opacity: 0; filter: alpha(opacity=0); cursor: pointer; width: 100%; height: 100%; font-size: 32px; } .upfileBG{ width:100%; height:100%; background: url(./images/upload.png) no-repeat; font-size: 14px; color: white; position: absolute; top:-1; left: -1; padding-left:10px; z-index:1; } 可是这个时候还是有点问题，就是万恶的IE下边。 IE下边的input标签默认都是有光标的，:file也不例外，而且IE下边必须要点击”Browse”或者双击input输入框才会有效果。那么这个时候在IE下就会出现如图的莫名其妙的问题，注意左边的光标，并且还需要双击才会弹出文件选择窗口。 这个时候如果你把input透明度设置成100显示出来，就会发现原来是这样的。 所以这个时候，如果是其他标准浏览器，那么设置好input的高宽就搞定了，而IE下边，还必须考虑如何让”Browse”按钮能铺满我们所自定的div样式。这样我们才能实现IE下不出现光标，而且单击弹出文件选择窗口。 这个时候，看似毫无办法，其实我们可以选择增加字体的大小。当字体变成32px的时候，就是这个样子的。 好了，这样我们就搞定了，将input:file 继续设置为完全透明。那个可恶的光标不见了，我们也可以实现IE下单击。当然，字体到底用多大的，要视你自己定义的视觉效果来看，自己调试吧。 Final CSS: .upfileOutWrap { cursor: pointer; width: 199px; height: 42px; line-height: 42px; position: relative; } .upfileWrap{ width: 100%; height: 100%; position: absolute; top:-1; left: -1; z-index:2; } .upfileWrap input{ opacity: 0; filter: alpha(opacity=0); cursor: pointer; width: 100%; height: 100%; } .upfileBG{ width:100%; height:100%; background: url(./images/upload.png) no-repeat; font-size: 14px; color: white; position: absolute; top:-1; left: -1; padding-left:10px; z-index:1; }","link":"/custom-inputfile/"},{"title":"Debian安装SSH","text":"要求： 安装Debian 8.* 64位操作系统 分区要求: 不要分区 Partitioning mehod: use entire disk Partitioning scheme: All files in one partition 选择源镜像 (mirror country) 请选择china 然后选择ftp.cn.debian.org 程序和服务需求 debian 默认最小安装，安装的时候不用安装桌面环境和标准系统实用程序(以下两个不需要勾选): - Debian destop environment - Standard system utilities 如果有SSH server选项，请务必勾选，会省很多麻烦 安装SSH debian最小安装默认是没有配置apt-get源的，这个时候无法实用apt-get install命令，所以在安装SSH之前，我们需要先配置apt-get: 配置apt-get 终端内操作 1234567# 首先我们需要备份原有配置文件cp /etc/apt/sources.list /etc/apt/sources.listbak# 然后对资源列表文件进行编辑vi /etc/apt/sources.list# 当然也可以实用nano命令nano /etc/apt/source.list PS: 如果对VI操作不熟悉的，可以看这里 vi编辑器常见命令实用 如果安装的时候按照之前我给的步骤来，那么这会的sources.list应该是这样的 对文件进行更改，将以下命令加入文件并保存: 12deb http://ftp.cn.debian.org/debian/ jessie main contrib non-free deb-src http://ftp.cn.debian.org/debian/ jessie main contrib non-free 更改后的文件如图: main, contrib, non-free 分属不同的源，添加后可以从不同的源仓库更新文件索引 至此apt-get源就配置完毕，接下来我们就可以安装SSH了 安装SSH 在终端内输入以下命令: 12345678# 更新apt-get源apt-get update# 更新系统apt-get upgrade# 安装SSHapt-get install ssh 这样就好了，SSH安装完毕 注意 如果一直安装不能成功，请往下看: 首先，请ping ftp.cn.debin.org 和 ping mirrors.163.com 来测试一下能否ping的通域名，如果ping不通，请往下看: 有的时候机房安装debian后会出现域名解析问题,这又是另外一个问题。比如ping 123.111.123.111 是OK的，但是如果ping对应的域名如: ping mirrors.163.com就会出现unknow host的问题。 似乎linux很大一部分都会出现这种问题，能ping的通IP但是ping不通域名。那么请查看以下原因解决: 1. 查看DNS解析是否有问题，确定设置了域名服务器: cat /etc/resolv.conf 123nameserver 114.114.114.114 nameserver 8.8.8.8 nameserver 8.8.4.4 2. 确保网关已设置 grep GATEWAY /etc/sysconfig/network-scripts/ifcfg* 1/etc/sysconfig/network-scripts/ifcfg-eth0:GATEWAY=192.168.40.1 如果未设置，则通过以下方法增加网关 route add default gw 192.168.40.1 或者手工编写/etc/sysconfig/network-scripts/ifcfg* 然后重启network服务: service network restart 3. 确保可用dns解析 grep hosts /etc/nsswitch.conf 文件打开后为: 1hosts: files dns 4. 查看是否防火墙的问题 因为域名解析用到了53端口,需要把下面设置配置到防火墙里: 1234iptables -A INPUT -p udp --sport 53 -j ACCEPT iptables -A OUTPUT -p udp --dport 53 -j ACCEPT iptables -A INPUT -p udp --dport 53 -j ACCEPT iptables -A OUTPUT -p udp --sport 53 -j ACCEPT 如果找不到原因或者不知道怎么设置，那么就用以下最笨的方法: 如果出现这样的问题，更新sources.list后会无法更新也无法安装ssh. 如果出现这样的问题，更新sources.list地址为一下地址: 12345678deb http://123.58.173.186/debian/ jessie main non-free contribdeb http://123.58.173.186/debian/ jessie-updates main non-free contribdeb http://123.58.173.186/debian/ jessie-backports main non-free contribdeb-src http://123.58.173.186/debian/ jessie main non-free contribdeb-src http://123.58.173.186/debian/ jessie-updates main non-free contribdeb-src http://123.58.173.186/debian/ jessie-backports main non-free contribdeb http://123.58.173.186/debian-security/ jessie/updates main non-free contribdeb-src http://123.58.173.186/debian-security/ jessie/updates main non-free contrib 利用IP地址代替域名，但是测试下来只有163的镜像可以这样做。来源为网易镜像的帮助文档: Debian镜像使用帮助","link":"/debian-install-ssh/"},{"title":"Digits recognition","text":"The code address of this article is: digit recognition Convolution operation demo 123456789101112131415import pylabimport numpy as npfrom scipy import signal# set imgimg = np.array([[10, 10, 10, 10, 10],[10, 5, 5, 5, 10], [10, 5, 5, 5, 10], [10, 5, 5, 5, 10], [10, 10, 10, 10, 10]])# set convolutionfil = np.array([[-1, -1, 0], [-1, 0, 1], [0, 1, 1]])# convolution the imgres = signal.convolve2d(img, fil, mode='valid')# output the resultprint(res) output 123[[ 15 10 0] [ 10 0 -10] [ 0 -10 -15]] A image demo 1234567891011121314151617181920212223import matplotlib.pyplot as pltimport pylabimport cv2import numpy as npfrom scipy import signal# read the imageimg = cv2.imread('./data/weixin.jpg', 0) # Any picture# show the imageplt.imshow(img, cmap='gray')pylab.show()# set the convolutionfil = np.array([[-1,-1,0], [-1, 0, 1], [0, 1, 1]])# convolution operationres = signal.convolve2d(img, fil, mode='valid')print(res)# show convolution imageplt.imshow(res, cmap = 'gray')pylab.show() use LeNet model to recognize Mnist handwritten digits 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import kerasfrom keras.datasets import mnistfrom keras.layers import Conv2D, MaxPooling2Dfrom keras.layers import Dense, Flattenfrom keras.models import Sequentialimport warningswarnings.filterwarnings('ignore')# load data(train_x, train_y), (test_x, test_y) = mnist.load_data()train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)train_x = train_x / 255test_x = test_x / 255train_y = keras.utils.to_categorical(train_y, 10)test_y = keras.utils.to_categorical(test_y, 10)# create sequential modelsmodel = Sequential()# The first convolutional layer: 6 convolution kernels, the size is 5*5, relu activation functionmodel.add(Conv2D(6, kernel_size = (5,5), activation='relu', input_shape=(28, 28, 1)))# the second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size = (2, 2)))# the third convolutional layer: 16 convolution kernels, the size is 5*5, relu activation functionmodel.add(Conv2D(16, kernel_size = (5, 5), activation = 'relu'))# the second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size = (2, 2)))# Flatten the parameters, which is called a convolutional layer in leNet5. in fact, this layer is a one-dimensional vector, the same as the fully connected layermodel.add(Flatten())model.add(Dense(120, activation = 'relu'))# Fully connected layer, the number of output nodes is 84model.add(Dense(84, activation = 'relu'))# The output layer uses the softmax activation function to calculate the classification probabilitymodel.add(Dense(10, activation='softmax'))# set the loss function and optimizer configurationmodel.compile(loss = keras.metrics.categorical_crossentropy, optimizer = keras.optimizers.Adam(), metrics = ['accuracy'])# Incoming training data for trainingmodel.fit(train_x, train_y, batch_size = 128, epochs = 2, verbose = 1, validation_data = (test_x, test_y))# Evaluate the resultsscore = model.evaluate(test_x, test_y)print('Error: %.4lf' % score[0])print('Accuracy: ', score[1]) 12345678Train on 60000 samples, validate on 10000 samplesEpoch 1/260000/60000 [==============================] - 37s 616us/step - loss: 0.3091 - accuracy: 0.9102 - val_loss: 0.1010 - val_accuracy: 0.9696Epoch 2/260000/60000 [==============================] - 36s 595us/step - loss: 0.0876 - accuracy: 0.9731 - val_loss: 0.0572 - val_accuracy: 0.981410000/10000 [==============================] - 3s 328us/stepError: 0.0572Accuracy: 0.9814000129699707","link":"/digits_recognition/"},{"title":"使用Plotnine制作元素周期表","text":"首先需要了解元素周期表以及元素数据: 维基百科的元素周期表词条 元素数据 元素周期表基本构成如下: 族：表中的每一列就是一族，从左向右依次为 1、2……18 族。 周期：表中的行。 元素：每个方框表示一个元素，其中包括元素符号、名称、原子序数、原子量。 在主表下面还有镧系元素和锕系元素表。 用颜色区分金属、非金属等常见的物质状态。 最终呈现: 其他形状元素周期表 导入和处理数据 1234567# 导入依赖import pandas as pdimport numpy as npfrom plotnine import *# 读取数据elements = pd.read_csv('~/data/cbcpv/elemanets/elements.csv') 研究数据集 1234567891011121314151617181920212223242526272829303132elements.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 118 entries, 0 to 117Data columns (total 21 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 atomic number 118 non-null int64 1 symbol 118 non-null object 2 name 118 non-null object 3 atomic mass 118 non-null object 4 CPK 118 non-null object 5 electronic configuration 118 non-null object 6 electronegativity 97 non-null float64 7 atomic radius 71 non-null float64 8 ion radius 92 non-null object 9 van der Waals radius 38 non-null float64 10 IE-1 102 non-null float64 11 EA 85 non-null float64 12 standard state 99 non-null object 13 bonding type 98 non-null object 14 melting point 101 non-null float64 15 boiling point 94 non-null float64 16 density 96 non-null float64 17 metal 118 non-null object 18 year discovered 118 non-null object 19 group 118 non-null object 20 period 118 non-null int64 dtypes: float64(8), int64(2), object(11)memory usage: 19.5+ KB&quot;&quot;&quot; 特征group就是该元素所在的族，但是，如果用elements['group'] 查看所有内容，会发现有的记录中用 '-' 标记，说明它不属于任何族，说明它们应该是镧系元素或者锕系元素。根据数据分析的通常要求，'-' 符号最好用数字表示，这里用 ﹣1 转化数据集 123456789101112131415161718# 转换族elements['group'] = [-1 if g=='-' else int(g) for g in elements['group']]elements['group']&quot;&quot;&quot;0 11 182 13 24 13 ..113 14114 15115 16116 17117 18Name: group, Length: 118, dtype: int64&quot;&quot;&quot; 特征 bonding type、metal 都是分类数据，因此在类型上进行转化。 123# 转化分类数据elements['bonding type'] = elements['bonding type'].astype('category')elements['metal'] = elements['metal'].astype('category') 将原本的整数型atomic number特征,转化为字符串类型 1elements['atomic_number'] = elements['atomic number'].astype(str) 元素周期表有两个部分,上面一部分每个元素是属于某一个族的,即group特征中的1-18, 而对于值是-1的则表示这些元素应该在下面的镧系或者锕系元素表中。下面分别用 top 变量和 bottom 变量引用这两部分元素集合. 123## 分别用top和bottom变量引用上下部分元素集合top = elements.query('group != -1').copy()bottom = elements.query('group == -1').copy() 元素周期表中横向表示的是族（group），纵向表示的是周期（period），用下面的方式在 top 中创建两个特征，分别为“族”和“周期”的值。 1234567891011121314151617181920212223242526272829303132333435363738## 在top中区分“族”(group)和“周期”(period)的值&quot;&quot;&quot;横向表示族,纵向表示周期&quot;&quot;&quot;top['x'] = top.grouptop['y'] = top.periodtop['x']&quot;&quot;&quot;0 11 182 13 24 13 ..113 14114 15115 16116 17117 18Name: x, Length: 90, dtype: int64&quot;&quot;&quot;top['y']&quot;&quot;&quot;0 11 12 23 24 2 ..113 7114 7115 7116 7117 7Name: y, Length: 90, dtype: int64&quot;&quot;&quot; 除了上面的部分之外，下面的锕系和镧系元素也要做类似的配置。不过，横坐标不能用 group 特征的值，因为前面设置为 ﹣1。 12345678nrows = 2&quot;&quot;&quot;hshift 和 vshift 分别表示横、纵间距，这样就为每个锕系和镧系元素增加了横纵坐标值。&quot;&quot;&quot;hshift = 3.5vshift = 3bottom['x'] = np.tile(np.arange(len(bottom) // nrows), nrows) + hshiftbottom['y'] = bottom.period + vshift 每个元素都占了一个小方块,所以,这个小方块(元素块)的大小要设置一下 123## 设置元素占据的小矩形tile_width = 0.95tile_height = 0.95 开始画图 1234(ggplot(aes('x', 'y')) + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height))) 这里只有美学映射,没有传入数据集.因为在图层对象中,要传入不同的数据集: “top”和“bottom”. top表示主表中的, bottom表示下面的锕、镧系元素 geom_tile绘制安放元素块图层,并使用top数据集,在引入一个图层,绘制bottom对应的图层. 但是我们发现表反了, 所以需要实现在Y轴方向上的坐标轴翻转. 123456(ggplot(aes('x', 'y')) +geom_tile(top, aes(width=tile_width, height=tile_height)) +geom_tile(bottom, aes(width=tile_width, height=tile_height)) # 在Y轴上进行翻转 +scale_y_reverse() # new) 基本样式已经有了。 前面已经把特征“metal”的数据转换为分类数据，下面用这些数据对不同元素的小矩形（以后简称“元素块”）上色。 1234567(ggplot(aes('x', 'y')) # 对数据不同的元素块进行上色 + aes(fill='metal') # new + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + scale_y_reverse() ) 然后,我们要将化学元素的有关信息写到这些元素块上,这里要写到元素块上的包括: 原子序数，对应着数据集中的特征是“atomic number”； 元素符号，对应着数据集中的特征是“symbol”； 元素名称，对应着数据集中的特征是“name”； 原子量，对应着数据集中的特征是“automic mass” 在这里,我们要绘制四个图层,以便安放四个元素信息, 每个图层上面一个特征,并且每个图层的位置、字号大小等都不相同. 为此我们写一个函数方法来实现: 1234567891011121314151617181920&quot;&quot;&quot;nudge_x: 文本在水平方向上的相对位置nudge_y: 文本在竖直方向上的相对位置ha: 可选'left', 'center', 'right', 标示水平方向的对齐方式va: 可选'top', 'center', 'bottom', 表示竖直方向的堆砌方式size: 字号大小fontweight: 字族中的字体粗细&quot;&quot;&quot;def inner_text(data): layers = [geom_text(data, aes(label='atomic_number'), nudge_x=-0.40, nudge_y=-.40, ha='left', va='top', fontweight='normal', size=6), geom_text(data, aes(label='symbol'), nudge_y=.1, size=9), geom_text(data, aes(label='name'), nudge_y=-0.125, fontweight='normal', size=4.5), geom_text(data, aes(label='atomic mass'), nudge_y=-.3, fontweight='normal', size=4.5) ] return layers 然后我们将函数inner_text应用到绘图流程中去 12345678910111213&quot;&quot;&quot;分别调用两次是因为有top和bottom两个数据&quot;&quot;&quot;(ggplot(aes('x', 'y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) # 绘制上部分图层 + inner_text(top) # new # 绘制下部分图层 + inner_text(bottom) # new + scale_y_reverse()) 是不是觉得图很难看,原因在于我们还没对其进行调整,下面我们就要细微的调整图层,包括大小等 12345678910111213(ggplot(aes('x', 'y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) + scale_y_reverse() # coord_equal作用是设置坐标系的横轴和纵轴 # expand=False, 意味着坐标系的大小由制图所用数据决定 + coord_equal(expand=False) # new # 一个新主题,规定了图纸的尺寸 + theme(figure_size=(12,6)) # new) 在默认的主题中，横纵坐标的图上长度相等，也就是图像是呈现在一张正方形的图纸上，coord_equal 的作用就是设置坐标系的横轴和纵轴，它与 coord_fixed 是完全等效的，能够改变图纸的大小和长宽比例。参数 expand 的值是布尔值，如果为 False，则意味着坐标系的大小（即图纸的大小）由制图所用数据决定。 新增的第二个图层对象是一个新的主题，在其中规定了图纸的尺寸。 我们仔细研究元素周期表,发现Lu和Lr两个元素比较特殊,其实它们不是单独的元素,而是对应着下部分两行的,因此要对这两个进行处理,以区分出与其他元素的不同. 我们将其分为两半,使用过PS作图的同学应该能想到两个不同颜色的图层叠加,上面的图层只有下面图层的一半,那么看起来就像是被分成了两半. 123456# split_df 是绘制新元素块所需要的数据集。split_df = pd.DataFrame({ 'x': 3-tile_width/4, 'y': [6,7], 'metal': pd.Categorical(['lanthanoid', 'actinoid'])}) 123456789101112(ggplot(aes('x','y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) # 将新的数据集用于叠加Lu和Lr的图层上进行遮挡 + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) # new + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) + scale_y_reverse() + coord_equal(expand=False) + theme(figure_size=(12, 6))) 基本制作完成了,下面来美化一下: 123456789101112131415161718(ggplot(aes('x', 'y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) + scale_y_reverse() # 对元素块填充色进行转换 + scale_fill_brewer(type='qual', palette=3) + coord_equal(expand=False) # 增加了一个经典的主题图层对象 + theme_void() + theme(figure_size=(12, 6), # 增加一个主题图层,并设置了该图层的尺寸和背景色 plot_background=element_rect(fill='white') )) 到最后了,我们要解决主表中的元素表上族和周期的问题 观察主表中的每一列,注意我们已经把Y轴映射反序了,如果在H元素的元素块上标注族的序号为“1”, 那么这个“1”的Y轴坐标应该是y=1, 同样,Sc元素块上标注族的需要“3”, 那么“3”的Y轴坐标应该是y=4. 这样,我们就可以创建每列及其对应的Y轴坐标了. 1234567## 创建每列(即:族, 编号为1-18)及其对应的Y轴坐标groupdf = pd.DataFrame({ 'group': range(1, 19), 'y': np.repeat([1,2,4,2,1], [1,1,10,5,1])})groupdf group y 0 1 1 1 2 2 2 3 4 3 4 4 4 5 4 5 6 4 6 7 4 7 8 4 8 9 4 9 10 4 10 11 4 11 12 4 12 13 2 13 14 2 14 15 2 15 16 2 16 17 2 17 18 1 让我们来标注族的序号 123456789101112131415161718192021222324252627## 标注族序号(ggplot(aes('x','y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) # 标注每一列族序号的文本图层 # aes('group', 'y', label='group') 重写了X轴和Y轴的映射 # inherit_aes=False, 不继承映射配置 + geom_text(groupdf, aes('group', 'y', label='group'), color='gray', nudge_y=.525, va='bottom', fontweight='normal', size=9, inherit_aes=False ) # 以Y轴调转坐标轴 + scale_y_reverse() # 对元素块填充色进行转换 + scale_fill_brewer(type='qual', palette=3) + coord_equal(expand=False) # 增加了一个经典的主题图层对象 + theme_void() + theme(figure_size=(12, 6), # 增加一个主题图层,并设置了该图层的尺寸和背景色 plot_background=element_rect(fill='white'), )) 最终,我们标注玩周期就完成了. 周期是对每一行的标注,一共7行,因为标注在左侧,可以把它看成是左侧的Y轴标示,可以在图层上通过对Y轴标示的设置完成周期的标注. 1234567891011121314151617181920212223242526272829303132333435363738## 开始标注周期, 最终完成(ggplot(aes('x', 'y')) # 把特征'metal'的数据转换为分类数据,进行元素块上色 + aes(fill='metal') # 创建上部元素块 + geom_tile(top, aes(width=tile_width, height=tile_height)) # 创建Lu和Lr的半个元素块 + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) # 创建下部元素块 + geom_tile(bottom, aes(width=tile_width, height=tile_height)) # 创建文字图层, 把化学元素的有关信息写到元素块中 + inner_text(top) + inner_text(bottom) # 标注每一列族序号的文本图层 # aes('group', 'y', label='group') 重写了X轴和Y轴的映射 # inherit_aes=False, 不继承映射配置 + geom_text(groupdf, aes('group', 'y', label='group'), color='gray', nudge_y=.525, va='bottom', fontweight='normal', size=9, inherit_aes=False ) # 以Y轴调转坐标轴, 增加了纵坐标主刻度标示数字。 + scale_y_reverse(breaks=range(1, 8), limits=(0, 10.5) ) # 对元素块填充色进行转换 + scale_fill_brewer(type='qual', palette=3) + coord_equal(expand=False) # 增加了一个经典的主题图层对象 + theme_void() + theme(figure_size=(12, 6), # 增加一个主题图层,并设置了该图层的尺寸和背景色 plot_background=element_rect(fill='white'), # 增加了参数 axis_text_y，对 Y 轴标示的显示格式进行了设置。 axis_text_y=element_text(margin={'r':5}, color='gray', size=9) )) 完成...","link":"/elements_by_plotnine/"},{"title":"enlight VS snapseed2","text":"本文知乎专栏地址 在Enlight出来之前，Snapseed曾经是我最主要的修图工具。 当然，Enlight确实是大而全的一款App，但是使用中，却让我有了无法忍受的一点。那就是丢失部分图片信息。 导出的时候，Enlight导出所用的格式是PNG，而Snapseed仍然是jpg导出。自然大家都知道PNG图片所用的格式是无损的，这就是说，Snapseed导出的图像相比Enlight要小很多，因为iPhone默认拍照所保存的都是jpg格式，所以即便再导出PNG格式，那所增加的部分应该是软件通过算法添加进去的。 而，Enlight在导出的PNG格式图片里，却丢失了相机信息，地理位置信息和更重要的拍照时间信息。照片的时间被导出的时间所代替。 同样一张照片，我们先使用Enlight进行修图 然后使用Snapseed 2 修图 以下是使用Metapho查看的照片信息。 1. 首先我们来看Enlight导出的照片信息: 可以看到Camera和Location信息都没有了，照片被导出成PNG格式，而且照片大了很多。 2. 我们再来看看Snapseed导出的照片信息 可以看到Camera和Location信息都保存完好。 这也是Enlight最让我无法接受的一点，其实Enlight在设置里是可以选择增加地理信息的，也就是会调用GPS模块，但是当导出的时候这些信息就全都没有了，即便是用Enlight来拍照。不知道下一版本的Enlight是否能够改善这点。如果对这些信息无所谓的同学，Enlight倒是现今为止最强大的修图工具，Snapseed虽然更新到版本2了，但是似乎依然没有加入曲线。Snapseed2的具体更新，可以点击原文链接查看「领客」内的文。 ———","link":"/enlight-VS-snapseed2/"},{"title":"A Preliminary Study of Machine Learning","text":"Gradient 123456789101112131415def loss(k): return 3 * (k ** 2) + 7 * k -10# -b / 2a = -7 / 6def partial(k): return 6 * k + 7k = ramdom.randint(-10, 10)alpha = 1e-3 # 0.001for i in range(1000): k = k + (-1) * partial(k) * alpha print(k, loss(k))","link":"/example_002/"},{"title":"Introduction to Artificial Intelligence","text":"The code address of this article is: Example 01 The source code is in ipynb format, and the output content can be viewed. rule based 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import random from icecream import ic#rules = &quot;&quot;&quot;#复合句子 = 句子 , 连词 句子#连词 = 而且 | 但是 | 不过#句子 = 主语 谓语 宾语#主语 = 你| 我 | 他 #谓语 = 吃| 玩 #宾语 = 桃子| 皮球# #&quot;&quot;&quot;rules = &quot;&quot;&quot;复合句子 = 句子 , 连词 复合句子 | 句子连词 = 而且 | 但是 | 不过句子 = 主语 谓语 宾语主语 = 你| 我 | 他 谓语 = 吃| 玩 宾语 = 桃子| 皮球 &quot;&quot;&quot;def get_grammer_by_description(description): rules_pattern = [r.split('=') for r in description.split('\\n') if r.strip()] target_with_expend = [(t, ex.split('|')) for t, ex in rules_pattern] grammer = {t.strip(): [e.strip() for e in ex] for t, ex in target_with_expend} return grammer#generated = [t for t in random.choice(grammer['句子']).split()]#test_v = [t for t in random.choice(grammer['谓语']).split()]def generate_by_grammer(grammer, target='句子'): if target not in grammer: return target return ''.join([generate_by_grammer(grammer, t) for t in random.choice(grammer[target]).split()])if __name__ == '__main__': grammer = get_grammer_by_description(rules) #ic(generated) #ic(test_v) #ic(generate_by_grammer(grammer)) ic(generate_by_grammer(grammer, target='复合句子')) water pouring 12345678910111213141516171819202122232425262728293031323334353637383940def water_pouring(b1, b2, goal, start=(0, 0)): if goal in start: return [start] explored = set() froniter = [[('init', start)]] while froniter: path = froniter.pop(0) (x, y) = path[-1][-1] for (state, action) in successors(x, y, b1, b2).items(): if state not in explored: explored.add(state) path2 = path + [(action, state)] if goal in state: return path2 else: froniter.append(path2) return []def successors(x, y, X, Y): return { ((0, y+x) if x + y &lt;= Y else (x + y - Y, Y)): 'X -&gt; Y', ((x + y, 0) if x + y &lt;= X else (X, x + y - X)): 'X &lt;- Y', (X, y): '灌满X', (x, Y): '灌满Y', (0, y): '倒空X', (x, 0): '倒空Y', }if __name__ == '__main__': print(water_pouring(4, 9, 5)) print(water_pouring(4, 9, 5, start=(4, 0))) print(water_pouring(4, 9, 6))","link":"/example_01/"},{"title":"Initial exploration of machine learning","text":"The code address of this article is: Example 02 The source code is in ipynb format, and the output content can be viewed. ## Gradient 123456789101112131415161718192021222324import randomdef loss(k): return 3 * (k ** 2) + 7 * k - 10 # -b / 2a = -7 / 6def partial(k): return 6 * k + 7k = random.randint(-10, 10)alpha = 1e-3 # 0.001for i in range(1000): k = k + (-1) * partial(k) *alpha print(k, loss(k)) # out&quot;&quot;&quot;7.959 124.32404299999999-7.918246 122.66813714954799show more (open the raw output data in a text editor) ...-1.1833014444482555 -14.082503185837805&quot;&quot;&quot; Cutting Problem All the dynamic programming: sub-problems Overlapping sub-problems parse solution 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from collections import defaultdictfrom functools import lru_cache# least recent usedprices = [1, 5, 8, 9, 10, 17, 17, 20, 24, 30, 33]complete_price = defaultdict(int)for i, p in enumerate(prices): complete_price[i+1] = p solution = {}cache = {}#&lt;- if when n .... is huge. size(cache)# keep most important information.@lru_cache(maxsize=2**10)def r(n): # a very classical dynamic programming problem # if n in cache: return cache[n] candidates = [(complete_price[n], (n, 0))] + \\ [(r(i) + r(n-i), (i, n - i)) for i in range(1, n)] optimal_price, split = max(candidates) solution[n] = split # cache[n] = optimal_price return optimal_pricedef parse_solution(n, cut_solution): left, right = cut_solution[n] if left == 0 or right == 0: return [left+right, ] else: return parse_solution(left, cut_solution) + parse_solution(right, cut_solution)if __name__ == '__main__': print(r(19)) print(parse_solution(19, solution)) # out&quot;&quot;&quot;55[11, 6, 2]&quot;&quot;&quot; Dynamic 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748from collections import defaultdictfrom functools import wrapsfrom icecream import icoriginal_price = [1,5,8,9,10,17,17,20,24,30,33]price = defaultdict(int)for i, p in enumerate(original_price): price[i+1] = p def memo(func): cache = {} @wraps(func) def _wrap(n): if n in cache: result = cache[n] else: result = func(n) cache[n] = result return result return _wrap @memodef r(n): max_price, split_point = max( [(price[n],0)] + [(r(i) + r(n-i), i) for i in range(1, n)], key=lambda x: x[0] ) solution[n] = (split_point, n-split_point) return max_price def not_cut(split): return split == 0def parse_solution(target_length, revenue_solution): left, right = revenue_solution[target_length] if not_cut(left): return [right] return parse_solution(left, revenue_solution) + parse_solution(right, revenue_solution) solution = {}r(50)ic(parse_solution(20,solution))ic(parse_solution(19,solution))ic(parse_solution(27,solution))# out&quot;&quot;&quot;ic| parse_solution(20,solution): [10, 10]ic| parse_solution(19,solution): [2, 6, 11]ic| parse_solution(27,solution): [6, 10, 11][6, 10, 11]&quot;&quot;&quot; Gradient descent 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npimport matplotlib.pyplot as pltimport randomfrom icecream import icdef func(x): return 10 * x**2 + 32*x + 9def gradient(x): return 20 *x + 32 x = np.linspace(-10, 10)steps = []x_star = random.choice(x)alpha = 1e-3for i in range(100): x_star = x_star + -1*gradient(x_star)*alpha steps.append(x_star) ic(x_star, func(x_star))fig, ax = plt.subplots()ax.plot(x, func(x))&quot;&quot;&quot;ic| x_star: 9.368, func(x_star): 1186.3702400000002ic| x_star: 9.14864, func(x_star): 1138.732618496show more (open the raw output data in a text editor) ...ic| x_star: -0.1157435825983131, func(x_star): 5.430171125980905[&lt;matplotlib.lines.Line2D at 0x7fd6d19545d0&gt;]&quot;&quot;&quot;for i, s in enumerate(steps): ax.annotate(str(i+1), (s, func(s))) plt.show() k-means-finding-centers K-means 123456789101112131415161718192021222324252627282930313233343536373839404142from pylab import mplmpl.rcParams['font.sans-serif'] = ['FangSong'] # Specify the default fontmpl.rcParams['axes.unicode_minus'] = False # Solve the problem that the minus sign'-' is displayed as a square in the saved imagecoordination_source = &quot;&quot;&quot;{name:'兰州', geoCoord:[103.73, 36.03]},{name:'嘉峪关', geoCoord:[98.17, 39.47]},{name:'西宁', geoCoord:[101.74, 36.56]},{name:'成都', geoCoord:[104.06, 30.67]},{name:'石家庄', geoCoord:[114.48, 38.03]},{name:'拉萨', geoCoord:[102.73, 25.04]},{name:'贵阳', geoCoord:[106.71, 26.57]},{name:'武汉', geoCoord:[114.31, 30.52]},{name:'郑州', geoCoord:[113.65, 34.76]},{name:'济南', geoCoord:[117, 36.65]},{name:'南京', geoCoord:[118.78, 32.04]},{name:'合肥', geoCoord:[117.27, 31.86]},{name:'杭州', geoCoord:[120.19, 30.26]},{name:'南昌', geoCoord:[115.89, 28.68]},{name:'福州', geoCoord:[119.3, 26.08]},{name:'广州', geoCoord:[113.23, 23.16]},{name:'长沙', geoCoord:[113, 28.21]},//{name:'海口', geoCoord:[110.35, 20.02]},{name:'沈阳', geoCoord:[123.38, 41.8]},{name:'长春', geoCoord:[125.35, 43.88]},{name:'哈尔滨', geoCoord:[126.63, 45.75]},{name:'太原', geoCoord:[112.53, 37.87]},{name:'西安', geoCoord:[108.95, 34.27]},//{name:'台湾', geoCoord:[121.30, 25.03]},{name:'北京', geoCoord:[116.46, 39.92]},{name:'上海', geoCoord:[121.48, 31.22]},{name:'重庆', geoCoord:[106.54, 29.59]},{name:'天津', geoCoord:[117.2, 39.13]},{name:'呼和浩特', geoCoord:[111.65, 40.82]},{name:'南宁', geoCoord:[108.33, 22.84]},//{name:'西藏', geoCoord:[91.11, 29.97]},{name:'银川', geoCoord:[106.27, 38.47]},{name:'乌鲁木齐', geoCoord:[87.68, 43.77]},{name:'香港', geoCoord:[114.17, 22.28]},{name:'澳门', geoCoord:[113.54, 22.19]}&quot;&quot;&quot; Feacutre Extractor 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768city_location = { '香港': (114.17, 22.28)}test_string = &quot;{name:'兰州', geoCoord:[103.73, 36.03]},&quot;import repattern = re.compile(r&quot;name:'(\\w+)',\\s+geoCoord:\\[(\\d+.\\d+),\\s(\\d+.\\d+)\\]&quot;)for line in coordination_source.split('\\n'): city_info = pattern.findall(line) if not city_info: continue # following: we find the city info city, long, lat = city_info[0] long, lat = float(long), float(lat) city_location[city] = (long, lat)city_location# output&quot;&quot;&quot;{'香港': (114.17, 22.28), '兰州': (103.73, 36.03),show more (open the raw output data in a text editor) ... '澳门': (113.54, 22.19)}&quot;&quot;&quot;import mathdef geo_distance(origin, destination): &quot;&quot;&quot; Calculate the Haversine distance. Parameters ---------- origin : tuple of float (lat, long) destination : tuple of float (lat, long) Returns ------- distance_in_km : float Examples -------- &gt;&gt;&gt; origin = (48.1372, 11.5756) # Munich &gt;&gt;&gt; destination = (52.5186, 13.4083) # Berlin &gt;&gt;&gt; round(distance(origin, destination), 1) 504.2 &quot;&quot;&quot; lon1, lat1 = origin lon2, lat2 = destination radius = 6371 # km dlat = math.radians(lat2 - lat1) dlon = math.radians(lon2 - lon1) a = (math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon / 2) * math.sin(dlon / 2)) c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)) d = radius * c return d Vector Distances 余弦距离 Cosine Distance 欧几里得距离 Euclidean Distance 曼哈顿距离 Manhattan distance or Manhattan length 12345678910111213import matplotlib.pyplot as pltimport networkx as nximport warningswarnings.filterwarnings('ignore')%matplotlib inline# set plt, show chineseplt.rcParams['font.sans-serif'] = ['Arial Unicode MS']plt.rcParams['axes.unicode_minus'] = Falsecity_graph = nx.Graph()city_graph.add_nodes_from(list(city_location.keys()))nx.draw(city_graph, city_location, with_labels=True, node_size=30) K-means: Initial k random centers 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394k = 10import randomall_x = []all_y = []for _, location in city_location.items(): x, y = location all_x.append(x) all_y.append(y)def get_random_center(all_x, all_y): r_x = random.uniform(min(all_x), max(all_x)) r_y = random.uniform(min(all_y), max(all_y)) return r_x, r_yget_random_center(all_x, all_y)# output&quot;&quot;&quot;(93.61182991130997, 37.01816228131414)&quot;&quot;&quot;K = 5centers = {'{}'.format(i+1): get_random_center(all_x, all_y) for i in range(K)}from collections import defaultdictcloset_points = defaultdict(list)for x, y, in zip(all_x, all_y): closet_c, closet_dis = min([(k, geo_distance((x, y), centers[k])) for k in centers], key=lambda t: t[1]) closet_points[closet_c].append([x, y])import numpy as npdef iterate_once(centers, closet_points, threshold=5): have_changed = False for c in closet_points: former_center = centers[c] neighbors = closet_points[c] neighbors_center = np.mean(neighbors, axis=0) if geo_distance(neighbors_center, former_center) &gt; threshold: centers[c] = neighbors_center have_changed = True else: pass ## keep former center return centers, have_changeddef kmeans(Xs, k, threshold=5): all_x = Xs[:, 0] all_y = Xs[:, 1] K = k centers = {'{}'.format(i+1): get_random_center(all_x, all_y) for i in range(K)} changed = True while changed: closet_points = defaultdict(list) for x, y, in zip(all_x, all_y): closet_c, closet_dis = min([(k, geo_distance((x, y), centers[k])) for k in centers], key=lambda t: t[1]) closet_points[closet_c].append([x, y]) centers, changed = iterate_once(centers, closet_points, threshold) print('iteration') return centerskmeans(np.array(list(city_location.values())), k=5, threshold=5)# output&quot;&quot;&quot;iterationiterationiterationiterationiteration{'1': array([99.518, 38.86 ]), '2': array([117.833, 39.861]), '3': array([91.11, 29.97]), '4': array([106.81, 27. ]), '5': array([116.87166667, 27.6275 ])}&quot;&quot;&quot;plt.scatter(all_x, all_y)plt.scatter(*zip(*centers.values())) 12for c, points in closet_points.items(): plt.scatter(*zip(*points)) 1234567891011121314151617181920212223city_location_with_station = { '能源站-{}'.format(i): position for i, position in centers.items()}city_location_with_station# output&quot;&quot;&quot;{'能源站-1': (108.82946246581274, 26.05763939719317), '能源站-2': (97.96769355736322, 22.166113183141032), '能源站-3': (114.05390380408154, 38.7698708467224), '能源站-4': (118.49242085311417, 28.665716162786204), '能源站-5': (125.08287617496866, 25.55784683330647)}&quot;&quot;&quot;def draw_cities(citise, color=None): city_graph = nx.Graph() city_graph.add_nodes_from(list(citise.keys())) nx.draw(city_graph, citise, node_color=color, with_labels=True, node_size=30)%matplotlib inlineplt.figure(1,figsize=(12,12)) draw_cities(city_location_with_station, color='green')draw_cities(city_location, color='red') About the dataset This contains data of news headlines published over a period of 15 years. From the reputable Australian news source ABC (Australian Broadcasting Corp.) Site: http://www.abc.net.au/ Prepared by Rohit Kulkarni 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as np import pandas as pd import matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.feature_extraction import textfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.cluster import KMeansfrom nltk.tokenize import RegexpTokenizerfrom nltk.stem.snowball import SnowballStemmerimport warningswarnings.filterwarnings('ignore')%matplotlib inlinedata = pd.read_csv(&quot;./data/abcnews-date-text.csv&quot;,error_bad_lines=False,usecols =[&quot;headline_text&quot;])data.head()# output&quot;&quot;&quot;headline_text0 aba decides against community broadcasting lic...1 act fire witnesses must be aware of defamation2 a g calls for infrastructure protection summit3 air nz staff in aust strike for pay rise4 air nz strike to affect australian travellers&quot;&quot;&quot;data.to_csv('abcnews.csv', index=False, encoding='utf8')data.info()# output&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 1103665 entries, 0 to 1103664Data columns (total 1 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 headline_text 1103665 non-null objectdtypes: object(1)memory usage: 8.4+ MB&quot;&quot;&quot; Deleting dupliate headlines(if any) 12data[data['headline_text'].duplicated(keep=False)].sort_values('headline_text').head(8)data = data.drop_duplicates('headline_text') NLP Preparing data for vectorizaion However, when doing natural language processing, words must be converted into vectors that machine learning algorithms can make use of. If your goal is to do machine learning on text data, like movie reviews or tweets or anything else, you need to convert the text data into numbers. This process is sometimes referred to as “embedding” or “vectorization”. In terms of vectorization, it is important to remember that it isn’t merely turning a single word into a single number. While words can be transformed into numbers, an entire document can be translated into a vector. Not only can a vector have more than one dimension, but with text data vectors are usually high-dimensional. This is because each dimension of your feature data will correspond to a word, and the language in the documents you are examining will have thousands of words. TF-IDF In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf. Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification. One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model. 123456789101112131415punc = ['.', ',', '&quot;', &quot;'&quot;, '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',&quot;%&quot;]stop_words = text.ENGLISH_STOP_WORDS.union(punc)desc = data['headline_text'].valuesvectorizer = TfidfVectorizer(stop_words = stop_words)X = vectorizer.fit_transform(desc)word_features = vectorizer.get_feature_names()print(len(word_features))print(word_features[5000:5100])# output&quot;&quot;&quot;96397['abyss', 'ac', 'aca', 'acacia', 'acacias', 'acadamy', 'academia', 'academic', 'academics', 'academies', 'academy', 'academys', 'acai', 'acapulco', 'acars', 'acason', 'acasuso', 'acb', 'acbf', 'acc', 'acca', 'accan', 'accc', 'acccc', 'acccs', 'acccused', 'acce', 'accedes', 'accelerant', 'accelerants', 'accelerate', 'accelerated', 'accelerates', 'accelerating', 'acceleration', 'accelerator', 'accen', 'accent', 'accents', 'accentuate', 'accentuates', 'accentuating', 'accenture', 'accept', 'acceptability', 'acceptable', 'acceptably', 'acceptance', 'acceptances', 'accepted', 'accepting', 'acceptor', 'acceptors', 'accepts', 'accerate', 'acces', 'access', 'accessary', 'accessed', 'accesses', 'accessibility', 'accessible', 'accessing', 'accessories', 'accessory', 'accesss', 'acci', 'accid', 'accide', 'acciden', 'accidenatlly', 'accidenbt', 'accident', 'accidental', 'accidentally', 'accidently', 'accidents', 'acciona', 'accis', 'acclaim', 'acclaimed', 'acclamation', 'acclimatise', 'acco', 'accolade', 'accolades', 'accom', 'accomm', 'accommoda', 'accommodate', 'accommodated', 'accommodates', 'accommodating', 'accommodation', 'accomo', 'accomodation', 'accomommodation', 'accompanied', 'accompanies', 'accompaniment']&quot;&quot;&quot; Stemming Stemming is the process of reducing a word into its stem, i.e. its root form. The root form is not necessarily a word by itself, but it can be used to generate words by concatenating the right suffix. For example, the words fish, fishes and fishing all stem into fish, which is a correct word. On the other side, the words study, studies and studying stems into studi, which is not an English word. Tokenizing Tokenization is breaking the sentence into words and punctuation, 12345stemmer = SnowballStemmer('english')tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')def tokenize(text): return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())] Vectorization with stop words(words irrelevant to the model), stemming and tokenizing 123456789101112131415vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)X2 = vectorizer2.fit_transform(desc)word_features2 = vectorizer2.get_feature_names()print(len(word_features2))print(word_features2[:50]) # output&quot;&quot;&quot;65232[&quot;'a&quot;, &quot;'i&quot;, &quot;'s&quot;, &quot;'t&quot;, 'aa', 'aaa', 'aaahhh', 'aac', 'aacc', 'aaco', 'aacta', 'aad', 'aadmi', 'aag', 'aagaard', 'aagard', 'aah', 'aalto', 'aam', 'aamer', 'aami', 'aamodt', 'aandahl', 'aant', 'aap', 'aapa', 'aapt', 'aar', 'aaradhna', 'aardman', 'aardvark', 'aargau', 'aaron', 'aaronpaul', 'aarwun', 'aat', 'ab', 'aba', 'abaaoud', 'ababa', 'aback', 'abadi', 'abadon', 'abal', 'abalon', 'abalonv', 'abama', 'abandon', 'abandond', 'abandong']&quot;&quot;&quot;vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)X3 = vectorizer3.fit_transform(desc)words = vectorizer3.get_feature_names() For this, we will use k-means clustering algorithm. ### K-means clustering (Source Wikipedia) Elbow method to select number of clusters This method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified. Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance. Basically, number of clusters = the x-axis value of the point that is the corner of the \"elbow\"(the plot looks often looks like an elbow) 123456789101112from sklearn.cluster import KMeanswcss = []for i in range(1,11): kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0) kmeans.fit(X3) wcss.append(kmeans.inertia_)plt.plot(range(1,11),wcss)plt.title('The Elbow Method')plt.xlabel('Number of clusters')plt.ylabel('WCSS')plt.savefig('elbow.png')plt.show() As more than one elbows have been generated, I will have to select right amount of clusters by trial and error. So, I will showcase the results of different amount of clusters to find out the right amount of clusters. 123456print(words[250:300])# output&quot;&quot;&quot;['decis', 'declar', 'defenc', 'defend', 'delay', 'deliv', 'demand', 'deni', 'despit', 'destroy', 'detent', 'develop', 'die', 'director', 'disabl', 'disast', 'discuss', 'diseas', 'dismiss', 'disput', 'doctor', 'dog', 'dollar', 'domest', 'donald', 'donat', 'doubl', 'doubt', 'draw', 'dri', 'drink', 'drive', 'driver', 'drop', 'drought', 'drown', 'drug', 'drum', 'dump', 'dure', 'e', 'eagl', 'earli', 'eas', 'east', 'econom', 'economi', 'edg', 'educ', 'effort']&quot;&quot;&quot; 3 Clusters 12345678910111213kmeans = KMeans(n_clusters = 3, n_init = 20, n_jobs = 1) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)kmeans.fit(X3)# We look at 3 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid)) # output&quot;&quot;&quot;0 : new, say, plan, win, council, govt, australia, report, kill, fund, urg, court, warn, water, australian, nsw, open, chang, year, qld, interview, wa, death, face, crash1 : polic, investig, probe, man, search, offic, hunt, miss, arrest, death, car, shoot, drug, seek, attack, assault, say, murder, crash, charg, driver, suspect, fatal, raid, station2 : man, charg, murder, court, face, jail, assault, stab, die, death, drug, guilti, child, sex, accus, attack, woman, crash, arrest, car, kill, miss, sydney, alleg, plead&quot;&quot;&quot; 5 Clusters 123456789101112131415kmeans = KMeans(n_clusters = 5, n_init = 20, n_jobs = 1)kmeans.fit(X3)# We look at 5 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))# output&quot;&quot;&quot;0 : man, plan, charg, court, govt, australia, face, murder, accus, jail, assault, stab, urg, drug, death, attack, child, sex, die, woman, guilti, say, alleg, told, car1 : new, zealand, law, year, plan, open, polic, home, hospit, centr, deal, set, hope, australia, look, appoint, announc, chief, say, south, minist, govt, rule, servic, welcom2 : say, win, kill, report, australian, warn, interview, open, water, fund, nsw, crash, death, urg, year, chang, wa, sydney, claim, qld, hit, attack, world, set, health3 : council, plan, consid, fund, rate, urg, seek, new, merger, water, land, develop, reject, say, mayor, vote, chang, elect, rise, meet, park, push, want, govt, approv4 : polic, investig, man, probe, search, offic, hunt, miss, arrest, death, car, charg, shoot, drug, seek, attack, assault, murder, crash, say, driver, fatal, suspect, raid, woman&quot;&quot;&quot; 6 Clusters 12345678910111213141516kmeans = KMeans(n_clusters = 6, n_init = 20, n_jobs = 1)kmeans.fit(X3)# We look at 6 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid)) # output&quot;&quot;&quot;0 : council, govt, australia, report, warn, urg, fund, australian, water, nsw, chang, qld, wa, health, elect, rural, countri, hour, sa, boost, climat, govern, servic, south, consid1 : man, charg, murder, court, face, jail, assault, stab, die, death, drug, guilti, child, sex, accus, attack, woman, crash, arrest, car, kill, miss, sydney, plead, alleg2 : polic, investig, probe, man, search, offic, hunt, miss, arrest, death, car, shoot, drug, seek, attack, crash, assault, murder, charg, driver, say, fatal, suspect, raid, warn3 : win, kill, court, interview, crash, open, death, sydney, face, year, claim, hit, attack, world, set, final, day, hous, die, home, jail, talk, return, cup, hospit4 : new, zealand, law, year, plan, open, council, polic, home, hospit, centr, deal, set, hope, australia, appoint, look, announc, chief, say, govt, south, minist, mayor, welcom5 : say, plan, council, govt, water, need, group, chang, labor, minist, govern, opposit, public, mp, health, union, green, hous, develop, resid, report, expert, cut, australia, mayor&quot;&quot;&quot; 8 Clusters 123456789101112131415161718kmeans = KMeans(n_clusters = 8, n_init = 20, n_jobs = 1)kmeans.fit(X3)# Finally, we look at 8 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid)) # output&quot;&quot;&quot;0 : polic, say, man, miss, arrest, jail, investig, car, search, murder, attack, crash, kill, probe, die, hunt, shoot, assault, offic, drug, stab, accus, fatal, guilti, bodi1 : death, hous, polic, toll, investig, man, probe, inquest, rise, woman, coron, blaze, price, public, white, babi, sentenc, famili, road, spark, jail, prompt, blame, custodi, report2 : plan, council, govt, water, new, say, develop, hous, group, chang, unveil, reject, park, urg, centr, public, expans, green, resid, health, reveal, labor, govern, opposit, power3 : court, face, man, accus, told, hear, murder, high, case, appear, rule, charg, alleg, appeal, drug, jail, woman, death, assault, order, sex, stab, challeng, teen, polic4 : australia, govt, kill, report, warn, australian, urg, fund, nsw, interview, water, open, crash, qld, chang, wa, year, day, claim, hit, attack, sydney, set, health, world5 : new, council, zealand, law, fund, year, consid, water, urg, open, say, seek, rate, centr, mayor, govt, elect, look, develop, land, deal, hope, set, push, home6 : win, award, cup, titl, open, gold, stage, world, final, tour, elect, australia, lead, seri, aussi, claim, second, australian, big, england, grand, m, battl, race, record7 : charg, man, murder, face, assault, drug, polic, child, sex, woman, teen, death, stab, drop, alleg, attack, rape, men, guilti, shoot, bail, sydney, fatal, driver, yo&quot;&quot;&quot; Because even I didn't know what kind of clusters would be generated, I will describe them in comments. Other discussions 1234567891011121314151617181920212223242526272829303132333435363738394041import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom nltk.corpus import stopwordsfrom sklearn.feature_extraction.text import CountVectorizerimport gensimfrom collections import Counterimport stringfrom nltk.stem import WordNetLemmatizer, PorterStemmerfrom nltk.tokenize import word_tokenizeimport pyLDAvis.gensim_modelsfrom wordcloud import WordCloud, STOPWORDSfrom textblob import TextBlobfrom spacy import displacyimport nltkimport warningswarnings.filterwarnings('ignore')# set pltplt.rcParams['font.sans-serif'] = ['Arial Unicode MS']plt.rcParams.update({'font.size': 12})plt.rcParams.update({'figure.figsize': [16, 12]})# plt.figure(figsize = [20, 20])plt.style.use('seaborn-whitegrid')df = pd.read_csv('../data/abcnews-date-text.csv', nrows = 10000)df.head()# output&quot;&quot;&quot;publish_date headline_text0 20030219 aba decides against community broadcasting lic...1 20030219 act fire witnesses must be aware of defamation2 20030219 a g calls for infrastructure protection summit3 20030219 air nz staff in aust strike for pay rise4 20030219 air nz strike to affect australian travellers&quot;&quot;&quot; The data set contains only two columns, the release date and the news title. For simplicity, I will explore the first 10,000 rows in this dataset. Since the titles are sorted by publish_date, they are actually two months from February 19, 2003 to April 7, 2003. Number of characters present in each sentence Visualization of text statistics is a simple but insightful technique. They include: Word frequency analysis, sentence length analysis, average word length analysis, etc. These really help to explore the basic characteristics of text data. For this, we will mainly use histograms (continuous data) and bar graphs (categorical data). First, let me look at the number of characters in each sentence. This can give us a rough idea of the length of news headlines. 1df['headline_text'].str.len().hist() number of words appearing in each news headline The histogram shows that the range of news headlines is 10 to 70 characters, usually between 25 and 55 characters. Now, we will continue to explore the data verbatim. Let's plot the number of words that appear in each news headline. 1df['headline_text'].str.split().map(lambda x: len(x)).hist() Analysing word length Obviously, the number of words in news headlines is in the range of 2 to 12, and most of them are between 5 and 7. Next, let's check the average word length in each sentence. 1df['headline_text'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x)).hist() The average word length is between 3 and 9, and the most common length is 5. Does this mean that people use very short words in news headlines? Let us find out. One reason that may not be the case is stop words. Stop words are the most commonly used words in any language (such as \"the\", \"a\", \"an\", etc.). Since the length of these words may be small, these words may cause the above graphics to be skewed to the left. Analyzing the number and types of stop words can give us some in-depth understanding of the data. To get a corpus containing stop words, you can use the nltk library. Nltk contains stop words from multiple languages. Since we only deal with English news, I will filter English stop words from the corpus. Analysing stopwords 1234567891011121314151617181920212223242526272829# Fetch stopwordsimport nltknltk.download('stopwords')stop=set(stopwords.words('english'))# output&quot;&quot;&quot;[nltk_data] Downloading package stopwords to /Users/xx/nltk_data...[nltk_data] Package stopwords is already up-to-date!&quot;&quot;&quot;# Create corpuscorpus=[]new= df['headline_text'].str.split()new=new.values.tolist()corpus=[word for i in new for word in i]from collections import defaultdictdic=defaultdict(int)for word in corpus: if word in stop: dic[word]+=1 # Plot top stopwordstop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] x,y=zip(*top)plt.bar(x,y) Draw popular stop words Most common words We can clearly see that in the news headlines, stop words such as \"to\", \"in\" and \"for\" dominate. So now that we know which stop words appear frequently in our text, let's check which words other than these stop words appear frequently. We will use the counter function in the collection library to count the occurrence of each word and store it in a list of tuples. This is a very useful feature when we are dealing with word-level analysis in natural language processing. 12345678910counter=Counter(corpus)most=counter.most_common()x, y=[], []for word,count in most[:40]: if (word not in stop): x.append(word) y.append(count) sns.barplot(x=y,y=x) Wow! In the past 15 years, \"America\", \"Iraq\" and \"War\" have dominated the headlines. \"We\" here may mean the United States or us (you and me). We are not a stop word, but when we look at the other words in the picture, they are all related to the United States-the Iraq War and \"we\" here may mean the United States. Ngram analysis Ngram is a continuous sequence of n words. For example, \"Riverbank\", \"Three Musketeers\" and so on. If the number of words is two, it is called a double word. For 3 characters, it is called a trigram, and so on. Viewing the most common n-grams can give you a better understanding of the context in which the word is used. Bigram analysis To build our vocabulary, we will use Countvectorizer. Countvectorizer is a simple method for labeling, vectorizing and representing corpora in an appropriate form. Can be found in sklearn.feature_engineering.text Therefore, we will analyze the top news in all news headlines. 123456789101112def get_top_ngram(corpus, n=None): vec = CountVectorizer(ngram_range=(n, n)).fit(corpus) bag_of_words = vec.transform(corpus) sum_words = bag_of_words.sum(axis=0) words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) return words_freq[:10]top_n_bigrams=get_top_ngram(df['headline_text'],2)[:10]x,y=map(list,zip(*top_n_bigrams))sns.barplot(x=y,y=x) Trigram analysis We can observe that dualisms such as \"anti-war\" and \"killed\" related to war dominate the headlines. How about triples? 123top_tri_grams=get_top_ngram(df['headline_text'],n=3)x,y=map(list,zip(*top_tri_grams))sns.barplot(x=y,y=x) We can see that many of these hexagrams are a combination of \"face the court\" and \"anti-war protest.\" This means that we should spend some effort on data cleaning to see if we can combine these synonyms into a clean token. Topic modelling Use pyLDAvis for topic modeling exploration Topic modeling is the process of using unsupervised learning techniques to extract the main topics that appear in the document set. Latent Dirichlet Allocation (LDA) is an easy-to-use and efficient topic modeling model. Each document is represented by a topic distribution, and each topic is represented by a word distribution. Once the documents are classified into topics, you can delve into the data for each topic or topic group. But before entering topic modeling, we must do some preprocessing of the data. we will: Tokenization: The process of converting sentences into tokens or word lists. remove stopwordslemmatize: Reduce the deformed form of each word to a common base or root. Convert to word bag: word bag is a dictionary where the key is the word (or ngram/tokens) and the value is the number of times each word appears in the corpus. With NLTK, you can easily tokenize and formalize: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import nltknltk.download('punkt')nltk.download('wordnet')# output&quot;&quot;&quot;[nltk_data] Downloading package punkt to /Users/xx/nltk_data...[nltk_data] Package punkt is already up-to-date![nltk_data] Downloading package wordnet to /Users/xx/nltk_data...[nltk_data] Unzipping corpora/wordnet.zip.True&quot;&quot;&quot;def preprocess_news(df): corpus=[] stem=PorterStemmer() lem=WordNetLemmatizer() for news in df['headline_text']: words=[w for w in word_tokenize(news) if (w not in stop)] words=[lem.lemmatize(w) for w in words if len(w)&gt;2] corpus.append(words) return corpus corpus = preprocess_news(df)# Now, let's use gensim to create a bag of words modeldic=gensim.corpora.Dictionary(corpus)bow_corpus = [dic.doc2bow(doc) for doc in corpus]# We can finally create the LDA model:lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 4, id2word = dic, passes = 10, workers = 2)lda_model.show_topics()# output&quot;&quot;&quot;[(0, '0.010*&quot;say&quot; + 0.007*&quot;cup&quot; + 0.006*&quot;war&quot; + 0.005*&quot;world&quot; + 0.005*&quot;back&quot; + 0.005*&quot;plan&quot; + 0.005*&quot;green&quot; + 0.004*&quot;win&quot; + 0.004*&quot;woman&quot; + 0.004*&quot;new&quot;'), (1, '0.010*&quot;govt&quot; + 0.009*&quot;war&quot; + 0.009*&quot;new&quot; + 0.007*&quot;may&quot; + 0.005*&quot;sars&quot; + 0.005*&quot;call&quot; + 0.005*&quot;protest&quot; + 0.005*&quot;boost&quot; + 0.005*&quot;group&quot; + 0.004*&quot;hospital&quot;'), (2, '0.018*&quot;police&quot; + 0.015*&quot;baghdad&quot; + 0.014*&quot;man&quot; + 0.005*&quot;missing&quot; + 0.005*&quot;claim&quot; + 0.005*&quot;court&quot; + 0.005*&quot;australia&quot; + 0.004*&quot;move&quot; + 0.004*&quot;murder&quot; + 0.004*&quot;charged&quot;'), (3, '0.030*&quot;iraq&quot; + 0.015*&quot;war&quot; + 0.007*&quot;iraqi&quot; + 0.007*&quot;council&quot; + 0.006*&quot;troop&quot; + 0.005*&quot;killed&quot; + 0.004*&quot;crash&quot; + 0.004*&quot;soldier&quot; + 0.004*&quot;open&quot; + 0.004*&quot;say&quot;')]&quot;&quot;&quot; Theme 0 represents things related to the Iraq war and the police. Theme 3 shows Australia's involvement in the Iraq War. You can print all the topics and try to understand them, but there are tools that can help you run this data exploration more effectively. pyLDAvis is such a tool, it can interactively visualize the results of LDA. Visualize the topics 123pyLDAvis.enable_notebook()vis = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dic)vis On the left, the area of each circle represents the importance of the topic relative to the corpus. Because there are four themes, we have four circles. The distance between the center of the circle indicates the similarity between themes. Here you can see that Topic 3 and Topic 4 overlap, which indicates that the themes are more similar. On the right, the histogram of each topic shows the top 30 related words. For example, in topic 1, the most relevant words are \"police\", \"new\", \"may\", \"war\", etc. Therefore, in our case, we can see many war-related words and topics in the news headlines. Wordclouds Wordcloud is a great way to represent text data. The size and color of each word appearing in the word cloud indicate its frequency or importance. It is easy to create a wordcloud using python, but we need to provide data in the form of a corpus. 123456789101112131415161718192021stopwords = set(STOPWORDS)def show_wordcloud(data, title = None): wordcloud = WordCloud( background_color='white', stopwords=stopwords, max_words=100, max_font_size=30, scale=3, random_state=1 ) wordcloud=wordcloud.generate(str(data)) fig = plt.figure(1, figsize=(12, 12)) plt.axis('off') plt.imshow(wordcloud) plt.show() show_wordcloud(corpus) Similarly, you can see that terms related to war are highlighted, indicating that these words often appear in news headlines. There are many parameters that can be adjusted. Some of the most famous are: stopwords: stop a group of words appearing in the image. max_words: Indicates the maximum number of words to be displayed. max_font_size: Maximum font size. There are many other options to create beautiful word clouds. For more detailed information, you can refer to here. Text sentiment Sentiment analysis is a very common natural language processing task in which we determine whether the text is positive, negative or neutral. This is very useful for finding sentiments related to comments and comments, allowing us to gain some valuable insights from text data. There are many projects that can help you use python for sentiment analysis. I personally like TextBlob and Vader Sentiment. 1234567from textblob import TextBlobTextBlob('100 people killed in Iraq').sentiment# output&quot;&quot;&quot;Sentiment(polarity=-0.2, subjectivity=0.0)&quot;&quot;&quot; Textblob Textblob is a python library built on top of nltk. It has been around for a while and is very easy to use. The sentiment function of TextBlob returns two attributes: Polarity: It is a floating-point number in the range of [-1,1], where 1 means a positive statement and -1 means a negative statement. Subjectivity: refers to how personal opinions and feelings affect someone’s judgment. The subjectivity is expressed as a floating point value with a range of [0,1]. I will run this feature on news headlines. TextBlob claims that the text \"100 people killed in Iraq\" is negative, not a view or feeling, but a statement of fact. I think we can agree to TextBlob here. Now that we know how to calculate these sentiment scores, we can use histograms to visualize them and explore the data further. 123456def polarity(text): return TextBlob(text).sentiment.polaritydf['polarity_score']=df['headline_text'].\\ apply(lambda x : polarity(x))df['polarity_score'].hist() You will see that the polarity is mainly between 0.00 and 0.20. This shows that most news headlines are neutral. Let's categorize news as negative, positive, and neutral based on the scores for a more in-depth study. Postive , Negative or Neutral ? 1234567891011121314def sentiment(x): if x&lt;0: return 'neg' elif x==0: return 'neu' else: return 'pos' df['polarity']=df['polarity_score'].\\ map(lambda x: sentiment(x)) plt.bar(df.polarity.value_counts().index, df.polarity.value_counts()) Yes, 70% of news is neutral, only 18% of positive news and 11% of negative news. Let's look at the positive and negative headlines. 1234567891011df[df['polarity']=='neg']['headline_text'].head(5)# output&quot;&quot;&quot;7 aussie qualifier stosur wastes four memphis match23 carews freak goal leaves roma in ruins28 council chief executive fails to secure position34 dargo fire threat expected to rise40 direct anger at govt not soldiers crean urgesName: headline_text, dtype: object&quot;&quot;&quot; Vader The next library we are going to discuss is VADER. Vader is better at detecting negative emotions. It is very useful in the context of social media text sentiment analysis. The VADER or Valence Aware dictionary and sentiment reasoner is an open source sentiment analyzer pre-built library based on rules/dictionaries and is protected by the MIT license. The VADER sentiment analysis class returns a dictionary that contains the possibility that the text appears positive, negative, and neutral. Then, we can filter and select the emotion with the highest probability. We will use VADER to perform the same analysis and check if the difference is large. 1234567891011121314151617181920212223from nltk.sentiment.vader import SentimentIntensityAnalyzernltk.download('vader_lexicon')sid = SentimentIntensityAnalyzer()def get_vader_score(sent): # Polarity score returns dictionary ss = sid.polarity_scores(sent) #return ss return np.argmax(list(ss.values())[:-1]) &quot;&quot;&quot;[nltk_data] Downloading package vader_lexicon to[nltk_data] /Users/xx/nltk_data...&quot;&quot;&quot;df['polarity']=df['headline_text'].\\ map(lambda x: get_vader_score(x))polarity=df['polarity'].replace({0:'neg',1:'neu',2:'pos'})plt.bar(polarity.value_counts().index, polarity.value_counts()) Yes, the distribution is slightly different. There are even more headlines classified as neutral 85%, and the number of negative news headlines has increased (to 13%). Named Entity Recognition Named entity recognition is an information extraction method in which entities existing in the text are classified into predefined entity types, such as \"person\", \"location\", \"organization\" and so on. By using NER, we can gain insight into the entities that exist in a given text data set of entity types. Let us consider an example of a news article. In the above news, the named entity recognition model should be able to recognize Entities, such as RBI as an organization, Mumbai and India as Places, etc. There are three standard libraries for named entity recognition: Stanford Nell space NLTK I will use spaCy, which is an open source library for advanced natural language processing tasks. It is written in Cython and is known for its industrial applications. In addition to NER, spaCy also provides many other functions, such as pos mark, word to vector conversion, etc. SpaCy’s Named Entity Recognition has been published in OntoNotes 5 has been trained on the corpus and supports the following entity types There are three kinds of pre-trained models for English in SpaCy. I will use en_core_web_sm to complete our task, but you can try other models. To use it, we must first download it: 12345678910111213141516171819202122232425# !python -m spacy download en_core_web_sm# Now we can initialize the language model:import spacyfrom spacy import displacyimport en_core_web_smnlp = en_core_web_sm.load()# nlp = spacy.load(&quot;en_core_web_sm&quot;)# One of the advantages of Spacy is that we only need to apply the nlp function once, and the entire background pipeline will return the objects we needdoc=nlp('India and Iran have agreed to boost the economic \\viability of the strategic Chabahar port through various measures, \\including larger subsidies to merchant shipping firms using the facility, \\people familiar with the development said on Thursday.')[(x.text,x.label_) for x in doc.ents] &quot;&quot;&quot;[('India', 'GPE'), ('Iran', 'GPE'), ('Chabahar', 'GPE'), ('Thursday', 'DATE')]&quot;&quot;&quot; We can see that India and Iran are confirmed as geographic locations (GPE), Chabahar is confirmed as a person, and Thursday is confirmed as a date. We can also use the display module in spaCy to visualize the output. 123from spacy import displacydisplacy.render(doc, style='ent') This can make sentences with recognized entities look very neat, and each entity type is marked with a different color. Now that we know how to perform NER, we can further explore the data by performing various visualizations on the named entities extracted from the data set. First, we will run named entity recognition on news headlines and store entity types. NER Analysis 123456789101112def ner(text): doc=nlp(text) return [X.label_ for X in doc.ents] ent=df['headline_text'].apply(lambda x : ner(x))ent=[x for sub in ent for x in sub]counter=Counter(ent)count=counter.most_common()# Now, we can visualize the entity frequency:x,y=map(list,zip(*count))sns.barplot(x=y,y=x) Now we can see that GPE and ORG dominate the headlines, followed by the PERSON entity. We can also visualize the most common tokens for each entity. Let's check which places appear the most in news headlines. Most common GPE 12345678910def ner(text,ent=&quot;GPE&quot;): doc=nlp(text) return [X.text for X in doc.ents if X.label_ == ent] gpe=df['headline_text'].apply(lambda x: ner(x,&quot;GPE&quot;))gpe=[i for x in gpe for i in x]counter=Counter(gpe)x,y=map(list,zip(*counter.most_common(10)))sns.barplot(y,x) I think we can confirm the fact that \"America\" means America in news headlines. Let's also find the most common names that appear on news headlines. Most common person 123456per=df['headline_text'].apply(lambda x: ner(x,&quot;PERSON&quot;))per=[i for x in per for i in x]counter=Counter(per)x,y=map(list,zip(*counter.most_common(10)))sns.barplot(y,x) Saddam Hussein and George Bush served as presidents of Iraq and the United States during the war. In addition, we can see that the model is far from perfect to classify \"vic govt\" or \"nsw govt\" as individuals rather than government agencies. Pos tagging Use nltk for all parts of speech markup, but there are other libraries that can do the job well (spaacy, textblob). 123456789101112import nltknltk.download('averaged_perceptron_tagger')sentence=&quot;The greatest comeback stories in 2019&quot;tokens=word_tokenize(sentence)nltk.pos_tag(tokens)# Notice:# You can also use the spacy.displacy module to visualize the sentence part of the speech and its dependency graph.doc = nlp('The greatest comeback stories in 2019')displacy.render(doc, style='dep', jupyter=True, options={'distance': 90}) We can observe various dependency labels here. For example, the DET tag indicates the relationship between the word \"the\" and the noun \"stories\". You can check the list of dependency labels and their meanings here. Okay, now that we know what a POS tag is, let's use it to explore the title data set. Analysing pos tags 1234567891011def pos(text): pos=nltk.pos_tag(word_tokenize(text)) pos=list(map(list,zip(*pos)))[1] return pos tags=df['headline_text'].apply(lambda x : pos(x))tags=[x for l in tags for x in l]counter=Counter(tags)x,y=list(map(list,zip(*counter.most_common(7))))sns.barplot(x=y,y=x) We can clearly see that nouns (NN) dominate in news headlines, followed by adjectives (JJ). This is typical for news reports, and for art forms, higher adjective (ADJ) frequencies may happen a lot. You can investigate this in more depth by investigating the most common singular nouns in news headlines. Let us find out. Nouns such as \"war\", \"Iraq\", and \"person\" dominate the news headlines. You can use the above functions to visualize and check other parts of the voice. Most common Nouns 123456789101112131415def get_adjs(text): adj=[] pos=nltk.pos_tag(word_tokenize(text)) for word,tag in pos: if tag=='NN': adj.append(word) return adjwords=df['headline_text'].apply(lambda x : get_adjs(x))words=[x for l in words for x in l]counter=Counter(words)x,y=list(map(list,zip(*counter.most_common(7))))sns.barplot(x=y,y=x) Dependency graph 12doc = nlp('She sells seashells by the seashore')displacy.render(doc, style='dep', jupyter=True, options={'distance': 90}) Text readability Textstat 12from textstat import flesch_reading_easedf['headline_text'].apply(lambda x : flesch_reading_ease(x)).hist() complex headlines? Almost all readability scores exceed 60. This means that an average of 11-year-old students can read and understand news headlines. Let's check all news headlines with a readability score below 5. 123456789101112131415x=[i for i in range(len(reading)) if reading[i]&lt;5] &quot;&quot;&quot;rror loading preloads:Failed to fetch dynamically imported module: https://file+.vscode-resource.vscode-webview.net/Users/xx/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/datascience-ui/errorRenderer/errorRenderer.js&quot;&quot;&quot;news.iloc[x]['headline_text'].head() &quot;&quot;&quot;Error loading preloads:Failed to fetch dynamically imported module: https://file+.vscode-resource.vscode-webview.net/Users/xx/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/datascience-ui/errorRenderer/errorRenderer.js&quot;&quot;&quot; Final thoughts In this article, we discussed and implemented various exploratory data analysis methods for text data. Some are common and little known, but all of them can be an excellent addition to your data exploration toolkit. Hope you will find some of them useful for your current and future projects. To make data exploration easier, I created a \"exploratory data analysis of natural language processing templates\", which you can use for your work. In addition, you may have seen that for each chart in this article, there is a code snippet to create it. Just click the button below the chart. Happy exploring! From: https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr","link":"/example_02/"},{"title":"Machine Learning Part-01","text":"Linear Regression Example Implement Linear Regression for Boston House Price Problem 123456789import randomimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsimport numpy as npfrom sklearn.datasets import load_bostonfrom matplotlib.animation import FuncAnimationimport re Part-01: Linear Regression 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200housing_price = load_boston()dataframe = pd.DataFrame(housing_price['data'])dataframe.columns = housing_price['feature_names']dataframe['price'] = housing_price['target']# sns.heatmap(dataframe.corr(), annot=True, fmt='.1f')# plt.show()print(dataframe.columns) &quot;&quot;&quot;Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'price'], dtype='object')&quot;&quot;&quot;rm = dataframe['RM']lst = dataframe['LSTAT']target = dataframe['price']def model(x, w, b): return np.dot(x, w.T) + bdef loss(yhat, y): return np.mean( (yhat - y) ** 2)def partial_w(x1, x2, y, yhat): return np.array([2 *np.mean((yhat - y) * x1), 2 * np.mean((yhat - y) * x2)])def partial_b(x1, x2, y, yhat): return 2 * np.mean((yhat - y))w = np.random.random_sample((1, 2))print(w)b = 0alpha = 1e-5epoch = 200history = []history_k_b_loss = [] &quot;&quot;&quot;[[0.76646144 0.3095512 ]]&quot;&quot;&quot;for e in range(epoch): losses = [] for batch in range(len(rm)): random_index = random.choice(range(len(rm))) x1, x2 = rm[random_index], lst[random_index] y = target[random_index] yhat = model(np.array([x1, x2]), w, b) loss_v = loss(yhat, y) w = w - partial_w(x1, x2, y, yhat) * alpha b = b - partial_b(x1, x2, y, yhat) * alpha losses.append(loss_v) history_k_b_loss.append((w, b, loss_v)) if batch % 100 == 0: print('Epoch: {}, Batch: {}, loss: {}'.format(e, batch, np.mean(losses))) history.append(np.mean(losses)) &quot;&quot;&quot;Epoch: 0, Batch: 0, loss: 151.86271856102778Epoch: 0, Batch: 100, loss: 263.5872813250959show more (open the raw output data in a text editor) ...Epoch: 199, Batch: 500, loss: 28.308274447364248&quot;&quot;&quot;````## Logstic Regression```pythonhousing_price = load_boston()dataframe = pd.DataFrame(housing_price['data'])dataframe.columns = housing_price['feature_names']dataframe['price'] = housing_price['target']rm = dataframe['RM']lst = dataframe['LSTAT']price = dataframe['price']print(np.percentile(price, 66)) &quot;&quot;&quot;23.53&quot;&quot;&quot;# plt.hist(target)# plt.show()dataframe['expensive'] = dataframe['price'].apply(lambda p: int(p &gt; np.percentile(price, 66)))expensive = dataframe['expensive']# print(dataframe.head())print(dataframe['expensive']) &quot;&quot;&quot;0 11 0 ..505 0Name: expensive, Length: 506, dtype: int64&quot;&quot;&quot;def logistic(x): return 1 / (1 + np.exp(-x))def model(x, w, b): return logistic(np.dot(x, w.T) + b)def loss(yhat, y): return -1 * np.sum(y*np.log(yhat) + (1 - y) * np.log(1 - yhat))def partial_w(x1, x2, y, yhat): return np.array([np.sum((yhat - y) * x1), np.sum((yhat - y) * x2)])def partial_b(x1, x2, y, yhat): return np.sum(yhat - y) w = np.random.random_sample((1, 2))print(w) &quot;&quot;&quot;[[0.69565948 0.90768813]]&quot;&quot;&quot;b = 0alpha = 1e-5epoch = 200history = []history_k_b_loss = []for e in range(epoch): losses = [] for batch in range(len(rm)): random_index = random.choice(range(len(rm))) x1, x2 = rm[random_index], lst[random_index] y = expensive[random_index] yhat = model(np.array([x1, x2]), w, b) loss_v = loss(yhat, y) w = w - partial_w(x1, x2, y, yhat) * alpha b = b - partial_b(x1, x2, y, yhat) * alpha losses.append(loss_v) history_k_b_loss.append((w, b, loss_v)) if batch % 100 == 0: print('Epoch: {}, Batch: {}, loss: {}'.format(e, batch, np.mean(losses))) history.append(np.mean(losses)) &quot;&quot;&quot;Epoch: 0, Batch: 0, loss: 3.14765267665445e-06Epoch: 0, Batch: 100, loss: 13.555508645878497show more (open the raw output data in a text editor) ...Epoch: 199, Batch: 500, loss: 0.31372698791846687&quot;&quot;&quot;predicated = [model(np.array([x1, x2]), w, b) for x1, x2 in zip(rm, lst)]true = expensivedef accuracy(y, yhat): return sum(1 if i == j else 0 for i, j in zip(y, yhat)) / len(y) print(accuracy(true, predicated)) &quot;&quot;&quot;0.0&quot;&quot;&quot; decision boundary Linear Regression: Regression is implemented, including the definition of linear functions, why use linear functions, the meaning of loss, the meaning of gradient descent, stochastic gradient descent Use Boston house price dataset. The data set of Beijing housing prices in 2020, why didn’t I use the data set of Beijing housing prices? Boston: room size, subway, highway, crime rate have a more obvious relationship, so it is easier to observe the relationship Beijing's housing prices:! Far and near! Room Condition ==》 School District! ! ! ! =&gt; Very expensive Haidian District 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import randomimport numpy as npimport pandas as pdfrom sklearn.datasets import load_bostondataset = load_boston()data = dataset['data']target = dataset['target']columns = dataset['feature_names']dataframe = pd.DataFrame(data)dataframe.columns = columnsdataframe['price'] = target# print(dataframe.corr()) # show the correlation of dataframe variables# correlation =&gt; If one value increases, it will cause another value to increase, and the correlation coefficient is closer to 1 if it increases in a certain proportion.# correlation =&gt; 0 means there is no relationship between the two# correlation =&gt; -1 One value increases, the other value must decrease, and the decrease is in equal proportion# sns.heatmap(dataframe.corr())# plt.show()# RM: The average number of bedrooms in the community# LSTAT: Percentage of low-income people aroundrm = dataframe['RM']lstat = dataframe['LSTAT']def linear(x, w, b): # vectorized model return np.dot(x, w.T) + bdef loss(yhat, y): # numpy broadcast numpy广播方法 return np.mean( (yhat - y) ** 2)def partial_w(x, y, yhat): return np.array([2 * np.mean((yhat - y) * x[0]), 2 * np.mean((yhat - y) * x[1])])def partial_b(x, y, yhat): return 2 * np.mean((yhat - y))def optimize(w, b, x, y, yhat, pw, pb, learning_rate): w = w + -1 * pw(x, y, yhat) * learning_rate b = b + -1 * pb(x, y, yhat) * learning_rate return w, b def train(model_to_be_train, target, loss, pw, pb): w = np.random.random_sample((1, 2)) # w normal b = np.random.random() # 0 深度学习的时候会和大家详细解释 learning_rate = 1e-5 epoch = 200 losses = [] for i in range(epoch): batch_loss = [] for batch in range(len(rm)): # batch training index = random.choice(range(len(rm))) rm_x, lstat_x = rm[index], lstat[index] x = np.array([rm_x, lstat_x]) y = target[index] yhat = model_to_be_train(x, w, b) loss_v = loss(yhat, y) batch_loss.append(loss_v) w, b = optimize(w, b, x, y, yhat, pw, pb, learning_rate) if batch % 100 == 0: print('Epoch: {} Batch: {}, loss: {}'.format(i, batch, loss_v)) losses.append(np.mean(batch_loss)) return model_to_be_train, w, b, losses if __name__ == &quot;__main__&quot;: import matplotlib.pyplot as plt target = dataframe['price'] model, w, b, losses = train(linear, target, loss, partial_w, partial_b) plt.plot(losses) predicate = model(np.array([19, 7]), w, b) print(predicate) plt.show() &quot;&quot;&quot;Epoch: 0 Batch: 0, loss: 165.0318036522631Epoch: 0 Batch: 100, loss: 1936.2111196826459show more (open the raw output data in a text editor) ...Epoch: 199 Batch: 500, loss: 0.024829543832110872[88.74340551]&quot;&quot;&quot; Logstic Regression Linear Regression: Regression is implemented, including the definition of linear functions, why use linear functions, the meaning of loss, the meaning of gradient descent, stochastic gradient descent Use Boston house price dataset. The data set of Beijing housing prices in 2020, why didn’t I use the data set of Beijing housing prices? Boston: room size, subway, highway, crime rate have a more obvious relationship, so it is easier to observe the relationship Beijing's housing prices:! Far and near! Room Condition ==》 School District! ! ! ! =&gt; Very expensive Haidian District Harder than deep learning: 1. compiler 2. programming language &amp; automata 3. computer graphic 4. complexity system 5. computing complexity 6. operating system 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130from sklearn.datasets import load_bostonimport pandas as pdimport numpy as npdataset = load_boston()data = dataset['data']target = dataset['target']columns = dataset['feature_names']dataframe = pd.DataFrame(data)dataframe.columns = columnsdataframe['price'] = target# print(dataframe.corr()) # show the correlation of dataframe variables# correlation =&gt; If one value increases, it will cause another value to increase, and the correlation coefficient is closer to 1 if it increases in a certain proportion.# correlation =&gt; 0 means there is no relationship between the two# correlation =&gt; -1 One value increases, the other value must decrease, and the decrease is in equal proportion# sns.heatmap(dataframe.corr())# plt.show()# RM: The average number of bedrooms in the community# LSTAT: Percentage of low-income people aroundrm = dataframe['RM']lstat = dataframe['LSTAT']price = dataframe['price']greater_then_most = np.percentile(price, 66)dataframe['expensive'] = dataframe['price'].apply(lambda p: int(p&gt; greater_then_most))target = dataframe['expensive']print(dataframe[:20]) &quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX \\0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 5 0.02985 0.0 2.18 0.0 0.458 6.430 58.7 6.0622 3.0 222.0 6 0.08829 12.5 7.87 0.0 0.524 6.012 66.6 5.5605 5.0 311.0 7 0.14455 12.5 7.87 0.0 0.524 6.172 96.1 5.9505 5.0 311.0 8 0.21124 12.5 7.87 0.0 0.524 5.631 100.0 6.0821 5.0 311.0 9 0.17004 12.5 7.87 0.0 0.524 6.004 85.9 6.5921 5.0 311.0 10 0.22489 12.5 7.87 0.0 0.524 6.377 94.3 6.3467 5.0 311.0 11 0.11747 12.5 7.87 0.0 0.524 6.009 82.9 6.2267 5.0 311.0 12 0.09378 12.5 7.87 0.0 0.524 5.889 39.0 5.4509 5.0 311.0 13 0.62976 0.0 8.14 0.0 0.538 5.949 61.8 4.7075 4.0 307.0 14 0.63796 0.0 8.14 0.0 0.538 6.096 84.5 4.4619 4.0 307.0 15 0.62739 0.0 8.14 0.0 0.538 5.834 56.5 4.4986 4.0 307.0 16 1.05393 0.0 8.14 0.0 0.538 5.935 29.3 4.4986 4.0 307.0 17 0.78420 0.0 8.14 0.0 0.538 5.990 81.7 4.2579 4.0 307.0 18 0.80271 0.0 8.14 0.0 0.538 5.456 36.6 3.7965 4.0 307.0 19 0.72580 0.0 8.14 0.0 0.538 5.727 69.5 3.7965 4.0 307.0 PTRATIO B LSTAT price expensive 0 15.3 396.90 4.98 24.0 1 1 17.8 396.90 9.14 21.6 0 2 17.8 392.83 4.03 34.7 1 3 18.7 394.63 2.94 33.4 1 4 18.7 396.90 5.33 36.2 1 5 18.7 394.12 5.21 28.7 1 6 15.2 395.60 12.43 22.9 0 7 15.2 396.90 19.15 27.1 1 8 15.2 386.63 29.93 16.5 0 9 15.2 386.71 17.10 18.9 0 10 15.2 392.52 20.45 15.0 0 11 15.2 396.90 13.27 18.9 0 12 15.2 390.50 15.71 21.7 0 13 21.0 396.90 8.26 20.4 0 14 21.0 380.02 10.26 18.2 0 15 21.0 395.62 8.47 19.9 0 16 21.0 386.85 6.58 23.1 0 17 21.0 386.75 14.67 17.5 0 18 21.0 288.99 11.69 20.2 0 19 21.0 390.95 11.28 18.2 0 &quot;&quot;&quot;def sigmoid(x): return 1 / (1 + np.exp(-x))def model(x, w, b): return sigmoid(np.dot(x, w.T) + b)def loss(yhat, y): return -np.sum(y*np.log(yhat) + (1 - y)*np.log(1 - yhat))def partial_w(x, y, yhat): return np.array([np.sum((yhat - y) * x[0]), np.sum((yhat - y) * x[1])])def partial_b(x, y, yhat): return np.sum((yhat - y)) model, w, b, losses = train(model, target,loss, partial_w, partial_b)random_test_indices = np.random.choice(range(len(rm)), size=100)decision_boundary = 0.5 &quot;&quot;&quot;Epoch: 0 Batch: 0, loss: 5.380792320433632Epoch: 0 Batch: 100, loss: 4.821708458450062show more (open the raw output data in a text editor) ...Epoch: 199 Batch: 500, loss: 0.052809537616594626&quot;&quot;&quot;for i in random_test_indices: x1, x2, y = rm[i], lstat[i], target[i] predicate = model(np.array([x1, x2]), w, b) predicate_label = int(predicate &gt; decision_boundary) print('RM: {}, LSTAT: {}, EXPENSIVE: {}, Predicated: {}'.format(x1, x2, y, predicate_label)) &quot;&quot;&quot;RM: 5.701, LSTAT: 18.35, EXPENSIVE: 0, Predicated: 0RM: 4.973, LSTAT: 12.64, EXPENSIVE: 0, Predicated: 0show more (open the raw output data in a text editor) ...RM: 6.678, LSTAT: 6.27, EXPENSIVE: 1, Predicated: 1&quot;&quot;&quot; One thing left is to check the accuracy of our model! ! How to measure the quality of the model: 1. accuracy precision recall f1, f2 score AUC-ROC curve Introduce a very very important concept: -&gt; over-fitting and under-fitting (over-fitting and under-fitting) The entire machine learning process is constantly adjusting over-fitting and under-fitting!","link":"/example_03/"},{"title":"Machine Learning Part-02","text":"Data Pre-processing Feature-Extractor Split Training, Test, Validation Build Model Gradient Descent Evaluation Predicat Analysis House Price Regression 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129## load datafrom sklearn.datasets import load_boston## ususlly will load in csvdata = load_boston()print(data['DESCR'])&quot;&quot;&quot;_boston_dataset:Boston house prices dataset---------------------------**Data Set Characteristics:** :Number of Instances: 506 show more (open the raw output data in a text editor) ...Morgan Kaufmann.&quot;&quot;&quot;import pandas as pdimport numpy as npdf = pd.DataFrame(data['data'])df.columns = data['feature_names']df[df['CHAS'] == 1]&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT142 3.32105 0.0 19.58 1.0 0.8710 5.403 100.0 1.3216 5.0 403.0 14.7 396.90 26.82... 1.1296 24.0 666.0 20.2 347.88 8.88&quot;&quot;&quot;## Pre-processingdf.std()&quot;&quot;&quot;CRIM 8.601545ZN 23.322453INDUS 6.860353CHAS 0.253994NOX 0.115878RM 0.702617AGE 28.148861DIS 2.105710RAD 8.707259TAX 168.537116PTRATIO 2.164946B 91.294864LSTAT 7.141062dtype: float64&quot;&quot;&quot;df['CHAS'] = df['CHAS'].astype('int')df['CHAS'] = df['CHAS'].astype('category')df['RAD'] = df['RAD'].astype('int')df['RAD'] = df['RAD'].astype('category')df&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98...505 0.04741 0.0 11.93 0 0.573 6.030 80.8 2.5050 1 273.0 21.0 396.90 7.88506 rows × 13 columns&quot;&quot;&quot;df['RAD']&quot;&quot;&quot;0 11 22 2...505 1Name: RAD, Length: 506, dtype: categoryCategories (9, int64): [1, 2, 3, 4, ..., 6, 7, 8, 24]&quot;&quot;&quot;from sklearn.preprocessing import OneHotEncoderonehoter = OneHotEncoder()chas_and_rad_vec = onehoter.fit_transform(df[['CHAS', 'RAD']])## Standarlizefrom sklearn.preprocessing import StandardScalerss = StandardScaler()df.shape&quot;&quot;&quot;(506, 13)&quot;&quot;&quot;real_vec = ss.fit_transform(df.drop(columns = ['CHAS', 'RAD']))chas_and_rad_vec[0].toarray()&quot;&quot;&quot;array([[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])&quot;&quot;&quot;import numpy as npnp.mean(real_vec, axis = 0)&quot;&quot;&quot;array([-1.12338772e-16, 7.89881994e-17, 2.10635198e-16, -1.96592852e-16, -1.08828186e-16, -1.47444639e-16, -8.42540793e-17, 0.00000000e+00, -4.21270397e-16, -7.44244367e-16, -3.08931624e-16])&quot;&quot;&quot;np.std(real_vec, axis = 0)&quot;&quot;&quot;array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])&quot;&quot;&quot;real_vec.shape&quot;&quot;&quot;(506, 11)&quot;&quot;&quot;chas_and_rad_vec.shape&quot;&quot;&quot;(506, 11)&quot;&quot;&quot;## Feature-ExtractorX = np.concatenate((real_vec, chas_and_rad_vec.toarray()), axis = 1)y = data['target']## Split Training, Test, Validationdef split_train_val_test(X, y, test_ratio = 0.2, val_ratio = 0.2): indices = np.random.choice(range(len(X)), size = len(X), replace=False) train_indices = indices[:int(len(X) * (1-test_ratio) * (1 - val_ratio))] val_indices = indices[int(len(X)*(1-test_ratio) * (1-val_ratio)): int(len(X) * (1-test_ratio))] test_indices = indices[int(len(X) * (1-test_ratio)):] return (X[train_indices], y[train_indices]), (X[val_indices], y[val_indices]), (X[test_indices], y[test_indices])(X_train, y_train), (X_val, y_val), (X_test, y_test) = split_train_val_test(X, y) sklearn.model_selection.train_test_split also could be used Build-Model 1234567from sklearn.linear_model import LinearRegressionregression = LinearRegression()regression.fit(X_train, y_train)&quot;&quot;&quot;LinearRegression()&quot;&quot;&quot; Question: If overfittiing or underfitting? Explain: Why validation set is more useful in deep learning Gradient Descent Evaluation 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152regression.score(X_train, y_train)&quot;&quot;&quot;0.7477980609064946&quot;&quot;&quot;regression.score(X_val, y_val)&quot;&quot;&quot;0.7611715890963341&quot;&quot;&quot;regression.score(X_test, y_test)&quot;&quot;&quot;0.711869928554872&quot;&quot;&quot;## Interpreterregression.coef_&quot;&quot;&quot;array([-1.04208922, 1.30263494, 0.29143618, -2.31827512, 2.40383155, 0.25013857, -3.55953868, -1.68823412, -2.37743843, 0.74411049, -3.79489254, -0.79143926, 0.79143926, -2.51995654, -2.20671004, 0.65594998, -0.31683083, -0.07929752, -2.15244627, -0.06686364, 1.93167854, 4.75447632])&quot;&quot;&quot;regression.intercept_&quot;&quot;&quot;22.070279554739386&quot;&quot;&quot;### PredictX_test[0]&quot;&quot;&quot;array([ 1.68404594, -0.48772236, 1.01599907, 1.07378711, 0.21279502, 1.11749449, -0.93188642, 1.53092646, 0.80657583, -3.61192313, 2.29842066, 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. ])&quot;&quot;&quot;regression.predict([X_test[0]])&quot;&quot;&quot;array([9.64589284])&quot;&quot;&quot;import matplotlib.pyplot as pltfor i in range(5): plt.scatter(X[:, 5], y) plt.scatter(X[:, 5], regression.predict(X))plt.show() 1234567891011121314151617import matplotlibmatplotlib.colors%matplotlib inlinedef show_predication_result(x, target): width = 3 fig,ax = plt.subplots(x.shape[1]//width + 1, width, figsize = (40,40)) for i in range(x.shape[1]): ix = np.unravel_index(i, ax.shape) plt.sca(ax[ix]) ax[ix].title.set_text('Feature-{}'.format(i)) plt.scatter(x[:, i], target) plt.scatter(x[:, i], regression.predict(x)) show_predication_result(X_train, y_train) 1show_predication_result(X_val, y_val) 1show_predication_result(X_test, y_test) Outliers Part-02 Logstic Regression Data Pre-processing Feature-Extractor Split Training, Test, Validation Build Model Gradient Descent Evaluation Predicat Analysis Pre-processing 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom struct import unpackdef loadmnist(imagefile, labelfile): # Open the images with gzip in read binary mode images = open(imagefile, 'rb') labels = open(labelfile, 'rb') # Get metadata for images images.read(4) # skip the magic_number number_of_images = images.read(4) number_of_images = unpack('&gt;I', number_of_images)[0] rows = images.read(4) rows = unpack('&gt;I', rows)[0] cols = images.read(4) cols = unpack('&gt;I', cols)[0] # Get metadata for labels labels.read(4) N = labels.read(4) N = unpack('&gt;I', N)[0] # Get data x = np.zeros((N, rows*cols), dtype = np.uint8) #Initialize numpy array y = np.zeros(N, dtype = np.uint8) # Initialize numpy array for i in range(N): for j in range(rows*cols): tmp_pixel = images.read(1) # Just a single byte tmp_pixel = unpack('&gt;B', tmp_pixel)[0] x[i][j] = tmp_pixel tmp_label = labels.read(1) y[i] = unpack('&gt;B', tmp_label)[0] images.close() labels.close() return (x, y) X_train, y_train = loadmnist('~/data/course_data/t10k-images-idx3-ubyte','~/data/course_data/t10k-labels-idx1-ubyte') X_test, y_test = loadmnist('~/data/course_data/train-images-idx3-ubyte','~/data/course_data/train-labels-idx1-ubyte') X_train.shape &quot;&quot;&quot; (10000, 784) &quot;&quot;&quot; X_test &quot;&quot;&quot; array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]], dtype=uint8) &quot;&quot;&quot; y_test &quot;&quot;&quot; array([5, 0, 4, ..., 5, 6, 8], dtype=uint8) &quot;&quot;&quot; plt.figure(figsize = (20, 4))for index, (image, label) in enumerate(zip(X_train[0:5], y_train[0:5])): plt.subplot(1, 5, index+1) plt.imshow(np.reshape(image, (28, 28))) plt.title('Traininng: %i\\n' % label, fontsize = 20) We only choose label with 0 and 6 1234567891011121314151617181920212223242526272829303132333435zero_train_indices = np.where(y_train == 0)one_train_indices = np.where(y_train == 6)train_indices = np.concatenate((zero_train_indices[0], one_train_indices[0]))zero_test_indices = np.where(y_test == 0)one_test_indices = np.where(y_test == 6)test_indices = np.concatenate((zero_test_indices[0], one_test_indices[0]))train_indices = np.random.choice(train_indices, size = len(train_indices), replace=False)test_indices = np.random.choice(test_indices, size = len(test_indices), replace=False)val_ratio= 0.2train_indices = train_indices[: int(len(train_indices) * (1 - val_ratio))]val_indices = train_indices[int(len(train_indices) * (1 - val_ratio)):]binary_x_train = X_train[train_indices]binary_x_test = X_test[test_indices]binary_x_val = X_train[val_indices]binary_y_train = y_train[train_indices]binary_y_test = y_test[test_indices]binary_y_val = y_train[val_indices]import randombinary_y_train&quot;&quot;&quot;array([6, 0, 0, ..., 6, 0, 0], dtype=uint8)&quot;&quot;&quot;plt.imshow(np.reshape(binary_x_train[1], (28,28)))plt.title('Training: %i\\n' % binary_y_train[1], fontsize =20)&quot;&quot;&quot;Text(0.5, 1.0, 'Training: 0\\n')&quot;&quot;&quot; 123456789101112131415from collections import CounterCounter(binary_y_train)&quot;&quot;&quot;Counter({6: 768, 0: 782})&quot;&quot;&quot;Counter(binary_y_test)&quot;&quot;&quot;Counter({6: 5918, 0: 5923})&quot;&quot;&quot;Counter(binary_y_val)&quot;&quot;&quot;Counter({0: 148, 6: 162})&quot;&quot;&quot; Build model 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from sklearn.linear_model import LogisticRegressionclf = LogisticRegression(random_state = 0, solver = 'lbfgs')# L-BFGS-B - Software for Large-scale Bound-constrained Optimizationimport warningswarnings.filterwarnings('ignore')clf.fit(binary_x_train, binary_y_train)&quot;&quot;&quot;LogisticRegression(random_state=0)&quot;&quot;&quot;clf.coef_&quot;&quot;&quot;array([[ 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,show more (open the raw output data in a text editor) ... 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])&quot;&quot;&quot;clf.intercept_&quot;&quot;&quot;array([0.00016519])&quot;&quot;&quot;clf.score&quot;&quot;&quot;&lt;bound method ClassifierMixin.score of LogisticRegression(random_state=0)&gt;&quot;&quot;&quot;clf.score(binary_x_train, binary_y_train)&quot;&quot;&quot;1.0&quot;&quot;&quot;clf.score(binary_x_val, binary_y_val)&quot;&quot;&quot;1.0&quot;&quot;&quot;binary_x_test.shape&quot;&quot;&quot;(11841, 784)&quot;&quot;&quot;binary_y_test.shape&quot;&quot;&quot;0.9865720800608057&quot;&quot;&quot;predicated_result = clf.predict(binary_x_test)np.where(binary_y_test != predicated_result)&quot;&quot;&quot;(array([ 17, 45, 66, 137, 260, 279, 323, 453, 529, 739, 753, 947, 1034, 1248, 1290, 1422, 1434, 1444, ... 10677, 10739, 10750, 10979, 11010, 11058, 11104, 11113, 11366, 11389, 11421, 11458, 11528, 11659, 11760]),)&quot;&quot;&quot;lookup_index = 1184plt.imshow(np.reshape(binary_x_test[lookup_index], (28, 28)))plt.title('Actual Value: {} ; Predict Value: {} \\n'.format(binary_y_test[lookup_index], predicated_result[lookup_index]), fontsize = 20)&quot;&quot;&quot;Text(0.5, 1.0, 'Actual Value: 6 ; Predict Value: 6 \\n')&quot;&quot;&quot; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from sklearn import metricsbinary_y_test[0]&quot;&quot;&quot;6&quot;&quot;&quot;predicated_result[0]&quot;&quot;&quot;6&quot;&quot;&quot;metrics.precision_score(binary_y_test, predicated_result, average = 'macro')&quot;&quot;&quot;0.9865879016517065&quot;&quot;&quot;metrics.precision_score(binary_y_test, predicated_result, pos_label = 6)&quot;&quot;&quot;0.9837056946077608&quot;&quot;&quot;metrics.recall_score(binary_y_test, predicated_result, pos_label = 6)&quot;&quot;&quot;0.9895234876647516&quot;&quot;&quot;fpr, tpr, threshold = metrics.roc_curve(binary_y_test, predicated_result, pos_label = 6)metrics.auc(fpr, tpr)&quot;&quot;&quot;0.9865733258009728&quot;&quot;&quot;cm = metrics.confusion_matrix(binary_y_test, predicated_result)import seaborn as snsfrom sklearn.metrics import confusion_matrixdata = confusion_matrix(binary_y_test, predicated_result)data&quot;&quot;&quot;array([[5826, 97], [ 62, 5856]])&quot;&quot;&quot;df_cm = pd.DataFrame(data, columns = np.unique(binary_y_test), index = np.unique(binary_y_test))plt.figure(figsize = (10, 7))sns.set(font_scale=1.4) # for label sizesns.heatmap(df_cm, cmap='Blues', annot=True, annot_kws = {'size': 16}) # font size&quot;&quot;&quot;&lt;AxesSubplot:&gt;&quot;&quot;&quot; 1234567df_cm.index.name = 'Actual'df_cm.columns.name = 'Predicted'plt.figure(figsize = (10, 10))sns.heatmap(df_cm, cmap='Blues', annot=True, annot_kws={'size': 16})&quot;&quot;&quot;&lt;AxesSubplot:xlabel='Predicted', ylabel='Actual'&gt;&quot;&quot;&quot; Boston code reproduction and reference answers 123456789101112131415161718192021222324252627282930313233# Import package# Used to load the Boston housing price data setfrom sklearn.datasets import load_boston# pandas toolkit For students who are new to pandas, please refer to the official 10-minute tutorial: https://pandas.pydata.org/pandas-docs/stable/10min.htmlimport pandas as pd# seaborn for drawingimport seaborn as snsimport numpy as np # numpy# Show drawing%matplotlib inlinedata = load_boston()data.keys()&quot;&quot;&quot;dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])&quot;&quot;&quot;df = pd.DataFrame(data['data'])df.head()&quot;&quot;&quot; 0 1 2 3 4 5 6 7 8 9 10 11 120 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.981 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.142 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.033 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.944 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33&quot;&quot;&quot;data['feature_names']&quot;&quot;&quot;array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='&lt;U7')&quot;&quot;&quot; Field meaning 名称 中文描述 CRIM 住房所在城镇的人均犯罪率 ZN 住房用地超过 25000 平方尺的比例 INDUS 住房所在城镇非零售商用土地的比例 CHAS 有关查理斯河的虚拟变量（如果住房位于河边则为1,否则为0 ） NOX 一氧化氮浓度 RM 每处住房的平均房间数 AGE 建于 1940 年之前的业主自住房比例 DIS 住房距离波士顿五大中心区域的加权距离 RAD 离住房最近的公路入口编号 TAX 每 10000 美元的全额财产税金额 PTRATIO 住房所在城镇的师生比例 B 1000(Bk-0.63)^2,其中 Bk 指代城镇中黑人的比例 LSTAT 弱势群体人口所占比例 MEDV 业主自住房的中位数房价（以千美元计） 1234567891011121314151617181920df.columns = data['feature_names']df.head()&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.981 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.142 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.033 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.944 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33&quot;&quot;&quot;df['price'] = data['target']df.head(2)&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT price0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.9 4.98 24.01 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.9 9.14 21.6&quot;&quot;&quot;sns.heatmap(df.corr(), annot=True, fmt='.1f') 12345import matplotlib.pyplot as pltplt.scatter(df['RM'], df['price'])&quot;&quot;&quot;&lt;matplotlib.collections.PathCollection at 0x7fe0f984f810&gt;&quot;&quot;&quot; 12345678910111213plt.figure(figsize = (20, 5))features = ['LSTAT', 'RM']target = df['price']for i, col in enumerate(features): plt.subplot(1, len(features), i+1) x = df[col] y = target plt.scatter(x, y, marker = 'o') plt.title('{} vs price'.format(col)) plt.xlabel(col) plt.ylabel('price') 1234567891011121314151617181920x = df['RM']y = df['price']history_notes = {_x: _y for _x, _y in zip(x, y)}history_notes[6.575]&quot;&quot;&quot;24.0&quot;&quot;&quot;# Find the top three prices closest to RM:6.57,similary_ys = [y for _, y in sorted(history_notes.items(), key=lambda x_y: (x_y[0]-6.57) ** 2)[:3]]similary_ys&quot;&quot;&quot;[23.8, 24.0, 24.8]&quot;&quot;&quot;np.mean(similary_ys) # Calculate the average of three&quot;&quot;&quot;24.2&quot;&quot;&quot; Using historical data to predict data that has never been seen before, the most direct method K-Neighbor-Nearst 1234567891011def knn(query_x, history, top_n=3): sorted_notes = sorted(history.items(), key=lambda x_y: (x_y[0] - query_x) ** 2) similar_notes = sorted_notes[:top_n] similar_ys = [y for _, y in similar_notes] return np.mean(similar_ys)knn(5.4, history_notes)&quot;&quot;&quot;15.700000000000001&quot;&quot;&quot; In order to obtain results faster, we hope to obtain predictive power by fitting a function \\[ f(rm) = k * rm + b \\] Random Approach \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} (\\hat{y_i} - y_i) ^ 2 \\] \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] 12345678910111213141516171819202122232425262728293031323334def loss(yhat, y): return np.mean((yhat - y) **2)import randommin_loss = float('inf')best_k, bes_b = None, Noneprint(min_loss)min_loss = float('inf')best_k, bes_b = None, Nonefor step in range(1000): min_v, max_v = -100, 100 k, b = random.randrange(min_v, max_v), random.randrange(min_v, max_v) y_hats = [k * rm_i + b for rm_i in x] current_loss = loss(y_hats, y) if current_loss &lt;min_loss: min_loss = current_loss best_k, best_b = k, b print('In step {}, we have obtained the function f(rm) = {} * rm + {}, at this time loss is: {}'.format(step, k, b, current_loss))&quot;&quot;&quot;In step 0, we have obtained the function f(rm) = 14 * rm + -78, at this time loss is: 212.87040239525695In step 70, we have obtained the function f(rm) = 10 * rm + -47, at this time loss is: 88.70654683794466In step 256, we have obtained the function f(rm) = 13 * rm + -55, at this time loss is: 68.45390542094862In step 526, we have obtained the function f(rm) = 10 * rm + -37, at this time loss is: 54.977297826086954&quot;&quot;&quot;plt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x])&quot;&quot;&quot;&lt;matplotlib.collections.PathCollection at 0x7fe0980f37d0&gt;&quot;&quot;&quot; Monte Carlo simulation Supervisor \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{k}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i) * rm_i \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{b}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i)\\] 12345678910111213141516171819202122232425262728293031323334def partial_k(k, b, x, y): return 2 * np.mean((k*x+b-y) *x)def partial_b(k, b, x, y): return 2*np.mean(k*x+b-y) k, b = random.random(), random.random()min_loss = float('inf')best_k, best_b = None, Nonelearning_rate = 1e-2for step in range(2000): k,b = k+(-1*partial_k(k,b,x,y) * learning_rate), b+(-1*partial_b(k,b,x,y) * learning_rate) y_hats = k * x +b current_loss = loss(y_hats, y) if current_loss &lt; min_loss: min_loss = current_loss best_k, best_b = k, b print('On the {} step, we have func f(rm) = {} * rm + {}, loss is {} now'.format(step, k, b, current_loss))&quot;&quot;&quot;On the 0 step, we have func f(rm) = 6.968714597804018 * rm + -21.099847342593957, loss is 45.86961514375004 nowOn the 1 step, we have func f(rm) = 6.9692276199804555 * rm + -21.103110737199852, loss is 45.86852398135223 nowshow more (open the raw output data in a text editor) ...On the 1999 step, we have func f(rm) = 7.783005326604901 * rm + -26.279646762684518, loss is 44.468037178267025 now&quot;&quot;&quot;best_k, best_b&quot;&quot;&quot;(10, -37)&quot;&quot;&quot;plt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x]) Supervised Learning We turn the forecast of housing prices into a more responsible and sophisticated model. What should we do? \\[ f(x) = k * x + b \\] \\[ f(x) = k2 * \\sigma(k_1 * x + b_1) + b2 \\] \\[ \\sigma(x) = \\frac{1}{1 + e^(-x)} \\] 1234def sigmoid(x): return 1 / (1 + np.exp(-x))sub_x = np.linspace(-10, 10)plt.plot(sub_x, sigmoid(sub_x)) 12345678910def random_linear(x): k, b = random.random(), random.random() return k * x + bdef complex_function(x): return (random_linear(x))for _ in range(10): index = random.randrange(0, len(sub_x)) sub_x_1, sub_x_2 = sub_x[:index], sub_x[index:] new_y = np.concatenate((complex_function(sub_x_1), complex_function(sub_x_2))) plt.plot(sub_x, new_y) We can implement more complex functions through simple, basic modules and repeated superposition For more and more complex functions? How does the computer seek guidance? What is machine learning? The shortcomings of the KNN method, what is the background of the proposed linear fitting How to obtain faster function weight update through supervision method The combination of nonlinear and linear functions can fit very complex functions Deep learning we can fit more complex functions through basic function modules Assigment \\[ L2-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{(\\hat{y} - y)}^2 \\] \\[ L1-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{|(\\hat{y} - y)|} \\] Change L2-Loss in the code to L1Loss and implement gradient descent Realize L1Loss gradient descent from 0 1 Import package 12import numpy as npimport pandas as pd 2 Load data set 123456789101112131415161718192021222324252627282930313233from sklearn.datasets import load_bostonboston = load_boston()boston.keys()&quot;&quot;&quot;dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])&quot;&quot;&quot;X = boston.datay = boston.targetdf = pd.DataFrame(boston.data, columns = boston.feature_names)df.head()&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.981 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.142 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.033 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.944 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33&quot;&quot;&quot;df.describe() # Data description, you can view the statistics of each variable&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTATcount 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000mean 3.613524 11.363636 11.136779 0.069170 0.554695 6.284634 68.574901 3.795043 9.549407 408.237154 18.455534 356.674032 12.653063std 8.601545 23.322453 6.860353 0.253994 0.115878 0.702617 28.148861 2.105710 8.707259 168.537116 2.164946 91.294864 7.141062min 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 187.000000 12.600000 0.320000 1.73000025% 0.082045 0.000000 5.190000 0.000000 0.449000 5.885500 45.025000 2.100175 4.000000 279.000000 17.400000 375.377500 6.95000050% 0.256510 0.000000 9.690000 0.000000 0.538000 6.208500 77.500000 3.207450 5.000000 330.000000 19.050000 391.440000 11.36000075% 3.677083 12.500000 18.100000 0.000000 0.624000 6.623500 94.075000 5.188425 24.000000 666.000000 20.200000 396.225000 16.955000max 88.976200 100.000000 27.740000 1.000000 0.871000 8.780000 100.000000 12.126500 24.000000 711.000000 22.000000 396.900000 37.970000&quot;&quot;&quot; 3 Data preprocessing Normalization or standardization can prevent a certain dimension or a few dimensions from affecting the data too much when there are very many dimensions, and secondly, the program can run faster. There are many methods, such as standardization, min-max, z-score, p-norm, etc. How to use it depends on the characteristics of the data set. Extended reading-the deep learning field of the myth of data standardization 12345678910111213141516171819from sklearn.preprocessing import StandardScalerss = StandardScaler() # z = (x-u) / s u is the mean, s is the standard deviationX = ss.fit_transform(df) # For linear models, normalization or standardization is generally required, otherwise there will be a gradient explosion, which is generally not required for tree modelsdf = pd.DataFrame(X, columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX' ,'PTRATIO','B','LSTAT'])df.describe()&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTATcount 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02mean 2.808469e-17 6.599903e-16 -4.633974e-16 -4.353127e-16 1.404235e-16 -1.755293e-17 2.176564e-16 -1.685082e-16 -5.055245e-16 8.987102e-16 -1.067218e-15 4.493551e-16 -2.246775e-16std 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00min -4.197819e-01 -4.877224e-01 -1.557842e+00 -2.725986e-01 -1.465882e+00 -3.880249e+00 -2.335437e+00 -1.267069e+00 -9.828429e-01 -1.313990e+00 -2.707379e+00 -3.907193e+00 -1.531127e+0025% -4.109696e-01 -4.877224e-01 -8.676906e-01 -2.725986e-01 -9.130288e-01 -5.686303e-01 -8.374480e-01 -8.056878e-01 -6.379618e-01 -7.675760e-01 -4.880391e-01 2.050715e-01 -7.994200e-0150% -3.906665e-01 -4.877224e-01 -2.110985e-01 -2.725986e-01 -1.442174e-01 -1.084655e-01 3.173816e-01 -2.793234e-01 -5.230014e-01 -4.646726e-01 2.748590e-01 3.811865e-01 -1.812536e-0175% 7.396560e-03 4.877224e-02 1.015999e+00 -2.725986e-01 5.986790e-01 4.827678e-01 9.067981e-01 6.623709e-01 1.661245e+00 1.530926e+00 8.065758e-01 4.336510e-01 6.030188e-01max 9.933931e+00 3.804234e+00 2.422565e+00 3.668398e+00 2.732346e+00 3.555044e+00 1.117494e+00 3.960518e+00 1.661245e+00 1.798194e+00 1.638828e+00 4.410519e-01 3.548771e+00&quot;&quot;&quot; \\[ y=Σwixi+b \\] Because the derivation of b is all 1, add a bias b to the data and set it to 1, as a feature of the data and update the gradient wi*b=wi 1234567891011121314151617df['bias'] = 1df&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT bias0 -0.419782 0.284830 -1.287909 -0.272599 -0.144217 0.413672 -0.120013 0.140214 -0.982843 -0.666608 -1.459000 0.441052 -1.075562 11 -0.417339 -0.487722 -0.593381 -0.272599 -0.740262 0.194274 0.367166 0.557160 -0.867883 -0.987329 -0.303094 0.441052 -0.492439 12 -0.417342 -0.487722 -0.593381 -0.272599 -0.740262 1.282714 -0.265812 0.557160 -0.867883 -0.987329 -0.303094 0.396427 -1.208727 13 -0.416750 -0.487722 -1.306878 -0.272599 -0.835284 1.016303 -0.809889 1.077737 -0.752922 -1.106115 0.113032 0.416163 -1.361517 14 -0.412482 -0.487722 -1.306878 -0.272599 -0.835284 1.228577 -0.511180 1.077737 -0.752922 -1.106115 0.113032 0.441052 -1.026501 1... ... ... ... ... ... ... ... ... ... ... ... ... ... ...501 -0.413229 -0.487722 0.115738 -0.272599 0.158124 0.439316 0.018673 -0.625796 -0.982843 -0.803212 1.176466 0.387217 -0.418147 1502 -0.415249 -0.487722 0.115738 -0.272599 0.158124 -0.234548 0.288933 -0.716639 -0.982843 -0.803212 1.176466 0.441052 -0.500850 1503 -0.413447 -0.487722 0.115738 -0.272599 0.158124 0.984960 0.797449 -0.773684 -0.982843 -0.803212 1.176466 0.441052 -0.983048 1504 -0.407764 -0.487722 0.115738 -0.272599 0.158124 0.725672 0.736996 -0.668437 -0.982843 -0.803212 1.176466 0.403225 -0.865302 1505 -0.415000 -0.487722 0.115738 -0.272599 0.158124 -0.362767 0.434732 -0.613246 -0.982843 -0.803212 1.176466 0.441052 -0.669058 1506 rows × 14 columns&quot;&quot;&quot; Divide the data set, where 20% of the data is used as the test set X_test, y_test, and the other 80% are used as the training set X_train, y_train, where random_state is the random seed 1234567891011from sklearn.model_selection import train_test_splitX_train, X_test, y_train,y_test = train_test_split(df, y, test_size = 0.2, random_state = 42)print('X_train.shape, y_train.shape:', X_train.shape, y_train.shape)print('X_test.shape, y_test.shape', X_test.shape, y_test.shape)&quot;&quot;&quot;X_train.shape, y_train.shape: (404, 14) (404,)X_test.shape, y_test.shape (102, 14) (102,)&quot;&quot;&quot;X_train = np.array(X_train) Model training and gradient update 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697def l1_cost(X, y, theta): &quot;&quot;&quot; X: 特征 y: 目标值 theta: 模型参数 &quot;&quot;&quot; k = X.shape[0] total_cost = 0 for i in range(k): total_cost =+ 1/k * np.abs(y[i] - theta.dot(X[i, :])) return total_cost def l2_cost(X, y, theta): k = X.shape[0] total_cost = 0 for i in range(k): total_cost += 1/k * (y[i] - theta.dot(X[i, :])) ** 2 return total_cost np.zeros(10).shape&quot;&quot;&quot;(10,)&quot;&quot;&quot;def step_l1_gradient(X, y, learning_rate, theta): &quot;&quot;&quot; Function to calculate the gradient of the MAE loss function Return the gradient value 0 for the non-differentiable point at 0 X: feature vector y: target value learing_rate: learning rate theta: parameter &quot;&quot;&quot; n = X.shape[0] print(n) e = y-X @ theta gradients = -(X.T @ np.sign(e)) / n theta = theta-learning_rate * gradients return theta def step_l2_gradient(X, y, learning_rate, theta): k = X.shape[0] x = X.shape[1] gradients = np.zeros(n) for i in range(k): for j in range(n): gradients[j] += (-2/k) * (y[i] - (theta.dot(X[i, :]))) * X[i, j] theta = theta - learning_rate * gradients return theta def step_gradient(X, y, learning_rate, theta): &quot;&quot;&quot; X: feature vector y: target value learing_rate: learning rate theta: parameter &quot;&quot;&quot; m_deriv = 0 N = len(X) for i in range(N): # Calculate the partial derivative # -x(y-(mx + b)) / |mx + b| m_deriv +=-X[i] * (y[i]-(theta*X[i] + b)) / abs(y[i]-(theta*X[i] + b)) # We subtract because the derivatives point in direction of steepest ascent theta -= (m_deriv / float(N)) * learning_rate# theta = theta-learning_rate * gradients return thetadef gradient_descent(X_train, y_train, learning_rate, iterations): k = X_train.shape[0] n = X_train.shape[1] theta = np.zeros(n) loss_values = [] print(theta.shape) for i in range(iterations): theta = step_l1_gradient(X_train, y_train, learning_rate, theta) loss = l1_cost(X_train, y_train, theta) loss_values.append(loss) print(i, 'cost:', loss) return theta, loss_values # Training parameterslearning_rate = 0.04 # Learning rateiterations = 300 # number of iterationstheta ,loss_values = gradient_descent(X_train, y_train, learning_rate, iterations)&quot;&quot;&quot;(14,)4040 cost: 0.045943991727139124041 cost: 0.045848379493882215404show more (open the raw output data in a text editor) ...299 cost: 0.017838215258874083&quot;&quot;&quot; Heart Practise 1234import pandas as pdpath = '~/data/'dataPath = path + 'heart.csv'train_data = pd.read_csv(dataPath) Field meaning 字段名 含义 age 年龄 sex 性别(1 = 男性, 0 = 女性) cp 胸部疼痛类型(值1：典型心绞痛，值2：非典型性心绞痛，值3：非心绞痛，值4：无症状） trestbps 血压 chol 胆固醇 fbs 空腹血糖（&gt; 120 mg/dl，1=真；0=假） restecg 心电图结果（0=正常，1=患有ST-T波异常，2=根据Estes的标准显示可能或确定的左心室肥大） thalach 最大心跳数 exang 运动时是否心绞痛（1=有过；0=没有） oldpeak 运动相对于休息的ST slop 心电图ST segment的倾斜度(值1:上坡，值2:平坦，值3:下坡） ca 透视检查看到的血管数 thal 缺陷种类（3=正常；6=固定缺陷；7=可逆缺陷） target 是否患病（0=否，1=是） Print a brief summary of the data set 12345678910111213141516171819202122232425262728293031train_data.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 303 entries, 0 to 302Data columns (total 14 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 303 non-null int64 1 sex 303 non-null int64 2 cp 303 non-null int64 3 trestbps 303 non-null int64 4 chol 303 non-null int64 5 fbs 303 non-null int64 6 restecg 303 non-null int64 7 thalach 303 non-null int64 8 exang 303 non-null int64 9 oldpeak 303 non-null float64 10 slope 303 non-null int64 11 ca 303 non-null int64 12 thal 303 non-null int64 13 target 303 non-null int64 dtypes: float64(1), int64(13)memory usage: 33.3 KB&quot;&quot;&quot;train_data.target.value_counts()&quot;&quot;&quot;1 1650 138Name: target, dtype: int64&quot;&quot;&quot; Change the \"sex\" column to two columns \"sex_0\" and \"sex_1\". 1sex = pd.get_dummies(train_data['sex'], prefix = &quot;sex&quot;) Add \"sex_0\" and \"sex_1\" to the data set. 1train_data = pd.concat([train_data,sex], axis = 1) And delete the sex column 1train_data = train_data.drop(columns = ['sex']) Print out the first five lines. Check whether sex_0, sex_1 are added successfully, and whether sex is deleted successfully. 123456789train_data.head()&quot;&quot;&quot; age cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target sex_0 sex_10 63 3 145 233 1 0 150 0 2.3 0 0 1 1 0 11 37 2 130 250 0 1 187 0 3.5 0 0 2 1 0 12 41 1 130 204 0 0 172 0 1.4 2 0 2 1 1 03 56 1 120 236 0 1 178 0 0.8 2 0 2 1 0 14 57 0 120 354 0 1 163 1 0.6 2 0 2 1 1 0&quot;&quot;&quot; Get sample label 12345y_data = train_data.target.valuestrain_data.shape&quot;&quot;&quot;(303, 15)&quot;&quot;&quot; Get sample feature set 12345x_data = train_data.drop(['target'],axis=1)x_data.shape&quot;&quot;&quot;(303, 14)&quot;&quot;&quot; Divide the data set Parameters: test_size=0.3, random_state=33 12from sklearn.model_selection import train_test_splitX_train,X_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=33) Normalization Import the StandardScaler package and initialize 12from sklearn.preprocessing import StandardScalerstandardScaler = StandardScaler() fit function/module is used to train model parameters 1standardScaler.fit(X_train) Standardize the training set and test set 12X_train = standardScaler.transform(X_train)X_test = standardScaler.transform(X_test) Define logistic regression model 123from sklearn.linear_model import LogisticRegression log_reg = LogisticRegression()log_reg.fit(X_train,y_train) Calculate the training set score 1234log_reg.score(X_train,y_train)&quot;&quot;&quot;0.8537735849056604&quot;&quot;&quot; Calculate the test set score 1234log_reg.score(X_test,y_test)&quot;&quot;&quot;0.8461538461538461&quot;&quot;&quot; Use the classification_report function to display a text report of the main classification indicators 12345678910111213from sklearn.metrics import classification_reporty_predict_log = log_reg.predict(X_test)print(classification_report(y_test,y_predict_log))&quot;&quot;&quot; precision recall f1-score support 0 0.93 0.78 0.85 50 1 0.78 0.93 0.84 41 accuracy 0.85 91 macro avg 0.85 0.85 0.85 91weighted avg 0.86 0.85 0.85 91&quot;&quot;&quot;","link":"/example_04/"},{"title":"Machine Learning Part-03","text":"Decision trees Machine learning basics - use decision trees to make predictions about coupons In order to get close to real life and applications, the processing of actual data sets is the main focus. From January 1, 2016 to June 30, 2016, real online and offline consumption behaviors are predicted to be used by users within 15 days after receiving coupons in July 2016. Note: In order to protect the privacy of users and businesses, all data is anonymized, and biased sampling and necessary filtering are used. Data set ccf_offline_stage1_train.csv (training data) Field Description User_id 用户ID Merchant_id 商户ID Coupon_id 优惠券ID：null表示无优惠券消费，此时Discount_rate和Date_received字段无意义 Discount_rate 优惠率：x 代表折扣率；x:y表示满x减y。单位是元 Distance user经常活动的地点离该merchant的最近门店距离是x*500米（如果是连锁店，则取最近的一家门店），x\\(\\in[0,10]\\)；null表示无此信息，0表示低于500米，10表示大于5公里； Date_received 领取优惠券日期 Date 消费日期：如果Date=null &amp; Coupon_id != null，该记录表示领取优惠券但没有使用，即负样本；如果Date!=null &amp; Coupon_id = null，则表示普通消费日期；如果Date!=null &amp; Coupon_id != null，则表示用优惠券消费日期，即正样本； 123456789101112131415161718192021222324252627282930313233343536373839404142# load pluginimport pandas as pdimport numpy as np# load datatrain_data = pd.read_csv('~/data/ccf_offline_stage1_train.csv')train_data.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 1754884 entries, 0 to 1754883Data columns (total 7 columns): # Column Dtype --- ------ ----- 0 User_id int64 1 Merchant_id int64 2 Coupon_id float64 3 Discount_rate object 4 Distance float64 5 Date_received float64 6 Date float64dtypes: float64(4), int64(2), object(1)memory usage: 93.7+ MB&quot;&quot;&quot;train_data.head()&quot;&quot;&quot; User_id Merchant_id Coupon_id Discount_rate Distance Date_received Date0 1439408 2632 NaN NaN 0.0 NaN 20160217.01 1439408 4663 11002.0 150:20 1.0 20160528.0 NaN2 1439408 2632 8591.0 20:1 0.0 20160217.0 NaN3 1439408 2632 1078.0 20:1 0.0 20160319.0 NaN4 1439408 2632 8591.0 20:1 0.0 20160613.0 NaN&quot;&quot;&quot;print(train_data.shape)data = train_data.dropna(how = 'any')print(train_data.shape)&quot;&quot;&quot;(1754884, 7)(1754884, 7)&quot;&quot;&quot; Discount_rate是object类型的，object在pandas中代表字符串，字符串类型不能输入模型中，所以需要改为数值类型 123456789101112print('Discount_rate 类型: \\n', data['Discount_rate'].unique())# [0,1] 表示折扣率# x:y 表示满 x 减 y&quot;&quot;&quot;Discount_rate 类型: ['20:1' '20:5' '30:5' '50:10' '10:5' '50:20' '100:10' '30:10' '50:5' '30:1' '100:30' '0.8' '200:30' '100:20' '10:1' '200:20' '0.95' '5:1' '100:5' '100:50' '50:1' '20:10' '150:10' '0.9' '200:50' '150:20' '150:50' '200:5' '300:30' '100:1' '200:10' '150:30' '0.85' '0.6' '0.5' '300:20' '200:100' '300:50' '150:5' '300:10' '0.75' '0.7' '30:20' '50:30']&quot;&quot;&quot; Convert Discount_rate into numerical features Discount type x:y 表示满 x 减 y 将 x:y 类型的字符串设为1 [0,1] 表示折扣率 将 [0,1] 类型的字符串设为 0 12345678910111213141516171819202122232425262728293031323334353637def getDiscountType(row): if ':' in row: return 1 else: return 0data['Discount_rate'] = data['Discount_rate'].apply(getDiscountType)&quot;&quot;&quot;See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy import sys&quot;&quot;&quot;data.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;Int64Index: 67165 entries, 6 to 1754880Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 User_id 67165 non-null int64 1 Merchant_id 67165 non-null int64 2 Coupon_id 67165 non-null float64 3 Discount_rate 67165 non-null int64 4 Distance 67165 non-null float64 5 Date_received 67165 non-null float64 6 Date 67165 non-null float64dtypes: float64(4), int64(3)memory usage: 4.1 MB&quot;&quot;&quot;# load plugin# Import DecisionTreeClassifier modelfrom sklearn.tree import DecisionTreeClassifier# Import train_test_split, used to divide the data set and test setfrom sklearn.model_selection import train_test_split# Import accuracy_score accuracy indexfrom sklearn.metrics import accuracy_score add label row to the dataset Labeling Label Label which samples are positive samples y=1 and which are negative samples y = -1 Forecast goal: the user's consumption within 15 days after receiving the coupon (Date-Date_received &lt;= 15) means to receive the coupon and use it within 15 days, that is, a positive sample, y = 1 (Date-Date_received&gt; 15) means that the coupon has not been used within 15 days, that is, a negative sample, y = 0 pandas tutorial on time https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html 1234567891011def label(row): if row['Date'] != 'null': td = pd.to_datetime(row['Date'], format = '%Y%m%d') - pd.to_datetime(row['Date_received'], format = '%Y%m%d') if td &lt;= pd.Timedelta(15, 'D'): return 1 return 0data['label'] = data.apply(label, axis = 1)&quot;&quot;&quot;See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy&quot;&quot;&quot; Statistics positive and negative samples 123456print(data['label'].value_counts())&quot;&quot;&quot;1 570600 10105Name: label, dtype: int64&quot;&quot;&quot; Divide the data set 80% training set 20% test set 80% train 20% test 1X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.2, random_state=3) Check the number and category distribution of training samples 12345678y_train.value_counts()&quot;&quot;&quot;2751537 966641735 86 ..4461556 1Name: User_id, Length: 34984, dtype: int64&quot;&quot;&quot; Check the number and type distribution of test samples 12345678y_test.value_counts()&quot;&quot;&quot;6641735 272751537 22 ..89464 1Name: User_id, Length: 11405, dtype: int64&quot;&quot;&quot; Initialize the classification decision tree model, the depth is 5 layers 1model = DecisionTreeClassifier(max_depth=6, random_state = 1) Model training 1model.fit(X_train, y_train) Model prediction 1y_pred = model.predict(X_test) Model evaluation 1234accuracy_score(y_test, y_pred)&quot;&quot;&quot;0.011315417256011316&quot;&quot;&quot; Change the standard of the model selection feature to entropy 1model = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=2) Model training 1model.fit(X_train, y_train) predict 1y_pred = model.predict(X_test) Evaluate 1accuracy_score(y_test, y_pred) In addition to the above key steps, you can explore the data by yourself, as well as any other forms of feature preprocessing methods and feature engineering processing. I hope to focus on understanding the development process of machine learning tasks. For the skills and methods of data processing, it is encouraged to invest more time to explore. iris 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifier, export_graphviz iris = load_iris()X = iris.data y = iris.targettree_clf = DecisionTreeClassifier()tree_clf.fit(X, y)export_graphviz( tree_clf, out_file=&quot;~/data/course_data/iris_tree.dot&quot;, feature_names=iris.feature_names, class_names=iris.target_names, rounded=True, filled=True)for line in open('~/data/course_data/iris_tree.dot'): print(line)&quot;&quot;&quot;digraph Tree {node [shape=box, style=&quot;filled, rounded&quot;, color=&quot;black&quot;, fontname=helvetica] ;edge [fontname=helvetica] ;0 [label=&quot;petal length (cm) &lt;= 2.45\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]\\nclass = setosa&quot;, fillcolor=&quot;#ffffff&quot;] ;1 [label=&quot;gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]\\nclass = setosa&quot;, fillcolor=&quot;#e58139&quot;] ;0 -&gt; 1 [labeldistance=2.5, labelangle=45, headlabel=&quot;True&quot;] ;2 [label=&quot;petal width (cm) &lt;= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]\\nclass = versicolor&quot;, fillcolor=&quot;#ffffff&quot;] ;0 -&gt; 2 [labeldistance=2.5, labelangle=-45, headlabel=&quot;False&quot;] ;3 [label=&quot;petal length (cm) &lt;= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]\\nclass = versicolor&quot;, fillcolor=&quot;#4de88e&quot;] ;2 -&gt; 3 ;4 [label=&quot;petal width (cm) &lt;= 1.65\\ngini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]\\nclass = versicolor&quot;, fillcolor=&quot;#3de684&quot;] ;3 -&gt; 4 ;5 [label=&quot;gini = 0.0\\nsamples = 47\\nvalue = [0, 47, 0]\\nclass = versicolor&quot;, fillcolor=&quot;#39e581&quot;] ;show more (open the raw output data in a text editor) ...16 [label=&quot;gini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]\\nclass = virginica&quot;, fillcolor=&quot;#8139e5&quot;] ;12 -&gt; 16 ;}&quot;&quot;&quot; Salient Features 1tree_clf.feature_importances_ Build Decision Tree: CART 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import pandas as pdmock_data = { 'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'], 'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'], 'family_number': [1, 1, 2, 1, 1, 1, 2], 'bought': [1, 1, 1, 0, 0, 0, 1],}dataset = pd.DataFrame.from_dict(mock_data)import numpy as npfrom collections import Counterdef entropy(elements): counter = Counter(elements) probabilities = [counter[e] / len(elements) for e in elements] return -sum(p * np.log10(p) for p in probabilities)def find_the_min_spilter(training_data: pd.DataFrame, target: str) -&gt; str: x_fields = set(training_data.columns.tolist()) - {target} spliter = None min_entropy = float('inf') for f in x_fields: elements = set(training_data[f]) for e in elements: sub_spliter_1 = training_data[dataset[f] == e][target].tolist() entropy_1 = entropy(sub_spliter_1) sub_spliter_2 = training_data[dataset[f] != e][target].tolist() entropy_2 = entropy(sub_spliter_2) entropy_v = entropy_1 + entropy_2 if entropy_v &lt; min_entropy: min_entropy = entropy_v spliter = (f, e) print('spliter is: {}'.format(spliter)) print('the min entropy is: {}'.format(min_entropy)) return spliterfind_the_min_spilter(dataset, 'bought')&quot;&quot;&quot;spliter is: ('income', '+10')the min entropy is: 0.7176797562470717('income', '+10')&quot;&quot;&quot;dataset[dataset['income'] == '-10']&quot;&quot;&quot; gender income family_number bought1 F -10 1 16 M -10 2 1&quot;&quot;&quot;dataset[dataset['income'] != '-10']&quot;&quot;&quot; gender income family_number bought0 F +10 1 12 F +10 2 13 F +10 1 04 M +10 1 05 M +10 1 0&quot;&quot;&quot;","link":"/example_05/"},{"title":"Machine Learning Part-04","text":"SVM 12345678910111213import numpy as nplabel_a = np.random.normal(6, 2, size = (50,2))label_b = np.random.normal(-6, 2, size = (50,2))import matplotlib.pyplot as plta = [1, 2, 3]b = [-1,-2, -3]plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))plt.show() 12345678910111213141516171819202122232425262728293031323334353637label_a_x = label_a[:, 0]label_b_x = label_b[:, 0]def f(x, k, b): return k*x -b k_and_b = []for i in range(100): k, b = (np.random.random(size = (1,2)) * 10 - 5)[0] if np.max(f(label_a_x, k, b)) &lt;= -1 and np.min(f(label_b_x, k, b)) &gt;= 1: print(k, b) k_and_b.append((k, b))&quot;&quot;&quot;-3.4732670434285517 -2.3248316389039325-3.654276254462583 0.01110189858052646-2.4609031871010014 -0.3932180655739925-2.9206497777762843 0.2595456609552631-4.07589152330003 -0.6463313059119606-3.1950366475236835 -1.8558958669742989-4.316670785852706 -3.1033030808371653-4.124339773909792 -1.5741734685470687-4.20817621470405 0.4368323022696625-3.7098120657624003 -0.38196175566618784-3.2053446683533315 0.12822700803583054-4.534694169094692 1.143734501297419-4.8124714209376425 0.8707258703100704&quot;&quot;&quot;plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))for k, b in k_and_b: x = np.concatenate((label_a_x, label_b_x)) plt.plot(x, f(x, k, b))plt.show() 1234567plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))k,b = sorted(k_and_b, key = lambda t: abs(t[0]))[0]x = np.concatenate((label_a_x, label_b_x))plt.plot(x, f(x, k, b))plt.show() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from sklearn.datasets import load_bostondatasets = load_boston()data, target = datasets['data'], datasets['target']import pandas as pddf = pd.DataFrame(data)df.columns = datasets['feature_names']import randomdef random_select(df, drop_num = 4): columns = random.sample(list(df.columns), k = len(df.columns) - drop_num) return df[columns] from sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeRegressorsample_x = random_select(df)regressioner = DecisionTreeRegressor()(X_train, X_test, y_train, y_test) = train_test_split(sample_x, target, test_size = 0.3)regressioner.fit(X_train, y_train)regressioner.score(X_train, y_train)&quot;&quot;&quot;1.0&quot;&quot;&quot;regressioner.score(X_test, y_test)&quot;&quot;&quot;0.8110635350395325&quot;&quot;&quot;def random_tree(train_X, train_y, test_X, test_y, drop_n = 4): train_sample = random_select(train_X, drop_num = drop_n) regressioner = DecisionTreeRegressor() regressioner.fit(train_sample, train_y) train_score = regressioner.score(train_sample, train_y) test_score = regressioner.score(test_X[train_sample.columns], test_y) print('train score = {}; test score = {}'.format(train_score, test_score)) y_predicat = regressioner.predict(test_X[train_sample.columns]) return y_predicat def random_forest(train_X, train_y, test_X, test_y, tree_n = 4): predicat = np.array([random_tree(train_X, train_y, test_X, test_y) for _ in range(tree_n)]) return np.mean(predicat, axis = 0) (X_train, X_test, y_train, y_test) = train_test_split(df, target, test_size = 0.3)forest_predict = random_forest(X_train, y_train, X_test, y_test)&quot;&quot;&quot;train score = 1.0; test score = 0.5367061884031776train score = 1.0; test score = 0.4983695562874999train score = 1.0; test score = 0.6715869370883646train score = 1.0; test score = 0.6210922529610217&quot;&quot;&quot;forest_predict&quot;&quot;&quot;array([10.925, 21.1 , 30.625, 28.025, 22.525, 17.65 , 20.6 , 17.325, 29.175, 14.95 , 40.775, 19.55 , 12.175, 23.675, 10.775, 22.1 , ... 15.575, 20.5 , 22.775, 30.725, 18.975, 16.45 , 22.05 , 18.925])&quot;&quot;&quot;from sklearn.metrics import r2_scorer2_score(y_test, forest_predict)&quot;&quot;&quot;0.7840500839091215&quot;&quot;&quot; Entropy: 熵 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npfrom collections import Counterfrom icecream import icfrom functools import lru_cachedef pr(es): counter = Counter(es) def _wrap(e): return counter[e] / len(es) return _wrapdef entropy(elements): # Information Entropy p = pr(elements) return -np.sum(p(e) * np.log(p(e)) for e in set(elements))def gini(elements): p = pr(elements) return 1-np.sum(p(e) ** 2 for e in set(elements)) pure_func = giniic(pure_func([1, 1, 1, 1, 1, 0]))ic(pure_func([1, 1, 1, 1, 1, 1]))ic(pure_func([1, 2, 3, 4, 5, 8]))ic(pure_func([1, 2, 3, 4, 5, 9]))ic(pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'c']))ic(pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'd']))&quot;&quot;&quot;ic| pure_func([1, 1, 1, 1, 1, 0]): 0.2777777777777777ic| pure_func([1, 1, 1, 1, 1, 1]): 0.0ic| pure_func([1, 2, 3, 4, 5, 8]): 0.8333333333333333ic| pure_func([1, 2, 3, 4, 5, 9]): 0.8333333333333333ic| pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'c']): 0.44897959183673464ic| pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'd']): 0.61224489795918370.6122448979591837&quot;&quot;&quot; Random forest 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879from sklearn.datasets import load_bostonfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitimport numpy as npimport pandas as pdfrom sklearn.metrics import r2_scorehouse = load_boston()X = house.datay = house.targetx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)tree_reg = DecisionTreeRegressor()tree_reg.fit(x_train, y_train)print('whole dataset train acc: {}'.format(tree_reg.score(x_train, y_train)))print('whole dataset test acc: {}'.format(tree_reg.score(x_test, y_test)))&quot;&quot;&quot;whole dataset train acc: 1.0whole dataset test acc: 0.6776520888466615&quot;&quot;&quot;def random_forest(train_x, train_y, test_x, test_y, drop_n=4): random_features = np.random.choice(list(train_x.columns), size=len(train_x.columns)-drop_n) sample_x = train_x[random_features] sample_y = train_y reg = DecisionTreeRegressor() reg.fit(sample_x, sample_y) train_score = reg.score(sample_x, sample_y) test_score = reg.score(test_x[random_features], test_y) print('sub sample :: train score: {}, test score: {}'.format(train_score, test_score)) y_predicated = reg.predict(test_x[random_features]) return y_predicated, test_score with_feature_names = pd.DataFrame(X)with_feature_names.columns = house['feature_names']x_train, x_test, y_train, y_test = train_test_split(with_feature_names, y, test_size=0.3, random_state=0)tree_num = 4predicates = []for _ in range(tree_num): predicated, score = random_forest(x_train, y_train, x_test, y_test) predicates.append((predicated, score))&quot;&quot;&quot;sub sample :: train score: 1.0, test score: 0.5640870175410873sub sample :: train score: 1.0, test score: 0.29024437819534354sub sample :: train score: 1.0, test score: 0.37812117132843814sub sample :: train score: 1.0, test score: 0.5650888856735524&quot;&quot;&quot;predicates_value = [v for v, s in predicates]forest_scores = [s for v, s in predicates]print('the score of forest is : {}'.format(r2_score(y_test, np.mean(predicates_value, axis=0))))&quot;&quot;&quot;the score of forest is : 0.680193104551715&quot;&quot;&quot;weights = np.array(forest_scores) / np.sum(forest_scores)weights_score = np.zeros_like(np.mean(predicates_value, axis=0))for i, v in enumerate(predicates_value): weights_score += v * weights[i] print('the score of weighted forest is : {}'.format(r2_score(y_test, weights_score)))&quot;&quot;&quot;the score of weighted forest is : 0.6956613076019385&quot;&quot;&quot; Show SVM 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npimport matplotlib.pyplot as pltlabel_a = np.random.normal(6, 2, size=(50, 2))label_b = np.random.normal(-6, 2, size=(50, 2))plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))label_a_x = label_a[:, 0]label_b_x = label_b[:, 0]def f(x, w, b): return w * x + b k_and_b = []for i in range(100): k, b = (np.random.random(size=(1, 2)) * 10 - 5)[0] if np.max(f(label_a_x, k, b)) &gt;= -1 and np.min(f(label_b_x, k, b)) &gt;= 1: print(k, b) k_and_b.append((k, b))&quot;&quot;&quot;0.17732109082579406 3.9508645615428843-0.8649868307954458 1.7349996177756957...-2.2969567032985783 2.171321001904926&quot;&quot;&quot;for k, b in k_and_b: x = np.concatenate((label_a_x, label_b_x)) plt.plot(x, f(x, k, b)) print(k_and_b)&quot;&quot;&quot;[(0.17732109082579406, 3.9508645615428843), (-0.8649868307954458, 1.7349996177756957), (-0.818317924604357, 0.352843348193578), (-0.19730603224472976, 4.002168852007262), ...(-2.2969567032985783, 2.171321001904926)]&quot;&quot;&quot;w, b = min(k_and_b, key=lambda k_b: k_b[0])all_x = np.concatenate((label_a_x, label_b_x))plt.plot(all_x, f(all_x, w, b), 'r-o')plt.show() Integrated learning Ensemble learning is a machine learning paradigm that solves the same problem by training multiple models. In contrast to ordinary machine learning methods that try to learn a hypothesis from training data, ensemble methods try to construct a set of hypotheses and use them in combination. Next, we will use the decision tree and its integrated version to model the classic data set Mnist and observe the differences in different integration methods. 123456789!ls!unzip mnist_test.csv.zip &amp;&amp; unzip mnist_train.csv.zipimport numpy as npimport pandas as pdfrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier Build a data set The Mnist data set used this time is not in the original format. In order to more easily adapt to this training, the 28 * 28 pictures in the original data set are flatten operation, it becomes 784 features, the columns in the DataFrame below: 1x1, 1x2, ..., 28x28, representing the i row and j column in the picture The pixel value of is a grayscale image, so the pixel value is only 0 and 1 1234567891011train_df = df = pd.read_csv('~/data/mnist_train.csv')train_df.head()&quot;&quot;&quot; label 1x1 1x2 1x3 1x4 1x5 1x6 1x7 1x8 1x9 ... 28x19 28x20 28x21 28x22 28x23 28x24 28x25 28x26 28x27 28x280 5 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 01 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 02 4 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 03 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 04 9 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 05 rows × 785 columns&quot;&quot;&quot; View training data information:, whether there is NaN, how many pieces of data are there... 1234567891011121314151617181920212223242526272829train_df.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 60000 entries, 0 to 59999Columns: 785 entries, label to 28x28dtypes: int64(785)memory usage: 359.3 MB&quot;&quot;&quot;test_df = df = pd.read_csv('~/data/mnist_test.csv')test_df.head()&quot;&quot;&quot; label 1x1 1x2 1x3 1x4 1x5 1x6 1x7 1x8 1x9 ... 28x19 28x20 28x21 28x22 28x23 28x24 28x25 28x26 28x27 28x280 7 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 01 2 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 02 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 03 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 04 4 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 05 rows × 785 columns&quot;&quot;&quot;test_df.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 10000 entries, 0 to 9999Columns: 785 entries, label to 28x28dtypes: int64(785)memory usage: 59.9 MB&quot;&quot;&quot; Build training and test data 12345678910X_train = train_df.iloc[:, 1:]y_train = train_df.iloc[:, 0]X_test = test_df.iloc[:, 1:]y_test = test_df.iloc[:, 0](X_train.shape, y_train.shape), (X_test.shape, y_test.shape)&quot;&quot;&quot;(((60000, 784), (60000,)), ((10000, 784), (10000,)))&quot;&quot;&quot; Decision Tree First train a simple decision tree to see how it performs 1234567891011121314151617181920dtc = DecisionTreeClassifier()dtc.fit(X_train, y_train)dtc.score(X_train, y_train)&quot;&quot;&quot;1.0&quot;&quot;&quot;dtc.score(X_test, y_test)&quot;&quot;&quot;0.8753&quot;&quot;&quot;dtc = DecisionTreeClassifier(min_samples_leaf=8)dtc.fit(X_train, y_train)dtc.score(X_train, y_train), dtc.score(X_test, y_test)&quot;&quot;&quot;(0.9311666666666667, 0.8795)&quot;&quot;&quot; From the above results, we can see that by adjusting the parameter min_samples_leaf, the overfitting situation has been alleviated. What does this parameter mean? Why increasing it can alleviate the overfitting problem? The meaning of min_samples_leaf is the minimum number of samples contained in the leaf nodes of the decision tree. By increasing this parameter, the decision tree can not capture any of the subtle features of the training data during training, resulting in excessive training data. Fitting: The large number of samples of leaf nodes can also play a role in voting and enhance the generalization performance of the model. You can try to continue to increase the value of this parameter and try to find the best parameter. In addition to this parameter, you can also try to adjust the parameters such as min_samples_split and max_features. For the specific meaning, please refer to sklearn documentation Second question: Try to adjust other parameters to see the performance of the decision tree on the test set Random Forest Take a look at the bagging version of the decision tree and how the random forest performs! 1234567rfc = RandomForestClassifier(n_estimators = 10)rfc.fit(X_train, y_train)rfc.score(X_train, y_train), rfc.score(X_test, y_test)&quot;&quot;&quot;(0.99905, 0.9513)&quot;&quot;&quot; It is worthy of the integrated version. It basically achieves better performance under the default parameters. The accuracy of the test set is about 7% higher than that of the ordinary decision tree. However, comparing the training and test results, it can be found that there is still a certain degree of overfitting. , Try to adjust some parameters below 1234567rfc = RandomForestClassifier(n_estimators = 20)rfc.fit(X_train, y_train)rfc.score(X_train, y_train), rfc.score(X_test, y_test)&quot;&quot;&quot;(0.9999, 0.96)&quot;&quot;&quot; After increasing the parameter n_estimators, the accuracy of the test set has increased by about 1%. The meaning of this parameter is to train 20 decision trees at the same time, and finally integrate the results. The increase of this parameter can be simply regarded as voting The number of people increases, so the final result will inevitably be more robust. You can try to continue to increase this parameter, or adjust other parameters such as max_samples, appropriately less than the total amount of training data, which can increase the difference between different sub-models and further improve the generalization performance. It can also adjust the parameters of the base learner (decision tree). For the meaning of the parameters, see sklearn documentation GBDT Let's compare the performance of the boosting version of the decision tree GBDT! 1234567gbc = GradientBoostingClassifier(n_estimators=10)gbc.fit(X_train, y_train)gbc.score(X_train, y_train), gbc.score(X_test, y_test)&quot;&quot;&quot;(0.8423, 0.846)&quot;&quot;&quot; As expected, the performance has been greatly improved, and the indicators of the training set are basically the same as those of the test set, and there is no overfitting, so it should be possible to continue to try to improve this parameter. Generally, in the absence of overfitting, we only need to consider continuing to increase the complexity of the model. This is the fastest way to improve performance. When the complexity of the model increases to the point of over-fitting, we then consider using some methods to reduce over-fitting. Bagging The aforementioned random forest and GBDT are ensemble learning algorithms based on decision trees, but it should be noted that ensemble learning is not exclusive to decision trees. Any other learner can be used as a base learner for ensemble learning, such as Logistic regression, support vector machine. Bagging is short for \"bootstrap aggregating\". This is a meta-algorithm, which takes M sub-samples (with replacement) from the initial data set, and trains the prediction model on these sub-samples. The final model is obtained by averaging all sub-models, which usually produces better results. The main advantage of this technique is that it combines regularization, all you need to do is choose good parameters for the base learner. The following uses the general api provided by sklearn to construct an integrated learning algorithm 12345678# Still use decision tree as base learnerbgc = BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=1.0, n_estimators=20)bgc.fit(X_train, y_train)bgc.score(X_train, y_train), bgc.score(X_test, y_test)&quot;&quot;&quot;(0.9935166666666667, 0.9506)&quot;&quot;&quot; Third question Logistic regression as a base learner 1234567bgc = BaggingClassifier(LogisticRegression(max_iter = 500), max_samples=0.5, max_features=1.0, n_estimators=20)bgc.fit(X_train, y_train)bgc.score(X_train, y_train), bgc.score(X_test, y_test)&quot;&quot;&quot;(0.9421166666666667, 0.9228)&quot;&quot;&quot; Above we have successfully used logistic regression as the base learner to complete integrated learning. You can try to use only logistic regression for training, and compare the performance of the single model with the bagging version of logistic regression. Boosting Boosting refers to a series of algorithms that can transform a weak learner into a strong learner. The main principle of boosting is to combine a series of weak learners (only better than random guessing). For those samples that were misclassified in the early stages of training, the boosting algorithm will give more attention. Then combine the predictions by weighted majority voting (classification) or weighted sum (regression) to produce the final prediction. 123456abc = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=10, learning_rate=0.01)abc.fit(X_train, y_train)abc.score(X_train, y_train), abc.score(X_test, y_test)&quot;&quot;&quot;(1.0, 0.875)&quot;&quot;&quot; Comparing the boosting integrated version of decision tree and logistic regression, we can find that logistic regression has better generalization ability, and decision tree is easier to overfit 123456abc = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=8), n_estimators=10, learning_rate=0.01)abc.fit(X_train, y_train)abc.score(X_train, y_train), abc.score(X_test, y_test)&quot;&quot;&quot;(0.9981833333333333, 0.9532)&quot;&quot;&quot; In fact, over-fitting is not a bad thing. If your model cannot be over-fitted, it means that it cannot fit the training data well. Therefore, the decision tree is very over-fitted at the beginning, which also shows its potential. , You can see that after the above parameters are adjusted, the boosting version of the decision tree easily exceeds the boosting version of the logistic regression","link":"/example_06/"},{"title":"RNN","text":"Simple RNN Define function Import the required libraries 12345678import ioimport osimport unicodedataimport stringimport globimport torchimport random 123# alphabet small + capital letters + &quot;.,;'&quot;ALL_LETTERS = string.ascii_letters + &quot;.,;'&quot;N_LETTERS = len(ALL_LETTERS) Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427 123456def unicode_to_ascii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in ALL_LETTERS ) 123456789101112131415161718192021def load_data(): # Build the category_lines dictionary, a list of names per language category_lines = {} all_categories = [] def find_files(path): return glob.glob(path) # Read a file and split into lines def read_lines(filename): lines = io.open(filename, encoding = 'utf-8').read().strip().split('\\n') return [unicode_to_ascii(line) for line in lines] for filename in find_files('~/data/course_data/names/*.txt'): category = os.path.splitext(os.path.basename(filename))[0] all_categories.append(category) lines = read_lines(filename) category_lines[category] = lines return category_lines, all_categories To represent a single letter, we use a “one-hot vector” of size &lt;1 x n_letters&gt;. A one-hot vector is filled with 0s except for a 1 at index of the current letter, e.g. \"b\" = &lt;0 1 0 0 0 ...&gt;. To make a word we join a bunch of those into a 2D matrix &lt;line_length x 1 x n_letters&gt;. That extra 1 dimension is because PyTorch assumes everything is in batches - we’re just using a batch size of 1 here. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# Find letter index from all_letters, e.g. &quot;a&quot; = 0def letter_to_index(letter): return ALL_LETTERS.find(letter) # Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensordef letter_to_tensor(letter): tensor = torch.zeros(1, N_LETTERS) tensor[0][letter_to_index(letter)] = 1 return tensor # Turn a line into a &lt;line_length x 1 x n_letters&gt;,# or an array of one-hot letter vectorsdef line_to_tensor(line): tensor = torch.zeros(len(line), 1, N_LETTERS) for i, letter in enumerate(line): tensor[i][0][letter_to_index(letter)] = 1 return tensor def random_training_example(category_lines, all_categories): def random_choice(a): random_idx = random.randint(0, len(a) - 1) return a[random_idx] category = random_choice(all_categories) line = random_choice(category_lines[category]) category_tensor = torch.tensor([all_categories.index(category)], dtype = torch.long) line_tensor = line_to_tensor(line) return category, line, category_tensor, line_tensor if __name__ == '__main__': print(ALL_LETTERS) print(unicode_to_ascii('Ślusàrski')) category_lines, all_categories = load_data() print(category_lines['Italian'][:5]) print(letter_to_tensor('J')) # [1, 57] print(line_to_tensor('Jones').size()) # [5, 1, 57] &quot;&quot;&quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,;'Slusarski['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])torch.Size([5, 1, 56])&quot;&quot;&quot; Second Example 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# Import the required librariesimport torchimport torch.nn as nnimport matplotlib.pyplot as pltclass RNN(nn.Module): # implement RNN from scratch rather than using nn.RNN def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(input_size + hidden_size, hidden_size) self.i2o = nn.Linear(input_size + hidden_size, output_size) self.softmax = nn.LogSoftmax(dim = 1) def forward(self, input_tensor, hidden_tensor): combined = torch.cat((input_tensor, hidden_tensor), 1) hidden = self.i2h(combined) output = self.i2o(combined) output = self.softmax(output) return output, hidden def init_hidden(self): return torch.zeros(1, self.hidden_size)category_lines, all_categories = load_data()n_categories = len(all_categories)n_hidden = 128rnn = RNN(N_LETTERS, n_hidden, n_categories)# one stepinput_tensor = letter_to_tensor('A')hidden_tensor = rnn.init_hidden()output, next_hidden = rnn(input_tensor, hidden_tensor)print(output.size())print(next_hidden.size())&quot;&quot;&quot;torch.Size([1, 18])torch.Size([1, 128])&quot;&quot;&quot;# whole sequence/nameinput_tensor = line_to_tensor('Albert')hidden_tensor = rnn.init_hidden()output, next_hidden = rnn(input_tensor[0], hidden_tensor)print(output.size())print(next_hidden.size())&quot;&quot;&quot;torch.Size([1, 18])torch.Size([1, 128])&quot;&quot;&quot;def category_from_output(output): category_idx = torch.argmax(output).item() return all_categories[category_idx] print(category_from_output(output))&quot;&quot;&quot;German&quot;&quot;&quot;criterion = nn.NLLLoss()learning_rate = 0.005optimizer = torch.optim.SGD(rnn.parameters(), lr = learning_rate)def train(line_to_tensor, category_tensor): hidden = rnn.init_hidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_to_tensor[i], hidden) loss = criterion(output, category_tensor) optimizer.zero_grad() loss.backward() optimizer.step() return output, loss.item() current_loss = 0all_losses = []plot_steps, print_steps = 1000, 5000n_iters = 100000for i in range(n_iters): category, line, category_tensor, line_tensor = random_training_example(category_lines, all_categories) output, loss = train(line_tensor, category_tensor) current_loss += loss if (i + 1) % plot_steps == 0: all_losses.append(current_loss / plot_steps) current_loss = 0 if (i + 1) % print_steps == 0: guess = category_from_output(output) corrent = 'CORRECT' if guess == category else f'WRONG ({category})' print(f'{i+1} {(i+1) / n_iters *100} {loss:.4f} {line} / {guess} {corrent}') &quot;&quot;&quot;5000 5.0 2.5063 Bureau / Scottish WRONG (French)10000 10.0 1.4726 Bitar / Arabic CORRECT15000 15.0 1.9405 Bazilevitch / Russian CORRECT20000 20.0 1.5565 Dupont / French CORRECT25000 25.0 0.1202 Majewski / Polish CORRECT30000 30.0 1.1579 Kucharova / Czech CORRECT35000 35.0 1.0075 Sheng / Chinese CORRECT40000 40.0 0.8343 Masih / Arabic CORRECT45000 45.0 0.5371 Fan / Chinese CORRECT50000 50.0 0.3260 Vinh / Vietnamese CORRECT55000 55.00000000000001 2.5464 Pahlke / Polish WRONG (German)60000 60.0 1.5921 Clark / Scottish CORRECT65000 65.0 4.3648 Paulis / Greek WRONG (Dutch)70000 70.0 1.3289 Thian / Vietnamese WRONG (Chinese)75000 75.0 2.2715 Kelly / English WRONG (Irish)80000 80.0 1.0069 Siu / Korean WRONG (Chinese)85000 85.0 0.8168 Kan / Chinese CORRECT90000 90.0 0.2283 Dinh / Vietnamese CORRECT95000 95.0 2.0048 Abbascia / Japanese WRONG (Italian)100000 100.0 0.6310 O'Shea / Irish CORRECT&quot;&quot;&quot;plt.figure()plt.plot(all_losses)plt.show() 12345678910111213141516171819202122232425262728293031def predict(input_line): print(f'\\n &gt; {input_line}') with torch.no_grad(): line_tensor = line_to_tensor(input_line) hidden = rnn.init_hidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) guess = category_from_output(output) print(guess) while True: sentence = input('Input: ') if sentence == 'quit': break predict(sentence)&quot;&quot;&quot; &gt; ChineseIrish &gt; EnglishEnglish &gt; JapaneseFrench &gt; FrenchGerman&quot;&quot;&quot; LSTM Modeling trigonometric functions Use LSTM to fit sine and cosine functions Use numpy to build time series data based on sine function Use keras to build a simple regression network, mainly using the LSTM network structure to fit the periodicity of the sine function, and visualize the fitted sine function image and the real function image Related knowledge points Time series data construction and forecasting Time series model building, training, evaluation and visualization based on keras LSTM 123456789101112131415161718# Import necessary libraries# Build dataimport numpy as np# Build a modelfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Inputfrom tensorflow.keras.layers import LSTMfrom tensorflow.keras.layers import Dense# Printing progress barfrom tqdm import tqdm# Visualizationimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline 1. Construct a data set This module will use numpy to construct time series data. There are two main steps: Define the sine function (cosine function) Select historical data window size to construct time series data 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def ground_func(x): &quot;&quot;&quot; sine / cosine function Args: x: numpy.ndarray return: sin(x) or cos(x) &quot;&quot;&quot; y = np.sin(x) return ydef build_data(sequence_data, n_steps): &quot;&quot;&quot; Use sine function data to build X, y Args: sine_data: numpy.ndarray n_steps: history data window size return: X: numpy.ndarray, y: numpy.ndarray &quot;&quot;&quot; # init X, y = [], [] seq_len = len(sequence_data) for start_idx in tqdm(range(seq_len), total=seq_len): end_idx = start_idx + n_steps if end_idx &gt;= seq_len: break cur_x = sequence_data[start_idx: end_idx] cur_y = sequence_data[end_idx] X.append(cur_x) y.append(cur_y) X = np.array(X) y = np.array(y) X = X.reshape(*X.shape, 1) return X, y # Construct the original sine/cosine function sequencexaxis = np.arange(-50 * np.pi, 50 * np.pi, 0.1)sequence_data = ground_func(xaxis)len(sequence_data)# Take 1000 data for visualizationplt.figure(figsize = (20, 8))plt.plot(xaxis[:1000], sequence_data[:1000]) 1234567n_steps = 20X, y = build_data(sequence_data, n_steps)X.shape, y.shape&quot;&quot;&quot; 99%|█████████▉| 3122/3142 [00:00&lt;00:00, 1557955.63it/s]((3122, 20, 1), (3122,))&quot;&quot;&quot; 2. Build the model This module builds a timing model based on the LSTM and Dense layer in keras. The following points need to be noted: 1. Choose the right hidden size 2. Choose a suitable activation function, such as relu, tanh 3. The optimizer chooses sgd, adam, etc. 3. The loss function chooses cross entropy loss function (cross_entropy) or mean square error (mse), etc. 123456789101112131415161718192021222324252627282930313233343536def create_model(): &quot;&quot;&quot; Build a LSTM model fit sine/cosine function. hints: 1. a LSTM fit time pattern (ref: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) 2. a Dense for regression (ref: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) &quot;&quot;&quot; model = Sequential() model.add(Input(shape = (20, 1))) model.add(LSTM(32, activation='tanh')) model.add(Dense(1, activation='tanh')) model.compile(optimizer = 'adam', loss = 'mse') return model# Initialize the model and print related informationmodel = create_model()model.summary()&quot;&quot;&quot;Instructions for updating:Call initializer instance with the dtype argument instead of passing it to the constructorModel: &quot;sequential&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================lstm (LSTM) (None, 32) 4352 _________________________________________________________________dense (Dense) (None, 1) 33 =================================================================Total params: 4,385Trainable params: 4,385Non-trainable params: 0_________________________________________________________________&quot;&quot;&quot; 3. Model training 12345678910111213141516# Try to change epochs and add callbacks, such as EarlyStopping (https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)history = model.fit(X, y, batch_size = 32, epochs = 25, verbose = 1)plt.plot(history.history['loss'], label='loss')plt.legend(loc ='upper right') # draw the loss image&quot;&quot;&quot;Instructions for updating:Use tf.where in 2.0, which has the same broadcast rule as np.whereEpoch 1/253122/3122 [==============================] - 4s 1ms/sample - loss: 0.1433Epoch 2/253122/3122 [==============================] - 3s 879us/sample - loss: 0.0072show more (open the raw output data in a text editor) ...Epoch 25/253122/3122 [==============================] - 3s 858us/sample - loss: 2.2191e-05&quot;&quot;&quot; 4. Forecast This module uses a function different from the training data to construct test data to verify the generalization performance of the model. The main steps are as follows: 1. Define a new function (sine/cosine) 2. Use the trained model to make predictions 3. Visually compare model prediction results with real values 123456789101112131415161718192021222324252627282930313233343536def test_func(x): &quot;&quot;&quot; sine/cosine function, different from ground_func above. Args: x: numpy.ndarray return: sin(x) or cos(x) &quot;&quot;&quot; y = np.cos(x) return y test_xaxis = np.arange(0, 10 * np.pi, 0.1)test_sequence_data = test_func(test_xaxis)# Use the initial n_steps of historical data to start forecasting, and the subsequent data will use the predicted data as historical data for further forecastingy_preds = test_sequence_data[:n_steps]# Step by step forecastfor i in tqdm(range(len(test_xaxis)-n_steps)): model_input = y_preds[i: i+n_steps] model_input = model_input.reshape((1, n_steps, 1)) y_pred = model.predict(model_input, verbose = 0) y_pred = np.append(y_preds, y_pred)plt.figure(figsize = (10,8))plt.plot(test_xaxis[n_steps:], y_preds[n_steps:], label ='predictions')plt.plot(test_xaxis, test_sequence_data, label ='ground truth')plt.plot(test_xaxis[:n_steps], y_preds[:n_steps], label ='initial sequence', color ='red')plt.legend(loc ='upper left')plt.ylim(-2,2)plt.show()&quot;&quot;&quot;100%|██████████| 295/295 [00:01&lt;00:00, 183.91it/s]&quot;&quot;&quot; Recurrent Neural Networks source 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import pandas as pd# load datatimeserise_revenue = pd.read_csv('~/data/course_data/time_serise_revenue.csv')sales_data = pd.read_csv('~/data/course_data/time_serise_sale.csv')timeserise_revenue.head()&quot;&quot;&quot; Unnamed: 0 day_1 day_2 day_3 day_4 day_5 day_6 day_7 day_8 day_9 ... day_51 day_52 day_53 day_54 day_55 day_56 day_57 day_58 day_59 day_600 0 2.622866 2.657832 2.771121 2.815845 2.876267 2.859229 2.844758 2.793797 2.736443 ... 1.228701 1.290414 1.474886 1.563295 1.736197 1.797285 1.978940 2.198979 2.277908 2.403300...4 4 1.702631 1.825995 2.038047 2.194083 2.313903 2.417883 2.567613 2.650782 2.729691 ... 1.258760 1.137150 1.109007 1.104999 1.150137 1.204513 1.221350 1.327023 1.387304 1.5573635 rows × 61 columns&quot;&quot;&quot;def sample_from_table(sample_size, dataframe): sample_row = dataframe.sample().values[0] begin_column = random.randint(0, len(sample_row) - sample_size - 1) return (sample_row[begin_column: begin_column + sample_size], sample_row[begin_column + 1: begin_column + sample_size + 1]) import torchimport torch.nn as nnfrom torch.nn import functional as Ffrom torch.autograd import Variablefrom torch import optimimport numpy as npimport math, randomimport matplotlib.pyplot as pltimport seaborn as sns# Generating a noisy multi-sin waveclass FullyConnected(nn.Module): def __init__(self, x_size, hidden_size, output_size): super(FullyConnected, self).__init__() self.hidden_size = hidden_size self.linear_with_tanh = nn.Sequential( nn.Linear(10, self.hidden_size), nn.Tanh(), nn.Linear(self.hidden_size, self.hidden_size), nn.Tanh(), nn.Linear(self.hidden_size, output_size) ) def forward(self, x): yhat = self.linear_with_tanh(x) return yhat class SimpleRNN(nn.Module): def __init__(self, x_size, hidden_size, n_layers, batch_size, output_size): super(SimpleRNN, self).__init__() self.hidden_size = hidden_size self.n_layers = n_layers self.batch_size = batch_size # self.inp = nn.Linear(1, hidden_size) self.rnn = nn.RNN(x_size, hidden_size, n_layers, batch_first=True) self.out = nn.Linear(hidden_size, output_size) # 10 in and 10 out def forward(self, inputs, hidden=None): hidden = self.__init__hidden() # print('Forward hidden {}'.format(hidden.shape)) # print('Forward inps {}'.format(inputs.shape)) output, hidden = self.rnn(inputs.float(), hidden.float()) # print('Out1 {}'.format(output.shape)) output = self.out(output.float()) # print('Forward outputs {}'.format(output.shape)) return output, hidden def __init__hidden(self): hidden = torch.zeros(self.n_layers, self.batch_size, self.hidden_size, dtype = torch.float64) return hidden # Set datasetsource_data = sales_data# Fully Connected Modeln_epochs = 100n_iters= 50hidden_size = 2 # try to change this parametersn_layers = 2batch_size = 5seq_length = 10n_sample_size = 50x_size = 1fc_model = FullyConnected(x_size, hidden_size, output_size = seq_length)fc_model = fc_model.double()criterion = nn.MSELoss()optimizer = optim.SGD(fc_model.parameters(), lr = 0.01)losses = np.zeros(n_epochs)plt.imshow(fc_model.state_dict()['linear_with_tanh.0.weight'])plt.show() 1234567891011121314151617181920212223242526272829303132333435for epoch in range(n_epochs): for iter_ in range(n_iters): _inputs, _targets = sample_from_table(n_sample_size, source_data) inputs = Variable(torch.from_numpy(np.array([_inputs[0:10], _inputs[10:20], _inputs[20:30], _inputs[30:40], _inputs[40:50]], dtype = np.double))) targets = Variable(torch.from_numpy(np.array([_targets[0:10], _targets[10:20], _targets[20:30], _targets[30:40], _targets[40:50]], dtype = np.double))) outputs = fc_model(inputs.double()) optimizer.zero_grad() loss = criterion(outputs, targets) loss.backward() optimizer.step() losses[epoch] += loss if iter_ % 10 == 0: plt.clf() plt.ion() plt.title('Epoch {}, iter {}'.format(epoch, iter_)) plt.plot(torch.flatten(outputs.detach()), 'r-', linewidth = 1, label = 'Output') plt.plot(torch.flatten(targets), 'c-', linewidth = 1, label = 'Label') plt.plot(torch.flatten(inputs), 'g-', linewidth = 1, label = 'Input') plt.draw() plt.pause(0.05) A total of 5 * 99 pictures were rendered in the middle, so I won’t show them one by one. RNN Model 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263n_epochs = 100n_iters = 50hidden_size = 2 # try to change this parametersn_layers = 2batch_size = 5seq_length = 10n_sample_size = 50x_size = 1output_size = 1rnn_model = SimpleRNN(x_size, hidden_size, n_layers, int(n_sample_size / seq_length), output_size)criterion = nn.MSELoss()optimizer = optim.SGD(rnn_model.parameters(), lr = 0.01)losses = np.zeros(n_epochs)for epoch in range(n_epochs): for iter in range(n_iters): _inputs, _targets = sample_from_table(n_sample_size, source_data) inputs = Variable(torch.from_numpy(np.array([_inputs[0:10], _inputs[10:20], _inputs[20:30], _inputs[30:40], _inputs[40:50]], dtype = np.double)).unsqueeze(2)) targets = Variable(torch.from_numpy(np.array([_targets[0:10], _targets[10:20], _targets[20:30], _targets[30:40], _targets[40:50]], dtype = np.double)).unsqueeze(2).float()) # [49] # print('Inputs {}, targets {}'.format(inputs.shape, targets.shape)) # Use teacher forcing 50% of the time # force = random.random() &lt; 0.5 outputs, hidden = rnn_model(inputs.double(), None) optimizer.zero_grad() loss = criterion(outputs, targets) loss.backward() optimizer.step() losses[epoch] += loss if iter % 10 ==0: plt.clf() plt.ion() plt.title('Epoch {}, iter {}'.format(epoch, iter)) plt.plot(torch.flatten(outputs.detach()), 'r-', linewidth = 1, label = 'Output') plt.plot(torch.flatten(targets), 'c-', linewidth = 1, label = 'Label') plt.plot(torch.flatten(inputs), 'g-', linewidth = 1, label = 'Input') plt.draw() plt.pause(0.05)# if epoch &gt; 0:# print(epoch, loss) A total of 5 * 99 pictures were rendered in the middle, so I won’t show them one by one. 12plt.plot(losses[20:])plt.show()","link":"/example_08/"},{"title":"Advanced Deep Learning","text":"Different optimer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npimport torchx = np.random.random(size=(100, 8))linear = torch.nn.Linear(in_features=8, out_features=1)sigmoid = torch.nn.Sigmoid()linear2 = torch.nn.Linear(in_features=1, out_features=1)model = torch.nn.Sequential(linear, sigmoid, linear2).double()train_x = torch.from_numpy(x)print(model(train_x).shape)yture = torch.from_numpy(np.random.uniform(0, 5, size=(100, 1)))# print(x)print(yture.shape)&quot;&quot;&quot;torch.Size([100, 1])torch.Size([100, 1])&quot;&quot;&quot;loss_fn = torch.nn.MSELoss()optimer = torch.optim.SGD(model.parameters(), lr=1e-5)for e in range(100): for b in range(100 // 1): # stochastic gradient descent # for b in range(100 // 10): # mini-batch gradient descent # for b in range(100 // 100): # batch gradient descent batch_index = np.random.choice(range(len(train_x)), size=20) yhat = model(train_x[batch_index]) loss = loss_fn(yhat, yture[batch_index]) loss.backward() print(loss) optimer.step()&quot;&quot;&quot;tensor(5.0873, dtype=torch.float64, grad_fn=&lt;MseLossBackward&gt;)tensor(3.4337, dtype=torch.float64, grad_fn=&lt;MseLossBackward&gt;)show more (open the raw output data in a text editor) ...tensor(2.1481, dtype=torch.float64, grad_fn=&lt;MseLossBackward&gt;)&quot;&quot;&quot; Matrix dimension 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from torch import nnimport torchimport numpy as npx = torch.from_numpy(np.random.random(size=(4, 10)))print(x.shape)&quot;&quot;&quot;torch.Size([4, 10])&quot;&quot;&quot;model = nn.Sequential( nn.Linear(in_features=10, out_features=5).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Softmax())ytrue = torch.randint(8, (4, ))print(ytrue)&quot;&quot;&quot;tensor([4, 0, 7, 7])&quot;&quot;&quot;loss_fn = nn.CrossEntropyLoss()print(model(x).shape)print(ytrue.shape)loss = loss_fn(model(x), ytrue)print(torch.randint(5, (3, )))loss.backward()for p in model.parameters(): print(p, p.grad) Advanced deep learning 123456789101112131415# Basic computing libraryimport numpy as np# Deep learning libraryimport torchimport torch.nn as nnimport torch.optim as optimimport torchvisionimport torch.nn.functional as Fimport torchvision.transforms as transforms# Auxiliary drawing galleryimport matplotlib.pyplot as plt# Time operation libraryimport time# Progress bar control libraryfrom tqdm import tqdm Project 1: Forward propagation of simple neural network Question 1: Define the initial parameters and activation function You need to use numpy to implement the forward propagation process of the neural network and calculate the final output result of the output layer. In order to complete the above tasks, we need to make the following assumptions: 1. The value entered is [3,5] 1. The two weights of the hidden layer h1 are [2,4], [4,-5] 1. The two weights of the hidden layer h2 are [-1,1], [2,2] 1. The weight of the output layer is [-3,7] 1. All layers do not use bias 1. All hidden layers need to add tanh activation function 12345678910111213141516# TODO: Define a numpy array with the input data of the neural network:input_data = np.array([3, 5])# TODO: Define a numpy array with the content of the hidden layer and output layer weights of the neural network:# Tips: The weight dictionary has been built, you only need to fill in the corresponding value according to the hidden layer nameweights = {'h11': np.array([2, 4]), 'h12': np.array([4, -5]), 'h21': np.array([-1, 1]), 'h22': np.array([2, 2]), 'out': np.array([-3, 7])}# TODO: Improve the following tanh activation function:def tanh(x): return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) Question 2: Calculate the neural network output layer by layer In the calculation of the neural network, it is necessary to first multiply the weight of the layer to be calculated with its input data, and then sum, and then through the operation of the activation function, it can be output to the next layer. Below we will use the layer as the unit to perform calculations: The first is the first hidden layer. You need to multiply, sum, and input the data of the input layer and the weight of the hidden layer into the activation function. 123456789101112131415161718print(input_data * weights['h11'])a = tanh(input_data * weights['h11']).sum()b = tanh((input_data * weights['h11']).sum())print(a,b)&quot;&quot;&quot;[ 6 20]1.9999877116507956 1.0&quot;&quot;&quot;# TODO: multiply, sum, and input the data of the input layer and the weight of the first hidden layer into the activation function.hidden_11_value = tanh(input_data * weights['h11']).sum()hidden_12_value = tanh(input_data * weights['h12']).sum()hidden_1_output = np.array([hidden_11_value, hidden_12_value])&quot;&quot;&quot;1.9999877116507956-7.550282621338056e-11[ 1.99998771e+00 -7.55028262e-11]&quot;&quot;&quot; Next is the second hidden layer, the operation of this layer is exactly the same as the previous layer. 12345# TODO: multiply, sum, and input the data output by the upper layer and the weight of the second hidden layer into the activation function.hidden_21_value = tanh(hidden_1_output * weights['h21']).sum()hidden_22_value = tanh(hidden_1_output * weights['h22']).sum()hidden_2_output = np.array([hidden_21_value, hidden_22_value]) Finally, there is the output layer. At this time, there is only one node that needs to be calculated, and there is no need to add an activation function. 12# TODO: multiply and sum the data output by the upper layer and the weight of the output layeroutput = (hidden_2_output * weights['out']).sum() At this point, you have completed all the calculations. Now let's print out the output of these layers and have a look. 1234print(output)&quot;&quot;&quot;9.887385002294863&quot;&quot;&quot; Project 2: CIFAR-10 Image Classification Preparation The data set used in this project can be directly exported from the torchvision library. Here are some basic data operations (data download may take a few minutes, please be patient). 1234567891011121314151617181920212223242526272829##Define various transformation operations on the image, including converting the array to tensor, and regularizing the image#transforms.Compose is mainly used for some common graphics transformations, such as cropping and rotation#Traverse the list array and perform each transforms operation on the img in turntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.48216, 0.44653), (0.24703, 0.24349, 0.26159))))#Export the CIFAR10 data set in torchvision. The root is the directory where the data is stored after downloading. The train controls whether it is in the training phase, the download controls whether it needs to be downloaded, and the transform passes in a series of image transformations.trainset = torchvision.datasets.CIFAR10(root='~/data/course_data/', train=True, download=True, transform=transform)testset = torchvision.datasets.CIFAR10(root='~/data/course_data/', train=False, download=True, transform=transform)#Used to divide the training data into multiple groups, this function throws a group of data each time.trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)#Used to divide the test data into multiple groups, this function throws a group of data each time.testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False)&quot;&quot;&quot;Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ~/data/course_data/cifar-10-python.tar.gz170499072it [02:24, 1181561.38it/s] Extracting ~/data/course_data/cifar-10-python.tar.gz to ~/data/course_data/Files already downloaded and verified&quot;&quot;&quot; After the data download is complete, we can simply check the data label to see if it is correct with the data set in the exercise description. 12345678910111213trainset.classes&quot;&quot;&quot;['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']&quot;&quot;&quot; Let's check the data image again. 123456789101112131415161718192021222324252627282930#Display the pictures visually#Define drawing functiondef imshow(inp, title = None): &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot; # Define the canvas for drawing fig = plt.figure(figsize = (30, 30)) # Convert the dimensions of the picture inp = inp.numpy().transpose((1,2,0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) # Standardize the picture inp = std * inp + mean # The value of the entire image array is limited to the specified value a_min, and a_max inp = np.clip(inp, 0, 1) # Visual display of pictures plt.imshow(inp,)# Get a batch of datainputs, classes = next(iter(trainloader))# Display in grid format, the function is to combine several images into one imageout = torchvision.utils.make_grid(inputs)# plt.imshow() can display the picture and also display its formatimshow(out, title = [trainset.classes[x] for x in classes]) Question 1: Build a simple neural network After the data is ready, you need to build a simple neural network. 12345678910111213141516# TODO: define a layer 3 fully connected neural network, the input dimension is 32*32*3, the output dimension of the first layer is 1000, the output dimension of the second layer is 500, and the output dimension of the third layer is 10class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(32*32*3, 1000) self.fc2 = nn.Linear(1000, 500) self.fc3 = nn.Linear(500, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) return self.fc3(x)# Instantiate the neural network classnet = Net() After the model structure is defined, the loss function and optimizer need to be determined. 12345# Define loss function-cross entropycriterion = nn.CrossEntropyLoss()# Define the optimizer, pass the parameters of the neural network to the optimizer, and define the learning rateoptimizer = optim.Adam(net.parameters(), lr = 3e-4) Question 2: Neural Network Training The main content of the model has been completed, and the training can be carried out below. In the process of model training, the following steps are generally followed: Big for loop-epochs, used to manage a set of data loop training several times Small for loop-step, used to retrieve data from dataloader in batchsize unit Clear the gradient of the optimizer Read in data and label, and perform shape transformation (can be done or not) Run the forward propagation process of the model Generate the final result based on the model output Calculate the loss Calculate the gradient based on the loss Update parameters based on gradient 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# TODO: training modelnum_epochs = 10since = time.time()net.train()for epoch in range(num_epochs): print(f'Epoch {epoch + 1} / {num_epochs}') running_loss = 0.0 running_corrects = 0 # Take out each batch of data in a loop from the trainloader for data in tqdm(trainloader): # TODO: Completion code inputs, labels = data inputs = inputs.view(-1, 32 * 32 * 3) optimizer.zero_grad() outputs = net(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) loss.backward() optimizer.step() # Calculation of the loss function of a batch of data running_loss += loss.item() * inputs.size(0) # Calculation of the accuracy of a batch of data running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / trainloader.dataset.data.shape[0] epoch_acc = running_corrects.double() / trainloader.dataset.data.shape[0] print('train loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc)) print('-' * 20)time_elapsed = time.time()-sinceprint('Trainning complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed% 60))&quot;&quot;&quot;Epoch 1 / 10100%|██████████| 3125/3125 [01:04&lt;00:00, 48.74it/s]train loss: 1.6377 Acc: 0.4185--------------------Epoch 2 / 10100%|██████████| 3125/3125 [01:04&lt;00:00, 48.15it/s]train loss: 1.4254 Acc: 0.4962--------------------Epoch 3 / 10100%|██████████| 3125/3125 [01:06&lt;00:00, 47.29it/s]train loss: 1.3065 Acc: 0.5372--------------------Epoch 4 / 10100%|██████████| 3125/3125 [01:04&lt;00:00, 48.76it/s]train loss: 1.2026 Acc: 0.5729--------------------Epoch 5 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 49.98it/s]train loss: 1.1129 Acc: 0.6033--------------------Epoch 6 / 10100%|██████████| 3125/3125 [01:01&lt;00:00, 51.17it/s]train loss: 1.0252 Acc: 0.6343--------------------Epoch 7 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 49.67it/s]train loss: 0.9373 Acc: 0.6668--------------------Epoch 8 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 49.63it/s]train loss: 0.8545 Acc: 0.6936--------------------Epoch 9 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 50.02it/s]train loss: 0.7770 Acc: 0.7242--------------------Epoch 10 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 50.16it/s]train loss: 0.7020 Acc: 0.7492--------------------Trainning complete in 10m 33s&quot;&quot;&quot; Question 3: Model evaluation After completing the model training, the model needs to be evaluated to verify the accuracy of the model on the test set. Tips: In the model training log, the accuracy acc is also printed, but this is the accuracy of the model on the training set, not the accuracy on the test set. You can observe the accuracy of the training set and the accuracy of the test set to see if there is any difference. 12345678910111213141516# TODO: Complete model evaluationcorrect, total = 0, 0net.eval()for data in tqdm(testloader): inputs, labels = data inputs = inputs.view(-1, 32 * 32 * 3) outputs = net(inputs) _, predicted = torch.max(outputs, 1) total += labels.size(0) correct += (predicted == labels).sum().item()print('The testing set accuracy of the network is: %d %%'% (100 * correct / total))&quot;&quot;&quot;100%|██████████| 625/625 [00:03&lt;00:00, 157.71it/s]The testing set accuracy of the network is: 53 %&quot;&quot;&quot;","link":"/example_07/"},{"title":"CNN","text":"The source code: example_09: CNN CNN Principle 123456789101112import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom struct import unpackfrom torchvision.datasets import MNISTfrom sklearn.linear_model import LogisticRegressionimport torchfrom PIL import Imagefrom torch import nnmnist_dataset_train = MNIST(root = '~/data/course_data', train=True, download = True)mnist_dataset_test = MNIST(root = '~/data/course_data', train=False, download = True) The first machine vision problem: Let the computer automatically distinguish between 0 and 6 123456789101112X_train = mnist_dataset_train.data.numpy()y_train = mnist_dataset_train.targets.numpy()X_test = mnist_dataset_test.data.numpy()y_test = mnist_dataset_test.targets.numpy()&quot;&quot;&quot;Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gzDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ~/data/course_data/MNIST/raw/train-images-idx3-ubyte.gz9913344it [00:02, 4759648.85it/s] ...5120it [00:00, 12492633.21it/s]Extracting ~/data/course_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ~/data/course_data/MNIST/raw&quot;&quot;&quot; Explain CNN principles 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140def conv(image, filter_): # Convolution operation print(image.shape) print(filter_.shape) assert image.shape[-1] == filter_.shape[-1] test_image = image height, width = filter_.shape[0], filter_.shape[1] filter_result = np.zeros(( test_image.shape[0]-height + 1, test_image.shape[1]-width + 1 )) for h in range(test_image.shape[0]-height + 1): for w in range(test_image.shape[1]-width + 1): sub_windows = test_image[h: h + height, w: w + width, :] op = np.sum(np.multiply(sub_windows, filter_)) filter_result[h][w] = op return filter_result# Part 2: Strides&quot;&quot;&quot;Try to modify stride in Conv Function&quot;&quot;&quot;# Part3: Pooling&quot;&quot;&quot;Create a pooling cell for conv&quot;&quot;&quot;# Part4: Volume&quot;&quot;&quot;Create 3-d volume filter&quot;&quot;&quot;# Part5: Fully Connected Layers&quot;&quot;&quot;Create Fully Connected Layer, to flatten&quot;&quot;&quot;# Part6: Cross-Entropy&quot;&quot;&quot;Create Cross-Entropy cell to get loss value&quot;&quot;&quot;# Part7: ResNet&quot;&quot;&quot;Why we need resNet, and its functions&quot;&quot;&quot;class ResBlock(nn.Module): &quot;&quot;&quot; A very basic ResNet unit The unit passed: batch normal The output value retains the original input value, so that our result does not dissipate &quot;&quot;&quot; def __init__(self, n_channel): super(ResBlock, self).__init__() self.conv = nn.Conv2d(n_channel, n_channel, kernel_size = 3, padding=1, bias = False) self.bath_norm = nn.BatchNorm2d(num_features = n_channel) torch.nn.init.constant_(self.bath_norw.weight, 0.5) torch.nn.init.zeros_(self.bath_norm.bias) torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity ='relu') # sum(windows * filter) ==&gt; The larger the windows, the larger the added value, the smaller the windows, the smaller the value def forward(self, x): out = self.conv(x) out = self.conv(out) out = self.bath_norm(out) out = torch.relu(out) return out + x if __name__ == '__main__': image = Image.open('~/data/course_data/doo.jpeg') image_array = np.array(image) plt.imshow(image_array) # Robert 算子 rebert_1_kernel = np.array([ [1, 0], [0, -1] ]) robert_2_kernel = np.array([ [0, 1], [-1, 0] ]) #Sobel 算子 sobel_x_kernel = np.array([ [-1, 0, 1], [-2, 0, 2], [-1, 0, 1] ]) sobel_y_kernel = np.array([ [-1, -2, -1], [0, 0, 0], [1, 2, 1] ]) # Laplacian 算子 laplacian_kernel = np.array([ [0, 1, 0], [1, -4, 1], [0, 1, 0] ]) filters = [ np.array([sobel_x_kernel] * 3), np.array([sobel_y_kernel] * 3), np.array([laplacian_kernel] * 3) ] for i, f in enumerate(filters): print('applying filter: {}'.format(i)) plt.subplot(3, 3, i * 3 + 1) plt.imshow(image_array) filter_result = conv(image_array, f) plt.subplot(3, 3, i * 3 + 2) plt.imshow(filter_result) plt.subplot(3, 3, i * 3 + 3) plt.imshow(filter_result, cmap = 'gray')plt.show()#ResNet&quot;&quot;&quot;applying filter: 0(1931, 1931, 3)(3, 3, 3)applying filter: 1(1931, 1931, 3)(3, 3, 3)applying filter: 2(1931, 1931, 3)(3, 3, 3)&quot;&quot;&quot; Identification codes Train a model to classify and recognize the characters in the verification code, and finally complete the verification code recognition The data set used contains a total of 36 characters from 0-9 and AZ. There are 50 pictures for each character in the training set, and 10 pictures for each character in the verification set. The verification code data set is composed of 4 character pictures taken out randomly. become. Related knowledge points Data Reading Use torch to build, train, and verify models Model prediction and image segmentation analyze Question 1-Establish a character comparison table We can reverse each pair of keys and values by traversing the dictionary and store them in a new dictionary. The sample code is as follows: 1new_dict = {v: k for k, v in old_dict.items()} #### Question 2-Define datasets and dataloader In opencv-python, you can use image = cv2.medianBlur(image, kernel_size) for median filtering. #### Question 3-Define the network structure In torch, the convolution and fully connected layers are defined as follows: 12conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)fc = nn.Linear(in_features, out_features, bias) #### Question 4-Define the model training function The model training process of the torch framework includes operations such as clearing the gradient, forward propagation, calculating the loss, calculating the gradient, and updating the weight, among which: 1. Clear the gradient: the purpose is to eliminate the interference between step and step, that is, use only one batch of data loss to calculate the gradient and update the weight each time. Generally can be placed first or last; 1. Forward propagation: use a batch of data to run the process of forward propagation to generate model output results; 1. Calculate the loss: use the defined loss function, model output results and label to calculate the loss value of a single batch; 1. Calculate the gradient: According to the loss value, calculate the gradient value required in this optimization in the ownership of the model; 1. Update weight: Use the calculated gradient value to update the value of all weights. The sample code of a single process is as follows: 12345&gt;&gt;&gt; optimizer.zero_grad() # Clear the gradient (can also be placed in the last line)&gt;&gt;&gt; output = model(data) # forward propagation&gt;&gt;&gt; loss = loss_fn(output, target) # Calculate loss&gt;&gt;&gt; loss.backward() # Calculate the gradient&gt;&gt;&gt; optimizer.step() # update weight Programming Import the library to be used in this project 123456789101112131415import osimport cv2import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.utils.data import Dataset, DataLoaderimport torchvisionimport torchvision.transforms as transformsimport numpy as npimport pickleimport PILimport matplotlib.pyplot as pltfrom PIL import Imageos.environ['KMP_DUPLICATE_LIB_OK'] = 'True' Understanding the data set Define the data path 123train_data_dir = '~/data/course_data/train_data.bin'val_data_dir = '~/data/course_data/val_data.bin'verification_code_dir = '~/data/course_data/verification_code_data.bin' The data set used is stored in a binary file, and we need to define a function to read the picture in the binary file. 1234def load_file(file_name): with open(file_name, mode ='rb') as f: result = pickle.load(f) return result See what the data set looks like: 123456789101112train_data = load_file(train_data_dir)img_test = list()for i in range(1, 1800, 50): img_test.append(train_data[i][1])plt.figure()for i in range(1, 37): plt.subplot(6, 6, i) plt.imshow(img_test[i-1]) plt.xticks([]) plt.yticks([])plt.show() View single big picture 12345# plt.subplot(6, 6, i)plt.imshow(train_data[500][1])plt.xticks([])plt.yticks([])plt.show() It can be seen that there is a lot of noise in the character picture, and the noise will have an adverse effect on the model prediction result, so we can use a specific filter to eliminate the picture noise during data preprocessing. Question 1-Establish a character comparison table A simple observation shows that there are no duplicates in the key and value in the character dictionary just defined. Therefore, the key and value in the dictionary can be reversed so that we can use the value to find the key (convert the model prediction result into a readable character) Now you need to complete the following code to reverse the keys and values in the dictionary (for example: dict={'A':10,'B':11} and get new_dict={10:'A ',11:'B'} 12345char_dict = {'0':0,'1':1,'2':2,'3':3,'4':4,'5':5,'6':6,'7':7,'8':8,'9':9,\\ 'A':10,'B':11,'C':12,'D':13,'E':14,'F':15,'G':16,'H':17,'I':18,'J':19,'K':20,'L':21,'M':22,\\ 'N':23,'O':24,'P':25,'Q':26,'R':27,'S':28,'T':29,'U':30,'V':31,'W':32,'X':33,'Y':34,'Z':35 }new_char_dict = {v : k for k, v in char_dict.items()} Question 2-Define datasets and dataloader We need to use torch.utils.data.Dataset as the parent class to define our own datasets in order to standardize our own datasets. 123456789101112131415class iDataset(Dataset): def __init__(self, file_name, transforms): self.file_name = file_name # file name self.image_label_arr = load_file(self.file_name) # read binary file self.transforms = transforms # Image converter def __getitem__(self, index): label, img = self.image_label_arr[index] img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) # Convert the picture to grayscale img = cv2.medianBlur(img, 5) # Use median blur to remove image noise img = self.transforms(img) # Transform the image return img, char_dict[label[0]] def __len__(self): return len(self.image_label_arr) Now we can define transform and dataloader. 12345678910transform = transforms.Compose([transforms.ToPILImage(), transforms.Resize([28, 28]), # Adjust the image size to 28*28 transforms.ToTensor(), # Convert the picture to tensor transforms.Normalize(mean = [0.5], std = [0.5])]) # Perform normalization processingtrain_datasets = iDataset(train_data_dir, transform)train_loader = DataLoader(dataset=train_datasets, batch_size=32, shuffle = True)val_datasets = iDataset(val_data_dir, transform)val_loader = DataLoader(dataset=val_datasets, batch_size = 32, shuffle = True) Question 3-Define the network structure After the data is ready, we need to define a simple convolutional neural network. The input of the neural network is [batchsize,chanel(1),w(28),h(28)], and the output is 36 categories. Our neural network will use 2 convolutional layers with 2 fully connected layers. The parameter settings of these four layers are shown in the following table (the default parameters can be used directly if they are not marked): 1. conv1: in_chanel=1, out_chanel=10, kernel_size=5 1. conv2: in_chanel=10, out_chanel=20, kernel_size=3 1. fc1: in_feature=2000, out_feature=500 4. fc2: in_feature=500, out_feature=36 1234567891011121314151617181920212223class ConvNet(nn.Module): def __init__(self): super().__init__() # TODO: self.conv1 = nn.Conv2d(1, 10, 5) self.conv2 = nn.Conv2d(10, 20, 3) self.fc1 = nn.Linear(20 * 10 * 10, 500) self.fc2 = nn.Linear(500, 36) def forward(self, x): # inputsize: [b, 1, 28, 28] in_size = x.size(0) # b out = self.conv1(x) out = F.relu(out) out = F.max_pool2d(out, 2, 2) out = self.conv2(out) out = F.relu(out) out = out.view(in_size, -1) out = self.fc1(out) out = F.relu(out) out = self.fc2(out) out = F.log_softmax(out, dim = 1) return out Question 4-Define the model training function Next, we need to complete the model training function to achieve the following operations: 1. Clear the gradient 1. Forward propagation 1. Calculate the gradient 1. Update weights 12345678910def train(model, train_loader, optimizer, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if(batch_idx + 1) % 10 == 0: print('Train Epoch: {} [{} / {} ({:.0f} %)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) Define model test function 123456789101112def test(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: output = model(data) test_loss += F.nll_loss(output, target, reduction = 'sum') pred = output.max(1, keepdim = True)[1] correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%) \\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) Define model and optimizer We define the model structure we just built as model and choose to use the Adam optimizer. 12model = ConvNet()optimizer = optim.Adam(model.parameters()) Model training and testing We can first set the number of epochs to 3 and perform model training to see how accurate the model is and whether it meets the requirements of verification code recognition. If the model accuracy is not enough, you can also try to adjust the number of epochs and retrain. 123456789101112131415161718192021222324252627282930EPOCHS = 3for epoch in range(1, EPOCHS + 1): train(model, train_loader, optimizer, epoch) test(model, val_loader) &quot;&quot;&quot;Train Epoch: 1 [288 / 1800 (16 %)] Loss: 3.340514Train Epoch: 1 [608 / 1800 (33 %)] Loss: 2.872326Train Epoch: 1 [928 / 1800 (51 %)] Loss: 1.977929Train Epoch: 1 [1248 / 1800 (68 %)] Loss: 1.098688Train Epoch: 1 [1568 / 1800 (86 %)] Loss: 0.535660Test set: Average loss: 0.2888, Accuracy: 328/360 (91%) Train Epoch: 2 [288 / 1800 (16 %)] Loss: 0.072813Train Epoch: 2 [608 / 1800 (33 %)] Loss: 0.139866Train Epoch: 2 [928 / 1800 (51 %)] Loss: 0.109487Train Epoch: 2 [1248 / 1800 (68 %)] Loss: 0.058259Train Epoch: 2 [1568 / 1800 (86 %)] Loss: 0.013144Test set: Average loss: 0.0099, Accuracy: 360/360 (100%) Train Epoch: 3 [288 / 1800 (16 %)] Loss: 0.010245Train Epoch: 3 [608 / 1800 (33 %)] Loss: 0.004797Train Epoch: 3 [928 / 1800 (51 %)] Loss: 0.002203Train Epoch: 3 [1248 / 1800 (68 %)] Loss: 0.006250Train Epoch: 3 [1568 / 1800 (86 %)] Loss: 0.005230Test set: Average loss: 0.0028, Accuracy: 360/360 (100%)&quot;&quot;&quot; Define model test function 123456789101112def test(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: output = model(data) test_loss =+ F.nll_loss(output, target, reduction = 'sum') pred = output.max(1, keepdim = True)[1] correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print('\\nTest set: Average loss: {:.4f}, Accuracy : {}/{} ({:.0f}%) \\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) Define model and optimizer We define the model structure we just built as model and choose to use the Adam optimizer. 12model = ConvNet()optimizer = optim.Adam(model.parameters()) Model training and testing We can first set the number of epochs to 3 and perform model training to see how accurate the model is and whether it meets the requirements of verification code recognition. If the model accuracy is not enough, you can also try to adjust the number of epochs and retrain. 1234567891011121314151617181920212223242526272829EPOCHS = 3for epoch in range(1, EPOCHS + 1): train(model, train_loader, optimizer, epoch) test(model, val_loader)&quot;&quot;&quot;Train Epoch: 1 [288 / 1800 (16 %)] Loss: 3.508450Train Epoch: 1 [608 / 1800 (33 %)] Loss: 3.288610Train Epoch: 1 [928 / 1800 (51 %)] Loss: 2.584805Train Epoch: 1 [1248 / 1800 (68 %)] Loss: 1.180833Train Epoch: 1 [1568 / 1800 (86 %)] Loss: 0.564084Test set: Average loss: 0.0088, Accuracy : 316/360 (88%) Train Epoch: 2 [288 / 1800 (16 %)] Loss: 0.173177Train Epoch: 2 [608 / 1800 (33 %)] Loss: 0.043262Train Epoch: 2 [928 / 1800 (51 %)] Loss: 0.054462Train Epoch: 2 [1248 / 1800 (68 %)] Loss: 0.052596Train Epoch: 2 [1568 / 1800 (86 %)] Loss: 0.013714Test set: Average loss: 0.0006, Accuracy : 360/360 (100%) Train Epoch: 3 [288 / 1800 (16 %)] Loss: 0.004590Train Epoch: 3 [608 / 1800 (33 %)] Loss: 0.007654Train Epoch: 3 [928 / 1800 (51 %)] Loss: 0.004135Train Epoch: 3 [1248 / 1800 (68 %)] Loss: 0.003140Train Epoch: 3 [1568 / 1800 (86 %)] Loss: 0.003019Test set: Average loss: 0.0001, Accuracy : 360/360 (100%) &quot;&quot;&quot; The model has been trained! Does the test set accuracy of the last epoch exceed 99%? Identification codes After successfully implementing the digital recognition, we can start the verification code recognition! First, import the verification code data set: 1verification_code_data = load_file(verification_code_dir) Let's choose a picture at random (Figure 6) to see what the verification code looks like. 1234image = verification_code_data[6]IMG = Image.fromarray(cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB))plt.imshow(IMG)plt.show() Let's take a look at what effect the median filter can have on the captcha image. 123img = cv2.medianBlur(image.copy(), 5)plt.imshow(img)plt.show() Finally, let us look at the actual results of verification code recognition: 1234567891011121314151617181920212223242526272829IMAGES = list()NUMS = list()for img in verification_code_data: IMAGES.append(img) img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) image_1 = img[:, :80] image_2 = img[:, 80:160] image_3 = img[:, 160:240] image_4 = img[:, 240:320] img_list = [image_1, image_2, image_3, image_4] nums = [] for one_img in img_list: one_img = transform(one_img) one_img = one_img.unsqueeze(0) output = model(one_img) nums.append(new_char_dict[torch.argmax(output).item()]) NUMS.append('Verification_code: '+ ''.join(nums))plt.figure(figsize = (20, 20))plt.subplots_adjust(wspace = 0.2, hspace=0.5)for i in range(1, 11): plt.subplot(5, 2, i) plt.title(NUMS[i-1], fontsize = 25, color = 'red') plt.imshow(IMAGES[i - 1]) plt.xticks([]) plt.yticks([])plt.show()","link":"/example_09/"},{"title":"Business Intelligence(BI)","text":"Use LeNet model to recognize Mnist handwritten digits 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import tensorflow as tf#print(tf.__version__)from tensorflow.keras import layersfrom tensorflow.keras.datasets import mnistfrom tensorflow.keras.layers import Conv2D, MaxPooling2Dfrom tensorflow.keras.layers import Dense, Flattenfrom tensorflow.keras.models import Sequentialimport numpy as npimport warningswarnings.filterwarnings('ignore')# Data loading#(train_x, train_y), (test_x, test_y) = mnist.load_data() #Download the data set from the Internetdata = np.load('~/data/course_data/mnist.npz') #Read data set from local#print(data.files)train_x, train_y, test_x, test_y = data['x_train'], data['y_train'], data['x_test'], data['y_test']warnings.filterwarnings('ignore')# Input data is mnist data settrain_x = train_x.reshape(train_x.shape[0], 28, 28, 1)test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)train_x = train_x / 255test_x = test_x / 255train_y = tf.keras.utils.to_categorical(train_y, 10)test_y = tf.keras.utils.to_categorical(test_y, 10)# Create sequential modelmodel = Sequential()# The first layer of convolutional layer: 6 convolution kernels, the size is 5*5, relu activation functionmodel.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))# The second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size=(2, 2)))# The third layer of convolutional layer: 16 convolution kernels, size 5*5, relu activation functionmodel.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))# The second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size=(2, 2)))# Flatten the parameters, which is called a convolutional layer in LeNet5. In fact, this layer is a one-dimensional vector, the same as the fully connected layermodel.add(Flatten())model.add(Dense(120, activation='relu'))# Fully connected layer, the number of output nodes is 84model.add(Dense(84, activation='relu'))# The output layer uses the softmax activation function to calculate the classification probabilitymodel.add(Dense(10, activation='softmax'))# Set the loss function and optimizer configurationmodel.compile(loss=tf.keras.metrics.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])# Pass in training data for trainingmodel.fit(train_x, train_y, batch_size=128, epochs=2, verbose=1, validation_data=(test_x, test_y))# Evaluate the resultsscore = model.evaluate(test_x, test_y)print('error:%0.4lf' %score[0])print('Accuracy:', score[1])&quot;&quot;&quot;Train on 60000 samples, validate on 10000 samplesEpoch 1/260000/60000 [==============================] - 39s 643us/sample - loss: 0.3172 - acc: 0.9096 - val_loss: 0.1105 - val_acc: 0.9626Epoch 2/260000/60000 [==============================] - 39s 652us/sample - loss: 0.0892 - acc: 0.9725 - val_loss: 0.0664 - val_acc: 0.979010000/10000 [==============================] - 4s 358us/sample - loss: 0.0664 - acc: 0.9790error:0.0664Accuracy: 0.979&quot;&quot;&quot; Use LR to classify MNIST handwritten digits 12345678910111213141516171819202122232425262728293031323334from sklearn.model_selection import train_test_splitfrom sklearn import preprocessingfrom sklearn.metrics import accuracy_scorefrom sklearn.datasets import load_digitsfrom sklearn.linear_model import LogisticRegressionimport matplotlib.pyplot as plt# Download Datadigits = load_digits()data = digits.data# Data Explorationprint(data.shape)# View the first imageprint(digits.images[0])# The meaning of the numbers represented by the first imageprint(digits.target[0])# Display the first imageplt.gray()plt.title('Handwritten Digits')plt.imshow(digits.images[0])plt.show()&quot;&quot;&quot;(1797, 64)[[ 0. 0. 5. 13. 9. 1. 0. 0.] [ 0. 0. 13. 15. 10. 15. 5. 0.] [ 0. 3. 15. 2. 0. 11. 8. 0.] [ 0. 4. 12. 0. 0. 8. 8. 0.] [ 0. 5. 8. 0. 0. 9. 8. 0.] [ 0. 4. 11. 0. 1. 12. 7. 0.] [ 0. 2. 14. 5. 10. 12. 0. 0.] [ 0. 0. 6. 13. 10. 0. 0. 0.]]0&quot;&quot;&quot; 12345678910111213141516# Split the data, use 25% of the data as the test set, and the rest as the training settrain_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)# Adopt Z-Score standardizationss = preprocessing.StandardScaler()train_ss_x = ss.fit_transform(train_x)test_ss_x = ss.transform(test_x)# Create LR classifierlr = LogisticRegression()lr.fit(train_ss_x, train_y)predict_y=lr.predict(test_ss_x)print('LR accuracy rate: %0.4lf'% accuracy_score(predict_y, test_y))&quot;&quot;&quot;LR accuracy rate: 0.9644&quot;&quot;&quot;","link":"/example_12/"},{"title":"FREEDOM","text":"许很多的影片我都应该从新温习一遍。不只是因为我学的就是这个，更重要的是每次看一遍都能理解一些新的东西。 勇敢的心，1995年电影界最成功影片。先不去评论其他技术上的细节。我只是为一个名族英雄折服。freedom。 中国古话里就说过：生命诚可贵，爱情价更高，若为自由故，两者皆可拋。 不知道从什么时候开始，这句话被滥用了。人们总是以此来津津乐道自由的重要。歪曲的道理不能称之为道理。这里的自由，我根本不原意理解成为个人的自由。至少国有国法，家有家规这话我还不会去颠覆它。 任何环境总是有规矩才会成方圆，才会有乐趣。 自由，更深层的含义是民族上的。。。为本民族的自由而战，豪情万丈。所以梅尔.吉普森最后的”freedom”如此震撼人心。。 至此，我仍然相信，民族利益高于一切。一切政治上的形式主义都可以扔到一边。for my people.I will…","link":"/freedom/"},{"title":"Natural Language Processing NLP","text":"Resnet Visualize 12345678910111213141516171819202122232425262728293031323334353637383940414243import torchvisionimport torch.nn.functional as Ffrom torchvision.transforms import transformsfrom torch import nnimport torchimport matplotlib.pyplot as pltfrom icecream import icfrom PIL import Imageimport numpy as npdef visualize_model(model, input_, output): width = 8 fig, ax = plt.subplots(output[0].shape[0] // width, width, figsize=(20, 20)) for i in range(output[0].shape[0]): ix = np.unravel_index(i, ax.shape) plt.sca(ax[ix]) ax[ix].title.set_text('filter-{}'.format(i)) plt.imshow(output[0][i].detach()) plt.show()preprocess = transforms.Compose([ transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),])resnet = torchvision.models.resnet18(pretrained=True) # transfer step 1: load pretrained modelconv_model = [m for _, m in resnet.named_modules() if isinstance(m, torch.nn.Conv2d)]&quot;&quot;&quot;Downloading: &quot;https://download.pytorch.org/models/resnet18-f37072fd.pth&quot; to /Users/lilithgames/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth100%|██████████| 44.7M/44.7M [00:34&lt;00:00, 1.36MB/s]&quot;&quot;&quot;for m in conv_model: m.register_forward_hook(visualize_model)myself = preprocess(Image.open('~/data/course_data/doo.jpeg'))with torch.no_grad(): resnet(myself.unsqueeze(0)) # un-squeeze for convert myself to [ [myself] ] Only some pictures are posted here Transfer Example 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import torchvisionimport torch.nn.functional as Ffrom torchvision.transforms import transformsfrom torch import nnimport torchimport matplotlib.pyplot as pltfrom icecream import icpreprocess = transforms.Compose([ transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),])cifar_10 = torchvision.datasets.CIFAR10('~/data/course_data/', download=False, transform=preprocess)train_loader = torch.utils.data.DataLoader(cifar_10, batch_size=128, shuffle=True)resnet = torchvision.models.resnet18(pretrained=True) # transfer step 1: load pretrained modelfor param in resnet.parameters(): param.requires_grad = False # frozen weights feature_num = resnet.fc.in_featuresresnet.fc = nn.Linear(feature_num, 10) # rewrite fc classifieric(resnet(cifar_10[0][0].unsqueeze(0)))criterion = nn.CrossEntropyLoss()optimizer = torch.optim.SGD(resnet.parameters(), lr=1e-3, momentum=0.9)epochs = 2losses = []&quot;&quot;&quot;return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)ic| resnet(cifar_10[0][0].unsqueeze(0)): tensor([[-0.0763, -0.4537, 0.8168, 0.2136, -0.0465, 0.4844, -0.4026, 0.8763, -0.7048, -0.7375]], grad_fn=&lt;AddmmBackward&gt;)&quot;&quot;&quot;for epoch in range(epochs): epoch_loss = 0 for i, (images, labels) in enumerate(train_loader): ic(epoch, i) output = resnet(images) loss = criterion(output, labels) optimizer.zero_grad() loss.backward() optimizer.step() epoch_loss += loss.item() if i &gt; 0: print('Epoch: {} batch:{}, loss ==&gt; {}'.format(epoch, i, epoch_loss / i)) losses.append(epoch_loss / i)&quot;&quot;&quot;ic| epoch: 0, i: 0ic| epoch: 0, i: 1ic| epoch: 0, i: 2Epoch: 0 batch:1, loss ==&gt; 5.118020296096802ic| epoch: 0, i: 3Epoch: 0 batch:2, loss ==&gt; 3.8235710859298706ic| epoch: 0, i: 4...ic| epoch: 0, i: 203Epoch: 0 batch:202, loss ==&gt; 1.4433288293899875...&quot;&quot;&quot;plt.plot(losses)plt.show()&quot;&quot;&quot;Because the last time is too long to run, the losses are not assigned&quot;&quot;&quot; Resnet Transfer Learning 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import torchvisionimport torch.nn.functional as Fcifar_10 = torchvision.datasets.CIFAR10('~/data/course_data', download=False, transform=preprocess)train_loader = torch.utils.data.DataLoader(cifar_10, batch_size=512, shuffle=True)plt.imshow(cifar_10[10][0].permute(1, 2, 0))for param in res_net.parameters(): param.requires_grad = False# Parameters of newly constructed modules have requires_grad=True by defaultnum_ftrs = res_net.fc.in_featuresres_net.fc = nn.Linear(num_ftrs, 10) # only update this part parameters criterion = nn.CrossEntropyLoss()# Observe that only parameters of final layer are being optimized as# opposed to before.optimizer_conv = optim.SGD(res_net.fc.parameters(), lr=0.001, momentum=0.9)# Decay LR by a factor of 0.1 every 7 epochslosses = []epochs = 10for epoch in range(epochs): loss_train = 0 for i, (imgs, labels) in enumerate(train_loader): print(i) outputs = res_net(imgs) loss = criterion(outputs, labels) optimizer_conv.zero_grad() loss.backward() optimizer_conv.step() loss_train += loss.item() if i &gt; 0 and i % 10 == 0: print('Epoch: {}, batch: {}'.format(epoch, i)) print('-- loss: {}'.format(loss_train / i)) losses.append(loss_train / len(train_loader)) Show Resnet 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import cv2import numpy as npimport torchfrom torchvision import transformsfrom torchvision.models import resnet18from torchsummary import summaryimport matplotlib.pyplot as pltdef show_one_model(model, input_, output): width = 8 fig, ax = plt.subplots(output[0].shape[0] // width, width, figsize=(20, 20)) for i in range(output[0].shape[0]): ix = np.unravel_index(i, ax.shape) plt.sca(ax[ix]) ax[ix].title.set_text('Filter-{}'.format(i)) plt.imshow(output[0][i].detach()) # plt.pause(0.05) input('this is conv: {}, received a {} tensor, press any key to continue: '.format(model, input_[0].shape)) plt.show() def main(img): &quot;&quot;&quot; Forward propagation, print feature maps during the transfer process &quot;&quot;&quot; # Define device, transforms transform = transforms.Compose([transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.ToTensor(), ]) # Process pictures, define models img = transform(img).unsqueeze(0) model = resnet18(pretrained=True) # Print model summary, which can be used for convolutional layer comparison summary(model, (3, 224, 224)) for p in model.parameters(): print(p) conv_models = [m for _, m in model.named_modules() if isinstance(m, torch.nn.Conv2d)] for conv in conv_models: conv.register_forward_hook(show_one_model) with torch.no_grad(): model(img) # conv_models = [m for _, m in model.named_modules() if isinstance(m, torch.nn.Conv2d)] # # first_conv = conv_models[0] # # show_one_model(first_conv, img, output=first_conv(img)) if __name__ == '__main__': img = cv2.imread('~/data/course_data/doo.png') main(img) &quot;&quot;&quot;---------------------------------------------------------------- Layer (type) Output Shape Param #================================================================ Conv2d-1 [-1, 64, 112, 112] 9,408 BatchNorm2d-2 [-1, 64, 112, 112] 128show more (open the raw output data in a text editor) ... -2.5093e-02, 6.7847e-03, -1.7868e-02, -7.8250e-04, -6.3448e-03], requires_grad=True)&quot;&quot;&quot;","link":"/example_10/"},{"title":"G1 app2sd 完全教程","text":"声明1:你所需要的软件在这里可以下载的到! 声明2.app2sd虽然可以省却手机内存,但是也有许多不便的地方!操作后SDcard就是机子的一部分,不能随便摘取.我用的4G的卡,在机子挂在后存储有问题!不知道其他卡如何.所以在存储文件和音乐的时候还是需要用到读卡器,而这个时候我必须选择关机!直接卸载SDcard会造成机子程序出错!而不得不从新执行一遍app2sd的过程!并且执行过后也会存在一些不可知的问题!如果对稳定性比较看重的人这里可以飘过了! 声明3.我的sdcard已经在手机内通过!懒得再刷,所以没有用我的card抓图!本教程图片多为网上现成图片来完成!而图片不是一个地方抓取的!所以图片上的容量会有差距.但是刷机过程没有错 从新格盘,正好用自己的图!顺便说一下,ubuntu下的默认抓图真恶心!每抓一张都要从新启动一次程序! 所需要的准备的工作: 1.SDcard(必须) 2.分区软件(必须,windows下可以使用Acronis Disk Director Suite,支持vista.linux下可以直接利用终端分区!) 3.Android SDK(非必须,可以再网上下载Terminal Emulator.apk,安装后在手机上输入adb下的指令完成操作!) 首先我们要将SDcard分区,分成fat32和ext2,至于ext3是否可行我没有测试过,有兴趣的可以试试并且留言告诉我测试报告! 我选择的是在ubuntu的终端执行,这样操作比较靠谱.而在windows下的分区软件不是很稳定!会造成诸多不可见的错误! windows下的分区软件有Acronis Disk Director Suite以及PartitionManager,至于分区魔术师可以略过,因为它不支持分区SDcard.Acronis Disk Director Suite软件分区可以移步到此查看! 以USB内存卡方式插上电脑，或者用读卡器插上电脑 像我的ubuntu，它会自动挂载你的卡。 把东西备份好，然后卸载。一定要卸载，不然无法分区 启动ubuntu或者您的linux系统,在终端内输入如下代码: dmesg //查看所连接的设备! 可以看到sdb或者sdc之类的设备名称!假设我以下操作都为sdc设备! sudo fdisk /dev/sdc //这里需要说明,如果linux下非root,必须要输入sudo来取得root权限进行操作.以下类同! p是显示当前分区 n是创建 d是删除 w是应用你的操作 doo@ubuntu:~# sudo fdisk /dev/sdc Command (m for help): d &lt; ==删除当前分区 Command (m for help): p &lt;==显示一下，确定已删除 Disk /dev/sdc: 3965 MB, 3965714432 bytes 122 heads, 62 sectors/track, 1024 cylinders Units = cylinders of 7564 * 512 = 3872768 bytes Disk identifier: 0x9dfd42a5 Device Boot Start End Blocks Id System Command (m for help): Command (m for help): m &lt; ==查看帮助 Command action a toggle a bootable flag b edit bsd disklabel c toggle the dos compatibility flag d delete a partition l list known partition types m print this menu n add a new partition o create a new empty DOS partition table p print the partition table q quit without saving changes s create a new empty Sun disklabel t change a partition's system id u change display/entry units v verify the partition table w write table to disk and exit x extra functionality (experts only) Command (m for help): n &lt;==新建分区，选择主分区 Command action e extended p primary partition (1-4) p Partition number (1-4): 1 &lt;==指定该主分区为1号 First cylinder (1-1024, default 1): &lt;==敲回车，直接使用SD卡的最开头 Using default value 1 Last cylinder or +cylinders or +sizeK(K,M,G) (1-1024, default 1024): +3300M &lt;==填入分区的大小 Command (m for help): n &lt;==新建分区，选择扩展分区(所有逻辑分区加起来就是扩展分区) Command action e extended p primary partition (1-4) p Partition number (1-4): 2 &lt;==扩展分区的序号是2 First cylinder (895-1024, default 895): &lt;==敲回车，直接接着剩余空间的最开头 Using default value 895 Last cylinder or +cylinders or +sizeK(K,M,G) (895-1024, default 1024): &lt;==敲回车，用默认的，使用全部剩余空间 Using default value 1024 Command (m for help): Command (m for help):p Disk /dev/sdc: 3965 MB, 3965714432 bytes 122 heads, 62 sectors/track, 1024 cylinders Units = cylinders of 7564 * 512 = 3872768 bytes Device Boot Start End Blocks Id System /dev/sdc1 1 894 733792+ 83 Linux /dev/sdc2 729 1024 272128+ 83 Linux 创建好两个分区后, 我们还需要用命令t修改分区卷标, 选择分区1改卷标为c 命令为 Command (m for help):t t &lt; ==修改卷标 partition number (1-4): 1 &lt;==输入1来制定第一个分区. Hex code (type L to List codes): c &lt;==输入C来制定卷标 Changed system type of partition 1 to c (W95 FAT32 (LBA)) Command (m for help): w &lt;==将缓冲写入SD卡,应用你的操作 The partition table has been altered! Calling ioctl() to re-read partition table. WARNING: If you have created or modified any DOS 6.X partitions, please see the fdisk manual page for additional information. Syncing disks. doo@ubuntu:~# doo@ubuntu:~# sudo ls /dev/sdc* &lt; ==查看分区情况 /dev/sdc /dev/sdc1 /dev/sdc2 doo@ubuntu:~#sudo mkfs.vfat /dev/sdc1 &lt;==格式化第一个主分区。 mkfs.vfat 3.0.1 (23 Nov 2008) doo@ubuntu:~# sudo mkfs.ext2 /dev/sdc2 &lt;==格式化第二个分区 mke2fs 1.41.4 (27-Jan-2009) warning: 139 blocks unused Filesystem laber= OS type: Linux Block size=1024 (log=0) Fragment size=1024 (log=0) 123360 inodes, 491521 blocks 24583 blocks (5.00%) reserved for the super user First data block=1 Maximum filesyetem blocks=67633152 68 block groups 8192 blocks per group, 8192 fragments per group 2856 inodes per group Superblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409 Writing inode tables: done Writing superblocks and filesystem accounting information: done This filesystem will be automatically checked every 38 mounts or 180 days.whichever comes first. Use tune2fs -c or -i to override. doo@ubuntu:-$ 分区完毕后ubuntu会自动挂在两个盘符.表示成功! 然后需要手机必须为app2sd版本的rom,在windows 命令提示符下输入命令查看: 以下步骤必须安装android sdk.(其实一下步骤不一定需要在windows cmd下进行,在网上下载一个android的终端Terminal Emulator.apk,然后启动此程序在手机内输入以下指令是一样的!只是在sdcard的系统盘下建立app文件夹并挂载到android rom上! ) C:and Settings&gt;cd c:&lt; ==cd到sdk adb.exe C:&gt;adb devices &lt; ==查看连接的硬件和设备 List of devices attached 000000000000 device &lt;==分区过硬盘以后连接会显示000000000000 的硬件号 C:&gt;adb shell # su &lt; ==如果你还没有取得root权限,那么这一步通不过. su # ls /system &lt;==查看一下system目录下的文件夹 ls /system lib framework media fonts etc customize build.prop usr bin xbin app sd lost+found busybox df -h &lt; ==查看系统盘情况!如果分区成功,那么会在android的系统下显示分区.如下我的385.8M的分区在android的系统内!再往下是sdcard的系统!如果没有那表示分区失败.当然还有一种可能就是你的手机不是app2sd rom busybox df -h Filesystem Size Used Available Use% Mounted on tmpfs 48.3M 0 48.3M 0% /dev tmpfs 4.0M 12.0k 4.0M 0% /sqlite_stmt_journals /dev/block/mtdblock3 67.5M 67.5M 0 100% /system /dev/block/mtdblock5 74.8M 30.4M 44.3M 41% /data /dev/block/mtdblock4 67.5M 1.2M 66.3M 2% /cache /dev/block/mmcblk0p2 385.8M 2.0k 366.5M 0% /system/sd &lt; ==由于在ubuntu下分区后手机内读取sdcard出错,所以后便又分了一次!但是没有抓图,所以容量上和上图有差距.再者本身linux和windows读取SDcard的容量上就有不同! /dev/block//vold/179:1 3.3G 4.0k 3.3G 0% /sdcard # mkdir /system/sd/app &lt;==建立sdcard分区上的app文件夹!如果以前sdcard曾做过app2sd,那么这个文件夹是存在的!会有命令符提示文件夹存在! mkdir /system/sd/app # cd /data cd /data # cp -a app /system/sd/app cp -a app /system/sd/app # rm -r app rm -r app # ln -s /system/sd/app /data/app ln -s /system/sd/app /data/app # reboot reboot 手机自动重启后就OK了.放心安装你所想要的apk程序吧! 顺便说一句:ubuntu的9.04快要放出正式版了!欢迎大家下载试用.","link":"/g1-app2sd/"},{"title":"G1上打造Hero!(更新tips&amp;app)","text":"这本来是安卓上发布的一片帖子,我写在这里主要是为了为自己增加点浏览量!顺便解解眼馋.但是我并不打算在这里提供教程和下载! (由于flickr最近不稳定,所以图片显现不出来!)是由于博客上的flickr插件的原因.. 有兴趣的看这里吧.... 教程中需要的是C6的卡,但是我的是金士顿东京原厂8G卡.试试用一下吧!稍后发试用报告!","link":"/g1-hero-tipsapp/"},{"title":"G1 提权刷机顺序","text":"本人主要是写给在我博客中留言的@leo 以及和他一样刷机无从入手的朋友!技术是一直在更新的,老的技术文档虽然不太适用了,但是参考价值是一定有的!所以也可以翻看我之前的文档,对比本文参看! 长久不用就是会忘记哈...唉..已经很久没有从源头开始刷起了! 拿起公司的黑色美版G1,按一直以来的刷机程序来刷,傻眼了....recovery的版本不对,是最早的版本,机子虽然是1.6版本,但是没有提权,无法获得root权限.也就根本无从刷机. 所以需要从新捡起以前的知识来从源头开始刷起..这就需要一个逻辑性的顺序问题.也就是我们需要将G1降级到RC29或者RC7版本,RC29对应的是美版第一个文件,而我的白色英版对应的则是RC7. 顺序上应该是: 将手机降级并获取root权限 push recovery.img,获取testkey 恢复出厂设置 升级所需要版本. 升级RC版本 升级G2版本 升级APP2SD版本 在第一步就卡着了,源头版本文件没有保存.各大论坛也都没有存档了!找的比较费劲,所以有些文件还是需要保存的.... 具体可以参看博客内的相关链接! 另外,关于技术文档上比较重要的一点就是,在获取testkey以后,最好更换recovery的版本,因为以前的版本选项比较少,只能将刷机包更名为update.zip来进行刷机,而在后期的版本中加入了list,可以选择根目录内的zip包来进行刷机而无需命名为update,并且可以wipe system以及sdcard等多个内容.这就是所说的full wipe,现在很多rom都需要full wipe才可以刷机,避免出错! 并且更重要的是,在后期版本中的recovery中加入了ext3 to ext4的更换分区功能.玩过linux的应该对这个分区很熟悉!而在黑色美版中我刷入了1.61版的recovery,依旧刷的1.6版本rom.","link":"/g1-mentioned-order-of-the-right-brush/"},{"title":"G1 蓝牙传输更新 Bluetooth File Transfer 1.4","text":"不知道有没有人和我遇到了一样的问题!在G1rom更新到固件1.6以后,app2sd的Bluex1.12版本无法安装,而无app2sd版本的在安装后只能发送文件而无法打开蓝牙端口接收! 有兴趣的朋友可以到这里下载以后试试. 而这次寻到一个新的软件,版本为1.4版本,相信不是以前的bluex的版本更新,其实我自己也不确定是否为更新版本,因为图标已经换的很彻底了,并且内部结构也与以前大不一样!最重要的一点,原来支持文档类别存放,而现在却统一放在一个文件夹下!确实有些很不方便... 不过这个apk只需要在sdcard内直接安装就可以使用,需要提供root权限...而传输和接受都OK,并且比较顺畅! 另外,此软件我没有在hero的固件上测试,不知道是否支持hero,有兴趣的自己试试吧!而hero上的蓝牙传输其实有解决办法了...如下: More progress 11:04pm CST 8/26/09 Tracked down what calls the BTIP service, it's /system/lib/libandroid_runtime.so . Tried replacing it with a cupcake build, rebooted and ran into the issue where /system/framework/framework.jar is still referencing calls that were in the Hero libandroid_runtime.so . So replaced framework.jar and framework.odex from cupcake build and got the following error. D/AndroidRuntime( 1517): &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; AndroidRuntime START &lt; &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; D/AndroidRuntime( 1517): CheckJNI is OFF I/dalvikvm( 1517): DexOpt: mismatch dep signature for '/system/framework/core.odex' E/dalvikvm( 1517): /system/framework/framework.jar odex has stale dependencies I/dalvikvm( 1517): Zip is good, but no classes.dex inside, and no valid .odex file in the same directory D/libc-abort( 1517): abort() called in pid 1517 Any \"educated\" ideas? Questions &amp; Progress 01:09pm CST 8/17/09 So lately what I've been trying to do is find where a reference is made to actually call the BTIPS service. I've been lookiing in /system/framework and /data/app_s/Settings.apk but haven't found it yet. What I'm hoping to do is modify the file and have it call BT the same way cupcake did. Has anyone else found where a reference to \"btips\" is at? Settings.apk, which is what pops up when on home screen and you hit menu-&gt;settings, only makes a call to \"android:targetClass=\"com.android.settings.bluetoo th.BluetoothSettings\" Anywho, if you find it in any system libraries or framework files let me know. Please no PM's or posts about where you \"THINK\" it may be at. I've already tried the random guessing stuff, now I'm going through libraries one by one trying to find it. Some more notes 12:30pm CST 7/24/09 Here are some notes of interest. There are two versions of the /system/bin/bts daemon that are floating around on the Hero builds md5sum bts 29ffa46f12c01e3690690752b4e2d58d bts md5sum bts 5aeaca42d67d3b3c64ceda9ee4bfec1a bts There are also two versions of the TIInit_5.3.53.bts firmware files. One is actually just the brf6300.bin file renamed to match what Hero is looking for in /etc/firmware md5sum TIInit_5.3.53.bts d7a214bdb9b4fbc2b4e2dd7e3ab95df0 TIInit_5.3.53.bts md5sum TIInit_5.3.53.bts cb3d2ecbfc97c026a0dcceb8c959b7db TIInit_5.3.53.bts If you run \"strings\" on /system/bin/bts and grep for \"TII\" you'll be able to tell which firmware files that version supports TIInit_3.4.27.bts TIInit_4.2.38.bts TIInit_5.2.34.bts TIInit_5.3.53.bts TIInit_6.2.31.bts Nice picture illustrating BT architecture in Android 7:04pm CST 7/17/09 A note for ROM devs 02:27pm CST 7/17/09 Something to note, Hero does not use any of the following legacy services and therefore they can be removed from init.rc &amp; init.trout.rc . This is mainly something the ROM cookers should pay attention to. The btips service actually handles all of this now. REMOVE THE FOLLOWING: service hcid /system/bin/hcid -s -n -f /etc/bluez/hcid.conf socket bluetooth stream 660 bluetooth bluetooth socket dbus_bluetooth stream 660 bluetooth bluetooth # init.rc does not yet support applying capabilities, so run as root and # let hcid drop uid to bluetooth with the right linux capabilities group bluetooth net_bt_admin misc disabled service hciattach /system/bin/hciattach -n -s 115200 /dev/ttyHS0 texas 4000000 flow user bluetooth group bluetooth net_bt_admin disabled service hfag /system/bin/sdptool add --channel=10 HFAG user bluetooth group bluetooth net_bt_admin disabled oneshot service hsag /system/bin/sdptool add --channel=11 HSAG user bluetooth group bluetooth net_bt_admin disabled oneshot Found something 01:48pm CST 7/17/09 I was looking through init.trout.rc and noticed the following lines chown bluetooth bluetooth /sys/devices/platform/msm_serial_hs.0/serial_lock_cpu chmod 0660 /sys/devices/platform/msm_serial_hs.0/serial_lock_cpu This may not seem like much but this node does not actually exist in our builds. It's possible, and probably likely, that HTC modified their kernel to support the changes that were made in the bts (btips) daemon. We all are pretty much not using the HTC kernel, we're using custom compiled kernels from JAC or Cyanogen. I tried using the RUU kernel but couldn't boot at all. Is anyone able to get their phone booting off the RUU kernel and NOT one of the custom kernels that are floating around in these ROMs? If so, can you check if this device node exists? I believe booting off that kernel could be the answer to the UART clock issues I'm getting and missing devices in /sys . NEXT I have been toying around with the following value in init.rc that seems to affect whether or not I get an error. /proc/sys/net/unix/max_dgram_qlen The default is 10, the RUU release of Hero sets it to 999. If I change that to 10000 then it pauses the BT services and just sits there. If I revert to default I get the same error that I see when its set to 999. Wondering if there's a happy medium in queue length (qlen). Just me thinking out loud. Latest progress 11:43pm CST 7/15/09 I wanted to post some newer results I've been having with BT debugging on Hero. I found out how to circumvent the UART disable error. This is done by having the service btips statement in init.rc to look as follows service btips /system/bin/bts socket bluetooth stream 666 bluetooth bluetooth socket dbus_bluetooth stream 666 bluetooth bluetooth group bluetooth net_bt_admin root misc disabled oneshot The most important part is \"oneshot\" which tells Android to NOT restart the btips service after it dies. If you leave this off then it will relaunch btips service and tie up the I2C bus. The newest error I'm getting is the inability to launch HCI. This is hopefully the LAST error before I can get BT functional! Anyways, just wanted to update everyone that I have not stopped working on bluetooth. 1247718990.888806 BTSTACK(778) INFO | UATRAN: HCI Command was not acknowledged with an event [ vendor/ti/btips-linux/B_TIPS/btstack/hcitrans/uart/uarttran.c:298 ] 1247718990.889935 BTSTACK(778) INFO | HCI: HCI_Process detected transport failure [ vendor/ti/btips-linux/B_TIPS/btstack/stack/hci/hci_proc.c:1596 ] 1247718990.890179 BTSTACK(778) INFO | RADIOMGR: RmgrHciCallback: 0x6 [ vendor/ti/btips-linux/B_TIPS/btstack/stack/radiomgr.c:364 ] 1247718990.890362 BTSTACK(778) INFO | RADIOMGR: HCI init failed (retrying) [ vendor/ti/btips-linux/B_TIPS/btstack/stack/radiomgr.c:386 ] 1247718990.890484 BTSTACK(778) INFO | RADIOMGR: HCI init error [ vendor/ti/btips-linux/B_TIPS/btstack/stack/radiomgr.c:335 ] 1247718990.890637 BTSTACK(778) INFO | ME: HCI Init complete status: 22 [ vendor/ti/btips-linux/B_TIPS/btstack/stack/me/me.c:1220 ] 1247718990.890789 BTSTACK(778) INFO | CMGR: Received event HCI_INIT_ERROR [ vendor/ti/btips-linux/B_TIPS/btstack/profiles/common/conmgr.c:591 ] 1247718990.890942 BTSTACK(778) INFO | Dbus | inside _BTBUS_COMMON_BTL_callback with event: 6 0[ vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_common.c:62 ] 1247718990.893536 BTSTACK(778) INFO | sending dbus message from BTBUS_COMMON_BTL_callback in {vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_common.c:84}[ vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_utils.c:189 ] 1247718990.898022 BTSTACK(778) INFO | Dbus | _BTBUS_COMMON_BTL_callback signal sent: 6 0[ vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_common.c:87 ] 1247718990.898358 BTSTACK(778) FATAL | HCI Init Status Received while neither FM nor BT On in progress[ vendor/ti/btips-linux/EBTIPS/btl/ti_chip_mngr/ti_chip_mngr.c:1232 ] 1247718990.898541 BTSTACK(778) Assert | 0[ vendor/ti/btips-linux/EBTIPS/btl/ti_chip_mngr/ti_chip_mngr.c:1232 ] 1247718990.899121 BTSTACK(778) FATAL | signal 11 sent to our program from address 0xdeadbaad and code 1[ vendor/ti/btips-linux/EBTIPS/apps/btt_task.c:102 ] I'll update this main post as I, or others, come up with progress or advancements. The directories for this are already created in the latest Hero init.rc . Just need to create the ddb file touch /data/btips/TI/BtDeviceDb.ddb chmod 666 /data/btips/TI/BtDeviceDb.ddb The results of making these changes is you are able to get ALL bluetooth services and sockets created. Bluetooth is working from the commandline, just not on the frontend where we need it. PS:xda那边似乎有人已经放出hero shippment rom, 蓝牙问题应该已经解决了....静待佳音吧! :[download id=\"6\"] | skydrive 下载","link":"/g1-bluetooth-file-transfer-1-4/"},{"title":"Google  Android 4.0","text":"10,19日上午十点左右,Google,三星在香港联合举办了发布会,推出了下一代Android系统(Ice Cream Sandwich)Android4.0,以及下一代Nexus智能手机,三星Galaxy Nexus Prime. 这次的软件开发包也在这次大会宣布可以下载使用,在developer,android.com上. 这次更新重新设计了浏览器的界面,改进了Gmail功能,增强了照相和摄影.可以进入编辑模式,使用各种系统自带的滤镜效果和其他编辑操作! 当然,我最欣赏的改进就是这个全新分享功能 一种全新的分享方法，Andriod Bean，使得我们分享变得非常简单。比如说现在他有一个Andriod的手机，把手机对过去想和我分享，就是通过Andriod Bean来分享，把两个手机背靠背的放在一起内容就过来了，这个文章就已经过来了，就是这样轻轻一碰。我想看一下Google Map，现在我看到的是一个地图，在香港弥敦道上面的，现在要把这个地图直接发到我的手机上，同样是把两个手机背靠背放在一起，我手机上也出现了同样的地图。再和大家分享最后一个例子，他在玩一个很酷的游戏，我手机上没有，但是我也很想玩这个游戏，虽然我也可以到Andriod市场上浏览、下载，但是我选择用更酷的方式来做。也是把手机背靠背的放在一起，然后就直接把我带到Andriod市场上的这个游戏页面上，我就不用浏览了，直接下载就可以了。Andriod Bean可以让我们加入某一个社交群体、分享图片、内容，极大的释放了我们的想象力。 关于更新问题,怀疑这次N1这个亲儿子估计也应该可以赶上末班车,因为Google官方不是声明,但凡运行Android2.3的机子,都可以更新并运行4.0么!现在能做的就是坐等CM大神了!","link":"/google-android-40/"},{"title":"Google+ 研究","text":"已經忘記今天是第幾天試用Google+了，這兩天圈內當然免不了的大部分都在討論Google+，而什麼所謂的G+應用技巧，什麼25條關於G+的，還有什麼50條，100條類似的！反正不止是G+內部，包括twitter，facebook，微博以及博客圈內，大家都在樂此不彼的討論Google+！ 可是我這裡不想討論一些技術上的東西，也不想討論它是否真的能夠打敗Facebook或者twitter！我只想從真正的社交上去分析一下我自己的對G+內部細微的探討！ 找不到好圖，又懶得自己做。所以借用一下+lucifr Liu的圖！反正都做了，表浪費！原圖有 PSD文檔可以下載！我按照自己的標準稍稍修改了一下！ 讓我們先來探討一下G+中的circles，雖然所有的社交網絡中都有這樣一種概念，但是沒有Google那樣去深挖它，延伸它！在多部分的社交網絡中，或許都是在模仿Facebook，或許就是在模仿Twitter！而這兩者的基本機制就是相互添加對方為好友，才算開始真正的社交！當然，Twitter的機制要鬆散一點，即便雙方沒有相互fo對方或者只有一方fo了，一樣可以互相聊天！而Facebook對於社交圈子的概念就相對封閉了一點，如果只有一方fo了對方，那麼一樣不構成相互溝通的條件！ 而比較尷尬的是，Facebook作為一個最大的SNS網站，基本上老老少少都在上邊進行基礎社交，這也構成了現在年輕人逐步放棄Facebook的主因！因為這個如果都相互fo了對方，那麼這個circles的範圍就太大了。只要我發佈了一條消息，我的老師，我的父母，我的同學以及我的基友，大家都可以看到這條消息！而在真實的社交範圍中，這是不可想像的！比如，包二奶這裡就是個十分不適合的場所！當然，我們可以私信！！！！！！好吧，我舉的例子有些屎，我承認！那麼讓我們想像一下，總有些事情是不希望父母或者另外的人知道的，而只想在小範圍內的圈子內傳播的！那麼，Google+給了你這個實現的方式，就是circles概念的區分和交集！這個還需要你對聯繫人進行具體的整理才可以！有些人，即是朋友又是同事，這就是兩個circles的交集，那麼我一條信息不管發佈在我的朋友圈還是同事圈，這個人都可以收到我的信息！加入說，同事圈裏又有些話題需要討論而不想讓老闆看到該如何，讓我們建立一個將老闆排除在外的同事圈！ 大概瞭解了Circles的概念以後，我想討論的就是Google+的一些細節的地方！我們都知道，無論是微博還是SNS社交網站比如開心，人人網都會有Share分享功能！這個功能的作用在於，看到好的東西以後，可以直接分享給自己的社交圈裏的所有人觀看！這也是網絡病毒營銷的根源！而在Google+中，我們注意到一個細節，並且進行了一番討論。Share這個功能被分割了。我們以大家熟悉的微博為例，當我Share某話題以後，如果原帖被修改或者刪除，那麼我Share過去的內容也會跟著變動！而且原帖會被注明轉帖和評論了多少次！在G+中就完全不是這樣，當我Share某個帖子後，這個帖子就變成我複製過去的了！只會聲明原帖出自什麼人。並且評論不會跟著原帖一起變動，而我在新Share的帖子下邊回覆，也不被記錄在原帖的評論裏！這就是我們所討論的被分割！可是我們思考一下真實社交圈的情況，我們從某個人那裡聽到了一個消息，然後四處散播！當那個人再對同樣的圈子內修正了這個消息或者否定了這個消息以後，我所散播出去的圈子并不能收到這樣一個反饋，那麼，只有我自己去再去修正一下自己所說過的話！那麼也就是，每個人都必須對自己所說的話和所散播的言論負責！而當你所散播了消息以後，即便和你之前聽到的是一模一樣的，那麼這個討論也只限制在你自己的圈子內而和原來的圈子無關！這個說明，Google+在細節上真的是在充分考慮真實社交的環境而在努力模擬再現！ 大家也都發現，G+可以被當作郵箱來用！當你發表一篇消息給某個特定的人或者一個圈子的人。那麼對方會收到一個消息。這個消息最終如何接受有三種情況！第一種情況，對方沒有G+帳號，那麼這條消息肯定是直接發佈到對方郵箱裏！第二種情況，對方有G+帳號而你所寫接收方是聯繫人或者圈子，並且對方有關閉郵件提醒，那麼對方會在G+上接收到這條信息。第三種情況就是對方有G+帳號並且沒有關閉郵件提醒，那麼就是G+和郵箱都會收到這條消息！ 不過，對於給沒有G+帳戶的人用G+發佈邀請還可以，如果要當作邮箱来用，還是奉勸大家不要這樣做！因為G+發佈郵件的郵箱並不是你的郵箱，是隨機生成的一個郵箱地址，並不是固定的！也就是說對方即便收到也無法回覆！而我做了一個實驗，給我的163，QQ，以及yahoo和gmail郵箱都各發了一個郵件！有的郵件無影無蹤了，而即便收到的郵件，也是等了很久以後才收到的！所以，如果要發郵件的話，請點擊黑又長上邊+You後邊的Gmail選項，老老實實的用Gmail發郵件吧！ PS：此處之後還會不斷更新，分享我對Google+的一些想法！當然，是有了感悟後才會來更新！以後關於Google+的研究如無特殊情況也都在本帖內討論！想到哪寫到哪，不分那麼多條條框框了！ 其實，關於Google+，還有很多值得研究的細節，Google也在不斷完善！可以看得出這真的是一個很有誠意的互聯網產品！相應號召，大家都搬家吧！","link":"/google-research/"},{"title":"Gmail 的转发上限","text":"在Gmail中创建转发过滤器是有上限的,这个估计没有多少人知道!我想也是因为没有多少人有实际的需求… 今天我在Twitter上抱怨Gmail里的邮件过多,占用了很大一部分空间,以至于我的Gmail空间已经临近上限… 也终于迫使我开始清理邮件! ###问题产生:### 在清理的时候发现,除了一些比较大附件的邮件以外,大部分占用空间的邮件都是一些广告邮件! 我很少设置过滤器删除广告邮件,特别是一些推送服务的广告,不过大多都是直接略过收件箱存档而已,有空了还可以去看看…这也使得我这几年来邮箱里布满了此类邮件! 当然,清理的过程还是比较顺利的,毕竟这些邮件的时效性的原因,这些邮件删除起来一点也不心疼! 那么,对于新邮件该怎么处理呢,总不能继续占用存储空间吧.也不好直接删除,毕竟有的时候我还是要看看的. 那么,过滤器这个时候就起作用了! 我的想法很简单,既然QQ邮箱空间是无上限的,而广告邮件又无关乎隐私问题,所以可以放心的转发到QQ空间进行保存!所以对一些广告邮件设置了forward filter…然后问题来了,当我建立到一定的数量的时候,Gmail开始提醒我“转发邮件地址过多,无法创建过滤器 这可怎么办,总该是有解决办法的!于是查阅Gmail的帮助手册发信,Gmail是支持布尔运算符来查找邮件的!而这些布尔运算符一样可以创建到filter里. OK,问题到这里就变得简单了… ###解决方案:### 比如说,我要将 renren.com kaixin001.com 高朋 卓越 美团 拉手等邮件过滤直接forward到QQ邮箱里,那么其实创建一条filter就够了!按如下格式renren.com OR kaixin001.com OR 高朋 OR 卓越 OR 美团 OR 拉手,其中“OR”是Gmail中所支持的布尔运算符,而且必须大写. 延伸阅读: 希望这篇文可以帮助更多的人更好的利用Gmail.更详尽的运算符可以查阅Gmail的帮助手册","link":"/gmail-forward-upper-limit/"},{"title":"Gphone SD卡分区(更新)","text":"以前写过一个Gphone app2sd的完全教程,在那个教程当中有完整的分区步骤,可是那个分区模式是需要linux系统才可以执行,而我当时用的是ubuntu. 现在可行的分区模式也就是linux下,然后windows下的分区软件.譬如PartitionManager,AcronisDiskDirector,不要去想PQ魔术师,那个不支持sd卡分区!就这么多了么?...其实,如果你有Gphone手机的话,完全可以用windows dos来分区! 先说条件: 1.Gphone手机 2.SDcard 3.android sdk 4.一条usb连接线 步骤: 首先开机进入recovery模式，按ALT+X进入“console” 打开cmd,输入: adb shell parted /dev/block/mmcblk0 print 可以看到分区的情况,一般来说都是一个分区,如果以前做过app2sd那么就是两个...删除这两个分区.如下图为三个分区: 然后输入代码删除分区: rm 1 rm 2 rm 3 (如果只有两个分区或者一个分区的,执行一步操作就可以了,也就是rm&lt; 数字&gt;) 在完成后就是一个未分区的SDcard 然后对SDcard重新分区,注意需要根据你的卡大小来分配各分区的大小,一般linux-swap最大32M.ext分区500M足够了,最大不要超过1GB.不过似乎有将linux-swap分成96Mb的...输入以下代码分区: mkpartfs primary fat32 0 7445 mkpartfs primary ext2 7445 7873 300-500都可 mkpartfs primary linux-swap 7873 7969 务必是96M 不然你有C6卡也不能全速体验HERO了... 至此,分区工作就完成了.可以输入print来检查一下. 以下为可执行可不执行步骤...就是将ext2转换为ext3/ext4 转换为ext3输入以下命令: upgrade_fs 转换ext4输入以下命令: tune2fs -O extents,uninit_bg,dir_index /dev/block/mmcblk0p2 e2fsck -fpDC0 /dev/block/mmcblk0p2 upgrade_fs 结束以后,输入 parted /dev/block/mmcblk0 print 验证是否升级到ext3/ext4 然后quit退出,重启手机. 本教程参考了安卓网上安装hero的步骤","link":"/gphone-sdcard/"},{"title":"Logistic regression to diagnose heart disease","text":"The preject source code url : Heart load data 1234567891011121314import pandas as pdfrom sklearn.linear_model import LogisticRegressionfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_splitdata = pd.read_csv('./data/heart.csv')# the csv url: https://github.com/hivandu/colab/blob/master/AI_Data/data/heart.csv# Print a brief summary of the data setdata.info()data.shapedata.target.value_counts() The params meaning 123456789101112131415Params Meaning age 年龄 sex 性别(1 = 男性, 0 = 女性)cp 胸部疼痛类型(值1：典型心绞痛，值2：非典型性心绞痛，值3：非心绞痛，值4：无症状）trestbps 血压 chol 胆固醇 fbs 空腹血糖（&gt; 120 mg/dl，1=真；0=假） restecg 心电图结果（0=正常，1=患有ST-T波异常，2=根据Estes的标准显示可能或确定的左心室肥大） thalach 最大心跳数 exang 运动时是否心绞痛（1=有过；0=没有）oldpeak 运动相对于休息的STslop 心电图ST segment的倾斜度(值1:上坡，值2:平坦，值3:下坡） ca 透视检查看到的血管数 thal 缺陷种类（3=正常；6=固定缺陷；7=可逆缺陷）target 是否患病（0=否，1=是） Perform analysis 12345678910111213141516171819202122232425# Change the &quot;sex&quot; column into two columns &quot;sex_0&quot; and &quot;sex_1&quot;sex = pd.get_dummies(data['sex'], prefix = 'sex') # Add &quot;sex_0&quot; and &quot;sex_1&quot; to the data set. data = pd.concat([data, sex], axis = 1)# And delete the sex column.data = data.drop(columns = ['sex'])# Print out the first five lines. Check whether sex_0, sex_1 are added successfully, and whether sex is deleted successfully.data.head()# Get sample labeldata_y = data.target.valuesdata_y.shape# Get sample feature setdata_x = data.drop(['target'], axis = 1)data_x.shape# Divide the data settrain_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size = 0.3, random_state=33) Normalization 1234567891011121314151617181920212223# initializess = StandardScaler()# The fit function/module is used to train model parametersss.fit(train_x)# Standardize the training set and test settrain_x = ss.transform(train_x)test_x = ss.transform(test_x)# Define a logistic regression modellr = LogisticRegression()lr.fit(train_x, train_y)# Calculate the training set scorelr.score(train_x, train_y)# Calculate test set scorelr.score(test_x, test_y)# Use the classification_report function to display a text report of the main classification indicatorspredict = lr.predict(test_x)print(classification_report(test_y, predict))","link":"/heart/"},{"title":"Hello 2012","text":"以2012起头,其实主要是想要写2011 今天是2011年的最后一天了,明天正式步入毁灭年!谁知道去,也许是真的呢...这样的社会,早点灭了的好! 总体来说,今年是非常不顺的一年.不管是对世界,对中国,还是对我来说... 由于微博的兴旺,各种事情仿佛一瞬间被放大,被拉近...而我们忽然发现,世界原来可以如此精彩...不过这精彩的代价,确实让人有些心痛!这一年里,最欣喜的事情就是在Google Plus里看@变态辣椒的漫画.真是一个好地方啊... 不过这个月初开始不太愿意在那里发言,主要原因还是发现其实放在什么地方,都会有脑残的出现! 而最让人头疼的是,脑残们都不会觉得自己是脑残,根本说不通道理...而他们那些偏激的想法,让我感到恐惧! 是的,如果人类经历一次灭顶之灾,而忽然发现自己周围仍然在酝酿这种灭顶之灾的苗头,你会感到恐惧! 所幸,临到结尾,@韩寒 抛出了三篇惹人口风的文章...看来丫是想给大家留个值得咀嚼的话题在年内!而在问内提到里\"清算\"这样的字眼... 唉,说多了,其实也是废话! 况且说, 我这里真不适合谈论这些.","link":"/hello-2012/"},{"title":"Bluehost上安裝Habari","text":"Habari我還在試用階段...不得承認wordpress確實是一個好的博客程序,但是對於他的日進臃腫我有點微詞... 可憐我不是代碼編寫出身,所以很多問題不得不求助於人,還好身邊有一個好老師\"天佑\"給我提供了很多幫助! 以下就寫寫近期的一些內容. 因為bluehost原本就只願PHP5,而其他的一些標準我不太懂,但是據我安裝下來基本都已經全部滿足,唯一的就是需要自己開啟pdo for mysql. 在這之前你最好是下載一個requirements.php來測試一下你的主機是否已經為你的habari做好了準備. 將requirements.php上傳到自己需要安裝的文件夾下,輸入http://youblogurl/requirements.php 如果切OK,那么就會如下顯示: 如若不然,就會有如下顯示: 我這裡提示的是需要安裝或者打開pdo,如果你也是bluehost,那么基本就是這個提示了! 其實到這步是不需要聯繫客服幫你打開的,默認bluehost就已經安裝了pdo,主要是需要打開而已! 在自己的服務器上建立php.ini文檔,在CP上點擊PHP Config,然後選擇install php.ini master file 不出意外在根服務器上就已經建立php.ini了,加入如下語句: extension_dir = \"/usr/lib/php/extensions/no-debug-zts-20060613\" extension=pdo.so extension=pdo_mysql.so 到這裡當然還沒有結束,需要將php.ini複製到你所要安裝的habari文件夾... 這步一定要做.我不知道原理,所以不要問我,我只是如此操作了,成功了.反而刪除后就會出現需要激活pdo的提示. 然後自然就是下載habari,上傳到安裝目錄,輸入路徑...然後就是和wordpress的安裝順序一樣了! 這裡要提示一點:在網上有說安裝habari要將其目錄設定為777,我的經驗是不要這樣設置,這樣會造成訪問此目錄的時候出現505錯誤頁面...\"天佑\"幫我測試的時候就是如此!...後來我更改回來后就正常訪問了... 當然,很多人都是wordpress的用戶,所以如果你需要導入wordpress的原始數據的話需要做如下工作: 安裝一個插件,用以取消自動保存. 禁用後臺的多版本保存. 刪除數據庫中多餘的文章版本. 完成一上步驟后在habari導入的時候才不會有重複文章導入! 當然,在我的habari測試中還出現了亂碼問題,不僅僅是導入的時候出現亂碼,在輸入博客標題,寫新文章都會出現中文亂碼問題,在發此文的時候這個問題還沒有得到解決,只能讓各位待續了! 再次感謝\"天佑\"的幫助!","link":"/install-habari-at-bluehost/"},{"title":"Google正式推出代码搜索 Code Search","text":"来自小熊在线 Google新一轮的发布热潮在黄金周涌现。这就是Google刚推出的代码搜索，即Google Code Search。根据Garett Rogers的介绍，Google代码搜索结果来自Google的索引。也就是说理论上Google能找到的代码，你都可以利用Google代码搜索找到。 你可以利用Google代码搜索来搜索各种函数的定义以及相关的示例代码，还可以直接使用正则表达式搜索以获得更精确的结果。另外，你还可以限定搜索某种语言、许可或文件名。 对于程序员而言，这个代码搜索工具应用挺实用的。Google Code Search允许编程人员搜索代码用法范例，以更好地理解代码功能。该服务中索引了数十亿行代码，来源是Web上保存的文档以及SourceForge、Google Code等开源软件项目库。 Google还提供了一个API允许第三方开发人员将代码搜索框整合到他们的开发工具中。 点击进入Google Code Search","link":"/google-code-search/"},{"title":"habari的時區和more","text":"在這之前還一直在煩惱habari的時區問題和more代碼截斷. fireyy給了我一些幫助. habari最新的版本已經加上了時區的調整.而這個版本是需要svn方式獲取才可以.基於bluehost默認沒有開通ssh方式.那么我就只有選擇等待habari下一版本的正式發布,又或者聯繫bluehost增加ssh訪問,并花一定時間來學習和熟悉svn獲取的方式! 另外一個就是代碼截斷.在habari總一樣可以試用more代碼來對文章進行截斷,在所用模板總加入: // Only uses the tag, with the 'more' as the link to full post Format::apply_with_hook_params( 'more', 'post_content_out', 'more' ); 這樣在編寫entry的時候就與wordpress的效果是相同的了. 而整個habari的模板代碼和wordpress是比較類似的!頭疼的是plugin和wordpress有很多不一樣的地方!所以對於不懂代碼的我來說,就無從著手了... 奇怪的一點是我現在說使用的habari模板激活后有一個setting的選項,點擊以後并無反應! 我想本身是可以修改的... 比較丟人的地方在於,前邊的文章總提到了歸檔頁面,其實并不是真的在頁面中直接添加代碼就可以了!而是我所用的模板中已經加入了兼容代碼...對於如何建立頁面來實現仍然不清楚! 2009-2-21 01:53 BTW:好吧,我承認我自己又傻了一回...并不是需要在服務器上安裝SVN才可以的..也可以將svn服務器上的程序下載到本地然後再上傳,效果是一樣的!而我現在說安裝的habari就更新到了0.6beta.這樣就完全解決了時區問題!並且增加了zh_tw的文件包!雖然漢化并不完全,但是已經很好了!","link":"/habaris-time-zone-and-more/"},{"title":"Debian上安装Node服务器","text":"本教程只针对我自己，记录用，而且并不是完整版，不对其他人负责。请尽量不要参照本文 最近一直在学习NodeJS，本地上玩的差不多了，总要去架设个服务器跑一下，选择了digitalocean加的$5/mo 服务，安装了个Debian，至于为什么是Debian，好吧，因为。。。 言归正传，一遍操作一遍记录一下，好记性不如烂笔头嘛。 至于怎么注册DigitalOcean这里就不在详述了，从SSH登录开始吧。 SSH登录 未免麻烦，最好是选择SSH登录，官方有详细的介绍： How To Use SSH Keys with DigitalOcean Droplets 这里要说一下，DigitalOcean每次登录的时候都会告诉我密码过期，害得我重置了无数次密码。如果你也遇到这种问题，那么就先选择Conole Access, 然后在弹出的窗口控制台进行操作，修改root密码后再在本地操作。 12345678910111213141516171819202122// 创建SSH keyssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (~/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in ~/.ssh/id_rsa.Your public key has been saved in ~/.ssh/id_rsa.pub.The key fingerprint is:4a:dd:0a:c6:35:4e:3f:ed:27:38:8c:74:44:4d:93:67 demo@aThe key's randomart image is:+--[ RSA 2048]----+| .oo. || . o.E || + . o || . = = . || = S = . || o + = + || . o + o . || . o || |+-----------------+ 1cat ~/.ssh/id_rsa.pub 然后添加到digitalocean的SSH Keys里，Name随便起 之后我们就可以链接服务器了 1cat ~/.ssh/id_rsa.pub | ssh root@[your.ip.address.here] &quot;cat &gt;&gt; ~/.ssh/authorized_keys&quot; 然后，就可以直接登录了: 1ssh root@[your.ip.address.here] 安装Node 我选择的方式是源码安装 1234567891011121314151617// update system$ sudo apt-get update$ sudo apt-get install git-core curl build-essential openssl libssl-dev// Clone node$ cd /usr/local/src$ git clone https://github.com/nodejs/node$ cd node// select checkout$ git tag$ git checkout v4.4.7// install$ ./configure$ make$ sudo make install 漫长的等待，然后就可以查询了$ node -v， 这会就会出现安装的node version 安装NPM 12345$ wget https://npmjs.org/install.sh --no-check-certificate$ chmod 777 install.sh$ ./install.sh$ npm -v3.10.5 安装zsh(不是必要) 123456789sudo apt-get zshgit clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh// copy defult zshrccp ~/.zshrc ~/.zshrc.bak// set oh-my-zsh to usecp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrccash -s /bin/zshsudo shutdown -r now 安装Ruby 安装rbenv 1234567891011git clone git://github.com/sstephenson/rbenv.git ~/.rbenv# 用来编译安装 rubygit clone git://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build# 用来管理 gemset, 可选, 因为有 bundler 也没什么必要git clone git://github.com/jamis/rbenv-gemset.git ~/.rbenv/plugins/rbenv-gemset# 通过 gem 命令安装完 gem 后无需手动输入 rbenv rehash 命令, 推荐git clone git://github.com/sstephenson/rbenv-gem-rehash.git ~/.rbenv/plugins/rbenv-gem-rehash# 通过 rbenv update 命令来更新 rbenv 以及所有插件, 推荐git clone git://github.com/rkh/rbenv-update.git ~/.rbenv/plugins/rbenv-update# 使用 Ruby China 的镜像安装 Ruby, 国内用户推荐git clone git://github.com/AndorChen/rbenv-china-mirror.git ~/.rbenv/plugins/rbenv-china-mirror 12345echo 'export PATH=&quot;$HOME/.rbenv/bin:$PATH&quot;' &gt;&gt; ~/.bashrcecho 'eval &quot;$(rbenv init -)&quot;' &gt;&gt; ~/.bashrc#rbenv install 2.3.1 特么什么都能遇到，远端locale和本地不符，提示无法安装 1sudo locale-gen en_US.UTF-8 // or 1sudo dpkg-reconfigure locales 1vim /etc/ssh/ssh_config 注释或删除AcceptEnv LANG LC_* (服务器SSH配置) 然后断开SSH重新登录，不行重启一下服务器，就好了。 1sudo shutdown -r now 继续: 1rebnv install 2.3.1 部署Nginx 1$ sudo apt-get install nginx 其实Nginx也不是必要装的，Node自己可以跑服务！ 1nohup node app.js 但是如果要多域名的话，需要用到Nginx反代，额，这部分还不懂。再去研究下！ 顺便，加一个删除Nginx的步骤: 1234567sudo apt-get --purge remove nginxsudo apt-get autoremovedpkg --get-selections|grep nginx// 罗列出与nginx相关的软件， nginx-common deinstall 然后sudo apt-get --purge remove nginx-common","link":"/install-node-to-Debian/"},{"title":"hexo-generator-feed","text":"前段时间想通过QQ邮箱将自己的写的博客里的文章转发到QQ空间去。众所周知，这种转发似乎只有通过QQ邮箱发邮件转发一条路。要不就只有Ctrl+C，Ctrl+V。我是很厌恶这种方式的。 可是在QQ邮箱里，并没有收到自己从使用hexo以后的任何更新。好吧，我知道问题了，我失去了feed订阅路径。而Google的结果是，hexo自身并不带feed订阅，如果要支持订阅需要安装插件，也就是“hexo-generator-feed\" 这货。 后来，良久之后@lucifr也是这么告诉我的。 前段时间家里有事，离开上海，回来以后又因为工作需要恶补，一直没时间弄！这刚刚才弄好。。。。 说一下这货，其实是蛮简单的，我也就不再细述了，官方文档也将如何安装写的很清晰。自己查阅一下就好。","link":"/hexo-generator-feed/"},{"title":"King of pop 2014","text":"I MISS U.","link":"/king-of-pop-2014/"},{"title":"Mac MAX_open","text":"在使用Hexo的过程里，经常会卡在deploy指令上，错误原因之一可能是因为Mac的MAX_open数小的原因，Linux默认为1024，而Mac上只有256，所以只要修改MAX_open数就可以了。指令如下： 1234567891011$ sudo sysctl -w kern.maxfiles=20480kern.maxfiles: 12288 -&gt; 20480$ sudo sysctl -w kern.maxfilesperproc=18000kern.maxfilesperproc: 10240 -&gt; 18000$ ulimit -S -n 2048bubbyroom.com$ ulimit -n2048 其中，$ ulimit -n是用于查看Mac的MAX_open数的指令。只执行修改之前可以先执行此指令查看一下。 后记：在Terminal中修改了MAX_open仅适用于当前窗口，新建Tab，窗口后在新的Tab和窗口里都会失效。","link":"/mac-max_open/"},{"title":"Mac隐藏&#x2F;显示桌面图标","text":"Shell: 1defaults write com.apple.finder CreateDesktop -bool FALSE;killall Finder &amp; 1defaults delete com.apple.finder CreateDesktop;killall Finder AppleScript: 1234567display dialog &quot;桌面图标设置为可见或隐藏?&quot; buttons {&quot;可见&quot;, &quot;隐藏&quot;} with icon 2 with title &quot;Switch to presentation mode&quot; default button 1set switch to button returned of resultif switch is &quot;隐藏&quot; then do shell script &quot;defaults write com.apple.finder CreateDesktop -bool FALSE;killall Finder&quot;else do shell script &quot;defaults delete com.apple.finder CreateDesktop;killall Finder&quot;end if","link":"/mac-False-Desktop-icon/"},{"title":"mac apache","text":"经常忘记命令，还是自己稍微记录一下好，免得每次都去Google。 打开 apache sudo apachectl start 關閉 Apache： sudo apachectl stop 重開 Apache： sudo apachectl restart","link":"/mac-apache/"},{"title":"mac:port occupied","text":"查询: 1$ lsof -i:3000 显示: 12COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEnode 2243 du 12u IPv6 0xc9b8c91a94a8da89 0t0 TCP *:hbci (LISTEN) 结束: 1$ kill -9 2243","link":"/mac-port-occupied/"},{"title":"Mac显示隐藏文件","text":"转载自Mac疯，记录而已，不长用，老是记不住，每次都要Google，郁闷！ 显示文件： defaults write com.apple.finder AppleShowAllFiles -bool true 隐藏文件 defaults write com.apple.finder AppleShowAllFiles -bool false 记得KillAll Finder来重启Finder","link":"/macxian-shi-yin-cang-wen-jian/"},{"title":"「马拉松跑步数据","text":"先引入数据,准备进行分析 1234# 引入数据import pandas as pddata = pd.read_csv('~/data/cbcpv/marathon/marathon.csv')data.sample(5) OUT: age gender split final 19841 34 M 01:55:25 04:50:03 11002 28 W 01:55:00 04:11:00 11619 26 M 01:40:28 04:13:52 4068 34 M 01:38:30 03:30:21 6922 35 M 01:37:44 03:48:37 这个数据集有以下几个特征： age，运动员的年龄 gender，运动员的性别 split，半程所用时间 final，全程所用时间，即最终成绩 自然,要先了解下数据的具体情况 1234567891011121314data.info()# 输出结果&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 37250 entries, 0 to 37249Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 37250 non-null int64 1 gender 37250 non-null object 2 split 37250 non-null object 3 final 37250 non-null objectdtypes: int64(1), object(3)memory usage: 1.1+ MB 可以看到并没缺失值, 不过split和final特征中的数据不实数字类型, 用字符串表示了所用时间长度, 所以我们需要进行转化: 1234import datetimedef convert_time(s): h,m,s=map(int, s.split(':')) return datetime.timedelta(hours=h, minutes=m, seconds=s) 使用完成的方法进行数据转换,我们从新读取一下数据: 123456789101112131415df=pd.read_csv( '~/data/cbcpv/marathon/marathon.csv', converters={ 'split': convert_time, 'final': convert_time })df.dtypes# 输出结果age int64gender objectsplit timedelta64[ns]final timedelta64[ns]dtype: object 这次数据已经转换为timedelta64类型, 下面我们就要转化时间为整数, 一般的做法都是秒或者毫秒数: 1234567d = datetime.timedelta(hours=1, minutes=0, seconds=0)df2 = pd.DataFrame({'time':[d]})df2.astype(int)# 输出结果 time0 3600000000000 我们看到的输出结果,是“纳秒“(ns)单位: \\[1s=10^9ns\\] 我们还需要转化为秒: 1234567d=datetime.timedelta(hours=1, minutes=0, seconds=0)df2=pd.DataFrame({'time':[d]})df2.astype(int) * 1e-9# out time0 3600.0 然后我们要讲split和final两个特征的数据进行转化 123df['split_sec']=df['split'].astype(int) * 1e-9df['final_sec']=df['final'].astype(int) * 1e-9df.sample(5) OUT: age gender split final split_sec final_sec 11725 35 M 0 days 01:53:53 0 days 04:14:19 6833.0 15259.0 19815 24 M 0 days 01:58:45 0 days 04:49:57 7125.0 17397.0 5754 49 M 0 days 01:42:39 0 days 03:41:05 6159.0 13265.0 33166 46 M 0 days 02:31:37 0 days 06:06:17 9097.0 21977.0 9226 36 W 0 days 01:49:06 0 days 04:01:55 6546.0 14515.0 现在多了两个特征split_sec和final_sec, 都是以秒为单位的浮点数. 描述统计 先了解数据: 1df.describe() OUT: age split final split_sec final_sec count 37250.000000 37250 37250 37250.000000 37250.000000 mean 40.697369 0 days 02:03:54.425664429 0 days 04:48:09.303597315 7434.425664 17289.303597 std 10.220043 0 days 00:22:55.093889674 0 days 01:03:32.145345151 1375.093890 3812.145345 min 17.000000 0 days 01:05:21 0 days 02:08:51 3921.000000 7731.000000 25% 33.000000 0 days 01:48:25 0 days 04:02:24 6505.000000 14544.000000 50% 40.000000 0 days 02:01:13 0 days 04:44:25 7273.000000 17065.000000 75% 48.000000 0 days 02:16:11 0 days 05:27:36 8171.000000 19656.000000 max 86.000000 0 days 04:59:49 0 days 10:01:08 17989.000000 36068.000000 居然年龄上最大的数据是86, 让我们看看特征的数据分布: 123%matplotlib inlineimport seaborn as snsax=sns.boxplot(x=df['age']) 这个箱线图反应了, 数据里确实有一些“离群值”. 数据分布 研究下数据分布, 看看split_sec和final_sec 1sns.displot(df['split_sec']) 1sns.displot(df['final_sec']) 整体看来,两个特征下的数据都符合正态分布, 但是final_sec的分布图比较胖. 这次我们把gender这个分类特征添加进来: 1sns.violinplot(x='gender', y='final_sec', data=df) 这些看到, 男性运动员在总体上还是比女性运动员要快一些. 寻找优秀的原因 跑马拉松或者了解这项运动的人都清楚, 运动员很关注整个赛程中前后半程的时间比较,好的选手是后半程用时和前半程近似. 因此, 我们来研究下, 这些运动员前后半程用时情况. 12345g=sns.jointplot('split_sec', 'final_sec', data=df, kind='hex')# 绘制一条直线, 作为参考import numpy as npg.ax_joint.plot(np.linspace(4000, 16000), np.linspace(8000, 32000), ':k') 横坐标是splict_sec特征, 即半程用时. 纵轴表示final_sec特征, 全程用时. 途中可以看出, 的确是越优秀的运动员,前半程用时越接近全程用时的一半, 甚至还有少数后半程跑的更快的. 我们做个计算来深入研究下: 12df['split_frac']=1-2*df['split_sec']/df['final_sec']df.sample(5) OUT: age gender split final split_sec final_sec split_frac 2065 35 W 0 days 01:31:41 0 days 03:14:40 5501.0 11680.0 0.058048 9001 43 W 0 days 01:58:19 0 days 04:00:44 7099.0 14444.0 0.017031 30039 34 M 0 days 02:25:17 0 days 05:39:21 8717.0 20361.0 0.143755 27456 62 W 0 days 02:13:28 0 days 05:25:01 8008.0 19501.0 0.178709 13335 41 M 0 days 01:45:36 0 days 04:21:00 6336.0 15660.0 0.190805 用直方图再增加一个参考线来看看split_frac特征中的数据分布: 1234import matplotlib.pyplot as pltsns.displot(df['split_frac'], kde=False)# 垂直于 x 轴的直线，0 表示 x 轴位置plt.axvline(0, color='k', linestyle='--') 从这张图中, 更清晰的看到全体参赛者的运动安排. 再来探究下不同特征之间的关系: 12345sns.pairplot( data=df, vars=['age','split_sec','final_sec','split_frac'], hue='gender') 让我们来看下80岁选手的数量: 1234(df.age&gt;=80).sum()# OUT15 下面, 我们划分下年龄段,看看各年龄段的成绩分布: 12345678910df['age_dec']=df['age'].map(lambda age: 10*(age//10))sns.violinplot( x='age_dec', y='split_frac', hue='gender', data=df, split=True, inner='quartile', palette=['lightblue', 'lightpink']) 看这张图, 我们发现,不同性别的运动员的split_frac特征数据分布中, 年龄越大,前后端的时间分布比相对集中. 再看看全程用时分布比较: 123456789sns.violinplot( x='age_dec', y='final_sec', hue='gender', data=df, split=True, inner='quartile', palette=['lightblue', 'lightpink']) 从30岁往后, 明显年纪越大,用时越长.","link":"/marathon/"},{"title":"NFC和O2O","text":"总体来说,这是我最看好的移动应用之一! 注意,是之一. 首先,我要说的是SNS网站真的没有再大的作为了!人们已经开始从那些疯狂中开始慢慢冷却了下来,而真正实际用到SNS网站的人也是小部分而已.造势,炒作,宣传,营销...当然,我不能否认这些平台在作为传统媒体的延伸甚至是替代品上的功效. 可是又真有多少人会真的坚持活在这种亢奋的状态下.除非有大批的粉丝不停的为自己打鸡血吧? 所以我总认为,移动社交产品可以打住了.至于查找附近聊友的功能,诸如微信以及添加新功能的米聊以及后来居上的一些产品,不要看宣传的标语很美,实际上还不是沦为宅男腐女查找就近炮友的必备品而已! 好吧,对SNS产品的吐槽结束了,我真正要说的是NFC以及O2O这两个东西! 首先我们需要扫盲一下,NFC就是Near Field Communication,翻译过来就是近距离无线通信，是一种短距离的高频无线通信技术，允许电子设备之间进行非接触式点对点数据传输，在十厘米（3.9英吋）内，交换数据。这个技术由免接触式射频识别（RFID）演变而来.而我们平时所用的交通卡等一类射频卡种,都是RFID技术. &lt;而O2O(Online to Offline)就不是什么技术了,而是一种互联网商业模式,就是把线上的消费者带到现实中的商店中去在线支付购买线下商品和服务,再到线下去享受服务. 而实际上,团购,就是一种O2O模式.可是我心里的O2O模式,完全不是像团购那样的垃圾.这种模式在中国,已经早早的走入了死胡同,一条死路!我所要说的O2O,是一种集合所有商家优惠折扣的消费资源,并一卡通吃的卡片发行商. 而也终于让我遇到一个,这就是么卡.有兴趣的可以去看看,我可没有要推销么卡的意思,也无意为他写什么行销软文.只是在这里纯探讨而已! 好吧,下面让我们实际展望一下,现在很多手机已经开始内置NFC技术.比如iPhone 4S,三星盖世兔等等,那么,你能理解我说什么了,我们以后也就完全可以不必身上大卡小卡的带在身上,一个手机就OK了.交通卡,银行卡,还有各大商场卖场的贵宾卡折扣卡,都可以完完全全的装载我们的手机里! 这其实已经应该不是什么新技术,手机代替交通卡和信用卡,日本早就开始做先行者了!而实际上,O2O模式在中国也才起头而已,而重点,并不是技术怎样怎样,而是线下的资源掌握的如何.要不团购团队中实际上最大的是营销团队呢. 而像么卡这样将所有会员卡全部装到手机里的模式,一样取决于,谁掌握了最大的市场!当你的卡通吃了全世界大大小小的商家卖场的时候,相信所有人都愿意只带着一部手机走遍天下的感觉! 好了,展望结束...大家想去吧,哈哈!","link":"/nfc-and-o2o/"},{"title":"nitrous.io","text":"世界，真心是美好的啊！ 好吧，再使用nitrous之前，我一直都不知道，原来世界可以如此美好。 不需要Dropbox君，不需要git的master了，而且可以随时随地hexo。 妈妈再也不用担心我的md文档丢失了。。。 这货不仅仅是node.js,还有Rails，python以及Go环境哦。。。 亲们，还在等什么。如果windows下的Ruby环境配置让你想砸机子的冲动，那么赶快来nitrous.io吧！","link":"/nitrousio/"},{"title":"Note about my ubuntu","text":"这是我为了便于自己以后方便而记录的一些关于我ubuntu上设置所需要的内容！当然其中内容都是通用的，只是如果你们要拿去用的话记得将路径以及一些变量变成你自己的！ about system $ sudo gedit /etc/apt/sources.list 12345678910deb http://mirrors.163.com/ubuntu/ karmic main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-bac. kports main restricted universe multiverse $ sudo apt-get update and $ sudo apt-get upgrade and hosts $ gedit /etc/hosts # My Dropbox have new hosts file about ATI Donload form This link 12cd **sudo sh **.run That's ok about Keepass 2 so...apt-get 123$ sudo apt-add-repository ppa:jtaylor/keepass$ sudo apt-get update$ sudo apt-get install keepass2 10.04 所需软件包 1 install mono #[link](http://mono-project.com/DistroPackages/Ubuntu) 1234567Click on &quot;System&quot;, &quot;Administration&quot;, &quot;Software Sources&quot;.Click on the &quot;Other Software&quot; tab.Click on &quot;Add...&quot;, and enter the line:**deb http://badgerports.org lucid main**Click on &quot;Add Source&quot;Click on &quot;Authentication&quot;, then on &quot;Import Key File&quot;Download this [GPG key file](http://badgerports.org/directhex.ppa.asc), ID 0E1FAD0C, and select it in the &quot;Import Key File&quot; windowClick on &quot;Close&quot;, then &quot;Reload&quot; when the pop-up appears. You're all set! 2 $ sudo apt-add-repository ppa:jtaylor/keepass 3 $ sudo apt-get update 4 $ sudo apt-get install keepass2 about Java Download from This link and 1234$ cd **$ tar **.tar.gz(64bit)$ sudo update-alternatives --install &quot;/usr/bin/java&quot; &quot;java&quot; &quot;/home/hivan/software/jdk1.7.0_04/bin/java&quot; 1$ sudo update-alternatives --config java 其实就是配置一下默认路径就OK了！ 或者以下方法： $ gedit ~/.bashrc 末尾添加变量 1234JAVA_HOME=/home/hivan/software/jdk1.7.0_04JRE_HOME=${JAVA_HOME}/jreCLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libPATH=${JAVA_HOME}/bin:$PATH and 123456789$ source ~/.bashrc$ sudo update-alternatives --install /home/hivan/software/jdk1.7.0_04/bin/java 300 $ sudo update-alternatives --install /home/hivan/software/jdk1.7.0_04/bin/javac 300 $ sudo update-alternatives --install /home/hivan/software/jdk1.7.0_04/bin/jar 300 $ sudo update-alternatives --config java #用于替换Java，可能会跳出选择框$ java -version #测试java version &quot;1.7.0_04-ea&quot;Java(TM) SE Runtime Environment (build 1.7.0_04-ea-b19)Java HotSpot(TM) 64-Bit Server VM (build 23.0-b20, mixed mode) about goagent 1234567$ cd$ mkdir bin$ cd bin建立一个proxy.sh file$ gedit proxy.sh输入： python /***/goagent/local/proxy.py save and quit$ chmod 700 proxy.sh about ruby 12345678$ sudo apt-get install apache2 curl git libmysqlclient-dev mysql-server nodejs$ bash -s stable &lt; &lt;(curl -s https://raw.github.com/wayneeseguin/rvm/master/binscripts/rvm-installer)$ echo '[[ -s &quot;$HOME/.rvm/scripts/rvm&quot; ]] &amp;&amp; . &quot;$HOME/.rvm/scripts/rvm&quot; # Load RVM function' &gt;&gt; ~/.bashrc$ source .bashrc$ rvm requirements$ sudo apt-get install build-essential openssl libreadline6 libreadline6-dev curl git-core zlib1g zlib1g-dev libssl-dev libyaml-dev libsqlite3-0 libsqlite3-dev sqlite3 libxml2-dev libxslt-dev autoconf libc6-dev ncurses-dev automake libtool bison subversion$ rvm install 1.9.3$ rvm 1.9.3 –-default #有可能出错**RVM is not a function, selecting rubies with 'rvm use ...' will not work.** 则执行：`$ rvm alias create default 1.9.3 about Rails 12$ gem install bundler rails rdoc #rdoc 为Octopress所需组建$ rails -v #如果“**程序“rvm”尚未安装。**”则检查一下.bashrc的路径配置 about python 123456789$ python -VPython 2.6.6$ curl -kL http://github.com/utahta/pythonbrew/raw/master/pythonbrew-install | bash$ . $HOME/.pythonbrew/etc/bashrc$ pythonbrew install 2.7.1$ pythonbrew switch 2.7.1Switched to Python-2.7.1$ python -VPython 2.7.1 ubuntu 12中默认是2.7，如果要安装3.2和上述步骤一样！改一下版本号 about Sublime Text 2 12345678910#!/usr/bin/env xdg-open[Desktop Entry]Name=Sublime Text 2Comment=Sublime Text 2Exec=/home/hivan/software/&quot;Sublime Text 2&quot;/sublime_textIcon=/home/hivan/software/Sublime Text 2/Icon/128X128/sublime_text.pngTerminal=falseType=ApplicationCategories=Application;Development;StartupNotify=true 设置权限：可执行文件！ and 12$ cd /home/hivan/software/&quot;Sublime Text 2&quot;/$ sudo cp &quot;Sublime Text 2.desktop&quot; /usr/share/applications Ctrl+` 1import urllib2,os;pf='Package Control.sublime-package';ipp=sublime.installed_packages_path();os.makedirs(ipp) if not os.path.exists(ipp) else None;open(os.path.join(ipp,pf),'wb').write(urllib2.urlopen('http://sublime.wbond.net/'+pf.replace(' ','%20')).read()) Ctrl+Shift+P 1 ZenCoding 2 Alignment 3 Markdown 4 setting user { \"ignored_packages\": [] } download setting from this link Finally, input right, you can only choose scim,so... 12$ sudo apt-get install scim$ sudo apt-get install scim-pinyin ... 12scim设置－&gt;全局设置－&gt;将预编辑字符串嵌入到客户端中勾去掉scim设置-&gt;gtk-&gt;嵌入式候选词标 勾去掉 about Retext 123sudo add-apt-repository ppa:mitya57sudo apt-get updatesudo apt-get install retext or This link about Octopress Go to This Link add and remove ppa 12$ sudo add-apt-repository ppa:name/name$ sudo add-apt-repository -r ppa:name/name Temporary end, to be continued...","link":"/note-about-my-ubuntu/"},{"title":"拿福能千人挑战活动","text":"如何加入: 在之前@mg12 那里有看到一个博客广告.右下角的地方有一个拿福能的广告. 就我所见,吴钊童鞋很少会在页面上添加广告的,除非他认为可以做的.这和我的页面上那么多广告不同.所以他的页面一直很清爽!这次少见的添加了拿福能,我想一定有些搞头.所以就去注册了,并且,我从@Denis那里也看到了相应的广告,还看到了一篇拿福能博主北京聚会的Post,而且对其大加赞赏!So,我想,这个广告提供商有搞头. 拿福能是做什么的 总的说来,拿福能是一个广告提供商,可以称作网络营销公司.说起来Feedsky以前做过付费软文大家应该都不陌生了.不过我知道众多的博主都被Feedsky的付费评论给搞伤了,很难相信同类的公司能做的起来.自然,我也有些将信将疑.不过对于博主们来说,有多一份的收入总比没有的强.在国内,写独立博客确实是一件非常让人心酸的事情,在国外独立博主获得丰厚的回报的时候,我们从几年前就开始在探讨国内博客的生存之道,到今天为止仍然还是没有找到合适的出路.那么这次拿福能既然为博主们提供了如此好的一个机遇,大家不妨做做看! 并且,拿福能有一些和其他不同的是,他们有一些线下的付费活动.就如@mg12 介绍的,他们会提供一些电影或者之类的观赏活动,然后在博主回来之后写影评或者剧透.当然,这基本属于一种电影宣传活动!更重要的是,有无数的独立博主可以相互交流.不管你是哪一类的博客,总有一个主题是适合你的! 活动内容 据说,拿福能在国外已经做了好几年了.并且有一些非常成功的案例,而此次开发国内市场,当然是期望在国内也有所作为.所以搞了这么一个千人挑战活动,分发50,000元奖金来招募一千个博主.届时,人满之后这一千个博主将会平分这50,000元奖金. 有兴趣的,可以去拿福能的千人挑战活动主页参看!","link":"/nuffnang-challenge-1000/"},{"title":"pokemon","text":"引入依赖和数据 123456789import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as pltimport ospath = os.path.expanduser('~/data/cbcpv/pokemon/')df = pd.read_csv(path + 'pokemon.csv', index_col=0, encoding='cp1252') 探索数据 1df.sample(5) OUT: Name Type 1 Type 2 Total HP Attack Defense Sp. Atk Sp. Def Speed Stage Legendary # 75 Graveler Rock Ground 390 55 95 115 45 45 35 2 False 82 Magneton Electric Steel 465 50 60 95 120 70 70 2 False 79 Slowpoke Water Psychic 315 90 65 65 40 40 15 1 False 123 Scyther Bug Flying 500 70 110 80 55 80 105 1 False 9 Blastoise Water NaN 530 79 83 100 85 105 78 3 False 对比并了解下数据集的各个特征类型: 12345678910111213141516171819202122df.info()# OUT&lt;class 'pandas.core.frame.DataFrame'&gt;Int64Index: 151 entries, 1 to 151Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 151 non-null object 1 Type 1 151 non-null object 2 Type 2 67 non-null object 3 Total 151 non-null int64 4 HP 151 non-null int64 5 Attack 151 non-null int64 6 Defense 151 non-null int64 7 Sp. Atk 151 non-null int64 8 Sp. Def 151 non-null int64 9 Speed 151 non-null int64 10 Stage 151 non-null int64 11 Legendary 151 non-null bool dtypes: bool(1), int64(8), object(3)memory usage: 14.3+ KB 可以看到Type 2这个特征有缺失值, 其他的没有, 而且显示的为正数型, 很符合数据分析的要求. 接下来用散点图研究特征Attack和 Defense的关系 1234567sns.lmplot( x='Attack', y='Defense', data=df, fit_reg=False, hude='Stage') 我们这里参数使用了fit_reg=False, 隐藏了回归线. 在Seaborn中是没有单独绘制散点图的方法的,但是通过参数设置,实现了散点图的绘制.如果此参数设置为True 接下来用箱线图看下各特征数据分布: 1sns.boxplot(data=df) 这个结果显示出, Total, Stage以及Legendary特征的数据是不适合在这里绘制散点图的, 需要对特征进行适当选择 12stats_df=df.drop(['Total', 'Stage', 'Legendary'], axis=1)sns.boxplot(data=stats_df) 这样,比较清晰的看出几个特征的数据分布情况了, 非数字的特征自动摒弃. 在研究Seaborn, 我们知道还有用i中研究数据分布的函数sns.violinplot, 我们尝试用它绘制特征Attack相对于特征Type 1的数据(这是一个分类行特征)的分布. 123456df['Type 1'].unique()# OUTarray(['Grass', 'Fire', 'Water', 'Bug', 'Normal', 'Poison', 'Electric', 'Ground', 'Fairy', 'Fighting', 'Psychic', 'Rock', 'Ghost', 'Ice', 'Dragon'], dtype=object) 上面显示了特征Type 1中唯一数据, 即数据的值. 123456789101112131415161718192021222324252627282930313233343536373839sns.set( style='whitegrid', rc={ 'rigure.figsize':(11.7, 8.27) # 设置了画布的尺寸 })pkmn_type_colors=[ '#78C850', # Grass '#F08030', # Fire '#6890F0', # Water '#A8B820', # Bug '#A8A878', # Normal '#A040A0', # Poison '#F8D030', # Electric '#E0C068', # Ground '#EE99AC', # Fairy '#C03028', # Fighting '#F85888', # Psychic '#B8A038', # Rock '#705898', # Ghost '#98D8D8', # Ice '#7038F8', # Dragon]sns.violinplot( x='Type 1', y='Attack', data=df, inner=None, # 去掉提琴图中的竖线 palette=pkmn_type_colors)sns.swarmplot( x='Type 1', y='Attack', color='k', # 数据的点的颜色 alpha=0.7 )plt.title('Attack by Type') pkmn_type_colors是一个列表, 列出的颜色对应着特征Type 1中的唯一值. 因为去掉了提琴图内部的竖线,所以整个图没有太乱, 想知道有竖线的是什么样子, 可以注释掉inner=None这个参数. 之前我们删除了三个特征得到了一个变量stats_df引用的数据集: 1stats_df.sample() OUT: Name Type 1 Type 2 HP Attack Defense Sp. Atk Sp. Def Speed # 128 Tauros Normal NaN 75 100 95 40 70 110 数据结果中看出来, 特征HP Attack Defense Sp.Atk Sp.Def Speed都是整数, 在df.info()中也能看出来.现在有需求, 如果把这些特征分布进行可视化, 而且要放到一个坐标系中进行比较? 参考: 先使用pd.melt函数, 将所指定的特征进行归并 123456melted_Df=pd.melt( stats_df, id_vars=['Name', 'Type 1', 'Type 2'], # 保留的特征 var_name='Stat' # 其余特征规定到这一列内)melted_df.sample(10) OUT: Name Type 1 Type 2 Stat value 291 Kabutops Rock Water Attack 115 406 Marowak Ground NaN Defense 110 821 Machoke Fighting NaN Speed 45 129 Gyarados Water Flying HP 95 281 Lapras Water Ice Attack 85 586 Vaporeon Water NaN Sp. Atk 110 483 Nidoqueen Poison Ground Sp. Atk 75 93 Gengar Ghost Poison HP 60 791 Vulpix Fire NaN Speed 65 481 Nidoran‰ªÛ Poison NaN Sp. Atk 40 这样,在melted_df数据集中的Stat特征中的数据就是分类数据, 值是stats_df中被归并的特征名称. 12345melted_df['Stat'].unique()# OUTarray(['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed'], dtype=object) 在此基础上, 我们绘制反应分类特征数据分布的图示. 12345sns,swarmplot( x='Stat', y='value', data=melted_df) 还可以在此基础上,再叠加一层分类: 1234567sns.swarmplot( x='Stat', y='value', data=melted_df, hue='Type 1' # 叠加一层分类)plt.legend(bbox_to_anchor=(1, 1), loc=2)","link":"/pokemon/"},{"title":"picasaweb","text":"这一下就经过了很多日子，本来前些天是想写一些教程的。 现在脑子里的想法大多都被时间冲淡了。只是随便写写吧。 由于我的机子重新装了系统的原因。迅雷里的原始连接我也找不到了，有兴趣的朋友可以自己去找找看 picasa2和picasaweb有很多的不同，最大的不一样大概就是现在没有推出中文版本。一切都是E文。自信能看得懂的朋友可以下载来试试。 picasaweb是和自己的google账号绑定的。google的picasaweb服务在网上有250M的存储空间。可以利用picasaweb直接上传到自己的账号上。我去试验了一下，服务很不错，而且有很多新的功能，可惜的是不支持外连，估计快可以了，因为每一张图片都有自己的地址。 Picasaweb和原版本还有很多新加的功能。其中我比较感兴趣的就是色相搜索。输入color:xxx(比如red)就会有相应色相的相片被搜索出来。效果大家可以自己下载以后自己看看@。在我使用过程中，虽然E文给我找了不少麻烦，但是总体上感觉这次的升级还是有很多实用的功能在里边。 好了，软件谈完了，说说最近看的电影。。我这个人不喜欢在网上下载，质量太侮辱我眼睛。所以我一般都是等到碟版出来以后买回来看。。。。等到现在才看了达芬奇密码和碟三，但是我并不是要向大家说这两部片子，而是我看的另外一部:《东京审判》这是一部设计历史题材和政治题材的片子。1945年日本签署投降书的以后，联盟国法官团赶赴东京对日本的20多位战犯进行审判。长达817次审判和最后一次宣判共818次的审判历程。 印象最深的是曾志伟饰演的日本人。从他嘴里说出来的:狗日的日本鬼子。 不知道这句话是因为深受其害还是因为别的原因。再怎么样，一个日本人不会如此辱骂自己的国家吧。就象中国人自己，如果说了一句中国猪，反映会是怎样的？ so。。。最后还是不得不感叹战争带来的危害。所有的一切，也都仅仅是战争引起来的而已 不过再如何，日本到现在为止的历史观确实让我恶心。我到现在才知道靖国神社原来一共供奉着2000多位战犯，而其中有7位是甲级战犯，而最大的也是最可恶的战端挑起者东条英机，也被供奉在里边。这，是对人类文明的一种侮辱。 OK。。。写到这里了。。该去看书了","link":"/picasaweb/"},{"title":"解决Mac M1 原生Photoshop找不到CEP扩展面板","text":"研究这个问题, 也属于是撞上了! 我现在使用的PS版本为: 在Photoshop内, 我画画时一直使用的是第三方的色轮插件Coolorus, 长这样: 好用与否, 可以说, 谁用谁知道. 前些日子发现Photoshop2022有M1原生版本, 不用再使用转译版本, 我心想, 应该速度上会快很多吧! 兴奋的更新完后才发现, Coolorus面板无论如何找不到了, 原本应该在窗口下的“扩展(旧版)”菜单也找不到. 无论怎么折腾都不行, 而且设置面板里的增效工具也是灰色无法设置: 正当我心灰意冷准备返回2021时, 忽然想到, Adobe早就在PS中启用UXP插件了, 而CEP插件因为历史原因一直也无法完全取消, 那么既然是早就做的事情, 为什么2022版本里全给抹杀了呢, 重点是, 面板里设置项虽然不可点击, 但是还在? 问题应该不是出在版本上, 而是出在M1原生的问题上, 我试着去设置了转译, 再次重新打开PS, 果然不出所料, 扩展(旧版)菜单又回来了. 好吧, 这回知道是怎么回事了, 也就好解决了. 方法一: 返回PS 22.21版本, 简单粗暴 方法二: 讲PS2022设置为Rosetta转译打开方式, 同样简单粗暴! 回答一下可能大家问到的问题: 速度上, 感觉不出有什么太大的变化 是的, Coolous照样无法使用, 我快崩溃了, 正在纠结到底是放弃Coolous使用Photoshop原始色轮, 还是回到2021 反正问题是这么个问题, 解决方案也有了!大家自行抉择吧!","link":"/ps-find-cep-for-m1/"},{"title":"重裝「Yosemite」","text":"重裝系統之後，很多東西需要重裝，特別是開發環境。而開發環境的先後順序和設置，一直是我頭疼的事情。這次就着從新安裝了一遍，把很多東西都記錄下來。之前那個環境被我搞的亂七八糟，並且恢復不回來了。 多大多數次序參照 @mrzhang 系統偏好設置 更改電腦名稱 共享 允許安裝任何來源APP 安全性與隱私 --》通用 設置快捷鍵 鍵盤 --》 快捷鍵 配置VPN以及SSH 很重要，因爲很多源都在牆外了 安裝輸入法 下載並安裝\"Squirrel\" 下載並安裝\"SCU\" 安裝Sublime Text 3 設置package Ctrl+ , and setting: 12345678910111213141516171819202122232425262728293031{ &quot;caret\\_style&quot;: &quot;phase&quot;, &quot;color\\_scheme&quot;: &quot;Packages/Color Scheme - Default/Solarized (Light).tmTheme&quot;, &quot;font\\_face&quot;: &quot;Monaco&quot;, &quot;font\\_size&quot;: 13.0, &quot;hightlight\\_line&quot;: true, &quot;hightlight\\_modified\\_tabs&quot;: true, &quot;ignored\\_packages&quot;: [ &quot;Vintage&quot; ], &quot;indent\\_to\\_bracket&quot;: true, &quot;draw\\_centered&quot;: false, //居中显示 &quot;line\\_numbers&quot;: true, //显示行号 &quot;gutter&quot;: true, //显示行号边栏 &quot;fold\\_buttons&quot;: true, //显示折叠按钮 &quot;fade\\_fold\\_buttons&quot;: true, //始终显示折叠按钮 &quot;rulers&quot;: [], //列显示垂直标尺，在中括号里填写数字，宽度按字符计算 &quot;spell\\_check&quot;: false, //拼写检查 &quot;hot\\_exit&quot;: true, //保留未保存内容 &quot;line\\_padding\\_bottom&quot;: 1, &quot;line\\_padding\\_top&quot;: 1, &quot;scroll\\_past\\_end&quot;: true, //文本最下方缓冲区 &quot;tab\\_size&quot;: 2, // Tab制表宽度 &quot;translate\\_tabs\\_to\\_spaces&quot;: true, //缩进和遇到Tab键用空格替代 &quot;wide\\_caret&quot;: true, &quot;word\\_wrap&quot;: true, &quot;match\\_tags&quot;: true, //HTML下突出显示光标所在标签的两端。 &quot;match\\_selection&quot;: true, //全文高亮当前选中字符 &quot;wrap\\_width&quot;: 80} 編輯設置 /etc/paths 123456/usr/local/bin/usr/local/sbin/usr/bin/usr/sbin/bin/sbin 安裝Xcode 1xcode-select --install 安裝Homebrew 1ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/homebrew/go/install)&quot; PS: 這裏可能會很長時間的等待 設置Sublime終端鏈接 1ln -s /Applications/Sublime\\ Text.app/Contents/SharedSupport/bin/subl /usr/local/bin/sm Git, autojump 1brew install git autojump Oh My Zsh 1curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | sh 設置 ~/.zshrc: 1234export NVM\\_NODEJS\\_ORG\\_MIRROR=&quot;http://npm.taobao.org/dist&quot;[[ -s &quot;$HOME/.nvm/nvm.sh&quot; ]] &amp;&amp; . &quot;$HOME/.nvm/nvm.sh&quot;export NODE\\_PATH=$NVM\\_DIR/$(nvm\\_ls current)/lib/node\\_modules 安裝 NodeJS 12nvm install 0.11.15nvm alias default 0.11.15 安裝rbenv 123456789git clone git://github.com/sstephenson/rbenv.git \\~/.rbenv# 用来编译安装 rubygit clone git://github.com/sstephenson/ruby-build.git \\~/.rbenv/plugins/ruby-build# 用来管理 gemset, 可选, 因为有 bundler 也没什么必要git clone git://github.com/jamis/rbenv-gemset.git \\~/.rbenv/plugins/rbenv-gemset# 通过 gem 命令安装完 gem 后无需手动输入 rbenv rehash 命令, 推荐git clone git://github.com/sstephenson/rbenv-gem-rehash.git \\~/.rbenv/plugins/rbenv-gem-rehash# 通过 rbenv update 命令来更新 rbenv 以及所有插件, 推荐git clone https://github.com/rkh/rbenv-update.git \\~/.rbenv/plugins/rbenv-update 設置~/.zshrc 12export PATH=&quot;$HOME/.rbenv/bin:$PATH&quot;eval &quot;$(rbenv init -)&quot; 其他 安裝Ruby 1234rbenv install -l # list all available versionsrbenv install 2.1.5 # install a Ruby versionrbenv global 2.1.5 # set the global versionrbenv versions # list all installed Ruby versions 配置gem源 1234gem sources -a http://ruby.taobao.org/ -r https://rubygems.org/echo 'gem: --no-document' \\&gt;\\&gt; \\~/.gemrcgem updategem update --system 安装 MongoDB, MySQL 1brew install mongodb mysql 設置開機自啓動「可選」 123mkdir -p \\~/Library/LaunchAgentsln -sfv /usr/local/opt/mongodb/\\*.plist \\~/Library/LaunchAgentsln -sfv /usr/local/opt/mysql/\\*.plist \\~/Library/LaunchAgents 安装 Pow 12curl get.pow.cx | shgem install powder Powder 是一套管理工具 SSH-KeyGen 12ssh-keygen -t rsacat \\~/.ssh/id\\_rsa.pub 安裝Rails, sass, compass 以及 hexo 12gem install rails sass compassnpm install -g hexo 安裝必要工具 1234gem install mysql2gem install capistranogem install capistrano-ext Snippets - Download 安裝其他APP","link":"/reinstall-mac-osx-Yosemite/"},{"title":"How to set up networkx in Chinese","text":"Problem description Hi, everynone, when we use networkx to display Chinese, we will find that Chinese cannot be displayed. Solution Download the font in the attachment; https://github.com/hivandu/practise/blob/master/resource/SimHei.ttf Execute in jupyter notebook 12import matplotlibprint(matplotlib.__path__) Find the path to matplotlib, and then cd to this path, after cd to this path, continue cd, cd to the path map-data/fonts/ttf. Then replace the file DejaVuSans,ttf with the file we just. 1$ mv SimHei.ttf nx.draw(city_graph, city_location, with_labels = True, node_size = 10).ttf Among them, the ttf font used. I have uploaded it to everyone.","link":"/set_chinese_for_networkx/"},{"title":"在Yosemite中设置Pow","text":"在Yosemite中，Pow安装和启动是有问题的。这是因为ipfw被移除了，所以如果要在Yosemite中跑Pow，需要做些设置才可以。 1, 添加文件com.pow到/etc/pf.anchors/目录内 sudo vim /etc/pf.anchors/com.pow 2, 在文件内添加代码: 1234rdr pass on lo0 inet proto tcp from any to any port 80 -&gt; 127.0.0.1 port 20559rdr pass on en0 inet proto tcp from any to any port 80 -&gt; 127.0.0.1 port 20559rdr pass on en9 inet proto tcp from any to any port 80 -&gt; 127.0.0.1 port 20559 NOTE: 代码后一行必须要有换行符，否则会出现语法错误 3, 打开文件/etc/pf.conf 4, 添加代码: rdr-anchor \"pow\"，需要添加到rdr-anchor \"com.apple/*\"下一行 5, 打开文件/etc/pf.anchors/com.apple, 并添加代码: 12load anchor &quot;pow&quot; from &quot;/etc/pf.anchors/com.pow&quot; NOTE: 一样必须有换行符 6, 终端执行: sudo pfctl -f /etc/pf.conf 7, 好了，现在可以打开pf了: sudo pfctl -e","link":"/setting-pow-at-Yosemite/"},{"title":"模拟器的app2sd","text":"一个读者@XXX(因为个人意愿隐掉名字) 发mail询问我关于windows下模拟器app2sd的问题,先不说有没有必要,说一下我测试的结果! 本来我给他回邮件是让他试一下apptosd.apk的,后来越想越不对劲,所以自己做了下测试!结果如下: 然后通过adb shell也无法操作. # mkdir /system/sd/app mkdir /system/sd/app mkdir failed for /system/sd/app, No such file or directory 不过仔细想想,app2sd必须满足的条件我们在windows上根本就不存在,首先我们必须要一个app2sd的支援固件!然后我们需要sdcard分出一个ext2的分区... 而这两个条件全部都不满足!那基本上可以说没有办法! 看以后有没有高手可以实现模拟器上安装修改固件,那么可以安装一个app2sd的固件,而另外一个必须满足的条件就是必须将建立的虚拟sdcard分出一个ext2分区来! 满足了这两个条件,那么所有的都会水到渠成! BTW:下午这位读者给我的回复: 非常感谢！ 我测试的结果跟您是一样的，不过后面那个建目录的不一样。下面是我对您博文上的一点分析。 然后通过adb shell也无法操作. 【XXX】google好象改过linux内核，adb shell登录以后几种命令都有权限限制。在虚拟sd卡上建立文件夹有所有权限，然而用adb push传上去的就没有可执行的权限，使用chmod命令修改权限也不成功。 # mkdir /system/sd/app 12mkdir /system/sd/appmkdir failed for /system/sd/app, No such file or directory 【XXX】在system目录下adb shell 命令是没有写权限的。你这个尝试如果是mkdir /system/sd就会报“mkdir failed for sd, Read-only file system”的错误，但是在data目录下就能够创建目录。 不过仔细想想,app2sd必须满足的条件我们在windows上根本就不存在,首先我们必须要一个app2sd的支援固件!然后我们需要sdcard分出一个ext2的分区… 而这两个条件全部都不满足!那基本上可以说没有办法! 看以后有没有高手可以实现模拟器上安装修改固件,那么可以安装一个app2sd的固件,而另外一个必须满足的条件就是必须将建立的虚拟sdcard分出一个ext2分区来! 【XXX】appsd的固件这个是什么概念？用mksdcard创建的虚拟sdcard不就是对应的手机上的sdcard么？虚拟sdcard为什么要分出一个ext2分区呢？ext2分区一个什么概念，sdcard要分出ext2分区的原理是什么？ 能否简单介绍一下？或者介绍一下相关的资料？谢谢。 另：对您给我传的那个apk我不是很了解，这个文件从哪里来的，它都做了些什么事？ 下面是我adb shell后ls –l查看到的各文件夹权限，是有加载虚拟sdcard的。 123456789101112131415drwxrwxrwt root root 2009-08-10 04:45 sqlite_stmt_journalsdrwxrwx--- system cache 2009-07-21 09:01 cached---rwxrwx system system 2009-08-10 04:52 sdcardlrwxrwxrwx root root 2009-08-10 04:45 etc -&gt; /system/etcdrwxr-xr-x root root 2009-05-15 00:53 systemdrwxr-xr-x root root 1970-01-01 00:00 sysdrwxr-x--- root root 1970-01-01 00:00 sbindr-xr-xr-x root root 1970-01-01 00:00 proc-rwxr-x--- root root 9075 1970-01-01 00:00 init.rc-rwxr-x--- root root 1677 1970-01-01 00:00 init.goldfish.rc-rwxr-x--- root root 106568 1970-01-01 00:00 init-rw-r--r-- root root 118 1970-01-01 00:00 default.propdrwxrwx--x system system 2009-05-15 00:58 datadrwx------ root root 1970-01-01 00:00 rootdrwxr-xr-x root root 2009-08-10 04:46 dev 这里请注意system，sdcard和data它们的权限以及各自的意义。 123d---rwxrwx system system 2009-08-10 04:52 sdcarddrwxr-xr-x root root 2009-05-15 00:53 systemdrwxrwx--x system system 2009-05-15 00:58 data 下面说下我对各个信息的理解。首先第一列这是表示的各用户的权限，d代表这是文件夹，rwx分别代表读、写、执行权限。 d后面第一组三个字符表示当前用户的读写执行权限，第二组代表group用户的权限，第三组表示other用户的权限。 然后是第二列，表示当前用户对该文件夹的权限级别，第三列代表该文件夹的当前用户。 如果我对这组信息的含义理解方式正确的话，那么这里我就有疑问了： 1. linux下面有system这个权限级别吗？我有个同事说只有root、group和other，所以我很奇怪这里的system这个权限级别是怎么回事，它有什么样的权限，能做到怎么样。 2. sdcard这个目录非常奇怪，自己的用户权限都没有，group和other用户却有所有权限，在sdcard目录里面建立的目录权限跟sdcard的权限一样。 3. 我们自己写的应用程序，不知道是属于什么样的权限级别，是作为什么样的用户来访问各目录包括sd卡的，手机sd卡和虚拟sd卡。","link":"/simulator-app2sd/"},{"title":"Run xgboost on Mac and Regression data","text":"The source code: xgboost_regression update at 2021-09-07: Install xgboost on Apple M1 1234567git clone --recursive https://github.com/dmlc/xgboostmkdir xgboost/my_buildcd xgboost/my_buildCC=gcc-11 CXX=g++-11 cmake ..make -j4cd ../python_package/Users/xx/miniforge3/envs/tf/bin/python setup.py install u must install miniforge for M1, conda create -n tf python=3.9.5 Run xgboost In the process of using xgboost, I encountered a small obstacle, that is, xgboost cannot be run normally on the M1 of the Mac. It needs to be tossed. The following is the installation process: 1. Homebrrew is required first 2. Install gcc and cmake 123brew install gccbrew install cmakebrew install libomp 3. Download xgboost package Yes, you cannot use the network package to install, you need to download, compile and install by yourself. Fortunately, the process is not troublesome: Source: http://mirrors.aliyun.com/pypi/simple/xgboost/ I downloaded xgboost-1.4.2.tar.gz 4. Installation Enter cd ~/download/ run 1pip install xgboost-1.4.2.tar.gz Okay, you can introduce it to try 12from xgboost import XGBClassifierxb = XGBClassifier() xgboost Regression load data 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import pandas as pdimport warnings%pylab inlinewarnings.filterwarnings('ignore')# load data from urldf = pd.read_csv('./data/Titanic.txt', sep=',', quotechar='&quot;', encoding='ISO 8859-15')df.info()df.head()# Filter some featuresfeatures = df[['pclass', 'age', 'sex']]# Labellabel = df['survived']features.info()# Missing values ​​are filled with meanfeatures['age'].fillna(df['age'].mean(), inplace=True)features.info()# Divide the datasetfrom sklearn.model_selection import train_test_splittrain_x, test_x, train_y, test_y = train_test_split(features, label, test_size = 0.25, random_state=33)# Feature vectorizationfrom sklearn.feature_extraction import DictVectorizervec = DictVectorizer(sparse = False)train_x = vec.fit_transform(train_x.to_dict(orient='record'))test_x = vec.transform(test_x.to_dict(orient='record'))# Random forest training and predictionfrom sklearn.ensemble import RandomForestClassifierrfc = RandomForestClassifier()rfc.fit(train_x, train_y)print('The accuracy of random Forest Classifier on testing set:', rfc.score(test_x, test_y))&quot;&quot;&quot;The accuracy of random Forest Classifier on testing set: 0.7781155015197568&quot;&quot;&quot;# xgboost training and predictionfrom xgboost import XGBClassifierxb = XGBClassifier()xb.fit(train_x, train_y)print(f'The accuracy:', xb.score(test_x, test_y))&quot;&quot;&quot;The accuracy: 0.7750759878419453&quot;&quot;&quot;","link":"/run_xgboost_on_M1_and_regression/"},{"title":"sketch中打开高版本文件","text":"sketch也不知道什么时候开始年费化了，也不能打开高版本文件了。（妈蛋） 据说是为了促进销量和保护版本。 打开包文件，然后打开包内的meta.json 替换头部: 1{&quot;commit&quot;:&quot;335a30073fcb2dc64a0abd6148ae147d694c887d&quot;,&quot;appVersion&quot;:&quot;43.1&quot;,&quot;build&quot;:39012 替换尾部 1&quot;commit&quot;:&quot;335a30073fcb2dc64a0abd6148ae147d694c887d&quot;,&quot;build&quot;:39012,&quot;appVersion&quot;:&quot;43.1&quot;,&quot;variant&quot;:&quot;NONAPPSTORE&quot;,&quot;version&quot;:88},&quot;version&quot;:88,&quot;saveHistory&quot;:[&quot;NONAPPSTORE.39012&quot;],&quot;autosaved&quot;:0,&quot;variant&quot;:&quot;NONAPPSTORE&quot;} 这里实际有几个key:commit, appVersion, build, version,NONAPPSTORE value替换成相应的值就OK了。","link":"/sketch-open-hight-version-file/"},{"title":"SOLOVE Air-M20000","text":"第一次为产品写评测吧我这是，也不太记得了。不过以下这个电源，觉得值得写上一篇。 「SOLOVE移动电源 Air-M20000」 照例先来几张开箱图 本来应该再早一个月拿到这款电源的，因为产能的原因，跳票了。好吧，后来又因为快递单的遗漏，跳票了更久。不过好东西都是值得等待的。 第一次打开包装盒的时候，拿在手上满满的质感，身躯娇小，容量却很大。不过说起来，因为控制了高宽的原因（高宽仅与一张信用卡大小相仿），厚度不太理想。达到了一枚1元硬币的厚度，虽然握在手上的手感十分舒服，但是如果对于想揣在衣服口袋里的人来说，可能这个厚度稍稍有点不甚让人满意，说起来，10000mAh的容量，能做到如此地步已经很不容易了。 而对于SOLOVE Air-M20000的设计，相信也是能俘获很多人的心。电源灯的效果真的是很漂亮。 也许是因为控制尺寸的原因吧，SOLOVE Air-M20000并没有像其他大容量移动电池一样配备两个输出插孔，只有一个。输入Micro-USB, 输出为USB-A. 不过联想多数时候，我的小米电源另一个输出口都空的情况，其实一个已经足够了，重点是轻便易携带。 上周五这款电源已经到货了，没有第一时间拿来写当然是为了接受下周末的检验。 实际使用中，周五的晚上电源满电，周六出门12点半到下午6点，iPhone 6从3点开始电量百分之15，接入SOLOVE，1小时后达到92%，期间不停的在发微信。然后取下电源，周六没有为SOLOVE充电，周日12点多左右出门，到下午6点多回来，iPhone 6经历了两次空电的情况。而两次SOLOVE都将电量充满。 实际使用情况下，SOLOVE的容量还是不错的，iPhone 6来回充满三到四次应该是不成问题，更重要的是，充电速度很快。在不断使用过程中，一个小时基本就可以完全充满。当然，每个不同型号的手机可能都有差别。 所以SOLOVE的Air-M20000, 无论是从设计，做工还是续航情况，都还是蛮值得入手的。唯一遗憾的点，为什么给了我一个红色的，而不是黑色或白色的。。。。","link":"/solove/"},{"title":"玩转Stable Diffusion WebUI 各类模型","text":"Stable Diffusion WebUI 最有意思的地方不是在安装好之后生成图像，而是各种各样的模型。 提前警告：如果你的硬盘空间不够大的话，还是不要随便玩模型了，随随便便就是好几 G，又得甚至于 10 多个 G。 目前我仅留了最常用的 SD V1.5 和 SD V2.1两个模型，大小为 13G。 另外还需要说明一点，就是我曾经测试过用 NAS 来存储模型使用，完全不能用，暂时没有时间具体去研究到底什么原因。只有老老实实的继续在本地硬盘上跑。所以 NAS 上存了大量模型，真需要用到的时候再复制过来。 写这篇文章也是因为近期玩模型过程中打算整理一波，一是方便自己，二么也算是对其他小伙伴做些贡献。 Stable Diffusion 各种模型层出不穷，要说完估计需要费一番功夫，所以我摒弃其他小模型，只整理收集大模型，就是 ckpt 和 safetensors。如果你也打算跟着我一起玩模型但是还未安装，可以先参看我之前的文章： 在 Apple Silicon M1/M2 Mac 上安装和运行Stable Diffusion 说实话，我找了好多关于如何在 M1/M2 上安装和运行 Stable Diffusion 的教程和帖子，发现相互之间借鉴的不少，但是能用的确实没几个。 寻找一番后，发现其实没那么复杂。也不知道为什么网上的那么多教程搞得那么复杂，又是这个又是那个的一大堆，简单实现的方式有好几种： https://www.hivan.me/How%20to%20install%20and%20run%20Stable%20Diffusion%20on%20Apple%20Silicon 还是先从最基础的模型开始： Stable Diffusion 其他多数模型基本上都是从这个基础模型上再次训练得到的。 Stable Diffusion v2.1 SDv2.1提升了人物生成能力，因为SDv2.0大量增加了风景、建筑物和动物的数据集，减少了人物的学习量。 SDv2.1提高了NSFW过滤器准确度，因为SDv2.0的成人过滤器过滤的太狠，错误判定很多 即使是极端长宽比的图像也能顺利生成。 解剖学的身体和手（特别是手掌）的描写精度提高。 512 X 512 model : stabilityai/stable-diffusion-2-1-base · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-1-base 768 X 768 model: stabilityai/stable-diffusion-2-1 · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-1 img2img model stabilityai/stable-diffusion-2-depth · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-depth 重绘model stabilityai/stable-diffusion-2-inpainting · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-inpainting 超分 model stabilityai/stable-diffusion-x4-upscaler · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler Stable Diffusion V 1.5 runwayml/stable-diffusion-v1-5 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main Stable Diffusion V 1.4 CompVis/stable-diffusion-v-1-4-original · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/CompVis/stable-diffusion-v-1-4-original NovelAI 大名鼎鼎的 NovelAI，属于商业泄露模型。经过人在回路精细微调，可以生成高质量的二次元图像。但是千万时刻记得这个可是商用泄露模型，要注意避免法律风险： pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/animefull-latest.tar Waifu Diffusion 基于 Stable Diffusion 模型训练得到，增加了动漫及人物训练得到的模型，基本平时各种公开场合看到 WD 就是他。 WD 和 NovelAI 模型有些同质化，但是 NovelAI 实际是商用模型泄露，在某些使用情况下是有风险的。而 WD 不是，不过也不是说他绝对安全，毕竟 WD 也使用 Danbooru 进行学习，所以如果你关心这个需要注意一点。 Waifu Diffusion V1.5 这个模型使用是需要一个 yaml 文件的，究其原因是这个模型是基于 SD V2 得出的，需要把和 Model 同名的 yaml 文件放在模型所在的文件夹下，目前 1.5 模型是 beta2 版本，持续迭代 ing… Waifu Diffusion v1.5 beta waifu-diffusion/wd-1-5-beta · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/waifu-diffusion/wd-1-5-beta VAE(1.4 VAE 通用) vae/kl-f8-anime2.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt YAML waifu-diffusion/wd-1-5-beta2 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/waifu-diffusion/wd-1-5-beta2/tree/main/checkpoints Waifu Diffusion V1.4 和 1.5 版本一样，基于 SD V2得到的，依然需要下载 yaml 文件放在 model 同文件夹下。 Waifu Diffusion V 1.4 wd-1-4-anime_e1.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e1.ckpt wd-1-4-anime_e2.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e2.ckpt VAE(1.5 通用） vae/kl-f8-anime2.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt YAML e2 和 e1 是通用的，但是需要改名 hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/tree/main *Elysium Anime* 生成偏真实风格的动漫图片，风格比较偏向西式，光影还不错。 模型推荐写下面这些负面提示，可有效提升质量。 1lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry Elysium_V1 偏真实风的模型，手画的还不错，模型底稿基本是以西方人为主，所以生成的脸也偏西方人。 Elysium_V1.ckpt · hesw23168/SD-Elysium-Model at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hesw23168/SD-Elysium-Model/blob/main/Elysium_V1.ckpt *SD_Elysium_Kuro_Model* 与Anything 4.0、WD 1.4等合并后经过微调的二次元用模型。已经包含 WD 的“kl-f8-anime2”VAE 文件，因此无需使用额外的 VAE 文件 Elysium_Kuro_Anime_V1.safetensors · hesw23168/SD_Elysium_Kuro_Model at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hesw23168/SD_Elysium_Kuro_Model/blob/main/Elysium_Kuro_Anime_V1.safetensors *Elysium_Anime_V3* 动漫的附加学习模型，NSFW化相当严重，有更清晰的轮廓和轻微的三维效果。基于Elysium_V1 Elysium_Anime_V3.safetensors · hesw23168/SD-Elysium-Model at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hesw23168/SD-Elysium-Model/blob/main/Elysium_Anime_V3.safetensors *Anything系列* Anything是个神奇的二次元模型，据说是基于几十种模型融合+未知图片训练而来，随便写几个提示，就能到的不错的结果。不过这个模型整个就是一团混沌，实际训练模型，过程，方法，作者全部都是未知的。模型容易过拟合，非专业人士，请不要在此基础上训练模型。 Anything v3.0 “应该”是基于NAI模型+WD+SD等几十种模型+手部图片强化训练得出的。实际训练模型，过程，方法，作者全部都是未知的。如果没有.vae.pt，图片整体颜色浓度（饱和度）会更很浅。PS：Anything v3.0 的 .vae.pt 文件可以用于 NAI。 Anything V3.0 fp16: magnet:?xt=urn:btih/:45cd353ac4fa87098db5e3a6a349539710a3a1f5&amp;dn=Anything-V3.0-fp16.zip Anything v3.0 fp32: magnet:?xt=urn:btih/:d9db662ab5ace77004b3348c23c9381380c27156&amp;dn=Anything-V3.0-fp32.zip Anything v3.0 full-ema: magnet:?xt=urn:btih/:80460036625fb61dce4bc6e7dab744744309a2a0&amp;dn=Anything-V3.0-fullema.zip huggingface.co https://huggingface.co/Linaqruf/anything-v3-better-vae/tree/main Anything v4 自称是Anything最新版本的模型，实际一切都是未知的。仅需几个提示即可生成详细的 2D 插图的能力以及使用 danbooru 标签的能力。整体比过拟合的v3更自然，人物姿势等更容易操作。 anything-v4.0-pruned.safetensors · andite/anything-v4.0 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/andite/anything-v4.0/blob/main/anything-v4.0-pruned.safetensors Anything v4.5 貌似是Anything v4的进化，但实际一切都是未知的。比v4画风更柔和一点。 anything-v4.5-pruned.safetensors · andite/anything-v4.0 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/andite/anything-v4.0/blob/main/anything-v4.5-pruned.safetensors Zeipher 生成更符合真人解剖结构的真人模型，训练集以女性图像为主官方网站是 https://ai.zeipher.com，已经关闭。请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 F222 f222.safetensors · acheong08/f222 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/acheong08/f222/blob/main/f222.safetensors F111 f111.ckpt · Reachout/F111 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/Reachout/F111/blob/main/f111.ckpt 3DKX 因为Zeipher官方已经GG，这是热心网友创建的衍生3DKX模型如果你想让你的 3D 角色有一张更“二次元”的脸，提示词最开始写 “3d cartoon of”，或者如果你想要经典的 3D 渲染外观，写“a 3d render of”高分辨率模型，推荐分辨率为 1152 x 768 或更高 3DKX_1.0b f111.ckpt · Reachout/F111 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/Reachout/F111/blob/main/f111.ckpt R34 从网站“rule34.xxx”的 150,000 张图像中进行训练。rule34.xxx几乎全是NSFW图片，所以你懂的 r34_e4 1.99 GB file on MEGA https://mega.nz/file/yJgDUCzA#zOD2yeE6QLBqPEjEpIi2b4FWOlb64yVUveOd_eW6teI 磁力链接：magnet:?xt=urn:btih/:ed9f0e3f849d7119107ef4e072c6abeb129e1a51&amp;dn=r34_e4.ckpt EVT pixiv排行榜模型 基于pixiv排行图片训练，夹杂有部分R18排行图片 Evt_V4_e10_ema Evt_V4_e10_ema.safetensors · haor/Evt_V4-preview at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/haor/Evt_V4-preview/blob/main/Evt_V4_e10_ema.safetensors EVT_V3 haor/Evt_V3 · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/haor/Evt_V3 EVT_V2 haor/Evt_V2 · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/haor/Evt_V2 Basil_mix 逼真的真人模型，基于亚洲风格训练，支持Danbur标签提示词需要加载VAE，不然画面色彩浓度和边缘会很淡提示词应尽可能简单不要堆砌大量质量标签和负面提示，不然会适得其反。请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 basil_mix basil mix.ckpt · nuigurumi/basil_mix at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/nuigurumi/basil_mix/blob/main/basil%20mix.ckpt VAE vae-ft-mse-840000-ema-pruned.ckpt · stabilityai/sd-vae-ft-mse-original at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.ckpt Chillout Mix 逼真的真人模型，基于亚洲风格训练，支持Danbur标签提示词请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 chillout mix _ NiPruned Fp32 Fix chilloutmix_NiPrunedFp32Fix.safetensors · Inzamam567/useless_Chillout_mix at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/Inzamam567/useless_Chillout_mix/blob/main/chilloutmix_NiPrunedFp32Fix.safetensors Uber Realistic Porn Merge 如名字所说，逼真的真人Porn模型，简称 URPM 模型请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 Uber Realistic Porn Merge Uber Realistic Porn Merge (URPM) | Stable Diffusion Checkpoint | Civitai For early access builds and to support daily work on URPM, please check out my patreon! https://www.patreon.com/uber_realistic_porn_merge , or disc... https://civitai.com/models/2661/uber-realistic-porn-merge-urpm","link":"/stable-diffusion-webUI-models/"},{"title":"vue 2.0 自定义filter并挂载到全局使用","text":"其实早几天前就进入了第三部分,而因为第二部分面向对象程序设计糊里糊涂,搞得再接下来的学习里有很多实例根本看不懂,或者很难回忆起所学的知识点.不得不回头又一个字一个字的老老实实的看了一遍,并且将每个字都敲出来更新到了Sites上!至此算是比较牢固了..而更新的部分,相信对于想进入Java世界的新手们也算是一个好的参考. 其中前两部分分为八章,第一部分为基础程序设计,第二部分为面向对象程序设计 ,第三部分则是实战的Java应用程序设计.我询问过一个朋友,说是对于Android开发来说,Java的前两部分属于基础,非常重要.而第三部分也就无所谓了.不过想来,还是将所有的东西全部抓牢以后再开始.毕竟Java已经学到这一步,规规矩矩做一个Java开发者也未尝不可.下面给出Java前两部分的链接,算是再次推广我的Learn Wiki.而第三部分将不会在此更新了.有兴趣的自己跟着我在Sites上的更新翻看吧,更新速度要视我自己的学习速度而定!现在手上的教程为:《Java 开发与实战经典》 第一部分:Java基础程序设计 1.Java概述及开发环境搭建 2.简单的Java程序 3.Java基础程序设计 4.数组与方法 第二部分:Java面向对象程序设计 5.面向对象(基础篇) 6.面向对象(高级篇) 7.异常的捕获与处理 8.包及访问控制权限 第三部分将不会继续在这里给出链接.请自行查看 http://learn.hivan.me/index/home/java-learn/Java-DAP","link":"/the-third-part-of-the-java-learning/"},{"title":"vue 2.0 自定义filter并挂载到全局使用","text":"vue 2.0 开始，取消默认filter, 需要自定义。 而自定义之后每次在需要使用的组件内引用也确实蛮麻烦的。 所以我们就来将定义的filter挂载到全局使用。 vue2.0 filter相关文档 定义 引用 挂载 使用 /src/filters/ - format.js 123export default function(val){ ...} index.js 12345import format from &quot;./format&quot;;export default{ format: format,} /src/ - main.js 123456789...import commonFiltes from './filters/index'Object.keys(commonFiltes).forEach(function (key, index, arr) { Vue.filter(key, commonFiltes[key]);})... /src/components/ - xxx.vue 1234567&lt;template&gt;...&lt;div&gt;{{ data | format }}&lt;/div&gt;&lt;/template&gt;&lt;script&gt;...&lt;/script&gt;","link":"/vue2.0-custom-filter-to-global/"},{"title":"vux更改Tabbar选中状态","text":"在vux的文档和示例中，都没有明确的说明tabbar上v-model的使用 文档中将v-model说明放在了TabbarItem示例下，但是其实这个应该是放在Tabbar上 1234567891011121314151617181920212223&lt;template&gt; &lt;router-view class=&quot;view&quot; v-on:changeTab=&quot;changeTab&quot;&gt;&lt;/router-view&gt; &lt;tabbar v-model=&quot;index&quot;&gt; &lt;tabbar-item&gt;&lt;/tabbar-item&gt; ... &lt;tabbar-item&gt;&lt;/tabbar-item&gt; &lt;/tabbar&gt;&lt;/template&gt;&lt;script&gt;data(){ return{ index:0, ... }}methods:{ changeTab(num){ ... this.index = num; ... }}&lt;/scirpt&gt; 然后子组件中调用 123mounted(){ this.$emit('changeTab', 2)} 这样就便于在不同的组件内都可以更改Tabbar选中状态","link":"/vux-tabbar-selected/"},{"title":"什么是ifttt,ifttt怎么玩?","text":"原帖出自師北寰的網絡日誌，當中很多條目內容也都是來自於Twitter！由於原帖地處偏僻，需要翻山越嶺，所以為了照顧國內用戶，特轉此貼！這也是我為數不多的轉帖之一。當然，我知道，這個服務可能過段時間完全開放後又會和國內無緣！就像G+才出世兩天就被牆了一樣！不過這也充分證明了G+的優秀！而現階段，最好玩的網絡應用就是G+和ifttt。所以一般都是if oo then xx! 什么是ifttt？ ifttt是一坨网站：ifttt.com 即If This Then That，你可以在ifttt上设定一个条件，当达到你设定的一个条件时，便触发一个（你指定）动作。这里的「条件」和「动作」是指开放的互联网服务，比 如flickr，twitter，facebook，youtube等。别问我有没有新浪微博、人人网，优秀且可靠的互联网服务都在国外。 国内没法抄ifttt。ifttt最重要是服务稳定，就国内普遍鸡贼的情况来看，if端服务时不时封闭一下是常事。创业企业抄好了，大公司眼红把自己网站接口关闭再抄一个出来，创业公司就得玩完。大公司抄的话，竞争对手们也会关闭自己的服务，这玩意儿在国内没法玩。 ifttt的稳定性是关键，可以定制一连串的if…then任务出来，但如果中间某一个服务出问题，后面的任务就全失效了（当然，这么定制也挺笨的）。 ifttt非常重要的一个优点是，将常用服务（twitter，加星或分享的Google reader条目，加过标签的instagram和flickr照片）中的重要资料，全部发送到一个存储服务（Dropbox，evernote，Gmail），需要用的时候检索起来将非常方便。 ifttt可以怎么玩？ 好玩的可以有：if某女谈论「失恋」、「男友+讨厌」、「伤心」、「难过」，then 发送一条短信。ifttt泡妞必备… 非常实用的应用可以有：New fav tweets to evernote ifttt还解决了我以前在北京十分急需的一个功能：if 北京美国大使馆空气监测站的空气质量指数超过250，then 发送一条短信…当然，其它方法也能实现，但ifttt方便太多了。 未来的应用有：ifttt的出现真可以实现未来你挂了也能一个人办丧事：if三十天未发推，then启动一系列任务：1.发邮件告诉殡仪馆来收尸（亲，你可以看我的google location）；2.自动转账；3.发表遗书告诉亲友可于30天后到某处悼念；4.分享生前录好的视频，最后再操一遍GFW。 在未来，随着越来越多社交服务的出现，以及多条件任务功能的推出，玩ifttt的花样将呈爆发式增长，乐趣无穷。 ifttt还可以有什么玩法？ @mranti: ifttt应用举例：if 某男A和某女B同时check in同一个地方，then 短信我的手机：“A和B有奸情，而且正在进行”。八卦利器啊！ @hecaitou: 理想状态下的ifttt应用场景：一旦老婆的推上出现“加班”字样，立即激活一条手机短信通知。同时，自动检测谷歌日历，找出几个今晚没有事情的老友。随后，在FB上新建一个活动“今晚喝大酒”，一旦超过3人同意，触发一条订餐消息给餐厅。餐厅查询Evernote，找到这群人最喜欢的菜和酒。on Twitter: http://twitter.com/hecaitou/status/85927850749857792 @mranti: ifttt应用举例：if 明天下雨，发推DM给自己的心仪女友：“亲爱的，明天出门带伞，我是你的阳光”。 on Twitter: http://twitter.com/mranti/status/85927810924937219 最后这两条和菜头和安替的推是我在twitter fav之后，在写这篇文章过程中自动保存到evernote的，服务十分流畅，文章写起来太方便了有没有？ —-以下内容为下午四点十五分更新—- 出去溜达了一圈回来，脑子里一直在想ifttt，ifttt简直是个太科幻的产品了，第一次觉得人工智能——不对，是机器智能——离自己这么近。ifttt比Google、Facebook都要伟大得太多。 ifttt就是一个反射，它把你想象得到的任何一个动作反射为另外一个你能想象得到的动作，并且它不像生物体一样会被躯干束缚。ifttt上将出现拉马克进化？ @hecaitou: ifttt里面，如果在Channel之上，提供一个Task的自由市场。让各种Geek做出各种奇奇怪怪的Task来，用户添加Task而不是点选Channel，那就连盈利的问题都解决了。 on Twitter: http://twitter.com/wuyagege/status/85959272638324736 @mranti: 在ifttt的世界里面，各位姑娘小心了，什么恋爱短信啊、花啊、DM关怀啊、贴心礼品啊，都可能是程序的Task算出来的。而且ifttt的世界中，一个人死了，他对一个女生的关心也可以一直持续下去，仿佛天天都在。 on Twitter: http://twitter.com/mranti/status/85974845216665600 @boatman: ifttt神就神在即使被墙，只要设置好this和that的关联性，墙并无法阻止this触发that，除非GFW把所有的channel全部封锁才有可能抑制ifttt，但当ifttt支持自定义channel时，就是神也难救方滨兴。 on Twitter: http://twitter.com/Ryan_XxOo/status/85975132866220032 @Doriscafe: 我死后，请你替我照顾她。每天给她发短信叫早，订花，在推特上mention她，赞她，天气好提醒加衣，天气不好提醒带伞，请你替我照顾她，只要服务器不倒下，就直到永远。#ifttt on Twitter: http://twitter.com/Doriscafe/status/85975909429018624 @juicy_luna: 我个人觉得吧，#ifttt 就是把生物里的神经反射运用进了网路里，甚至还会扩展到物质生活。也就是说，它担负起神经链的作用，将能把一切行动串联起来，形成纵横的网络。。奇妙的世界。。 on Twitter: http://twitter.com/juicy_luna/status/85976819626549248 @duck_1984: 超级多米诺啊 蝴蝶效应啊 ifttt毁灭世界啊 自寻死路啊愚蠢的人类 还有更多⋯⋯你来补充。","link":"/what-is-ifttt-and-how-to-play/"},{"title":"Yosemite访问用户级服务器目录","text":"升级到OSX 10.10(Yosemite)以后，localhost是可以正常访问的，只是localhost/~user无法打开了，提示403错误。 网上查找资料，说是随着系统的更新，Apache本本更新到2.4.9，PHP也更新到了5.5.14，所以Apache的配置就需要做相应的修改。 首先，我们需要确定打开了Apache 1sudo apachectl start 然后设置允许访问用户目录 修改httpd.conf配置 1sudo subl /etc/apache2/httpd.conf command + f 查找代码，并去掉注释符 # 123456LoadModule authz_core_module libexec/apache2/mod_authz_core.soLoadModule authz_host_module libexec/apache2/mod_authz_host.soLoadModule userdir_module libexec/apache2/mod_userdir.soLoadModule php5_module libexec/apache2/libphp5.soInclude /private/etc/apache2/extra/httpd-vhosts.confInclude /private/etc/apache2/extra/httpd-userdir.conf 修改httpd-userdir.conf配置 1sudo subl /etc/apache2/extra/httpd-userdir.conf command + f 查找以下代码，去掉注释符# 1Include /private/etc/apache2/users/*.conf 修改yourUserName.conf配置 1sudo subl /etc/apache2/users/username.conf PS: username为你的用户名称，如果没有该文件则新建一个，然后将内容修改为: 123Options Indexes MultiViewsAllowOverride NoneRequire all granted 然后设置文件权限为755 1sudo chmod 755 /etc/apache2/users/haibor.conf 最后我们需要重启Apache 1sudo apachectl restart","link":"/yosemite-open-usersite/"},{"title":"从美国三大协会说供应链管理的演变","text":"前面说到，供应链管理从采购、运营和物流管理发展而来，它是对从供应商的供应商到客户的客户的产品流、信息流和资金流的集成管理，以实现对客户价值的最大化，以及供应链成本的最小化。企业之间的竞争不再是企业与企业之间的竞争，而是供应链与供应链之间的竞争。过去二三十年来，美国的汽车业在日本汽车大厂的强大攻势下一败涂地，就是一个供应链战胜另一个供应链的例子。 在美国，这种集成的供应链管理概念不是一蹴而就的，而是经过几十年的演进发展而来的。这里我们从采购、运营和物流职业协会的发展历史出发，阐述供应链管理在美国的发展——要知道，判断一个职能的发展，最简单的方式就是看相应行业、职业协会的发展。 简言之，供应链管理不是一个领域，而是三个。过去不是，现在不是，在可以预见的将来仍会保持在多个领域齐头并进。这从目前美国跟供应链管理相关的行业协会可见一斑。 供应管理协会（ISM） 供应管理协会是世界上规模最大、影响最大的供应管理组织，拥有四万多会员。它发布的采购经理人指数（PMI）跟踪生产、库存、订单量等变化，是美国经济的风向标，被新闻媒体、学术研究、华尔街和政府部门广泛引用。它的前身是美国采购经理联合会（NAPM）。在100多年的发展过程中，伴随着供应管理在美国公司的重要性不断提升，该协会的侧重点从采购发展到供应管理，再到供应链管理。 图1-3简单地表述了在美国，采购从采购代理发展到采购管理，再发展到供应管理的过程。最早的采购是采购代理，即内部客户确定了需求，找好了供应商，价格也往往都谈好了，采购负责签合同、下订单，把东西买回来。当时的专业协会就叫“采购代理人协会”（1915年成立）。后来，采购说，我不但可以帮你下订单，而且可以帮你找供应商、管供应商、谈价钱、谈合同，于是就变成采购管理，协会也改名“采购经理人协会”（1968年）。再到后来，采购说，我不但可以帮你找供应商、管供应商，而且可以处理运输、物流、进出口，把一切打点好，直到产品进了我们的仓库，于是就演变成了供应管理，专业组织也改名“供应管理协会”（2002年）。 但是，供应管理协会的核心仍然是采购与供应管理，而不是广义上的供应链管理。2002年前后，采购经理协会改名为供应管理协会时，有人问，为什么不改为“供应链管理协会”？ISM的答复是供应链管理太广泛，还不够定型。是的，在供应链领域，不管你问美国还是中国的职业经理人，你们的头衔是什么，答复大多是采购、运营或物流，而不是供应链管理。即使是供应链经理，他们的职责也往往侧重某个领域，并不是我们真正意义上的大供应链。但毫无疑问，从采购管理到供应管理，是向供应链管理迈进了一大步。 图1-3 采购管理发展到供应链管理 这也体现在职业认证上。供应管理协会原来的认证是注册采购经理（C.P.M.），在30多年的历史中，全球认证人数超过4万，认证内容覆盖价格、质量、交付、合同管理、供应商选择、供应商谈判、国际贸易、公司管理及人力资源管理等。为适应采购向供应管理的发展，供应管理协会在2008年推出“供应管理专业人士认证”（CPSM），以取代C.P.M.，这标志着向供应链管理更进一步。相对而言，CPSM的要求比C.P.M.更高，也反映了供应（链）管理比采购管理要求更高。[1] [1] 对于CPSM认证的细节，可阅读《供应链管理的职业认证》一文，收在我的《供应链管理：实践者的专家之路》一书中，机械工业出版社于2017年出版；或在我的“供应链管理专栏”（www.scm-blog.com）上查询同名文章。 运营管理协会（APICS） APICS是美国生产与库存管理协会的缩写。与供应管理协会侧重采购相对应，APICS历来侧重于生产与库存管理。为适应向供应链管理发展的趋势，该组织在2004年更名为APICS—运营管理协会（具体如图1-4所示）。考虑到APICS在美国乃至世界的影响，运营管理协会仍旧保留APICS字眼，也显示不放弃在生产与库存控制方面的传统优势。 图1-4 APICS更名，进入供应链管理时代 在美国宏观经济中，APICS的地位没有供应管理协会高，比如没有像供应管理协会的PMI那样有影响力的宏观经济指数。但在生产与库存管理领域，APICS享有崇高的声望。它的生产与库存管理认证（CPIM）在生产企业受到普遍重视。其认证内容侧重于生产的计划、控制和实施，即如何把销售计划转变为需求计划、生产主计划，然后细化到生产计划、物料供应计划，再到生产线的排程和控制，内容包罗万象。自1973年首次推出以来，全球总共有10.7万人得到此项认证。[1] 伴随着更名，APICS在2005年推出“供应链专业人士认证”（CSCP）。从字面上看，该认证针对供应链管理；从内容上看，该认证试图覆盖供应管理的三大范畴（采购与供应管理、生产运营管理、物流管理）；从级别上看，该认证比CPIM高，它要求一定年限的相关工作经验，而CPIM则没有。在过去10多年来，该认证与CPIM并存，可视作CPIM向供应链管理的延伸，以实现APICS在供应链管理领域与供应管理协会（ISM）两分天下的目的。 2014年4月30日，APICS宣布与Supply Chain Concil（SCC）合并。这种合并，一方面是为了抱团取暖—2008年金融危机以来，经济低迷，美国专业协会的经费大减；另一方面是为了挽救运营管理在美国日益衰落的局面——外包盛行，供应链全球化下，很多公司的生产制造被外包给低成本国家的供应商，原来由运营部门负责的任务转移到采购部门，原来的生产、库存、计划等专业人士也纷纷转业，而以这些人为基础的运营管理协会也就每况愈下。 这从他们的董事会成员可以看出：APICS的董事会成员中（2018年），12个董事会成员，只有两位是《财富》500强的副总，其余大多是些总监、经理、顾问[2]——生产外包到别的国家，有些生产运营的专家就只能转入咨询业。作为对比，看看其竞争对手供应管理协会的董事会，13个董事会成员中，只有一两个不是《财富》500强的副总裁、首席采购官级别（2018年）[3]。我们说这些，并不是某相声中说的，两个人拿名片打牌，看谁的名片上的头衔大；而是说，这些职业协会的董事会大部分是志愿者，大企业的高管们是否愿意把自己的时间贡献出来，从侧面反映了一个协会在行业的影响力。 好消息是，运营管理协会APICS这些年来继续与别的职业协会合并。2015年7月，它与原来的美国运输和物流协会合并，算是正式跨入物流领域，也给它的认证库里增加了一个新成员：运输与物流认证（CTL）。通过这一系列的兼并，运营管理协会可以说成为美国三大供应链职业协会中涉猎范围最广的一个。当然，兼收并蓄的风险呢，也是可能变成大而杂，丧失聚焦点，变成三不像。 [1] CPIM Transformed for Today’s Busy Professionals. APICS网站，www.apics.org. [2] 2018 APICS Board of Directors. APICS网站，www.apics.org. [3] ISM Board of Directors. ISM网站，www.instituteforsupplymanagement.org. 供应链管理专业人士协会（CSCMP） 这是美国第三个与供应链管理相关，也有相当影响力的协会。它的前身是物流管理协会（CLM）。顺应物流管理向供应链管理的过渡，供应链管理专业人士协会（CSCMP）试图从物流管理的角度出发，来“蚕食”供应链管理这一热门行业。 物流管理协会的影响更多是在物流教育领域，这从他们的主席人选可见一斑：物流专业的一些知名教授曾担任过该组织的主席，例如国内熟悉的鲍尔索克斯（密歇根州立大学教授，1964~1965年任主席）、门泽尔（田纳西大学教授，2000~2001年任主席）等。 相信在短时间内，供应管理专业人士协会很难大幅增加在工业界的影响，也很难成为一个纯粹意义上的供应链管理协会。这从该协会的董事会可见一斑：15位董事会成员中，6位是教授或来自大学。作为对比，运营管理协会APICS的董事会只有1位教授，而供应管理协会ISM则是清一色的大企业高管。 在发展历史上，供应链管理专业人士协会很好地诠释了从小到大、从部分到全部的发展历程。如图1-5所示，1963年，美国实物配送协会成立，表明最早的物流以运输为主，简单地说就是管着一帮卡车司机，做着把东西从A点搬送到B点的“实物配送”。[1]到了20世纪80年代，这些人说，我们不但可以把东西从A点搬到B点，而且可以对付整个过程的仓储、配送、海关等多道手续，以及伴随而来的信息流，这就变成了物流管理。于是在1985年，实物配送协会改为物流管理协会，覆盖运输以外的更多业务。再后来，物流管理说，我们也可以对付采购、运营的事啊。得，这就变成了供应链管理—2004年，物流协会改名供应链管理专业协会，正式从物流跨入供应链领域。不过对我们供应链领域的人而言，一看你是供应链管理职业人协会的，就知道你的前世今生是物流。 图1-5 从实物配送到物流管理到供应链管理 从认证角度而言，长期以来，这个机构一直没有能与供应管理协会和运营管理协会相匹敌的认证。最近注意到，供应链管理专业人士协会推出了SCPro的认证。整个认证分为三级：第一级是供应链管理核心知识，覆盖供应链管理领域的八个方面；第二级是供应链挑战的分析与实施，基于案例来测试学员对供应链管理知识的应用能力；第三级是供应链转型，需要在学术机构导师的指导下，分析具体企业的真实状况，规划一个供应链改进项目，来取得真实的业务成果，比如提高投资回报率、缩短周转周期等。[2]这个认证目前在国内还没有看到，在美国的影响也尚需建立。[3] 从上述美国三大职业协会的发展可以看出，在可预见的未来，供应链管理仍将以一个综合领域的面目存在，在采购、运营和物流的基础上继续发展。但集成的趋势很明显，不但在行业协会，而且在工业界、教育界。 很多公司在集成采购、运营和物流管理三个部门，设立全球供应链部。ERP软件提供者如SAP、Oracle促进了这一趋势：它们的软件使跨职能协作更加容易。学术界也有越来越多的系、专业改名为供应链管理。美国MBA排名中也增设了供应链管理/物流管理专业，与传统的会计、金融、营销、国际管理等分庭抗礼，说明供应链管理作为一个专业已经形成。 资源 CPSM认证由美国供应管理协会提供，详情见www.ism.ws CPIM和CSCP认证由美国运营管理协会提供，详情见www.apics.org 这些认证在国内都可以参加，其中CPSM已经汉化。 延伸阅读 《供应链管理在国内的发展》，节选自我的另一本书《供应链管理：实践者的专家之路》。十几年前，我在申请北美商学院时，第一次听说供应链管理；七八年前，国内的一些大型企业启动供应链转型；最近几年，越来越多的中小企业着眼供应链，解决日益严峻的成本和库存问题。扫描二维码，阅读全文。 [1] 有个学术刊物，名字叫International Journal of Physical Distribution &amp; Logistics Management，翻译过来就是《实物配送和物流管理国际学刊》。从1970年创刊至今，都快半个世纪了，还看不到一点要寿终正寝的样子。 [2] 见供应链管理专业人士协会的网站：http://cscmp.org [3] 判断一个认证的价值，最简单的就是看招聘网站上，有多少职位要求或者建议应聘者有这个认证。我到Monster.com（这是美国的一个主要招聘网站）上，搜索SCPro认证，只发现两个职位；搜索ISM的CPSM认证，发现487个岗位；搜索APICS的CPIM认证，出来700个岗位；搜索CSCP，有358个岗位（2018年10月10日，搜索时不限职位所在的地域）。再搜索“注册供应链管理师”CSCM认证，发现17个岗位—这个认证最近突然在国内冒起来，到处都有人在宣传，说是国家人力资源和社会保障部认可的，引得很多读者三天两头到我这里求证，问这个认证是不是主办者宣称的那样，是个美国主流认证，这里算是一并答复。","link":"/%E4%BB%8E%E7%BE%8E%E5%9B%BD%E4%B8%89%E5%A4%A7%E5%8D%8F%E4%BC%9A%E8%AF%B4%E4%BE%9B%E5%BA%94%E9%93%BE%E7%AE%A1%E7%90%86%E7%9A%84%E6%BC%94%E5%8F%98/"},{"title":"15. 使用LLMChain连接Google和计算器","text":"大家好，我是茶桁. 在上一节课中，我们学习了如何使用LangChain这个Python包链式调用OpenAI的API。通过链式调用，我们可以将需要多轮询问AI才能解决的问题封装起来，将需要多轮自然语言调用才能解决的问题变成一个函数调用。 然而，LangChain对我们的帮助远不止于此。最近，ChatGPT发布了Plugins插件机制。通过Plugins，ChatGPT可以浏览整个互联网，还可以接入诸如Wolfram这样的科学计算工具，能够解决许多大语言模型难以解决的问题。不过，这是需要Plus用户才可享用的，并且每一个小时内的对话Token都是有限制的。 但是，这并不重要，我们通过LangChain也能实现类似的功能。在今天的课程中，我们将继续深入挖掘Langchain，看看它如何解决这些问题。 解决 AI 数理能力的难题 虽然许多人发现 ChatGPT 在回答各种问题时表现得很好，但是当涉及到计算三位数乘法时，它就显得有些力不从心了。它似乎只是快速估算一个数字，而不是真正准确计算。为了解决这个问题，我们需要进一步研究 AI 数学能力的提升。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E4%BD%BF%E7%94%A8LLMChain%E8%BF%9E%E6%8E%A5Google%E5%92%8C%E8%AE%A1%E7%AE%97%E5%99%A8/"},{"title":"使用Python库unstructured揭秘文本数据","text":"在数据的世界里，文本数据是特别复杂的。它不像数字数据那样被分成整齐的行和列。作为一个副业，我正在开发自己的个人人工智能助手。其目的是利用我的笔记和文件中的数据来回答我的问题。重要的好处是，所有的数据处理都将在我的电脑上进行，确保没有文件被上传到云端，而且我的文件将保持隐私。 为了处理这种非结构化的数据，我发现unstructured的Python库非常有用。它是一个灵活的工具，可以处理各种文档格式，包括Markdown、、XML和HTML文档。 从unstructured的开始 你可以通过以下方式轻松安装该库： 1pip install unstructured 装载和分割文件 你想对你的文件做的第一件事是把它分割成更小的部分或章节。这个过程被称为分区，使其更容易分类和提取文本。 以下是你如何做的： 123from unstructured.partition.auto import partitionelements = partition(filename=&quot;example-docs/note.md&quot;) example-docs/note.md： 123## My test titleAnd here is a sample text. 当我们分割一个文档时，输出是一个文档元素对象的列表。这些元素对象代表了源文档的不同组成部分。unstructured库支持各种元素类型，包括Title, NarrativeText, 和ListItem。要访问元素类型，你可以使用category方法： 1234for element in elements: print(f&quot;{element.category}:&quot;) print(element) print(&quot;\\n&quot;) Output: 123456TitleMy test titleNarrativeTextAnd here is a sample text. 文档元素的列表可以用convert_to_dict函数转换为字典的列表： 123from unstructured.staging.base import convert_to_dictdict_data = convert_to_dict(elements) Output: 1234567891011121314151617181920[{'type': 'Title', 'coordinates': None, 'coordinate_system': None, 'layout_width': None, 'layout_height': None, 'element_id': 'a3114599252de55bea36c288aa9aa199', 'metadata': {'filename': 'sample-doc.md', 'filetype': 'text/markdown', 'page_number': 1}, 'text': 'My test title'}, {'type': 'NarrativeText', 'coordinates': None, 'coordinate_system': None, 'layout_width': None, 'layout_height': None, 'element_id': '6e78562ede477550604528df644630e8', 'metadata': {'filename': 'sample-doc.md', 'filetype': 'text/markdown', 'page_number': 1}, 'text': 'And here is a sample text.'}] 但由于我想把这些文本块存储在数据库中，并对数据进行一些探索性分析，所以我用convert_to_dataframe函数把文本元素转换成pandas数据框架： 123from unstructured.staging.base import convert_to_dataframedf = convert_to_dataframe(elements) 获取元数据 unstructured库的一个整洁的特点是它如何跟踪它从文档中提取的元素的各种元数据。例如，你可能想知道哪些元素来自哪个页码。你可以像这样提取某个文档元素的元数据： 12doc_metadata = elements[0].metadata.to_dict()print(doc_metadata) Output: 1{'filename': 'note.md', 'filetype': 'text/markdown', 'page_number': 1} 当源文件中的信息可用时，所有文件类型都会返回以下元数据字段：filename、file_directory、date、filetype和page_number。 筹备Transformers 当你准备将你的文本送入转化器模型进行进一步处理时，你可以使用stage_for_transformers函数。这个函数通过将你的文本元素分割成适合模型注意力窗口的大块来准备。 在下面的例子中，我使用了一个叫做SentenceTransformers的库： 12345from sentence_transformers import SentenceTransformerfrom unstructured.staging.huggingface import stage_for_transformersmodel = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)chunked_elements = stage_for_transformers(elements, model.tokenizer) And now I can load all the notes in a specific directory, so I can convert them to embedding vectors later: 1234567all_elements = []root_dir = '/corpus'for directory, subdirectories, files in os.walk(root_dir): for file in files: full_path = os.path.join(directory, file) all_elements += partition(filename=full_path) unstructured 的局限性 这个库也有一些问题和限制。 当加载和解析docx文件时，它不能正确地将子弹头识别为ListItem，大多数情况下将它们标记为NarrativeText或Title。这使得标题识别也不可靠，因为当你查看输出时，你无法确定每个标题实际上是一个标题还是一个被错误地标记为标题的列表项。(issue on github) 当处理大型文档时，没有办法知道每个段落或标题的父类是什么。这可能是一个非常有用的功能，特别是在将数据反馈给LLM的时候。 (issue on github) 替代品 在玩了unstructured之后，我试图看看是否有更好的替代品可以用python来阅读文档。虽然我需要加载各种格式的文件，但我缩小了搜索范围，首先找到阅读docx文件的替代品（因为这是你从Google Drive下载一大文件夹的文件时得到的格式）。以下是我找到的东西： python-docx 它看起来很强大，但操作起来很复杂。 我试着加载和解析了几个docx文件。我遇到的最大问题是加载任何包含超链接的文本。由于某种未知的原因，超链接的文本在最后的输出中被返回为空。这使得它不能用于我的目的，因为链接文本提供了文本中的宝贵信息。 优点：它能够为标题提供标题级别的信息（如Heading 1、Heading 2等）。 docx2txt 它在hood下使用 python-docx。 只返回加载的文档的一个巨大的全文字符串。这就要求我把我的文档分割成有意义的小块，这可不是一件容易的事。 优点：它对超链接没有任何问题，而且输出的文本是可读的、有用的。 优点：它也非常容易使用。 simplify_docx 它在 python-docx 的基础上工作。 这个库基本上将python-docx的复杂输出转换为更容易使用的json输出。 它对超链接也有同样的问题，当段落中有一个链接时，会返回空文本。 所以我现在会继续使用unstructured。值得一提的是，使用LangChain或其他类似的工具可以更容易地完成这一点。然而，我建立这个个人AI助手的部分动机是学习之旅。通过使用unstructured加载文档和其他类似工具进行嵌入等，我对底层流程有了更深的了解，而不是使用LangChain这样的一站式解决方案。 我将在未来的文章中分享更多关于我在构建个人人工智能助手方面取得的进展，敬请关注「坍缩的奇点」， 或到外网关注「茶桁- MAMT」。","link":"/%E4%BD%BF%E7%94%A8Python%E5%BA%93unstructured%E6%8F%AD%E7%A7%98%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE/"},{"title":"使用 Transformers 进行语音转文本的完整入门指南","text":"我与音频数据打交道的次数比我意识到的要多得多。 世界上充满了音频数据和亟待解决的相关问题。我们可以使用机器学习来解决其中的许多问题。您可能对用于训练机器学习模型的图像、文本和表格数据以及用于解决这些领域问题的机器学习并不陌生。随着Transformer架构的出现，解决音频相关问题的准确性大大高于之前已知的方法。我们将学习音频ML的基础知识，使用变压器将语音转换为文本，并学习使用Huggingface库通过机器学习解决音频相关问题。 了解音频机器学习的基础知识并获得相关背景知识。 了解如何为机器学习收集、存储和处理音频数据。 了解一项常见且有价值的任务：使用机器学习将语音转换为文本。 了解如何使用Huggingface工具和库来完成音频任务--从寻找数据集到训练模型，并使用它们利用Huggingface Python库通过机器学习解决音频问题。 本文作为AI系列文章的附加部分，但是并不放入系列之内，以保证其整体性。 自 2010 年代初期深度学习革命发生以来，AlexNet 在识别物体方面超越了人类的专业知识，Transformer 架构可能是自那时以来最大的突破。Transformers 使以前无法解决的任务成为可能，并简化了许多问题的解决方案。虽然它最初的目的是为了在自然语言翻译中获得更好的结果，但很快它不仅被应用于自然语言处理中的其他任务，而且还被跨领域应用——ViT或视觉变压器用于解决与图像相关的任务，决策变压器用于决策强化学习代理中的制作，最近一篇名为 MagViT 的论文演示了 Transformer 在各种视频相关任务中的使用。 这一切都始于现在著名的论文《Attention is All You Need》，该论文介绍了导致Transformers 诞生的注意力机制。本文并不假设您已经了解 Transformers 架构的内部工作原理。 尽管在公共领域和普通开发人员领域，ChatGPT 和 GitHub Copilot 是非常著名的名字，但深度学习已经在许多领域的许多实际用例中使用——视觉、强化学习、自然语言处理等。 近年来，我们了解了许多其他用例，例如药物发现和蛋白质折叠。音频是深度学习尚未完全解决的迷人领域之一；从某种意义上说，Imagenet 数据集中的图像分类是通过卷积神经网络解决的。 我假设您有使用 Python 的经验。基本的Python知识是必要的。您应该了解库及其常见用法。 我还假设您了解机器学习和深度学习的基础知识。 不需要具备Transformers 知识，但会有所帮助。 关于音频数据的注意事项：该平台不支持插入音频，因此我创建了一个包含所有代码和音频数据的 Colab 笔记本。你可以在这里找到它。在Google Colaboratory中启动它，您可以从笔记本上播放浏览器中的所有音频。 您可能已经见过音频 ML 的实际应用。说“Hi, Siri”或“Okay, Google”就会启动各自平台的助手——这就是与音频相关的机器学习的实际应用。这种特殊的应用被称为“关键字检测”。 但在这个领域中，使用 Transformer 很有可能解决许多问题。但是，在开始使用 Transformer 之前，让我快速告诉您在 Transformer 之前如何解决与音频相关的任务。 在《Transformers 》出现之前，音频数据通常被转换为梅尔谱图——描述手头音频剪辑的图像，并将其视为一幅图像并输入卷积神经网络进行训练。在推理过程中，音频样本首先被转换为梅尔谱图表示，CNN 架构将基于此进行推理。 现在我将快速向您介绍“librosa”Python 包。这是一个处理音频数据非常有用的包。我将生成一个梅尔光谱图，让您了解它们的外观。您可以在网上找到librosa 文档。 首先，通过从终端运行以下命令来安装 librosa 库： 1pip install librosa 然后，在您的笔记本中，您必须像这样简单地导入它： 1import librosa 我们将使用与库捆绑在一起的一些数据来探索该库的一些基本功能。 1array, sampling_rate = librosa.load(librosa.ex(&quot;trumpet&quot;)) 我们可以看到librosa.load()方法返回一个音频数组以及喇叭声音的采样率。 12345import matplotlib.pyplot as pltimport librosa.displayplt.figure().set_figwidth(12)librosa.display.waveshow(array, sr=sampling_rate) 这会将音频数据值绘制成如下图： 在 X 轴上，我们看到时间，在 Y 轴上，我们看到剪辑的幅度。通过以下方式收听： 123from IPython.display import Audio as audaud(array, rate=16_000) 您可以在我为此博文创建的Colab 笔记本中聆听声音。 使用 librosa 直接绘制梅尔谱图。 1234567891011121314151617import numpy as npS = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=128, fmax=8_000)S_dB = librosa.power_to_db(S, ref=np.max)plt.figure().set_figwidth(12)librosa.display.specshow(S_dB, x_axis=&quot;time&quot;, y_axis=&quot;mel&quot;, sr=sampling_rate, fmax=8000)plt.colorbar() 我们使用梅尔谱图而不是其他表示形式，因为它比其他表示形式包含更多的信息——一条曲线中的频率和幅度。您可以访问有关 Analytics Vidhya 的这篇精彩文章，了解有关频谱图的更多信息。 这正是 Transformer 之前的音频 ML 中的大量输入数据的样子，用于训练卷积神经网络。 正如《Attention is All You Need》论文中介绍的那样，注意力机制成功地解决了与语言相关的任务，因为从高层次来看，注意力头在预测下一个序列时决定序列的哪一部分比其他部分更值得关注令牌。 现在，音频是序列数据的一个非常合适的例子。音频自然是由自然界或我们的语音器官（例如人类语音或动物声音）的振动产生的连续信号。但计算机既不能处理也不能存储连续数据。所有数据都是离散存储的。 音频的情况也是如此。仅存储特定时间间隔的值；这些功能足以听歌、看电影以及通过电话或互联网与我们自己交流。 变压器也处理这些数据。 就像NLP（自然语言处理）一样，我们可以根据不同的需求使用不同架构的Transformer。我们将使用编码器-解码器架构来完成我们的任务。 如前所述，我们将在每个流程步骤中使用 Huggingface 库。您可以导航到 Huggingface 数据集中心来查看音频数据集。我们将在这里计算的数据集是 MINDS 数据集。它是来自不同语言的说话者的语音数据的数据集。数据集中的所有示例都带有完整注释。 让我们加载数据集并对其进行一些探索。 首先，安装 Huggingface 数据集库。 1pip install datasets pip install 确保我们下载的数据集库增加了对音频相关功能的支持。 然后我们探索 MINDS 数据集。我强烈建议您浏览数据集的Huggingface 页面并阅读数据集卡。 在 Huggingface 数据集页面上，您可以看到数据集具有非常相关的信息，例如任务、可用语言和使用数据集的许可证。 现在我们将加载数据并了解更多信息。 123456from datasets import load_dataset, Audiominds = load_dataset(&quot;PolyAI/minds14&quot;, name=&quot;en-AU&quot;, split=&quot;train&quot;)minds = minds.cast_column(&quot;audio&quot;, Audio(sampling_rate=16_000)) 请注意数据集的加载方式。名字在前，我们只对澳大利亚口音英语感兴趣，我们只对训练分组感兴趣。 在输入训练或推理任务之前，我们希望所有音频数据具有相同的采样率。这是通过代码中的“Audio”方法完成的。 我们可以研究个别例子，如下所示： 12example = minds[0]example {‘path’: ‘/root/.cache/huggingface/datasets/downloads/extracted/a19fbc5032eacf25eab0097832db7b7f022b42104fbad6bd5765527704a428b9/en-AU~PAY_BILL/response_4.wav’,‘audio’: {‘path’: ‘/root/.cache/huggingface/datasets/downloads/extracted/a19fbc5032eacf25eab0097832db7b7f022b42104fbad6bd5765527704a428b9/en-AU~PAY_BILL/response_4.wav’,‘array’: array([2.36119668e-05, 1.92324660e-04, 2.19284790e-04, …,9.40907281e-04, 1.16613181e-03, 7.20883254e-04]),‘sampling_rate’: 16000},‘transcription’: ‘I would like to pay my electricity bill using my card can you please assist’,‘english_transcription’: ‘I would like to pay my electricity bill using my card can you please assist’,‘intent_class’: 13, ‘lang_id’: 2} 这很容易理解。它是一个带有级别的 Python 字典。我们已经存储了路径和采样率。查看字典中的转录键。当我们对自动语音识别感兴趣时，它包含标签。[“audio”][“aray”]包含我们将用于训练或推断的音频数据。 我们可以轻松收听任何我们想要的音频示例。 123from IPython.display import Audio as audaud(example[&quot;audio&quot;][&quot;array&quot;], rate=16_000) 您可以在Colab Notebook中收听音频。 现在，我们清楚地了解数据的外观及其结构。我们现在可以继续从自动语音识别的预训练模型中进行推断。 Huggingface hub 有许多模型，可用于各种任务，如文本生成、摘要、情感分析、图像分类等。我们可以根据我们想要的任务对中心中的模型进行排序。我们的用例是语音到文本，我们将探索专门为此任务设计的模型。 为此，您应该导航到https://huggingface.co/models，然后在左侧边栏上单击您想要的任务。在这里，您可以找到可以立即使用的模型，或者找到一个很好的候选模型来微调您的特定任务。 在上图中，我已经选择了自动语音识别作为任务，并且我得到了右侧列出的所有相关模型。 注意不同的预训练模型。像 wav2vec2 这样的一种架构可以有许多针对特定数据集进行微调的模型。 您需要进行一些搜索并记住可用于使用该模型或微调的资源。 我认为Facebook 的wav2vec2-base-960h将适合我们的任务。我再次鼓励您访问模型页面并阅读模型卡。 Huggingface 有一个非常友好的 API，可以帮助完成各种与 Transformer 相关的任务。 之前，我们找到了任务所需的模型，现在我们将其与上一节中看到的 Pipeline 方法一起使用。 首先，安装 Huggingface 变压器库。 1pip install transformers 然后，导入 Pipeline 类并选择任务和模型。 1234567from transformers import pipelineasr = pipeline(&quot;automatic-speech-recognition&quot;, model=&quot;facebook/wav2vec2-base-960h&quot;)print(asr(example[&quot;audio&quot;][&quot;example&quot;])) # example is one example from the dataset 输出是： 1{'text': 'I WOULD LIKE TO PAY MY ELECTRICITY BILL USING MY CAD CAN YOU PLEASE ASSIST'} 您可以看到这与我们上面看到的注释非常匹配。 这样，您就可以从任何其他示例中得到推论。 在本指南中，我介绍了音频数据处理和探索的基础知识以及音频机器学习的基础知识。在简要讨论用于音频机器学习的 Transformer 架构之后，我向您展示了如何在 Huggingface 中心使用音频数据集以及如何通过 Huggingface 模型中心使用预训练模型。 您可以使用此工作流程解决许多与音频相关的问题，并通过利用变压器架构来解决这些问题。 音频机器学习涉及通过机器学习技术解决音频领域现实世界中出现的与音频相关的问题。 由于音频数据存储为数字序列，因此可以将其视为与序列相关的问题，并使用我们已有的用于解决其他序列相关问题的工具来解决。 由于 Transformer 成功解决了与序列相关的问题，我们可以使用 Transformer 架构来解决音频问题。 由于语音数据和音频数据通常由于年龄、口音、说话习惯等因素而存在很大差异，因此针对特定数据集使用微调的解决方案总是更好。 Huggingface 拥有许多与音频相关的解决方案，涉及数据集、训练模型以及使用和调整训练和微调的简单方法。 Huggingface Audio ML 课程，了解有关音频机器学习的更多信息 Allen Downey 的《Think DSP》深入研究数字信号处理 Q1. 什么是音频机器学习？ 答：音频机器学习是使用机器学习技术解决与音频数据相关的问题的领域。示例包括：通过关键字检测打开和关闭智能家居中的灯，通过语音转文本向语音助手询问当天的天气等。 Q2。如何收集机器学习的音频数据？ 答：机器学习通常需要大量数据。要收集音频机器学习的数据，必须首先决定要解决什么问题。并收集相关资料。例如，如果您正在创建一个名为“Jarvis”的语音助手，并希望用“Good day, Jarvis”这句话来激活它，那么您需要收集来自不同地区、不同年龄、属于不同国家的人说出的这句话。多种性别 - 并使用适当的标签存储数据。在每个音频任务中，标记数据非常重要。 Q3。什么是机器学习中的音频分类？ 答：音频分类是一项机器学习任务，旨在将音频样本分类为一定数量的预定类别。例如，如果在银行部署音频模型，则可以使用音频分类根据客户的意图对来电进行分类，以将呼叫转发到适当的部门（贷款、储蓄账户、支票和汇票、共同基金） ， ETC。","link":"/%E4%BD%BF%E7%94%A8Transformers%E8%BF%9B%E8%A1%8C%E8%AF%AD%E9%9F%B3%E8%BD%AC%E6%96%87%E6%9C%AC/"},{"title":"14. 使用链式调用简化多步提示语","text":"Hi, 大家好，我是茶桁。 OpenAI 的大语言模型提供了 Completion 和 Embedding 两个核心接口。 我们可以通过增加提示语（Prompt）历史记录来提高模型的回答准确性和自然性。还可以将 Embedding提前索引好存起来，以此做到让AI根据外部知识来回答问题， 在我们多次与AI对话的过程中，讲AI返回的答案放在新的问题里，那么我们就可以让AI帮主我们给自己的代码撰写单元测试了。 以上这些方法是自然语言类应用中常见的模式。为了方便应用开发者使用这些模式，开源社区开发了名为 Langchain 的开源库，使用 Langchain，我们可以更加快速地实现之前利用大语言模型实现过的功能，并且可以更好地将模型集成到我们的业务系统中，实现更加复杂、有价值的功能。 何谓链式调用 在第 11 讲中，我们学习了 llama-index 的使用，并在此过程中已经安装了 Langchain。虽然 Langchain 也有类似 llama-index 的功能，但这不是 Langchain 的主要卖点。Langchain 带来的第一个主要优势就在于它的名字，也就是链式调用。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E4%BD%BF%E7%94%A8%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%E7%AE%80%E5%8C%96%E5%A4%9A%E6%AD%A5%E6%8F%90%E7%A4%BA%E8%AF%AD/"},{"title":"供应链的根本是协作，那为什么不协作","text":"我们知道，供应链是采购把东西买进来，生产来加工增值，物流负责配送给客户。自从有现代企业以来，就有人做采购，有人做生产运营，有人做物流配送。也就是说，采购、运营和物流管理由来已久，至少有上百年的历史，那为什么供应链管理是个新概念，直到20世纪80年代才出现？根本原因：单一指标驱动下，职能之间山头林立，协作度低，形不成供应链。让我细细道来。 职能之间山头林立，协作度低，形不成供应链 传统模式下，企业是职能导向，驱动员工行为的是职能目标，即自上而下的目标。比如对采购来说，就是采购价格最低，因为老板对采购的最大要求就是省钱；对生产来说，就是产能利用率最高，这也是老板最为关注的。你知道，价格没有最低，只有更低，如果牺牲交付、质量和服务的话。生产也类似：为了提高产能利用率，降低单位生产成本，那就减少换线，以延长交付周期，牺牲交付绩效为代价。 也就是说，在传统模式下，驱动员工行为的是竖向的效率型指标（比如成本更低、产能利用率更高），缺乏横向的服务型指标（比如交付更快、质量更好）。在绩效考核的驱动下，每个职能关注的重点是顶头上司，而不是兄弟职能的诉求，这样，职能与职能之间就串不起来，形不成供应链。这也是职能之间协作度低的根本原因。 这也是为什么兄弟职能之间经常互相挖坑，不管是自觉还是不自觉地；而上下级之间就很少互坑，协作也容易多了。这里的关键是强相关的指标：上级的目标百分之百传递给下级，下级的绩效也百分之百汇总给上级，大家是同一条绳子上的蚂蚱。但跨职能之间则不是，职能之间的横向联系不够强。也就是说，纵向指标之下，你关注的是来自上司的需求；横向指标缺失，你就不会那么关注兄弟职能，也就是内部客户的诉求。结果就是山头林立，局部优化盛行。 当然，有人会说，我也有横向指标啊，比如交付和质量。你当然有，但问题是，把你敲得满头是包的，是没完成你老板的事，还是兄弟职能不满意？或者说，让你晚上睡不着觉的，是你老板的事，还是兄弟职能的？如果是前者的话，说明横向的质量、交付等指标还是不够强大；支配我们行为的，还是自上而下的指标。 而解决方案呢，就是建立强相关的横向指标，让职能之间有类似于上下级之间的强联系。对于每个具体的职能、具体的员工来说，他们不但要有纵向的效率指标（比如成本、产能利用率、库存周转率），还要有横向的服务指标（比如按时交货率、质量合格率），这是一对表面矛盾，但实际相辅相成的指标[1]，是打破职能壁垒，促进跨职能协作，形成供应链的关键。 这道理不难，你天生就懂，因为兄弟职能不配合，你的第一大招就是想方设法让对方“背指标”，背的就是横向的服务型指标。那为什么总是给“背”不上呢？或者说名义上“背”上了，实际上却没有呢？ 这里的根本原因有二：其一，横向指标在绩效考核中权重太低，没法起到“强相关”的作用，引导员工的行为改变。其二，企业的管理精细度不够，没法有效客观量化横向指标——没法客观量化，就不知道；不知道，就没法管理，还是没法“强相关”。 第一个原因无须多言。对于第二个原因，让我们拿供应商的按时交货率为例来说明。之所以用按时交货率，是因为这是所有横向指标中最为直观，也最为简单明了的：要么按时，要么不按时，有什么可争辩的？且慢，这问题远没有那么简单。 先说什么是按时。计划说，按时就是能够满足客户需求，也就是说，以客户的需求日期为基准。采购马上就有异议：供应商的正常交期是30天，客户的需求日期只给3天，这怎么能做到呢？不公平。那什么叫公平呢？采购就说按照供应商的正常交期，要么是合同约定，要么是报价时约定。这时候销售、计划、生产马上“跳”起来了：这世界不是个完美的世界，如果客户每次都给我们足够的响应周期，那还要采购干什么？ 就这样，计划基于需求日期，3月1日就要货；采购基于标准交期，3月31日才交付。两个极端，对另一方都不公平。不公平就没有约束力——连法律都有规定，强迫签订的不公平合同不具法律效力。那什么叫公平？供应商说，我理解你们3月1日要货，但物理定律没法违背：车工需要x天，铣工需要y天，最后的精加工需要z天，也就是说，最快也是3月15日，否则要货没有，要命一条。就这样，计划、采购和供应商三方达成一致，3月15日就成为按时不按时的标准，也就是说，基于供应商的承诺日期。 这道理很简单，是不是？没错，对于一个具体的订单来说，这是不难。但想想看，一个公司，每天动辄有几十几百个订单，每个订单都这么来回拉锯，达成三方一致，可不是件容易的事。好不容易达成一致，第二天需求变了，得，又得重新来一次。这工作量有多大，离开电子商务的支持，简直不可想象。 在电子商务发达的企业，采购订单由ERP自动生成，发给供应商；供应商确认交期、数量、单价，通过电子商务传递给采购方；如果供应与需求匹配，这就作为供应商的承诺写入ERP，成为后续判断是否按时的标准，不需要任何人工介入；如果不匹配，系统会自动提醒供应商做出更好的承诺；还不够好的话，采购员、物控员、催货员就人工介入，打电话，发邮件，找老板，督促供应商改进交付，直到供应商做出三方能够达成一致的承诺。第二天需求变了，这样的流程就重来一次。 看得出，有电子商务支持的话，百分之八九十的事儿由信息系统做了，员工只是负责那5%、10%的例外；没有电子商务的话，员工就不得不把所有的情况都当例外，我还没见过一个公司，能人工确认每个订单的供应商承诺日期，并随着需求日期的更新而更新。遗憾的是，大多数企业都没有这样的电子商务，所以就没有能力做精、做细，在订单和料号层面客观统计按时交货率。 没有三方认可的承诺日期，计划就基于需求日期统计，按时交货率自然很差；采购为了自保，就基于正常交期统计，按时交货率自然很好。告到老板那里，老板一看，双方都有道理啊，只好批评教育，再宣教一番“以客户为导向”，这事儿就不了了之了。结果呢，计划只能以内部客户的身份，从道义上给采购压力；而采购呢，虽然“背”着供应商的按时交付指标，但实际上形同虚设，起不到“强相关”的作用，驱动他们的仍然是单一的价格指标。 最简单的按时交付都这么难以客观统计，质量、服务等指标就更难客观量化。就拿供应商的质量问题来说，每一个质量问题，都意味着生产线、质检、供应商以及设计之间无穷尽的扯皮，大量Email乱飞，大多企业根本没有资源来梳理清楚。同理，没法客观统计的就没法管理，这样，供应商质量指标就没法落实到采购头上，驱动采购的呢，依然是自上而下的成本指标。 做不精细，企业没法有效量化横向指标，就不得不借助企业文化来推动跨职能协作，让大家“学雷锋”，但没法从根本上解决问题。 既然职能之间的横向指标难以客观建立，有些企业就采取组织措施，成立集成供应链部门，让采购、运营、物流、计划、客服等职能统一汇报到同一个总监，通过组织措施打通这些职能之间的壁垒。 图1-6所描述的，就是一个本土名企的“集成供应链”。该公司设立首席供应官，与负责营销、产品的两位老总平行，一起汇报给CEO。在首席供应官下，有负责供应商选择的寻源、负责工厂的生产，以及端对端的供应链。 有人或许会问，既然寻源与生产都是供应链的一部分，为什么没有包括在集成供应链里？这里主要有两个原因：其一，生产管理成百成千的员工，有很多琐碎杂务；寻源要跟设计、供应商打交道，有很多商务关系要维护，供应链总监的精力有限，没法对付那么多的事情。其二，成本压力大的时候，企业就倾向于集中采购，把寻源单列出来，在更高层面整合需求，增加规模优势，获取更好的采购价格。 图1-6 集成供应链是通过组织措施打通部门墙 有趣的是，也正是在这个名企，寻源有时候归供应链，有时候又独立出来。其后的驱动因素呢，就是企业的业务需求：当速度不够快的问题更大时，寻源划归供应链，从寻源到订单处理都在同一个职能，以快速响应市场需求；当成本不够低的问题突出时，寻源就单列出来，以获取更大的规模优势。很多企业的采购时而集中，时而分散，后面的驱动因素也是一样。 集成供应链让客服、计划、执行采购、仓储、配送等职能处于同一部门，其好处是，即便职能之间没有客观的横向指标，也可以通过组织措施，促进这些子职能之间的协作，以控制局部优化，推动全局优化。 比如有个公司，原来采购、物流分别汇报给不同的总监，再到不同的副总，最后到同一个高级副总裁。两个职能，两条迥异的汇报线，从员工到经理到总监到副总，都是单一指标驱动：采购希望供应商发货越快越好，物流希望运输成本越低越好，就经常出现互坑的情况，比如采购员动不动就让加急运输，而加急运输费用呢，则由物流部门来买单。 当这个公司成立集成供应链后，采购经理和物流经理都汇报给同一个供应链总监，总监层面既对采购的按时交付负责，也对物流的运输成本负责。在一对相互矛盾的指标驱动下，总监一看到加急运费那么高，就马上找手下采购经理的麻烦；采购经理就找采购员的麻烦，于是采购员也就“理性”多了，再也不敢动不动就24小时加急，超额的物流费用也就得以控制。 集成供应链的另一个好处是“冤有头，债有主”，给销售等内部客户一个解决方案。当没有集成供应链时，销售问责计划，为什么交付不按时；计划一转身，就把问题推给了生产，说生产不及时；生产自然有采购垫背，说供应商没有按时交付；采购就把设计拉出来，说设计变更；而设计，则把球踢给了销售，说都怪客户的需求变更。最后，没有一个职能真正对销售负责。成立集成供应链部门后，责任到此为止：从接到客户订单开始，计划、采购、生产、包装、配送，都归供应链总监负责，供应链总监对这些职能“要打要骂”随便，但最终的交付呢，找供应链总监就行了。 业界人士说 以前PMC、制造、品保各自独立的时候，产线和供应商一旦有问题，马上就暴露出来；现在把这些职能集成到生产事业部（跟集成供应链类似——作者注），供应端的问题反倒不容易暴露，而一旦发现，就是大问题。 刘宝红答 在供应链上，没问题是最大的问题。这就如小孩子们在一起，总是会打打闹闹的；一旦没声音了，有经验的妈妈都知道，完了，肯定是在什么地方干坏事呢。 没有一种组织结构是完美的。一种组织结构解决了一些问题，必然会产生另一些问题。关键是要看解决的问题多，还是制造的多。但如果这种集成能够更好地解决更多问题，集中、集成还是值得的。 组织越是集成、集中，组织内的问题就越不容易暴露。比如当生产和计划分离时，你会经常听到生产抱怨计划；而当生产部门自己做计划、自己做执行的时候，抱怨的声音就小了很多——谁会自己抱怨自己呢？但你知道，那并不是因为生产自己做计划做得更好。 有些企业习惯于独立各职能，目的之一就是暴露问题。我的经验是，越是管理粗放的企业，比如大型央企、国企和内地的一些大型民企，职能之间的集成度越低，职能与职能之间的监督就越强，防止贪腐等行为。当然，这种多权分立会造成别的问题，比如唯一责任人缺失，在后文的“多权分立，供应商成了公共草地”部分还会详细讲到。 实践者问 供应链管理部门得如何设置，才能让供应链更加有效？ 刘宝红答 不知道，因为组织结构一定要跟业务需求联系起来，才能讨论有效无效。比如当成本压力大时，集中采购是很好的组织结构；但是，当速度不够快成为大问题时，集中采购就不是有效的组织形式。 这里我要补充的是，你不一定得有供应链管理部门，才能管理供应链。供应链管理更多是流程型管理，而不是组织型管理。流程稳健，完全可以不要“供应链管理”部门。比如苹果就没有“供应链管理”部。我的老东家也是。流程稳健的标志是职能之间的横向指标完善。比如在一些企业，计划、采购、生产、物流等职能之间设立了强相关的指标，在计划的驱动下，各部门各司其职，也能取得良好的供应链绩效。 [1] 比如对采购来说，既要价格低，又要质量好；对计划来说，既要交付好，又要库存低；对设计来说，既要产品性能好，又要满足目标成本。这些都是表面上相互矛盾，实际上却一致的指标。工作做到位，两者都能达到。比如计划做好了，知道计划客户要的，交付就好；也知道不计划客户不要的，库存就低。这点在《供应链的三道防线：需求预测、库存计划、供应链执行》一书中有详细阐述，刘宝红、赵玲著，机械工业出版社于2018年出版。 小贴士 供应链管理的“儒家”与“法家” 传统的日本供应链是长期关系，或者说，更像“儒家”的做法。[1]在长期关系下，绩效考核相对很次要。这就如一家人，相互之间很少会设定指标。而约束双方行为的呢，也正是长期关系，是未来，因为在长期关系下，双方都有很多可失去的，所以就更加理性。 比如在论资排辈的终身雇用制下（当然，现在的日本也早已不尽如此），员工表现不好，就没有好的晋升机会；跳槽后，又得从头开始，从最基本的做起，损失反倒更大，这促使员工在现有工作上尽职尽责（当然也承受很多委屈和压力，你到日本的地铁上，一眼望去，职业人的满头灰白就是证明）。同理，在长期合作下，供应商知道未来一部分业务是它的，如果不把现在的事做好，风险就是失去未来生意，这也驱使供应商更好地干活。 但是，对于北美和中国企业来说，这就很难适用。美国和中国其实惊人地相似，放在企业行为上，那就是短期关系。如果非要说有什么不同的话，那就是中国比美国更短期罢了。不管是企业之间，还是企业与员工之间，短期关系意味着没有未来；没有未来就意味着没有可失去的；没有可失去的，你自然就没法拿未来来约束对方。那怎么办？就只能推行“法家”的做法，基于契约来管理。 契约有两种：其一，竖向的契约，这是上下级之间的契约，也是最基本的契约，驱动职能内部上下级之间的协作；其二，横向的契约，这是兄弟职能、公司与公司之间的契约，驱动跨部门、跨公司协作。前者体现为纵向指标，后者体现为横向指标。纵向指标大家都熟悉，横向指标的好处呢，就是别光顾着把所有的水都放到自己田里：种好自己的一亩三分地要紧，分点水到邻居的地里也要紧。分多少呢，不是靠发扬风格，而是约定好的，即横向指标。 过去三四十年来，本土企业从没有契约的“大锅饭”，过渡到有竖向契约的市场经济，现在正在建立横向契约的路上。但是，对本土企业来说，契约化还远未完成。一方面是文化原因：传统的文化是基于关系的，要变成冷冰冰的契约关系，会有各种挑战；另一方面是能力原因：企业的管理精细度还不够，没法有效客观量化绩效，建立强相关的横向指标。 于是，很多企业就“儒”“法”并举，一方面大张旗鼓宣扬企业文化，这是儒家的做法；另一方面推行绩效管理，典型的法家做法。而做得好的企业，这两方面都做得不错。 就拿华为和海尔来说，这是一南一北两个非常有代表性的本土名企，在外人看来都是企业文化非常强的企业，似乎靠的就是任正非和张瑞敏的一张嘴。但是，华为和海尔不是靠企业文化吃饭的，如果把它们理解为儒家信徒就大错特错了；企业文化背后，它们靠的是异常严酷的绩效考核，典型的“法家”做法，而这正是局外人不知道，或者不愿意知道的。 比如海尔的“日清日毕”，字面上文质彬彬，翻译成白话可就不了：今天的事儿没做完，晚上你就不要回家。他们甚至为每一个员工独立核算，你对内部客户做了多少事，那是你的营收；内部供应商为你做了多少事，那是你的成本，力求每个人、每件事的账都算得清清楚楚。 再比如华为的能上能下，几年前我跟他们的两位销售高管会面，其中一位总监指着另一位副总说，以前他们两个的职位正好相反，后来因为绩效原因，两个人就倒过来了。能者上，不能者下，这话说起来多么容易，但在注重关系文化的氛围里，有几个企业能真正做到？华为可以说把法家精神发挥到了极点。 这里要补充的是，传统的日本企业虽然是儒家做法，绩效考核不是很严格，但不要忽视它们的绩效统计能力。在我所熟悉的全球企业中，日本企业可以说是数据最齐全，分析最到位的。夸张点说，它们的每一件事都有数据支持，员工的Excel用得烂熟，决策更多的是基于数据，而不是判断。 你可以不算账，但不能没有账。放在绩效管理上，就是你不一定要考核，但不能不统计。要知道，企业大了，几亿元、几十亿元的规模，离开绩效统计，就没人知道真相——不统计就不知道，而不知道就无法管理。管理能力的一大标志就是数据的充分与否。放在古代帝国的文明程度上，就是能否造册征税：中原文明有能力做全国普查，能够按丁、按亩征税；而草原上的野蛮人呢，就像匈奴，称雄北方几个世纪，往往连自己有多少人马都弄不清，最后连片瓦片都没留下，就消失在历史的长河中。 [1] 这里说的“传统”，主要指20世纪日本崛起的那段时间，大致在八九十年代前后。当时美国系统学习日本的做法，现在能看到的关于日本管理方式的文献，大都是那个时段产生的。当然，过去二三十年里，日本经历了显著的变化，管理方式也在变化，在有些做法上与欧美更加趋同，因文献不足，这里就不予探讨了。 案例 找替换供应商时，技术与质量不积极 有位职业人新近晋升供应链经理，全面负责公司的采购、质量和物流管理。摆在他面前的第一要事就是供应商质量问题：有些关键的供应商，质量问题一直没法解决。这位经理的解决方案呢，就是启动供应商淘汰机制，另行选择更好的供应商。但是，技术和质量人员都不够积极。他问我该如何设置绩效考核，提高这些职能与采购协作的意愿，尽快找到替换供应商。 案例企业年度营收十亿元左右，不大也不小，职能之间的部门导向有，但壁垒远远没有大公司的那么高。在开发替换供应商上，技术、质量人员积极性不高，看上去是个“不愿意”的问题，其实是因为供应商没选好、没管好，根本上是个“不能够”的问题。而解决方案呢，要从选好、管好供应商，争取首发命准上找，而不是第一个供应商没选好、没管好，就再找一个，让各部门重复投入资源。 遗憾的是，很多人分不清“不愿意”和“不能够”，误把后者当前者，一味地在绩效考核上做文章，自然没法解决问题。 就拿案例公司来说，供应商选好后，后续管理跟不上，有选择、没管理，好供应商也会变坏（这点后文还会详细谈到）。就质量问题来说，他们先是单纯依靠质量部门来应对，比如驻场管理，自然没法解决；最终变成了整体供应商问题，采购就拿淘汰代替管理，让技术、质量开发替代供应商，意味着更多的技术验证、质量评估工作。质量、技术虽然不知道根本的解决方案，却明白找替代供应商并不能真正解决问题，因为新供应商一进来，也会有老供应商的问题——这两个职能早已吃过很多亏，受过很多苦，对这点有很多切身体会。 所以，在找新供应商上，技术与质量消极反抗，出工不出力，也就不足为奇了。而解决方案呢，不是给这两个职能定指标，让他们更加愿意开发替代供应商；而是要改进供应商的选择与管理，争取第一次就把供应商选到位、管到位。 类似的情况很多。 比如有个企业的供应链老总说，他们的销售预测保守，不愿意建库存，导致供应链赶工加急。怎么才能促使销售多建库存？这不是个绩效考核问题，这是个能力建设问题：该公司采取“销售提需求”的做法，让一线销售人员做需求预测；那么多的一线销售，每人预测自己的客户需求，预测的颗粒度那么小，预测的准确度注定很低；预测准确度低，库存的风险大，销售人员建库存就自然保守。所以解决方案不是给销售绩效考核，而是改善需求预测流程，比如在公司层面集中预测，兼顾关键销售人员的职业判断，提高预测的准确度[1]，让销售能够更有效地管控库存风险，从而更愿意建库存。 延伸阅读 对于跨职能协作，我的基本观点是要通过绩效考核，设立强相关的横向指标来解决不愿协作的问题。当然，过犹不及，绩效考核也有副作用。比如《孔雀效应》一文中讲到的单向选择，还有我们都熟悉的“手术很成功，病人却死了”，都是常见的“绩效考核病”。在我的“供应链管理专栏”（www.scm-blog.com） 上搜索题目，或者扫描二维码，即可阅读《孔雀效应》全文。 [1] 对于一线销售为什么做不好需求预测，详细内容可参见我和赵玲的《供应链的三道防线：需求预测、库存计划、供应链执行》一书，106~117页。","link":"/%E4%BE%9B%E5%BA%94%E9%93%BE%E7%9A%84%E6%A0%B9%E6%9C%AC%E6%98%AF%E5%8D%8F%E4%BD%9C%EF%BC%8C%E9%82%A3%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%8D%8F%E4%BD%9C/"},{"title":"17. 利用LangChain让AI做决策","text":"Hi，大家好。我是茶桁。 在第 11 讲中，我向您介绍了如何将各种资料内容向量化，借助Llama-index建立索引，对我们自己的文本资料进行问答。在过去的3讲中，我们深入了解了如何使用Langchain。该工具可帮助我们整合AI对语言的理解和组织能力、外部各种资料或者SaaS的API，以及您自己编写的代码。通过整合这些功能，我们可以使用自然语言完成更复杂的任务，而不仅仅是闲聊。 但到目前为止，我们所有基于ChatGPT的应用基本上都是“单项技能”，例如前面关于“藤野先生”的问题或上一讲中查询最新天气或通过Python进行算术运算。这本质上是限制AI只针对我们预先索引或实时搜索的数据进行回答。 给AI加上多项选择能力 要做一个能跑在生产环境上的 AI 聊天机器人，需要的不止一个技能。在电商领域，最起码需要以下三个技能： 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E5%88%A9%E7%94%A8LangChain%E8%AE%A9AI%E5%81%9A%E5%86%B3%E7%AD%96/"},{"title":"20. 尝试让机器拥有声音","text":"大家好，我是Hivan。 好久不见了，今天我们来讨论下如何让机器拥有声音。 回顾一下我们上一讲的内容，我们已经成功使用Whisper模型使得AI能够理解我们说的话。这为我们带来了很多应用，例如让AI代替我们收听播客并总结内容。然而，这只是单向的交流模式。现在，让我们探索更深入的可能性，让AI不仅仅能够“听懂”我们的话，而且通过ChatGPT回答我们的问题，并将所有内容合成语音，用声音与我们进行双向交互。 这就是我们本次探索的主题：让AI说话。我们将学习如何使用云端API进行语音合成（Text-To-Speech），同时也会介绍开源模型，使您能够在本地CPU上实现这一功能，让数据安全问题不再是困扰。 让我们一起，给机器赋予声音吧！ 使用 Azure 云进行语音合成 语音合成技术早已迈入成熟阶段，你所听到的许多短视频配音都借助此技术实现。无论是科大讯飞、阿里云、百度、AWS Polly还是Google Cloud，国内外的大公司纷纷提供了类似的云服务。然而，今天我们将带您领略微软Azure云的语音合成API，主要是因为以下两个原因： 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E5%B0%9D%E8%AF%95%E8%AE%A9%E6%9C%BA%E5%99%A8%E6%8B%A5%E6%9C%89%E5%A3%B0%E9%9F%B3/"},{"title":"19. 快速倾听和总结音频内容","text":"Hi，大家好，我是茶桁。 其实到第18章的时候，我们处理文本的内容就全部都结束了，从本节课开始，我们要开始学习如何处理音频和图像。 我不知道有没有人和我一样的习性，就是比起视频和音频文件来说，还是跟喜欢看文本文件。这其中最主要的一个原因就是因为文本内容我们可以准确定位，而对于文本内容的接收速度还取决于我们输入设备（眼睛和处理信息的脑部）速度。而音频或者视频则不然，我们必须听完讲述者所说的话，即便你开到2倍速，速度依然受限，而且无法准确定位。那有没有什么办法能快速完成对音频文件内信息的获取呢，自然就是将语音内容转换成文本的能力。 其实到这一步，类似于Premiere或者剪映等剪辑软件都可以完成，不仅如此，在AI大行其道的今天，市面上应该也有不少Audio2Text的服务或者应用。接下来，我们要讲的就是一个杀手级服务了。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E5%BF%AB%E9%80%9F%E5%80%BE%E5%90%AC%E5%92%8C%E6%80%BB%E7%BB%93%E9%9F%B3%E9%A2%91%E5%86%85%E5%AE%B9/"},{"title":"观点：我们无法通过改造自己摆脱气候危机","text":"让我们面对现实吧——气候变化是人类最大的失误。我们已经知道它近一个世纪了。科学是清楚的。然而，我们什么也没做。真是太尴尬了。 现在，全球领导人终于开始忙着收拾残局。但是，尽管我们需要的大多数气候解决方案已经存在，但我们似乎无法按照所需的速度和规模部署它们。 简而言之，世界正在变暖，而我们却无法让它降温。去年，人类向大气中排放的\\(CO_2\\)比以往任何时候都多（呃……WTF？）。 可以理解的是，领导者们都害怕极了。这促使他们探索一些非常愚蠢且完全危险的想法。他们最糟糕的脑波之一是地球工程——也就是用地球的气候扮演上帝的角色。（这里使用的“地球工程”并不是指碳去除技术，据我们所知，碳去除技术是相当合法的。） 其中一些建议包括增亮云层、改变海洋的化学成分，或者向大气中发射粒子来使太阳的光线变暗——会出现什么问题呢？ 虽然这些提议听起来像是反乌托邦科幻电影中的内容，但改变地球气候实际上非常容易且成本低廉。 太阳能地球工程是这些“解决方案”中最具争议性的一种。其最受欢迎的衍生产品是平流层气溶胶喷射，涉及将灰尘喷射到大气中，以减少照射到地球表面的阳光量。这项技术的灵感来自于火山云，众所周知，火山云在一次大喷发后可以使整个地球冷却多年。 云增加了地球表面的反射率。平流层气溶胶喷射旨在通过将灰尘喷洒到高层大气中来复制这种效果，以期冷却气候。 虽然平流层气溶胶注入对于阻止全球变暖可能非常有效，但它可能会打开潘多拉魔盒的问题。根据联合国最近的一份报告，干扰全球自然气候可能会破坏臭氧层，改变全球降雨模式，并导致严重的地缘政治紧张局势。 尽管支持者称太阳能地球工程将是对抗变暖的短期措施，但《科学美国人》最近发表的一项研究表明，如果政客们确实决定向大气中发射尘埃，他们可能会在“几个世纪或更长时间”内危险地依赖它。 为了向大气中排放足够的灰尘来抑制变暖，每年可能需要数万次高空飞行。这一过程的突然停止可能会导致温度飙升，其速度可能快于生命的适应速度，这一概念被称为“终止休克”。 还有一个道德问题，即技术修复可以减轻政客和企业尽快脱碳的压力。 简而言之，太阳能地球工程相当于气候变化创可贴。 尽管存在风险，美国政府去年还是启动了一项为期五年的研究计划，探索将更多阳光反射回太空的方法，为进一步资助这项新兴技术奠定了基础。 比尔·盖茨、乔治·索罗斯和 Facebook 联合创始人达斯汀·莫斯科维茨等亿万富翁都表达了兴趣，而 60 名著名科学家则希望进行小规模太阳能地球工程现场实验。 甚至还有一家名为 Make Sunsets 的初创公司，基于其向大气中释放二氧化硫以遏制变暖的承诺而预售碳信用额。 美国初创公司 Make Sunsets 因未经批准进行平流层气溶胶注入测试而被逐出墨西哥。《麻省理工科技评论》的几位研究人员谴责了“Make Sunsets”，称其努力“为时过早”。 预防原则——或者对普通人来说“如果有疑问，就不要考虑”——是健全环境决策的基本前提之一，也是我们今后应该注意的原则。 欧盟本周宣布，呼吁就气候地球工程可能使用带来的风险进行“最高国际级别”会谈，这可能是积极的一步。 欧盟官员在周三的联合通讯中表示：“这些技术给人类和生态系统带来了新的风险，同时也可能加剧国家之间的权力失衡，引发冲突并引发无数道德、法律、治理和政治问题。” “我们不能用造成问题的思维方式来解决问题。 尽管欧盟正在采取预防措施，但它并不完全反对这些技术，而是寻求制定管理这些技术的“规则”。 其他人则采取更强硬的立场。 乌得勒支大学哥白尼可持续发展研究所的弗兰克·比尔曼在去年发表的一份声明中警告说，“人们对太阳能地球工程的风险知之甚少，而且永远无法完全了解。” 比尔曼是一群著名气候科学家的领导者，呼吁就太阳能地球工程达成不使用协议。换句话说，全球范围内禁止其开发。 “太阳能地球工程的研究并不是像其倡导者所说的那样，为预防气候灾难而准备 B 计划。相反，它只会推迟和破坏当前的全球气候政策，”他说。 “此外，现有的国际机构体系无法有效监管这项技术在全球范围内的部署。太阳能地球工程不是解决方案。” 我完全同意，教授。通过扮演上帝的角色来操纵气候，我们不仅面临着使我们的困境恶化的风险，而且还发出了一个危险的信息——人类可以简单地通过设计方法来解决问题，而不是从根本上解决问题（想想广泛的文化、社会和政治）变换）。 正如爱因斯坦的一句名言：“我们不能用创造问题的思维方式来解决问题。”","link":"/%E6%88%91%E4%BB%AC%E6%97%A0%E6%B3%95%E9%80%9A%E8%BF%87%E6%94%B9%E9%80%A0%E8%87%AA%E5%B7%B1%E6%91%86%E8%84%B1%E6%B0%94%E5%80%99%E5%8D%B1%E6%9C%BA/"},{"title":"18. 根据垂直需求微调模型","text":"大家好，我是茶桁。 最近事情太多，这一节课更新的有些晚了。 首先我们先了解一下我们本节课讲要讲一些什么，我们之前介绍过 llama-index 和 LangChain，学习了将大语言模型和自己的知识库组合来解决问题的方法。这个方法中，我们不需要调整我们使用的模型，而是使用嵌入向量索引我们的数据，并在需要时查询索引来解决问题。 然而，我们也可以完全利用自己的数据，创建一个新的模型来解决问题。这种方法是OpenAI提供的微调模型功能。这也是我们要探讨的大语言模型的最后一个主题。 如何微调模型 我们都知道，AI其实是建立在大语言模型之上的，而模型再如何补全，也没有办法全知全能。在很多时候，AI所回答的内容常常错漏百出，甚至于一些垂直领域可能完全词不达意。这些其实都是因为缺少了特定领域的训练数据，而我们要做的，就是要补全这一部分数据进行训练，为我们自己的需求微调出一个擅长本领域的模型。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E6%A0%B9%E6%8D%AE%E5%9E%82%E7%9B%B4%E9%9C%80%E6%B1%82%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B/"},{"title":"不要只看供应链管理的一个方面","text":"供应链管理不是软件？ 其实产生疑惑不无道理。供应链管理的范畴非常广泛，对它的认识就像盲人摸象：很多人是IT背景，他们想到的就是供应链管理软件，例如Oracle、SAP；对很多采购人员来说，供应链管理就是采购和供应商管理，即确保供应商保质、保量、按时提供价格合理的产品或服务；对于生产管理出身的人讲，供应链不过是生产管理的延伸罢了；对于物流行业的人来说，供应链管理则往往被等同于物流运输、车辆调度、仓储管理等。 在国内，很多人分不清供应链管理与物流管理。2010年，我拜访一位本土顶尖企业的首席执行官。该首席执行官几次提到物流管理，说ISM可以在物流管理上做出更大贡献。我想他指的应该是供应链管理，因为ISM侧重于供应链管理中的采购与供应管理，跟物流管理还离得比较远。当然，因为远离操作层，在一个上千亿元人民币规模的大公司CEO看来，这些区别或许不怎么重要。 大致在20世纪90年代，物流和供应链管理两个概念先后传入国内。这并不是说以前就没有物流和供应链——有人类的时候就有，只是不一定这么叫罢了。当时的大背景是物流成本太高，约束着本土供应链的效率。物流成本包括仓储、分销、运输、库存、物料搬运、第三方物流等费用，跟一个国家的基础设施息息相关。当时美国的物流成本是GDP的10%左右，得益于发达的高速公路网和信息基础设施；中国的物流成本是GDP的20%左右，跟当时落后的基础设施不无关系。 在国内，物流是供应链的瓶颈，因而就成为供应链管理的重点，乃至一叶障目，不见森林，让很多人误把物流管理当作供应链管理。当然，过去一二十年来，国内大规模投资高铁、高速公路等基础建设，这一情况得到显著改善，物流成本占GDP的百分比逐年下降，比如到2018年有望降到14%左右。作为对比，美国这些年来也是一路下降，达到7.2%，全球平均水平为11.2%。我想，这跟这些年来信息技术的发展不无关系：信息流驱动产品流，信息流的效率高了，物流的效率也会改善。 到底什么是供应链管理呢？ 供应链是从客户的客户到供应商的供应商，供应链管理是对贯穿其中的产品流、信息流和资金流的集成管理，以最大化给客户的价值、最小化供应链的成本。它是一个综合管理、全局优化的思想，以摆脱单个公司、单个职能层面的局部优化，实现供应链条的全局优化为目标。 在实践操作中，供应链管理由三大块构成： 供应管理（寻源） 运营管理（加工） 物流管理（交付） 跨越企业管理中的供、产、销三大块（如图：） 资料来源：Supply Chain Council. 简单地说，就是采购把东西买进来，生产来加工增值，物流来配送给客户。这三大块是执行职能，由计划驱动——也可以说计划是供应链的第四大领域。计划是供应链的引擎。很多执行层面的问题，看上去是没做到，其实往往是没想到——计划不到位造成的。这也是为什么在供应链运营模型（SCOR）中，计划处于采购、运营和物流之上。 从三大职能上讲，供应管理侧重于采购和供应商管理，使供应商成为公司的有机延伸；生产运营管理力求以最有效的方式完成产品、服务的增值过程；而物流管理则力求以最经济、迅捷的方式把货物从A点流动到B点。 从三条流上讲，产品流从供应商向客户流动，是供应链的实物流（如果是从客户向供应商方向的话，则称为逆向物流）；资金流是从客户流向供应商，是供应链的血液；而信息流则是双向流通，构成供应链的神经系统。 在竖向集成盛行的年代，供、产、销大都处于同一个公司内部。例如20世纪早期的福特汽车，从炼铁厂到零部件再到整车组装，都曾试图集中在自己旗下，尽管从来没有真正集成到这一步。最近二三十年以来，竖向集成解体，外包盛行，这三大功能越来越依赖供应商，例如零部件来自供应商，生产靠外包制造商，物流靠第三方物流公司。作为采购方，对这三部分的集成管理是供应链管理的重点。同样的道理，竞争也不再局限于公司与公司之间，竞争变成供应链与供应链之间的竞争。 值得注意的是，任何一个新的领域，都是在已有领域的基础上发展而来的。供应链管理也不例外，它从供应管理、运营管理、物流管理等分别向相邻的领域扩展而成。反映在学术机构，在北美，虽然专业都叫供应链管理，但不同大学的供应链管理专业侧重点不同。比如在亚利桑那州立大学，供应链专业历来以采购见长，而田纳西大学则侧重物流，麻省理工侧重运输。根本原因呢，就是这些学校的供应链管理是从这些具体领域发展而来的。 除了采购、生产和物流外，工业工程也是供应链管理的“近亲”。在亚利桑那州立大学工业工程的很多教授都在供应链管理系任教，后来甚至有一位成为供应链管理系的系主任。一位在密歇根大学工业工程系就读的博士，研究的却是供应链管理，后来到俄勒冈大学的商学院任教，教授的也是供应链管理。 在美国，很多大学的供应链管理专业设在商学院。比如MBA排名中，一个分支就是供应链管理。除此之外，也有很多工学院设立供应链管理的研究生专业。这几年，有好几个大学设立一年制的供应链管理硕士课程，比如马里兰大学、俄亥俄州立大学、南加州大学等，吸引了大批的中国留学生，造成同质化严重，也是个问题。得州大学达拉斯分校也有供应链专业。 在研究领域，有很多杰出的研究者都是从别的领域来的，对供应链管理专业的建立贡献巨大。比如斯坦福大学的李效良（Hau Lee）教授，他关于“牛鞭效应”的研究可以说奠定了供应链的理论基础。但这些研究大都是他在工学院时做的，师承工业工程、管理科学和运筹学，上溯到MIT的系统动力学（System Dynamics）。李效良在担任《管理科学》（Management Science）杂志主编期间（1997~2002年），在这个管理学领域最权威的学术期刊之一上，刊登了大量的供应链管理文章，可以说让供应链管理正式成为一个专业领域。 在工业界，鲜有能够跨越供应链的三个领域的实践者。尽管很多公司试图把采购、运营和物流等职能集中到一起，组成全球供应链或全球运营部，但下面的分支部门仍旧围绕三个职能划分。道理很简单：没有人能够掌握所有的采购、运营、物流，外加计划等众多领域的专业技能。对供应链管理的认识仍会处于“盲人摸象”状态：采购背景的人说是采购的延伸，物流的人说是物流的延伸，而生产部门则认为是运营管理的延伸。一些流程分析、软件背景的人，则更多地从端对端的流程角度出发，理顺供应链的产品流、信息流和资金流，提供了一个全新的供应链管理视角。条条大路通罗马，这些都可成为公司搭建卓越供应链的起点。 就本土企业来说，20世纪90年代后期，华为导入IBM的集成供应链的概念，旨在打通职能部门之间的横向联系，提高供应链的效率，可以说是开了国内集成供应链的先河。华为所在的电信设备行业批量小，品种多，复杂度高，集成供应链确实是关键的解决方案之一。2005年，联想并购IBM的PC业务，全盘接受了IBM的供应链管理体系。 这几年，供应链管理的概念更加深入各行各业，不光是大企业，还有中小企业；不光是制造业，还有建筑业、电商业、餐饮业等。除了制造业外，还有建筑、零售、服装、餐饮、电商等行业。根本原因呢，是这些企业认识到供应链的价值，认识到打通部门之间的壁垒、通过全局优化来提高公司绩效的重要性。 20多年来，供应链管理在中国遍地开花 有个朋友，曾经担任西贝餐饮的副总裁。他说，餐饮看上去是开餐馆，其实比拼的还有供应链实力。比如原材料的获取、储存、加工等——很多原材料有很强的季节性，比如西贝用的羊肉来自内蒙古草原，内蒙古羊肉最好的在秋季，羊一定要在那两三个月内从内蒙古的牧场收齐、宰杀、冰冻，供后面的一整年用，这些都需要一流的供应链计划和执行来支持。因为餐馆所处位置一般为市内黄金地段，租金很贵，所以店面都较小，大多菜的加工其实是在中央厨房完成的，及时运送到店面后，做些简单的最后加工就上菜了。中央厨房就跟制造业的工厂差不多。从这个意义上讲，西贝这样的餐饮业跟生产、零售业没有本质区别。 2000年国内很少听到供应链管理的概念，更不用说有这专业了；十多年后，国内已经有很多大学设立了供应链管理专业，各种各样的供应链公司如雨后春笋，供应链管理的概念也更加深入。尤其是经历二三十年的高速发展后，企业普遍面临“增长陷阱”[1]，越来越多的人意识到，企业要生存，不但需要开发好的产品（主要是设计的责任）、卖个好价钱（销售的任务），而且要以适当的成本、速度生产出来（供应链的责任）。随着整体经济的进一步成熟、放缓，降本增效的压力必将加剧，而作为降本增效的主要源泉，供应链管理任重道远。 在之后篇幅里，会从采购、运营和物流的角度学习供应链管理，希望能有个全面的认识。 !&gt; 这里是脚注。（由于docsify缺乏脚注功能，所以只能直接写了，没有链接回跳） 「1」：增长陷阱”指企业发展到一定阶段，营收增速放缓，不再增长甚至下跌，而成本由于惯性还会继续上升（比如不管使用与否，设备折旧会照旧；不管生意好坏，员工每年的工资总得加几个点），导致利润率越来越低，甚至亏本。详细内容可参考我的另一本书《供应链管理：高成本、高库存、重资产的解决方案》，机械工业出版社于2016年出版。 采购和供应链管理 在供应链管理的三大职能中，供应管理与供应链管理只是一字之差，可以说是供应链管理的“近亲”。但是，供应管理的重点是供应商这一外在战略资源，与运营管理侧重公司内部生产运营、物流管理侧重产品和信息的流通形成对比。 供应管理起源于采购管理。从严格意义上讲，供应管理的范畴远大于采购管理。但为了行文方便，采购管理和供应管理在本书中通用（如果没有特别注明的话）。 在美国，传统上采购的地位不是很高，因为传统上美国公司的竖向集成度挺高，对外来资源依赖度低。作为管理外来资源的采购部门，其主要任务是围绕订单处理日常交易。简单地说，内部客户（如工程师）说，我要买这个，采购的任务就是下订单，确认价格、交期，把货按时拿到。 采购部门的吸引力有限，就成了那些百无一用的人的最后落脚点。就如我在亚利桑那州立大学读书时，一位叫皮尔森的教授曾经说，如果一个人干不了销售、设计、生产等，那只能去做采购了；如果连钱也不会花，那就只能卷起铺盖另谋高就，去祸害我们的竞争对手吧。 美国如此，中国也是：传统的计划经济下，外在资源主要依靠国家统一调配，公司A的产品给B做原材料，价格都由政府规定了，采购自然也就可有可无了。在那些比较封闭的行业，比如军工、航空业，还能看到传统经济的影子：长期以来，中国的航空业是半竖向集成的，整个行业其实就是中航工业和它的子公司们，行政命令历来扮演重要角色。现在为了开发商用大飞机，得跟那么多的全球供应商打交道，用工业界通用的方式做生意，采购面临的挑战可想而知。 在采购管理上，经常听人说，如果你连花钱都不会（做采购），那可真是百无一用了。就如李鸿章对儿子说，如果你连做官都不会，你可就一无是处了（大意）。其实我们都知道，做官的学问可大了，离开了那些官僚，一个国家的运作就会大受影响。采购也是：采购是一个大职业，尤其是在有些行业，产品成本的百分之七八十都来自供应商的情况下，采购已经远远超越持币购物，而是在管理公司百分之七八十的增值活动——供应商表面上在赚我们百分之七八十的钱，实际上在帮我们做百分之七八十的事，而采购呢，则对选择和管理供应商负责，对这些增值活动负责。 更进一步，常言说得好，卖得好不如买得好：采购每节省一块钱的开支，利润就增加一块；销售增加一块钱的销售，利润大致增加一毛。不管是零售业，还是制造业、服务业，采购的重要性毋庸置疑。在有些行业，比如电商和贸易行业，由于没有生产，供应链的所有增值环节都在供应商处，采购的价值就更大了。 随着很多行业转向外包战略，外购额逐渐增长，成为公司开支中的最大一块，公司对供应商的依赖度越来越高。而作为管理供应商的对口职能，采购的重要性也在日益上升。在美国，设置首席运营官的公司越来越少，设置首席采购官的则越来越多，根本原因就在于增值活动以前主要发生在公司内部，由首席运营官负责，现在则越来越多地外包给供应商，由首席采购官负责。 采购的地位提升，其重心也从订单处理转为对供应商的战略管理，过渡到供应管理。20世纪80年代，麦肯锡的一位顾问在《哈佛商业评论》上发表文章，题为《采购必须成为供应管理》[1]，吹响了这一战略转移的号角。但是，整个过程花了二三十年。2002年，美国采购经理人联合会（NAPM）更名为供应管理协会（ISM），是这一过程的里程碑事件，标志着供应管理正式成为主流。 与采购管理的围绕订单处理相对应，供应管理更侧重供应商的战略管理，通过分析开支、确认需求、评估供应商、选择供应商、签订协议、管理供应商绩效来确保以合适的成本保质保量地获取资源。从时间跨度上讲，供应管理向前延伸到设计和新产品开发，向后延伸到产品的生命周期结束；从影响的对象上讲，供应管理延伸到对公司的资产、现金流等的管理，直接影响公司的盈利。 在北美，有些公司已经开始统计供应管理的贡献，例如净利润率是10%，其中0.5%是供应管理通过降低采购成本等来实现的。首席采购官这一头衔能够与首席财务官、首席运营官等相提并论，也反映了采购与供应管理战略地位的提高。 从供应链的角度来看，采购处于公司的内外结合点，是管理供应链的理想选择。作为采购部门，突破对传统职能的认识，在管理供应链上发挥更大作用，也是提升采购在公司地位的一个有效办法。采购对内管理需求（比如设计的新产品寻源、生产部门的量产需求）、对外管理供应商（比如供应商选择和绩效管理），通过理顺需求来理顺供应，其实就是在管理供应链，或者说管理供应链的一大块。 在一些大型国企、央企，以及管理粗放的民营企业，采购并没有意识到这些。他们对自己的定位主要是招投标，以及供应商出了问题后的应急反应。[2]没有了需求管理，很多需求一落地就是紧急需求，给后续的供应链执行带来很大挑战；没有系统的供应商管理，供应商层面的问题没有解决，导致订单层面的问题不断，供应绩效长期在低水平徘徊。这些都是采购面临的大问题，也是供应链管理的大挑战，我们在后面还会详细讲到。 资源 美国高级采购研究中心（CAPS Research，www.capsresearch.org）。该中心是美国供应管理协会与亚利桑那州立大学合作成立的，有一系列专题研究，侧重采购与供应管理，是全球该领域的顶尖研究机构。 资源 微信公众号“宫迅伟采购频道”，有一系列的采购与供应管理方面的原创文章。 延伸阅读 有一个门类专门讲采购管理（http://scm-blog.com/cat-23），可以阅读更多采购管理方面的文章。 [1] 文章英文名为Purchasing Must become Supply Management，发表于1983年9月，作者为Peter Kraljic（中文译名“卡拉杰克”）。卡拉杰克是麦肯锡的咨询顾问，在德国汉诺威工业大学获博士学位，对采购界的影响深远。有名的“卡拉杰克矩阵”就是由他提出的：他参照投资模型，按照收益影响和供应风险两个维度，把采购项分为四类，区别对待。这是采购管理中一个最为根本的模型，有很大的指导意义。更多细节可参考百度百科“卡拉杰克模型”词条。 [2] 可以说，招投标是采购工作的一部分；但如果是采购的主要任务，这个企业的采购注定是“小采购”。采购的主要任务是选择、管理供应商，招投标是供应商选择的一种方法，而且是很不完美的方法。在招投标盛行的企业、机构，伴随着招投标的往往是供应商的有选择、没管理，供应商绩效一塌糊涂。我们在后文还会详细阐述解决方案。 物流管理：从A点到B点 简单讲完了采购和供应管理，我们来看一下物流管理。 原美国物流管理协会、现供应链管理专业人士协会对物流管理的定义如下： 物流管理是供应链管理的一部分，即为满足客户需求，通过计划、实施和控制，促成产品、服务和信息从发源地到消费点的有效流动及储藏。 这定义有点长，拗口，但说明了几点： 第一，物流管理是供应链管理的一部分。作为美国物流管理方面的权威组织，供应链专业人士协会的定义有相当的权威性，确定了物流与供应链的关系。在2004年，该协会名称从物流管理改为供应链管理，也反映了物流管理向供应链管理的延伸。与此类似，运营管理、采购与供应管理也在向供应链管理延伸、靠拢，从它们的更名上可见一斑：运营管理协会以前叫美国库存与生产控制学会（2004年改名），供应管理协会以前叫美国采购经理联合会（2002年改名）。 第二 ，物流管理的对象是产品、服务、信息的流动与储存。简而言之，就是把产品从A点搬到B点，并处理过程中的服务、信息。值得注意的是，它不负责采购（那是供应管理的任务），也不负责生产（那是生产和运营管理的事）。这个界限表明了物流管理想与运营管理、供应管理三分供应链管理的天下。 第三，物流管理不但管理产品、服务、信息的正向流动（从供应商到客户），而且管理其反向流动（从客户到供应商，即逆向物流）。逆向物流日趋重要，是退货、保修、返修等售后服务的重要一环，也更难管理。国内可能还体会不到，如果你在美国，到沃尔玛这样的大超市去看看，节假日后，退货的队跟买货的差不多长，你就知道逆向物流面临的挑战了。 按照上述定义，物流管理的对象包括运输、车队、仓储、物料处理、订单履行、物流网络设计、库存管理，以及对第三方物流服务商的管理等。当然，有时候物流管理也会涉及采购、生产、包装和客户服务等。它不但要优化物流的各环节，而且要考虑与其他职能的集成。 在国内，很多人片面地把物流等同于运输，就是把产品从A点搬到B点，看上去很简单，其实不然。光从它占美国、日本GDP的7%左右，占中国GDP的15%左右来说就不简单。单拿它的分支行业来说，运输业、仓储业等本身就大得不得了，整个物流行业，你能想象有多大吗？ 有趣的是，查一下几十年前的定义，物流管理还包括采购，采购被视作入厂物流[1]的一部分。这也与当时竖向集成为主、采购的地位低下不无关系。在有些公司，比如欧洲的一些公司，负责订单处理的采购员汇报给物流，而不是采购部门。上汽大众也是类似的设置，估计是受德国大众的影响。中国有物流与采购联合会、物流与采购网，都是物流在先，采购在后，一定程度上也反映了采购与物流的关系。 在美国，采购管理领域的研究者，有很多原先也是毕业于物流管理系。例如，美国经典的采购教科书的作者David Burt教授，原来就是毕业于斯坦福大学的物流管理专业。我在亚利桑那州立大学的教授Lisa Ellram呢，虽然研究方向主要是采购，但博士学位却来自俄亥俄州立大学的物流管理。 在国内，物流（logistics）早些年被译作后勤学，又称军事物流学。这跟物流与军事联系由来已久不无关系。诸葛亮六出岐山，据说是一人打仗，需要五人做后勤支持，后勤是最大的挑战，而输也是输在后勤上。左宗棠在西北平叛，“惟秦陇之事，筹饷难于筹兵，筹粮难于筹饷，而筹转运尤难于筹粮，窘迫情形，为各省所未见。”[2]——在陕甘一带，筹钱比招兵难，筹粮比筹钱难，而粮草的运转比筹粮更难，说的也是物流后勤之难。 第二次世界大战后期的诺曼底登陆，表面上是一场战役的成功，不为人所知的是后勤物流的杰作。以前说美军能够在24小时内开赴全球的任何地方；现在呢，美军第82空降师可以在18个小时内到达世界的任何地方，拼的还是物流的实力。2017年，中印的6·18洞朗对峙事件，之所以能够和平解决，与中国的物流运输能力分不开——得益于这些年在西藏的铁路、公路建设，我们能够迅速地在西藏投放大批重型武装，对印度形成有效吓阻。 资源 微信公众号“物流沙龙” www.logclub.com 是物流管理领域的一个交流平台。这个沙龙已有十余年的历史，一直坚持在物流管理领域。 [1] 入厂物流是inbound logistics的翻译，简单地说，就是把原材料、半成品等运入厂区，比如从供应商到工厂。 [2] 胜利在望却甘愿求和，左宗棠西北平乱为何要选马家军做朝廷代理人.百家号“史料不辑”. 运营管理：千遍万遍不走样 运营管理是供应链管理的三大组成之一，当然也可以说供应链管理是运营管理的延伸。那究竟什么是运营管理呢？ 微软的英卡特百科全书对运营管理定义如下：“运营管理是对主要商业活动的管理，即组织和控制最基本的商业活动，为客户提供产品或服务。”这与美国运营管理经典教科书[1]的定义挺接近：“运营管理是对公司相关体系的设计、运作和改进，以制造产品和提供服务。”它是把原材料、人力、技术、资金、设备等转化为产品、服务的增值过程，是每一个管理人员都没法回避的。 运营管理协会，即原来的美国生产与库存控制学会，对运营管理的定义有明显的生产和库存管理的痕迹，但贴切地反映了运营管理的兼容并蓄：“运营管理是对研发、工业工程、管理信息系统、质量管理、生产管理、库存管理、会计等职能的集成，以有效地规划、利用和控制生产或服务机构。” 运营管理不是制造业专有，从“制造与服务业运营管理学会”的名字就可见一斑。在美国，国内生产总值GDP的79.7%来自服务业（2017年）[2]，运营管理的研究重心也在从制造业向服务业转移。很多起源于制造业的概念，也被移植到服务业。麦当劳把流水生产线用到快餐服务，就是一个例子——流水线最早由福特汽车导入，是个制造行业的实践。 一位在戴尔担任过运营经理的朋友说，运营管理都是些琐碎繁杂的事。没错，不过运营管理的这些柴米油盐事，却关系到公司的基本运作，如质量、交货、服务等，任何一件小事都可能让你的生产线停顿下来，所以非常重要。一位纳斯达克100的大公司的首席运营官说，他的全球运营部门是“啥事都牵扯”（in the middle of everything），也是同样的道理。 琐碎繁杂，微不足道，干一遍没什么难，难就难在千遍万遍不走样。这就如麦当劳的炸薯条本身没什么了不起，真正了不起的是，不管在世界什么地方，由什么肤色的人炸，是早晨还是晚上，这薯条都炸得一个样。就如海尔集团首席执行官张瑞敏所说，“不简单，就是将简单的事做千遍万遍做好；不容易，就是将容易的事做千遍万遍做对。”背后没有成套的系统、流程是不可能的。运营管理的价值就体现在对这些系统、流程的设计、运营和改进上。而且只有从日常运营的繁杂琐事中上升到流程、系统的实质问题并改进，运营管理者才能脱颖而出。这点同样适用于供应链管理。 在北美的大公司，运营管理和供应链管理相互搭接。例如，在IBM这类推行集成供应链管理的公司，运营管理是供应链管理的一部分；而在另一些公司，采购、物流等是全球运营部门的一部分，汇报给全球运营部。究竟是运营汇报给供应链，还是供应链汇报给运营，这并不重要：水无定型、法无定法，关键是组织结构要能够满足公司的业务要求，并随着业务的发展而调整。 业界人士说 在一些国企，运营管理部是虚岗，跳出实际的业务流程去进行所谓的运营优化管理。在我看来，这种做法非常低效：一方面，该岗位平时可有可无，即使该部门全部放假也完全不影响业务进程，对从业人没有任何直接的业务压力，没有压力就没有动力，很难出成绩和效果；另一方面，会产生外行指挥内行的现象，反而干扰正常业务流程。如果用跨部门的专项项目，或者类似精益生产的改善小组，效果应该更好。常设专职的工作组、委员会一般都不是解决问题的好组织形式。——米良疯，微信公众号“供应链管理专栏”读者 刘宝红答 其实何止国企，大企业都有这问题。专业分工下，内行埋头干活儿，没时间抬头看路；外行在教人干活儿，但不知道活儿是怎么干的。干活儿的跟教人干活儿的是两层皮，注定效果会打折扣。最早的日本企业的“质量圈”（quality circle），就是干活儿的人自己在改进，两层皮的问题就比较小。 [1] 该书英文名为Operations Management for Competitive Advantage。作者为Richard Chase，F. Robert Jacobs和Nicholas Aquilano，2005年由McGraw-Hill/Irwin出版社出版。该书的更新版本有中文版，名为《运营管理》，由任建标翻译，机械工业出版社2015年出版。 [2] List of Countries by GDP Sector Composition.维基百科，www.wikipedia.org. 供应链管理的几个“小亲戚” 除了采购、运营和物流管理外，供应链管理还有好几个“小亲戚”，比如运筹学、系统动力学、工业工程、信息技术等。 运筹学为供应链的优化提供了工具，比如线性规划、数理统计等。如果你看20世纪60年代以来的文献，库存计划、生产排程、配送网络优化等领域到处都是运筹学的影子。而供应链的真正优化，也离不开这些数理统计模型。国内高校中，有些供应链教授就是运筹学背景。比如上海交大安泰学院的陈晓荣博士，就是个运筹学专家，现负责全球运营领袖MBA课程（交大和MIT合办，在我看来是国内最好的供应链管理MBA）。 运筹学有很多模型和算法，相对北美而言，也是国内教授比较擅长之处。在美国，供应链管理的顶级研究，比如发表在《管理科学》（Management Science）等上面的论文，大多也离不开数理模型，那些杰出的研究者呢，也是以华人和印度裔为主。 供应链管理的另一个“亲戚”，甚至可以说是“近亲”，是系统动力学。该学科源自麻省理工的Jay Forrester教授，着眼供应链条上各个环节之间的互动，力图全局优化，可以说是供应链管理的鼻祖。作为供应链管理的经典游戏，“啤酒游戏”就是由Forrester教授在20世纪60年代开发的，后来演化成多种版本，用来展示供应链上没法回避的“牛鞭效应”[1]，也能在系统动力学上找到起源。 供应链管理最早在制造业发展起来，而制造业离不开IE和IT—供应链管理的另两个“亲戚”。前者是工业工程，可以说是现代管理之母，生产线、仓储配送设施等的优化，都离不开工业工程；后者是信息技术，比如以ERP为核心的信息系统，撑起了企业和供应链的框架。供应链的流程，特别是订单层面的基本流程，其实是固化在信息系统里。 我在硅谷工作的那些年，经常跟负责ERP的分析员们开玩笑，说公司把我们这些负责供应链业务的人都开掉也没关系，只要保留他们那些维护ERP的人员就行了——企业的基本流程，比如订单处理，是固化在ERP中的，而这些分析员最熟悉ERP和业务流程，招些新人，由他们培训就可以了。 也是因为这个原因，有些公司的供应链改进由CIO牵头。比如时不时有公司联系我，希望我来帮助他们改进供应链管理，联系人的头衔中屡屡就有CIO的字眼。有个计算机巨头邀请我去培训他们的IT人员（主要是分析员），主要原因就是他们最熟悉业务流程，需要承担供应链绩效改进的责任。电商、贸易行业，我就见过好几个CIO在负责制定需求预测、库存计划逻辑。 这有很多问题。最主要的是CIO虽说熟悉基本的业务流程，但并不一定熟悉业务本身。打个比方：CIO很熟悉在ERP里，库存如何从一个库存点转移到另一个库存点，但这跟库存控制没有关系——库存控制取决于合理的需求预测、合理的库存计划，光熟悉那些ERP里的指令是远远不够的。所以，CIO在供应链的组织设计、绩效考核、主干流程方面往往经验不足，因而不是主导供应链绩效改进的合适人选。[2] [1] 简单地说，“牛鞭效应”就是由于信息不对称，需求变动沿着供应链传递时会逐级放大，越是远离需求源，放大的幅度越大。后文会详细探讨。 [2] 这方面有个案例，在我和赵玲合著的《供应链的三道防线：需求预测、库存计划、供应链执行》一书中（145~151页）。该案例讲的是在一个本土企业，供应链改进原来由IT驱动，最后转向由集成供应链来负责，因为后者更加熟悉业务机制。","link":"/%E7%9B%B2%E4%BA%BA%E6%91%B8%E8%B1%A1/"},{"title":"6. Python的高阶函数","text":"Hi，大家好。 我是茶桁。 本节课，我们来学习一下Python中的「高阶函数」。 递归函数 让我们先来了解一下，什么是递归函数。 递归函数就是定义一个函数，然后在此函数内，自己调用自己。 既然是自己调用自己，那这个函数必须要有一个结束才行，否则会一直重复的调用下去，直到调用层数越来越多，最终会导致栈溢出。 让我们先写一个雏形： 1234567891011# 初步认识一下递归函数def recursion(num): print(num) recursion(num - 1)recursion(3)# 执行结果3 2 1 0 -1 -2 -3 -4 -5 -6 -7 ....RecursionError: maximum recursion depth exceeded while calling a Python object 最后，导致栈溢出，程序报错。 那么这个程序到底做了什么？ 首先，我们定义了一个函数，然后执行，执行的时候给了一个参数3。 进入程序之后，先将3打印了一遍，然后在函数内部，又调用了一遍自己，参数为3-1，也就是传了一个参数2，在进入函数之后，打了了2， 继续自己调用自己，传参2-1，1-1, 0-1, ...就这样一直循环下去。 那么我们怎么样让这个程序停下来？就是在函数自己调用自己之前，加上一个限制条件： 1234567891011121314# 初步认识一下递归函数 3 2 1 0def recursion(num): print(num) # 检测当前值是否到0 if num &gt; 0: # 调用函数本身 recursion(num - 1)recursion(3)# 执行结果3210 我们给调用之前加了一个条件，如果num &gt; 0才允许继续执行，这样，当程序传递了1-1之后，执行了最后一次打印，然后就不向下执行了。 不过不要以为程序到这里就结束了，我们多加一行代码试试看： 123456789101112# 初步认识一下递归函数 3 2 1 0def recursion(num): print(num, end=&quot; &quot;) # 检测当前值是否到0 if num &gt; 0: # 调用函数本身 recursion(num - 1) print(num, end=&quot; &quot;) # 又加了一个print函数recursion(3)# 执行结果3 2 1 0 0 1 2 3 如果你不知道程序做了什么，我们稍微分析一下： 1234567891011解析当前递归函数的执行过程：```recursion(3) ==&gt; 3 recursion(3-1) ==&gt; 2 recursion(2-1) ==&gt; 1 recursion(1-1) ==&gt; 0 recursion(0) ==&gt; 0 recursion(1) ==&gt; 1 recursion(2) ==&gt;2recursion(3) ==&gt; 3``` 也就是，在递归函数中，程序是一层一层的进入，然后再一层一层的返回。 这就好像是， 我们在上学的时候，你坐在最后一排，但是你有个心仪的女孩坐在最前面。你想要对方电话，这个时候你传递一个纸条给前面的同学，前面的同学再往前传，一直往前传到女孩手里。女孩看完之后，写完回复再一次次的传回来。最后你满怀期待的打开一看：“滚。” 当然，我们的递归函数和这个不同的地方是最后不会多加那个“滚”字。 回调函数 什么是回调函数呢？ 我们首先来思考一个问题： 1234def func(a): print(a)func(a) 在这个简单的函数中，我们已经学会了传值a给到func()，那么参数到底可以传一些什么进去？a可以是什么？能不能是一个函数呢？ 这就引出了我们现在的内容： 1234567891011121314# 带有回调函数参数的函数def func(obj): print(obj, type(obj)) # 并且在函数中调用传递进来的形参函数 obj()def _self(): print(&quot;i am _self&quot;)func(_self)# 执行结果&lt;function _self at 0x111ed4280&gt; &lt;class 'function'&gt;i am _self 可以看到，我们选择执行的是func函数，但是最后打印出了_self函数中语句。原因就是我们在执行func函数的时候，将_self函数作为参数传递给了func的形参obj， 我们在其中打印了obj以及obj的类型，并且最后执行了一下obj， 实际上也就是执行了一遍_self函数。 如果在一个函数中要求传递的参数是一个函数作为参数，并且在函数中使用了传递进来的函数，那么这个函数我们就可以称为是一个回调函数。 我们拿系统内部的一个现成的函数来重新封装一个新的函数来试试： 123456789101112131415# 做一个数学计算的函数def func(x, y, obj): &quot;&quot;&quot; 此函数用来整合其他的数学运算 在当前函数中，需要接收三个参数，前两个为数值，最后一个为函数 x, y: int f: function :return: &quot;&quot;&quot; print(obj(x, y))func(2, 3, pow)# 执行结果8 在日后使用这个函数的时候，就可以传入数值和要做什么计算的方法，就可以了。 当然，这个函数写的并不完善，比如，我们在执行func(2, 3, sum)的时候就会报错，原因是因为sum()函数内部是要进行迭代的的，然而int类型中没有魔法方法__iter__, 所以无法迭代。所以，要想这个函数具有通用性，还需要在内部完成很多工作。 闭包函数 之前我们在回调函数中将函数作为参数进行了传递，那么问题来了，既然函数能作为参数进行传递，那能不能作为参数被return呢？ 12345678910def person(): money = 0 def work(): print(money) return workperson()# 执行结果&lt;function __main__.person.&lt;locals&gt;.work()&gt; 我们可以看到，work函数被成功返回出来了。但是并未继续执行， 因为其内部的print()没起作用。 我们用一个变量来接收这个返回的函数： 1234567891011def person(): money = 0 def work(): print(money) return workres = person()res()# 执行结果0 说明res接收到返回的work()函数，并且最后执行成功了。 好了，让我们继续为这个函数做一点什么，看看有什么变化。 12345678910111213def person(): money = 0 def work(): nonlocal money money += 100 print(money) return workres = person()res()# 执行结果100 这个结合前几节所讲的内容就很好理解了对吧？ nonlocal关键字拿到上一层函数定义的变量，然后在内层函数中进行使用，最后打印出来。 那我们继续执行会如何？让我们多执行几次： 12345678910111213141516171819# 定义一个函数def person(): money = 0 # 函数中定义了一个局部变量 # 定义内函数 def work(): nonlocal money # 在内函数中使用了外函数的临时变量 money += 100 print(money) # 在外函数中返回了内函数，这个内函数就是闭包函数 return workres = person() # return work res = workres() # res() == work()res()res()res()res()# 此时 就不能够在全局中对money这个局部变量进行任何操作了，# 闭包的作用：保护了函数中的变量不受外部的影响，但是又能够不影响使用 你会不会认为会一直打印100? 让我们看看执行结果到底是怎样的： 12345100200300400500 怎么样，是不是完全没想到？这个就是闭包函数的特点。 在一个函数内返回了一个内函数，并且这个返回的内函数还使用了外函数中局部变量，这个就是闭包函数。其特点为： 在外函数中定义了局部变量，并且在内部函数中使用了这个局部变量 在外函数中返回了内函数，返回的内函数就是闭包函数 ⚠️ 主要在于保护了外函数中的局部变量，既可以被使用，又不会被破坏。 检测一个函数是否为闭包函数，可以使用func.__closure__，如果是闭包函数返回cell 匿名函数 -- lambda表达式 首先，我们先弄清楚什么是匿名函数： 匿名函数的意思就是说可以不使用def来定义，并且这个函数也没有名字。 在Python中，我们可以使用lambda表达式来定义匿名函数。我们需要注意，lambda仅仅是一个表达式，并不是一个代码块，所以lambda又称为一行代码的函数。 在lambda表达式中，也有形参，并且不能够访问除了自己的形参之外的任何数据，包括全局变量。其语法如下： 12# 语法：lambda [参数列表]:返回值 让我们来尝试写写看，我们先来定义一个普通的加法运算的函数： 123456# 封装一个函数做加法运算# 普通函数def sum(x, y): return x+yprint(sum(2, 3)) 毫无疑问，执行结果为5。那么接下来，用lambda该怎么写呢？ 123456# 改成lambda表达式来封装res = lambda x, y:x+yprint(res(4, 4))# 执行结果8 结合闭包函数的讲解，这里就应该很容易看懂了吧？一样的地方就是，使用了一个变量res来接收这个返回的函数，然后执行res函数。 让我们再来一段： 12345res = lambda sex:&quot;很man&quot; if sex=='male' else '很nice'print(res('female'))# 执行结果很nice 只看结果的话，我们很清楚这段函数最后执行到了else语句内。但是是如何进入的呢？让我们将这段代码用普通函数的写法展开来看看： 1234567def func(sex): if sex == 'male': return '很man' else: return '很nice'func('female') 这样是不是很清晰了？回过头来让我们看刚才那段lambda表达式，我们可以这样去看： 12345# lambda 参数列表: 真区间 if 表达式判断 else 假区间lambda sex: '很man' if sex=='male' else '很nice'# 然后用一个变量接收函数res = lambda sex: '很man' if sex=='male' else '很nice' 所以可以看出来，其实lambda十分的方便，并且并不难理解，当你习惯了lambda之后，会非常便捷。 迭代器 迭代器是一个很有意思的功能，可以说是Python中最具特色的功能之一，它是访问集合元素的一种方式。 迭代器是一个可以记住访问遍历的位置的对象。从集合的第一个元素开始访问，直到集合中的所有元素被访问完毕。 迭代器只能是从前往后一个一个的遍历，不能后退。 我们把之前一直使用的range()拿过来看： 123456# range(10, 3, -1) 返回一个可迭代的对象for i in range(10, 3, -1): print(i, end=&quot; &quot;) # 执行结果10 9 8 7 6 5 4 表面上来看，似乎range()本身就是一个迭代器，可是我们来尝试做个实验： 12345x = range(5)print(next(x))# 执行结果TypeError: 'range' object is not an iterator 当我们尝试调用next()函数的时候报错了，被告知range不是一个迭代器。 那么，range不是迭代器，究竟是什么呢？这里我们就要先深入研究下迭代器的特性： 严格来说，迭代器是指实现了迭代协议的对象，迭代协议是指实现了iter方法并返回一个实现了next方法的迭代器对象，并通过StopIterator一场标识迭代完成。 iter() iter()能把迭代的对象，转为一个迭代器对象，其参数为可迭代的对象(str, list, tuple, dict)， 返回值为迭代器对象。其中需要注意的一点是：迭代器一定是一个可以迭代的对象，但是可迭代的对象并不一定是迭代器。 我们在迭代器上使用iter会得到相同的对象： 123456789i = iter([1, 2, 3])print(iter(i) is i)print(id(iter(i)))print(id(i))# 执行结果True44255020964425502096 基于此，我们可以这样实现： 12345i = iter([1, 2, 3, 4])list(zip(i, i))# 执行结果[(1, 2), (3, 4)] next() next()函数可以去调用迭代器，并返回迭代器中的下一个数据。 我们使用iter函数可以从任何可迭代对象中获取一个迭代器： 123456789a = iter([1, 2, 3])print(a)print(next(a))print(1 in a)# 执行结果&lt;list_iterator object at 0x124111a50&gt;1False 可以看到，我们使用iter函数可以从任何可迭代对象中获取一个迭代器。而且迭代器有个特点，即每次用完一个元素即消耗掉该元素，不会保留在迭代器中，也就是说，是一次性的。 1234567891011res = iter([1, 2, 3, 4])print(next(res), end=&quot; &quot;)print(next(res), end=&quot; &quot;)print(next(res), end=&quot; &quot;)print(next(res), end=&quot; &quot;)# print(next(res))r = list(res)print(r)# 执行结果1 2 3 4 [] 可以看到，list里面已经空了。 我们用for来取值： 123456789res = iter([1, 2, 3, 4])for i in res: print(i, end=&quot; &quot;)r = list(res)print(r)# 执行结果1 2 3 4 [] for直接将迭代内的元素全部取完了，所以最后打印下一个值的时候也显示空了。所以我们可以得到迭代器的取值方案： 迭代器的取值方案 next()：调用一次获取一次，直到数据被取完。 list()：使用list函数直接取出迭代器中的所有数据。 for：使用for循环遍历迭代器的数据 总结一下： 迭代器是一个可以记住遍历的位置的对象。 迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。 迭代器有两个基本的方法：iter() 和 next()。 字符串，列表或元组对象都可用于创建迭代器： 那么，再回过头来看看range range可以像任何其他可迭代对象一样循环使用，但是它并不具备迭代器中的一些特性，比如，我们之前实验过，range并不能使用next方法，而我们可以从range中得到一个迭代器： 1234iter(range(3))# 执行结果&lt;range_iterator at 0x124184570&gt; 我们在迭代器中使用元素就会消耗掉该元素，但是我们遍历一个range对象并不消耗它， 比如： 很明显我们可以重复使用。 来一个更直接的，我们之前用for获取了迭代里的值，我们对range()也来使用一下看看会不会有不同的结果： 123456789res = range(1,5)for i in res: print(i, end=&quot; &quot;)r = list(res)print(r)# 执行结果1 2 3 4 [1, 2, 3, 4] 一样的代码，对象不同。我们可以明显看到区别，range拿到最后里面的元素并没有减少。这也说明了，range并不是迭代器。 实际上，range的迭代是通过iter协议来实现的，只是一种类似迭代器的鸭子类型，并非真正的迭代器。 其实，有一种可以直接检测迭代器和可迭代对象的方法： 12345678910111213141516171819202122232425# 检测迭代器和可迭代对象from collections.abc import Iterator, Iterable varstr = '123456'res = iter(varstr)r = range(1, 5)# isinstance() 检测一个数据是不是一个指定的类型# Iterable: 迭代对象，Iterator: 迭代器r1 = isinstance(varstr, Iterable) r2 = isinstance(varstr, Iterator)r3 = isinstance(res, Iterable)r4 = isinstance(res, Iterator) r5 = isinstance(r, Iterable) r6 = isinstance(r, Iterator)print(f'varstr 是迭代对象：{r1}, \\t varstr 是迭代器: {r2}')print(f'res 是迭代对象：{r3}, \\t res 是迭代器: {r4}')print(f'r 是迭代对象：{r5}, \\t r 是迭代器: {r6}')# 执行结果varstr 是迭代对象：True, varstr 是迭代器: Falseres 是迭代对象：True, res 是迭代器: Truer 是迭代对象：True, r 是迭代器: False 今天的知识点讲到这就结束了，接下来，让我们来做两个小练习。 练习题 递归查询斐波那契数列位数 还记得我们之前讲过的斐波那契数列吗？不记得没关系，我们来复习一下： 1# 斐波那契数列: 0, 1, 1, 2, 3, 5, 8, 13... 我们这次来实现一个函数，用于查询斐波那契数列中当前位置的数值是多少。 1234567891011121314# 递归实现斐波那契数列def fibonacci(n): if n == 1: return 0 elif n == 2 or n == 3: return 1 else: return fibonacci(n-1) + fibonacci(n-2) res = fibonacci(6)print(res)# 执行结果5 我为大家画了张图，来看看程序内部到底做了些什么： 从这张图中，我们可以看到递归的步骤和返回的结果。 递归实现阶乘 什么是阶乘？比如我们实现7的阶乘，那么就是 \\[ 1\\times2\\times3\\times4\\times5\\times6\\times7 \\] 让我们来试着实现一下: 123456789101112# 实现阶乘def factorial(n): if n == 1: return 1 else: return n*factorial(n-1) res = factorial(7)print(res)# 执行结果5040 验证一下看看： 1234print(1*2*3*4*5*6*7)# 执行结果5040 看来结果没问题，那让我们来看看程序内发生了什么： 12345678910111213141516'''factorial(7) =&gt; 7 * factorial(6) =&gt; 6 * factorial(5) =&gt; 5 * factorial(4) =&gt; 4 * factorial(3) =&gt; 3 * factorial(2) =&gt; 2 * factorial(1) =&gt; factorial(1) = 1 2 * 1 = 2 3 * 2 = 6 4 * 6 = 24 5 * 24 = 120 6 * 120 = 7207 * 720 = 5040''' 虽然实现了，最后还是不得不说几点注意事项： 递归函数的效率并不高，所以尽量能不用就不要用。 一个函数如果调用后没有结束，那么栈空间中就一直存在，直到这个函数运算结束才会销毁。 好了，今天的课程到此结束。大家课后记得多练习。下课！","link":"/Higher-order-functions/"},{"title":"7. Python的内置函数","text":"Hi，大家好。我是茶桁。 讲完了基础函数和高阶函数之后，我们这一节来研究下Python的内置函数，看看Python在安装完毕之后的解释器里，到底都预先给我们提供好了哪些可用的函数。 本节内容着重介绍一些常用函数，并且会做一些应用上的示例。当然，对于Python的内置函数，我们还可以查询官方文档，我这节参照的为3.10版本文档 range()函数 这几节课中，我们频繁使用并且着重介绍过这个函数，那我们就从它开始介绍吧。 一般我们需要遍历一个数值序列的时候，range()函数就会派上用场，它生成算数级数。 12345678910'''range() 函数功能： 能够生成一个置顶的数值序列参数： start: 开始的值，默认为0 stop: 结束的值 [, step]: 可选，步进值， 默认为1返回值： 可迭代的对象，数字序列'''range(start, stop, [, step]) 让我们来看一下： 12345res = range(10)print(res, type(res))-----------------------------range(0, 10) &lt;class 'range'&gt; 可以看到这其实就是一个range的类，其实在我们Python中，任何数据都是一个对象而已。 来看案例： 123456# range函数的使用方式res = range(11)print(list(res))-----------------------------[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 当我们的range内只写一个参数时，这个参数就是stop值，也就是从start的默认值0开始到输入的参数值（stop）之前为止，比如这段代码中，stop会结束到11之前，也就是10。 我们在这段代码中，将range的内容转化成一个list并打印了出来。当然，我们也可以使用循环，依次去除range内的内容： 12345for i in res: print(i, end=&quot; &quot;) -----------------------------0 1 2 3 4 5 6 7 8 9 10 记得上节课我们提到过，range()是不支持next()函数的，不过如果我们将其转成迭代器，就可以使用next()函数调用： 123456789res = iter(range(11))print(next(res))print(next(res))print(list(res))-----------------------------01[2, 3, 4, 5, 6, 7, 8, 9, 10] 可以看到，使用iter转成迭代器之后，可以正常使用next()函数，并且我们再次查看res的内容，0,1已经被拿走，只将剩余内容转化为list打印了出来。 当我们在range中添加两个参数的时候，start就是第一个参数，第二个参数就是stop值。 123456# 添加两个参数for i in range(5, 10): print(i, end=&quot; &quot;)-----------------------------5 6 7 8 9 当我们输入三个参数的时候，第一个参数为start, 第二个参数为stop, 第三个参数就是[, step]， 比如： 123456# 添加三个参数for i in range(1, 10, 3): print(i, end=&quot; &quot;) -----------------------------1 4 7 这段代码的含义就是从1开始, 以3为步进来提取数字，并打印出来，一直到10之前的数字为止。 如果不太理解步进值的可以执行数一遍就理解了，比如我们从1开始顺序往后数3个数，那就是2、3、4，数到了4, 再继续往后数3个数，就是5、6、7，数到了7。再继续往后就是8、9、10。但是，我们代码中的stop值为10，所以到9就结束了，也就是说，我们这段代码就只取出了1, 4, 7三个值。 三种参数值的情况我们都了解之后我们可以思考下，难道我们只能选择顺序取值吗？其实不然，我们还可以倒叙取值，聪明的小伙伴可能想到了，调换一下start和stop值不就可以了嘛？我们从10开始取值，取到0为止： 12for i in range(10, 0): print(i, end=&quot; &quot;) 执行一下，哎，似乎什么都没打印出来。这又是为什么呢？是不是出BUG了？ 其实，什么都没打印出来才是正确的，这是因为，虽然我们给了开始和结束值，但是我们遗忘了一个重要的参数，那就是步进值step，这个值默认可是1，从10开始+1来计数，无论如何也算不到0。所以，我们将步进值改成负数，也就是倒着数了: 12345for i in range(10, 0, -1): print(i, end=&quot; &quot;) -----------------------------10 9 8 7 6 5 4 3 2 1 至此，我们可以得到结论，是否倒叙取值除了开始和结束值，更重要的是看step是正数还是负数。 123456res = range(-10,-20,-1) # [-10, -11, -12, -13, -14, -15, -16, -17, -18, -19]res = range(-20,-10) # [-20, -19, -18, -17, -16, -15, -14, -13, -12, -11]res = range(-10,10)# [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9] zip()函数 zip() 函数可以接受多个可迭代的对象，然后把每个可迭代对象中的第i隔元素组合在一起，形成一个新的迭代器。 12345'''参数： *iterables, 任意个的可迭代对象返回值： 返回一个元组的迭代器'''zip(*iterables) 让我们来直接看示例： 123456789n1 = '1234'n2 = ['a', 'b', 'c']n3 = ['A', 'B', 'C', 'D']# 调用zip函数，合成新的元组迭代器res = zip(n1, n2, n3)print(list(res))-----------------------------[('1', 'a', 'A'), ('2', 'b', 'B'), ('3', 'c', 'C')] 我知道你们看到这个执行结果会有很多疑问，先别着急，我们先看一下它是否包含迭代器的特性： 12for i in res: print(i) 当你执行这段代码的时候就会发现，似乎什么都没发生。 那到底是怎么回事？我们不用for，让我们再转换一次list之后看看里边有什么： 1234print(list(res))-----------------------------[] 列表居然是空的。是不是瞬间想到了什么？ 没错，这个似乎就是迭代器的特性之一，当其中元素被使用之后，会删掉使用过的元素。而我们之前在执行print(list(res))的时候，已经将内部元素都转成list并展现过，所以现在res内的元素都被删掉了。 没事，让我们再重新来定义一次，也就是重新给res内填满元素然后直接for循环一次看看： 12345678910111213n1 = '1234'n2 = ['a', 'b', 'c']n3 = ['A', 'B', 'C', 'D']# 调用zip函数，合成新的元组迭代器res = zip(n1, n2, n3)for i in res: print(i) -----------------------------：('1', 'a', 'A')('2', 'b', 'B')('3', 'c', 'C') 我们可以看到，每次打印i的时候都打印了一个元组，而这个元组就是一个新元素，比如第一行('1', 'a', 'A'), 这整个元组就是一个新元素。 让我们再用next试试（当然我又重新填满了res）： 123456print(next(res))print(next(res))-----------------------------('1', 'a', 'A')('2', 'b', 'B') next函数也能正常执行，那可以说明，zip确实组合成了一个新的迭代器。 现在我们返回来再看一遍代码中的n1,n2,n3, 分别是1234, [‘a’, 'b', 'c'], ['A', 'B', 'C', 'D']。最后组成的迭代器对象为：[('1', 'a', 'A'), ('2', 'b', 'B'), ('3', 'c', 'C')]。 通过分析可以看出来，zip的工作原理是先分别取可迭代对象的第一个元素组合成一个元组，然后再分别取第二个元素组合成一个元组，依次往后取... 可是n1, n3分别都是四个元素，为什么我们最后只组合成了三个元组？那是因为n2中只包含了三个元素，当在其中找不到第四个元素的时候，就会放弃组合。 来，让我们在看一个示例： 1234567n1 = [1, 2, 3, 4]n2 = [22, 33, 44, 55]res = zip(n1, n2)print(list(res))-----------------------------[(1, 22), (2, 33), (3, 44), (4, 55)] 大家看到最后的执行结果有没有觉得很眼熟？可能很多小伙伴一时间想不到，我们来调整一下： 123456[ (1, 22), (2, 33), (3, 44), (4, 55)] 记住这个数据结构，我们在后期做数据分析的时候， 当我们做矩阵运算的时候用的非常多。 不知道大家是否都学过高等数学里的线性代数、微积分，包括概率统计。这些在我们之后做数据分析，数据挖掘，包括机器学习、人工智能这些科学运算里面，非常重要的一些数学功底。 不太记得了也没关系，这些我后面将会专门拿几节出来给大家补一下这方面。 让我们继续，zip还有一种应用方式，当其与*运算符结合使用的时候，可以用来拆解列表： 12345678910# zip 与 * 运算符相结合使用x = [1, 2, 3]y = [4, 5, 6]print(zip(x, y))print(*zip(x, y))-----------------------------&lt;zip object at 0x107b8d200&gt;(1, 4) (2, 5) (3, 6) 可以看到，zip是一个迭代器，*zip这生成了组合好的多个元组数据。 比如： 12345678x1 = [1, 2, 3]y1 = [4, 5, 6]x2, y2 = zip(*zip(x, y))print(x2, y2)-----------------------------(1, 2, 3) (4, 5, 6) 这样，我们就将两个列表转换成了两个元组。当然，其实我们这样操作还不如直接使用tuple函数来的方便快捷一点。 那下面，我们就看看都有哪些数据类型转换相关的内置函数。 数据类型转换相关的内置函数 这些函数的功能非常简单和单一，属于拿来就用的函数，我们就仅列出来，不多做介绍了。 int() 将其它类型数据转为整型 float()转为浮点类型 bool()转为布尔类型 complex()转为复数 str()转为字符串类型 list 转为列表类型 tuple转为元组类型 dict 转为字典类型 set 转为集合类型 变量相关函数 id() 获取当前数据的ID标识 type() 获取当前数据的类型字符串 print()数据的打印 input()获取输入的数据 isinstance()检测是否为指定的数据类型 数学相关函数 abs()获取一个数的绝对值 1234print(abs(-99.99))-----------------------------99.99 sum()求和 从 start 开始自左向右对 iterable 中的项求和并返回总计值 1234print(sum([1,2,3]))-----------------------------6 max() 获取最大值 123456print(max([1,2,3]))print(max(99,12,45))-----------------------------399 min() 获取最小值 123456print(min([2,1,6,-9]))print(min(6,7,1,0,-2))------------------------------9-2 pow(x, y)幂运算 返回 x 的 y 次幂 1234print(pow(2,3)) -----------------------------8 round(x, n) 对x四舍五入，小数点保留n位 123456print(round(3.1415926))print(round(3.1415926,2))-----------------------------33.14 round这个函数不是绝对意义上的四舍五入，在取整这个问题是是奇进偶退： 123456print(round(3.5))print(round(4.5))-----------------------------：44 进制函数及字符集 bin() 将数值类型转为二进制 1234print(bin(123)) -----------------------------0b1111011 int() 将二进制转化为整型 1234print(int(0b1111011))-----------------------------123 oct() 转为八进制数 1234print(oct(123))-----------------------------0o173 hex() 转为十六进制数 1234print(hex(123))-----------------------------0x7b ASCII及字符集 ASCII，全称为美国信息互换标准代码。是一套基于拉丁字母的字符编码，共收录了 128 个字符，用一个字节就可以存储，它等同于国际标准 ISO/IEC 646。它一共有128个支付，最后更新是1986年。 我们要知道的是，ASCII 编码是美国人给自己设计的，他们并没有考虑欧洲那些扩展的拉丁字母，也没有考虑韩语和日语，我大中华几万个汉字更是不可能被重视。计算机也是美国人发明的，起初使用的就是 ASCII 码，只能显示英文字符。各个国家为了让本国公民也能正常使用计算机，开始效仿 ASCII 开发自己的字符编码，例如 ISO/IEC 8859（欧洲字符集）、shift_Jis（日语字符集）、GBK（中文字符集）等。 从65开始到90为止，是大写字母（A ~ Z), 97到122是小写字母(a ~ z)，48到57是0 ~ 9。 而我们经常使用的是GB2312-80, GBK和GBK18030以及Unicode字符集。 GB2312-80 是 1980 年制定的中国汉字编码国家标准。共收录 7445 个字符，其中汉字 6763 个。 GBK 于1995年制定 收录了 21003 个汉字。GBK向下与 GB 2312 编码兼容， GBK18030 2001年的1月正式强制执行，是我国制订的以汉字为主并包含多种我国少数民族文字（如藏、蒙古、傣、彝、朝鲜、维吾尔文等）的超大型中文编码字符集强制性标准，其中收入汉字70000余。 Unicode（统一码、万国码、单一码）是计算机科学领域里的一项业界标准，包括字符集、编码方案等。 它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。1990年开始研发，1994年正式公布。 UTF-8 以字节为单位对Unicode进行编码。 我们现在写代码的时候基本遵循UTF-8编码为主。 有的时候，我们是需要将字符转为ASCII， 也有对应的方法： 1234print(ord('a'))-----------------------------a 将ASCII转为字符也一样： 1234print(chr(65))-----------------------------A 高阶函数 和上一节课不同，我们现在要讲的高阶函数，是Python解释器里的内置高阶函数。 sorted() 很多时候，我们在处理数据的时候都需要对数据进行排序。不管是以序号，名称还是日期的方式。sorted()就是我们最常用的排序函数： 12345678910sorted(iterable, [reverse, key])‘’‘运行原理：把可迭代数据里面的元素，一个一个的取出来，放到key这个函数中进行处理，并按照函数中return的结果进行排序，返回一个新的列表功能：排序参数： iterable:可迭代的数据 （容器类型数据，range数据序列，迭代器） reverse:可选，是否反转，默认为False，不反转， True反转 key:可选， 函数，可以是自定义函数，也可以是内置函数返回值：排序后的结果’‘’ 我们来看几个示例，首先我们先来看看默认的排序方式：从小到大： 123456arr = [3,7,1,-9,20,10]res = sorted(arr) print(res)-----------------------------[-9, 1, 3, 7, 10, 20] 当然，既然我们能从小到大来进行排序，那就可以用从大到小的方式： 12345arr = [3,7,1,-9,20,10]print(sorted(arr,reverse=True))-----------------------------[20, 10, 7, 3, 1, -9] 现在我们得到了从小到大排序，也得到了从大到小排序。然后我们再来作妖：能不能按照所有数字的绝对值大虾哦进行排序呢？哎，还记得我们刚讲过的数学相关的函数里有一个求绝对值的函数嘛？既然sorted()这个函数里的参数key可以接收函数，那让我们结合在一起试试看： 123456arr = [3,7,1,-9,20,10]res = sorted(arr,key=abs)print(res)-----------------------------[1, 3, 7, -9, 10, 20] 果然，我们得到了想要的结果。来分析下内部到底做了什么： 1234[3,7,1,-9,20,10] # 原始列表3 7 1 9 20 10 # 求绝对值1 3 7 9 10 20 # 给绝对值进行排序1 3 7 -9 10 20 # 转换成原本的值 那现在，我再多尝试一下，我试试看自己定义一个函数： 12def func(num): return num % 2 函数定义好了，让我们尝试使用自定义函数对数据进行排序： 12345arr = [3,2,4,6,5,7,9]print(sorted(arr, key = func))-----------------------------[2, 4, 6, 3, 5, 7, 9] 看似起结果了。那到底函数内干了些什么呢？让我们在其中多打印一点东西出来，看个明白： 1234567891011121314151617def func(num): print(num, num % 2, end=&quot; &quot;) print() return num % 2 arr = [3,2,4,6,5,7,9]print(sorted(arr, key = func))-----------------------------3 1 2 0 4 0 6 0 5 1 7 1 9 1 [2, 4, 6, 3, 5, 7, 9] 这样我们就很清晰的看到了对原数字和取余结果，在对取余进行排序之后，再在取余的结果上进行默认的从小到大进行排序，就得到了最后的结果[2, 4, 6, 3, 5, 7, 9] 不过，这种功能大多数时候我们基本是临时用一下，特意写一个方法似乎有点多余。还记得咱们之前讲的匿名函数吧？让我们用匿名函数优化一下： 1234567# 用匿名函数优化arr = [3,2,4,6,5,7,9]res = sorted(arr, key=lambda num:num%2)print(res)-----------------------------[2, 4, 6, 3, 5, 7, 9] 正是我们所要的结果。 从这就能看出来，高阶函数sorted()的key因为能接收自定义函数，所以给了我们很大的可玩空间。小伙伴们还能想到哪些方法，快去做做实验。 map() 这个函数在对传入的可迭代数据中的每一个元素进行处理，然后返回一个新的迭代器： 12345678map(func, *iterables)'''功能： 对传入的可迭代数据中的每个元素放入到函数中进行处理，返回一个新的迭代器参数： func 函数 自定义函数|内置函数 iterables：可迭代的数据返回值：迭代器''' 让我们先来看一个普通的函数： 12345678910# 把一个字符串数字的列表转为整型列表items = ['1', '2', '3', '4']new_list = []for i in items: new_list.append(int(i))print(new_list)-----------------------------[1, 2, 3, 4] 我们将一个内部元素均为字符串的列表，转成了一个整型列表。 不过这个函数看起来似乎还是有些麻烦，让我们再用map试试看： 123456items = ['1', '2', '3', '4']res = map(int, items)print(list(res))-----------------------------[1, 2, 3, 4] 看，是不是简便多啦？当然，最后map最后生成的是迭代器而并不是列表，我们还是需要转化一下数据类型。 这段代码中map的处理方式其实非常简单，它将items里的每一个元素用int方法转换成整型，转换完之后形成一个新的迭代器，然后返回。 再让我们看一个示例感受一下, 这次我们将两段对比写在一起： 123456789101112131415161718# 普通方法进行实现items = [1, 2, 3, 4]res = []for i in items: x = i ** 2 res.append(x)print(res)# 使用map函数处理items = [1, 2, 3, 4]def func(x): return x ** 2res = map(func, items)print(res, list(res))-----------------------------[1, 4, 9, 16]&lt;map object at 0x107ea5030&gt; [1, 4, 9, 16] 我们看到执行结果完全一样，不过使用map()的方式更简洁，逻辑也更清晰。我们要知道，Python本身是自带幂次方方法的。即便是我们自己来实现，其实我们还可以把代码写的更简洁： 123456items = [1, 2, 3, 4]res = map(lambda x:x**2, items)print(res,list(res))-----------------------------&lt;map object at 0x107c98610&gt; [1, 4, 9, 16] 基于map的应用，我们来个小作业吧： 我们现在有这样一个列表：['a','b','c','d'] 要求将其转换成：[65,66,67,68] 最后，再给大家留个课后作业， 我给大家两个函数及其功能介绍，大家自己去尝试看看，然后自己琢磨下其用法。 reduce(func, iterable) 功能：每一次从 iterable 拿出两个元素，放入到func函数中进行处理，得出一个计算结果，然后把这个计算结果和iterable中的第三个元素，放入到func函数中继续运算，得出的结果和之后的第四个元素，加入到func函数中进行处理，以此类推，直到最后的元素都参与了运算 filter(func, iterable) 功能：过滤数据，把 iterable 中的每个元素拿到 func 函数中进行处理，如果函数返回True则保留这个数据，返回False则丢弃这个数据。 这两个函数在处理数据上作用都非常大。 好了，下课。大家有问题记得留言。","link":"/python-Built-in-functions/"},{"title":"8. 数据类型 - 字符串详解","text":"Hi, 大家好。我是茶桁。 前几节课中我们学习了函数，那么这节课开始，我们花几节课返过头来详细的学习一下Python内的数据类型。第一节课，让我们先从字符串开始： 回顾字符串的定义方式 了解转义字符 字符串格式化的方法 字符串相关函数 字符串的定义方式 单引号定义字符串 ‘ ’ 双引号定义字符串“ ” 三引号定义字符串‘’‘内容’‘’或者“”“内容”“” 字符串定义时，引号可以互相嵌套 转义字符 一个普通的字符出现在转义符\\的后面时，实现了另外一种意义。 \\ 转义符，续行符。 作为转义符时，在\\后面出现的字符可能会实现另外一种意义。 作为续行符时，在行尾使用了\\后，可以换行继续书写内容。 123456str = '123'\\ '456'print(str)---123456 打印结果看，并未换行，说明续行符起作用了。 \\n 代表一个换行符 123456str = &quot;岁月是一把杀猪刀， \\n但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪刀， 但是它拿长得丑的人一点办法都没有。 \\r代表光标位置（从\\r出现的位置开始作为光标的起点） 12345str = &quot;岁月是一把杀猪刀， \\r但是它拿长得丑的人一点办法都没有。&quot;print(str)---但是它拿长得丑的人一点办法都没有。 \\t代表一个水平制表符（table 缩进） 12345str = &quot;岁月是一把杀猪刀\\t但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪刀 但是它拿长得丑的人一点办法都没有。 \\b 代表一个退格符 12345str = &quot;岁月是一把杀猪刀\\b但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪但是它拿长得丑的人一点办法都没有。 注意看，并不是毫无改变的打印出来了，整句话中\\b前面的刀这个字被退格了。 \\\\ 反转义\\，输出了\\，取消\\的转义效果 12345str = &quot;岁月是一把杀猪刀\\\\n但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪刀\\n但是它拿长得丑的人一点办法都没有。 第二个\\被前面的\\转义了，所以n就不会再被转义，也就没有换行。 r， 如果我们想把转义字符也作为普通字符输出，那我们可以在字符串的最前面加上r 12345str = r&quot;岁月是一把杀猪刀\\n但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪刀\\n但是它拿长得丑的人一点办法都没有。 字符串内的转移字符\\n被打印了出来。 字符串相关的操作 字符串+操作, 将参与运算的字符串相加后组成一个新的字符串。 123456789str=&quot;君不见，黄河之水天上来，奔流到海不复回。&quot;str2 = &quot;君不见，高堂明镜悲白发，朝如青丝暮成雪。&quot;res = '将进酒\\n'+ str + '\\n' + str2print(res)---将进酒君不见，黄河之水天上来，奔流到海不复回。君不见，高堂明镜悲白发，朝如青丝暮成雪。 字符串*操作，str*n就是 将当前字符串重复n遍 1234567str = '重要的话说三遍\\n' * 3print(str)---重要的话说三遍重要的话说三遍重要的话说三遍 字符串[]切片操作 字符串的索引操作，字符串中只能使用[]下标访问，不能修改。 123456str[start:stop:step]功能，获取str的特定下标或者对str进行切片操作参数： start: 可选，开始值，默认为0 stop: 可选，结束值，默认为len(str) step：可选，步进值，默认为1 因为所有参数都是可选项，所以其实我们可以什么参数都不给，直接使用默认值： 12345678str = '凡诗之所谓风者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。'print(str)# 等同于print(str[::])---凡诗之所谓风者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。凡诗之所谓风者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。 当我们写一个值，那就是获取指定下标的元素： 1234print(str[6])---者 但是当我们只写一个值，并且后面跟上符号::, 那含义就是从start开始，向后取完： 1234print(str[6::])---者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。 从这我们可以看出来，当我们只写一个单独的值而没有加::的时候，含义就是从start开始，但是并不向后继续取值，而有了::就是继续向后取值。其实，只写一个:也是一样的，因为只要知道向后取值，step默认值就是为1: 1234print(str[6:])---者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。 那如果我们在这个基础上加上一个值，那就是从start开始直到stop之前。和range()一样，取不到stop。 1234print(str[2:6])---之所谓风 然后再多加一个值，和range()一样，就是往后数step个数再取值： 1234print(str[2:15:2])---之谓者多于巷谣 其实，这里比较饶的并不是如何取值，二是::这两个符号。当我们将上面讲的这些内容了解通透后，就可以玩转字符串的切片了。 那对应的，如果我们想将字符串完全取值，但是是隔一个取一个，那我们就可以使用start和stop的默认值，而只定义step 1234print(str[::2])---凡之谓者多于巷谣作所男相咏，言情也 那如果我们想让整个字符串倒过来呢？ 1234print(str[::-1])---。也者情其言各，歌咏与相女男谓所，作之谣歌巷里于出多，者风谓所之诗凡 字符串的格式化方法 常用的字符串的格式化方法就是format() 先让我们看看最普通的方式： 12s = '茶桁'str = '乘舟将欲行，忽闻岸上踏歌声。' 我定义了这两个字符串，现在我想将两段字符串合在一起变成一句“茶桁乘舟将欲行，忽闻岸上踏歌声。”（嗯，权吾乃青蓮居士。） 很多小伙伴是不是觉得太简单了，我们之前学了+号，直接拼接不就好了。自然也是可以的，只是我们现在要用更普遍和便捷的方式来完成： 123456s = '茶桁'str = '{}乘舟将欲行，忽闻岸上踏歌声。'.format(s)print(str)---茶桁乘舟将欲行，忽闻岸上踏歌声。 假如说，我们现在只有诗词的大半句，其中少了踏歌行这三个字，那我们又该如何？那我们就往format中传入两个参数，后面那个参数自定义出这三个字符就可以了： 123456s = '茶桁'str = '{}乘舟将欲行，忽闻岸上{}。'.format(s, &quot;踏歌行&quot;)print(str)---茶桁乘舟将欲行，忽闻岸上踏歌行。 看到这里，我们是不是认为字符串使用format就只能顺序传值？第一个答案填入第一个空，第二个答案填入第二个空... 其实不只是如此，字符串后使用format，其中的{}还可以接受索引传参： 123456s = '茶桁'str = '{1}乘舟将欲行，忽闻岸上{0}。'.format(&quot;踏歌行&quot;, s)print(str)---茶桁乘舟将欲行，忽闻岸上踏歌行。 通过索引传参的适用范围毕竟还是有限，我们很容易一不小心就会把参数顺序搞乱。那还有没有其他办法呢？ 我们还可以通过关键字传参： 12345str = '{s2}乘舟将欲行，忽闻岸上{s1}。'.format(s1 = &quot;踏歌行&quot;, s2 = &quot;茶桁&quot;)print(str)---茶桁乘舟将欲行，忽闻岸上踏歌行。 那假如说我们得到的是一个列表数据，是否需要先转换数据？其实也没必要，format支持对容器型数据的传参数： 12345str = '豪放派：{0[0]}，婉约派：{0[1]}，流氓派:{0[3]},蛋黄派：{0[2]}'.format(['李白','辛弃疾','达利园','茶桁'])print(str)---豪放派：李白，婉约派：辛弃疾，流氓派:茶桁,蛋黄派：达利园 那么如果是字典类型的呢？那就更简单了，我们之前提到的关键字传参，不就正好对应字典吗？ 123456dict = {'a':'茶桁', 'b':'蛋黄派'}str = '{a}乘舟将欲行，忽闻岸上{b}'.format(**dict)print(str)---茶桁乘舟将欲行，忽闻岸上蛋黄派 嗯，不错。似乎我们创建了一句新的诗句。 其实，format还有其他的用法，就是直接用关键字f， 比如： 12345str = f'{dict[&quot;a&quot;]}乘舟将欲行，忽闻岸上{dict[&quot;b&quot;]}'print(str)---茶桁乘舟将欲行，忽闻岸上蛋黄派 f是在3.7版本中新增的格式化方法，在使用的过程中，要注意字符串符号“”和‘’的嵌套关系。 在基本使用之外，我们还有一些风骚的特殊用法，比如，我们可以用format直接限定小数的位数： 12345str = '圆周率是多少：{:.5f}'.format(3.1415926)print(str)---圆周率是多少：3.14159 字符串相关函数 在Python中，字符串应该是最常见的数据类型，对应字符串的函数也有不少。大家可以去看看官方的文档 英文字符与字符检测相关函数 我们可以返回字符串的副本，并且将首字母大写，其余小写： 12345str = 'I am a data product manager'str.capitalize()---'I am a data product manager' 因为我在使用Jupyter Notebook，所以即便我么有使用print，依然可以打印出执行结果。只是仅可以打印最后一个执行的函数。 可以把字符串中的一个单词的首字母大写： 1234str.title()---'I Am A Data Product Manager' 可以全部改为大写： 1234str.upper()---'I AM A DATA PRODUCT MANAGER' 把字符串全部改为小写 1234str.lower()---'i am a data product manager' 字符串中的大小写字符转换，大写转小写，小写转大写: 1234str.swapcase()---'i AM A DATA PRODUCT MANAGER' 检测字符是否包含在字符串内： 1234print('o' in 'love')---True 检测字符串是否为全部大写字母组成 1234str.isupper()---False 检测字符串是否为全部小写字母组成 1234str.islower()---False 检测字符串是否符合标题title的要求 1234str.istitle()---False 检测字符串是否由数字和字母组成，如果字符串中包含来非数字字母的其它字符，则返回False 1234str.isalnum()---False 检测字符串是否全部由字符(包含英文字符和中文)组成 1234str.isalpha()---False 检测字符串是否由纯数字字符组成 1234'123'.isdigit()---True 检测当前字符串是否为 空格 字符组成 ' ’ 1234' '.isspace()---True 检测字符串是否以指定的字符开始的，也可以指定开始和结束的位置 1234str.startswith('I')---True 1234str.startswith('a', 5)---True 检测字符串是否以 指定的字符 结束的，也可以指定开始和结束的位置 12345678print(str.endswith('a'))print(str.endswith('a', 5, 11))print(str.endswith('a', 1, 6))---FalseTrueTrue 字符串的查找和操作相关函数（✨ 重点） 前面铺垫了那么多之后，接下来这部分，才是这一节的重点。 让我们先从查找来看： str.find(sub[, start[, end]]) find会返回一个子字符串，找到字符中符合条件的第一个字符出现的索引位置，未找到则返回-1 12345str = &quot;I am a data product manager.&quot;print(str.find('am'))---2 让我们用切片的方式反过来找一下看看： 12345res = str.find('am')str[res:res+2]---'am' 我们从之前可以知道res取值为2，现在等于是str[2:4]， 正好是am所在的位置。 find中有start和end，是支持切片查找的： 123456print(str.find('am', 0, 4))print(str.find('am', 4, 10))---2-1 可以看到，在从4开始找到10的时候找不到am, find有一个功能相同，但是方向不同的方法rfind(), 和find的不同点只是，rfind是从后往前找的。 str.index(sub[, start[, end]]) 类似于find()， 但在找不到子字符串的时候会引发ValueError 1234str.index('python')---ValueError: substring not found str.count(sub[, start[, end]]) 这个函数会在字符串中去查找sub在其中[start, end]范围内非重叠出现的次数。 123456print(str.count('a'))print(str.count('a', 5, 12))---63 接下来让我们看看字符串操作相关的函数： str.split(sep=None, maxsplit=-1) 这个方法可以按照指定的分隔符(sep)，把字符串分隔成列表。 12345str = 'user_admin_id_123'str.split('_')---['user', 'admin', 'id', '123'] 整个方法里的maxsplit是进行多少次拆分，比如1为一次拆分，也就是会返回2个元素。默认值为-1，意思是不限制拆分次数。 1234str.split('_', 1)---['user', 'admin_id_123'] str.rsplit(sep=None, maxsplit=-1) 和split方法相似，只是方向不同。这个是从后向前获取。 1234str.rsplit('_')---['user', 'admin', 'id', '123'] 这段代码可以看到功能上是完全一样的，如果我们把maxsplit加进去，就能看到方向上的不同： 1234str.rsplit('_', 1)---['user_admin_id', '123'] 这样就能清晰看到，rsplit是从后面开始拆分的。 str.join(iterable) join的功能和split可以看成是相反的，是使用指定的字符串，把一个容器中的元素连接成一整个字符串 12345str = ['user', 'admin', 'id', '123']'_'.join(str)---'user_admin_id_123' str.strip([chars]) 去除字符串左右两侧的指定字符, chars参数为置顶要溢出字符的字符串，默认移除空白符。 12345str = ' chaheng 'str.strip(' ')---'chaheng' 这个函数有两个伴生函数，一个是rstrip， 从方法名应该能猜的出来，这是去掉字符串右侧的指定字符，另一个是lstrip， 这是去除左侧的指定字符。 123456str.rstrip(' ')str.lstrip(' ')---' chaheng''chaheng ' len()函数可以获取当前字符串的长度 1234len(str)---9 str.replace(old, new[, count]) 可以替换对应的字符串，将old都替称为new。count则是替换次数。比如一个字符串内出现了十次old， 我`count给的5, 则只替换前5次出现的old字符串。 1234567str = 'abcabcabcabcabcabc'str.replace('a', 'e')str.replace('a', 'e', 2)---'ebcebcebcebcebcebc''ebcebcabcabcabcabc' 可以注意一下两次打印的区别。 这次就不留练习题了，字符串的查询和操作函数属于重中之重，大家最好是多去练习几遍，将其中的方法记会杯熟。 好，今天就到这里。咱们下节课再见。","link":"/Detailed-of-string/"},{"title":"9. 数据类型 - 列表详解","text":"Hi，大家好。我是茶桁。 最近几节课，我们都是在详细讲解Python内的数据类型，上一节课我们详细了解了字符串，这节课，让我们来详解一下列表。 首先，我们先有一个大的概念，列表，其实就是一组有序的数据组合；另外，列表中的数据是可以被修改的。也就是说，列表是一个可变序列类型。 列表定义 如何在Python的定义列表，记住以下几点就可以了： 可以使用中括号进行定义[] 可以使用list()函数定义 还可以使用列表推导式定义: [x for x in iterable] 在定义列表中的元素时，需要在每个元素之间使用逗号（英文逗号），进行分隔。[1, 2, 3] 列表中的元素可以是任意类型，通常用于存放同类项目的集合。 列表的基本操作 让我们先来定义一个列表: 1234567items = [1, 2, 3, 4]items2 = list('1234')print(items,'\\n', items2)---[1, 2, 3, 4] ['1', '2', '4', '5'] 我们使用了最基本的两个方式来定义列表。至于列表推导式， 先不用着急，我们后面会单独讲它。 我们可以看到，刚才我刻意将item和items两个列表定义了不同种类的元素，那他们到底能否拼接在一起？我们尝试一下列表的相加： 1234print(items + items2)---[1, 2, 3, 4, '1', '2', '3', '4'] 没问题，两种不同类型的元素拼接到了一起，组成了一个新的列表。 让我们将这段代码搞的复杂一点，新的列表对于我要的模拟数据来说太少了，我想再增加5倍的长度： 1234print((items + items2) * 5)---[1, 2, 3, 4, '1', '2', '3', '4', 1, 2, 3, 4, '1', '2', '3', '4', 1, 2, 3, 4, '1', '2', '3', '4', 1, 2, 3, 4, '1', '2', '3', '4', 1, 2, 3, 4, '1', '2', '3', '4'] 没毛病，也就是说，我将小学学到的基本数学运算用到这里完全适用。 那如果用到减法呢，虽然难以想象最后的结果，试试中可以： 1234print(items - items2)---TypeError: unsupported operand type(s) for -: 'list' and 'list' 果然是我想多了，完全不支持操作数类型。 那是不是关于列表的操作也就到此为止了？并不是，列表除了利用加和乘进行拼接和循环的操作之外，还有很多其他的基本操作，比如： 12345items[2] = 9print(items, &quot;\\t&quot;,items[3])---[1, 2, 9, 4] 4 这里，我们利用了列表的下标操作修改了列表内的下标[2]的元素（第三个），并且将修改后的列表和列表内下标[3]的元素打印了出来。 有这样一种情况大家想过没有，这个列表呢，我并不知道有多长，但是我知道最后一个数字，现在我就想把最后一个数字取出来该怎么办？用len()获取长度之后再-1? 是不是太麻烦了？ 还记得之前咱们讲过，下标是可以从后往前数的吗？ 1234items[-1]---4 嗯，我想再这个列表添加几个数字： 1234items[4] = 10---IndexError: list assignment index out of range 哎，我似乎想的并不对。本以为原列表下标[3]是最后一个元素，那我多加一个下标就会再多加一个元素，可是似乎并不行。那么我们该怎么在列表内最佳元素呢？ 可以尝试一下专门添加元素的append()函数： 123456items = [1, 2, 3, 4]items.append(2)print(items)---[1, 2, 3, 4, 2] 加是加了，可是我们之前是想加10的，现在不小心加成2了，不行，我要删了它。该怎么办？随便吧，我就记得windows的CMD命令中的删除文件似乎是del，试试看： 12345del items[-1]print(items)---[1, 2, 3, 4] 居然成了... 这就神奇了。看起来，Python并不是很难。不过我们这里不得不说，在Python中还有一个针对列表删除元素的方法：pop() 12345items = [1, 2, 3, 4]items.pop()---[1, 2, 3] pop([index=-1])函数专门用于移除列表中的一个元素，其中参数index为索引值，默认为-1，也就是说默认是从列表移除最后一个值。 1234567# 将索引值改为从前数第一个items = [1, 2, 3, 4]items.pop(0)print(items)---[2, 3, 4] 列表中的切片 在学习了列表的基本操作之后，我们来看看列表中的切片。提前说一声，在数据分析的应用中，对数据整理的过程绝大多数时候都需要用到列表的切片操作， 所以大家这部分要好好理解。 列表的切片操作基本语法其实很简单 list[开始值:结束值:步进值] 看起来很熟悉对吧？在我们之前介绍字符串相关的操作的时候，就是这种方式。其用法和字符串中也是如出一辙： list[开始值:] 从开始值索引到列表的最后 list[:结束值]从列表最前面索引到结束值之前 list[开始值:结束值]按照给到的开始值开始索引，到结束值之前为止。 当然，除了这三个基本的操作之外还有list[::], list[::步进值], list[开始值::步进值], list[:结束值:步进值],list[开始值:结束值:步进值]，我们下面一一的来看看，在字符串相关操作中没有特别理解的没关系，这里再来加深下印象： 12# 先来定义一个列表方便后续操作items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP'] 从开始值索引到最后： 1234print(items[2:])---['Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP'] 从下标[2]开始，也就是从第三个Ruby开始向后索引。 从最前面索引到结束值之前： 1234print(items[:2])---['Python', 'Java'] 现在我们让这两个语言单独露脸，算是对它们进行补偿了。 从开始值索引到结束值之前： 1234print(items[2:3])---['Ruby'] 哎，为什么只索引出来一个值？因为结束值为3，它之前不就是2吗。开始值也是2，那可不就只有一个值而已。 这回，我们把步进值加上： 12345# 加上步进值print(items[0:-1:2])---['Python', 'Ruby', 'C++', 'JavaScript'] 从最前面索引到最后，步进值为2，所以是隔一个索引一个。那为什么PHP没索引到？估计你又忘了，是索引到结束值之前，不包含结束值，自然PHP就没被索引到。 只有步进值会是什么情况？ 12345# 只有步进值print(items[::-2])---['PHP', 'JavaScript', 'C++', 'Ruby', 'Python'] 步进值为负数，那显然是从后向前索引了。隔一个索引一个，等等，为啥第一个Python被索引到了？ 那是因为，当我们开始值和结束值都没取值的情况下，默认是从头到尾索引，现在嘛，应该是从尾到头索引。也就是包含了头尾，不存在最后一个值之前，所以列表内的所有值都索引了一个遍，只是因为有步进值的关系，所以变成隔一个索引一个。 再让我们将所有值都去掉，只留下[::]试试看： 1234567# 删掉所有值试试print(items[::])print(items[:])---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP']['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP'] 从结果上看，中括号内一个冒号和两个冒号出来的结果是一样的。 在索引查找之后，我们来看看，利用切片的方式是否可以对列表内的元素进行更新和删除？ 从指定下标开始，到指定下标前结束，并替换为对应的数据(容器类型数据，会拆分成每个元素进行赋值) 123456items = [1, 2, 3, 4, 5]items[2:4] = [7, 8]print(items)---[1, 2, 7, 8, 5] 指定的切片内的元素被替换掉了。 刚才我们使用切片替换元素的时候元素是一一对应的，那如果我们没有对应的话会发生什么？ 1234567# 切片范围大于添加元素的个数items = [1, 2, 3, 4, 5]items[2:6] = [7]print(items)---[1, 2, 7] 结果并没有报错，而是将切片范围内的元素都移除之后添加了一个元素7。我们再试试其他的： 1234567# # 切片范围小于添加元素的个数items = [1, 2, 3, 4, 5]items[2:3] = [7, 8, 9, 0]print(items)---[1, 2, 7, 8, 9, 0, 4, 5] 可以看到，比起原本的列表，我们的值增加了。原本下标[2]的元素被移除之后，在这个位置插入了[7,8,9,0]四个元素。 以此，我们可以总结切片更新列表，实际上就是删除掉切片范围内的元素，再在原来的位置上插入新加的元素，并且将之后的元素向后移动。 那既然这样的话，我们是不是可以利用这种特性对列表内的元素进行删除？ 123456items = [1, 2, 3, 4, 5]items[2:4] = []print(items)---[1, 2, 5] 没毛病，确实可以这么用。 当然，除了这种插入空列表的方式之外，还有其他方式可以删除列表内的指定元素, 还记得前面我们介绍的del方法吗？ 123456items = [1, 2, 3, 4, 5]del items[2:4]print(items)---[1, 2, 5] 那既然我们可以用添加空列表的方式来删除列表内的元素，del是不是就没用武之地了？实际上，并非如此。del有一个特殊的用法，就是在利用步进值来跳着删除元素： 123456items = [1, 2, 3, 4, 5]del items[0:6:2]print(items)---[2, 4] 那聪明的小伙伴肯定想，添加空列表的方式也加上步进值就不行吗？我们来试试： 123456items = [1, 2, 3, 4, 5]items[0:5:2] = []print(items)---ValueError: attempt to assign sequence of size 0 to extended slice of size 3 报错提示我们，序列分配不正确。说明我们不能这样使用。如果要这样使用的话，替换的元素个数必须对应才行： 123456items = [1, 2, 3, 4, 5]items[0:4:2] = [9, 10]print(items)---[9, 2, 10, 4, 5] 列表相关函数(✨ 重点) 除了以上介绍的关于列表的一些方法之外，Python还为我们提供了一些列表常用的相关函数： len() 这个函数可以检测当前列表的长度，列表中元素的个数： 12345items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']len(items)---9 count() 这个函数可以检测当前列表中指定元素出现的次数： 1234items.count('Python')---1 append() 这个函数前面我们已经介绍过了，就是向列表尾部追加新的元素，返回值为None。 12345items.append('SQL')print(items)---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', 'SQL'] insert() 这个函数可以向列表中指定的索引位置添加新的元素。 12345items.insert(4, 'Go')print(items)---['Python', 'Java', 'Ruby', 'Rust', 'Go', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', 'SQL'] pop() 还记得我们之前删除列表中元素的时候介绍pop()函数吗？其实，pop()函数是对指定索引位置上的元素做出栈操作，然后返回出栈的元素 123456789items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']print(items.pop())print(items.pop(2))print(items)---PHPRuby['Python', 'Java', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node'] 默认的情况下，pop()是把列表的最后一个元素出栈，当给值之后，是将指定索引的元素进行出栈。 remove() 这个函数是专门删除特定元素用的，可以指定列表中的元素进行删除，只删除第一个，如果没有找到，则会报错。 12345678items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.remove('PHP')print(items)items.remove('Go')---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node']ValueError: list.remove(x): x not in list 可以看到，第一个remove成功删除了PHP，但是第二个remove并未在列表中找到Go，所以报错。 index() 这个函数可以查找指定元素在列表中第一次出现的索引位置 12345items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.index('PHP')---8 除此之外，index()还能接收索引值，当输入索引值的时候，index()会在指定范围内查找元素的索引位置： 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']print(items.index('Ruby', 0, 5))items.index('PHP', 0, 5)---2ValueError: 'PHP' is not in list 可以看到，指定范围内没有说要查找的元素的时候就会报错，告知元素不在列表内。 extend() 这个函数接收一个容器类型的数据，把容器的元素追加到原列表中 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']a = ['Go', 'MATLAB']items.extend(a)print(items)---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', 'Go', 'MATLAB'] 看到这，是不是感觉很像两个列表相加？那既然我们可以将两个列表相加了，这个方法似乎有些多余了。 这么想的小伙伴们，我们再来看两段示例： 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']a = (1, 2, 3)items.extend(a)print(items)---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', 1, 2, 3] 另外一段： 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']a = (1, 2, 3)items = items + aprint(items)---TypeError: can only concatenate list (not &quot;tuple&quot;) to list 可以看到，第二段代码直接报错了。那说明，相加这个操作必须两个都是列表才可以，不支持列表和元组相加。可是extend()方法是支持将任意一个容器类型的数据中的元素追加到原列表中的。 我们再来多看一段： 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']a = '1234'items.extend(a)print(items)---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', '1', '2', '3', '4'] 将a定义为一段字符串，一样可以使用extend()来接收并追加到原列表内。 clear() 这个函数比较简单，就是清空列表内容 123456items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.clear()print(items)---[] reverse() 这个函数可以对列表进行翻转 123456items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.reverse()print(items)---['PHP', 'Node', 'JavaScript', 'Swift', 'C++', 'Rust', 'Ruby', 'Java', 'Python'] sort() 该函数将对列表进行排序, 在默认的情况下，会对元素进行从小到大的排序 123456items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.sort()print(items)---['C++', 'Java', 'JavaScript', 'Node', 'PHP', 'Python', 'Ruby', 'Rust', 'Swift'] 额，这样似乎并不明显，我们重新换个案例。不过大家也可以想想现在这段代码中，为什么会有这样的结果。 123456items = [9, 3, 5, 2, 1, 7, 8, 0, 6]items.sort()print(items)---[0, 1, 2, 3, 5, 6, 7, 8, 9] 嗯，这回明显了。 除了从小到大排序外，我们还可以将其从大到小排序，利用关键参数reverse来开启： 123456items = [9, 3, 5, 2, 1, 7, 8, 0, 6]items.sort(reverse=True)print(items)---[9, 8, 7, 6, 5, 3, 2, 1, 0] OK，现在让我们来回过头来解释一下第一段代码中的结果：['C++', 'Java', 'JavaScript', 'Node', 'PHP', 'Python', 'Ruby', 'Rust', 'Swift']， 之所以会产生这样的结果，不是因为它按英文字母来排序，当然这么想也对但是不全对，而是因为它的排序依据是ASCII码，之前我们学习过，ASC II码只包含了128个字符，仅仅是美国的标准，128个字符里面都是西文码，那么如果中间包含了中文会怎样呢？ 不如我们直接来看看： 123456items = [9, 3, 5, 2, 1, 7, 8, 0, '茶', '桁']items.sort(reverse=True)print(items)---TypeError: '&lt;' not supported between instances of 'int' and 'str' 完了，直接报错。不过这个似乎和编码无关，而是数据类型的问题，告诉我们字符和整型之间不能排序。别问我怎么看懂的，我也是查字典。 知道是数据类型的问题就好办了，我们将数据类型变成一致的再试试： 123456items = ['9', '3', '5', '2', '1', '7', '8', '0', '茶', '桁']items.sort(reverse=True)print(items)---['茶', '桁', '9', '8', '7', '5', '3', '2', '1', '0'] 居然成功了，那既然是ASCII码，那为什么还会支持中文排序呢？还记得我们除了介绍ASCII码之外，还介绍过一个Unicode编码。那即是说，Python中的sort()排序的依据是Unicode编码。 当然，除了默认规则之外，我们还可以自己对排序进行干预，加上你想要的规则。sort(key)内的key参数可以接收一个函数，按照函数的处理结果进行排序： 123456items = [-5, -3, 5, 2, 0, -9, 12, 14, -1, -6]items.sort(key=abs)print(items)---[0, -1, 2, -3, -5, 5, -6, -9, 12, 14] 这一段是不是让小伙伴们想到之前我们在Python的内置函数中介绍高阶函数的内容？没错，就是一样的。所以，我们这次就不对函数内部排序过程进行分析了，有兴趣的小伙伴可以回去看看第七节的内容。 深拷贝与浅拷贝 接着，让我们来看看关于拷贝的问题，先说浅拷贝。 说到浅拷贝，实际上是仅拷贝了列表中的一维元素，如果列表中存在二维元素或容器，则为引用而不是拷贝。使用copy函数或者copy模块中的copy函数拷贝的都是浅拷贝。 123456items = [1, 2, 3]res = items.copy()print(items, '\\t', res)---[1, 2, 3] [1, 2, 3] copy()之后的新列表和原列表内容上是一样的。 接着让我们来操作一下copy之后的res 123456789items = [1, 2, 3]res = items.copy()del res[2]print(items)print(res)---[1, 2, 3][1, 2] 可以看到，对res进行操作完全不影响原列表的内容。这就说明，copy产生的新列表和原列表并不是一个列表，我们可以验证一下看看： 123456print(id(items))print(id(res))---46363598724636086464 当我们用id()函数的时候，可以看到他们是两个完全不同的id 刚才我们定义的items是一个一维列表，接着让我们再来定义一个多维列表来尝试一下: 123456789items = [1, 2, 3, 4, ['a', 'b', 'c']]res = items.copy()del res[3]print(items)print(res)---[1, 2, 3, 4, ['a', 'b', 'c']][1, 2, 3, ['a', 'b', 'c']] 我们可以看到，做删除操作之后，res内容变了，但是原列表items没变化。似乎和之前的并没有什么不同，让我们再继续试试: 1234567del res[3][1]print(res)print(items)---[1, 2, 3, ['a', 'c']][1, 2, 3, 4, ['a', 'c']] 发生了什么？我们明明是操作的res而不是原列表items， 为什么items也发生了变化？难道是id是相同的吗？来，试试就知道了。 123456print(id(items))print(id(res))---46364272644636085824 似乎并不相同。那既然不是同一个元素，为什么我们操作res的时候，items也会跟着一起变化？ 别着急，让我们接着看下面的操作: 123456print(id(items[4])) # items这个位置是列表['a', 'c']print(id(res[3])) # res这个位置是列表['a', 'c']---46352459524635245952 如何，一模一样对吧？这就说明，在items以及它的copy列表res中，这个嵌套的列表是同一份。这也就能解释为什么我们对res内的嵌套列表进行操作的时候, items也发生了变化。 这个就是我们在一开始说到的，copy仅仅是拷贝了列表中的一维元素，对二维元素和容器仅仅是引用，这个应用对象当然还是原来那个对象。所以，两者的id才是是同一个。 浅拷贝我们理解完之后，才看看什么是深拷贝。 深拷贝和浅拷贝比起来就有深度的多，嗯，这么讲是因为深拷贝不仅仅是拷贝了当前的列表，同时还把列表中的多维元素或容易也拷贝了一份，而不是像浅拷贝一样仅仅是引用。完成深拷贝的函数是copy模块中的deepcopy函数。 12345items = [1, 2, 3, ['a', 'b', 'c']]res = items.deepcopy()---AttributeError: 'list' object has no attribute 'deepcopy' 额，尴尬。居然报错了... 似乎deepcopy并不是和copy函数一样的用法。 细心的小伙伴应该之前就注意到了，在介绍copy函数和deepcopy函数的时候，我都在强调是copy模块中的这句话，确实，我们在使用deepcopy的时候，是需要先引用模块再使用的，并且，使用方式也有一些不同: 1234567import copyitems = [1, 2, 3, ['a', 'b', 'c']]res = copy.deepcopy(items)print(res)---[1, 2, 3, ['a', 'b', 'c']] 没错，我们这就对items完成了深拷贝，生成了新的列表res。 那到底是否是真的深拷贝呢？让我们试一试： 1234567891011print(id(items))print(id(res))print(id(items[3]))print(id(res[3]))---4636282048463479987246362851204637491072 没问题，我们打印出来的id各不一样，包括items内的二维列表以及res内的二维列表，id也都不同，说明确实是深拷贝。 不放心的小伙伴，我们再来更改列表元素测试一下: 12345678del res[3][0]print(res[3])print(items[3])---['b', 'c']['a', 'b', 'c'] 可以看到，当我们更改res内的二维列表时，items并未发生改变。说明二维列表我们也一样完成了拷贝，而不是像浅拷贝一样仅是引用了。 列表推导式 在本文最开始，我们介绍列表的时候提过三种列表生成方式，包括直接定义列表, 用list函数，最后一个就是列表推导式。那我们接下来，就要详细讲讲列表推导式。 列表推导式提供了一个更简单的创建列表的方法，常见的用法是把某种操作应用于序列或可迭代的每个元素上，然后使用其结果来创建列表，或者通过满足某些特定条件元素来创建子序列。 采用一种表达式的当时，对数据进行过滤或处理，并且把结果组成一个新的列表。 哎，最怕就是定义和文字过多，让我们直接上示例吧。 基本的列表推导式使用方式 结果变量 = [变量或变量的处理结果 for 变量 in 容器类型数据] 现在，假设我们想要创建一个平方列表： 123456789# 使用普通方法完成items = []for i in range(10): items.append(i**2)print(items)---[0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 这是用我们所学过的内容来进行创建，当然，我们还学过另外一种方式: 123456# 使用 map函数和list完成items = list(map(lambda x: x**2, range(10)))print(items)---[0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 这里似乎有一点复杂，咱们还是来分析一下吧。 首先，我们创建了一个匿名函数lambda x:x**2, 再创建了一个可迭代对象range(10)。 接着，我们给map函数传入了这两个参数，分别传给了func和*iterables, 关于map函数，我们在第七节：内置函数中有讲解过，完了的小伙伴可以翻看前面复习一下。 map函数在对传入的可迭代数据中的每一个元素进行处理，然后返回一个新的迭代器, 最后用list函数将这个新的迭代器转换成了一个列表。 然后，我们将传入的func函数用一个匿名函数 没错，我们使用map函数和list也可以进行实现。 那么最后，让我们来看看列表推导式如何完成这个需求： 123456# 列表推导式items = [i**2 for i in range(10)]print(items)---[0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 简简单单一句话，比用map函数更简单的逻辑，就完成了我们的需求。这一句话的代码其实逻辑桁清晰，也很好理解： 首先，我们用for来进行循环传值给i， 接着，我们用i**2来得到我们期望的值，最后生成列表。本质上，其实和我们用的第一种普通方法是一样的。 接着我们再来看一个， 我们现在有一个字符串'1234'， 想要得到这样一个列表[2, 4, 6, 8]。照例，从普通方法开始： 12345678910# 普通方法str = '1234'items = []for i in str: items.append(int(i)*2)print(items)---[2, 4, 6, 8] OK，没问题。我们继续： 123456789items.clear()print(items)items = list(map(lambda x:int(x)*2, str))print(items)---[][2, 4, 6, 8] 可以看到，我们先将items清空之后再继续操作的，这次我们用了list+map的方式，一样得到了我们想要的结果。 最后，当然是用列表推导式的方式： 123456789items.clear()print(items)items = [int(i)*2 for i in str]print(items)---[][2, 4, 6, 8] 同样，我们得到了想要的结果。 讲到这里了，我给大家秀一个小技巧，俗称骚操作，就是我们其实可以运用位运算操作符： 123456789items.clear()print(items)items = [int(i) &lt;&lt; 1 for i in str]print(items)---[][2, 4, 6, 8] 具体代码执行的时候发生了什么，就算是给大家留个小思考题。提示：可以回头翻看下我们之前讲到的位运算符。 带有判断条件的列表推导式 除了基本的列表推导式，我们还有一种带有判断条件的列表推导式。 结果变量 = [变量或变量的处理结果 for 变量 in 容器类型数据 条件表达式] 相比起基本的列表推导式，我们现在多了一个条件表达式，那么我们该怎么利用呢？来个需求：从0 ~ 9，求所有的偶数并且形成一个新的列表。这回，我们就只完成常规方法和列表推导式，对比着来观察一下： 12345678910# 常规方式items = []for i in range(10): if i % 2 == 0: items.append(i)print(items)---[0, 2, 4, 6, 8] 很好，我们完成了需求。接下来，大家试试不看我下面写的代码，自己从常规方式思考下该怎么写，然后自己运行一下试试写对了没，最后，再和我写的对比一下看看咱们写的有没有区别。 12345items = [i for i in range(10) if i % 2 == 0]print(items)---[0, 2, 4, 6, 8] 没错，就是这么简单，你做对了吗？ 带有条件判断的多循环推导式 现在有这样一个需求，我们拿到两个列表[1,2,3], [3,1,4], 要将这两个列表中的元素两两组合，要求组合的元素不能重复： 12345678910# 常规方法items = []for x in [1, 2, 3]: for y in [3, 1, 4]: if x != y: items.append((x,y))print(items)---[(1, 3), (1, 4), (2, 3), (2, 1), (2, 4), (3, 1), (3, 4)] 这样，我们就完成了刚才的需求。那用推导式该如何实现呢？ 12345items = [(x, y) for x in [1, 2, 3] for y in [3, 1, 4] if x != y]print(items)---[(1, 3), (1, 4), (2, 3), (2, 1), (2, 4), (3, 1), (3, 4)] 没毛病，我们完全实现了刚才的需求。这个很好理解对吧？ 让我们接着来看最后一个推导式的形式。 对于嵌套循环的列表推导式 这次我们直接写需求，然后上示例。 需求为，现在我们有一个3x4的矩阵，由3个长度为4的列表组成，我们现在要交换其行和列。 注意哦，这个行转列需求在处理数据的时候经常会用到。 1234567891011121314151617# 需求样例'''[ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]==&gt;[ [1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]''' 来，让我们尝试着实现一下： 1234567891011121314151617181920212223242526# 首先，定义初始数据，大家可以直接copy我给到的矩阵数据# 先定义数据arr = [ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]# 使用常规方法items = []for i in range(4): res = [] for row in arr: res.append(row[i]) items.append(res)print(items)# 使用列表推导式, 我们从内向外来写items = [[row[i] for row in arr] for i in range(4)]print(items)---[[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]][[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]] 这样，我们就完成了刚才的需求。我们拆解呢，还是从外层开始讲起： 首先，因为我们发现数据是4列，所以我们设定了一个range(4)来进行4次迭代，将0,1,2,3这四个下标分别传到内层循环。 然后我们开始在arr内循环找到当前的row, 循环会依次去寻找[1,2,3,4]，[5,6,7,8],[9,10,11,12]。然后将每一个row中的寻找当前的row[i]，并且填入一个新列表内。那么这三组列表中的row[i]就会是这样的： row[1]分别为1, 5, 9, row[2]分别为2, 6, 10.... 依次类推。当外层循环完成之后，就正好是组成了新的四个新的列表，最后再将新列表依次传到items这个空列表内，就完成了。 那同样都是两次for循环嵌套，为什么上面那个案例就是顺序写的，内层for循环写在了后面，而下面这个案例的内层for循环就写到了前面呢？ 好的，让我们来看看，如果将下面这个案例的内存for循环写在后面会是怎样的: 12345items = [row[i] for i in range(4) for row in arr]items---[1, 5, 9, 2, 6, 10, 3, 7, 11, 4, 8, 12] 看到了吗？顺序还是对的，只是依次传入了数据，并未形成矩阵。那有小伙伴就说了，那是不是因为没在row[i]上加[]从而形成列表呢？ 好的，让我们再来做一个实验： 12345items = [[row[i]] for i in range(4) for row in arr]items---[[1], [5], [9], [2], [6], [10], [3], [7], [11], [4], [8], [12]] 可以看到，列表是形成了，但是却是一个元素占一个列表，并没形成我们想要的矩阵。 估计小伙伴们看出来了，在推导式中，因为变量或变量的处理结果必须放在前面，所以我们为了要形成矩阵内层新的row，所以才必须将处理结果和内层循环方法放在一起，并加上[]来确保这组结果能形成一个列表, 也就是我们现在这样： 12345items = [[row[i] for row in arr] for i in range(4)]items---[[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]] 看一次没看懂的小伙伴，可以多看看，尝试着自己去分解，将其理解透彻。因为这个方法在我们后续的数据清洗中使用非常频繁。 小练习 为了能达到练习的目的，从这一节开始，所有练习可以不在课程中展示了。大家先做一下，然后可以在我下一节课中的源码中去找答案，然后来看看和自己做的是否一样。 以下所有练习必须使用列表推导式来实现 有些练习不止一个方法，大家尝试用多种方法来实现一下。 做完的小伙伴可以在课程后面留言讨论。 12345678910111213141516171819202122232425# 1. 让我们尝试将字典中的健值对转成`key = value`的数据格式{'user':'admin', 'age':'20', 'phone':'133'} 转为 ['user=admin','age=20','phone=133']# 2. 把列表中的所有字符全部转为小写['A', 'CCCC', 'SHIss', 'Sipoa','Chaheng', 'Python','dsAhio']# 3. x是0～5之间的偶数，y是0~5之间的奇数，把x，y组成一个元组，放到列表中# 4. 使用列表推导式完成九九乘法表# 5. 求M, N中矩阵和元素的乘积'''M = [ [1, 2, 3], [4, 5, 6], [7, 8, 9]]N = [ [2, 2, 2], [3, 3, 3], [4, 4, 4]]''' 最后，大家记得做练习并且留言，下课。","link":"/Detailed-of-list/"},{"title":"10. 数据类型 - 元组详解","text":"Hi，大家好。我是茶桁。 之前两节分别介绍了字符串和列表，今天，我们来讲讲另外一个常用到的数据类型：元组。 元组和列表很像，两者都是一组有序的数据的组合。但是也有很多不同点，比如元组内的元素一旦定义了就不可以再修改，因此元组称为不可变数据类型。 元组定义 元组的定义方式包括以下要点： 定义元组变量 = (), 或者变量 = tuple() 可以使用变量 = (*iterable)定义含有数据的元组 ⚠️ 需要注意：如果元组中只有一个元素时，这唯一的元素后面也必须加逗号，这是为了区分其他元素标识这是一个元组: (1,) 特例： 变量 = 1,2,3， 这种方式也可以定义为一个元组。 元组的相关操作 由于元组是一个不可变的数据类型，因此其在创建之后只能使用索引进行访问，无法进行其他操作。访问方式其实和列表一样，同样可以使用切片方式获取元素。 元组可以进行切片操作，在访问数据这件事情上和列表几乎一样，没有什么区别，所以完全可以借鉴上一节我讲的内容来看，这里就不详细介绍了，仅仅给大家写出一些案例： 1234567891011121314151617181920# 常见的元组切片索引查询操作tup = 1, 2, 3, 4, 5, 5, 4, 3, 2, 1print('[:]:\\t',tup[:]) # 获取全部print('[::]:\\t', tup[::]) # 获取全部print('[1:]:\\t', tup[1:]) # 从索引1开始获取到最后print('[1:3]:\\t', tup[1:3]) # 从索引1开始索引到3之前print('[:3]:\\t', tup[:3]) # 从0开始索引到3之前print('[1:5:2]:\\t', tup[1:5:2]) # 从1开始索引到5之前，步进值为2print('[::2]:\\t', tup[::2]) #从0开始索引到最后，步进值为2print('[5:1:-1]:\\t', tup[5:1:-1]) # 从5开始往前索引到1， 步进值为-1。---[:]: (1, 2, 3, 4, 5, 5, 4, 3, 2, 1)[::]: (1, 2, 3, 4, 5, 5, 4, 3, 2, 1)[1:]: (2, 3, 4, 5, 5, 4, 3, 2, 1)[1:3]: (2, 3)[:3]: (1, 2, 3)[1:5:2]: (2, 4)[::2]: (1, 3, 5, 4, 2)[5:1:-1]: (5, 5, 4, 3) 除了常用的切片操作之外，和列表一样，元组也能使用一些基本函数来完成查询操作 12345678910111213# 获取元组的长度print(len(tup))# 统计一个元素在元组中出现的次数print(tup.count(5))# 获取一个元素在元组内的下标（索引值）print(tup.index(5, 1, 9))---1024 除此之外，元组还可以引用基础的数学运算符+和*来进行加和乘的运算。 1234567# 加和乘操作print((1, 2, 3) + ('a', 'b'))print((1, 2, 3) * 5)---(1, 2, 3, 'a', 'b')(1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3) 有一个同学曾经问过我：既然元组是不可修改的，那为什么还能用加和乘的运算呢？ 不知道在座的小伙伴有没有这种想法？ 这样吧，我重新写一段代码，小伙伴们应该就明白了: 123456789101112# 元组是这样的tup = (1, 2, 3, 4)print(id(tup))tup2 = (3, 4, 5, 6)tup = tup + tup2print(id(tup))---4446580864445062912044506479844459297216 不知道大家看明白没有。解释一下，其实就是说，在进行加法和乘法运算的时候，即便我们的变量名是一样的，实际上也是生成了一个新的元组，而不是之前那一个了。所以这个并非是修改和更新，而是创建。 为了对比，我再写一段更新的代码给大家看： 12345678# 尝试更新元组tup = (1, 2, 3, 4, 5, 6)print(tup[2])del tup[2]print(tup)---TypeError: 'tuple' object doesn't support item deletion 可以看到，报错提示了，tuple对象不支持删除项目 作为对比，我们看看列表的： 123456789# 看看列表更新（只看id）items = [1, 2, 3, 4, 5, 6]print(id(items))del items[2]print(id(items), '\\t',items)---44504159364450415936 [1, 2, 4, 5, 6] 可以看到，不仅是内部元素被删除了，并且id完全没有变化。也就是说，我们是在这个列表本身做了删除动作，并未生成新的列表。关于这部分，我们上一节中的深拷贝和浅拷贝讲的很清楚，大家可以回去好好看看理解一下。 元组推导式 生成器 在起初，我们先来看看元组是否和列表一样支持使用推导式。 12345tup = (i for i in range(10))print(tup)---&lt;generator object &lt;genexpr&gt; at 0x109cfa570&gt; 这段并非是报错，而是打印出了tup的类型：生成器对象。 我们之前学过，使用列表推导式生成的结果是一个列表，但是元组似乎和列表并不一样，生成的结果是一个生成器对象。 12列表推导式 ==&gt; [变量运算 for i in 容器] ==&gt; 结果 是一个 列表元组推导式 ==&gt; (变量运算 for i in 容器) ==&gt; 结果 是一个 生成器 那这里就有个疑问了，什么是生成器？ 生成器是一个特殊的迭代器，生成器可以自定义，也可以使用元组推导式去定义。 生成器是按照某种算法去推算下一个数据或结果，只需要往内存中存储一个生成器，节约内存消耗，提升性能。 语法 里面是推导式，外面是一个()的结果就是一个生成器 自定义生成器，含有yield关键字的函数就是生成器 那么，我们到底应该怎样操作生成器呢？ 既然生成器是迭代器的一种，那我们是否可以使用迭代器的操作方法来操作生成器呢？ 说干就干，让我们直接操作做实验： 1234567891011tup = (i for i in range(10))print(next(tup))print(next(tup))print(next(tup))print(list(tup))---012[3, 4, 5, 6, 7, 8, 9] 没毛病，确实支持next()函数，并且内部元素在使用后也被移除了。 12345# 让我们将其转为元组print(tuple(tup))---() 哎，为什么里面是空的？那是因为，我们上一段代码中的最后一句，已经讲所有迭代器内的元素转为了列表，素衣目前迭代器tup内是没有任何元素了，所以我们转过来必须是空的。 再来生成一个，我们来试试用for对它进行循环： 123456tup = (i for i in range(10))for i in tup: print(i, end=&quot; &quot;) ---0 1 2 3 4 5 6 7 8 9 可以看到结果没有问题。可以推断出，生成器和迭代器没有任何区别，我们在平时使用的时候，就直接将它作为迭代器使用就可以了。 yield关键字 在之前，我们提到了，含有yield关键字的函数就是生成器。 它返回的结果是一个迭代器。我们可以理解为，生成器函数就是一个返回迭代器的函数。 那么yield有哪些需要注意的点呢？我们先在下面列一下，之后再带着大家一起过： yield和函数中的return有点像 共同点： 执行到这个关键字后会把结果返回来 不同点： return会把结果返回，并结束当前函数的调用 yield会返回结果，并记住当前代码执行的位置，下一次调用时会从上一次离开的位置继续向下执行。 上实验： 123456789101112# 定义一个普通函数def func(): print('Hello yield') return 'yield' print('Hello again')func()func()---Hello yieldHello yield 在这个自定义函数内，return执行的时候，就会结束当前函数的调用，而在之前，第一个print()函数正确执行了，但是第二个print()函数因为在return之后，所以并未运行。即便我们一共执行了两次函数，可是也仅仅是讲第一个print()函数执行了两次。 123456789101112131415# 尝试使用yield定义一个生成器函数def func(): print('Hello yield') yield 'yield' print('Hello again') yield 'again'# 调用生成器函数， 返回一个迭代器res = func()next(res)next(res)---Hello yieldHello again 可以看到，当我们使用next()函数的时候，迭代器起作用了，每执行一次，分别调用第一个yield之前和之后的print()，也就是说继续执行了。 那如何验证yield的返回呢？我们将这段代码改造一下： 12345678910111213141516171819# 尝试使用yield定义一个生成器函数def func(): print('Hello yield') yield 'return yield' print('Hello again') yield 'return again'# 调用生成器函数， 返回一个迭代器res = func()str = next(res)print(str)str = next(res)print(str)---Hello yieldreturn yieldHello againreturn again 没问题，依次打印出了返回值return yield和return again。 还记得我们之前教过，使用list函数去调用，可以讲迭代器的返回结果，作为容器的元素，让我们再来改造一下这段代码： 1234567891011121314151617# 尝试使用yield定义一个生成器函数def func(): print('Hello yield') yield 'return yield' print('Hello again') yield 'return again'# 调用生成器函数， 返回一个迭代器res = func()items = list(res)print(items)---Hello yieldHello again['return yield', 'return again'] 我们看见，确实，返回结果被依次放入了一个list容器中。 当然，除了list函数之外，还可以使用for来获取迭代器内容： 1234567891011121314151617181920# 尝试使用yield定义一个生成器函数def func(): print('Hello yield') yield 'return yield' print('Hello again') yield 'return again'# 调用生成器函数， 返回一个迭代器res = func()items = []for i in res: items.append(i)print(items)---Hello yieldHello again['return yield', 'return again'] 我们来分析一下在以上这几段代码中，生成器函数调用时到底是什么过程。 首先，调用生成器函数，返回一个迭代器： 第一次去调用迭代器，走到当前的生成器函数中，遇到第一个yield, 把return yield返回，并且记住当前的执行状态（位置），暂停了执行，等待下一次的调用 第二次去调用迭代器，从上一次遇到的yield位置开始执行，遇到了第二个yield，把return again返回，并重新记录状态，暂停执行，等待下一次调用。 如果最后又调用了迭代器，那么会从上一次的yield位置开始，可是后面没有了，就会超出范围，抛出异常：StopIteration:。 那么这种一次一次调用执行的方式什么时候适用呢？比如说，我们在处理一个非常大的数据，电脑可能吃不住，这个时候我们就可以拆开来一次一次的执行获取结果。 小练习 为了能达到练习的目的，从这一节开始，所有练习可以不在课程中展示了。大家先做一下，然后可以在我下一节课中的源码中去找答案，然后来看看和自己做的是否一样。 以下所有练习必须使用列表推导式来实现 有些练习不止一个方法，大家尝试用多种方法来实现一下。 做完的小伙伴可以在课程后面留言讨论。 上一节的练习已经放到本次教程的源码内，可以在此获取：https://github.com/hivandu/AI_Cheats/tree/main/Python 1今天就一个练习：使用生成器改写斐波那契数列函数","link":"/Detailed-of-tuple/"},{"title":"11. 数据类型 - 字典","text":"Hi，大家好。我是茶桁。 关于Python的数据类型，我们已经详细讲解了三种，字符串，列表和元组。那么今天，我们再来讲一种：字典。 字典也是一种数据的集合，由健值对组成的数据集合，字典中的键是不能重复的。 字典中的键必须是不可变的数据类型，常用的键主要是：字符串，整型... 实际上，在之前字符串和列表的铺垫之后，任何数据类型其实都会感觉差不多，当然，每个数据类型也都有自己的特点以及需要注意的地方，不过在方法，操作上也会有很多类同点。 那么，让我们开始学习字典吧。 字典的定义 字典可以通过把以逗号分隔的key:value对列表包含于花括号之内来创建字典。 也可以通过dict构造器来创建 {'jack': 666, 'stored': 777} 或者{666:'jack', 777:'stored'} 让我们开始写代码来做实验： 使用{}定义: 12345myDict = {'a':1, 'b':2, 'c':2}print(myDict)---{'a': 1, 'b': 2, 'c': 2} 使用dict(key=value, key=value)函数进行定义 12345myDict = dict(name='张三', sex='male', age=22)print(myDict)---{'name': '张三', 'sex': 'male', 'age': 22} 数据类型的转换：dict(二级容器类型) 列表或元组，并且只有二级容器才可以转换 12345myDict = dict([['a',1], ['b',2], ['c',3]])print(myDict)---{'a': 1, 'b': 2, 'c': 3} 让我们来试试如果不是二级容器类型会如何： 12345myDict = dict(['a',1], ['b',2], ['c',3])print(myDict)---TypeError: dict expected at most 1 argument, got 3 报错了，提示我们字典最多一个参数，但是现在里面有3个。 再继续试试其他情况： 12345myDict = dict([[['a',1],['b',2],['c',3]]])print(myDict)---ValueError: dictionary update sequence element #0 has length 3; 2 is required 再次抛出异常，提示字典更新序列元素长度为3，第2位是必填项。 以上可以看出，只有二级容器才能通过dict()函数来做数据类型的转换。 zip压缩函数，dict转类型 123456789ex1 = [1, 2, 3, 4]ex2 = ['a', 'b', 'c', 'd']# 压缩过后做的事情其实就是数据类型的转换myDict = dict(zip(ex1, ex2))print(myDict)---{1: 'a', 2: 'b', 3: 'c', 4: 'd'} 字典的操作 还记得吗，无论是列表还是元组，都支持数学的基本运算符+和*。那字典是不是也同样支持？ 123456ex1 = {'a':1, 'b':2, 'c':3}ex2 = {1:'a', 2:'b', 3:'c', 4:'d'}print(ex1 + ex2)---TypeError: unsupported operand type(s) for +: 'dict' and 'dict' 提示类型错误，*实际上也是一样，这里我们就不占用篇幅再多打印一次错误了。说明，字典并不支持这两个基本的数学运算符。想想我们之前提到的dict中key不能重复其实也就好理解了。如果支持+， 那相加的两个字典内key值如果相同，那到底舍去那一个呢？*法就更容易理解，原本*就是将相同的数据重复乘n份，不支持也就理所应当了。 那么，字典到底支持哪些操作呢？我们接着往下看实验： 首先，让我们尝试获取一下元素，既然字典是key:value形式的，那要想拿到value值，必然是使用key来获取： 12345res = ex1['a']print(res)---1 拿到元素了，那如果我们是要修改元素呢？直接赋值试试： 12345ex1['a'] = 111print(ex1)---{'a': 111, 'b': 2, 'c': 3} 看来是有效的，增删改查，我们现在来试试删除： 12345del ex1['a']print(ex1)---{'b': 2, 'c': 3} 也没毛病。 接下来，当然就是添加元素了： 12345ex1['aa'] = 'aaaaa'print(ex1)---{'b': 2, 'c': 3, 'aa': 'aaaaa'} 之前我们反复说过字典的一个特点，就是字典不能有重复的key，这也是我们无法使用+和*操作字典的原因。那么问题来了，如果我在添加元素的时候key重复了怎么办？ 什么怎么办，添加key重复了，那不就变成修改元素了吗？^_^ 检测和获取 增删改查我们前三个基本都已经讲完了，那剩下的，就是查了。让我们看看如何检测和获取元素。 成员检测，只能检测key， 无法检测value。是否注意到我们之前一直使用的一句代码for i in range(10), 大家应该都能明白这一句代码是做什么吧？其实，我们坚持是否包含的时候，就可以用in来实现： 123456print('AA' in ex1)print('AA' not in ex1)---FalseTrue 获取当前字典的长度，只能检测当前有多少个健值对： 1234print(len(ex1))---3 我们还可以获取当前字典中的所有key键： 1234print(ex1.keys())---dict_keys(['b', 'c', 'aa']) 当然，不只是key。实际上，字典中所有的value值，我们一样可以获取到： 1234print(ex1.values())---dict_values([2, 3, 'aaaaa']) 最后，让我们尝试把key和value一起获取到： 1234print(ex1.items())---dict_items([('b', 2), ('c', 3), ('aa', 'aaaaa')]) 字典的遍历 当我们谈到对字典的遍历时，实际上和检测、获取时一样的。只是写进了遍历循环里而已，让我们来看看吧： 在我们遍历当前字典时，只能获取当前的key, 但是我们可以通过获取到的key来完成获取当前key的value: 12345for i in ex1: print(i, ':', ex1[i], end=&quot;; &quot;) ---b : 2; c : 3; aa : aaaaa; 这种获取方式就显得略微繁琐一点，既然我们之前有提到一个将key和value一起获取到的函数方法，那我们在for里一样可以使用它来将key和value一起获取到，只是，我们需要用到两个参数来接收： 12345for k, v in ex1.items(): print(k, ':', v, end=&quot;; &quot;) ---b : 2; c : 3; aa : aaaaa; 既然之前介绍的获取上我们可以单独获取key和value， 当然这里也通通能用： 12345678910111213# 遍历所有的keyfor k in ex1.keys(): print(k, end=&quot;; &quot;)print()# 遍历所有的valuefor v in ex1.values(): print(v, end=&quot;; &quot;)---b; c; aa; 2; 3; aaaaa; 字典的相关函数 和列表、元组一样，字典也有一些相关函数。有些嘛，一看到就很熟悉，在其他地方也能用，可是也有一些事字典专用的。 len(dict): 获取字典的健值对个数 dict.keys() 获取当前字典的所有key键，组成的列表 dict.values() 获取当前字典的所有value值，组成的列表 dict.items()返回由字典项（（键，值）对）组成一个新视图 iter(dict)返回以字典的键为元素的迭代器。 1234567res = iter(ex1)print(next(res))print(list(res))---b['c', 'aa'] 接下来，让我们重新定义一个新的字典来继续下面的函数学习： 1myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5} dict.pop(key) 通过key从当前字典中弹出健值对，删除。 123456myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}myDict.pop('a')print(myDict)---{'b': 2, 'c': 3, 'd': 4, 'e': 5} 这里我们需要注意一个点，就是pop()这个函数其实是有返回值的，会返回当前删除的健值对的value, 我们拿一个变量来接收一下返回值看看： 123456myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}res = myDict.pop('a')print(res)---1 可以看到，res接收到了pop()方法的返回值1 dict.popitem(): 后进先出（LIFO）的方式删除健值对，我们这里需要理解一下什么叫后进先出，就是最后一个加入字典的元素，先出来。 123456myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}myDict.popitem()print(myDict)---{'a': 1, 'b': 2, 'c': 3, 'd': 4} 和pop方法一样，popitem方法也会有一个返回值，不过是返回一个元组。 12345res = myDict.popitem()print(res)---('e', 5) 上面我们在讲获取的时候提到，可以直接使用key来获取元素的value， 不过如果字典内如果没有这个key的话，程序会报错。除了使用key来直接获取，字典里还有一个get()方法可以用来获取一个元素，用get获取元素存在就返回，不存在也不回报错，而是回返回None 12345678910111213myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}print(myDict.keys('f'))---TypeError: dict.keys() takes no arguments (1 given)============# get方法获取myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}print(myDict.get('f'))---None 字典的update方法可以更新对字典进行更新，如果这个key存在的话，就是更新。如果key不存在，则会进行添加。update可是使用key = value的形式更新，也可以直接获取一个新字典进行更新。 1234567myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}myDict.update(a=11, b=22)myDict.update({'c':33, 'f':66})print(myDict)---{'a': 11, 'b': 22, 'c': 33, 'd': 4, 'e': 5, 'f': 66} 实际上可以这么理解，update方法在获取其他字典更新原字典就有点像使用数学运算符的+, 区别只是，update是强制把最终确定值定为 + 号后方的值。 字典中还有一个方法setdefault(), 完整的写法为:dict.setdefault(key[, default])这个方法会去字典中找寻存在的key，并且会返回它的值。如果这个key不存在，这会插入一个值为default的key， 并且返回default: 1234567891011myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}res = myDict.setdefault('aa', '123')print(res)res = myDict.setdefault('a', 2)print(res)print(myDict)---1231{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'aa': '123'} 需要注意的是，如果这个key在字典中本来就存在，则并不会修改原本key的值，即便你在后面设定了一个default。并且返回的也会是字典内原本的value。也就是说，这个方法只能用来查询和新增。 字典推导式 和之前介绍的数据类型一样，字典也可以使用推导式来实现一些功能。比如： 字典中的健值对位置进行交换，先用普通的方法实现： 12345678910myDict = {'a':1, 'b':2, 'c':3}newDict = {}for k, v in myDict.items(): newDict[v] = kprint(newDict)---{1: 'a', 2: 'b', 3: 'c'} 然后再让我们看看字典推导式如何完成： 123456myDict = {'a':1, 'b':2, 'c':3}newDict = {v:k for k, v in myDict.items()}print(newDict)---{1: 'a', 2: 'b', 3: 'c'} 有的小伙伴可能会在推导式前方只写了一个变量来进行接收，那会变成什么样呢？我们来看看： 123456myDict = {'a':1, 'b':2, 'c':3}newDict = {v for k, v in myDict.items()}print(newDict, type(newDict))---{1, 2, 3} &lt;class 'set'&gt; 可以看到，最终打印的字典似乎看起来怪怪的，不是key:value的对形式，而是只有一个值。我们type一下能看到，类型并非是字典，而是set， 也就是说这是一个集合。 来让我们再看一个案例，让我们把一个字典中的value值有偶数的对保留下来，并且交换健值对的位置，一样的，让我们先用普通方式做一遍： 12345678myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':6}# 使用普通方式完成newDict = {}for k,v in myDict.items(): if v%2 == 0: newDict[v] = kprint(newDict) 再让我们使用字典推导式来完成 12345newDict = {v:k for k,v in myDict.items() if v%2 == 0}print(newDict)---{2: 'b', 4: 'd', 6: 'f'} OK，关于字典的东西基本上也就这么多。在前面学习过字符串和列表之后，是不是其他的容器类数据就没那么难了？很多东西都是普遍适用的，所以我们要活学活用，多思考。 那今天就不留练习题了，咱们下节课是数据类型最后一节了，之后我们开始讲解具体实际应用。字符串和容器类数据是Python中的基础也是重点，大家一定要好好的巩固。 下一节：集合。咱们下节课再见。","link":"/Detailed-of-dictonary/"},{"title":"12. 数据类型 - 集合详解","text":"Hi， 大家好。我是茶桁 通过最近几节课的内容，我们已经了解到了大部分的容器类数据的特性和应用，今天这一节课是容器类数据的最后一部分。让我们今天来详细了解一下「集合」。 集合是确定的一组无序的数据的组合。注意这一句话中的几个概念： 首先是「确定的」，当前集合中的元素的值是不能重复的。 集合是由多个数据组合的容器类型数据 集合中的数据没有先后顺序 集合的作用大多数时候是为了从成员检测、从无序列中去除重复项。还有就是数学中的集合类计算，例如交集、并集、差集一集对称差集等等。 集合的定义 集合的定义和字典类数据的定义非常像，包含了三种定义方式： 可以直接使用{}来定义集合 可以使用set()进行集合的定义和转换 使用集合推导式来完成集合的定义 ⚠️ 需要注意：集合中的元素不能重复，集合中存放的数据为：Number, String, Tuple，冰冻集合 冰冻集合 在集合的定义部分，其他数据类型我们都能理解，唯独多出来一个冰冻集合似乎没有见过，也难以理解。 冰冻集合的定义，需要且仅能使用frozenset()函数来进行定义。故名思义，冰冻集合一旦定义之后，是不能进行修改的，只能做一些集合相关的运算，比如交集，差集等等。 回过头来看冰冻集合的定义函数frozenset()， 这个函数本身是一个强制转换类的函数，可以把其他任何容器类型的数据转为冰冻集合，然后参与集合运算。 123456789# 定义一个冰冻集合mySets = frozenset({'love', 666, 333, 2, 'a', 1, 2, 'MAMT','55IW'})# 遍历集合for i in mySets: print(i, end=&quot;, &quot;)---1, 2, 55IW, love, 333, MAMT, 666, a, 也是可以看到，打印的结果完全没有任何顺序。 冰冻集合当然也可以使用集合推导式： 12345res = frozenset({i&lt;&lt;1 for i in range(6)})print(res)---frozenset({0, 2, 4, 6, 8, 10}) 可以进行拷贝： 12345res = res.copy()print(res)---frozenset({0, 2, 4, 6, 8, 10}) 当然冰冻集合也可以进行集合的运算，不过这部分我们将在后面讲解集合的时候来学习。暂时我们只是对「冰冻集合」的概念有个了解就可以了。 集合的基本操作和常规函数 以往的几节，我们都是将集合的操作和函数分开来讲，而这次我们放在一起讲。其实也没其他原因，就是因为这部分的内容并没有多少，并且很容易理解。 123456# 定义集合mySets = {123,'abc',False,'love',True,(1,2,3),0,3.1415,'123',1}mySets---{(1, 2, 3), 123, '123', 3.1415, False, True, 'abc', 'love'} 打印的结果再一次验证了集合无序，除此之外，我们可以看到打印出来的集合比我们进行定义的时候似乎少了,这又是为什么呢？ 原来，在集合内布尔类型的数据其实就是0和1， True表示为1， False表示为0，而集合内的值是不能重复的，所以，布尔值和0,1就只能存在一个。 我们来尝试检测一下集合中的值，和其他容器类数据一样，我们可以直接使用for...in： 1234567# 检测集合中的值print('123' in mySets)print('123' not in mySets)---TrueFalse 然后一样，可以使用len()检测长度： 1234print(len(mySets))---8 遍历的方法依然是用for 123456789101112for i in mySets: print(i, type(i)) ---False &lt;class 'bool'&gt;True &lt;class 'bool'&gt;3.1415 &lt;class 'float'&gt;(1, 2, 3) &lt;class 'tuple'&gt;love &lt;class 'str'&gt;abc &lt;class 'str'&gt;123 &lt;class 'int'&gt;123 &lt;class 'str'&gt; 为什么我这次会将数据类型也打印出来呢？是因为我想让大家好好记住这些类型，目前集合就只支持这些数据类型，其他的并不支持放入。比如说列表，是无法进入集合内的： 12345# 看看列表是否能放入mySets = {123,'abc',False,'love',True,(1,2,3),0,3.1415,'123',1,['list', 2, 3, 4, 5]}---TypeError: unhashable type: 'list' 可以看到报错信息提示，不支持类型：列表。 那我们如何像集合中追加元素呢？可以使用add()： 12345678# 定义集合mySets = {123,'abc',False,'love',True,(1,2,3),0,3.1415,'123',1}res = mySets.add('茶桁')print(mySets, '\\nres:', res)---{False, True, 3.1415, (1, 2, 3), 'love', '茶桁', 'abc', 123, '123'} res: None 可以看到我们在其中追加了一个字符串茶桁, 但是我们再一次验证了集合的无序，新加入的字符串并没有和其他数据类型一样新加入的元素放在最末端。 并且我们注意到了，我用res来接收了add()的返回值，返回了一个None。 除了追加之外，当然我们也可以对集合进行删除元素的处理： 123456res = mySets.pop()print(mySets, '\\nres:', res)---{True, 3.1415, (1, 2, 3), 'love', '茶桁', 'abc', 123, '123'} res: False 用pop()删除集合内的元素是随机的，并且，会将删除的元素返回。 如果想指定删除集合中的元素有没有办法呢？其实也有，remove()和discard()都可以做到，但是两者又有些区别，我们接着看代码： 123456789# 使用remove()res = mySets.remove('abc')print(mySets)res = mySets.remove('aaa')---{True, 3.1415, (1, 2, 3), 'love', '茶桁', 123, '123'} res: NoneKeyError: 'aaa' 能看到，remove()确实可以删除集合内的指定元素，并给一个返回值None。不过当集合内没有此元素的时候，就会报错，提示关键词错误。 那让我们再来看看discard() 12345678910# 使用discardmySets = {123,'abc',False,'love',True,(1,2,3),0,3.1415,'123',1}res = mySets.discard('123')print(mySets, f'res:{res}')res = mySets.discard('aaa')print(mySets, f'res:{res}')---{False, True, 3.1415, (1, 2, 3), 'love', 'abc', 123} res:None{False, True, 3.1415, (1, 2, 3), 'love', 'abc', 123} res:None 和remove()一样，也删除了一个指定元素，并且返回了None。不同的是，当我们使用discard删除一个不存在的元素时，discard虽然没有删除任何内容，但是也没有报错。 一个个删除太麻烦了，这个集合我就想让它变成一个空集合，好办，用clear()做清空处理呗，和字典一样： 12345678mySets = {123,'abc',False,'love',True,(1,2,3),0,3.1415,'123',1}print(mySets)mySets.clear()print(mySets)---{False, True, 3.1415, (1, 2, 3), 'love', 'abc', 123, '123'}set() 空集合拿到了，可以放入我们喜欢的元素了。依然和字典一致，我们可以使用update: 12345678res = mySets.update({1, 2, 3, 4, 5})print(mySets, f'res:{res}')res = mySets.update({2, 3, 4, 5, 6})print(mySets, f'res:{res}')---{1, 2, 3, 4, 5} res:None{1, 2, 3, 4, 5, 6} res:None 结果中显示，我们更新成功了，新添加了一些元素进入集合。那第二次添加，为什么就只有6添加进去了呢？还记得么？集合不能有重复值，就跟字典不能有重复的key一样。在字典中使用update，遇到相同key后面的value会被更新，那其实集合也是一样的，只是因为只有一个值，所以更新完不还是这个值么。 在冰冻集合的时候我们用到过一次copy， 这里我们要单独拿出来说说，因为集合中的元素都是不可变的，包括元组和冰冻集合，所以当前集合的浅拷贝并不存在深拷贝的问题。换句话说，就是不存在在拷贝后，对集合中不可变的二级容器进行操作的问题。 123456mySets = {123,'abc',False,'love',True,(1,2,3),0,3.1415,'123',1}res = mySets.copy()print(res)---{False, True, 3.1415, (1, 2, 3), 'love', 'abc', 123, '123'} 集合是没有deepcopy方法的。 集合的运算和检测 集合的主要运算有四种，以下将列出这四种以及他们的方法: 交集 &amp;, set.intersection(), set.intersection_update() 并集 |, union(), update() 差集 -, difference(), difference_update() 对称差集 ^, symmetric_difference(), symmetric_difference_update() 我们先来看看符号运算： 123# 先定义两个集合mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'} 然后让我们先求交集&amp;: 123456# 求两个集合交集res = mySet1 &amp; mySet2print(res)---{'Python'} 求两个集合并集（求并集的时候会去除重复项）| 12345res = mySet1 | mySet2print(res)---{'Java', 'C', 'Rust', 'C++', 'Go', 'Python', 'Ruby', 'JavaScript', 'Swift'} 求两个集合差集- 12345678res = mySet1 - mySet2res2 = mySet2 - mySet1print(res)print(res2)---{'Go', 'Rust', 'C++', 'Swift'}{'Java', 'C', 'Ruby', 'JavaScript'} 这段代码结果中res和res2的区别在于，res是mySet1中有，而mySet2中没有， res2是mySet2中有，而mySet1中没有。 求两个集合对称差集^ 12345res = mySet1 ^ mySet2print(res)---{'Java', 'C', 'C++', 'Swift', 'Rust', 'Go', 'Ruby', 'JavaScript'} 看完符号运算，我们可以再来看看函数运算 交集的运算函数为set.intersection(), set.intersection_update(), 那这两个函数又有什么区别呢？ 12345678res = mySet1.intersection(mySet2)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---{'Python'}mySet1:{'Go', 'Python', 'Rust', 'C++', 'Swift'}, mySet2:{'Java', 'C', 'Python', 'Ruby', 'JavaScript'} 让我们先记住intersection()的结果, mySet1和mySet2并没有发生变化，而返回值为两个集合相同的内容。然后我们再来看看另外一个函数： 12345678res = mySet1.intersection_update(mySet2)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---NonemySet1:{'Python'}, mySet2:{'Java', 'C', 'Python', 'Ruby', 'JavaScript'} 首先我们就能看到，返回值为None, 并且mySet1发生了变化。也就是说，set.intersection_update()是将两者的交集重复赋值给到了头部的变量，这里就是mySet1，然后返回一个None值。 接着我们来看一下并集运算函数: union(), update() 1234567891011mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'}res = mySet1.union(mySet2)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---{'Java', 'C', 'Rust', 'C++', 'Go', 'Python', 'Ruby', 'JavaScript', 'Swift'}mySet1:{'Go', 'Python', 'Rust', 'C++', 'Swift'}, mySet2:{'Java', 'C', 'Python', 'Ruby', 'JavaScript'} 我们首先看到了返回值，正事两个集合的并集，两个原始集合也没有发生变化。 再来看看update() 1234567891011mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'}res = mySet1.update(mySet2)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---NonemySet1:{'Java', 'C', 'Rust', 'C++', 'Go', 'Python', 'Ruby', 'JavaScript', 'Swift'}, mySet2:{'Java', 'C', 'Python', 'Ruby', 'JavaScript'} 可以很明显看到区别：返回值为None，并集的计算结果被复制给了第一个变量，这里是mySet1。 再看完并集之后，就轮到差集了, 分别是这两个函数difference(),difference_update()： 1234567891011mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'}res = mySet1.difference(mySet2)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---{'Go', 'Rust', 'C++', 'Swift'}mySet1:{'Go', 'Python', 'Rust', 'C++', 'Swift'}, mySet2:{'Java', 'C', 'Python', 'Ruby', 'JavaScript'} 返回值为差集的计算结果, 这里是mySet1有的而mySet2没有的。那不用问，按照一贯的惯例， difference_update()一定是将计算结果返回给第一个变量，这回我们换一下，将mySet2换成第一个变量试试： 1234567891011mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'}res = mySet2.difference_update(mySet1)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---NonemySet1:{'Go', 'Python', 'Rust', 'C++', 'Swift'}, mySet2:{'Java', 'C', 'Ruby', 'JavaScript'} 果然就跟料想的一样，最终的计算结果赋值给了mySet2。 最后当然就是对称差集函数symmetric_difference() symmetric_difference_update()： 1234567891011mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'}res = mySet2.symmetric_difference(mySet1)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---{'Java', 'C', 'C++', 'Swift', 'Rust', 'Go', 'Ruby', 'JavaScript'}mySet1:{'Go', 'Python', 'Rust', 'C++', 'Swift'}, mySet2:{'Java', 'C', 'Python', 'Ruby', 'JavaScript'} res接收了计算结果，成为了一个新集合。 接下来，大家应该能猜到了吧？不过还是要做做实验才知道，万一和自己想的不一样呢。 1234567891011mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'}res = mySet2.symmetric_difference_update(mySet1)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---NonemySet1:{'Go', 'Python', 'Rust', 'C++', 'Swift'}, mySet2:{'Java', 'C', 'C++', 'Swift', 'Rust', 'Go', 'Ruby', 'JavaScript'} 嗯，这个结果，一点都没有惊喜和意外。 好吧，运算我们学完之后，接着我们要看看集合的检测方法, 一共有三个，记住用法就可以了： issuperset()检测是否为超集 issubset()检测是否为子集 isdisjoint()检测是否不相交 为了更好的说明，我们不能在用之前的集合，这回，我们定义三个集合来看： 123mySet1 = {1, 2, 3, 4, 5, 6}mySet2 = {1, 2, 3}mySet3 = {6, 7, 8} 接下来从第一个检测开始： 12345678print(mySet1.issuperset(mySet2))print(mySet2.issuperset(mySet1))print(mySet1.issuperset(mySet3))---TrueFalseFalse 不知道大家在数学里有没有学过「超集」的概念，我们从最后的打印结果可以看出来，mySet1是mySet2的超集，反过来则不是，并且，mySet1也不是mySet3的超集。观察三个集合内的元素我们可以得出结论，如果集合a是另外一个集合b的超集，那么集合b内的元素一定在集合a中都找得到。 再来检测子集： 12345678print(mySet1.issubset(mySet2))print(mySet2.issubset(mySet1))print(mySet3.issubset(mySet1))---FalseTrueFalse 从这个结果中我们能看到，子集的概念就和超集完全相反了。 最后就是检测两个集合是否相交了，也就是集合中的元素有没有重复的： 12345678910print(mySet1.isdisjoint(mySet2))print(mySet2.isdisjoint(mySet1))print(mySet3.isdisjoint(mySet1))print(mySet2.isdisjoint(mySet3))---FalseFalseFalseTrue 虽然前三个都打印的False， 最后一个打印的True，但是我们从集合中应该知道，只有mySet2和mySet3没有相交关系。所以我们可以知道，isdisjoint()这个函数其实是检测不相交的。也就是说，返回结果为False则证明相交，返回结果为True反而是不相交。 结语 至此，随着我们的集合内容讲完，咱们的容器类数据类型就全部讲完了。 咱们下一节开始，咱们要开始行的篇章。下一节内容预告：Python中File文件的操作。 本节课一样就不布置作业了，大家好好的将最近将的容器类数据好好的回顾一下，将基础打扎实。","link":"/Detailed-of-set/"},{"title":"13. Python的文件操作","text":"Hi，大家好。我是茶桁。 在之前的几节课程中，我们学习了Python的数据类型。和市面上大多数的Python教程不同的是，我先为大家介绍完函数之后才开始介绍数据类型，其中原因就是很多数据类型的方法及理解都需要先搞懂函数的基本语法。 在结束了Python数据类型学习之后，我们今天开始进入一个新的篇章。今天，让我们来详细了解一下在Python中如何去进行文件操作。 我们大家都使用过智能手机，电脑，iPad等电子产品。那我们肯定有打开文件的经验，比如说打开一个Word、Excel文档。最基础的操作实际上就两步，分别是1. 打开文件， 2. 关闭文件。 我们要理解的一点是，文件都是放在存储设备中的，这才是我们能打开它的基础。那我们在存储设备中对文件进行打开之后进行的读写操作，实际上就是文件I/O。 什么是I/O?I代表Input(输入)，O代表Output（输出）。当你打开一个文件的时候，就算你没有对文件进行更改，也依然已经有了I/O操作，毕竟文件只有读取之后，才能显示到你的屏幕上。 那么文件读写到底分了几步呢？让我们引用一下宋丹丹的经典小品中的一段： 问，把大象装进冰箱分几步？ 我们就不在这里进行分步讨论了，因为流程步骤实际上是一模一样的： 打开文件open() ： 打开冰箱。 读取文件read()/ 写入内容write()： 把大象装进冰箱。 关闭文件close()： 关闭冰箱。 可以说，你在你的设备上做的任何操作都逃不开这几步，区别无非就是你有没有写入内容，从哪里打开的，读取的文件是什么类型的。 那么复杂一点的，就是当你打开一个App的时候，这个App执行某项操作的时候去互联网上的服务器找相应的文件然后到本地之后打开，读取。我们不讨论在打开文件之前的一系列例如下载（这个下载动作有时是主动的，有时是被动的）操作，就只说到本地之后读取文件并展示，就一定包含这三步。 理解到这，可以了。我们接着正式来学习Python如何对文件进行操作。 文件操作 open() open函数就是用于最初的打开文件的动作，其基本格式为：open(文件路径, 打开方式, [字符集])， 完整的格式为：open(file, mode='r', buffering=None, encoding=None, errors=None, newline=None, closefd=True) 在大部分时候，我们使用基本格式就足够了。 123456'''打开文件 open() 参数1: 文件路径 参数2: 打开的方式 参数3: 字符集''' 路径，也就是url是一种统一资源定位符。其中包括相对路径和绝对路径。 相对路径，比如说我们被路人问路，我们就说：这条街往前，前面十字路口就是交道口，然后左转，再走100米左右就到了。 绝对路径， 这个就非常好理解了，北京市西城区鼓楼东大街28号，特别准确了对吧？ 这两个路径的描述呢，其实指向的是一个地方。只是一个是针对人所在的位置来告知你怎么走，另外一个是从最上层给到你一个绝对的地址。而电脑里的相对路径和绝对路径也基本就是这么个意思。 我们来看相对路径，主要是使用./和../来进行描述，这两个都有一个共同点，就是以当前文件为准。也就是当前文件向我们问路，我们站在当前文件的地方告诉它该怎么走去到达自己的目的地。 举例, 假设我们现在正在编辑index.py这个文件，也就是说，向我们问路的文件是index.py： 123456789- project | - index.py | - test.txt | img | - person.jpg | - dog.jpg | - cat.jpg- data | - person.csv 这样一个路径关系中： 当我们需要去访问person.jpg并打开的话，那就是index.py同目录下的img目录里面去寻找person.jpg， 那我们相对路径的写法为./img/person.jpg。 当我们要去找person.csv的时候，由于这个csv是存在于上一层目录的同级目录data内，那我们需要向上去寻找，就是../data/person.csv。 这就是./和../的区别，一个是当前目录同级内去寻找，一个是向上一级的目录内去寻找。那如果文件存储于上两层目录中呢？那就向上翻两层呗：../../这样，多层的时候，依次类推。 相对路径介绍完了，我们来看看绝对路径。 绝对路径的前提是必须找到根目录。在windows中我们其实都熟悉一个东西就是盘符。比如说C:\\，不严谨的说，盘符就算是绝对路径的根目录了。那为什么说不严谨的说呢？因为我们输入文件路径的时候可以输入：C:\\data\\person.csv这样去寻找。但是，盘符之上其实是整个硬盘，我们只是将硬盘虚拟成了不同的盘符用于划分空间而已。 在Mac或者Linux中，就是以整个硬盘为准去寻找文件的。比如说/Users/xx/Downloads，就是我们的下载目录。 那我们如果想要打开文件，这两种方式其实都可以，一般来说，为了代码能够适应环境变化，我们都会选择使用相对路径。 说完文件路径，让我们来说说打开方式，我先介绍一个模式，后面咱们再慢慢讲： w模式： write， 写入 如果文件不存在，创建这个文件； 如果文件存在，则打开这个文件，并且清空文件内容。文件打开后，文件的指针在文件的最前面。什么是指针呢？ 可以这么理解，当我们打开一个word文档的时候，我们的光标是不是都在这个文档的最上面？这个光标的位置，就是指针的位置。 write() write()是用于对文件写入内容来使用的，格式为:文件对象.write(内容) close() 格式为: 文件对象.close() , 可以关闭打开的文件。 我们需要注意一点，我们在对文件进行操作的时候，一定记得操作完要关闭它。否则，这个文件就会一直存在于内存地址中。 下面，让我们看看在Python中如何打开操作一个文件的。 以下所有的操作演示都会在../Python/13.ipynb中进行编写，所以我们的操作路径都会以这个文件为准。 让我们现在当前文件的中创建一个文件夹data，然后在其中放入一个文件13-1.txt，我们说要做的事情，就是打开这个文件，然后将我们之前写的内容复制一部分写入到这个txt文件中去，路径关系如下图： 12345678# 打开13-1 并且写入内容fp = open('./data/13-1.txt', 'w')print(fp, type(fp))fp.write('相对路径: 比如说我们被路人问路，我们就说：这条街往前，前面十字路口就是交道口，然后左转，再走100米左右就到了。\\n 绝对路径: 这个就非常好理解了，北京市西城区鼓楼东大街28号，特别准确了对吧？')fp.close()---&lt;_io.TextIOWrapper name='./data/13-1.txt' mode='w' encoding='UTF-8'&gt; &lt;class '_io.TextIOWrapper'&gt; 打印区打印的内容，实际上是我们print函数执行的结果，可以看到，我们打印fp这个变量的时候，显示的是&lt;_io.TextIOWrapper name='./data/13-1.txt' mode='w' encoding='UTF-8'&gt;, 其类型是&lt;class '_io.TextIOWrapper'&gt;。 这些先放在一边，让我们看看文件到底写入没有： 写入是写入了，可是这是什么鬼？ 啊，差点忘了，整个open()方法内后面还有一个参数encoding=， 这个参数是告诉我们这个文件以什么字符集去打开。默认的就是UTF-8，显然，我们保存的这个文件并不是，所以最终导致了乱码。 让我们修改一下代码，在open()内添加一下encoding，其他不变: 12345fp = open('./data/13-1.txt', 'w', encoding='GBK')...---&lt;_io.TextIOWrapper name='./data/13-1.txt' mode='w' encoding='GBK'&gt; &lt;class '_io.TextIOWrapper'&gt; 可以看到，打印出来的fp最后的encoding值已经发生了变化。让我们再去看看文件如何了： 果然没问题，内容能够正确显示而不会乱码了，我们注意到下方文件字符集确实为GBK。 关于字符集编码的问题这里有疑问的，自己回过头再去把我之前讲的课程好好翻腾一下，复习一下。 整段代码中，我们引用了刚才介绍的三个文件操作的函数： open(), write(), close()。 在简单了解了文件的操作步骤之后，我们接下来再继续看文件操作中另外一个比较重要的函数: read()。 read() 在对文件进行操作的时候，一定要记得流程一定是打开open在最前面，close关闭在最后面。至于中间你是要读取，写入还是别的什么操作，那都不违反文件操作的整个流程。 所以在下面一段代码里，我们可以尝试把之前的write()替换为read()，顺便可以学一下如何在代码中看看我们刚修改过的文件： 123456789fp = open('./data/13-1.txt', 'r', encoding='GBK')res = fp.read()fp.close()print(res)---相对路径: 比如说我们被路人问路，我们就说：这条街往前，前面十字路口就是交道口，然后左转，再走100米左右就到了。绝对路径: 这个就非常好理解了，北京市西城区鼓楼东大街28号，特别准确了对吧？ 可以看到，我们讲刚才写入的内容在打印区完整的打印了出来。 不知道小伙伴们有没有注意到，在使用open()函数的时候，其中的第二个参数「打开方式」这次发生了变化，改成了‘r’， 这中打开模式就是专门用于读取文件的，它在打开文件的时候，不会想‘w’的打开方式一样清空文件。 比如，我们讲之前的代码中换一下打开方式来试试： 12345678fp = open('./data/13-1.txt', 'w', encoding='GBK')res = fp.read()fp.close()print(res)---UnsupportedOperation: not readable 报错了，提示不可读。 我们再去直接打开13-1.txt的时候可以看到。文件内空空如也，之前写入的内容全都被清空了。 到这里为止，大家了解了文件操作的四个基本操作函数，在这里我可以教大家一个文件操作中的一些高级技巧，比如，我们可以使用with...as...来进行操作： 1234'''with open(文件路径, 打开模式) as 变量: 变量.操作()''' 让我们直接来看示例： 1234567with open('./data/13-1.txt', 'r+', encoding='GBK') as fp: res = fp.read() print(res) ---相对路径: 比如说我们被路人问路，我们就说：这条街往前，前面十字路口就是交道口，然后左转，再走100米左右就到了。绝对路径: 这个就非常好理解了，北京市西城区鼓楼东大街28号，特别准确了对吧？ 这样，我们也就直接完成了之前读取的操作。 read函数内是有参数的：read(count)， 接收的值为整型，这里是描述当前我要读取几个字节长度： 123456with open('./data/13-1.txt', 'r', encoding='GBK') as fp: res = fp.read(5) print(res) ---相对路径: 有的小伙伴看到我写到这可能就有疑问了，我为什么没有写close()函数，那是不是说，现在这个文件都还一直存在内容地址中。 其实并不是如此。在使用with...as...这个方式去打开一个文件的之后，在整个代码结束的时候会自动对当前打开的文件一遍执行close()函数。 好，让我接着继续： 12345678with open('./data/13-1.txt', 'r+', encoding='GBK') as fp: res = fp.read() print(res) fp.write(res)---相对路径: 比如说我们被路人问路，我们就说：这条街往前，前面十字路口就是交道口，然后左转，再走100米左右就到了。绝对路径: 这个就非常好理解了，北京市西城区鼓楼东大街28号，特别准确了对吧？ 打印区并未发生变化，原因就是我们的写入操作是在print之后进行的，我们直接打开文件来看看： 可以看到，内容确实被写入文件中了。注意我打标记的地方，并没有换行对吧，也就是说，我们在做写入的时候，指针是标记在这个位置的，然后继续往后写入。 另外整个代码中需要注意的就是打开模式了，我们之前已经用过的打开模式有‘w’和``'r'， 现在我们用了‘r+’的模式，那么r+呢就是既可以读，也可以写入。并且，不会一开始就清空文件的内容。 对应‘w’的清空模式，就是‘w+’, 虽然‘w+’也是可读可写的模式，但是它和‘w’的模式一致，打开文件的时候直接清空整个文件的内容。 除了这四个模式之外，还有'a'和‘'a+’模式，是追加写的模式，这种模式的特点是打开文件的时候，指针是放在文件最末尾的。所以这种模式使用read()的时候，是读不到任何内容的。 以为到这里就结束了吗？太单纯了，整个文件操作的打开模式中，还有一个‘x+’的模式，这种模式我们可以称它为异或，什么意思呢？就是这种模式只会新建文件来执行后续操作，否则就会报错： 12345with open('./data/13-1.txt', 'x+', encoding='GBK') as fp: print(fp.read()) ---FileExistsError: [Errno 17] File exists: './data/13-1.txt' 提示文件错误：文件存在。 如果我们操作的是一个本来不存在的文件，才可以正常的往下进行： 1234567891011with open('./data/13-x+.txt', 'x+', encoding='UTF-8') as fp: res = fp.read() print(res) fp.write('这里是&quot;x+&quot;模式下新加入的内容。')with open('./data/13-x+.txt', 'r+', encoding='UTF-8') as fp: res = fp.read() print(res) ---这里是&quot;x+&quot;模式下新加入的内容。 我们在用‘x+’模式打开一个文件的时候，它已经新建了这个文件，我们可以看到读取之后并未读取到任何内容，因为这个文件内还是空的。在进行写入操作之后，我们在下面再一次读取这个文件，可以看到内容已经被写入了。 详谈「打开模式」 其实mode这个参数并不只是我们演示的这么点内容，mode这个参数是接收两种值的，一个是刚才我们一直在讲的读写模式, 而另外一个则是文件格式： 读写模式： 读写模式的参数主要有四种， 分别是r, w,a以及一个特殊+， 其中r, w, a决定了当前文件默认是只读还是只写，还有就是指针位置。+是和前面三个结合使用的，无法单独使用，其主要作用是使的文件读写兼备。 文件格式： 文件格式主要是以两种格式为准，一种是普通的文本文件，一种是二进制格式文件。不要以为二进制格式没什么大不了，我们一般谈到非文本文件都属于二进制格式文件，比如：图片。 这两个格式控制字符一个是t: 以文本格式打开文件（默认值）， 一个是b: 以二进制格式打开文件。 一般来说，我们大部分时候都不会单独使用某一个参数吗，而是结合着一起使用。比如： r+， 打开一个文件用于读写，文件存在就打开，文件不存在则报错。指针在文件头。这种模式要注意，因为指针在文件头，所以新写入的内容会在原内容之前。 w+, 打开一个文件用于读写，文件存在就打开，并且会清空所有内容后进入编辑模式，如果文件不存在则会创建一个新文件。虽然指针也在文件头，但是因为它霸道的清空属性，所以也不存在新写入的内容会在原内容之前了。 a+， 以追加的模式打开一个文件用于读写，如果文件存在就打开，如果文件不存在，则会创建一个新文件用于读写。这种模式下和w+不同的地方在于它会将指针放在文件末尾，写入的时候是从文件尾部开始写。并且，它没那么霸道，要清空原内容才可以。 其他的模式就是在打开文件格式和读写模式的组合，一般我们不写是因为大部分时候我们操作的都是文本文件进行操作，而如果我们需要用二进制格式打开文件的时候，就不能使用默认的t而是b了，一般我们会是这样进行组合：rb, rb+, wb, wb+, ab, ab+。 当然，最后就是我们刚才用到的x+， 其实它也是一种组合形式，原本应该是x, 这种模式是在Python3中新添加的，它在文件不存在的时候它会创建一个新文件用于写入。如果这个文件存在，就会报错。 那么有了x这个参数之后，我们以前为了避免误操作覆盖原文件，那么我们会先去判断一个文件是否存在，然后再去执行后续的写入操作。可是使用x就没那么麻烦了，可以直接操作写入，反正文件如果存在会返回错误。 关于指针位置 那么我们在使用了r+之后，有没有什么办法可以让我们不在原内容之前写入内容而是从后开始写呢？ 答案是有办法，也就是调整指针位置，调整完毕之后再进行写入操作就可以了。 调整指针的方法为seek(offset[, whence])。我们来看一个对比： 12345678910111213141516171819202122232425# 创建一个新文件with open('./data/13-2.txt', 'w', encoding='UTF-8') as fp: fp.write('1. Hello Python.\\n2. Hello C++. \\n3. Hello Ruby.')# 正常状态下with open('./data/13-2.txt', 'r', encoding='UTF-8') as fp: print(fp.readline()) print(fp.readline()) print('--------------')# 设置指针重新偏移到头部with open('./data/13-2.txt', 'r', encoding='UTF-8') as fp: print(fp.readline()) fp.seek(0, 0) print(fp.readline())---1. Hello Python.2. Hello C++. ------------1. Hello Python.1. Hello Python. 在最开始，我们重新创建了一个文件，然后写了三行文字。分别是： 1231. Hello Python.2. Hello C++. 3. Hello Ruby. 然后我们开始用不同的方法进行读取，每次仅读取一行。 正常状态下，readline()这个方法是顺序往下执行的，第一次执行的时候读取的是第一行，第二次执行的时候就是读取的第二行。这种方式是不是感觉有些熟悉，像不像迭代器？ 回过头来，我们再来看两次执行的结果，不同的是，第二次我在两个readline()方法中间加入了一段fp.seek(0,0)来将指针再次调整到头部，别着急，我们一会讲为什么这样写，先来看看结果。 因为有了fp.seek(0,0)的存在，第二次执行和第一次完全不同。第一段内容被读取了两次。这就是seek()的作用，讲指针又调整到了文件头部。 现在，让我们来说说seek()内参数的含义，完整的写法是：seek(offset[, whence])，其中offset是偏移量，而whence是从哪开始。whence就只有三个值， 0, 1, 2， 0就表示是从头部开始偏移，1就表示从当前位置开始偏移，2就代表从文件末尾开始偏移。而我们写的(0,0)意思就是从文件头部开始偏移，偏移量为0。 再来看一段代码： 123456789101112# 设置指针重新偏移到头部with open('./data/13-2.txt', 'r+', encoding='UTF-8') as fp: fp.seek(0, 2) fp.write('这里是使用r+添加到末尾的内容')with open('./data/13-2.txt', 'r', encoding='UTF-8') as fp: print(fp.read()) ---1. Hello Python.2. Hello C++. 3. Hello Ruby.这里是使用r+添加到末尾的内容 通过前面的学习我们知道，r+在打开文件之后，指针是放在头部的，但是我们这里用seek(0,2)将指针调整到了最末尾，并且写入了一段文字。 学到这里，文件的基本操作也就差不多学完了，让我们来分别总结一下： 相关函数 open(), 打开文件, 格式：open(file_name [, access_mode][, buffering]) read(), 读取内容， 格式：fileObject.read([count]) 不设置count是从当前位置读取到文件末尾，设置count这是读取指定长度的字符。 readline(), 读取一行 不设置count是从当前位置读取到这一行末尾，设置count这是读取这一行中指定长度的字符。 readlines(), 读取所有行 不设置参数是表示读取所有汗，每一行作为一个参数，返回了一个列表。设置count是按照行进行读取，可以设置读取的字节数，设置的字节数不足一行按一行来读取。 write()，写入内容, 格式：fileObject.write(string) writelines(), 写入容器类型数据： 写入容器类数据的时候要注意，这个容器类数据必须是可更新的类型。 seek(), 设置文件指针的偏移, 格式: seek(offset[, whence]) close()， 关闭文件 当然，除了这几个之外，文件还有很多其他的函数，但是目前我们用这些进行读写操作就足够了。 打开模式（图） 关于打开模式， 我之前写的那些内容看懂理解了，其实也就不需要现在这两张图了，可是我担心的是有些小伙伴理解不了，那有了下面的图，至少操作的时候可以参考： 模式 描述 t 文本模式 (默认)。 x 写模式，新建一个文件，如果该文件已存在则会报错。 b 二进制模式。 + 打开一个文件进行更新(可读可写)。 U 通用换行模式（不推荐）。 r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。 rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。一般用于非文本文件如图片等。 r+ 打开一个文件用于读写。文件指针将会放在文件的开头。 rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。一般用于非文本文件如图片等。 w 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 w+ 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。 ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。 总结一下最常用的六种模式： 模式 r r+ w w+ a a+ 读 ✓ ✓ ✓ ✓ 写 ✓ ✓ ✓ ✓ ✓ 创建 ✓ ✓ ✓ ✓ 覆盖 ✓ ✓ 指针在开始 ✓ ✓ ✓ ✓ 指针在结尾 ✓ ✓ 下面这张经典的流程图可以告诉你在什么时候需要用什么： 结尾与预告 文件的基本操作就介绍到这里了，大家下课之后记得要去多多的熟悉和练习。 那么下一节课呢，我们会根据我们这之前所讲的所有内容，尝试做一个小demo， 实现一个简单的注册和登录功能。 这里先介绍一下这个demo: 123456实现功能：1. 用户输入用户名和密码以及确认密码2. 用户名不能重复3. 两次密码要一致4. 用户用已经注册的账户登录5. 密码如果错误3次，锁定，无法再登录。 好了，小伙伴们，咱们下节课再见。","link":"/file-operations/"},{"title":"14. 练习：登录注册系统","text":"Hi，大家好。我是茶桁。 上一节课，我们详细的介绍了文件读写的流程和原理，并用Python进行实际操作了一下。 那么这节课呢，我们利用之前所学的内容，尝试做一个小练习：建立一个登录注册系统。上节课我们在结尾的时候讲练习内容贴了出来，还记得要求吗？ 123456实现功能：1. 用户输入用户名和密码以及确认密码2. 用户名不能重复3. 两次密码要一致4. 用户用已经注册的账户登录5. 密码如果错误3次，锁定，无法再登录。 那么这节课呢，因为都是一些讲过的知识点，所以在整个实现过程中我就不详细讲解了，我们重点在于介绍思想和流程。那让我们开始吧。 注册功能 我们先把大的结构写出来，一个注册功能，那首先需要接收两个参数：用户名、密码。 并且，为了防止用户注册时候输错密码导致无法登录，还需要让用户确认一遍密码： 1234567891011121314151617181920# 先实现注册功能# 封装一个函数 完成注册功能def register(): # 用户输入用户名 username = input('欢迎注册，请输入用户名：') # 用户输入密码 password = input('请输入您的密码: ') # 请确认您的密码 re_password = input('请再输入一次您的密码: ') print(username, password, re_password)register()---du 111111 111111 下一步我们来细化这个框架，我们把用户名先放在一边，来挑一挑密码的刺： 首先，我们需要让密码保持安全性，那么我们对位数，组合就要有要求。这里我们简单点，必须输入6位以上吧。 然后，在确认密码的时候，肯定两次密码要一致才行。 OK，让我们补全这几个逻辑关系： 12345678910111213141516171819202122232425262728293031# 先实现注册功能# 封装一个函数 完成注册功能def register(): # 用户输入用户名 username = input('欢迎注册，请输入用户名：') # 用户名需要检测是否已经存在 # 用户输入密码 password = input('请输入您的密码: ') # 检测密码长度不能低于6位 if len(password) &gt;= 6: # 请确认您的密码 re_password = input('请再输入一次您的密码: ') # 检测密码和确认密码是否一致 if re_password == password: # 用户名和密码都正确，可以写入文件 pass else: print('两次输入的密码不同，请重新输入。', username, password, re_password) # 密码长度不够 else: print('密码格式不正确：', username, password)register()---两次输入的密码不同，请重新输入。 du 123456 1234567 可以看得出来，我确认密码的时候输入了其他密码，判断逻辑没有问题。 继续来细化，现在的问题是，当我们输入第一次密码的时候判断有问题，或者两次密码输入不一致的时候，我们没有办法跳到一开始让用户重新输入，那么下面我们就来解决这个问题： 12345678910111213141516171819202122232425262728293031323334353637# 先实现注册功能# 封装一个函数 完成注册功能def register(): # 用户输入用户名 username = input('欢迎注册，请输入用户名：') # 用户名需要检测是否已经存在 # 利用循环，都正确的时候结束循环。 while True: # 用户输入密码 password = input('请输入您的密码: ') # 检测密码长度不能低于6位 if len(password) &gt;= 6: # 请确认您的密码 re_password = input('请再输入一次您的密码: ') # 检测密码和确认密码是否一致 if re_password == password: # 用户名和密码都正确，可以写入文件 print('恭喜你，注册成功', username, password, re_password) # 结束循环 break else: print('两次输入的密码不同，请重新输入。', username, password, re_password) # 密码长度不够 else: print('密码格式不正确：', username, password)register()---密码格式不正确： du 12345两次输入的密码不同，请重新输入。 du 123456 1234567恭喜你，注册成功 du 123456 123456 外面套一层while循环，这样就解决了。 只有当密码正确输入的时候，循环才会结束，否则就会跳到循环的最开始，重新进行输入。 接着，我们就来看看当输入正确之后，我们如何写入文件呢？这个是我们上一节课刚讲解的课程，现在让我们来实现一下： 1234567891011121314151617# 先实现注册功能# 封装一个函数 完成注册功能def register(): ... if re_password == password: # 用户名和密码都正确，可以写入文件 # 打开文件，写入数据 with open('./data/user.txt', 'a+', encoding='UTF-8') as fp: fp.write(f'{username}:{password}\\n') print(f'注册成功，用户名:{username}') # 结束循环 break ...register()---注册成功，用户名:du 这样，我们就将用户名和密码以username:password的格式以一行的形式存储到了一个user.txt文件内。并且，因为我们使用的a+的模式，所以每次打开指针都会是放在文件的末尾进行添加。 来，让我们看看是否成功了： 可以看到，没问题。这样，我们在之后读取的时候就可以直接读取单行，并且以key:value的形式拿到我们说要的用户名和密码。用于验证key是否存在，value是否正确。 说到验证key是否存在，似乎我们还没写这一段验证代码，既然文件里已经有数据了，让我们把这段代码补全吧, 我们在register()这个自定义函数外面写一段读取文件的代码，写在外面是因为，我们读取文件这个动作，和整个注册动作不能说毫无关系，只能说是没有关联。 123456with open('./data/user.txt', 'r', encoding=&quot;utf-8&quot;) as fp: res = fp.readlines() print(res)---['admin:123456\\n', 'du:654321\\n'] 还记得吗？readlines()这个函数的应用，不记得的，看我教程的上一节内容复习下。 看似我们确实成功的读取到了全部内容，可是我们细想一下，这样想有没有问题？还记得我们上一节课上对r这个模式的说明吗？r是读取已有文件，但是如果文件不存在，那就会报错。 可是我们在整个程序运行过程中，不能因为这个原因就不让用户继续往下走了对吧？那么我们怎么去修改呢？来，我们一起尝试下： 12345678# 读取所有的注册信息with open('./data/user.txt', 'a+', encoding=&quot;utf-8&quot;) as fp: fp.seek(0,0) res = fp.readlines() print(res)---['admin:123456\\n', 'du:654321\\n'] 我们利用a+的特性，如果文件不存在则会创建，如果有，这将指针放在文件末尾。这样的话，这一段内容就没有问题了？为什么不用w+? 还不是因为w+太霸道，虽然新人胜旧人，但也不能就直接把旧人干掉吧。 但是a+是将指针放在文件末尾的，我们直接使用的话，什么内容也读不到，所以我们使用可seek(0,0)来将指针放在了文件头的位置。 好了，继续。我们只将文件读取出来没用，需要将其中的数据放在一个变量里供我们检测，所以下一步，我们就需要创建一个变量，并且接收文件中的所有数据： 12345678910111213141516171819# 先实现注册功能# 专门定义数据变量，存放已经注册的用户信息userlist = []pwdlist = []# 读取所有的注册信息with open('./data/user.txt', 'a+', encoding=&quot;utf-8&quot;) as fp: fp.seek(0,0) # 调整指针到文件头 res = fp.readlines() # 按照每一行读取所有用户数据 for i in res: # 循环读取每一行数据 r = i.strip() # 处理每一行尾部的换行符 mydict = r.split(':') # 分隔用户名和密码 userlist.append(mydict[0]) pwdlist.append(mydict[1]) print(userlist, pwdlist) ---['admin', 'du'] ['123456', '654321'] 这样，我们拿到用户名和密码，并且分别放入了两个list变量中。 接下来，我们对于注册那一步就需要进行判断了。 1234567891011121314151617# 封装一个函数 完成注册功能def register(): ... # 用户输入用户名 username = input('欢迎注册，请输入用户名：') # 用户名需要检测是否已经存在 if username in userlist: print('当前用户已经存在，请更换用户名。') else: # 利用循环，都正确的时候结束循环。 while True: # 用户输入密码 password = input('请输入您的密码: ') ...# register() 在代码中，我们判断用户名是否存在于列表中，如果存在，打印已经存在，如果不存在，则继续往下执行。将之前输入密码的while循环放入了else判断逻辑中。 看似问题解决了，不过新的问题又来了。虽然我们现在判断了用户是否存在，并且用户不存在的话可以继续往下执行，可是如果存在呢？我们需要的是让用户重新输入用户名，但是现在是打印完之后就结束了。所以，我们还得继续修改代码，这回简单了，和password的处理用一样的方法就可以了。 12345678910111213141516171819202122def register(): # 执行循环， 用户名操作 while True: # 用户输入用户名 username = input('欢迎注册，请输入用户名：') # 用户名需要检测是否已经存在 if username in userlist: print('当前用户已经存在，请更换用户名。') else: # 利用循环，都正确的时候结束循环。 while True: ... if len(password) &gt;= 6: ... if re_password == password: ... # 结束循环 break else: ...# register() 我们将register()内的所有判断代码放入了一个循环中，那么在if之后，没有向下执行else内的逻辑的时候，就会跳转到最开始的while循环重新执行。 不过这样似乎还是不行，那就是几遍else逻辑全部执行完毕，内部的break也是跳出里面那层while循环，外层循环只能无限的执行下去了。 我们目前所要做的，就是在所有的代码顺利执行到密码正确确认的时候，整个函数执行就全部终止了。那有什么办法吗？ 我们看到了while True， 然后开启了无限循环。那是不是说，如果True那里不为真，就无法进入循环了？ 嗯，既然这样，我们设定一个变量，在需要终止函数的时候，设定变量为Flase就行了。 123456789101112131415161718192021222324252627# 封装一个函数 完成注册功能def register(): # 定义一个变量，用于控制外循环 site = True # 执行循环， 用户名操作 while site: ... if username in userlist: print('当前用户已经存在，请更换用户名。') else: # 利用循环，都正确的时候结束循环。 while True: ... if len(password) &gt;= 6: ... if re_password == password: ... # 结束循环 # 结束外循环 site = False # 结束内循环 break else: ...# register() 这样，在密码确认成功之后，site设定为false， while循环判断条件为假，无法再次进入循环。执行代码，输出注册成功：用户名:du2， 并且跳出了循环不再继续向下执行。说明我们这一步已经没问题了。 那到这里，我们一个简易的可运行的注册方法就完成了，当然，这段代码中其实还有很多可以完善的空间，比如说，判断用户名是否存在之后，还要判断用户名是否合法，密码是否合法等等。还有就是我们一般接收账户密码的变量应该使用key:value形式的字典，不过大多数时候，我们还要考虑效率问题。分别保存成两份是不错的选择，那么这个时候，我们就需要用到获取当前字典中所有key值的方法了。以上代码在关键部位可以这样改： 1234567891011121314151617181920212223242526272829# 专门定义数据变量，存放已经注册的用户信息userdict = {}userlist = []# 读取所有的注册信息with open('./data/user.txt', 'a+', encoding=&quot;utf-8&quot;) as fp: fp.seek(0,0) # 调整指针到文件头 res = fp.readlines() # 按照每一行读取所有用户数据 for i in res: # 循环读取每一行数据 r = i.strip() # 处理每一行尾部的换行符 mydict = r.split(':') userdict.update({mydict[0]:mydict[1]}) userlist = userdict.keys() # 封装一个函数 完成注册功能def register(): # 定义一个变量，用于控制外循环 site = True # 执行循环， 用户名操作 while site: # 用户输入用户名 username = input('欢迎注册，请输入用户名：') # 用户名需要检测是否已经存在 if username in userlist: print('当前用户已经存在，请更换用户名。') else: ...register() 总之，代码的实现是需要一步一步细细琢磨的，而且实现的方式也不是只有一种。要考虑逻辑，效率等等因素。文章最后会给到我的源码文件，现在大家先一步步的跟着我往下走吧。 登录功能 在实现登录功能之前，我们来分析一下这个功能要做的事情： 需要使用已经注册的用户信息登录 密码输入错误3次之后，锁定账户信息（不能再使用这个账户进行登录操作） 这是两个最基本的功能，既然要核对用户信息和密码，那和注册一样，我们还是需要读取user.txt文件，拿到里面的信息后传给相应的变量，用于检测。那好，我们先把注册那边实现的部分功能拿过来，就无需自己再写一遍了： 12345678910111213141516171819# 搞定登录功能# 专门定义数据变量，存放已经注册的用户信息userdict = {}userlist = []# 读取所有的注册信息with open('./data/user.txt', 'a+', encoding=&quot;utf-8&quot;) as fp: fp.seek(0,0) # 调整指针到文件头 res = fp.readlines() # 按照每一行读取所有用户数据 for i in res: # 循环读取每一行数据 r = i.strip() # 处理每一行尾部的换行符 mydict = r.split(':') userdict.update({mydict[0]:mydict[1]}) userlist = userdict.keys() print(f'userdict:{userdict},\\nuserlist:{userlist}')---userdict:{'admin': '123456', 'du': '654321', 'du2': '123456', 'zhang': '123456', 'wang': '123456'},userlist:dict_keys(['admin', 'du', 'du2', 'zhang', 'wang']), 执行过后可以看到，三个变量已经分别存储了一个字典和两个列表，也就是字典存储了健值对的用户密码，以及将用户和密码再分别存储到两个列表内。 数据准备好了，接下来，我们开始定义函数吧： 1234567891011121314# 封装登录函数def login(): # 获取用户登录时输入的用户名 username = input('欢迎登录，请输入您的用户名：') # 检测当前用户名是否存在 if username in userlist: # 让用户输入密码 pass else: # 用户名不存在 print('用户名错误，亲重新输入') # 检测用户是否属于锁定状态 那么大的框架就定义好了。现在让我们逐步开始细化这些代码。 先不看让用户继续输入密码的逻辑，我们先来看看用户名不存在的逻辑该怎么做。 当用户名不存在的时候，我们肯定是先要告知提示用户，接下来应该是让用户重新输入一次用户名。那怎么做呢？之前实现注册那边实际上已经有经验了，用while循环呗，接下来，让我们完善一下： 123456789101112131415161718192021222324252627282930# 封装登录函数def login(): # 自定义变量, 控制登录外循环 isLogin = True # 创建循环 while isLogin: # 获取用户登录时输入的用户名 username = input('欢迎登录，请输入您的用户名：') # 检测当前用户名是否存在 if username in userlist: while True: # 让用户输入密码 pwd = input('请输入您的密码：') # 检测用户输入的密码是否正确 if pwd == userdict[username]: print(pwd) isLogin = False # 结束外循环 break # 结束内循环 else: print('您的密码输入有误。') else: # 用户名不存在 print('用户名错误，亲重新输入') # 检测用户是否属于锁定状态login() 在细化的代码中，我们用了和注册函数一样的方法，创建了一个外循环和一个内循环。原因就是我们分别需要判断用户名和密码，都需要返回来重新输入。 之前注册的循环已经讲的比较详细了，所以这里我们就不细致的讲解了，小伙伴们自己好好琢磨一下逻辑关系。 那到这里，我们还缺什么呢？看看需求，我们似乎还需要判断用户输入密码错误的次数对吧？ 好的，让我们继续，从定义变量开始： 12345678910111213141516171819202122232425262728293031# 封装登录函数def login(): ... # 定义变量，用来记录用户输入密码错误次数 errorNum = 3 # 创建循环 while isLogin: ... if username in userlist: while True: ... if pwd == userdict[username]: ... else: # 密码错误，修改变量次数 errorNum -= 1 # 判断当前密码错误次数 if errorNum == 0: print('给你机会你不中用啊细狗。账户已锁定，请联系管理人员并上供品。') isLogin = False break else: print(f'您的密码输入有误, 您还能再尝试{errorNum}次。') else: ... # 检测用户是否属于锁定状态login() 我们定义了一个errorNum的变量，因为我们只能尝试最多三次，所以我们给这个变量设定了一个3的值。 在之后的循环中，没尝试错误一次，我们就让这个变量-1操作，直到变为0为止。 然后，就是我们需要定义一个黑名单把用户名写入了。 1234567891011121314151617181920212223242526272829303132# 封装登录函数def login(): ... # 创建循环 while isLogin: if username in userlist: while True: ... if pwd == userdict[username]: ... else: # 密码错误，修改变量次数 errorNum -= 1 # 判断当前密码错误次数 if errorNum == 0: print('给你机会你不中用啊细狗。账户已锁定，请联系管理人员并上供品。') # 锁定当前账户，把锁定的用户拉入黑名单 with open('./data/black.txt', 'a+', encoding='UTF-8') as fp: fp.write(username+'\\n') isLogin = False break else: print(f'您的密码输入有误, 您还能再尝试{errorNum}次。') else: ...login()---您的密码输入有误, 您还能再尝试2次您的密码输入有误, 您还能再尝试1次给你机会你不中用啊细狗。账户已锁定，请联系管理人员并上供品。 这样，当我们的账户被输入错误三次之后，就会得到神的审判：关入小黑屋了。需要上供才行。（谁家这么设计产品，估计死的会很快吧？） 好了，继续往后，我们只写入了黑名单还不行，这样在执行的时候，还是无法判断用户是否输入的是锁定的账户。接下来需要做的事情，就是读取这个黑名单的文件，然后把里面的用户名全部以列表形式传到这个变量中。 再然后，我们只要在用户输入用户名的时候判断此用户是否被关小黑屋了就行了： 12345678910111213141516171819202122232425262728293031323334353637383940# 搞定登录功能# 专门定义数据变量，存放已经注册的用户信息...blackUserList = [] # 定义一个小黑屋专用变量# 读取所有的注册信息with open('./data/user.txt', 'a+', encoding=&quot;utf-8&quot;) as fp: ...# 读取所有黑名单用户with open('./data/black.txt', 'a+', encoding='utf-8') as fp: fp.seek(0,0) res = fp.readlines() for i in res: r = i.strip() # blackUserList.append(r)# 封装登录函数def login(): ... # 创建循环 while isLogin: # 获取用户登录时输入的用户名 username = input('欢迎登录，请输入您的用户名：') # 检测当前用户名是否存在 if username in blackUserList: print('您的账户已经被锁定，并且还未给管理员上供品。') isLogin = False # 结束外循环 elif username in userlist: while True: ... else: # 用户名不存在 ... # 检测用户是否属于锁定状态login() 功能合并 现在，我们把注册和登录功能都分别完成了，那么我们现在就剩下最后一步，将两个功能合并在一起。 注册和登录的函数是现成的，那么现在我们要做的事情就是： 将读取数据的两个with方法封装到一个函数内 定义一个主函数，进入进程后就执行 告知用户选择登录还是注册 在执行登录注册函数之前，初始化数据（加载读取数据的函数） OK，让我们来实现吧： 首先是封装读取数据的函数： 12345678910111213141516171819202122# 定义一个读取所有用户数据的函数def readAllUsers(): # 读取所有的注册信息 with open('./data/user.txt', 'a+', encoding=&quot;utf-8&quot;) as fp: global userdict global userlist fp.seek(0,0) # 调整指针到文件头 res = fp.readlines() # 按照每一行读取所有用户数据 for i in res: # 循环读取每一行数据 r = i.strip() # 处理每一行尾部的换行符 mydict = r.split(':') userdict.update({mydict[0]:mydict[1]}) userlist = userdict.keys() # 读取所有黑名单用户 with open('./data/black.txt', 'a+', encoding='utf-8') as fp: global blackUserList fp.seek(0,0) res = fp.readlines() for i in res: r = i.strip() # blackUserList.append(r) 注意我们这里用了global, 因为我们需要使用外部定义的变量，所以必须要讲变量改成全局变量，改动才会有效。 注册登录函数不用改动，直接原封不动的粘贴过来就可以了。再来我们就需要直接进入主进程进行选择： 123456789101112131415161718192021222324252627282930# 判断当前脚本是否作为一个主进程脚本在执行if __name__ == '__main__': ''' 这里的代码，只有在使用Python解释器直接运行时才会执行 如果当前脚本作为了模块被其他文件导入后使用，那么这个地方的代码不会执行 因此这个地方的代码，适合写当前脚本中的一些测试，这样不会影响其他脚本 ''' # 调用初始化方法，加载数据 readAllUsers() isBegin = True while isBegin: myStr = ''' ====================== ** 登录（0） 注册（1）** ====================== ''' print(myStr) # 让用户选择对应的操作 num = input('请输入对应的序号，体验功能：') if num == '0': login() isBegin = False elif num == '1': register() isBegin = False else: print('后续功能还在开发中。') isBegin = False 这样当我们输入python3 file.py的时候，就会直接进入这个主程序内。然后执行加载数据的函数，打印用户选择内容，然后等待用户选择并执行后续操作，如图： 而这其中我们设定一个变量isBegin来控制循环，每次执行完一次函数就会结束循环，然后需要重新执行。主要原因就是因为加载数据的方法我们写在了while循环外面，所以当我们注册之后再去登录，新创建的用户并不在用户列表内，无法完成登录。所以干脆结束掉之后重新加载数据，就可以执行登录了。 能不能改善呢？可以。只是今天的课程目的已经完毕了，所以当作留给大家的练习题吧。如何优化整个程序让其更合理高效，大家试试看。然后记得在评论区给我留言。 好了，这节课到这里就结束了，相关代码可以在我的仓库里去找到。 下一节课中，我们会讲解模块，系统的内置模块。我们下次见。","link":"/Exercise-register-system/"},{"title":"15. 系统内置模块","text":"Hi，大家好。我是茶桁。 上一节中，在我们的学习到达一个阶段的时候，我们用之前所学过的知识创建了一个简单的注册登录系统。不知道小伙伴们有没有在课后自己实现一遍呢？编程这种事情，还是要多上手多练才行。 那么今天这节课，我们来学习一下Python系统内置模块。 系统内置模块就是安装完Python解释器之后，系统本身所提供的模块。我知道，咱们之前的课程里有学习系统的内置函数，这个模块和函数不是一个东西。模块这种东西，是需要导入后才可以使用的，比如：json, re, os等等。 行，废话不多说，让我们进入正题。 序列化模块 序列化，就是指可以把Python中的数据，以文本或者二进制的方式进行转换，并且还能反序列化为原来的数据。数据在程序和网络中进行传输和存储的时候，需要以更加方便的形式进行操作，因此需要对数据进行序列化。 对数据进行序列化主要有两种方法，一种呢是Python专用的二进制序列化模块：pickle， 还有一种呢，是互联网通用的文本序列化模块json。 pickle 按照官方的定义来讲 pickle实现了对一个Python对象结构的二进制序列化和反序列化 它提供了一些可供使用的函数，下面让我们来一一介绍一下： 12345678import picklemyStr = 'I love you'res = pickle.dumps(myStr)print(res, type(res))---b'\\x80\\x04\\x95\\x0e\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\nI love you\\x94.' &lt;class 'bytes'&gt; 当前是将一段字符串使用dumps()进行了转化，那其他数据类型是否可以呢？我们来一段列表试试看： 123456myList = [1, 2, 3, 4, 5]res = pickle.dumps(myList)print(res, type(res))---b'\\x80\\x04\\x95\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00]\\x94(K\\x01K\\x02K\\x03K\\x04K\\x05e.' &lt;class 'bytes'&gt; 可以看到，依然进行了转化，并且类型还是bytes。其他的诸如字典、元组等都可以进行这样的转化，我们就不一一的在这里展示了。结论为，我们使用pickle.dumps方法可以进行序列化成为一个二进制的数据。 再让我们来看看反序列化的效果，我在源码中还做过一个元组的序列化，并且给res进行了赋值，我们就拿最后一次的结果来做演示（res = b'\\x80\\x04\\x95\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00(K\\x01K\\x02K\\x03K\\x04K\\x05K\\x06t\\x94.'）： 12345res = pickle.loads(res)print(res, type(res))---(1, 2, 3, 4, 5, 6) &lt;class 'tuple'&gt; 可以看到, 之前序列化成二进制数据的元组被loads() 反序列化转化回来恢复成了元组，我们打印其类型，为tuple。 除了以上两个方法之外，还有另外两个方法dump()和load()， 这四者的区别如下： dumps(obj, protocol=None, *, fix_imports=True, buffer_callback=None): 序列化，可以把一个Python的任意对象序列化成为一个二进制，返回一个序列化后的二进制数据。 dump(obj, file, protocol=None, *, fix_imports=True, buffer_callback=None): 序列化，把一个数据对象进行序列化并写入到文件中。注意，demps是返回并不写到文件中，而dump者是写入到文件中。所以多一个必填参数file， 就是写入的文件对象。 loads(data, /, *, fix_imports=True, encoding='ASCII', errors='strict', buffers=None): 反序列化，可以把一个序列化后的二进制数据反序列化为Python的对象。返回一个反序列化后的Python对象。 load(file, *, fix_imports=True, encoding='ASCII', errors='strict', buffers=None): 反序列化， 在一个文件中读取序列化的数据，并且完成一个反序列化。和loads最大的不同是加载的是读取的文件对象file，而不是data。 可以看到，基本上来说，dump和load是对文件进行操作的方法，那能不能使用dumps和loads来完成呢？让我们来试试： 1234567# 定义数据myDict = {'name':'茶桁', 'age':32, 'sex':'male'}# 进行序列化res = pickle.dumps(myDict)# 写入文件with open('./data/data.txt', 'wb') as fp: fp.write(res) 然后我们看，文件夹中确实多了一个data.txt文件，当我想要打开的时候，提示我为二进制文件。 那基本上可以确定，咱们所作的操作确实成功了。 借用其他的支持二进制文件的编辑器打开看看： 再来，我们把一个反序列的二进制文件读取处理，并完成反序列化： 123456789with open('./data/data.txt', 'rb') as fp: res = fp.read()# 进行反序列化myDict = pickle.loads(res)print(myDict)---{'name': '茶桁', 'age': 32, 'sex': 'male'} 以上两个方式，我们其实完全可以使用pickle模块提供的方法来完成，dump和load: 12345678910myDict = {'name':'茶桁', 'age':32, 'sex':'male'}with open('./data/data2.txt', 'wb') as fp: pickle.dump(myDict, fp)with open('./data/data2.txt', 'rb') as fp: newdict = pickle.load(fp) print(newdict) ---{'name': '茶桁', 'age': 32, 'sex': 'male'} 我们又重新创建了一个序列化，保存数据到data2.txt中，然后反序列化再从文件中读取转化。和之前我们用到的方法得到的结果一样，但是方法我们用的却完全不同。 JSON序列化 JSON的全称为: JavaScript Object Notation, 是一个受JS的对象字面量语法启发的轻量级数据交换格式。其在JS语言中是一个对象的表示方法，和Python中的字典的定义规则和语法都很像。 JSON在互联网中又是一种通用的数据交换，传输，定义的一种数据格式。 和之前的pickle序列化方法一样，JSON序列化也有四种函数，其功能基本是一模一样。只是最后转化的数据格式不同： json.dumps(): 完成JSON格式数据的序列化 json.loads(): 完成JSON格式数据的反序列化 json.dump(): 和pickle模块的dump方法一致 json.load(): 和pickle模块的load方法一致 这里，我们先不着急写代码，我觉得需要对JSON简单了解一下，其实很简单，一说就明白了： 我们之前定义了一个字典：myDict = {'name':'茶桁', 'age':32, 'sex':'male'}， 这个格式的数据在Python中是字典，但是在JS中，这个玩意是一个对象(Object)，如果它放在一个.json文件中，这会是正常的json格式的数据。 我们来做一下操作，上几张图就明白了，为了说明，我们创建一个15_json.html文件和15_json.json， 大家来看： 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; ... &lt;title&gt;Document&lt;/title&gt; &lt;script&gt; let person = {'name':'茶桁', 'age':32, 'sex':'male'} console.log(person, typeof(person)) &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;/body&gt;&lt;/html&gt; 我们在脚本中定义了一个person，格式和Python中的字典一模一样，但是在JS中，它被称为对象。我们在浏览器的控制台中打印出来看看： 那如果在一个JSON文件中呢，它就是一个最普通的JSON数据格式。只是稍微需要注意一下，虽然我们这样写并不会报错，但是总会提示格式问题，JSON最正规的写法，是需要用“\"，尽量不要使用‘’。 提前说这么多JSON的知识点，是因为接下来，如果我们没有说清楚，可能正的会分辨不清。好了，让我们转回Python中： 1234567891011import jsonmyDict = {'name':'茶桁', 'age':32, 'sex':'male'}print(myDict, type(myDict))# 使用JSON模块的dumps方法进行JSON格式的转换res = json.dumps(myDict)print(res, type(res))---{'name': '茶桁', 'age': 32, 'sex': 'male'} &lt;class 'dict'&gt;{&quot;name&quot;: &quot;\\u8336\\u6841&quot;, &quot;age&quot;: 32, &quot;sex&quot;: &quot;male&quot;} &lt;class 'str'&gt; 我们在代码中执行了两次打印，第一次是定义完字典之后，第二次是转换为JSON之后。我们可以看到两次打印结果，几乎是一模一样。当然，中文部分转化的比较明显，但是如果我讲name的值设定为英文，比如Hivan， 那么可以说几乎看不出区别。比较明显的，是我们将类型打印了出来，一个是dict, 一个就是str。 当然，和pickle一样，loads方法将会进行反序列化。 12345newDict = json.loads(res)print(newDict, type(newDict))---{'name': '茶桁', 'age': 32, 'sex': 'male'} &lt;class 'dict'&gt; JSON的转化，基本Python中所有的数据类型都可以进行序列化而不会出现报错的情况，但是有些数据是真的转为了JSON格式的数据，但是有的则只是转为了字符串而已。 让我们尝试将一个复杂的结构数据写到一个JSON文件中： 123myList = [{'name':'admin','age':1000, 'sex':'female'},{'name':'du','age':34,'sex':'male'}]with open('./data/15_data.json', 'w') as fp: json.dump(myList, fp) 我们去查看一下文件的内容： 读取的话，当然也和pickle中的用法也是一致的： 123456with open('./data/15_data.json', 'r') as fp: newList = json.load(fp) print(newList)---[{'name': 'admin', 'age': 1000, 'sex': 'female'}, {'name': 'du', 'age': 34, 'sex': 'male'}] 数学与数值 数学模块 Math Python中的内置数学模块Math提供了很多的数学相关运算。整个模块中的方法都非常简单，直接调用就可以了，当然，前提是需要导入数学模块。 下面我们就简单的介绍一下相关方法(当然，其实我们之前已经介绍过一部分了。)： 向上取整 math.ceil() 1234567import math# math.ceil() 向上取整res = math.ceil(3.14)print(res)---4 这个函数让你想到了什么？不知道小伙伴们还记得前面学习的内容不，是不是特别像我们曾经学过的round()方法？不过这两个方法还有不一样的地方，让我们来看： 123456print(round(3.14))print(round(3.5))---34 看到区别了吗？round实际上是一个四舍五入的函数，而ceil只是向上取整 ，则不管你小数点后面的数字大小。当然，有向上取整: math.floor()，这肯定有对应的向下取整，为了很清晰的看出来，我选择了一个向上接近整数的小数： 1234print(math.floor(3.94))---3 接下来是求幂次方: math:pow(x,y)，这个方法会传入两个数值，前一个数值为底数，后一个则为指数，结果是浮点数。 1234print(math.pow(1.2, 2))---1.44 下面一个是求开平方，结果也是浮点数： math.sqrt() 1234print(math.sqrt(4))---2.0 math.fabs()能够计算绝对值,结果是浮点 1234print(math.fabs(-3.14))---3.14 math.modf(): 把一个数值拆分成小数和整数组成的元组 1234print(math.modf(3.1415))---(0.14150000000000018, 3.0) 至于最后的打印结果，不用太纠结。这并不是Python中的BUG，而是与计算机如何处理浮点数有关，因为计算机存储数据实际上都是二进制的，而二进制无法正确处理。 举个栗子：1/3， 在十进制下，这个数是无限循环小数对吧？3.33333333，二进制实际上也有这种问题，比如说1/10, 十进制下是0.1，可是二进制下呢，就成了无限循环小数：0.000111001100110011...。 其实大部分情况之下，这并不影响我们的使用，只是得到的数值需要处理一下四舍五入就可以了，但是也有不适用的情况。实际上，Python中有专门处理精度计算的模块：decimal，这个等我们以后再详细讲。 math.copysign(x,y): 把第二个参数的正负符号拷贝给第一个参数，结果为浮点数： 123456print(math.copysign(-3, 99))print(math.copysign(3, -2))---3.0-3.0 math.fsum() : 将一个容器类型数据中的元素进行一个求和运算，结果为浮点数 123456print(math.fsum((1, 2, 3)))print(math.fsum({1, 2, 3}))---6.06.0 这个方法的参数值需要注意一下，容器中的元素必须是可以运算的number类型。 math.factorial(x): 以一个整数返回x的阶乘 1234print(math.factorial(4))---24 除了运算函数外，还有一些常量函数。最典型的就是数学常数 π = 3.141592...: 1234print(math.pi)---3.141592653589793 基本可以看出来，Python的数学模块基本都属于很简单的工具类函数， 列举几个之后，大家基本上就都能上手了。其官方的文档地址可以看这里：相关文档地址为：https://docs.python.org/zh-cn/3.10/library/math.html#module-math。 随机模块 random 随机模块也是一个比较简单的模块，大部分时候，我们是使用它来产生随机值使用的。 random模块中的random函数直接使用会返回一个0-1之间的随机小数（左闭右开） 什么是左闭右开呢？通俗点说，就是random这个函数，有可能取到0这个值，而无论如何不会取到1这个值。 1234print(random.random())---0.7363473107012012 random.randrange([开始值]，结束值，[步进值]):随机获取指定范围内的整数。对于这种需要开始值，结束值和步进值的参数形式的函数我们应该都已经非常熟悉了对吧？ 这里有三个值，除了结束值是必选之外，另外两个值都是可选值。当只有一个参数时，默认就是从0到整数之间的值，存在两个参数时，就从开始值到结束值之间的随机数，而当有三个参数时，就会按照步进值从开始值到结束值之间产生一个随机数。需要记住一点，这三种参数取值方式，都是左闭右开的形式。也就是说，结束值是不会被取到的。 随机数大量应用在数字验证码，抽奖以及高并发下生成订单号等应用。 12345678print(random.randrange(5))print(random.randrange(3,6))print(random.randrange(4,10,2))---458 random.randint() 会随机产生指定范围内的随机整数 1234print(random.randint(5, 10))---6 可能有的小伙伴会发出疑问了：茶桁老师，你这个解释是不是写错位置了？将randrange的解释直接复制了下来。其实没有，这两个的功能几乎一模一样，说几乎的意思当然是还是有不同点，唯一一点不相同的是，randint产生的随机整数，是左闭右闭的模式，也就是说，它是可以取到结束值的。 当然我们不能只随机整数对吧？实际应用场景中我们也需要大量的浮点数： random.uniform() 获取指定返回内的随机小数, 实际应用中我们需要注意，这个函数是没有开始值和结束值的，只有范围值，也就是说，你最小值和最大值填入的先后顺序无所谓。 那有的小伙伴会想，如果两个值我填入的数值一样会如何？嗯，那就产生一个唯一的浮点值呗。 12345678print(random.uniform(5, 10))print(random.uniform(10, 5))print(random.uniform(5, 5))---9.5402588987387797.3431368358999895.0 random.choice()， 随机获取容器类型中的值。这个函数的应用范围就非常广了，我们在做数据分析的时候经常会用得到。因为大部分时候，我们说面对的应该都是容器类数据。 123456print(random.choice('123'))print(random.choice([1,2,3,4]))---32 random.shuffle() 随机打乱当前列表中的值，没有返回值，仅仅是打乱原数据： 1234567arr = [1, 2, 3, 4, 5]res = random.shuffle(arr)print(f'arr:{arr} \\nres:{res}')---arr:[4, 1, 5, 2, 3] res:None 当然，我说介绍的函数都只是一部分，目的是打个样，让大家知道这些函数是个怎么回事。大部分时候会用到的函数抽出来讲解一下，更多的内容，还需要参考官方文档：https://docs.python.org/zh-cn/3.10/library/random.html#module-random 系统操作相关模块 OS模块 OS模块，就是操作系统接口模块。这个模块提供了一些方便使用操作系统相关功能的函数。我们之前重点学习的open()，就是这个模块中的相关函数。现在让我们来看看除了open之外，还有哪些函数可供我们日常使用： os.getcwd()获取当前的工作目录,注意获取的不是当前脚本的目录 123456import osres = os.getcwd()print(res)---/Users/du/git/AI_Cheats/Python 不过需要注意一点是，这个函数并不是获取现在这个文件的所在目录，而是当前此文件的执行目录。这个怎么理解呢？我给大家举例说明一下： 我们首先需要知道一个，就是我们在Linux中执行cd和pwd的时候，一个是进入某个目录，一个是打印当前目录路径： 那么这里的pwd所执行的结果，是随着进入目录不同而变化的，比如我们进入我们当前的文件目录： 也就是说，我在哪个目录下执行pwd，那么返回结果就是当前执行的这个目录，而不是pwd这个执行文件本身所在的目录。 gwtcwd()文件，和pwd实际上就是相同的特点，如果在当前目录执行这个脚本文件，那么getcwd获取的就是当前的文件目录。如果这个时候我切换到了其他目录，但是写了getcwd()方法的文件没有挪动位置，那么此时我获取的返回值就是我切换的其他目录，而非文件所在的位置。下面我们可以测试一下： 123456os.chdir('/Users/du/git/')res = os.getcwd()print(res)---/Users/du/git 可以看到，我们执行了和上main相同的代码，但是这个时候res接收返回值发生了变化，其原因就是我使用了os.chdir来改变了一下当前的工作目录。不知道大家现在是否能理解gwtcwd()的工作原理？还是无法理解的，可以多自己写一下代码，做做尝试。连我这种笨人之前学习的时候都能很快理解，小伙伴们肯定更没有问题。 刚才我们的实验中，引出了另外一个方法： os.chdir(), 如上所见，其功能就是修改当前工作目录。 下面我们直接介绍其他的函数： os.listdir() 获取当前或指定目录中的所有项（文件，文件夹，隐藏文件），组成的列表 这个方法和Linux中的list命令就十分像了，让我们先将当前工作目录切回我们当前文件本来所在的目录，然后在来执行一下这个函数： 123456os.chdir('/Users/du/git/AI_Cheats/Python/')res = os.listdir()print(res)---['12.ipynb', '10.ipynb', '.DS_Store', '15_json.html', '14.ipynb', '8.ipynb', '14.py', '11.ipynb', 'globals.ipynb', '13.ipynb', 'Kalman.ipynb', '9.ipynb', '15.ipynb', '7.ipynb', '5.ipynb', '1.ipynb', '3.ipynb', '4.ipynb', '6.ipynb', 'data', '2.ipynb'] 可以看到，目录内所有的文件，包括隐藏文件.DS_Store和文件夹data都被放进了一个列表当中。 这是在不指定目录的情况下，默认为当前工作目录，当然，我们还可以指定目录来获取那个目录下的内容： 12345res = os.listdir('/Users/du/AI/')print(res)---['AIGC', '.DS_Store', 'GPT', 'AI_core_competence', 'stable-diffusion-webui'] 这样，我们就获取到了我们希望查找的目录下的所有内容。我们继续： os.mkdir(文件夹路径, 权限) 这个函数用来创建文件夹，其命令 和Linux中是一模一样，功能也是： 1234567print(os.listdir('./data/'))os.mkdir('./data/test', 0o777)print(os.listdir('./data/'))---['black.txt', 'user.txt', '13-2.txt', '13-1.txt', 'data2.txt', 'data3.json', '13-x+.txt', '15_data.json', 'data.txt']['black.txt', 'user.txt', 'test', '13-2.txt', '13-1.txt', 'data2.txt', 'data3.json', '13-x+.txt', '15_data.json', 'data.txt'] 可以看到我们两次打印结果的对比，确实多了一个test的目录。前一个参数很好理解，重点是后一个参数，什么是权限？ 关于系统中的文件权限，我下面所讲的仅限Linux系统，确切的说是unix，因为包括Mac一样通用： 来，我们先进入目录打印出来看看： 我们主要来看一下data目录的drwxr-xr-x，分别来介绍一下： 第一个字母d代表的是一个目录，如果是-呢，这表示这是一个文件 前三位的rwx代表当前目录（文件）对所有人(u)的权限 中间位置的r-x代表的所属组(g)的权限 末尾的三位r-x代表的是其他人(o)的权限 三个位置介绍完了我们来看字母所代表的意义： r,w,x代表不同的操作权限，其中: r就是可读，权限针对文件，表示可以查看文件内容，针对目录，表示可以ls查看目录中存在的文件名称。 w就是可写，针对文件，表示可以更改文件的内容，针对目录，表示是否可以删除目录中的子文件或者子目录。 x是访问权限，针对文件，表示是否可以开启文件当中记录的程序，针对目录，这表示是否可以进入该目录。 那为什么是777呢？那是因为r代表是4, w代表是2, x代表的是1, 那么7就可以理解了，就是所有数值相加的结果。那么为什么是三个7呢？因为这是在设置三个不同目标的权限，三个位数分别是所有人，所有组，其他。 不过，大家还要注意的一点是，无法使用Python去创建一个比自己这个进程权限还要高的文件。 mkdir()方法是只能创建一个文件夹，无法递归创建文件夹，而当我们需要进行递归创建该怎么办呢？也就是说，我们不仅仅是想要创建test文件夹，而是想创建/test/a/b/c/d/e该怎么办？ os.makedirs()可是进行递归创建文件夹。我们先看一下当前目录结构，直接Finder来看吧： 好，让我们执行一下代码： 1os.makedirs('./data/test/a/b/c/d/e/f/') 再来看看目录结构： 这样就能最直观的看到执行这个函数之后的结果了。 在创建完一些无用目录之后，我当然想着是怎么删除它们。 os.rmdir*() 删除空文件夹，比如我们尝试着删除一下刚才我们创建的目录： 1234os.rmdir('./data/test')---OSError: [Errno 66] Directory not empty: './data/test' 报错了，告知我们无法删除一个空目录，原因就是我们在test里创建了好几层文件夹，那现在test肯定不是空目录，那让我们从内层开始试试： 1os.rmdir('./data/test/a/b/c/d/e/f/') 没有报错，应该是成功了，我们来看看： 确实，f文件夹被删除了。不过太烦了，一个个删除到什么时候去了，还不如我到Finder中直接手动删除呢。 os.removedirs()就是一个递归删除空文件夹的函数，我们来试试： 1234os.removedirs('./data/test/')---OSError: [Errno 66] Directory not empty: './data/test/' 居然又报错，告诉我们非空目录，这... 原来，removedirs方法使用必须是从后往前递归的，也就是说，我们需要将需要删除的所有目录的层级关系给到这个方法，在执行过程中，向上递归，路径中的所有空目录都会被删除： 1os.removedirs('./data/test/a/b/c/d/e/') 再执行一次，这回没问题了。从test开始，下层的所有空目录都被删除了。 然后就是删除文件了 os.remove()就是删除文件，为了测试这个方法，我在data目录下创建了一个空文件test.txt 不过在删除之前，我们还是要先用一下这个文件，来看看如何改名： os.rename(): 用于修改文件或者文件夹的名字 1os.rename('./data/test.txt', './data/test_update.txt') 好了，文件用完了，现在让我们删除吧： 1os.remove('./data/test_update.txt') 顺利删掉了刚才创建的文件。 os.system() 执行操作系统中的命令 比如，我们刚才在命令行里执行过ls -al的命令用于查看当前目录下的所有文件及目录，包括其相关权限，那么我们在Python里可以执行吗？来试试看就知道了： 1os.system('ls -al') 执行效果如图： 这个方法实际上不止是让你在Python中执行系统命令用的，可以用于执行其他.py， 也就是Python文件。比如我们创建了一个hello.py文件，里面写了如下代码： 1print('Hello Python。') 那么，我在其他Python文件中使用os.system方法就可以执行这段代码，当然，前提是你得写对路径。 1os.system('/file_path/hello.py') 这样，你在其他文件内写的一个方法就被当前文件执行了。 os.path 路径模块 在Python创建的整个工程或者某一个函数里，路径操作也是经常要做的事情。比如： os.path.abspath() 就是将相对路径转化为绝对路径吗，多数时候，我们是需要获取文件的绝对路径的，更多的是为了获取当前工作目录的绝对路径。 123456print(os.path.abspath('./15.ipynb'))print(os.path.abspath('./'))---/Users/du/git/AI_Cheats/Python/15.ipynb/Users/du/git/AI_Cheats/Python os.path.basename()， 这个方法可以获取到路径后截取返回主体部分，来看代码，一看就明白了： 12345res = os.path.abspath('./15.ipynb')print(os.path.basename(res))---15.ipynb 截取了路径中最末尾的文件名和扩展名，如果路径上最末尾的是一个文件夹不包含文件，那获取的就是那最后一个文件夹名称。 os.path.dirname()， 返回路径中主体部分之前的内容 12res = os.path.dirname('/Users/du/git/AI_Cheats/Python/data/data.txt')print(res) 不过使用这个方法的时候需要注意，如果你填入的是一个相对路径，它并不能打印出绝对路径： 12345res = os.path.dirname('./1.ipynb')print(res)---. join() 链接多个路径，组成一个新的路径 12345res = os.path.join('./data/test/', '2.txt')print(res)---./data/test/2.txt 实际上，它更像是一个字符串拼接，因为这个方法并不会去验证路径的有效性。 split() 这个方法和join()正好相反，用于拆分路径，把路径拆分为路径和主体部分。然后返回一个元组。 12345res = os.path.split('./data/test/2.txt')print(res)---('./data/test', '2.txt') splitext()拆分路径，可以拆分文件后缀名 12345res = os.path.splitext('./data/test/2.jpg')print(res)---('./data/test/2', '.jpg') os.path.getsize()获取文件的大小 , 单位是字节数 12345res = os.path.getsize('./data/15_data.json')print(res)---91 os.path.isdir()会检测是否是一个文件夹，检测其是否存在，返回True或者False 12345res = os.path.isdir('./data/test')print(res)---False os.path.isfile()会检测文件是否存在，一样是返回True或者False 12345res = os.path.isfile('./data/test_update.txt')print(res)---True exists() 是一个通用函数，检测路径是否存在。和以上两个不同的是，也可以检测文件，也可以检测路径： 12345678res = os.path.exists('./data/test_update.txt')print(res)res = os.path.exists('./data/test')print(res)---TrueFalse 当我们有一个相对路径和一个绝对路径，而我们想看看两个路径是否指向一个目标位置的时候，是不是要先获取相对路径的绝对路径之后，再去对比呢？ 其实没有那么麻烦, 只需要os.path.samefile(a, b)就可以了。 1234567a = './data/data.txt'b = '/Users/du/git/AI_Cheats/Python/data/data.txt'res = os.path.samefile(a, b)print(res)---True 使用这个方法，我们需要填入的两个值是真实存在的路径。 当然，官方的文档内还有更多的函数，我这里仅仅是列出了一些常用的。大家可以去官方文档内去看看。 shutil高级操作模块 shutil模块对文件和文件集合提供了许多的高级操作。其中就有支持文件复制和删除的一些功能。 要说，其实shutil这个模块的很多方法和Unix里的shell util都一样，所以会用命令行的小伙伴，对这个模块应该是极其容易上手： shutil.copy()， 一看就明白是干什么的是吧？就是将文件拷贝到指定目录的。 1234shutil.copy('./data/data.txt', './data/test/data.txt')---FileNotFoundError: [Errno 2] No such file or directory: './data/test/data.txt' 报错了，咋回事？看提示，应该目录或文件不存在。嗯，这个方法要操作之前，目标路径中的目录是必须存在的，它无法自动创建目录。 让我们手动创建目录之后再试试， 还记得我们之前创建目录的命令怎么做吗？ 12345os.mkdir('./data/test')shutil.copy('./data/data.txt', './data/test/data.txt')---'./data/test/data.txt' 有返回值了，那我们操作应该完成了。走，去目录里看看： 确实，目录和文件都存在了，这里注意我标注的两个文件，copy这个命令指示拷贝了一个副本到目标目录中，原文件还是存在于原来的位置。但是注意到时间了吗？修改时间被更改了，改为了我们执行当前操作的时间。 copy2是另外一个拷贝方法，它所有功能和copy都一样，但是如果真是一模一样的方法，也就没必要多创建一个了对吧？这个方法最大的不同，就是保留了原文件的信息，包括操作时间和权限等。 再让我们操作一下试试： 1shutil.copy2('./data/data.txt', './data/test/data.txt') 除了这两个拷贝方法外，还有一个拷贝方法copyfile()， 专门用于拷贝文件中的内容，写入到新的文件中去。让我们在./data/test/中新建一个data2.txt文件来接收写入内容： 1shutil.copyfile('./data/data.txt', './data/test/data2.txt') 我们来打开data2.txt之后查看一下内容，确实写入了： shutil.copytree('./a', './b')方法看名字应该就猜到是干什么的，是将整个目录结构全部拷贝到指定目录中。使用的时候要多注意，指定的目标目录必须不存在同名目录。 shutil.rmtree()，我们之前有用到带rm的方法，那么看名字也就知道了，这个方法是删除整个文件夹，包括文件夹下的所有目录和文件，和之前我们使用的removedirs不同，这个方法并不是从下往上递归，而是直接全部删除。让我再创建一次多级目录才测试一下： 1os.makedirs('./data/test/a/b/c/d/e/f/') 目录创建完成后，来让我们将整个test文件夹全部删除： 1shutil.rmtree('./data/test') 执行成功，test整个目录及其内部文件全部删除了。 我们最后再来看一个用的非常多的方法shutil.move()， 我们都知道，windows中有剪切和复制两种菜单命令，然后到新的目标目录后进行粘贴，如果是剪切命令，这原目录中文件会在粘贴完成后删除，而如果是复制，不会执行删除。Linux的逻辑稍微有些不同，是先进行拷贝，然后在目标目录之后决定是粘贴还是移动，如果是粘贴的话就保留原目录的文件，如果是移动，则会在粘贴完成之后在原位置删除文件。 虽然逻辑上有些许不同，但是不管是Windows还是Linux(包括Mac)，如果是移动某个文件的时候都遵循的是先复制一份到目标目录之后，再把原文件删除的先后顺序。 其实move()命令也是一样的逻辑， 基于这个逻辑，move实际上也可以用于修改文件夹或文件的名称。 1shutil.move('./data/data.txt', './data/data_copy.txt') 然后我们去看一下： 可以看到，执行成功了。我们多注意一下就会发现，move命令也有和copy2相同的特性，将原文件的信息保留了下来。 zipfile压缩模块 ZIP文件格式基本是互联网上最通用的一种压缩格式，常见的存档和压缩标准。该模块提供了用于创建，读取，写入，附加和列出ZIP文件的工具。 在日常使用中，我们也会经常用到这个模块的相关功能。和之前介绍方法不同，我们这一部分按需求来介绍： 压缩文件 123456import zipfile, oswith zipfile.ZipFile('temp.zip', 'w') as myzip: myzip.write('./data/data_copy.txt') myzip.write('./data/black.txt') myzip.write('./data/user.txt') 有没有发现，其方法和我们在对文件进行读写操作的时候很像，逻辑就是先创建一个压缩包文件，然后往里面扔入对应的文件。 解压缩文件 压缩之后，我们这次解压缩来看看压缩包内的文件是不是我们刚才扔进去的内容： 12with zipfile.ZipFile('temp.zip', 'r') as myzip: myzip.extractall('./data2') 执行之后结果： 文件的确都是原来的文件。 批量压缩 不过之前压缩文件的时候也太麻烦了，文件一个一个的列出来扔进压缩包，那有没有办法将指定文件夹中的文件全部打包呢？ 当然没问题，既然我们嫌弃手动一个个添加文件名太麻烦，那我们直接用机器添加不就好了，怎么做呢？ 首先第一步当然是获取文件夹下所有的文件，应该还记得listdir()这个方法吧？才学的。 获取列表之后，我们直接用代码一个个的扔到压缩包内就可以了，用for循环吧： 123456789with zipfile.ZipFile('temp.zip','w', zipfile.ZIP_DEFLATED) as myzip: # 获取当前目录中的所有的项 arr = os.listdir('./') print(arr) for i in arr: myzip.write(i) ---['12.ipynb', '10.ipynb', '.DS_Store', '15_json.html', '14.ipynb', '8.ipynb', 'temp.zip', '14.py', '11.ipynb', 'globals.ipynb', '13.ipynb', 'Kalman.ipynb', '9.ipynb', '15.ipynb', '7.ipynb', '5.ipynb', '1.ipynb', '3.ipynb', '4.ipynb', '6.ipynb', 'data', '2.ipynb'] 通过将打印出来的arr我们可以看到所有被成功扔进压缩包的内容。 其他压缩方法 zipfile是Python中内置的专门用于压缩zip格式压缩包的方法，但是其实，我们并不限于压缩成zip格式。那是不是还有rarfile, 7zfile等模块呢？那真是想多了，我们刚才学过的shutil方法，就可以进行压缩操作。不过不一样的是，虽然效果是一样的，但是我们这个方法是创建归档: shutil.make_archive()， 用于创建一个压缩文档。这个方法中有三个比较重要的参数，一个是创建的归档文件名称，第二个是指定的归档格式，第三个则是要归档的文件或文件夹路径。 1shutil.make_archive('temp', 'tar', './data') 成功完成tar格式的归档。 除了以上的这些介绍的内置模块之外，我们平时应用中还会用到许多其他的模块，比如日历模块calendar，时间模块time等等。我上方讲解模块使用的同时，更多的是想向大家传递一个思想就是内置模块基本都很易上手，并且就算一时之间不太明白，可以多看看官方文档。从官方文档上学习是一个很好的习惯。我们可以从官方Python模块索引中去找到自己需要的模块。 好了，那这节课就先到这里了，下一节课中，我们利用日历和时间模块来做一个练习：万年历。大家要提前做预习，去官方文档好好学习一下其相关模块，包括calendar、datetime、time等. 那小伙伴们，让我们下节课练习再见吧。","link":"/System-built-in-modules/"},{"title":"16. 练习：万年历","text":"Hi, 大家好。我是茶桁。 上一节课最后，我让我家去预习一下日历和时间的相关模块，不知道大家有没有去预习。不管如何，这节课，让我们开始做一个练习：万年历。 没有预习的小伙伴也跟着一起，在本次练习完成的时候，相信你会对这些模块有了初步的了解。 好，让我们开始吧。 首先，我们需要来看看calendar.monthrange()这个函数，它属于calendar模块内，返回指定年份和月份的数据，月份的第一天是周几，和月份中的天数。 1234567import calendarres = calendar.monthrange(2023, 6)print(res)---(3, 30) 我们接收了返回值，但是这个3和30分别是什么意思呢？我们打开日历看一下就明白了： 如图所见，2023年的6月份一共是30天，第一天是周四。这也正是(3, 30)的含义。之所以是3而不是4，是因为是从0开始计算的，也就是说，周一是0。比如，2023年5月的第一天就是周一，我们来看看是不是这么回事： 12345res = calendar.monthrange(2023, 5)print(res)---(0, 31) 那有了这个，我们要做一个当月的日历就简单了，还记得我们之前做过一个星星的矩阵吗？是一样的概念，这是这次直接换成了数字而已， 来，让我们从最基本框架开始（还是以6月份数据来做）： 1234567891011121314151617days = res[1]week = res[0] + 1d = 1while d &lt;= days: # 循环周 for i in range(1, 8): print('{:0&gt;2d}'.format(d), end=&quot; &quot;) d+=1 print()---01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 这样，我们就将天数打印出来了。可是，明眼人一眼就看出了问题，这一月只有30天，怎么得到的35天的？让我们来修复一下这个问题： 1234567891011121314151617181920212223days = res[1]week = res[0] + 1print('一 二 三 四 五 六 日')d = 1while d &lt;= days: # 循环周 for i in range(1, 8): # 判断是否输出 if d &gt; days: print('', end='') else: print('{:0&gt;2d}'.format(d), end=&quot; &quot;) d+=1 print()---一 二 三 四 五 六 日01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 我们在代码中加了一层判断，如果循环中的d大于days了，那我们就直接输出空格，否则才正确输出格式化的数字，那么这样就可以不输出31-35了。 完成了，顺便还打印了一行星期几。可是问题是，没有和实际情况对齐对吧？没事，我们继续来改动。 1234567891011121314151617181920212223days = res[1]week = res[0] + 1print('一 二 三 四 五 六 日')d = 1while d &lt;= days: # 循环周 for i in range(1, 8): # 判断是否输出 if d &gt; days or (d==1 and i&lt;week): print(' ', end='') else: print('{:0&gt;2d}'.format(d), end=&quot; &quot;) d+=1 print()---一 二 三 四 五 六 日 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 我们在之前判断d大于days的判断上再加上一层，不仅如此，当d==1并且i小于week的时候，也都是出制表符，那自然最开始和最末尾不该出现数字的地方都被制表符补齐了。 我们再来多做一次实验，将月份改成7月来看看和实际情况是否相符, 并且，这次我们多加一些内容，将其中的年份和月份也都打印出来： 1234567891011121314151617181920212223242526272829303132year = 2023month = 7res = calendar.monthrange(year, month)days = res[1]week = res[0] + 1print(f'========= {year} 年 {month} 月 =========')print('一 二 三 四 五 六 日')print('='*32)d = 1while d &lt;= days: # 循环周 for i in range(1, 8): # 判断是否输出 if d &gt; days or (d==1 and i&lt;week): print(' ', end='') else: print('{:0&gt;2d}'.format(d), end=&quot; &quot;) d+=1 print()---========= 2023 年 7 月 =========一 二 三 四 五 六 日================================ 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 我们来看看实际情况是不是如此： 没错，确实如此。7月份的第一天从周六开始，一个月有31天，周一为最后一天。那说明，我们上面写的内容真实有效。 那现在要干嘛呢？当然是封装成一个函数，以year和month为参数，这样，不管我想要查询任意月份，只要我输入对应参数就可以了： 123456789101112131415161718192021222324252627282930313233def showdate(year, month): res = calendar.monthrange(year, month) days = res[1] # 当前月份的天数 week = res[0] + 1 # 当前月份第一天是周几 print(f'========= {year} 年 {month} 月 =========') print('一 二 三 四 五 六 日') print('='*32) # 实现日历信息的输出 d = 1 while d &lt;= days: # 循环周 for i in range(1, 8): # 判断是否输出 if d &gt; days or (d==1 and i&lt;week): print(' ', end='') else: print('{:0&gt;2d}'.format(d), end=&quot; &quot;) d+=1 print()showdate(2023, 12)---========= 2023 年 12 月 =========一 二 三 四 五 六 日================================ 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 我们尝试调用了一下封装好的函数，输出2023年12月份日历，大家可以看看自己手机里的日历，绝对真实可靠。 好了，现在我们要完成万年历的制作了。 万年历，自然是有一个初始值，那这个初始值必须是当前时间最妥当。不然你们试试打开你们的日历，看是不是打开默认都是指向的「今天」。 那么首先，让我们获取一下当前系统的年月，这个就需要用到我们的time模块里的localtime()方法，其返回参数如下： 1time.struct_time(tm_year=2023, tm_mon=8, tm_mday=13, tm_hour=1, tm_min=50, tm_sec=38, tm_wday=6, tm_yday=225, tm_isdst=0) 那我们如何从中拿到我需要的内容？我们接着看： 12345678910111213141516import timedd = time.localtime()year = dd.tm_yearmonth = dd.tm_monshowdate(year, month)---========= 2023 年 8 月 =========一 二 三 四 五 六 日================================ 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 很明显，我们用year和month两个变量从得到的localtime里获取了其中的年份和月份信息。然后重新调用showdate()封装函数，将其传入。也就打印出了我们当前月份的日历。 可是这都是静态的，我们总不能就只看我们当月的月份。所以，我们接着扩展这个程序。 123456789101112131415161718192021222324252627282930313233343536import time...while True: # 默认输出当前年月的日历信息 showdate(year, month) print(' &lt; 上一月 下一月 &gt; ') c = input('请输入您的选择 &quot;&lt;&quot; or &quot;&gt;&quot;：') # 判断用户的输入内容 if c == '&lt;': month -= 1 elif c == '&gt;': month += 1 else: print('您输入内容错误，请重新输入&quot;&lt;&quot;或者&quot;&gt;&quot;来选择。')---========= 2023 年 8 月 =========一 二 三 四 五 六 日================================ 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 &lt; 上一月 下一月 &gt; &gt;========= 2023 年 9 月 =========一 二 三 四 五 六 日================================ 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 &lt; 上一月 下一月 &gt; 我们在程序运行中没有图形界面，无法接收鼠标信息，那就用输入&lt;和&gt;来代替一下，其逻辑是相同的。 可以看到，我们做了一个判断，当输入&lt;的时候，我月份数字减少，当我们输入&gt;的时候，月份数字增加。所以当我们输入&gt;的时候，表示下一月，数字增加，也就打印出了9月份的月份信息。 可是问题又来了，我们总不能无限加或者无限减下去吧，12月份之后不可能是13月份吧。这又该怎么办呢？ 别着急，我们继续研究下该怎么改善： 1234567891011121314151617181920import time...while True: ... # 判断用户的输入内容 if c == '&lt;': month -= 1 if month &lt; 1: month = 12 year -= 1 elif c == '&gt;': month += 1 if month &gt; 12: month = 1 year += 1 elif c == 'exit': break else: print('您输入内容错误，请重新输入&quot;&lt;&quot;或者&quot;&gt;&quot;来选择。') 既然月份是固定的数字，那就是最好办的，我们让变量控制在范围内不就好了。如果超过数字了，那就改变年份，将月份回滚为最小值或者最大值不就好了。两个简单的if解决了问题。 这就完了吗？并没有。在打印的过程当中，我发现一个问题，就是我们的月份信息不断的叠加，那导致打印区变的过长，最终都没打印完全。这并不是我们想要的，如图： 所以，其实我都还没验证到底12月份之后是否正常变为2024年1月了。忍不了，这个问题也必须要解决。 那如何解决呢？我想起来，在Linux命令中有一个clear命令，其功能就是将当前窗口内容清理掉。那Python中又有很多和系统操作相同的功能，这次有没有呢？就算没有，我记得os.system()似乎可以调用系统命令的。 那，我们试试看： 12345import oswhile True: os.system('clear') # 默认输出当前年月的日历信息 ... 实际操作了一下，无法在Jupyter Notebook中实现，但是当你将代码存储成.py文件之后，在shell中执行是完全可以实现的。如下图： 至此，我们本次的练习「万年历」就完成了。 大家可以下载我的源码来研究，第16课，包含一个.ipynb笔记本文件和一个.py完整文件。 有什么问题，评论区留言。 好了，下课，咱们下节课再见。","link":"/Exercise-perpetual-calendar/"},{"title":"17. 正则表达式","text":"Hi，大家好。我是茶桁。 不知不觉中，咱们针对人工智能的Python课程已经过去了一半。相信大家这段时间也都有所进步了。 今天这节课呢，我给大家划一个重点。不仅仅是Python，很多语言里都是通用的，而且非常的强大。这就是我们的正则表达式。 说起正则表达式，很多程序员其实对其都不是很重视，但是学好它，必定在处理数据的时候事倍功半。虽然内容看似不多，但是市面上有一本经典的「精通电子表达式」整本书还是非常厚的。当然，它比咱们今天要讲的内容详尽的多了。听完我这节课之后想继续研究正则的小伙伴，推荐这本书（唯一推荐）。 正则表达式是什么呢？其实就是使用字符、转义字符和特殊字符组成一个规则，使用这个规则对文本的内容完成一个搜索或匹配或替换的功能。 正则表达式的组成 正则表达式内，包含了普通字符，转义字符，特殊字符以及匹配模式： 1234普通字符： 大小写字母，数字，符号...转义字符： \\w, \\W, \\d, \\D, \\s, \\S ...特殊字符： . * ? + ^ $ [] {} () 匹配模式： I U ... 接下来我们看几个栗子： 1234567891011# 定义字符串myStr = 'iloveyou521tosimida'# 定义正则myReg = 'love'# 调用正则函数方法res = re.findall(myReg, myStr)print(res)---['love'] 这样，我们就匹配了一个字符串。如果我们想匹配数字，那myReg=521就能够匹配了。不过，现在我有一个新需求，我们重新定义一下myStr = 'iloveyou521to123simida'， 我们可以看到，这个字符串十分的混乱，数字和字母都是混在一起的。现在，我就想把数字都单独的拎出来，又该怎么做呢？来，让我们试试看： 12345678myStr = &quot;iloveyou521to123simida&quot;myReg = '\\d'# 调用正则表达式相关函数res = re.findall(myReg, myStr)print(res)---['5', '2', '1', '1', '2', '3'] 这样，我们使用\\d这个转义字符匹配到了字符串内相关的所有数字，返回了一个列表。 可是这还是不符合我们的要求，我们想要的是将其中的数字组合匹配出来，而不是单独的数字。接着继续改： 12345678myStr = &quot;iloveyou521to123si7894mida&quot;myReg = '\\d\\d\\d'# 调用正则表达式相关函数res = re.findall(myReg, myStr)print(res)---['521', '123', '789'] 将原本一个\\d改为了三个叠在一起的\\d\\d\\d， 这样，我们匹配到了三位数字的组合。注意，我在原本的字符串内又加入了一个四位的数字组合7894，但是也只匹配出了789，那也就是说，这种数字匹配方式，有几个转义字符组合在一起，那就匹配出多少位; \\d这个转义字符就是代表单个的数字。 整个代码中，findall就是正则中的相关函数，除了findall之外，还有一些其他函数，我们一起来认识下： re模块的函数 match与search match和search经常是被放在一起来进行讨论的，因为这两个函数很像。具体它们有什么作用和区别呢？我们直接上代码，一点点讲： 123456print(re.match('www', 'www.hivan.me').span()) # 起始位置匹配print(re.match('me', 'www.hivan.me')) # 不在起始位置匹配---(0, 3)None 可以看到，虽然第二段打印中，我输入的me也可以从字符串中找到，但是因为不是从起始位置匹配的，所以返回了None。 再来看看search： 123456print(re.search('www', 'www.hivan.me').span()) # 起始位置匹配print(re.search('me', 'www.hivan.me').span()) # 不在起始位置匹配---(0, 3)(10, 12) search方法中，无论我们要匹配的字符是在起始位置还是结束位置，只要是能找到，都会返回其位置。 有小伙伴们可能会奇怪，我在成功返回的末尾都加了一个span(), 是不是这个影响的原因？我们来看看这两个方法匹配成功后的返回值就明白了： 1234print(re.search('me', 'www.hivan.me')) # 不在起始位置匹配---&lt;re.Match object; span=(10, 12), match='me'&gt; 其完整的返回值应该是这样的，我后面加的span()只是为了获取返回值中的span信息。 所以对于这两个函数，我们可以稍微归纳一下： re.match()函数： 会从头开始进行匹配，如果第一个就符合要求，那么匹配成功 如果第一个不符合规则，则返回None 匹配成功后返回Match对象 成功后可以使用group()和span()方法获取数据和下标区间 re.search()函数： 从字符串的开头开始进行搜索式的匹配 匹配成功则返回Match对象，匹配失败者返回None 成功后可以使用group()和span()方法获取数据和下标区间 两者的区别： match方法是从字符串的开头进行匹配，如果开始就不符合正则的要求，则匹配失败，返回None。 search方法是从字符串的开始位置一直搜索到字符串的最后，如果在整个字符串中都没有匹配到，则失败，返回None 在看完match和search之后，我们再来看看re模块的其他函数： re.findall() 这个函数在文章开头我们就用到了，但是并未给大家进行详解。现在我们就来认识一下： 123456myStr = &quot;iloveyou521to123si7894lovemida&quot;myReg = 'love'print(re.findall(myReg, myStr))---['love', 'love'] 可以看到，和一开头我们所写的不同，这次返回的参数列表内出现了两个love， 原因就是我对myStr这个变量又重新定义了一下，在接近尾部的地方多加了一个love。 从这我们也能看出来了，findall这个函数是按照正则表达式的规则在字符中匹配所有符合规则的元素，结果返回一个列表，如果没有找到的情况下，会返回一个空列表。 re.finditer() 12345res = re.finditer(myReg, myStr)print(res)---&lt;callable_iterator object at 0x107ab3790&gt; 从返回的结果中看到，这个函数返回的是一个迭代器。那让我们利用迭代器规则来试试看： 12345678print(next(res))print(next(res))print(next(res))---&lt;re.Match object; span=(1, 5), match='love'&gt;&lt;re.Match object; span=(22, 26), match='love'&gt;StopIteration: 在第三个next方法的时候报错了，那和我们使用findall结果是一致的，返回了两个love。并且，finditer方法会返回每一个匹配值的下标范围。使用span()来获取到这个范围。 re.sub() 这个函数方法和之前介绍的方法有些不太一样了，以上我们所使用的可以说都是搜索、查找。那这个函数就是修改了。其功能是按照正则表达式的规则，在字符串中找到需要被替换的字符串，完成一个替换。主要参数有三个： pattern: 正则表示的规则，匹配需要被替换的字符串 repl: 替换后的字符串 string: 被替换的原始字符串 1234567myStr = &quot;iloveyou521to123si7894lovemida&quot;myReg = 'love'res = re.sub(myReg, 'live', myStr)print(res)---iliveyou521to123si7894livemida 这样，我们对整个myStr就完成了特定字符串的替换，将其中的love全部替换成了live。 re.split() 这个方法会按照指定的正则规则，进行数据切割。 123456myStr = 'hello my name is chaheng'res = re.split(' ', myStr)print(res)---['hello', 'my', 'name', 'is', 'chaheng'] 在这段代码中，我们将原字符串以空格来进行风格，将整个字符串分割成了一个列表。其原理和我们将字符串时讲到的基本一致，这里就不详细讲解这个函数了。 compile() 这个函数可以直接将正则表达式定义为「正则对象」， 使用正则对象直接操作。 我们现在来看一个示例： 假如说，我有下面这样一个列表： 123456arr = [ 'i love 123 you', 'i love 234 you', 'i love 456 you', 'i love 678 you'] 我现在想要从中找到所有的数字，那么使用之前所学的内容，当然我们想到的一定是for循环。 12345678910for i in arr: newReg = '\\d\\d\\d' res = re.search(newReg, i) print(res.group()) ---123234456678 可以看到，我们确实正确的拿到了arr中的相关数字。 这里，我们稍微讲解一下正则表达式中的\\d{n}， 在正则中，\\d是转义字符我们之前学过了，但是其实，我们并不需要写三个\\d来去匹配三个数字，那如果真是这样的话，我们要匹配几十个数字的时候怎么办呢？这个时候我们可以用到\\d{n}这样的写法，大括号中的n表示的就是前面这个\\d的匹配连续匹配n次。那么，我们原本的newReg='\\d\\d\\d'就可以改为newRge='\\d{3}'。 让我们回过头来继续，刚才我们使用了for循环来完成了依次取值对吧。这个时候，让我们深入search()这个方法的源码去看看，其中是长这样的： 12def search(pattern, string, flags=0): return _compile(pattern, flags).search(string) 那么根据这个return的结构来看，我们是不是可以这样来写： 12345678910myReg = re.compile('\\d{3}')for i in arr: res = myReg.search(i).group() print(res) ---123234456678 确实，我们获得了我们想要的结果。 那上下两种写法的区别在于哪里呢？ 其实，我们第一种写法里，我们定义了一个正则规则，然后传给search(myReg, i)之后，search方法在其内部先是调用了一下_compile, 生成了一个正则对象，在这之后，才又传给了search方法，最后得到结果。 而我们在第二种写法中，直接用compile函数将规则定义成了一个对象，使用search直接得到了结果。 你们注意看结构，是不是我第二个写法里，for循环里的myReg实际上就是search方法内的_compile(pattern, flags)。 那我们这样写有什么意义呢？呃，实际上，从性能上来说虽然是快了一些，但是也不见得快多少。更多的是想让大家养成一个去方法源码中探究逻辑的好习惯。 那么接下来，才是这节课的重点。大家集中注意力，我们开始。 正则表达式的规则 在本文的最开头，我们就先给到了正则表达式的基本规则，我们拿下来再复习一下： 1234普通字符： 大小写字母，数字，符号...转义字符： \\w, \\W, \\d, \\D, \\s, \\S ...特殊字符： . * ? + ^ $ [] {} () 匹配模式： I U ... 那其实，我们之前介绍相关函数方法的时候，所使用到的基本都是普通字符，其中也用到了\\d这个转义字符，明白\\d就是去匹配数字。 普通字符 普通字符实际上就是最简单的匹配方式，你写什么就是什么。可以理解为，我在全文中去搜索一个单词或者数字。而且我们之前也使用过多次了，所以这部分我们就不再继续向西介绍了。 转义字符 转义字符包括：\\w, \\W, \\d, \\D, \\s, \\S ... 什么都是从代码里去理解最直接，让我们先来定义一个字符串待用： 1myStr = 'a2$_ilove5 21you' 然后我们一个一个来看： \\w, 这个转义字符匹配的内容是单个字母、数字、下划线 123456myReg = '\\w'res = re.findall(myReg, myStr)print(res)---['a', '2', '_', 'i', 'l', 'o', 'v', 'e', '5', '2', '1', 'y', 'o', 'u'] 我们看，在最后的结果中，我们匹配到了这个字符串中所有的字母，数字和下划线。其中的特殊字符和制表符都被过滤掉了。 \\W， 注意，现在这个W是大写的。那这个转义字符规则会去匹配单个「非」字母、数字，下划线。啥意思？简单，看代码： 1234567# 转义字符myReg = '\\W'res = re.findall(myReg, myStr)print(res)---['$', '\\t'] 看到区别了吧？和\\w(小写)完全相反，之前匹配到的字母、数字、下划线一个没匹配到，而之前没被匹配到的特殊字符和制表符，则被匹配后组成了一个列表。 \\d，这个转义字符规则会匹配单个的数字。这个我们之前用过了，这里就不演示了。 \\D， 这个转义字符实际也非常简单，就是匹配非数字。注意到了吧？所有的转义字符里，小写字母大写之后，其匹配的内容都是相反的。 123456myReg = '\\D'res = re.findall(myReg, myStr)print(res)---['a', '$', '_', 'i', 'l', 'o', 'v', 'e', '\\t', 'y', 'o', 'u'] 结果也是，除了数字其他的内容都被匹配到了。 \\s， 这个转义字符规则是匹配单个的空格符或制表符。 123456myReg = '\\s'res = re.findall(myReg, myStr)print(res)---['\\t'] 结果却是如此，唯一的制表符被匹配了出来。（初学时，我一度长期混乱的认为\\s是匹配所有字符串，大家别犯我一样的错误。） \\S，那这个大写字母的匹配规则不用说，一定是匹配空格或制表符之外的所有内容： 123456myReg = '\\S'res = re.findall(myReg, myStr)print(res)---['a', '2', '$', '_', 'i', 'l', 'o', 'v', 'e', '5', '2', '1', 'y', 'o', 'u'] 打印结果验证了我们的猜想。 \\w{4}\\d， 基本转义字符我们都介绍完了，这里我们来看看组合在一起会是什么样。其中的\\w{4}大家也应该明白其含义，就是\\w\\w\\w\\w。 123456myReg = '\\w{4}\\d'res = re.findall(myReg, myStr)print(res)---['love5'] 可以看到，这样组合之后匹配出来的就是4个字母、数字、下划线+一个数字。那是不是这次匹配我们也可以写成\\w{5}呢？反正最后匹配出来的love5不也就是5个w的组合么？ 那我们试试看就知道了： 123456myReg = '\\w{5}'res = re.findall(myReg, myStr)print(res)---['_ilov', '21you'] 没想道结果是这样的对吧？这是因为，\\w{5}中，最后一位可以是字母，数字或者下划线都可以，而我们用\\w{4}\\d则是最后一位必须是数字，不能是其他的。 特殊字符 特殊字符包括：. * ? + ^ $ [] {} () 这回，让我们再重新定义一段合适的字符串 1myStr = '@ _2i4l22oveyou' 然后，我们还是一个一个的来看： .，表示匹配单个的任意字符，当然也有例外，就是除了换行符。 123456myReg = '.'res = re.findall(myReg, myStr)print(res)---['@', '\\t', '_', '2', 'i', '4', 'l', '2', '2', 'o', 'v', 'e', 'y', 'o', 'u'] 字符串内的所有内容，我们都匹配了出来，然后组成了一个列表。 *, 这个特殊字符需要和其他的匹配进行组合使用，它表示的是任意次数。具体什么意思呢？我们还是从代码里去看看是如何表现的： 1234567myStr = 'like chaheng @ _2i4l22oveyou'myReg = '\\w'res = re.search(myReg, myStr)print(res.group())---l 这段代码我们应该是比较熟悉了，\\w和search方法我们都已经学过了。那这个其实就是在字符串中从开头去匹配\\w对吧？ 我们再来继续往下看： 1234567myStr = 'like chaheng @ _2i4l22oveyou'myReg = '\\w*'res = re.search(myReg, myStr)print(res.group())---like 我们看结果，这次search在匹配到第一个\\w之后，又继续向后匹配了，直到遇到空格才停了下来。就是因为\\w后面加了一个*, 所以就会一直匹配任意次数，直到\\w不再匹配了才结束。 这里有一个概念，就是*代表的匹配任意次数，为什么我要强调这个呢？就是任意次数其实是包含0次的。也就是说，我们只要使用了*这个特殊字符，那么就算没有符合匹配项，一样是酸是匹配成功了，只是返回的是空而已。我们来看看： 1234567myStr = ' like chaheng @ _2i4l22oveyou'myReg = '\\w*'res = re.match(myReg, myStr)print(res)---&lt;re.Match object; span=(0, 0), match=''&gt; 我们在之前的字符串前面加了一个空格，按道理说是不符合\\w匹配的。那么match从字符串的开头开始进行搜索式的匹配,没有就返回None对吧？可是这次，并没有返回None，我们看到match=''， 也就是说，它匹配成功了，只是成功了0次。所以按照*匹配任意次的规则，它不会返回None。这里比较绕，大家好好理解一下。我们继续。 +， 这又是一个和其他规则配合使用的特殊字符，和*一样，它也表示匹配次数，但是这个表示的是至少要求匹配一次。正好，我们之前改造的字符串最前面多加了一个空格，让我们来看看： 123456myReg = '\\w+'res = re.search(myReg, myStr)print(res.group())---like 按search从字符串的开头开始进行搜索式的匹配，是不是这里我们应该返回l了？然而并没有，却返回了like，这是为什么呢？ 原因就在于+这个特殊字符，它也并不是只拿到第一个就罢休了，和\\w*一样，一直往后匹配，直到第二个空格的时候，才宣告罢休。所以\\+是「至少」匹配一次，0次不干，而1之后如果可以连续，那就继续匹配。 ?, 这个特殊字符的作用是「拒绝贪婪」，看着很特殊是吧？其实就是，在?之前的规则，只要达成即可。 123456myReg = '\\w+?'res = re.search(myReg, myStr)print(res.group())---l 看，我们之前使用\\w+的时候，+这孩子遇到第一个还不满足，非要向后继续拿。可是这个时候我多加了一个家长?，勒令+既然目的已经达到了，就不要再继续了。这就是?字符「拒绝贪婪」的作用。 {}， 这个特殊字符咱们用过了，应该大家也都知道它的含义。就是重复多少次。 123456myReg = '\\w{5}'res = re.search(myReg, myStr)print(res.group())---chahe 从结果我们可以看到，因为前面的like只有四位，并不符合连续匹配五次的标准，所以最后被匹配出来的是chahe。 {}这个特殊字符其实还有一种用法，就是可以给定范围。 123456myReg = '\\w{1,4}'res = re.search(myReg, myStr)print(res.group())---like 我们知道，下标1到下标4这个范围内，正好是like。 ‘[]’， 这个特殊字符代表字符的范围。用于匹配范围内说包含的字符。使用[]我们可以更精准的筛选出我们需要匹配的内容。 12345678910111213141516171819myStr = 'like chaheng @ ShO _2i4l22LoveYou'myReg = '[A-Z]'res = re.findall(myReg, myStr)print(res)myReg = '[a-z]'res = re.findall(myReg, myStr)print(res)myReg = '[0-9]'res = re.findall(myReg, myStr)print(res)myReg = '[_]'res = re.findall(myReg, myStr)print(res)---['S', 'O', 'L', 'Y']['l', 'i', 'k', 'e', 'c', 'h', 'a', 'h', 'e', 'n', 'g', 'h', 'i', 'l', 'o', 'v', 'e', 'o', 'u']['2', '4', '2', '2']['_'] 可以看到，不同的组合匹配出了不同的范围内的单个字符。 [A-Za-z0-9_] 这个组合等价于\\w。 1234567myStr = 'like chaheng @ ShO _2i4l22LoveYou'myReg = '[A-Za-z0-9_]'res = re.findall(myReg, myStr)print(res)---['l', 'i', 'k', 'e', 'c', 'h', 'a', 'h', 'e', 'n', 'g', 'S', 'h', 'O', '_', '2', 'i', '4', 'l', '2', '2', 'L', 'o', 'v', 'e', 'Y', 'o', 'u'] (),这个特殊字符代表的子组，括号中的表达式首先作为整个正则的一部分，另外会把符合小阔中的内容单独提取一份。我们先看一段代码： 1234567myStr = 'like chaheng @ ShO _2ial2345LoveYou'myReg = '\\w+\\d{4}\\w+'res = re.findall(myReg, myStr)print(res)---['_2ial2345LoveYou'] 这个组合我们匹配到了_2ial2345LoveYou， 有且只有这一个组合了。不过这不是我想要的，我想要的是什么呢？是这段匹配出的字符串，并且，我还想要这段字符串中那段数字作为单独的匹配出来。那这个时候我们怎么做呢？ 1234567myStr = 'like chaheng @ ShO _2ial2345LoveYou'myReg = '(\\w+)(\\d{4})(\\w+)'res = re.findall(myReg, myStr)print(res)---[('_2ial', '2345', 'LoveYou')] 我们将用()将前中后包裹了起来，希望得到一个子组，而中间的部分，就是我们想要得到的4个数字的组合。 ^和$这两个特殊字符实际上属于「定位符」，^是匹配输入字符串开始的位置。$是匹配输入字符串结尾的位置。 那我们从一个案例中来了解一下这两个定位符的使用： 12345678myStr = '186301916675'# 定义一个匹配手机号的正则表达式myReg = '^1\\d{10}$'res = re.search(myReg, myStr)print(res)---None 返回了None， 这又是为什么呢？原因就在于，我们用^限制了必须是1开始，而用了$来限制了到结尾必须是后面有十位数字，而我们数一下，我们给定了12位数字，超出了一位，才会匹配不上。 如果我们去掉限制结尾$再来看看： 12345678myStr = '186301916675'# 定义一个匹配手机号的正则表达式myReg = '^1\\d{10}'res = re.search(myReg, myStr)print(res.group())---18630191667 可以看到，我们遵循了开头位1和后面跟十位数字的规则，但是原字符串中多出来的一位数字被直接过滤掉了。那我们并不知道，用户输入的数字中到底是哪个位置多输入了一个数对吧？ 再来看，我们把结尾限制加上，但是开头限制改一个数字： 12345678myStr = '186301916675'# 定义一个匹配手机号的正则表达式myReg = '^2\\d{10}$'res = re.search(myReg, myStr)print(res)---None 因为开头匹配不对，所以返回了None。 所以，争取的匹配方式至少有三个条件： 1开头， 当然，如果能把运营商的所有开头数字都拿到，那我们能够匹配的条件就变多了。 必须全部是数字 必须是11位 这样，我们可以加上开头限制和结尾限制，正好能满足一个简易的手机号匹配规则： 1'^1\\d{10}$' 正则的模式 在正则的模式中，包含了一下几个模式： re.I: 不区分大小写 re.L: 作本地化识别匹配 re.M: 多行匹配，会影响到^个$ re.S：使用.匹配包括换行在哪的所有字符 re.U：根据Unicode字符集解析字符。这个标志影响\\w, \\W, \\b, \\B re.X: 改标志通过给予更灵活的格式以便将正则表达式写的更易于理解。 实际上，我们虽然列出了这么多模式，真正常用的，也就是re.I这个模式： 1234567891011121314# 正常匹配，给定规则大写字母myStr = 'like chaheng @ ShO _2i4l22LoveYou'myReg = '[A-Z]'res = re.findall(myReg, myStr)print(res)# 使用模式're.I'， 其他不变myReg = '[A-Z]'res = re.findall(myReg, myStr,re.I)print(res)---['S', 'O', 'L', 'Y']['l', 'i', 'k', 'e', 'c', 'h', 'a', 'h', 'e', 'n', 'g', 'S', 'h', 'O', 'i', 'l', 'L', 'o', 'v', 'e', 'Y', 'o', 'u'] 可以看到，当我设定了不区分大小写的情况下。字符串中的所有英文字母都被匹配了出来。 练习： 这次，我们和以往不同，将练习和课程放在了一起。至于为什么嘛，只是因为我课程写完之后发现还有时间。^_^ 这次我们作这样一个练习： 1231. 定义一个正则表达式来验证邮箱是否正确。2. 完善手机号码的正则表达式3. 定义一个匹配IP的正则表达式 [255.255.255.254] 验证邮箱 首先让我们来看，邮箱的格式基本包含以下内容： 123456@qq.com 纯数字 chaheng@qq.com 纯字母 chaheng75@126.com 数字加字母 cha_heng@163.com 混合型 chaheng@vip.163.com 多级域名 chaheng@hivan.me 企业邮箱（企业域名） cha.heng@gmail.com 包含特殊字符. 好，让我们来看，我们以@来前后区分，那么我们先看左边，会包含的内容就是数字，字母，下划线，特殊字符，让我们先来写一下规则试一下： [a-zA-Z0-9]+([_\\.][a-zA-Z0-9])* 那么右边的部分呢？ @(\\w)+\\.[a-z]{2,6} 让我们结合其实试试看： 123456789101112def mailReg(mail): myReg = '[a-zA-Z0-9]+([_\\.][a-zA-Z0-9]+)*@(\\w)+\\.[a-z]{2,6}' res = re.search(myReg, mail) if res: return res.group() else: print(res) mailReg('123456@qq.com')---'123456@qq.com' 感觉是OK。那让我们多测试下试试： 12345678910mailReg('cha.heng@gmail.com')---'cha.heng@gmail.com'=========================mailReg('chaheng@vip.163.com')---None 多级域名的测试没有通过，在函数内打印出了None。我们回过头来看看： 1myReg = '[a-zA-Z0-9]+([_\\.][a-zA-Z0-9]+)*@(\\w)+\\.[a-z]{2,6}' 既然是多级域名出现了问题，那问题肯定出现在多出的那一个点上，我们这样改： 1myReg = '^[a-zA-Z0-9]+([_\\.][a-zA-Z0-9]+)*@(\\w+\\.)+[a-z]{2,6}$' 注意，我们加上了^和$符号。 然后我们再重新试试看，这次呢，我不想一个个实验了，让我们来定义一个数组来批量测试： 12345678910111213141516171819202122232425262728293031def mailReg(mail): myReg = '^[a-zA-Z0-9]+([_\\.][a-zA-Z0-9]+)*@(\\w+\\.)+[a-z]{2,6}$' for i in mail: res = re.search(myReg, i) if res: print(res.group()) else: print(res)emailarr = [ '123456@qq.com', 'chaheng@qq.com', 'chaheng75@126.com', 'cha_heng@163.com', 'chaheng@vip.163.com', 'chaheng@hivan.me', 'cha.heng@gmail.com', ' list@gmail.com ']mailReg(emailarr)---123456@qq.comchaheng@qq.comchaheng75@126.comcha_heng@163.comchaheng@vip.163.comchaheng@hivan.mecha.heng@gmail.comNone 这一下子我们就将刚才想到的格式都测试完了，最后我们还特意加了一个不符合的格式来测试下康康是否被过滤了出来。结果说明我们写的正则没有问题。 手机号码 就像我们上面写验证手机号正则提到过的，我们可以定义一个所有运营商可能的开头来做头部验证： 113[0-9],14[5-9],15[0-3,5-9],16[2,5,6,7,8],17[0-8],18[0-9],19[0-3,5-9] 实际上，我们还可以分的更细一点，区分运营商。不过在这个练习中，没必要分的这么细了。 来，我们实现一下： 12345678910111213141516171819202122232425myReg = '^(13[0-9]|14[5-9]|15[0-3,5-9]|16[2,5,6,7,8]|17[0-8]|18[0-9]|19[0-3,5-9])\\d{8}$'phonearr = [ '13728739429', '13128319520', '17729231234', '23210023421', '189232198341', '19123214421']for i in phonearr: res = re.search(myReg, i) if not res: print('手机号码不正确:', i) else: print(res.group()) ---137287394291312831952017729231234手机号码不正确: 23210023421手机号码不正确: 18923219834119123214421 正确的辨认并打印了出来，不正确的也辨认了出来。 匹配IP地址 我们来看看，一个正确的IP地址（IPv4），是由四个三位数来组成的，包含： 10.0.0.0 ~ 255.255.255.255 既然是这样一个格式，我们来思考一下， 首先，我们需要匹配0 ~ 199的范围，也就是0或者1开头,这个比较简单： 1[0-1]?\\d{1,2} 这一段匹配中： [0-1]?表示匹配0或1一次或者零次 \\d就是要匹配任意单个数字。0～9都可以 {1,2}， 给定范围1 ～ 2， 表示前面的\\d出现1次或者2次。 然后我们需要来匹配200 ~ 255范围, 这个范围内比较复杂，包含了两种情况，一种是201 ~250的情况，一种是251 ~ 255的情况 12((5[0-5])|([0-4][0-9])) 这一段匹配，一开表示数字2开头 然后2之后用异或限定了开头可以是5或者可以是0-4之间的任意数字。 是5的话，后面需要匹配0-5，是4的话，后面需要匹配0-9`。 既然一次的匹配已经有了，那剩下的和第一次也都一样，就好些了，我们再多加一个.的匹配： 1((2((5[0-5])|([0-4]\\d)))|([0-1]?\\d{1,2}))(\\.((2((5[0-5])|([0-4]\\d)))|([0-1]?\\d{1,2}))){3} 让我们来试试： 123456789101112131415161718192021222324252627282930myReg = '((2(5[0-5]|[0-4]\\d))|[0-1]?\\d{1,2})(\\.((2(5[0-5]|[0-4]\\d))|[0-1]?\\d{1,2})){3}'ipArr = [ '25.232.123.241', '123.242.211.221', '0.0.123.421', '123.421.4.5', '212.444.523', '0.0.0.0', '214.113.231.256', '54.214.213.265']for i in ipArr: res = re.search(myReg, i) if not res: print('IP 地址不正确:', i) else: print(res.group())---25.232.123.241123.242.211.2210.0.123.42IP 地址不正确: 123.421.4.5IP 地址不正确: 212.444.5230.0.0.0214.113.231.2554.214.213.26 从结果中可以看到，到底还是有漏网之鱼。第三个和最后两个判断都失误了。这是为什么呢？ 似乎最后一位被截断判断了，也就是说，它并没有判断三位数。 哦，我大概猜到了。让我们把头部匹配和结尾匹配加上再试试： 123456789101112131415161718192021222324252627282930myReg = '^((2(5[0-5]|[0-4]\\d))|[0-1]?\\d{1,2})(\\.((2(5[0-5]|[0-4]\\d))|[0-1]?\\d{1,2})){3}$'ipArr = [ '25.232.123.241', '123.242.211.221', '0.0.123.421', '123.421.4.5', '212.444.523', '0.0.0.0', '214.113.231.256', '54.214.213.265']for i in ipArr: res = re.search(myReg, i) if not res: print('IP 地址不正确:', i) else: print(res.group())---25.232.123.241123.242.211.221IP 地址不正确: 0.0.123.421IP 地址不正确: 123.421.4.5IP 地址不正确: 212.444.5230.0.0.0IP 地址不正确: 214.113.231.256IP 地址不正确: 54.214.213.265 这回没问题了。我们的正则匹配算是完成了。 那这节课下来之后，大家要多去理解，多去练习。这节课对于打算玩数据的人正的很重要。 好了，那我们下节课再见吧。","link":"/regular-expression/"},{"title":"18. Python中的模块与包","text":"Hi, 大家好。我是茶桁。 这一段Python之旅怎么样？还算顺利吧？ 之前我们都学习了些什么？有基本常识，流程，函数，不同类型的数据以及一些模块对吧？并且还做了一些练习来巩固所学过的内容。 那么今天，我们接着来学习模块。不过今天要学的模块和以往不太一样了，以前我们学习的都是Python内置的一些模块，而今天呢，我们自己来打包模块。 模块 简单点说，当我们定义一个Python文件，其后缀名为.py的时候，那么这个文件就被称为模块。 模块中通常呢会定义一些相似的类、函数等代码内容，提供给别的程序引入使用。那对于应用，之前我们已经用过很多次了对吧？我们曾多次应用系统模块来使用，那这次，我们还是从系统模块开始吧。 系统模块 系统模块实际上就是一个Python的程序脚本，专门提供给我们自己的程序使用。它们是在安装好Python环境时，就已经存在的，需要的时候可以使用 import 导入到程序中使用。比如： 1import os, re, time, json, calendar 自定义模块 那知道了系统模块是什么东西，在理解自定义模块就轻松多了对吧？其实就是我们自己创建一个Python脚本，定义一些类或方法，供别的脚本导入后使用。 由于本节课比较特殊，所以课程源码除了18.ipynb这个笔记本文件之外，还有有一个文件夹，路径为./Python/packages/file，然后内部会有多个.py文件。 比如我们定义一个self.py文件如下： 123456789101112# self.py# 定义类class MyException(): pass# 定义函数def func(): print('我是一个模块中的func函数')# 定义变量myStr = 'iloveyou' 然后让我们在笔记本中引用这个文件（模块）以及其他模块，让我们来看看，还记得我们是怎么引入模块的嘛？来，回忆一下： 123456# 先引入一个系统模块：timeimport timeprint(f'time:{time.time()}')---time:1692005247.144672 我们引入了一个系统模块time，然后执行了一下模块里的time()方法，并把最终结果打印了出来。 既然都已经有例子了，那我们有样学样来试试引入我们自己创建的文件： 1234import self---ModuleNotFoundError: No module named 'self' 报错了，告诉我们并没有self这个模块。这个... 还记得我们刚才说过的文件路径嘛？./Python/packages/file，而我们当前文件18.ipynb是放在Python目录下的，层级关系如下： 123456- .- Python/- ...- 18.ipynb- packages/ | - self.py 也就是说，我们要应用self.py， 需要找对路径才行。那我们将路径加上去： 12# 引入自定义模块import packages.self 这回执行之后是没报错了，应该没问题了。 那下面呢，让我们来操作一下文件内的类、函数之类的试试： 123456# 使用模块中定义的类obj = packages.self.MyException()print(obj)---&lt;packages.self.MyException object at 0x10468cb80&gt; 没毛病，确实获取到了相关类病打印了出来。 可是我们这也太麻烦了，每次使用这个模块不是抖要输入这么长一段吧？packages.self.xxx， 不知道之前的学习中大家有没有注意到一个关键字as，这个我之前课程中都没有特意讲解过，但是在我们引入模块的时候会经常的用到。所以这里顺带讲一下吧，比如，我们在操作文件的时候有如下代码： 12with open('./file', 'w') as fp: fp.read() 那这个as我们能猜到是什么作用吗？其实，就是讲as前的内容放入as后面的这个变量里，然后将as身后的这个变量改为一个对象而已，在这段代码里，我们打开了文件，并且将其放入了fp这个变量里，变成了一个fp对象。也可以理解为，我们将as之前的内容起了一个别名。 那么我们导入文件的时候可以这么操作吗？我们试试看： 123456import packages.self as selfobj = self.MyException()print(obj)---&lt;packages.self.MyException object at 0x1046ce2c0&gt; 嗯，看来我们没搞错，确实可以这么用。 那让我们再来试试文件中的那个函数吧，函数内应该是执行了一段打印方法： 1234self.func()---我是一个模块中的func函数 确实正确执行了。这也太顺利了，趁热打铁，让我们再来获取其中的变量： 1234print(self.myStr)---iloveyou 导入模块其实不是仅可导入模块，还能从一个模块中导入类，方法甚至是变量： 12345from packages.self import funcfrom packages.self import myStr as strfunc()print(str) 应该能看出这一段代码的含义吧？就是from（从）一个模块中import导入一个对象。 模块中的测试代码 在自定义模块中，通常我们只是去定义类或函数，变量等，并不调用。如果在指定模块中，想要写一些测试代码，在当前模块作为主程序使用时执行，而作为模块被别的程序导入时不执行，那么可以把测试代码写到下面的代码块中： 12if __name__ == '__main__': print('这个位置的代码只有当前脚本被直接运行时才会运行。') 那么这个模块再被别的程序调用之后，这段代码中的程序是不会被执行的。因为只有这个模块作为主程序运行时才会运行这段代码。我们来看下面这些操作就明白了： 12345import packages.self as selfself---&lt;module 'packages.self' from '/Users/du/git/AI_Cheats/Python/packages/self.py'&gt; 按道理，我们引入模块之后应该会拿到该模块内的所有方法，可是刚才我们写的打印并没有被执行。现在我们在命令行内直接大概这个.py文件来试试： 能看到，if里面的print被直接执行了，打印出了里面的字符串。 在这整段代码中，__name__是一个特殊的变量，这个变量在当前脚本作为模块被别的程序导入时__name__的值是当前这个模块的名称，也就是说，我在笔记本中导入的时候__name__就是self， 而我们在if条件中的设定，是只有当前脚本被作为主程序直接由Python解析时才会进入判断，也就是__name__这个变量的值为__main__时。 我们来看看是不是如此，我们在self.py中加上一段代码： 12name = __name__print(f'__name__: {name}') 然后我们直接让self.py在Python解释器中运行： 现在让我们在笔记本中重新引入一下模块中的变量name，再打印出来看看： 打印的第一段内容为引入模块的时候，模块内的print(f'__name__: {name}')执行了一次，第二段内容则是在笔记本中输入的方法print(name)。 这样，我们就很直观的看到了__name__在不同位置时存储了不同的值。 我们在写程序的时候要记得，不要想着把所有的方法定义在一个脚本文件内。 包 那什么是包呢？包并不是模块。你可以将包理解为一个文件夹，这个文件夹里面包含了多个Python文件。 包的结构 12345678910'''package/ # 包(文件夹)├── __init__.py # 包中的初始化文件├── a.py # 包中的模块├── b.py└── ps/ # 子包 ├── __init__.py ├── c.py └── d.py''' 包的使用方法 其实，我们在刚才所讲的内容中，已经给大家演示过了包的使用方法，不知道小伙伴们能不能反应过来到底是哪里？不知道也没有关系，让我们从头来好好的盘一下这件事。 我们之前在当前目录下创建了一个文件夹packages， 里面有我们self.py文件。实际上，这就是一个包了。 让我们将这个包搞的复杂一点，按照上面我们写的结构来增加一些文件，然后我们看看现在的目录结构： 我们可以看到，除了我们之前设定的文件之外，还有多出来一个文件夹__pycache__以及文件self.cpython-310.pyc， 这个文件夹和文件是当这个包内的文件存在引入关系的时候，自动生成的缓存文件。大家可以不用管。 下面我们来看具体的包使用方法，我们预先在a, b, c, d这四个文件内都写入了一模一样的代码： 12def funca(): print('a.py') 当然，方法名和打印的内容都和文件同名的。 然后我们回到18.ipynb这个笔记本文件内，开始操作使用： 12345import packages as papa.a.funca---AttributeError: module 'packages' has no attribute 'a' 似乎并不行，我们好像并不能引用包来直接使用。那我们怎么办呢？前面我们介绍过一个引用的方法from ... import，我们在使用包内的模块时，需要这样去引用。 1234567from packages import a, ba.funca()b. funcb()---a.pyb.py 可以看到，这回我们引用成功了。那我们之前也学到了，在引入模块的时候，也可以直接就引用模块内的方法和变量，模块在包内也可以如此使用： 12345from packages.a import funcafunca()---a.py 那既然我们得到了这种方式来导入模块内的内容，同样的，包内层如果还存在一个包，而我们要使用子包里的模块，也是这样的导入方法： 12345from packages.ps import cc.funcc()---c.py 看到了，同样能够正常使用。 那如果再过分点，我们要想导入c.py里的函数可以吗？试试就知道了, 再使用.多链接一层： 12345from packages.ps.d import funcdfuncd()---d.pys 呐，完全没问题。 然后我们再反过来看最开始，其实呢，我们的第一种方法直接引用包不是不可以，这需要用到我们这个包内的__init__.py文件。 __init__.py是一个包内的初始化文件，可以说，没有这个文件，这只是一个文件夹，只有有了这个文件，这才是一个包。在初始化的时候，就把包内的模块导入一次，在__init__.py中写下以下代码： 1import a 然后我们再回到笔记本文件中直接导入包来使用试试： 12345import packagesa.funca()---a.py 这样就可以了。 好了，那如果这个时候我packages这个包里一大堆的模块，我不想一个个的来导入，有什么办法吗？也是有的，我们需要用到__all__这个参数，在__init__.py中将包内所有的模块名做成一个列表，然后赋值给__all__这个变量，那么我们在引入包内的模块的时候，就可以使用`*来代表所有文件： __init__.py文件： 1__all__ = ['a', 'b'] 然后进行引入： 12345from packages import *b.funcb()---b.py 这样，我们就一次性导入了packages这个包里的所有文件。 导入方式的分类 之前我们讲的内容中，把导入的方式都过了一遍。到现在这个位置，我们应该总结一下了。 具体的导入方式，我们可以将其分为两个类别，分别是绝对导入和相对导入。那两者有什么区别呢？ 绝对导入 绝对导入的方式会使用「搜索路径」去查找和导入指定的包或模块，包括以下几种方式： import module 导入模块 import package 导入包 import package.module导入包.模块 from module import func 从模块中导入函数 from package import module从包中导入模块 from package.module import func 从包.模块中导入函数 关于「搜索路径」，我们先简单的理解一下就是，从当前文件夹中去找，如果找不到，就会去Python的安装环境中去寻找。 相对导入 ⚠️ 相对导入智能在非主程序的模块中使用，不需要直接运行的模块文件。比如： from .包名/模块名 import 模块/内容 from ..包名/模块名 import 模块/内容 .和..我们之前已经了解过了，.代表的就是当前这一级，..代表的就是上一级。 举个栗子好理解：假设我们现在去修改一下ps/c.py这个文件，在这个模块中如果需要当前包中的d模块: 1from .d import funcd 注意啊，我们这个时候不要在c.py中直接运行funcd()方法，这样会导致报错： 1ImportError: attempted relative import with no known parent package 那我们需要怎么运行呢？我们需要讲c.py导入到其他文件中再执行。比如我们进入到笔记本18.ipynb中导入执行。 12345import packages.ps.c as cc.funcd()---d.py 然后让我们再在c.py中加上一段内容： 1from ..a import funca 小伙伴们应该都看出来了，我是在引用c.py的上一级的a.py。 让我们再在笔记本中执行一下试试： 1234567import packages.ps.c as cc.funcd()c.funca()---d.pya.py 这样，在我们引入了模块c之后，我们同时也拥有了c.py引入的同级和上一级中的d.py、a.py。 搜索路径 刚才我们简单提到了一下「搜索路径」， 这里我们详细的来展开说一下。 「搜索路径」就是在导入模块或者包的时候，程序查找的路径。主要的搜索路径包含以下三部分： 当前导入模块的程序所在的文件 python的扩展目录中 python解释器指定的其它 第三方模块位置 /lib/sitepackages 当然，如果你像我一样，系统中安装了多个Python版本，并且使用虚拟环境。那么你的「搜索路径」就不一定是在哪里了。那么我们到底该如何查找呢？我们来看一下以下查找方法： 123456# 在当前脚本中查看包或者模块的搜索路径import sysprint(sys.path)---['/Users/du/git/AI_Cheats/Python', '/Users/du/miniforge3/envs/glm/lib/python310.zip', '/Users/du/miniforge3/envs/glm/lib/python3.10', '/Users/du/miniforge3/envs/glm/lib/python3.10/lib-dynload', '', '/Users/du/miniforge3/envs/glm/lib/python3.10/site-packages'] 我们看到找到的搜索路径被以列表的形式呈现出来。当然，我们找到搜索路径后，其实是可以向其中添加一个的。 1sys.path.append('/Users/du/AI/GPT') 单入口程序 那什么是单入口程序呢？顾名思义，这种程序就只有一个入口。那单入口程序就是指整个程序都是经过一个主程序文件在运行，其它程序都封装成了包或模块。 单入口文件是作为程序直接被运行的唯一文件，其他都是作为模块或者包，被导入单入口中去执行。打个比方说，我们要去做一个ATM机的程序，我们来实现一个单入口程序。那么可能的情况如下： 1234567891011ATM/|-- main.py # 当前程序的主入口文件，单入口文件,唯一直接运行的文件|-- package/ # 主要程序模块包|-- |--- __init__.py # 包的初始化文件|-- |--- View.py # 视图函数模块|-- |--- Controller.py# 控制器模块|-- |--- Card.py # 银行卡模块|-- |--- User.py # 用户模块|-- databases/ # 数据存储文件夹|-- |-- user.txt|-- |-- user_id_card.txt 那么这个程序中，main就是程序的主入口文件，会被直接作为主程序运行。所以main.py文件必须使用「绝对导入」的方式。 好，那讲到这里，我们今天的内容也就结束了。不知道小伙伴们理解了多少？ 本节课也不太好放练习，那我们这节课就免了。下去之后，大家去拉取我的源码好好的研究一下引入关系，然后讲包、模块的概念好好的理解透。 那小伙伴们，咱们下节课再见。","link":"/Modules-and-packages/"},{"title":"19. 第三方库的管理和虚拟环境","text":"Hi， 大家好。我是茶桁。 在我们之前的课程中，讲解了数据，函数，类，模块以及包。这些基本上已经构成了Python的全部了。 那么，我们在学习Python的包之后，有没有思考过，既然Python有内置模块，我们也可以自己写一些模块来使用，那一定有很多第三方写过相应的模块来供我们使用。那么，这些包该如何去找，找到以后如何使用和管理呢？今天，就让我们来看看这个问题。 第三方库的管理 现在很多编程语言都有第三方库的提供，比如Ruby, Node等。而Python的生态也是发展的最好的之一。Python中比较牛逼的地方就是由大量的第三方库提供给你使用。生态的蓬勃发展也是Python广为流行的最大的原因之一。 Python的第三方库的管理网站：https://pypi.org/。 如何安装第三方库？ pip就是Python得包管理工具，解决了包直接的依赖关系，可以方便的管理第三方库（包）。类似于PHP中的Composer, 或者Nodejs中的npm, 又或者Mac中的Homebrew。 我们可以使用pip install 包名（库名）来进行安装。而如果是有多个Python环境的情况下，可能需要使用pip3。比如说，我们要安装pymysql这个库： 1pip install pymysql 在安装命令过程中，有的时候我们可能对版本会有一定的要求，并不是越新的版本越好。这个时候，我们也可以安装指定版本的包 1pip install 包名==版本 现在包都已经安装到本地了，可是因为安装的内容太多，我们可能有的时候会忘记自己以前是否安装过这个包。为了避免重复再装一次，我们可以搜索一下看看： 1pip show 包名 这样，这个包的所有信息就会打印出来供我们查看： 当然，我们也有类似于想要查看本地安装的所有包的需求： 1pip list 这样，我们就可以把本地说安装的包名以及版本都列出来进行查看： 这些呢，就是我们在包管理经常用到的一些命令。 等等，大家在执行安装的时候，一定会遇到安装特别缓慢的情况。多数时候可能是因为我们所在的环境因为各种原因连接不上官方的源服务器。 不过别着急，我们可以切换到镜像源上，找一个速度快的来下载安装。 目前国内的安装源有以下几个可供选择： 阿里云 http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 豆瓣(douban) http://pypi.douban.com/simple/ 清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/ 中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/ 修改源的方式也很简单，包括了「临时修改」和「永久修改」两种。 临时修改，顾名思义，就是我们有的时候临时需要切换到其他源上进行下载了。 我们可以使用pip的时候在后面加上-i参数，指定pip源： 1pip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple/ 多数情况下，我还是习惯于将源永久修改为一个速度较快的镜像上，也懒得每次都输入长传的命令。那么永久修改的方式稍微复杂点，我们分系统来看： 在Linux中，我们找到pip.conf这个文件，一般来说，它的位置应该是~/.pip/pip.conf，如果没有的话，那就创建一个，然后写入如下内容： 1234[global]timeout = 6000index-url = https://pypi.tuna.tsinghua.edu.cn/simpletrusted-host = pypi.tuna.tsinghua.edu.cn 在Windows内原理也是一样的，不同的是路径和文件有所不同。 我们在user目录中创建一个pip目录，如：C:\\Users\\du\\pip, 然后在PIP目录下新建一个pip.ini文件，然后写入内容： 1234[global]timeout = 6000index-url = https://pypi.tuna.tsinghua.edu.cn/simpletrusted-host = pypi.tuna.tsinghua.edu.cn 这样，我们在之后安装第三方库的时候就会发现，速度快多了。 虚拟环境 虚拟环境呢，就是在当前的系统环境中，去配置另外一个Python得运行环境。我们理论上是可以创建多个不同的虚拟环境的，Python得虚拟环境相互独立，互相之间不会影响。 那么虚拟环境下呢，具有以下一些特点： 虚拟环境中可以在没有权限的情况下安装新的库（Linux系统中可能会出现的问题） 不同的应用可以使用不同的库或不同的版本。 虚拟环境中的库升级也不影响其它环境 虚拟环境可以作为一个项目的专有环境。在需要部署时，一键导出项目的所需要的包 默认方式 Python本身就支持创建和管理虚拟环境。可以使用以下方式进行创建： 1python -m venv 虚拟环境名 创建完成后，我们可以使用下面的方式进入虚拟环境，激活虚拟环境 Linux 12# 使用source命令去执行v1/bin/目录下的activatelocalhost:code yc$ source v1/bin/activate Windows 12345# Windows系统需要进入v1/Scripts/这个目录cd v1/Scripts/# 运行activate.bat文件activate.bat(v1) F:\\code 在这之后，我们现在就处于某一个虚拟环境中了，可以执行安装等操作： 123pip install pymysqlpip show pymysql 那么如果我的某一个环境被我给搞乱了，我也找不到原因，还不如重新安装来的快。这会儿我们可能就想要退出并删除这个虚拟环境： 如果想要退出虚拟环境，在Linux中，我们可以输入下面这个命令： 1234# 退出虚拟环境# Linuxdeactivate 那如果是Windows中呢就比较简单了，直接Ctrl+C就好了。 在退出之后，我们直接删除虚拟环境的文件夹，就搞定了。 以上是我们不想要当前虚拟环境的情况下，那也有一种可能是我们需要更换电脑等原因，但是我想带着我的环境一起迁移，这该怎么办？ 这也好办，我们可以到处当前环境所有安装过的包： 1234567891011# 查看所有安装的包pip list```package version------------------------Numpy 1.3.1........# 导出所有包到文件pip freeze &gt; ./requirements.txt 然后在下一个环境中，我们直接执行安装文件内所有包就可以了： 1pip install -r requirements.txt Conda 虽然Python中已经有了包管理的方法，但是还是不得不说，有时候第三方提供的方案确实香。 目前，我现在都是使用conda(Andaconda)来管理我本地的虚拟环境。其使用也是非常的简单了，我们可以去其官网上（https://www.anaconda.com/）下载并安装对应自己系统的版本。 安装完成后，Conda就会创建一个默认的base环境，我们之前一直使用的Jupyter Notebook也一并是安装在环境中的。 那么在Conda中，我们经常会用的命令如下： 1234567891011121314151617# 安装包conda install 包名称# 安全方式安装包conda install -c conda-forge 包名称# 创建环境conda create --name 环境名 python=3.10 #最后是输入当前环境要用的Python版本# 切换（激活）环境conda activate 环境名# 查看环境列表conda info --env# 删除某个环境conda remove --name 环境名 --all 对比下来，conda真的是非常的方便。推荐大家使用。 那么，今天的课程就到这里结束了。我知道，今天的课程似乎显的特别的少。em....不是似乎，是确实。 原因在于这一部分必须拿出一个章节来介绍一下，否则大家平时在使用的过程中万一将自己的默认环境搞乱了，但是又不知道该怎么办，确实挺烦人的。所以我专门拿一节出来，将环境的问题好好的跟大家介绍下，顺便也是告诉大家，不管你做什么实验和操作，尽量新建一个环境来操作。这样，即便你把当前环境搞的乱七八糟无法恢复，删了就好了。 好，那我们这节课到这里也就结束了，咱们下节课讲讲如何处理异常。","link":"/Third-party-libraries-and-virtual-environments/"},{"title":"20. 异常处理","text":"Hi，大家好。我是茶桁。 在我们日常使用Python或者其他编程语言的时候，不可避免的都会出现报错和异常。那么，我们今天就来谈谈异常。 什么是异常？ 异常异常，根据名字简单理解，那就是非正常，也就是没有达到预期目标。 异常呢，其实就是一个事件，并且这个异常事件在程序的运行过程中出现，会影响程序的正常执行。而一般来说，异常被分为两种： 语法错误导致的异常 逻辑错误导致的异常 比如： 12345varlist = [1, 2, 3]print(varlist[3])---IndexError: list index out of range 这个时候，系统抛出了异常，提示我们列表索引超出范畴。 这里我们需要知道，「异常」在Python中实际上也是一个对象，表示一个错误。当我们的程序无法继续正常进行时，就会被抛出。 我们来完整的看看这个报错信息： 1234567---------------------------------------------------------------------------IndexError Traceback (most recent call last)Cell In[2], line 2 1 varlist = [1, 2, 3]----&gt; 2 print(varlist[3])IndexError: list index out of range Python在遇到异常之后，首先会给出一个「错误回溯」， 然后给出具体哪一句代码出现了问题。 然后在最后给出异常分类和解释。那么IndexError告知我们，这是一个「索引错误」，并且给出了具体的描述「列出索引超出范围」。其中IndexError是我们的异常类， list index out of range是我们的异常信息。 在程序运行过程中，会出现各种各样的异常类，常见标准异常类，我放在最下面作为一个附录。 如何处理异常 可预知 如果错误发生的情况是我们可以预知的，那么就可以使用流程控制进行预防处理。比如，两个数字的运算，其中一个不是数字，运算就会出错，这个时候就可以判断来预防： 123456789n2 = '3'if isinstance(n2, int): res = 10+n2 print(res)else: print('非整型。')---非整形 在这一段代码中，我们使用isinstance方法来检测第一个参数是否是第二个参数的所属类型。这是一个用来检测的方法，返回True或者False。那我们在if中，只有真才会打印结果，假则会打印另外一则消息。 有些小伙伴会想，那既然知道不是整型就会出错，那前面限制传如整型不就好了，干嘛还要费劲去做非整判断。 你要知道，很多时候一个程序的编写和维护并不是单一一个人来做的，即便是一个人在做，也不能完全保证自己某个地方埋下了隐患。那么在每一段代码中，我们对可能预知的情况做妥善的预防是必须的。 不可预知 那可预知的情况我们避免了，可是在我们编写代码的时候，更多的情况是我们自己都不知道我们到底埋了什么雷，哪一段没有遵循规则或者逻辑。那这种情况就是不可预知的。 对于这种不可预知的情况我们该怎么办呢？我们又没办法预先判断。那这种情况下，我们可以使用try...except...语句，在错误发生时进行处理。相关语法如下： 123456try: 可能发生异常错误的代码except: 如果发生异常这进入except代码块进行处理异常被捕获之后程序继续向下执行 我们来看个示例，比如我们之前做过的一个注册、登录练习。其中我们有一段代码是要去读取列表中的所有用户。之前我们的练习中，有提到过文件不存在的情况，所以我们使用了a+的方法，当文件不存在的时候，就新建。 那么现在，我们假设我们就用了r的方法，当文件不存在的时候，一定会报错对吧？这个时候，我们可以使用两种方式来进行处理。 第一种方式，就可以在读取前先判断当前文件是否存在。 第二种方式，就可以使用try...except...在错误发生的时候进行处理。 那么这里，我们用第二种方式来做一下处理： 12345678910111213# 假设我们读取的文件不存在，会发生错误try: with open('./data/user5.txt', 'r') as fp: res = fp.read() print(res)except: print('文件不存在。')print('程序继续运行...')---文件不存在。程序继续运行... 可以看到，我们准确的捕获了错误，并且之后程序仍然继续往后执行了。 ⚠️ try...except... 是在错误发生后进行处理，并不是提前判断。也就是说，错误其实还是发生了。这和if实际上有根本性的区别。 try...except... 详解 首先，我们认识try...except的一个特性，就是它可以处理指定的异常，如果引发了非指定的异常，则无法处理。比如，我们下面人为制造一个异常： 12345s1 = 'hello'int(s1)---ValueError: invalid literal for int() with base 10: 'hello' 可以看到，我们这一段代码引发了一个ValueError异常。 现在我们来捕获一下, 但是这次，我们为这个异常指定一个异常类再来看看，先看看正常状态下： 12345678try: s1 = 'hello' int(s1)except: print('程序错误。') ---程序错误。 接着我们来看指定异常之后： 12345678try: s1 = 'hello' int(s1)except IndexError as e: print('程序错误。') ---ValueError: invalid literal for int() with base 10: 'hello' 这里我们指定了一个IndexError的异常类，显然我们之前看到了，程序报错是ValueError异常类，两者并不匹配。所以最后依然还是报错。 那么之前我们谈到过标准的异常类，并且也知道异常实际上也就是一个对象。而我们平时在使用的时候，except实际上就是去这个「标准的异常类」的列表里去查找，如果没有对应的异常类，它依然是无法捕获的。不过大部分时候，我们基本不会遇到标准异常类之外的异常。而这种处理指定的异常类的特性，平时也可以被我们使用。 其中一个使用方式，就是进行多分支处理异常类，不同的异常可以走不通的except进行处理： 123456789101112s1 = 'hello'try: s1[5] # IndexErrorexcept IndexError as e: print('这里是IndexError', e)except KeyError as e: print('这里是KeyError', e)except ValueError as e: print('这里是ValueError', e) ---这里是IndexError string index out of range 是不是和if...elif的分支形式很像？ 让我们继续，在我们说指定的异常类中，实际上会有一个万能的通用异常类。那就是Exception。 12345678s1 = 'world'try: int(s1)except Exception as e: print('Exception ===',e) ---Exception === invalid literal for int() with base 10: 'world' 基本上所有的异常，都可以走到这个异常类。在这段代码中，我们之前记得int(s1)是属于一个ValueError， 但是我们使用Exception依然可以获取到这个错误。可是如果这两种异常类同时被指定的情况下会如何？ 12345678910s1 = 'world'try: int(s1)except Exception as e: print('Exception ===',e)except ValueError as e: print('ValueError ===', e) ---Exception === invalid literal for int() with base 10: 'world' 我们看到，就是按照程序的从上至下的顺序在执行。 所以，其实我们可以这样理解，当我们进行多分支异常类+通用异常类的时候，Exception是最后的一个保底。 123456789101112131415s1 = 'hello'try: # int(s1) # ValueError s1[5] # IndexErrorexcept IndexError as e: print('IndexError',e)except KeyError as e: print('KeyError',e)except ValueError as e: print('ValueError',e)except Exception as e: print('Exception',e) ---IndexError string index out of range 除此之外，try...except是支持else的，当try里的代码顺利执行没有捕获到任何错误之后，还可以走到else之中额外执行分支内的代码： 12345678910111213141516s1 = 'hello'try: str(s1) print(s1)except IndexError as e: print('IndexError',e)except ValueError as e: print('ValueError',e)except Exception as e: print('Exception',e)else: print('try代码块中没有引发异常时，执行') ---hellotry代码块中没有引发异常时，执行 我们再来了解一下finally, 这个方法是无论是否引发异常都会执行。通常情况下用于执行一些清理工作： 123456789101112131415161718192021s1 = 'hello'try: int(s1) print('如果前面的代码引发了异常，这个代码块将不在继续执行。。')except IndexError as e: print('IndexError',e)except ValueError as e: print('ValueError',e)except Exception as e: print('Exception',e)else: print('try代码块中没有引发异常时，执行')finally: print('无论是否引发了异常，都会执行这个代码块')print('如果上面的代码有异常并且进行了处理，那么后面的代码将继续执行')---ValueError invalid literal for int() with base 10: 'hello'无论是否引发了异常，都会执行这个代码块如果上面的代码有异常并且进行了处理，那么后面的代码将继续执行 这段代码中，我们引发了一个异常，也被捕获了。但是依然执行了finally内的代码，并且也未影响程序继续往后执行。 在我们平常写代码的过程中还有一种情况，就是我们需要自己制作一个异常信息，然后抛出。这个时候，我们需要用raise， 来主动抛出异常。 1234567try: raise Exception('发生错误')except Exception as e: print('Exception', e) ---Exception 发生错误 除了上述的异常处理之外，其实还有另外一种方式，是直接判断逻辑是否成立，不成立抛出AssertionError错误。就是使用assert进行断言。它在表达式错误的时候，会直接抛出AssertionError错误，如果表达式正确，这什么都不做。 1234assert 2 &gt; 3---AssertionError: 自定义异常处理类 虽然系统已经给到了很多异常处理的方式，而我们在平时开发中也会经常的使用。但是实际上，很多时候我们都需要一些自己的处理要求。比如说，当异常出现的时候，我们要将异常信息写入日志，在日后我们从日志里查看日常信息或者做数据分析，就是我们最常使用的。 那我们接下来看看，如果做一个异常处理的自定义开发: 再最开始，我们需要归纳一下，我们到底要保存怎样一个格式： 123# 日志的基本格式：- 日期时间， 异常的级别- 异常信息：引发的异常类别，异常的信息，文件及行号。 在确定了日志格式后，我们可以进入开发了，首先我们需要导入两个所需的库 123# 先导入所需模块import tracebackimport logging 让我们先来人为创建一个日常，并用try语句来捕获它： 1234 int('aaa') ---ValueError: invalid literal for int() with base 10: 'aaa' 这句代码报了一个ValueError异常类。 1234567try: int('aaa')except: print('在此进行异常的处理') ---在此进行异常的处理 没问题，我们捕获了异常并且正确的进入了except。那么，我们可以通过traceback模块来获取异常信息, 替换一下打印信息我们来查看一下。 123456789101112try: int('aaa')except: # 通过traceback获取异常信息 errormsg = traceback.format_exc() print(errormsg) ---Traceback (most recent call last): File &quot;/var/folders/h4/7cr1cmpn7v5b3x20_9wz8m740000gn/T/ipykernel_39689/2534911191.py&quot;, line 2, in &lt;module&gt; int('aaa')ValueError: invalid literal for int() with base 10: 'aaa' 接下来，就轮到logging模块了。该模块定义了实现用于应用程序和库的灵活事件日志记录系统的函数和类。 12345logging.basicConfig( filename = './data/error.log', # 日志存储的文件及目录 format='%(asctime)s %(levelname)s \\n %(message)s', # 格式化存储的日志格式 datefmt = '%Y-%m-%d %H:%M:%S') 在定义了logging的基本信息之后，我们就可以定义一下将刚才的errormsg写入日志了： 12# 写入日志logging.error(traceback.format_exc()) 那么我们完善一下整个代码就是这样： 12345678logging.basicConfig( filename = './data/error.log', # 日志存储的文件及目录 format='%(asctime)s %(levelname)s \\n %(message)s', # 格式化存储的日志格式 datefmt = '%Y-%m-%d %H:%M:%S')# 写入日志logging.error(traceback.format_exc()) 我们需要在异常出发的时候，将错误写入到日志内。那么需要将这段代码放到except中。可是我们总不能每次都写这么长一段代码，那怎么办呢？嗯，没错，我们需要封装一个函数用于多次调用。 1234567891011121314151617def Myexception(): # logging的基本配置 logging.basicConfig( filename = './data/error.log', # 日志存储的文件及目录 format='%(asctime)s %(levelname)s \\n %(message)s', # 格式化存储的日志格式 datefmt = '%Y-%m-%d %H:%M:%S' ) # 写入日志 logging.error(traceback.format_exc())# 使用自定义异常处理类try: int('bb')except: print('在此处进行异常的处理') Myexception() # 在异常处理的代码块中去调用自定义异常类 然后我们将导入库的方法也写进去，这样在我们需要的时候才会进行导入，顺便，我们将这个函数封装成一个类。就便于更多的文件调用： 123456789101112131415161718192021# 自定义异常日志处理类class Myexception(): def __init__(self): import traceback import logging # logging的基本配置 logging.basicConfig( filename='./error.log',# 日志存储的文件及目录 format='%(asctime)s %(levelname)s \\n %(message)s',# 格式化存储的日志格式 datefmt='%Y-%m-%d %H:%M:%S' ) # 写入日志 logging.error(traceback.format_exc())# 使用自定义异常处理类try: int('bb')except: print('在此处进行异常的处理') Myexception() # 在异常处理的代码块中去调用自定义异常类 这样，一个自定义的获取异常之后写入日常的类就定义好了，我们可以在任意地方导入并调用这个类方法，以便获取以及日后查看整个程序中的异常。 附录 标准的异常类 异常名称 描述 BaseException 所有异常的基类 SystemExit 解释器请求退出 KeyboardInterrupt 用户中断执行(通常是输入^C) Exception 常规错误的基类 StopIteration 迭代器没有更多的值 GeneratorExit 生成器(generator)发生异常来通知退出 StandardError 所有的内建标准异常的基类 ArithmeticError 所有数值计算错误的基类 FloatingPointError 浮点计算错误 OverflowError 数值运算超出最大限制 ZeroDivisionError 除(或取模)零 (所有数据类型) AssertionError 断言语句失败 AttributeError 对象没有这个属性 EOFError 没有内建输入,到达EOF 标记 EnvironmentError 操作系统错误的基类 IOError 输入/输出操作失败 OSError 操作系统错误 WindowsError 系统调用失败 ImportError 导入模块/对象失败 LookupError 无效数据查询的基类 IndexError 序列中没有此索引(index) KeyError 映射中没有这个键 MemoryError 内存溢出错误(对于Python 解释器不是致命的) NameError 未声明/初始化对象 (没有属性) UnboundLocalError 访问未初始化的本地变量 ReferenceError 弱引用(Weak reference)试图访问已经垃圾回收了的对象 RuntimeError 一般的运行时错误 NotImplementedError 尚未实现的方法 SyntaxError Python 语法错误 IndentationError 缩进错误 TabError Tab 和空格混用 SystemError 一般的解释器系统错误 TypeError 对类型无效的操作 ValueError 传入无效的参数 UnicodeError Unicode 相关的错误 UnicodeDecodeError Unicode 解码时的错误 UnicodeEncodeError Unicode 编码时错误 UnicodeTranslateError Unicode 转换时错误 Warning 警告的基类 DeprecationWarning 关于被弃用的特征的警告 FutureWarning 关于构造将来语义会有改变的警告 OverflowWarning 旧的关于自动提升为长整型(long)的警告 PendingDeprecationWarning 关于特性将会被废弃的警告 RuntimeWarning 可疑的运行时行为(runtime behavior)的警告 SyntaxWarning 可疑的语法的警告 UserWarning 用户代码生成的警告 那么，这节课到这里也就结束了。各位小伙伴，下去以后记得勤加练习。下课。","link":"/Get-the-exception/"},{"title":"21. 面向对象及特性","text":"Hi，大家好。我是茶桁。 今天开始，我们要迈向Python的另外一个台阶了，那就是面向对象。 面向对象编程（Object Oriented Programming)，简称为OOP，是一种以对象为中心的程序设计思想。 与之相对的，就是面向过程编程（Procedure Oriented Programming), 简称为POP, 是一种以过程为中心的程序设计思想。 面向对象和面向过程 接下来，让我们先了解一下这两个编程思想到底有什么不同。还记得咱们之前讲过宋丹丹老师小品里的经典的「把大象装进冰箱分几步」吗？小品给出的答案是三步对吧？ 第一步：打开冰箱门 第二步：把大象装进去 第三步：关上冰箱门 设计思想的不同 那么这个答案，就是一种面向过程的思维，遇到问题之后，分析解决问题的步骤，然后一步步的去实现。 那么如果是面向对象的话，又该如何去做？ 面向对象是通过分析问题中需要的抽象模型，然后根据需要的功能分别去创建模型对象，最终由模型对象来完成程序。那这个「把大象装进冰箱分几步」的问题我们该如何去考虑呢？ 首先，面向对象要解决这个问题，需要先建立出抽象模型，比如： 打开冰箱门和关闭冰箱门，这都属于一个冰箱的功能， 大象走进去，这就是大象的功能。 到此时我们就出现了两个抽象模型，一个是冰箱，一个是大象。 冰箱具有 打开和关闭的功能，大象具有走路的能力。 分析到这里，就是面向对象的思想，具体完成的话，就是去创建冰箱和大象这两个对象，最终完成这个程序 冰箱对象-开门，大象对象-走进冰箱，冰箱对象-关门 这个问题解决了，我们再来思考一个新的问题：「想吃清蒸鱼怎么办？」 当然是按照做菜的顺序一步一步来对吧？这就是典型的面向过程思维： 买鱼，买料 杀鱼和清理，并且腌制 锅里烧水 把鱼放进去，开始蒸鱼。 十分钟后开盖，把鱼端出来，然后浇汁。 这样，一步一步的完成这个愿望，就是面向过程所作的事情。 轮到面向对象，又该如何呢？ 需要一个对象：大厨。 告诉大厨，我想吃清蒸鱼。 那么大厨呢，有可能是我们自己训练的，也有可能是其他五星酒店挖过来的。不管如何，这是一个已经完善建立好的对象，我们直接拿来用就可以了。面向对象呢，就是这样寻找具体的对象去解决问题。对于我们来说，调用了对象，而对象完成了这个过程。 当然，具体大厨这个对象里肯定还是一步一步的去完成过程，也就是说，最终面向对象中是由面向过程的体现的。但是思维方式，也就是设计思想是完全不同的。 优缺点 既然有不同之处，那必然是有优缺点的。因为有对比嘛。面向过程有其优点，当然，面向对象也有其缺点。 面向过程的核心是过程，过程就是指几觉问题的步骤。其优缺点非常明显： 优点： 将负责的问题流程化，进而实现简答。 缺点： 扩展性差（更新、维护、迭代） 而面向对象的核心是对象，是一个特征和功能的综合体，其优缺点如下： 优点：可扩展性高 缺点：编程复杂度相对面向过程高一些，这里的复杂度指的是计算机在执行面向对象的程序时性能表现一般。 那总结起来呢，在去完成一些简单的程序时，可以使用面向过程去解决。但是如果有复杂的程序或任务，而且需要不断的进行迭代和维护，那么肯定是优先选择面向对象的编程思想 如何学习面向对象编程 那我们后面如何去学习面向对象编程呢？其实就两步： 学习面向对象编程的思想 学习面向对象编程的语法 这两步中，其实难的是第一步，学习面向对象编程的思想。 不管如何，什么事情都需要有个开头，那我们就从类和对象的基本概念开始好了。 认识类与对象 类： 类是对象的一个抽象的概念 对象（实例）：对象就是由类创建的实例 那么这两者的关系其实就是「模具和铸件」之间的关系。 类是由对象总结而来的，总结的这个过程叫做抽象。 对象是由类具体实施出来的，这个过程叫做实例化。 是不是听着有些迷糊了？这里我们还是用实际例子来解释一下的好，我们思考下面的问题： 水果是一个对象还是一个类？ 汽车是一个对象还是一个类？ 手机是一个对象还是一个类？ 我们在说水果的时候，你能想到什么？香蕉、苹果、西瓜、榴莲等等。对吧？那我们想了这么多不一样的东西，是不是这些所有的都称为是「水果」？那么我们将这些内容都叫做水果的过程就称为「归类」的过程。这个「水果」就是一个类，刚才我们总结的这个过程就叫做抽象，我们想到的香蕉、苹果...等等，就是对象。 汽车其实是一个概念，你能想到什么？奔驰、野马、奥迪、别摸我？那我们见过的车，就会在我们脑海中浮现，而这些具体的车总结出来一个类的过程就是「抽象的过程」，我们最后总结出来的「汽车」就是一个类。那些在我们脑海里浮现的具体的汽车，就是对象。 单我们去开车上班的时候，那么我们就是应用一个具体的对象去发生特定的功能。 再来想一个问题，我现在给大家写这个教程，用的是Macbook Pro，那么请问我当前正在使用的这个MBP是对象还是一个类？ MBP的特征：金属外壳，优美的外观。 MBP的功能：给大家写教程，编辑代码，听音乐，作曲，画画.... 当我描述了这么多之后，这个MBP到底是一个类还是一个对象？ 面向对象的基本实现 如果我们需要实例一个对象，那么我们就需要先抽象一个类。 举了栗子： 我们现在需要创建一个汽车，或者千千万万个汽车用于销售。那在这之前我们要做什么？ 首先，我们需要抽象一个汽车类，也就是我们要在一个设计图纸上设计处这个汽车。 然后，我们由这个设计图纸去创建（实例）出来的真实汽车就是一个对象。 那么接下来，就让我们具体到代码里去实现看看。 还记得我们之前介绍的怎么去创建一个类嘛？有没有小伙伴还记得？ 123# 定义一个汽车的类class Cart(): pass 没错，就是使用class关键字来定义一个类。其书写规范如下： 12345'''类名的书写规范，建议使用驼峰命名法 大驼峰：MyCar ChaHeng 小驼峰：myCar chaHeng''' 那么我们在类里需要声明些什么内容呢？一个类需要有「特征」和「功能」两个内容组成： 特征就是一个描述：颜色：黑色， 品牌：野马，排量：2.4...； 特征在编程中就是一个变量，在类中称为属性 功能就是某一项能力： 拉货，代步，上班....； 功能在编程中就是一个函数，在类中称为方法 在类中，属性一般定义在前面，方法定义在后面。 12345678910111213141516# 定义一个汽车的类class Car(): # 属性 =&gt; 特征 =&gt; 变量 color = 'black' # 表示颜色属性 brand = 'mustang' #表示品牌属性 displacement = 2.4 # 表示排量属性 # 方法 =&gt; 功能 =&gt; 函数 def pulling(self): print('小汽车能拉货。') def rode(self): print('小汽车能代步。') def onDuty(self): print('小汽车能上班。') 现在，我们拥有了一个具体的类，里面包含了特征和功能。那么我们如何通过类实例化对象并最终使用它们呢？ 很简单，将其赋值给一个具体的变量就可以了，比如，我们现在去4S店实际购买一个野马汽车： 12# 实例化一个对象buyNewCar = Car() 这样，就简单的实例化了一个购买的新车，让我们查看一下它的类别和各项属性： 123456789101112print(buyNewCar, type(buyNewCar))# 查看对象的品牌print(buyNewCar.brand)# 调用对象的方法buyNewCar.rode()---&lt;__main__.Car object at 0x105e6fbb0&gt; &lt;class '__main__.Car'&gt;mustang小汽车能代步。。 这样，我们就能看到这个对象是由类Car实例化得来的。并且查看到了品牌属性，试用了一下其“代步”这个功能。 成员属性和方法的操作 一个对象通过实例化之后，在类中定义的属性和方法，可以使用实例化的对象进行操作。 类中定义的属性也称为成员属性，类中定义的方法，也称为成员方法。 我们直接拿之前定义的类来实例化两个对象观察一下： 12345678a = Car()b = Car()print(a)print(b)---&lt;__main__.Car object at 0x105d37190&gt;&lt;__main__.Car object at 0x105d36fe0&gt; 我们来看，a,b分别实例化之后，我们将其打印出来。看到两个对象都是通过Car来实例化的，但是后面不同。就是说，这两者在实例化之后，完全就是两个不同的对象。那我们可以这么说，一个类可以实例化处多个对象。 对象的成员操作 在类的外部，使用对象操作成员，比如，我们可以通过对象访问类中的属性： 12345res = a.colorprint(res)---black 还可以通过对象访问类中的方法： 1234a.rode()---小汽车能代步。 那除了访问，我们是否可以对其进行修改呢？来看看： 123456a.color = 'red'res = a.colorprint(res)---red 可以看到，我们修改了对象的属性。那么，这个时候我们另外一个实例化对象b里是什么情况？ 1234print(b.color)---black 依然还是black，并未收到a内属性变化的影响。 也就是说，我们操作单个对象进行属性修改，并不影响最初的类，也不会影响同一个类实例化出来的其他对象。 我们还可以给对象添加本来没有的属性来丰富这个对象： 12345a.name = 'AE86'print(a.name)---AE86 同样的，我们对单个对象进行的操作，一样不会影响原本的类以及其他实例化对象： 1234print(b.name)---AttributeError: 'Car' object has no attribute 'name' 不出所料的报错了，错误类型为属性错误。告知我们并没有name这个属性。 好，再让我们来看看删除这个动作。我们就直接删除a对象刚创建的name: 1234567print(a.name)del a.nameprint(a.name)---AE86AttributeError: 'Car' object has no attribute 'name' 程序显示打印了一次name的值，说明我们能正常获取，然后删除a对象中的这个属性，然后再打印来看，警告我们AttributeError类型错误。说明，这个时候的name已经不存在了。 好，让我们删除a继承下来的属性brand,不过这次为了让后续程序还能正常运行，我们使用try来捕获一下错误。 12345678910try: del a.brandexcept AttributeError as e: print('AttributeError:', e)print('a.brand: ', a.brand)---AttributeError: branda.brand: mustang 可以看到，我们在执行删除a.brand的时候报错了，后面打印的结果也证明了a.brand这个属性还存在，可以被打印出来。 那么问题来了，为什么之前的a.name可以被删除，而a.brand不行？这两个属性到底有什么区别？ 其实，单我们执行删除一个对象的属性时，只能删除当前这个对象自己的属性才可以。而我们执行的操作中，brand并不是a自己的属性，而是属于Car这个类的。因为无法进行删除。 a.name则不一样，是单独在a对象内创建的属性，因此可以删除。 访问成员属性，会先访问对象自己的属性，如果没有，则去访问这个对象的类的属性。 修改对象的属性值时，实际上等于给这个对象创建了一个对象自己的属性。 添加对象的属性，是给对象创建了自己独有的属性。 删除属性，只能删除这个对象自己的属性，包括给对象添加的和修改的。 接着，我们来看看在类的外部，操作对象的方法。 访问对象的方法：实际上如果这个对象没有自己独立的方法，那么会访问这个对象的类的方法。 1234a.rode()---小汽车能代步 我们来进行修改对象的方法：给这个对象的方法重新定义： 12345678def func(): print('这里是重新定义的一个方法')a.rode = funca.rode()---这里是重新定义的一个方法 这样，我们就完成了方法的重新定义。 访问、修改之后，我们能不能给对象添加新的方法呢？ 12345a.func2 = funca.func2()---这里是重新定义的一个方法 看来也是可以的，我们现在给这个对象自己新创建了一个方法。 来，删除一下方法试试： 1del a.func2 并未报错，我们继续执行下试试： 1234a.func2()---AttributeError: 'Car' object has no attribute 'func2' 看报错，说明我们删除成功了。 方法实际上和属性一样，我们可以删除对象自己的方法，但是无法删除对象的类的方法。 至此，我们可以总结如下： 一个类定义类成员属性和成员方法，那么通过这个类实例化的对象，也具备了这些方法和属性。 实际上，创建对象的时候，并不会把类中的属性的属性和方法复制一份给对象，而是在对象中应用父类的方法。因此在访问对象的属性时，会先去找对象自己的属性，如果没有就去找这个类的属性和方法。 一个对象由类创建以后，是一个独立的对象，会应用父类中的属性和方法。如果在对象创建后，给对象的属性或方法，进行修改或添加，那么此时等于给这个对象创建了一个自己的属性和方法。所以在删除时，只能删除对象呗修改或添加的成员。 除了在类的实例化对象中对类的成员进行操作之外，我们还可以直接在类上进行操作。比如，我们可以执行下列操作： 1Car.brand = 'BMW' 那现在提一个问题，在原始类的成员修改之后，这个类创建的实例化对象会如何？ 12345678910print(a.brand) # 先执行一次打印，原始属性Car.brand = 'BMW'b = Car() # 新创建一个实例化对象print(b.brand) # 打印新创建的对象的属性print(a.brand) # 打印修改之前创建的对象的属性---mustangBMWBMW 很明显，我们直接在类上进行操作修改成员之后，不管是hi新创建的实例化对象，还是早已存在的实例化对象，其中的成员属性都被修改了。删除和新加都遵循着这样一个特性。 对成员属性和方法的操作，我们也就可以总结成两种，一是「对象操作成员」， 一种是「类操作成员」。当然，由于类修改后会影响具体的实例化对象，所以并不推荐这么去做。 对象操作成员 1234567891011成员属性： 访问： 对象.成员属性名 修改： 对象.成员属性名法 = 新值。（此时等于给这个对象创建了一个自己的属性） 添加： 对象.新成员属性 = 值 (此时是给这个对象自己新建了一个属性) 删除： del 对象.成员属性 (注意：只能删除这个对象自己的属性) 成员方法： 访问： 对象.成员方法名() 修改： 对象.成员方法名 = func（此时等于给这个对象创建了一个自己的方法） 添加： 对象.方法名 = func (此时是给这个对象自己新建了一个方法) 删除： del 对象.方法名 (注意：只能删除这个对象自己的方法) 类操作成员（不推荐） 1234567891011成员属性： 访问： 类名.成员属性名 修改： 类名.成员属性名法 = 新值。（此时通过这个类创建的对象都具有这个属性） 添加： 类名.新成员属性 = 值 (此时通过这个类创建的对象都具有这个属性) 删除： del 类名.成员属性 (注意：删除这个类的属性后，这个类创建的对象也没有这几个属性了) 成员方法： 访问： 类名.成员方法名() 修改： 类名.成员方法名 = func（此时通过类创建的对象都被修改） 添加： 类名.方法名 = func (此时通过类创建的对象都被修改) 删除： del 类名.方法名 (注意：此时通过类创建的对象都被修改) 最终总结一下如下： 一个类可以实例化出多个对象，每个对象在内存中都独立存在的 当通过类实例化对象时，并不会把类中的成员复制一份给对象，而去给对象了一个引用 访问对象成员的时候，如果对象自己没有这个成员，对象会向实例化它的类去查找 对象成员的添加和修改，都只会影响当前对象自己，不会影响类和其它对象 删除对象的成员时，必须是该对象自己具备的成员才可以，不能删除类中引用的成员 对类的成员操作，会影响通过这个类创建的对象，包括之前创建的。 成员方法中的self self在方法中只是一个形参，并不是关键字。从它本身的意义上来说，是可以用其他的关键字去替换的，但是长久以来的惯例大家都一直在使用self。 其作为英文单词的本意是：自己。那么在类的方法中则代表的是「当前这个对象」。不太明白？让我们来看一个实际的例子： 让我们先定义一个「Person」类，然后实例化一个「张三」： 123456789101112131415161718192021222324# 定义人class Person(): # 成员属性 name = 'name' age = 0 sex = 'sex' # 成员方法 def sing(self): print('会唱歌') def dance(self): print('会跳舞') def rap(self): print('会饶舌')# 实例化对象zs = Person()print(zs.name)---name 成功打印出了name， 说明我们成功实例化了。 通过实例化的对象，我们可以在类的外部去访问成员属性和成员方法。（对象.成员）。 同样的，我们其实也可以在类的内部去访问成员属性和成员方法。让我们做一个实验，来说明一下self到底是什么： 123456789101112131415161718# 定义人class Person(): # 成员属性 ... # 成员方法 ... def func(self): print(self)# 实例化对象zs = Person()# print(zs.name)print(zs)zs.func()---&lt;__main__.Person object at 0x10a048c10&gt;&lt;__main__.Person object at 0x10a048c10&gt; 我们修改了这个类，在内部创建了一个方法func(self)， 然后打印了self这个参数。 然后我们在外面打印了实例化的zs，还通过这个具体的实例化对象执行了类内部的func方法。实际上就是打印了一下此刻的self。可以看到，两个打印结果完全一样，那说明，这两者本身就是一个东西。 self代表调用这个方法的对象，谁调用了这个方法，self就代表的是谁。self就可以在类的内部代替对象进行各种操作。 我们通过self来进行的操作，其实完全就是实例化的对象所作的操作。我们在类中修改func这个方法，让其打印name， 修改name， 调用方法rap来试试看： 123456789101112131415161718192021222324# 定义人class Person(): # 成员属性 ... # 成员方法 ... def func(self): print(self) print(self.name) self.name = '茶桁' print(self.name) self.rap()# 实例化对象zs = Person()zs.name = &quot;张三&quot;zs.func() ---&lt;__main__.Person object at 0x10a06bf40&gt;张三茶桁会饶舌 我们就可以很清晰的看到self代表的含义，谁调用，self就代表谁。也就是说，只要是对象能干的事情，self就可以代表对象去完成，比如成员的添加、删除、更新、访问、调用等等。 我们再来修改一下类里的方法，让其更清晰的显示这个特性： 12345678910111213141516171819202122# 定义人class Person(): # 成员属性 name = 'name' ... # 成员方法 ... def rap(self): print(f'我是{self.name}, 我会饶舌') def func(self): ... self.rap()# 实例化对象zs = Person()zs.name = &quot;张三&quot;zs.func() ---我是张三, 我会饶舌 在类中，我们修改了一下rap方法，让其调用self.name， 在类被定义的时候，这个类中的name是被赋值为name的。然后，我们在func方法中调用了一下self.rap(), 我们对其进行实例化一个对象zs，并且在这个实例中对name进行了重新赋值张三, 接着，调用了实例化对象中的func()方法。 我们清晰的看到，func()调用了self.rap()，然后将张三打印在了屏幕上。充分说明了，这个时候的self代表的就是调用它的zs这个实例化对象。 我们直接调用类中的方法试试看： 1234Person.func()---TypeError: Person.func() missing 1 required positional argument: 'self' 我们收到了报错，被告知缺少必须的位置参数self。 好，那让我们再来做两个实验，第一个实验中，我们测试一下如果在类中的方法没有使用self接受参数会怎样： 123456789101112class Person(): def func(): print('我是一个没有`self`的方法。')Person.func()a = Person()a.func()---我是一个没有`self`的方法。TypeError: Person.func() takes 0 positional arguments but 1 was given 可以看到，我们可以使用类直接调用这个方法有效，但是我们创建一个实例化对象之后，利用实例化对象去调用则会报错。这个是因为，我们在用实例化对象去调用类中的方法的时候会传入一个参数。但是现在类中的func()方法并没有可以接受的参数，那么必定会报错。 第二个实验，我们试试不用self，而是其他的参数是否可以成功： 12345678910class Person(): def func(vars): print(f'我是{vars.name}, 我使用了vars来接受参数。')a = Person()a.name = 'admin'a.func()---我是admin, 我使用了vars来接受参数。 可以看到，完全没有问题。也就是说，用实例化对象调用类中的方法时，是一定会将自己作为一个参数传给这个方法，需要一个具体的参数去接受。而参数的名称是什么则无所谓，只是大家在习惯上都是用self。区别如下： 含有self或者可以接受对象作为参数的方法： 非绑定类方法 不含self或者不能接受对象作为参数的方法：绑定类方法 非绑定类方法，可以使用对象去访问, 绑定类方法，只能通过类去访问。 魔术方法 魔术方法是什么呢？ 魔术方法也和普通方法一样都是类中定义的成员方法。这是一种不需要去手动调用的，在某种情况下，自动触发（自动执行）的方法。魔术方法特殊就特殊在定义的时候，多数的魔术方法 前后都有两个连续的下划线。但是切记，这个方法并不是我们自己定义的，而是系统定义好的，我们来使用而已。 __init__ 初始化方法 这个初始化方法是在通过类实例化对象之后，自动触发的一个方法。 123456789101112131415161718class Person(): name = None age = None sex = None # 初始化方法 def __init__(self): print('我是一个初始化方法。') # 成员方法 def say(self): print('大家好，我是茶桁。')# 实例化对象zs = Person()---我是一个初始化方法。 注意到了么？我们仅仅是实例化的对象而已，并没有进行任何调用，初始化方法就执行了一遍。那么，我们可以得到下面这些内容： __init__ 触发机制： 在通过类实例化对象后，自动触发的一个方法 作用：可以在对象实例化之后完成对象的初始化（属性的复制，方法的调用）。 应用场景：文件的打开，数据的获取。 干活之前，做好一些准备工作。 以下，我们改造一下这个类，然后再实例化的时候多做一些动作： 123456789101112131415161718192021222324class Person(): name = None age = None sex = None # 初始化方法 def __init__(self,name,age, sex): print('我是一个初始化方法。') # 完成对象属性的初始化赋值 self.name = name self.age = age self.sex = sex # 成员方法 def say(self): print('大家好，我是茶桁。')# 实例化对象zs = Person('张三', 41, 'male')print(f'我叫{zs.name}, 我今年{zs.age}岁，性别:{zs.sex}')---我是一个初始化方法。我叫张三, 我今年41岁，性别:male 当然，我们还可以再初始化方法中调用say方法，完成自我介绍： 123456def __init__(self, name, age, sex): ... self.say() def say(self): print(f'打击好，我是{self.name}。') __del__： 析构方法 和初始化方法一样，我们直接来解析一下这个方法的触发机制，作用以及注意点。 触发机制： 析构方法方法会在对象被销毁时自动触发。 作用：关闭一些开发的资源 注意：对象被销毁时触发了析构方法，而不是析构方法销毁了对象。 我们还是从代码里来观察这个方法。 我们来定义一个类，完成一个日志的记录，调用这个对象的时候，传递一个日志信息。这个对象会创建一个文件，开始写入，并在最后关闭这个文件。 12345678910111213141516171819202122232425262728293031import timeclass writeLog(): # 成员属性 # 文件的路径 fileurl = './data' # 日志文件的名称 filename = str(time.strftime('%Y-%m-%d'))+'.log' # 初始化 打开文件 def __init__(self): # 完成文件的打开 print('初始化方法触发类，完成文件的打开') self.fileobj = open(self.fileurl+self.filename, 'a+', encoding='utf-8') # 写日志的方法 def log(self,s): print(f'把日志{s}写入到文件中') # 析构方法 def __del__(self): print('析构方法触发了，关闭打开的文件') # 在对象被销毁时，关闭在初始化方法中打开的文件对象 self.fileobj.close()l = writeLog()l.log('today is good day.')del l---初始化方法触发类，完成文件的打开把日志today is good day.写入到文件中析构方法触发了，关闭打开的文件 这段代码中，我们实例化了writeLog()类，调用了初始化方法。在方法中我们打开了文件，因为我用的是变量创建，所以不一定是什么文件。当前我操作的文件为2023-08-16.log。 然后我们调用l.log()， 也就是实例化对象中的log方法来对该文件写入一段日志内容：today is good day.， 在执行之后，我们又使用了del l来销毁这个实例。在销毁实例的时候，就会调用__del__方法来执行其中的方法。 那么对象会在什么情况下被销毁呢？ 当程序执行完毕，内存中所有的资源都会被销毁释放 使用 del 删除时 对象没有被引用时，会自动销毁 面向对象的三大特性 面向对象有三大特性，分别是「封装、继承、多态」， 那么它们具体都是什么呢？下面让我们分别来解释。 封装 封装，就是使用特殊的语法，对成员属性和成员方法进行包装，达到保护和隐藏的目的。就像我们送礼的时候，会找东西把礼物包起来一样。 但是一定注意，不能把成员全部封装死，就失去意义了。就好比我们买的笔记本电脑，无论如何都会给你留下一些接口的，比如说电源接口，USB接口等等。只有有了这些接口，我们才能插上鼠标啊，移动硬盘等等来进行使用。 被封装的成员主要是供类的内部使用。被特殊语法封装的成员，会有不同的访问的权限。比如笔记本内的硬盘，内存等等，这些并不是不让你使用，而是提供给笔记本本身使用，我们可以操作笔记本电脑来达到间接使用它们的目的。 封装分为了几个不同的级别，一般情况下有三种： 公有的 public 受保护的 protected 私有的 private 被特殊语法封装的成员，会有不同的访问权限。 123456789101112131415161718192021222324252627282930313233343536373839class Person(): # 成员属性 name = None age = None sex = None # 初始化方法 def __init__(self, name, age, sex): self.name = name self.age = age self.sex = sex # 成员方法 def say(self): print('talk about life.') def sing(self): print('sing a song.') def kiss(self): print('come on...')# 实例化对象zs = Person('张三', 49, 'male')# 查看对象的所有成员print(Person.__dict__) # 获取当前类的所有成员信息print(zs.__dict__) # 获取当前对象的所有成员信息# 我们也可以直接访问对象所有的方法print(zs.name)zs.kiss()---{'__module__': '__main__', 'name': None, 'age': None, 'sex': None, '__init__': &lt;function Person.__init__ at 0x111a355a0&gt;, 'say': &lt;function Person.say at 0x111a35750&gt;, 'sing': &lt;function Person.sing at 0x111a357e0&gt;, 'kiss': &lt;function Person.kiss at 0x111a35870&gt;, '__dict__': &lt;attribute '__dict__' of 'Person' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'Person' objects&gt;, '__doc__': None}{'name': '张三', 'age': 49, 'sex': 'male'}张三come on... 在整段代码中，我们实例化对象的时候，基本可以访问Person类中所有的成员。我们说定义的属性和方法，都可以无障碍访问。那么，我们现在说定义的这些成员，就都是Public级别。 现在想象一个场景，我们走在美国街头上，遇到一个美女，然后我们上前询问人家的年龄，大多数时候我们得不到想要的答案。而如果我们上去询问性别（现在知道为什么我要设定为美国街头了吧？），我估计这个就是保密的了吧，有可能一种情况就是当事人在当时的情况下，自己都不知道自己是什么性别。 那这个时候，我们就需要改写一下这段代码了, 改写之前，我们需要理解一下Python中不同级别成员的定义方式，分别为： str =&gt; 公共的 _str =&gt; 受保护的（约定俗成，在Python中没有具体实现） __str =&gt; 私有的。 在了解了定义方法之后，我们可以着手来做实验了： 123456789101112131415161718192021222324252627282930313233class Person(): # 成员属性 name = None _age = None # 这是一个protected 成员属性 __sex = None # 这是一个 private 成员属性 # 初始化方法 def __init__(self, name, age, sex): self.name = name self._age = age self.__sex = sex # 成员方法 def say(self): print('talk about life.') def _sing(self): # 这是一个protected 成员方法 print('sing a song.') def __kiss(self): # 这是一个private 成员方法 print('come on...')# 实例化对象zs = Person('张三', 49, 'male')# 查看对象的所有成员print(Person.__dict__) # 获取当前类的所有成员信息print(zs.__dict__) # 获取当前对象的所有成员信息---{'__module__': '__main__', 'name': None, '_age': None, '_Person__sex': None, '__init__': &lt;function Person.__init__ at 0x111f99630&gt;, 'say': &lt;function Person.say at 0x111f996c0&gt;, 'sing': &lt;function Person.sing at 0x111f99870&gt;, 'kiss': &lt;function Person.kiss at 0x111f99c60&gt;, '__dict__': &lt;attribute '__dict__' of 'Person' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'Person' objects&gt;, '__doc__': None}{'name': '张三', '_age': 49, '_Person__sex': 'male'} 可以看到，我们调用实例化方法得到的结果已经和之前有所不同了。最终拿到的__sex成员属性是属于类的。 现在让我们逐一来调用一下： 123456print(zs._age)print(zs.__sex)---49AttributeError: 'Person' object has no attribute '__sex' 可以看到，_age作为受保护的成员属性可以调用，但是__sex作为私有成员属性则不允许。 实际上，受保护的成员属性也是不能调用的，但是Python中因为没有具体实现，所以唯独在Python中可以调用。 123456zs._sing()zs.__kiss()---sing a song.AttributeError: 'Person' object has no attribute '__kiss' 那么，作为受保护的成员方法_sing被正常调用了，但是室友的成员方法__kiss调用的时候报错。看来和成员属性是一致的。 那么我们现在就可以总结如下： 公有的(Public) 受保护的(Protected) 私有的(Private) 在类的内部 可以访问 可以访问 可以访问 在类的外部 可以访问 不可以访问（Python中可以） 不可以访问 在实现上我们总结如下： 公有的(Public) 受保护的(Protected) 私有的(Private) 定义 默认定义的成员都属于公有成员 在成员名称前面加一个下划线 _成员名称 在成员名称前面加两个下划线 __成员名称 特征 公有的成员可以在任何位置进行访问和操作 受保护的成员和公有成员一样可以在任何位置进行访问，但是一般不要随便访问和操作受保护成员 私有的成员只能在当前类的内部去访问和操作，不能在类的外部进行操作 ⚠️ 这里我们需要注意Python特殊的亮点： 在python中并没有实现受保护的封装，属于开发者的约定俗成。 python中的私有化封装是通过改名策略实现的，并不是真正的私有化 继承 继承是什么？我们是不是经常听到「文化的继承，技艺的继承，衣钵的继承...」等等这些。 那计算机的继承又是什么？ 在面向对象中，一个类去继承父类，那么这个类就拥有了父类中除了私有成员之外的所有成员，包括属性和方法。这个，就叫做继承。 在整个继承过程中，被其他类继承的类就称为「父类」， 也可以称为「基类」或者「超类」。那么继承其他类的类，就被称为「子类」， 也可以称为「派生类」。 那么我们继承又什么意义吗？继承的主要意义，就是为了提高代码的重用性，建立新的类与类的关系，方便其他逻辑的操作。 继承实现起来其实非常方便： 123456# 继承的语法格式class 父类(): passclass 子类(父类): pass 我们直接看代码来理解，比如，我有如下定义： 123456789101112131415161718192021222324# 定义猫科动物class Felidae(): # 属性 coatColor = 'orange' # 毛色 sex = 'M' # 定义性别 # 成员方法 def run(self): print('轻盈的跳跃') def walk(self): print('走的猫步')# 定义猫class Cat(): coatColor = ' white' sex = 'M' # 定义性别 # 成员方法 def run(self): print('轻盈的跳跃') def walk(self): print('走的猫步') 我们看，猫是不是也是属于猫科动物的一种动物？那么在猫科动物中定义的所有成员，其实在猫这边我也会有。不过这样重复定义是不是感觉特别繁琐？其实，我们在Cat中完全不需要再次输入这么多，完全可以这样写： 12345678910111213141516171819# 定义猫科动物class Felidae(): # 属性 coatColor = 'orange' # 毛色 sex = 'M' # 定义性别 # 成员方法 def run(self): print('轻盈的跳跃') def walk(self): print('走的猫步')# 定义猫class Cat(Felidae): passmimi = Cat()mimi.run() 这样，我在定义Cat的时候就完成了对Felidae的继承，然后我们实例化一个Cat,再调用这个实例化对象中的方法run()， 也就输出了原本是属于类Felidae中的run()方法。 我们再继承父类的时候，之类还可以写入自己独有的成员属性或方法。 123456789101112131415161718class Cat(Felidae): size = 'small' def eat(self): print('吃猫粮。') pass mimi = Cat()mimi.run()print(mimi.size)mimi.eat()Felidae.eat()---轻盈的跳跃small吃猫粮。AttributeError: type object 'Felidae' has no attribute 'eat' 我们定义Cat的时候，除了继承Felidae里的成员之外，还定义了一个size成员属性和一个eat成员方法。然后我们在实例化对象中进行调用，都正常运行。 这个时候我们反过来，使用父类Felidae来调用在之类Cat中定义的成员，则会报错。说明这个成员是独属于之类的。 我们不仅可以继承的时候进行扩展，还可以复写父类中的方法，使的它与父类方法产生差异化。其方法是在子类中将父类的方法重新定义一遍就可以了。 那有什么办法在我重写父类方法的时候，仍然可以调用父类方法吗？也是可以的，就是使用super().父类方法名()来进行操作： 12345678910111213141516171819202122232425262728293031# 定义猫科动物class Felidae(): # 属性 coatColor = 'orange' # 毛色 sex = 'M' # 定义性别 # 成员方法 def run(self): print('轻盈的跳跃') def walk(self): print('走的猫步')# 定义猫class Cat(Felidae): size = 'small' def run(self): super().run() print('更加轻盈的跳跃。') def eat(self): print('吃猫粮。') passmimi = Cat()mimi.run()---轻盈的跳跃更加轻盈的跳跃。 我们可以看到，在子类中我们重写了父类中的run方法，但是由于我们在重写的时候在内部使用了super().run()。 所以父类中的方法被完全调用了一遍。 所以，我们目前可以总结继承的特征如下： 在不指定继承的父类时，所有类都继承自object类（系统提供） 了解 子类继承了父类后，就拥有了父类中的所有成员包括魔术方法（除了私有成员） 子类继承父类后，并不会把父类的成员复制给子类，而去引用 子类继承父类后可以重写父类中的方法，叫做 重写 子类重写父类的方法，依然可以使用super().父类方法名()的方式调用父类的方法 子类中如果定义了父类中不存在的方法，称为对父类的扩展 一个父类可以被多个子类继承，还可以存在 链式继承 。 链式继承：A类继承了B类，B类继承了C类，C类继承了D类。。。 单继承和多继承 一个类只能继承一个父类的方式，就叫做单继承。如果一个类继承了多个父类的方式，就称为多继承。直接看例子， 123456789101112131415class Person(): print('人的样子。')class Chusheng(): print('畜生的特性。')class Japanese(Person, Chusheng): passc = Japanese()c---人的样子。畜生的特性。 像代码中定义的Japanese类，同时继承了Person和Chusheng， 那这个，就属于多继承。我们来区分一下语法特征： 12345678910111213141516# 单继承class 父类(): passclass 子类(父类): pass # 多继承class 父(): pass class 母(): passclass 子(父，母): pass 在多继承的关系里，有一个有意思的部分，我们来看看： 12345678910111213141516class Tiger(): def eat(self): print('大口撕咬食物...')class Cat(): def eat(self): print('小口吞咽食物...')class C(Tiger, Cat): def eat(self): super().eat() print('到底该怎么吃？')# 实例化对象c = C()c.eat() 我们现在看到这段代码是一个多继承关系，我在C这个类中继承了Tiger和Cat两个类，并且复写了eat()这个方法。按道理来说，我们实例化C类之后，打印的结果一定是复写的结果。但是我们在C类的eat方法里还调用了super().eat()， 我们知道super()是调用一遍父类的方法。那么这里到底是调用Tiger里的eat方法，还是Cat里的eat方法呢？ 让我们看打印结果： 123---大口撕咬食物...到底该怎么吃？ 打印结果有没有出乎你的意料？那么这个原因是什么呢？其实也不复杂，就是因为Tiger的调用在前面，Cat在后面。让我们重新改一下看看： 1234567891011121314151617181920class Tiger(): def eat(self): print('大口撕咬食物...')class Cat(): def eat(self): print('小口吞咽食物...')class C(Cat,Tiger): def eat(self): super().eat() print('到底该怎么吃？')# 实例化对象c = C()c.eat()---小口吞咽食物...到底该怎么吃？ 这就证实了，谁在前面就调用谁的方法。 菱形继承（钻石继承） 先来看一个图形： 123 AB C D 那我们先有一个A类，下面有B和C类，再下面还有一个D类。 看图可能还是不太明白，它们之间的关系是这样的：B和C继承了A类，然后D又多继承了B和C。 那么这种继承关系就叫做菱形继承。 那么我们现在面临的一个问题就是：在这种菱形继承关系中，类与类是什么关系？super()调用时的顺序是怎样的？ 12345678910111213141516171819202122232425262728293031323334353637383940414243# 菱形继承# 祖先class A(): num = 111 def eat(self): print('学着凭借本能寻找食物...')# 父亲class B(A): num = 222 def eat(self): super().eat() print(super().num) print('进化了，学会大口吃肉。。。')# 母亲class C(A): num = 333 def eat(self): super().eat() print(super().num) print('进化的另外一个分支，小口吞咽...')# 子class D(B, C): num = 444 def eat(self): super().eat() print(super().num) print('居然退化了，又忘了怎么吃...')d = D()d.eat()---学着凭借本能寻找食物...111进化的另外一个分支，小口吞咽...333进化了，学会大口吃肉。。。222居然退化了，又忘了怎么吃... 那么我们来看一下，究竟是怎样的一个顺序： 1D.super() =&gt; B.super() =&gt;C.super() =&gt; A.print() -&gt; C.print() -&gt; B.print() -&gt; D.print() 上边这一段中，=&gt;是继承关系，-&gt;是执行顺序。 好，那我们这个时候要清楚一个点是，我们使用的d这个实例化去执行的，那么在这所有的继承类中，self全部都是c这个实例化对象。让我们来看看到底是不是： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 菱形继承# 祖先class A(): num = 111 def eat(self): print(self.num) print(self) print('学着凭借本能寻找食物...')# 父亲class B(A): num = 222 def eat(self): print(self.num) print(self) super().eat() print(super().num) print('进化了，学会大口吃肉。。。')# 母亲class C(A): num = 333 def eat(self): print(self.num) print(self) super().eat() print(super().num) print('进化的另外一个分支，小口吞咽...')# 子class D(B, C): num = 444 def eat(self): super().eat() print(super().num) print('居然退化了，又忘了怎么吃...')d = D()d.eat()---444&lt;__main__.D object at 0x111b71540&gt;444&lt;__main__.D object at 0x111b71540&gt;444&lt;__main__.D object at 0x111b71540&gt;学着凭借本能寻找食物...111进化的另外一个分支，小口吞咽...333进化了，学会大口吃肉。。。222居然退化了，又忘了怎么吃... 打印的结果证实了我们刚才的说法。 这个地方可能比较让人意外的是之前那个继承关系上，明明我B继承的是A， 怎么变成C了？我们来看看原因： 1234567891011121314'''在定义类之后，程序会自动生成一个继承的列表MRO(Method Realtion Order)方法关系列表MRO列表生成原则：1. 子类永远在父类的前面2. 同一等级的类，按照之类中的继承顺序摆放3. 先之类，后父类的顺序原则，最终的类是系统提供的obejct类MRO的调用方法类名.mro()'''D.mro()---[__main__.D, __main__.B, __main__.C, __main__.A, object] super在调用时，并不是查找父类，而是去MRO列表上找上一个类。 super方法在调用时，会自动把当前self传入到上一级的类的方法中。 所以我们之前会呈现出D=&gt;B=&gt;C=&gt;A的顺序。 看着有点晕是吧？别着急，我们接下来介绍一个方法，能很方便的看到类关系。 issubclass()类关系检测 这个方法是检测一个类是否是另一个类的之类的方法。用起来也非常简单： 1234567891011121314res = issubclass(D, B)print(res)res = issubclass(D, C)print(res)res = issubclass(D, A)print(res)res = issubclass(A, D)print(res)---TrueTrueTrueFalse 多态 对于同一个方法，由于调用的对象不同，产生了不同形态的结果。这个就叫做多态。 比如说，我们现在的电脑上有一个USB接口，那么这个接口在接入不同的设备的时候，产生的结果也是不一样的。插入鼠标，我们可以点击。插入键盘我们可以输入，插入U盘呢，我们可以读取。对吧？对于这个USB接口来说。就属于多态。 好的，让我们来实现一下，直接看代码： 12345678910111213141516171819202122232425262728293031323334353637# 定义电脑类class Computer(): # 在电脑类中定义一个 sub 的规范的接口 方法 def usb(self,obj): obj.start()# 定义鼠标类class Mouse(): def start(self): print('鼠标启动成功，可以双击单击嗨起来。。。')# 定义键盘类class KeyBord(): def start(self): print('键盘启动成功了，赶紧输入666。。。')# 定义 U盘 类class Udisk(): def start(self): print('U盘启动了，赶紧检查一下我的种子还在不在。。。')# 实例化对象c = Computer() # 电脑对象m = Mouse() # 鼠标对象k = KeyBord() # 键盘对象u = Udisk() # u盘对象# 把不同的设备插入到电脑的usb的接口中c.usb(m)c.usb(k)c.usb(u)---鼠标启动成功，可以双击单击嗨起来。。。键盘启动成功了，赶紧输入666。。。U盘启动了，赶紧检查一下我的种子还在不在。。。 这样，我们就实现了一个多态的程序。 我们在实例化Computer()之后，利用实例化对象c调用类中的方法usb， 将实例化对象传入，并且还传入了不同的obj， 这里的obj是我们之前实例化过的m, k, u。 那这样，我们obj代表了不同的实例化对象，那也就会启动不同的类方法。 那这样呢，属于一个普通的方式来实现，其实对于这段程序，我们还可以使用继承关系来完成。 我们先定义一个接口规范类，其他类都继承这个类，并实现（重写）父类中的方法。由于每个对象实现父类的方式或者过程都不相同，最后的结果是不一样的形态。 1234567891011121314151617181920212223242526272829303132333435363738394041# 继承关系写多态# 定义USBclass USB(): ''' info: 这个类是一个接口规范类，需要子类继承并实现start方法 start方法不做任何具体功能的实现 ''' # 在usb类中定义一个规范的接口方法，但是不实现任何功能 def start(self): pass# 定义鼠标类class Mouse(USB): def start(self): print('鼠标启动成功，可以双击单击嗨起来。。。')# 定义键盘类class KeyBord(USB): def start(self): print('键盘启动成功了，赶紧输入666。。。')# 定义 U盘 类class Udisk(USB): def start(self): print('U盘启动了，赶紧检查一下我的种子还在不在。。。')# 实例化对象m = Mouse()k = KeyBord()u = Udisk()m.start()k.start()u.start()---鼠标启动成功，可以双击单击嗨起来。。。键盘启动成功了，赶紧输入666。。。U盘启动了，赶紧检查一下我的种子还在不在。。。 我们回来看这段代码，实际上，如果抛开USB类，我们单独去写后面的类，并且把继承关系去掉。最后是不是也可以进行打印？可以... 可是这样的话，那这三个方法中的satrt方法之间就毫无关系，继承了USB中的start方法，也就是继承了规范。 而且这个继承的形式，和我们之前实现的普通版本其实并无什么差别，虽然代码实现上有不同，可是逻辑上是完全相同的。 好了，关于面向对象，我们就先介绍到这里。不过别着急，并不是讲完了，我们下节课还要接着讲「面向对象」。讲解一些高级语法和思想。小伙伴们记得关注。 另外，面向对象这个东西，确实蛮难的，并不是看我这一两节课就能学懂的。虽然我尽力，但是我还是有自知之明。 在这里给大家推荐一本好书，有它在，你想不懂都难。 ^_^ 领取优惠券再购买：","link":"/Object-Oriented-Programming/"},{"title":"23. 描述符和设计模式","text":"Hi， 大家好，我是茶桁。 上一节课中，我们讲解了面向对象中的一些高阶应用，给大家介绍了一些魔术方法。并在最后，我们预告这节课内容会讲解描述符和设计模式。 好了，让我们开始吧。 描述符 这个玩意，怎么讲合适呢？这么说吧，当某一个类中，包含了三个魔术方法（__get__, __set__, __delete__)中的任意一个，或者全部都有时，那么这个类就被称为是「描述符类」。 描述符的作用就是对一个类中的某一个成员进行一个详细的惯例操作，包括获取、赋值以及删除。也就是，描述符代理了一个类中的成员的操作，描述符属于类，只能定义为类的属性。 魔术方法咱们前面有提到，这里让咱们先来看看三个特殊的魔术方法，： __get__(self, instance, owner) 触发机制：在访问对象成员属性时自动触发(当该成员已经交给描述符管理时) 作用：设置当前属性获取的值 参数：1. self 描述符对象 2.被管理成员的类的对象。3.被管理成员的类 返回值：返回值作为成员属性获取的值 注意事项：无 __set__(self, instance, value) 触发机制：在设置对象成员属性时自动触发(当该成员已经交给描述符管理时) 作用：对成员的赋值进行管理 参数：1. self 描述符对象 2.被管理成员的类的对象。3.要设置的值 返回值：无 注意事项：无 __delete__(self, instance) 触发机制：在删除对象成员属性时自动触发(当该成员已经交给描述符管理时) 作用：对成员属性的删除进行管理 参数：1. self 描述符对象 2.被管理成员的类的对象。 返回值：无 注意事项：无 让我们先来看一个基本的类和实例化： 123456789class Person(): name = 'name'# 实例化对象zs = Person()print(zs.name)---name 然后我们定义一个「描述符类」 123456789101112# 定义描述符类class PersonName(): __name = 'abc' def __get__(self, instance, owner): pass def __set__(self, instance, value): pass def __delete__(self, instance): pass 接着我们重新更改一下刚才定义的普通类， 将其中的name成员属性交给刚定义的描述符类来实现： 1234# 定义的普通类class Person(): # 把类中的一个成员属性交给一个描述符类来实现 name = PersonName() 这个时候我们实例化之后打印其中的成员属性会如何？ 123456# 实例化对象zs = Person()print(zs.name)---None 我们可以看到，结果为None。 现在让我们依次将类中的__get__方法的参数都打印出来观察一下： 1234567891011121314# 修改其中的`__get__`方法def __get__(self, instance, owner): print(self) print(instance) print(owner) # 实例化后打印print(zs.name)---&lt;__main__.PersonName object at 0x108931d20&gt;&lt;__main__.Person object at 0x108930250&gt;&lt;class '__main__.Person'&gt;None 现在，具体self, instance, owner各自分别是什么，就非常清楚了。 那，既然self是PersonName类本身，那我们在其中定义的name成员属性是不是就可以拿到了？ 123456789101112131415161718192021222324# 定义描述符类class PersonName(): __name = 'abc' def __get__(self, instance, owner): return self.__name def __set__(self, instance, value): pass def __delete__(self, instance): pass# 定义的普通类class Person(): # 把类中的一个成员属性交给一个描述符类来实现 name = PersonName()# 实例化对象zs = Person()print(zs.name)---abc 没错，我们确实拿到了PersonName中的__name。 我们现在可以这么理解，普通类中的一个成员属性交给了一个描述符类来实现，类中的成员的值是另一个描述符类的对象， 那么当对这个类中的成员进行操作时，可以理解为就是对另一个对象的操作。现在的PersonName这个描述符类，相对于一个代理人的角色。把当前的描述符类赋值给了一个需要代理的类中的成员属性。 既然我们看到了get方法的结果之后，那么剩下两个魔术方法的作用也就很容易想到了： 123456789101112131415161718# 定义描述符类class PersonName(): ... def __set__(self, instance, value): self.__name = value ...# 定义的普通类class Person(): ... # 实例化对象...zs.name = '张三丰'print(zs.name)---abc张三丰 这里容易理解吧？当我们执行zs.name = ‘张三丰’这个赋值操作的时候，其就是走到了__set__方法内。其中的self不言而喻，就是PersonName， 而value就是刚才我们进行赋值操作的那个值。这个时候，我们可以设置self.__name = value，那就是满足了这个赋值操作。当然，我们也可以不这样给，来让我们调戏一下这个赋值。 12345678910# 只改动`__set__`def __set__(self, instance, value): # self.__name = value self.__name = '茶桁'zs.name = '张三丰'print(zs.name)---茶桁 当我们这样去改的时候，那么无论我们怎样去赋值，最终的结果都是打印出茶桁。 那么__del__怎么用呢？我们接着看： 12345678# 前面的代码都不做改动del zs.nameprint(zs.name)---茶桁茶桁 那么第一个茶桁是刚才我们赋值后的打印结果，第二个茶桁呢？就是我们在执行del zs.name之后的打印结果。按道理来说，我们执行了del命令之后。zs这个对象的name成员已经被删除了，现在应该是打印出类中的原始值，也就是abc, 那为什么这里打印出来的还是茶桁呢？ 原因就在于我们的__del__方法内没有任何操作。我们来改一下__del__内部： 12345678910# 只改动`__del__`def __delete__(self, instance): # print('我就是不行删除，气死你') del self.__namedel zs.nameprint(zs.name)---abc 这样我们就执行了del本该有的操作。不过大家也看到了，我中间有一段代码注释了，现在让我们替换一下注释： 123456789# 只改动`__del__`def __delete__(self, instance): print('我就是不行删除，气死你') # del self.__namedel zs.name---我就是不行删除，气死你 当我们执行del zs.name的时候，触发方法内的打印命令。那么，这个时候再让我们打印一下zs.name来看看： 1234print(zs.name)---茶桁 毫无意外的，茶桁还在，并没有变成abc。 需要注意的是，同时具备三个魔术方法的类才是「数据描述符类」，没有同时具备三个魔术方法的类呢？很简单，就是「非数据描述符类」。两者的区别就是一个是完整的，一个是不完整的。可以不可以应用呢？部分可以，但是不完整，__get__, __set__, __delete__中总有某些功能无法实现。 一个描述符应用 了解了描述符的概念以及怎么使用之后，我们来试试实现一个应用：定义一个学生类，需要记录学员的id, 名字和分数。 让我们先来起一个框架： 12345678910111213141516class Student(): def __init__(self, id, name, score): self.id = id self.name = name self.score = score def __repr__(self): return f'学员编号:{self.id}\\n学员姓名:{self.name}\\n学员分数:{self.score}' # 实例化对象zs = Student(37, '张三丰', 98)print(zs)---学员编号:37学员姓名:张三丰学员分数:98 这里，我们对这个方法有一个要求，就是学员的分数只能在0-100范围中, 那其实很简单了对吧？ 我们先来看看第一种最普通的实现方法： 12345678910111213141516171819class Student(): def __init__(self, id, name, score): self.id = id self.name = name # 检测分数范围 if score &gt;= 0 and score &lt;= 100: self.score = score else: print('当前分数不符号要求。') def __repr__(self): return f'学员编号:{self.id}\\n学员姓名:{self.name}\\n学员分数:{self.score}' # 实例化对象zs = Student(37, '张三丰', 101)print(zs)---当前分数不符号要求。AttributeError: 'Student' object has no attribute 'score' 尝试一下，确实打印了“分数不符合要求”，同时报错。 先不说怎么解决报错的问题，这简单的解决方案只能适用于对象初始化的时候有效。如果我们是中间单独对成员属性进行赋值，那么就会失效了 12345678...zs.score = -1print(zs)---学员编号:37学员姓名:张三丰学员分数:-1 那这个时候，大家还记不记得咱们之前学过的一个魔术方法setattr? 我们来给中间加一个__setattr__： 12345678910111213141516171819202122def __setattr__(self, key, value): # 检测是否给score进行赋值操作 if key == 'score': print(key, value) # 检测分数范围 if value &gt;= 0 and value &lt;= 100: object.__setattr__(self, key, value) else: print('当前分数不符号要求。') else: object.__setattr__(self, key, value)def __repr__(self): info = f'学员编号:{self.id}\\n学员姓名:{self.name}\\n学员分数:{self.score}' return info...zs.score = -1---score -1当前分数不符合要求 我们这样就使用__setattr__方法，检测如果score分数进行赋值时候，进行了分数的检测判断。 那我们在看，现在的一个问题是，假如学员的分数不止一个，我需要赋值多个分数怎么办？当前学员有：语文，数学，英语分数。 另外就是当前这个类中的代码是否比较繁杂？ 现在，我们再来看，思考一下使用描述符来代理我们的分数这个成员属性。让我们先来实现一下框架： 123456789101112131415161718192021class Score(): __score = None def __get__(self, instance, owner): pass def __set__(self, instance, value): pass def __delete__(self, instance): del self.__scoreclass Student(): score = Score() def __init__(self, id, name, score): self.id = id self.name = name self.score = score def returnSelf(self): info = f'学员编号:{self.id}\\n学员姓名:{self.name}\\n学员分数:{self.score}' return info 框架就实现好了，我们将原始的普通类中的score代理给了描述符类Score()。 那现在让我们来完善一下整个类中的方法。 首先，当我们进行获取的时候，直接return现有的值就可以了，当我们进行设置的时候，就需要进行判断，如果不符合要求就打印一个不符合要求。 12345678910111213141516171819202122232425# 定义描述符类，代理分数的管理class Score(): __score = None def __get__(self, instance, owner): return self.__score def __set__(self, instance, value): if value &gt;= 0 and value &lt;= 100: self.__score = value else: print('分数不符合要求') def __delete__(self, instance): del self.__scoreclass Student(): score = Score() def __init__(self, id, name, score): self.id = id self.name = name self.score = score def returnSelf(self): info = f'学员编号:{self.id}\\n学员姓名:{self.name}\\n学员分数:{self.score}' return info 让我们对其进行一下检测，看看是不是符合要求： 1234567# 实例化对象zs = Student(37, '张三丰', 132)zs.returnSelf()---分数不符合要求'学员编号:37\\n学员姓名:张三丰\\n学员分数:None' 没毛病，被告知了当前赋值不符合要求，并且最后分数上也为None，并未进行赋值。 在看看单独赋值： 12345678zs.score = -1zs.score = 88zs.returnSelf()---分数不符合要求'学员编号:37\\n学员姓名:张三丰\\n学员分数:88' 当赋值为-1的时候也是提示不符合要求，再次赋值88之后，正确赋值。然后我们打印出来的结果也正确。 那么我们的代理就正确的完成了它的工作。基本工作流程如下： 定义Score描述符类 把学生类中的score这个成员交给描述符类进行代理 只要在代理的描述符中对分数进行判断和赋值就可以了。 那么现在，我们就完成了一个描述符的应用案例。不知道大家是否都理解了？那么在下面，我给大家介绍一下描述符的三种定义格式： 格式一： 通过定义描述符来实现（推荐） 12345678910class ScoreManage(): def __get__(self, instance, owner): pass def __set__(self, instance, value): pass def __delete__(self, instance): passclass Student(): score = ScoreManage() 格式二： 使用property函数来实现 1234567891011121314151617181920class Student(): def __init__(self, id, name, score): self.id = id self.name = name self._score = score def getScore(self): return self._score def setScore(self, score): self._score = score def delScore(self): del self._score # 在 property 函数中指定对应的三个方法 # 对应的方法 1. `__get__`，2. `__set__`, 3. `__delete__` # 当然，名称不是固定的，也可以定义成其他的方法名 # 不管定义成什么，`property`中的方法名必须一致。 # 注意在类中将成员属性重新定义，可以为受保护的或者私有属性，避免递归调用。 score = property(getscore,setscore,delscore) 格式三：使用@property装饰器语法来实现 1234567891011121314151617class Student(): __score = None @property def score(self): print('get') return self.__score @score.setter def score(self,value): print('set') self.__score = value @score.deleter def score(self): print('delete') del self.__score 设计模式 我们谈设计模式的时候，实际上是一个比较抽象的东西。 设计模式，就是前人完成某个功能或者需求，根据经验和总结，对实现的代码步骤和代码设计进行了总结及归纳。成为了实现某个需求的经典模式。 设计模式可以说并不是什么固定的代码格式，而是一种面向对象编程的设计。 让我们先从单例开始。 单例（单态）设计模式 在当前脚本中，同一个类只能创建一个对象去使用，这种情况就称为单例（单态）。 我们以一个实际的思考案例来进行讲解，现在让我们来想： 单例和婚姻法的关系，特别像，就是一个人只能有一个结婚对象。在社会中是如何完成一夫一妻制的？如果想要结婚，必须要到民政局登记，民政局需要检测两个人的户口本，看看上面是否属于已婚的状态。如果是已婚，肯定就被撵出去了对吧。如果没有结婚，就可以盖章登记了。 那么按照这样的思路，我们又该如何去实现Python中的单例设计模式呢？来看哈： 需要一个方法，可以去控制当前对象的创建过程： 构建方法__new__ 需要有一个标识来存储和表示是否有对象：创建一个私有属性进行存储，默认为None； 在创建对象的方法中去检测和判断是否有对象： 如果没有对象，则创建对象，并且将对象存储起来，返回对象。那如果存储的是对象，则直接返回对象，就不需要创建新的对象了。 让我们依照这样一个思路来完成代码，让我们还是从框架开始： 12345class Demo(): # 定义构造方法 def __new__(cls, *args, **kwargs): return cls.obj 第一步我们完成了，现在让我们来看第二部，我们需要定义一个私有属性用于存储对象。 12345678class Demo(): # 定义私有属性存储对象 __obj = None # 定义构造方法 def __new__(cls, *args, **kwargs): return cls.__obj 接着，就要进入判断了： 123456789101112class Demo(): # 定义私有属性存储对象 __obj = None # 定义构造方法 def __new__(cls, *args, **kwargs): # 创建对象的过程中，判断是否有对象 if not cls.__obj: # 如果没有，则创建，并且存储起来 cls.__obj = object.__new__(cls) return cls.__obj 类完成了，让我们来证实一下看看，是否只会创建一个对象。 123456789# 实例化对象a = Demo()b = Demo()print(a)print(b)---&lt;__main__.Demo object at 0x10434e950&gt;&lt;__main__.Demo object at 0x10434e950&gt; 看到打印结果中，两次实例化对象不同，但是地址相同。可以证明确实为一个对象。 Mixin类 Mixin 必须是表示一种功能，而不是一个对象。 Mixin 的功能必须单一，如果有多个功能，那就多定义Mixin类 python 中的Mixin是通过多继承实现的 Mixin 这个类通常不单独使用，而是混合到其它类中，去增加功能的 Mixin 类不依赖子类的实现，即便子类没有继承这个Mixin,子类也能正常运行，可能就是缺少了一些功能。。 那使用Mixin混入类有什么好处呢？ 这个混入类的设计模式，在不对类的内容修改的前提下，扩展了类的功能。也提高代码的重用性，使的代码结构更加的简单清晰。可以根据开发需要任意调整功能（也就是创建新的Mixin混入类），避免设计多层次的复杂的继承关系。 我们之前学习继承，知道继承需要有一个必要的前提，就是继承应该是一个is-a的关系。 比如，苹果可以去继承水果，因为苹果is a水果， 那苹果是不能继承午饭的，因为午饭可以有苹果，也可以没有。 再比如，汽车可以继承交通工具，又是因为汽车本身is a交通工具。 遵循这样的一个规律，我们来思考，交通工具都有哪些呢？ 汽车、飞机、直升飞机，这些都属于交通工具对吧？当然，高铁什么的也是，我们无法穷举出来，那样就太多了。 那么如何去设计这些类的关系呢？我们可以创建一个交通工具类，然后属于交通工具的都来继承，再去实现... 等等，我们再来思考一个问题：飞机、直升飞机都可以飞，可是汽车呢？汽车并不能飞行。那么交通工具中如果去定义飞行这个功能，是不是就不合适了？ 你们现在是不是在想：那就在飞机和直升飞机类中分别实现飞行这个功能。可以是可以，但是重复代码是不是过多了？代码无法重用。 那该怎么办？其实，让我们分别去定义交通工具和飞行器这两个父类，这样飞机和直升飞机就可以去继承这两个类。对吧？ 来，让我们开始实现，一样的，先来个框架 1234567891011121314# 定义交通工具class vehicle(): # 运输货物 def cargo(): print('货物') # 搭载乘客 def person(): print('人')# 定义飞行器class flying(): def fly(self): print('可以飞') 现在刚才的思考得以实现，我们定义了两个父类。接着是不是就要考虑继承了？ 1234567891011# 定义飞机class airplane(vehicle, flying): pass# 定义直升机class helicopter(vehicle, flying): pass# 定义汽车class car(vehicle): pass 根据我们之前学习的继承关系，这样就完成了子类对父类的继承关系。来让我们分析下： 此时去定义一个飞行器的类Flying, 让需要飞行的交通工具直接继承这个类，可以解决问题。但是又两个问题，出现的类多继承，就违背了is-a， 飞行器这个类很容易被误解。那怎么办？ 其实解决方案还是使用多继承，但是给飞行器这个类定义为一个Mixin混合类，此时就是等于把飞行器这个类，作为一个扩展的功能，来扩展其他类。 让我们来改一下： 1234567891011121314151617181920212223242526# 定义交通工具class vehicle(): # 运输货物 def cargo(): print('货物') # 搭载乘客 def person(): print('人')# 定义飞行器class flyingMixin(): def fly(self): print('可以飞')# 定义飞机class airplane(vehicle, flyingMixin): pass# 定义直升机class helicopter(vehicle, flyingMixin): pass# 定义汽车class car(vehicle): pass 嗯，你没看错，就是这么简单，改一下名称。 那么在这段代码中，虽然直升机和飞机都是用了多继承，也就是继承了flyingMixin，但是由于flyingMixin类加了Mixin这个名，就告诉了后面阅读代码的人，这个类是一个Mixin类。 我知道你们在想什么，这个是不是太随便了？其实并不是，我们目前在谈论的是「设计模式」，这个flyingMixin类中除了名称之外，还要遵循一些特定的惯例规则，就是这个类中的功能必须是单一的。 在名称的含义上，Mixin表示混入(mix-in)， Mixin必须是表示一种功能，而不是一个对象。Mixin的功能必须单一，如果有多个功能，那就需要多定义几个Mixin类。在Python中的Mixin是通过多继承实现的。Mixin类通常不单独使用，而是混合到其他类中，去增加功能的。Mixin类不依赖子类的实现，即便子类没有继承这个Mixin类，子类也能正常运行，只是可能缺少一些功能。 抽象类 首先我们要明白，抽象类也是一个类。但是，这又是一个特殊的类，抽象类不能用，不能直接实例化称为一个对象。抽象类包含了抽象方法，抽象方法就是没有实现代码的方法。抽象类需要子类继承，并重写父类的抽象方法，才可以使用。 抽象类，一般应用在程序设计，程序设计中一般是要对功能和需求进行规划，其中有一些需求是明确的并且可以完成的，但是也可能会有一些需求是不明确的，或者不确定具体需要怎么实现，此时就可以把这个不确定怎么实现或者需要后面再去实现的方法，定义为抽象方法（只定义方法名，不写具体代码）。 我们还是拿一个实例来讲解： 比如公司有一项新的产品需要开发，交给了开发部门的大拿，也就是你。那么你就开始去规划设计怎么去完成这个产品的开发。比如项目需要用到不同的技术，不同的人来完成。这样，你作为老大，自己完成了一部分功能，但是依然有一部分定义了需求，但是还没有具体实现，需要其他人来进行实现。 那么此时，你已经写完的部分就是普通方法，定义了需求但是未完成的就可以理解为是抽象方法。 还是来直接看代码： 123456789101112131415import abc# 必须使用metaclass, 属性必须是abc.ABCMetaclass WriteCode(metaclass=abc.ABCMeta): # 需要抽象方法，使用装饰器进行装饰 @abc.abstractmethod def write_swift(self): pass def write_java(self): print('实现了Java代码的开发') def write_python(self): print('实现了Python代码的开发') 这样我们就在一个抽象类中定义好了一个抽象方法，和几个普通方法。至于为什么必须metaclass=abc.ABCMeta，那就又要扩展着去讲了。这里先记住，就跟背单词一样，到这里了就这么用就可以了。 前面我们讲了，抽象类是不能直接实例化的，让我们试试看： 12345# 抽象类不能直接实例化对象obj = WriteCode()---TypeError: Can't instantiate abstract class WriteCode with abstract method write_swift 报错了，直接告诉我们无法实例化抽象类。 那么我们到底要怎么用呢？ 我们可以定义一个子类来继承，并实现抽象类中的抽象方法。 1234# 定义子类，继承抽象类，并实现抽象类中的抽象方法class Demo(WriteCode): def write_swift(self): print('实现了swift代码的开发') 好了，现在让我们实例化子类试试： 12345obj = Demo()print(obj)---&lt;__main__.Demo object at 0x104ade8c0&gt; 没有报错，似乎是完成了继承和实现。接着当然是一次执行子类中的方法来看看： 12345678obj.write_java()obj.write_python()obj.write_swift()---实现了Java代码的开发实现了Python代码的开发实现了swift代码的开发 没毛病，现在我们完成了整个代码。 那小伙伴们现在估计最大的疑问是：抽象类我要应用在什么地方呢？ 比如说，我们现在要开发一个框架，这个框架要有一大堆的功能，包括a,b,c（哎，我忽然理解导入的为什么是abc这样起名了）。但是呢，具体用这个框架开发什么样的产品我们并不清楚，因此这个框架中能否知道你要做什么样的开发吗？肯定不知道。 框架具备了一定的功能即可，剩下的，需要具体开发项目的人来实现自己的业务逻辑。 那这个时候，我们就要用到抽象类了。 好了，到目前为止，我们关于面向对象编程也就介绍的差不多了。而我们的Python课程基本上也到了尾声，在后面的课程中，我们会介绍一下Python中的装饰器。然后去学习一下几个用的特别广的库，包括matplotlib,numpy以及pandas。 小伙伴们，大家对于此前的Python基础一定要好好的理解，好好练习。这样，越往后我们才能越轻松的开展后面的学习。 行，本节课到这里就结束了，下课。","link":"/OOP-Descriptor-and-design-patterns/"},{"title":"22. 面向对象 - 高阶","text":"Hi，大家好。我是茶桁。 之前的课程里面，我们简单的接触了面向对象编程，也和大家讲解了其思想，优缺点。相信上节课程结束之后，大家对面向对象都有了一定的理解。 那么我们这节课，就进入面向对象的一些高阶部分，让我们继续来学习一些魔术方法以及Python的内置成员，然后再来学习一下描述符与设计模式。 内置成员 魔术方法 描述符 设计模式 好，正课走起。让我们开始。 内置成员 当我们创建一个类之后，即便我们还什么都没做，这个类里面就已经有内容了，我们来看一下： 123456789class Demo(): pass# 获取类/对象的所属成员res = Demo.__dict__print(res)---{'__module__': '__main__', '__dict__': &lt;attribute '__dict__' of 'Demo' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'Demo' objects&gt;, '__doc__': None} 上节课我们学过了__dict__， 这个是获取类或者对象的成员的方法。打印结果我们看到其中的成员。 让我们添加些内容再来观察一下： 12345678910111213class Demo(): name = 'a' age = 20 def say(self): print('say something')# 获取类/对象的所属成员res = Demo.__dict__print(res)---{'__module__': '__main__', 'name': 'a', 'age': 20, 'say': &lt;function Demo.say at 0x1117a3e20&gt;, '__dict__': &lt;attribute '__dict__' of 'Demo' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'Demo' objects&gt;, '__doc__': None} 看到我们刚才定义的成员属性和成员方法也都在列了。我们还可以实例化之后获取对象的成员： 12345obj = Demo()print(obj.__dict__)---{} 当我们实例化一个对象obj之后，打印发现其成员是空的。这是为什么？ 原因就在于，这个方法用处其实是打印其对象的专有成员。我们再来为这个实例化对象创建一些成员再来看看： 12345obj.sex = 'female'print(obj.__dict__)---{'sex': 'female'} 可以看到，我们获取了刚才创建的成员属性。 以上，就是我们使用__dict__获取了类和对象的所属成员，方法为：类/对象.__dict__。 除了获取所属成员，我们还有其他方法，比如获取「文档信息」， 获取「类名称」， 获取「所在文件名称」，获取「当前类的父类列表」以及获取「当前类的『继承链』。来让我们依次看一下： 还记得我们之前在创建函数的时候可以添加文档吗？ 1234def obj(): ''' 这里是文档内容 ''' 同样的，类当中我们一样可以添加文档内容。然后我们可以通过__doc__来获取： 12345678910class Demo(): ''' 这里是一个Demo类，主要用于测试 ''' passprint(Demo.__doc__)---这里是一个Demo类，主要用于测试 同样的，__doc__不仅可以获取类的文档信息，同样可以获取到对象的。 我们使用__name__来获取类名称「组成的字符串」, 这个方法无法对对象使用。 1234print(Demo.__name__)---Demo __module__可以用来获取类/对象所在的文件名称 123456print(Demo.__module__)print(obj.__module__)---__main____main__ 如果其所在文件为当前文件，那么这里就会显示为__main__。 然后是__base__， 这个方法是用来获取当前类的父类列表。这个方法有两个版本，一个是__base__, 一个是__bases__。这两个方法的区别在于一个是获取继承的第一个父类，一个是继承所有的父类的列表，为了呈现的更明显，我们建立一个继承类： 1234567891011class A(Demo): passclass B(A, Demo): passprint(B.__base__)print(B.__bases__)---&lt;class '__main__.A'&gt;(&lt;class '__main__.A'&gt;, &lt;class '__main__.Demo'&gt;) 还有一个就是我们上节课讲过的，MRO列表，也就是__mro__方法，用于获取当前类的继承链。 1234print(B.__mro__)---(&lt;class '__main__.B'&gt;, &lt;class '__main__.A'&gt;, &lt;class '__main__.Demo'&gt;, &lt;class 'object'&gt;) 到此为止，我们介绍的就是常用的一些内置成员获取的一些方法。当然，这里不是全部，除此之外还有很多，因为并不是常用，所以这里我们就不多介绍了。 方法的分类 接下来呢，我们来看下面向对象的分类，包括： 对象方法 类方法 绑定类方法 静态方法 对象方法 其特征为： 1. 在类中定义方法，含有self参数 2. 含有self的方法，只能使用对象进行调用。 3. 该方法会把调用的对象传给进来。 12345678910111213class Demo(): # 对象方法 def objFunc(self): print(self) print('this is objFunc')# 实例化对象obj = Demo()obj.objFunc()---&lt;__main__.Demo object at 0x1171460e0&gt;this is objFunc 这个方法不能直接使用类直接调用，但是其实也不是绝对的。当我们使用类直接调用的时候，需要传递一个参数，也就是必须要self有参数可接收。 12345Demo.objFunc('a')---athis is objFunc 类方法 类方法呢，和对象方法有不一样的地方，也有相同的地方。两者定义十分相似，不同之处是使用装饰器材： 其特征为： 在类中定义的方法，使用了@classmethod进行了装饰 方法中有形参cls 可以不用实例化对象，直接使用类进行调用 会把调用这个方法的类或对象传递进来 来直接看代码理解： 12345678910111213141516class Demo(): # 类方法 @classmethod # 装饰器 def clsFunc(cls): print(cls) print('this is cls function: clsFunc')Demo.clsFunc()obj.clsFunc()---&lt;class '__main__.Demo'&gt;this is cls function: clsFunc&lt;class '__main__.Demo'&gt;this is cls function: clsFunc 看结果可以看到，我们用类进行调用的时候并没有像对象方法一样传递一个参数进去，这是因为调用的时候会直接传递调用的类给到cls参数。 而我们说不需要实例化对象，并不是实例化对象不可调用。对象调用也是可以的。 至于什么是「装饰器」，我们以后会详细讲到，这里先记住这种形式就可以了。 绑定类方法 这个方法不传递任何对象和类。在定义的时候，不设定任何的形参： 1234567891011class Demo(): # 绑定类方法 def bindClassFunc(): print('this is bind Class function: bindClassFunc')# 调用Demo.bindClassFunc()---this is bind Class function: bindClassFunc 那么绑定类方法既然没有定义形参，那么这个方法是无法使用实例化对象来调用的。 1234obj.bindClassFunc()---TypeError: Demo.bindClassFunc() takes 0 positional arguments but 1 was given 其特征如下： 1. 在类中定义的方法，不必须设置形参。 2. 只能使用类进行调用。 3. 可以传递任意参数，但是不会将类作为参数传递进来。 静态方法 「静态类方法」和「类方法」相似，也需要一个装饰器。并且，静态类方法也是不需要设置形参的。 123456789101112class Demo(): # 静态类方法 @staticmethod def staticFunc(): print('this is static method func')Demo.staticFunc()obj.staticFunc()---this is static method functhis is static method func 那从结果中我们可以看到，「静态类方法」可以使用类和对象进行调用，并且调用的时候不需要传递任何参数。 其特征如下： 1. 在类中定义的方法，使用装饰器 @staticmethod 进行了装饰 2. 可以使用对象或者类进行调用 3. 不会将对象或者类作为参数传递进来 ⚠️ 注意：这里我们需要注意的，「静态类方法」只是可以不设置参数，并不是不能设置参数，并且，就算是设置了参数之后，也是不接受类和对象作为参数传递的。比如： 1234567891011121314@staticmethoddef staticFunc(a, b): print(f'a:{a}, b:{b}') print('this is static method func')# 调用Demo.staticFunc('static', 'class')obj.staticFunc('static', 'obj')---a:static, b:classthis is static method funca:static, b:objthis is static method func 我们分别用类和对象进行了调用并传递了两个参数进行打印，而打印结果正常，并且没有对象或者类被传递。 相应的，「绑定类方法」也是这种特性，只是「绑定类方法」只支持类调用，不支持对象调用。 常用函数 其实在之前，关于「常用函数」我们已经接触过了一些，比如：issubclass(子类，父类)。有些小伙伴可能还记得，这个函数是用于检测一个类是否为另一个类的子类。 那除了这个之外，Python中还有很多其他的一些针对类和对象的常用函数，下面让我们来详细看一下。 isinstance(对象，类)， 用于检测一个对象是否是该类或者该类的子类的实例化结果。 12345obj = D()print(isinstance(obj, D))---True 这个结果显而易见，那么我们思考一下，既然D类继承了B类和C类，那么obj对象是否也是B或者C的实例化结果呢？ 1234print(isinstance(obj, B))---True 可见，对于继承了父类的子类，其实例化对象和父类之间也会被检测为True。 hasattr(对象/类,'成员名称')， 这个函数是用于检测类/对象是否包含指定名称的成员。 12345B.name = '张三'print(hasattr(obj, 'name'))---True 在这段代码中，我们给父类B添加了一个成员属性name，因为obj是D的实例化对象（之前的代码中）。而D类是继承自B类的，所以自然obj中也是包含了name这个成员属性的。所以我们的检测结果必然为True。 来，我们做另外一个实验： 12345678objB = B()D.age = 20print(D.age)print(hasattr(objB, 'age'))---20False 我们重新用B类实例化了一个对象objB， 然后我们给D类添加了一个成员属性age，并且打印了一遍证实其存在。这个时候我们检测了一下objB中是否含有age， 因为D为B的子类，它说添加的成员属性为独有属性，并不会更改到B类里，那自然B的实例化对象objB中是不可能存在这个成员属性的，结果自然为False。 `` getattr(对象/类,'成员名称'), 用于获取类/对象的成员的值 那这个函数就好理解了，我们可以用之前建立好的实例化对象objB和obj来获取一下试试看： 123456print(getattr(obj, 'age'))print(getattr(objB, 'name'))---20张三 没问题，结果如我们所料一般。那如果是获取objB中的age的值会如何？我们前面已经知道，objB中并未存在age这个成员属性，所以必然会报错： 1234print(getattr(objB, 'age'))---AttributeError: 'B' object has no attribute 'age' setattr(对象/类,'成员名称','成员的值'), 这个函数用于设置类/对象的成员的属性值。 123456print(setattr(obj, 'name', 'du'))print(obj.name)---Nonedu 如结果所见，这个方法的返回值为None，但是我们通过打印obj.name可知，方法确实更改了obj中name的值。 delattr(类/对象,'成员名称') 这个函数可以删除类/对象的成员属性，和del直接删除对象的成员是一样的结果。 123456print(delattr(obj, 'name'))print(obj.name)---None张三 可见，这个方法也是没有返回值的，返回了None。 不过，既然我们已经删除了obj中的name， 为啥还能打印出张三呢？有没有小伙伴知道为什么？ 其实，我们删除的name是之前使用setattr为obj设定的专有成员，当它被删除之后，我们的obj的继承类D中还存在着name这个成员属性，所以现在打印出来的张三是从D类中继承过来的。 那如果是我们新添加的但是其他类中没有的属性就会直接报错了，来看： 12345678setattr(obj, 'size', 'small')print(obj.size)delattr(obj, 'size')print(obj.size)---smallAttributeError: 'D' object has no attribute 'size' 我们分别打印了两次，第一次setattr了一个成员属性size，并且打印验证了。然后我们执行delattr，删除了刚才设置的成员属性size，这次再打印来看，报错了。 dir()这个函数可以获取当前对象所有可以访问的成员的列表。正好，让我们来看看是否从还存在从B类中继承的成员属性name： 12345res = dir(obj)print(res)---['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'age', 'name'] 可以看到打印结果的最后面，确实还存在着name这个成员属性。 以上函数在讲解的过程中，我们使用的都是成员属性，而成员方法其实是一样的。因为这几个函数说针对的对象都是「成员」。 另外需要注意的一点是，以上所有这些常用函数，都是在可访问的情况下才可执行。我们还有一些不可访问的情况，比如说「私有成员属性」，这种成员是无法被访问或者操作的，我们随便拿个函数来举一个例子看看： 12345678910111213class D(): name = '张三' _age = 25 __sex = 'female' print(f'Sex:{__sex}')obj = D()getattr(obj, '__sex')---Sex:femaleAttributeError: 'D' object has no attribute '__sex' 可以看到，当我们意图用getattr来获取实例化对象obj中的__sex属性时报错了。无法正确访问。 12345res = dir(obj)print(res)---['_D__sex', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_age', 'name'] 当我们使用dir()来查看的时候，其中也并没有__sex这个成员属性，有的只是类的私有成员属性:_D__sex， 我们需要借助类从内部才可访问到： 1234getattr(obj, '_D__sex')---'female' 这样是可以的。 魔术方法 我们在这节课之前，讲到过「魔术方法」， 那我们已经了解，魔术方法是不需要手动调用就可以自动执行的方法。 那我们之前已经讲解过__init__，这是一个初始化方法。然后还有一个__del__方法，是一个销毁方法。 这两个方法除了功能上的不同之外，还有一个最大的不同点就是被触发的机制是不一样的。其实，魔术方法中，最重要的一点就是要了解方法的触发机制是什么。 让我们先列出来常用的魔术方法，包括其触发机制，作用以及参数等等... __init__， 初始化方法, ***** 触发机制：当实例化对象之后就会立即触发的方法 作用：为当前创建的对象完成一些初始化的操作，比如：成员属性的赋值， 方法的调用， 打开或者创建一些资源等等。 参数：一个self， 接收当前对象，其他参数根据需求进行定义即可。 返回值：无 注意事项：无 __new__，构造方法, **** 触发机制：实例化对象时自动触发（在__init__之前触发） 作用：管理控制对象创建的过程 参数：一个cls接收当前类，其它参数根据初始化方法的参数进行决定 返回值：必须返回object.__new__(cls)进行对象的创建，如果没有返回值，则实例化对象的结果为None 注意事项： __new__方法的参数和__init__方法的参数要保持一致，除了第一个参数。必须返回object.__new__(cls)进行对象的创建，如果没有返回值，则实例化对象的结果为None 应用场景：设计模式中的单例设计模式。 __del__，析构方法, ***** 触发机制：当该类对象被销毁时，自动触发 作用： 关闭或释放对象创建时打开或创建的一些资源 参数： 一个self，接受当前的对象 返回值：无 注意事项： 无 __call__ , *** 触发机制: 把对象当作函数直接调用时自动触发 作用: 一般用于归纳类或对象的操作步骤，方便调用 参数：一个self接收当前对象，其它参数根据调用需求缺点 返回值：可有可无 5.__len__ 触发机制: 当使用len函数去检测当前对象的时候自动触发 作用: 可以使用len函数检测当前对象中某个数据的信息 参数: 一个self接收当前对象 返回值：必须有，并且必须是一个整型 注意事项：len要获取什么属性的值，就在返回值中返回哪个属性的长度即可 6.__str__ 触发机制: 当使用str或者print函数对对象进行操作时自动触发 作用: 代码对象进行字符串的返回，可以自定义打印的信息 参数：一个self，接收当前对象 返回值：必须有，而去必须是字符串类型的值 7.__repr__ 触发机制: 在使用repr方法对当前对象进行转换时自动触发 作用: 可以设置repr函数操作对象的结果 参数： 一个self，接收当前对象 返回值： 必须有，而去必须是字符串类型的值 注意：正常情况下，如果没有__str__这个魔术方法，__repr__方法就会代替__str__魔术方法 8.__bool__ 触发机制: 当前使用bool函数转换当前对象时，自动触发.默认情况下，对象会转为True 作用: 可以代替对象进行bool类型的转换，可以转换任何数据 参数: 一个self接收对象 返回值： 必须是一个布尔类型的返回值 以上，我们把常用魔术方法都列出来之后，然后我们来些代码进行讲解。让我们先创建一个Person类，然后在其中协商构造方法，初始化方法和析构方法。 123456789101112131415161718192021222324252627# 定义一个人class Person(): # 构造方法 def __new__(cls, *args, **kwargs): print(args) print(kwargs) # 初始化方法 def __init__(self, name, age, sex): print('触发初始化方法:__init__') self.name = name self.age = age self.sex = sex # 析构方法 def __del__(self): print('触发了析构方法:__del__')# 实例化对象zs = Person('张三丰', 210, '男')print(zs)---('张三丰', 210, '男'){}None 当我们完成实例化的时候，「构造方法」先是用*args接收了所有传递的参数，并且使用存储了元组。我们可以看到，**kwargs什么都没接收到，所以打印为空。 当「构造方法」执行完之后，也并没有去执行「初始化方法」和「析构方法」，这又是为什么呢？这是因为如果在「构造方法」中没有返回对象，这对象无法创建。要想对象进行创建，这我们必须返回object.__new__(cls)进行对象的创建。这在之前「构造方法」的说明里有说明。这个cls参数是什么呢？我们直接来看代码： 12345678910111213141516171819202122232425262728293031# 定义一个人class Person(): # 构造方法 def __new__(cls, *args, **kwargs): print(args) # print(kwargs) print(cls) # 如果该方法中没有返回对象，则无法创建对象 return object.__new__(cls) # 初始化方法 def __init__(self, name, age, sex): print('触发初始化方法:__init__') self.name = name self.age = age self.sex = sex # 析构方法 def __del__(self): print('触发了析构方法:__del__')# 实例化对象zs = Person('张三丰', 210, '男')print(zs)---('张三丰', 210, '男')&lt;class '__main__.Person'&gt;触发初始化方法:__init__&lt;__main__.Person object at 0x107f3c070&gt; 现在可以看到，我们打印了cls， 实际上就是Person这个类。当我们返回object.__new__(cls)之后，可以看到__init__初始化方法正确运行了，执行了方法内的打印方法。然后最后，我们打印了zs这个实例化对象。那为什么__del__析构方法没有触发？因为我们是在Jupyter中执行，并未执行释放，此时我们如果del zs，则会触发析构方法，或者，我们讲上述代码保存为一个22.py文件，然后单独执行，这个时候Python的垃圾回收机制会执行，就会进行释放，从而触发析构方法。如下图： 我们接着上面写的代码在22.py中继续写： 1234zs()---TypeError: 'Person' object is not callable 报警，告知我们这个类当中没有cllable。那如果我们讲这个类改造下，加上__call__: 1234567891011121314151617181920212223# 魔术方法# 定义一个人class Person(): # 构造方法 def __new__(cls, *args, **kwargs): ... # 初始化方法 def __init__(self, name, age, sex): ... def __call__(slef, *args, **kwargs): print('你把对象当成了函数进行调用。') # 析构方法 def __del__(self): ...# 实例化对象...zs()---...你把对象当成了函数进行调用。触发了析构方法:__del__ 这样，我们直接执行zs()就没问题了，可以把对象当作函数直接调用时自动触发。 让我们继续，返回到22.ipynb笔记本文件中，让我们重新定义一个类： 123456789class Demo(): items = []# 实例化对象obj = Demo()len(obj)---TypeError: object of type 'Demo' has no len() 报错信息中可以看出，这个实例化对象是没有len()方法的。我们如果给它加上__len__之后就会让其拥有len()方法。 1234567891011class Demo(): items = [] def __len__(self): return len(self.items)# 实例化对象obj = Demo()print(len(obj))---0 因为当前我们在类中定义的items里面没有数据，所以返回的长度必然也是0。但是这个返回值是必须要有的，需要返回一个整型才行。 来，我们看看如果这个方法的返回值写死会如何： 123456789101112class Demo(): items = [] def __len__(self): return 1# 实例化对象obj = Demo()obj.items = [1, 2, 3, 4, 5, 6, 7]print(len(obj))---1 很明显，我们重新给obj.items进行了赋值，目前其长度是7, 可是返回值依然是1。说明__len__的返回值只要四个整型就行。那我们就需要注意了，len需要获取什么属性的值，就在返回值中返回哪个属性的长度即可。当我们用正确的方式返回的时候就会是这样： 123456789101112class Demo(): items = [] def __len__(self): return len(self.items)# 实例化对象obj = Demo()obj.items = [1, 2, 3, 4, 5, 6, 7]print(len(obj))---7 让我们继续接着这段代码来玩： 123456...res = str(obj)print(res)---&lt;__main__.Demo object at 0x110631270&gt; 发现没有，虽然我们使用了str()方法，可是最后返回的结果，和直接打印obj的结果是一样的。那为什么会这样呢？这是因为我们这个当前的方法其实是对obj对象进行了一个转化字符串操作，而其本身就返回了一个&lt;__main__.Demo object at 0x110631270&gt;的字符串。 其实这个返回的字符串我们也是可以自定义的, 使用__str__方法给一个返回值就可以了。 123456789101112class Demo(): items = [] ... def __str__(self): return '&lt;__Demo__, 此字符串返回的是茶桁自定义的结果。&gt;'# 实例化对象obj = Demo()...print(obj)---&lt;__Demo__, 此字符串返回的是茶桁自定义的结果。&gt; 我们直接打印了obj对象，因为__str__方法的存在，所以现在直接打印了返回的字符串。也就是说，该方法可以代替对象进行str或者print的字符串信息返回。 继续来看： 1234567891011121314class Demo(): # def __str__(self): # ... def __repr__(self): return '这是一个repr返回的内容'# 实例化对象obj = Demo()print(obj)---这是一个repr返回的内容 可以看到我在类中注释了__str__方法，这是因为只有其不存在的情况下，__repr__ 方法才会起作用，可以替代__str__方法。 那么到底__str__和__repr__两个到底有什么区别呢？让我们直接在代码里找答案： 1234567num = 521print(str(num))print(repr(num))---521521 这个时候两个的结果都是一样的，似乎并看不出两者到底有什么区别。别急，让我们继续往后做这个实验： 123456789num = 521r1 = str(num)r2 = repr(num)print(f'r1: {r1}, {type(r1)}')print(f'r2: {r2}, {type(r2)}')---r1: 521, &lt;class 'str'&gt;r2: 521, &lt;class 'str'&gt; 两者的类型都是一样的，返回了一个字符串类。难道这两者就正的毫无区别吗？Python得创建者吃饱了撑的没事做两个功能一模一样但是名字不同的方法？ 123456789s = '521'r1 = str(s)r2 = repr(s)print(f'r1: {r1}, {type(r1)}')print(f'r2: {r2}, {type(r2)}')---r1: 521, &lt;class 'str'&gt;r2: '521', &lt;class 'str'&gt; 仔细看，两者似乎有了细微的差别。repr解析的结果带着引号。 那么，str和repr函数都可以把其他类型的数据转为字符串类型。 str函数会把对象转为更适合人阅读的形式，repr函数会把对象转为解释器读取的形式。 如果数据对象并没有更明显的区别的话，str和repr的转化结果还真没什么区别。 这两者的区别，其实只要了解一下就可以了。大部分时候，并不需要那么较真。 接着让我继续来看看__bool__： 12345res = bool(obj)print(res)---True 当我们对obj使用bool()方法的时候，返回值为True。 那说明其中包含了一个bool机制，并且默认返回值为True。 这个时候让我们来定义一下看看： 12345678910111213class Demo(): items = [] ... def __bool__(self): return bool(self.items)# 实例化对象obj = Demo()res = bool(obj)print(res)---False 由于我们的items中设置为空值，而我们将前面定义obj的items的那段代码删掉了，所以这个时候，传入方法的self.items的值也为空，必然返回值就是False。也证明了，bool(obj)拿到的返回值就是类里定义的的__bool__中返回的对象。 介绍完常用的一些魔术方法之后，我们再来看一些其他的魔术方法。同样是魔术方法，为什么我要明显的区别开来讲呢？那是因为现在开始说讲的魔术方法都是针对成员的，是一些成员相关魔术方法。 __getattribute__: 优先级最高 触发机制: 当访问对象成员时，自动触发，无论当前成员是否存在 作用: 可以在获取对象成员时，对数据进行一些处理 参数: 1. self接收对象，2. item接收当前访问的成员名称 返回值: 可有可无，返回的值就是访问的结果 注意事项: 在当前的魔术方法中，禁止对当前对象的成员进行访问，会触发递归。如果想要在当前魔术方法中访问对象的成员必须使用object来进行访问。格式： object.__getattribute__(self,item) __getattr__ 触发机制：当访问对象中不存在的成员时，自动触发 作用：防止访问不存在的成员时报错，也可以为不存在的成员进行赋值操作 参数: 1. self接收当前对象，2. item接收当前访问的成员名称 返回值：可有可无 注意事项：当存在 getattribute 方法时，会去执行 getattribute 方法。也要注意，不要在当前的方法中再次去访问这个不存在的成员，会触发递归操作 __setattr__ 触发机制： 当给对象的成员进行赋值操作时会自动触发（包括添加，修改） 作用： 可以限制或管理对象成员的添加和修改操作 参数： 1. self接收当前对象 2. key设置的成员名 3. val设置的成员值 返回值： 无 注意事项：在当前的魔术方法中禁止给当前对象的成员直接进行赋值操作，会触发递归操作。如果想要给当前对象的成员进行赋值，需要借助 object 格式： object.__setattr__(self,key,value) __delattr__ 触发机制： 当删除对象成员时自动触发 作用： 可以去限制对象成员的删除，还可以删除不存在成员时防止报错 参数：1. self接收当前对象 2. item删除的成员名称 返回值： 无 注意事项： 在当前魔术方法中禁止直接删除对象的成员，会触发递归操作。如果想要删除当前对象的成员，那么需要借助 object。 格式： object.__delattr__(self,item) 好了，按照惯例，让我们上代码, 先让我们来定义一个最正常的类，并且实例化它： 12345678910111213141516171819202122class Person(): name = 'name' age = 0 sex = 'male' def __init__(self, name, age, sex): self.name = name self.age = age self.sex = sex def say(self): print('say something...') def sing(self): print('sing a song...')# 实例化对象obj = Person('张三丰', 280, '男')print(obj.name)---张三丰 这个时候，让我们在类中定义一个方法：__getattrbute__()。 1234567891011class Person(): ... def __getattribute__(self, item): pass# 实例化对象obj = Person('张三丰', 280, '男')print(obj.name)---None 可以看到，虽然我们在实例化对象的时候传入了成员值，但是当我们打印的时候返回值为None。如果我们这个时候修改一下这个魔术方法： 1234567891011class Person(): ... def __getattribute__(self, item): return 'abc'...print(obj.name)print(obj.sex)---abcabc 那我们拿到的就是得到的内容。不仅是name， 任意我们传入的成员，返回的值都为__getattribute__返回的值。当获取对象成员时，这个方法进行处罚，其中的item形参就是我们想要获取的成员属性。第一次是obj.name, 第二次是obj.sex，但是无论你调用的是什么成员，拿到的都是这个方法的返回值abc。 那既然这样，是不是我们返回对象的成员属性就可以了？ 12345class Person(): ... def __getattribute__(self, item): return self.name 千万不要这么做，这样会引起方法的无限递归调用，最终导致栈溢出。那么是不是我们就没办法了？也不是，我们需要使用object.__getattribute__(self, item)： 123456789101112131415class Person(): ... # 获取对象成员的时候触发 def __getattribute__(self, item): return object.__getattribute__(self, item)# 实例化对象obj = Person('张三丰', 280, '男')print(obj.name)print(obj.sex)---张三丰男 这样，我们就获取到了正确的返回值。我们这里讲解一个__getattribute__方法，限于篇幅的原因，我们其他的几个方法就不细致讲了。在我们先前的列表内，每一个方法的触发机制，作用，参数和注意事项我们都有写清楚。大家可以执行去看看，并做一些测试。让我们赶紧进入下一个阶段，不过在这之前呢，我们还是需要讲访问成员的顺序给大家强调一下，这个还是比较重要： 调用 __getattribute__魔术方法 调用数据描述符 调用当前对象的成员 调用当前类的成员 调用非数据描述符 调用父类的成员 调用__getattr__魔术方法 以上步骤是调用某个成员时的顺序，前面的能够调用成功，后面则不再执行。至于描述符，咱们下节课来详细讲。 好了，本节课到这里就结束了，让我们先预告一下，下节课呢，我们来讲讲面向对象中的「描述符和设计模式」。大家期待一下吧。 记得课后好好做练习，目前我们的课程稍微有些难度了，只有保持一定的练习量，才能理解并记住。 小伙伴们，下节课再见了。下课。","link":"/Object-Oriented-Programming-Higher-Level/"},{"title":"24. 装饰器语法与应用","text":"Hi, 大家好。我是茶桁。 在最近几期的课程中，相信小伙伴们都频繁的看到一个词：「装饰器」， 那到底什么是装饰器，又有什么作用呢？我们这节课，就来好好的来了解一下。 装饰器定义 装饰器就是在不改变原有函数代码，且保持原函数调用方法不变的情况下，给原函数增加新的功能（或者给类增加属性和方法）。 核心思想：用一个函数（或者类）去装饰一个旧函数（或者类），造出一个新函数（或者新类）。 应用场景：引入日子，函数执行时间的统计，执行函数钱的准备工作，执行函数后的处理工作，权限校验，缓存等。 语法规则：在原有的函数上加上@符，装饰器会把下面的函数当作参数传递到装饰器中，@符又被称为「语法糖」。 装饰器原型 装饰器其实就是利用闭包，把函数当作参数传递，并在在函数内去调用传递进来的函数，并返回一个函数。 来，我们还是用代码来学习，先让我们定义一个普通函数： 12345678# 定义一个普通函数def old(): print('我是一个普通的函数')old() # 作为普通函数直接调用---我是一个普通的函数 现在让我们定义一个嵌套函数，分为外函数和内函数两部分： 12345678# 定义外函数，接受一个函数作为参数def outer(f): # 定义内函数， 并且在内函数中调用了外函数的参数 def inner(): print('我是外函数中的内函数1') f() print('我是外函数中的内函数2') return inner 这里面我们在内函数中打印了两句话，在两句话中间执行了一次外函数的参数（传递进来一个函数）。最后讲内函数作为参数返回。 然后我们讲刚才的普通函数old作为参数传进去，然后再用外函数返回的inner内函数重新赋值普通函数old，最后让我们再执行一遍old函数, 这个时候，因为old被重新赋值，其实等同于调用了inner函数。来，我们看看结果： 1234567old = outer(old) # outer返回了inner函数，赋值给了oldold()---我是外函数中的内函数1我是一个普通的函数我是外函数中的内函数2 是不是稍显繁杂？那让我们换个思路，现在我们已经先定义好了outerhe inter，两者关系不变。还是之前那些代码，那么我们如何利用装饰器来进行调用呢？ 1234# 装饰器用法@outer # 此处将outer作为了装饰器def old(): print('我是一个普通的函数') 我们在定义old函数的时候，直接加上一个@语法糖，就将outer作为了装饰器。这个装饰器的作用就等同于old = outer(old)。那让我们打印看看结果： 123456old()---我是外函数中的内函数1我是一个普通的函数我是外函数中的内函数2 那我们现在完成了装饰器的用法，按照定义，我们在不改变old函数的代码，且保持了old函数调用方法不变的情况下，增加了新的方法outer。 old函数经过outer装饰器进行了装饰，代码和调用方法不变，但是函数的功能发生了改变。 你是不是这个时候又有疑问了，那装饰器要用在什么地方呢？让我们来实现一个应用： 装饰器应用：统计函数的执行时间 在正式写代码之前，我还是习惯带着大家先思考一遍。我们需要统计函数的执行时间，那我们需要什么关键点？ 开始时间， 结束时间 开始时间和结束时间之间，就是程序在运行的过程。 好的，让我们来开始写代码，先来一段简单的需要运行的程序，为了能顺利统计时间，我们给它设定两个东西，一个循环，一个停止运行时长。这样，我们不会因为程序运行过快而看不到结果： 1234567import time# 定义一个普通函数def func(): for i in range(5): print(i, end=&quot; &quot;) time.sleep(1) 函数写完之后，让我们来执行一下看看： 1234func()---0 1 2 3 4 没问题，确实是一秒打印一次。 现在，再来让我们完成要称为装饰器的统计函数： 12345678# 定义一个统计函数执行时间的装饰器def runtime(f): def inner(): start = time.perf_counter() f() end = time.perf_counter() - start print(f'\\n函数的调用执行时间为：{end}') return inner 在函数inner中，我们最终是打印了最终的时间end， 然后将整个inner函数返回。 那么，让我们来尝试执行一下看看吧： 123456func = runtime(func)func()---0 1 2 3 4 函数的调用执行时间为：5.017943917075172 这样，我们就得到了func这个函数最后执行的时间，那现在的问题是，统计时间的函数是一个通用函数，我们很多函数中都需要用到它进行统计。但是我们总不能所有的函数都要用这种方法重新赋值之后再调用吧？ 那我们就用装饰器来解决就好了： 123456789101112# 定义一个普通函数@runtimedef func(): for i in range(5): print(i, end=&quot; &quot;) time.sleep(1) func()---0 1 2 3 4 函数的调用执行时间为：5.017888417001814 当然，最终这种函数的调用执行时间并不会像现在这样打印到前台，而是会写进log变为日志存储起来，便于之后分析使用。 装饰器嵌套语法 在这一段代码中，我们来约个妹子，完成一场约会。从哪开始呢？就从找妹子要微信开始吧： 1234567891011121314151617def begin(f): def begin_inner(): print('找妹子要微信，成功...') f() print('送妹子回家...') return begin_inner @begindef love(): print('跟妹子畅谈人生和理想...')love()---找妹子要微信，成功...跟妹子畅谈人生和理想...送妹子回家... 那这样，我们实现了一段最普通装饰器的定义。现在让我们在下面再定义一个装饰器函数，为什么呢？因为我渐渐不满足于只谈理想和人生了，要有点实际的行动了,顺便，我们写了一个列表，把和妹子要做的事情都列了个顺序，再来看看： 123456789101112131415161718192021222324252627def begin(f): def begin_inner(): print('找妹子要微信，成功... 1') f() print('送妹子回家... 5') return begin_innerdef evolve(f): def evolve_inner(): print('和妹子一起吃了个大餐.. 2') f() print('和妹子看了一场夜场电影... 4') return evolve_inner@evolve@begindef love(): print('跟妹子畅谈人生和理想... 3')love()---和妹子一起吃了个大餐.. 2找妹子要微信，成功... 1跟妹子畅谈人生和理想... 3送妹子回家... 5和妹子看了一场夜场电影... 4 这个...顺序似乎不太对啊。让我们改变一下装饰器的顺序试试： 12345678910111213@begin@evolvedef love(): print('跟妹子畅谈人生和理想... 3')love()---找妹子要微信，成功... 1和妹子一起吃了个大餐.. 2跟妹子畅谈人生和理想... 3和妹子看了一场夜场电影... 4送妹子回家... 5 这回没错了，我们在最开始要妹子微信和最后送妹子回家中间，又进行了点什么。也算是有些进展了。那么，我们怎么去理解这个程序运行顺序呢？ 先使用离得最近的begin装饰器，装饰love函数，返回了一个begin_inner函数 在使用上面的evolve, 装饰了上一次返回的begin_inner函数，又返回了一个evolve_inner函数。 在调用完成之后，就是需要顺序执行了，其执行的嵌套关系和顺序如下： 那这，就是我们嵌套装饰器的用法。当然，这种嵌套装饰器的用法并不常见，可是一旦我们遇到了，要理解他的运行机制和顺序，避免不必要的麻烦。 装饰带有参数的函数 上一个部分，我们做了一个约会妹子的函数，并且使用装饰器进行了装饰。使的我们成功的按照进度依次执行了自己的计划。 但是问题来了，我们到目前为止约会了那么多妹子，都不知道谁是谁（海王体质），这可怎么办。这次，我们吸取教训，先要名字，既然之前的流程很成功，我们直接拿来使用就行了。但是繁杂的步骤我们都去掉，直奔主题： 123# 带有参数的函数def love(name): print(f'跟{name}妹子在___畅谈人生...') 一个简单的函数，并且是一个填空题，你愿意待谁去哪里畅谈人生，随便。比如我，找「露思」去，去了哪里，恕不奉告了： 1234love('露思')---跟露思妹子在___畅谈人生... 可是即便如此，该有的流程还是不能丢，总不能凭空变出个妹子吧，还是得把必要的流程加上，原本定义的装饰器函数似乎不能使用了： 1234567891011# 定义装饰器def outer(f): def inner(): print(f'找到妹子，成功的拿到了微信...') f() print(f'约妹子去看一场午夜电影...') return inner@outerdef love(name): print(f'跟{name}妹子在___畅谈人生...') 流程上现在是没问题了，可是我们执行一下发现，报错了。 1234love('露思')---TypeError: outer..inner() takes 0 positional arguments but 1 was given 进行不下去了吧？那没办法，谁叫你之前和之后都把人家名字忘了呢，海王也得有点职业道德才行。既然我们在执行的时候有参数，那你整个过程中都得带上才行。不能玩着玩着忘记人家名字，对吧。让我们改进一下，既然我们已经知道，在使用装饰器装饰过后的love()执行实际上是执行装饰器inner， 那我们尝试给inner加上参数进行传递，还有很重要的，我们之前和之后，得把妹子名字记清楚才行，所以执行的时候也记得加上： 12345678910111213141516171819# 定义装饰器def outer(f): def inner(var): print(f'找到{var}妹子，成功的拿到了微信...') f(var) print(f'约{var}妹子去看一场午夜电影...') return inner# 带有参数的函数@outerdef love(name): print(f'跟{name}妹子在___畅谈人生...')love('露思')---找到露思妹子，成功的拿到了微信...跟露思妹子在___畅谈人生...约露思妹子去看一场午夜电影... 嗯，这样一场和「露思」妹子之间完美的从认识到约会流程就完成了。我们总结一下： 如果装饰器带有参数的函数，需要在内函数中定义形参，并传递给调用的函数。因为调用原函数等于调用内函数。 装饰多参数的函数 上一个流程跑完之后呢，我觉得还是不太妥当。主要是其中有两个选择题，一个是谁，一个是去哪里。对吧。再说了，我也得跟妹子自我介绍一下，加强一点印象。 好，我们这次再多设置几个参数，让整个约会过程更完善一些，那我们从最初就要规划一下，需要的参数包括：我，妹子，地点，行为等等。还蛮多的。 1234567891011121314# 装饰带有多参数的函数def outer(f): def inner(man,name,*args,**kwargs): print(f'{man}要到了{name}妹子的微信...') f(man, name, *args, **kwargs) print('天色渐晚...') return inner# 定义多参数的函数@outerdef love(man, name, *args, **kwargs): print(f'{man}跟{name}畅谈人生...') print(f'带{name}妹子去吃了很多美食：', args) print(f'和{name}妹子看了夜场电影:', kwargs) 这样我们就定义好了，至于*args以及**kwargs是什么，可以翻看之前的教程。 让我们现在来执行一下： 12345678love('茶桁','露思', '火锅', '海鲜', '饭后甜点', mov='封神第一部')---茶桁要到了露思妹子的微信...茶桁跟露思畅谈人生...带着露思妹子去吃了很多美食： ('火锅', '海鲜', '饭后甜点')和露思妹子看了夜场电影: {'mov': '封神第一部'}天色渐晚... 这样，多道选择题就被我们一一的化解了。相信「露思」妹子对我们的整体安排也是相当的满意了。 带有参数的装饰器 在我们平时使用Python各种第三方库的时候，不可避免的会遇到带有参数的装饰器。比如说，Django框架中的@login_required(redirect_field_name=\"my_redirect_field\")。 这种带有参数的装饰器是干嘛的呢？还是拿我们之前的海王约会流程来举例。之前的流程是都没有什么问题，可是有没有发现，所有事情都是我们自己做主了，似乎妹子一直都没有反对过，也没有说自己想要什么。这是不是不太符合现实？ 没错，我们也要给妹子装上一个会思考的大脑，也要学会做判断，好，让我们来实现一下： 既然我们这节是学习带有参数的装饰器，那么必然装饰器上是带参数的。呃，不要认为这句话是废话，我们看代码： 123@put(var)def love(): print('畅谈人生...') 就像这样，我们给装饰器加上了参数。 那么现在问题就来了，我们来看看我们之前写的装饰器函数： 1234def outer(f): def inner(): pass return inner 发现有什么问题了么？虽然我们的外层函数outer是有形参的，但是我们之前的过程中了解到，这个形参f是为了接收当前执行函数的。那还有什么其他地方接收非函数的普通参数嘛？ 既然outer中很重要的作用，除了接收函数在内函数内执行，还有一个就是返回inner内函数，那么我们在不改变outer的基础之上，再加一个接收普通参数的函数不就行了。 123456789101112131415161718192021def put(var): def outer(f): def inner1(): print('妹子给了你微信') def inner2(): print('妹子给了你她闺蜜的微信') def inner3(): print('妹子送了你一句感人肺腑的话：滚...') # 装饰器壳的参数，可以用于在函数内去做流程控制。 if var == 1: return inner1 elif var == 2: return inner2 else: return inner3 return outer@put(2)def love(): print('畅谈人生...') 定义完成之后，我们来执行一下看看： 1234love()---妹子给了你她闺蜜的微信 家人们谁懂啊，妹子真把我当海王了嘛？最后，我还是老老实实的接受了妹子的好意。 从整段代码中我们可以看出来，如果装饰器中有参数，需要有一个外壳函数来接收参数，传参之后就会进入到下一层函数中，并且传递当前对象。再然后才会再进入下一层中去。当然，我们在这里，利用传递的参数写了一段if判断，用于确定妹子的决定是什么。然后我们就返回哪个决定的函数。最后别忘记，在外壳函数中，我们还需要讲outer函数返回出去。此时虽然love函数是outer函数，但是在之前，put装饰器已经将参数传递给了外壳函数put(var)。在装饰器函数的争端代码中，我们都没有再执行过传进来的参数，也就是函数love()， 所以此段代码中love函数中的打印方法并未执行。 其执行步骤为： 1put(var) =&gt; outer() =&gt; outer(love) =&gt; inner2() 用类装饰器装饰函数 之前我们所有的代码中，装饰器一直使用的都是函数装饰器。那我们能否用类来当装饰器装饰函数呢？ 试试不就知道了。 12345678910111213# 类装饰器装饰函数class Outer(): def __call__(self, func): self.func = func return self.inner def inner(self, who): print('拿到妹子的微信...') self.func(who) print('看一场午夜电影...')@Outer()def love(who): print(f'{who}和妹子谈理想与人生...') 写完了，这下我们省略了那么多杂七杂八的流程，因为我发现，那么多流程下来，最终感动的人只有自己。妹子愿意，怎么都愿意，不愿意的，无论做多少都不愿意。 来，让我们跑一下程序试试： 123456love('茶桁')---拿到妹子的微信...茶桁和妹子谈理想与人生...看一场午夜电影... 没问题，正确的执行。那这个时候的love函数到底是什么呢？我们来打印出来看看： 1234print(love)---&lt;bound method Outer.inner of &lt;__main__.Outer object at 0x106646c80&gt;&gt; 可以看到，此时的love函数就是Outer类中的inner函数。那我们怎么理解整个代码呢？ 我们在love函数上使用了装饰器Outer, 那么这个时候Outer就会实例化出来一个对象obj，然后这个@obj就等同于obj(love)。 然后我们实例化对象进入Outer()内部，进入之后遇到了魔术方法__call__, 它会把该类的对象当作函数调用时自动触发。也就是obj()触发。 还记得类的实例化么？会传入一个参数，也就是实例化对象本身：obj。并且，第二个参数func用来接收了传递进来的函数love， 设置了self.func = func, 把传进来的函数作为对象的成员方法。最后返回了一个函数inner， 这个返回的函数是类中定义好的，于是作为实例化对象也将这个成员方法继承了下来，所以self.inner可以直接被返回出去。 这个定义好的inner接收了两个形参，一个是实例化对象本身，一个就是传递进来的函数love的参数who。然后，中间执行了一下魔术方法__call__内定义好的self.func(who)，实际上也就是obj.love(who)。 看着迷糊？这样，我写一个注释过的完整版本。 1234567891011121314151617181920212223242526272829# 类装饰器装饰函数class Outer(): # 魔术方法：当把该类的对象当作函数调用时，自动触发obj() def __call__(self, func): # 把传进来的函数作为对象的成员方法 self.func = func # 返回一个函数 return self.inner # 在定义的需要返回的新方法中，去进行装饰和处理 def inner(self, who): print('拿到妹子的微信...') self.func(who) print('看一场午夜电影...')'''Outer() 实例化对象 =&gt; obj@obj 就等于 obj(love)进入类后 =&gt; __call__(love)接收返回参数`inner()`'''@Outer()def love(who): print(f'{who}和妹子谈理想与人生...')# inner('茶桁')love('茶桁')# 此时的love就是属于`Outer`类这个对象中的inner方法print(love) 不知道这段注释代码加上刚才的解说，大家能否看懂？有没有发现，用类做装饰器比起函数装饰器反而更清晰一点？不需要写那么多外层函数和内层函数。 让我们继续... 类方法装饰函数 刚才我们将整个类都用作了一个装饰器，那我们思考一下，是不是我们还可以用类中的方法来做装饰器呢？ 说干就干，直接上代码测试： 12345678910111213141516171819202122232425262728# 用类方法装饰函数class Outer(): def newinner(f): # 把传递进来的函数定义为类方法 Outer.func = f # 同时返回一个新的类方法 return Outer.inner def inner(): print('拿到妹子微信...') Outer.func() print('看一场午夜电影...')'''Outer.newinner(love) 接收返回参数：Outer.inner'''@Outer.newinnerdef love(): print('和妹子谈谈人生喝喝茶...')# love() 等于 Outer.inner()love()---拿到妹子微信...和妹子谈谈人生喝喝茶...看一场午夜电影... 在经历了几场约会之后，我们的耐心也渐渐没了。连是谁都不管，也没耐心去谈理想了。喝点茶聊聊天就直奔主题了都是。 到目前为止以上所有形式的装饰器，包括「函数装饰器」、「类装饰器」、「类方法装饰器」，都有一个共同特点：都是在给函数去进行装饰，增加功能。 那我们这个时候就不满足了，既然能装饰函数，那是否也能装饰类呢？ 用装饰器装饰类 还真有一种装饰器是专门装饰类的，也就是在类的定义的前面使用@装饰器这种语法。和装饰函数并无什么区别，只是放在了类前面而已： 123@装饰器class Demo(): pass 装饰器给函数进行装饰，目的是不改变函数调用和代码的情况下给原函数增加新的功能。 装饰器给类进行装饰，目的是不改变类的定义和调用的情况下给类增加新的成员（属性或者方法）。 来，让我们具体的看看： 函数装饰器装饰类 123456789101112131415161718192021222324# 定义函数，接收一个类。返回修改后的类def expand(cls): def func2(): print('我是在装饰器中追加的新方法，func2') cls.func2 = func2 # 把刚才定义的方法赋值给 类 cls.name = '我是在装饰器中追加的新属性 name' # 返回时，把追加类新成员的类返回去 return cls@expand # expand(Demo) ==&gt; cls ==&gt; Democlass Demo(): def func(): print('我是Demo类中定义的func方法')Demo.func() # 此时在调用的Demo类是通过装饰器，更新过的Demo类Demo.func2()print(Demo.name)---我是Demo类中定义的func方法我是在装饰器中追加的新方法，func2我是在装饰器中追加的新属性 name 这样，我们在原来的Demo这个类中，使用装饰器增加了一个成员方法func2，并且增加了一个成员属性name， 并最终返回到原始类中。从而扩展了这个原始类Demo中的方法和属性。 类装饰器装饰类 123456789101112131415161718192021222324252627282930# 使用类装饰器装饰类class expand(): def __call__(self, cls): # 把接收的类，赋值给当前对象，作为一个属性 self.cls = cls # 返回一个函数 return self.newfunc def newfunc(self): self.cls.name = '我是在类装饰器中追加的新属性 name' self.cls.func2 = self.func2 # 返回传递进来的类的实例化结果，obj return self.cls() def func2(self): print('我是在类装饰器中追加的新方法 func2')'''expand() ==&gt; obj ==&gt; @obj(Demo) ==&gt; __call__(Demo) ==&gt; newfunc'''@expand() class Demo(): def func(self): print('我是Demo类中定义的func方法')obj = Demo() # Demo() ==&gt; newfunc() ==&gt; objobj.func()obj.func2()print(obj.name) 在之前那么多案例过后，相信大家这一段代码应该能看的出来吧。 那我这个地方要处一个思考题了，请问：此时的 obj这个对象，是哪个类的对象。Demo还是expand? 1234print(obj)---??? 这个问题的答案，我放在源码中了，大家要记得思考之后再去看答案。 那么，本节课的内容到这里也就结束了。 课程进行到这里，我们Python本身的所有内容就已经介绍完了。下节课开始，我们就要考试讲第三方库。 好，下课。","link":"/Decorator-syntax-and-application/"},{"title":"25. matplotlib","text":"Hi, 大家好。我是茶桁。 在上一节课中，我们结束了Python正式的所有内容，但是咱们的Python课程还未结束。从这节课开始，我们要来学习一下Python的第三方库。 Python的生态非常完善也非常活跃，我们不太可能讲目前所有的第三方库全部都介绍一遍，只介绍几个有影响力并且和处理数据相关的。那今天第一Part，我们就先来学习matplotlib。 在之后的课程中，Python的基础理论我就不会细讲了，咱们重点是要快速的认识和学会第三方库。有什么语法上的问题，可以翻看前面几节的教程。 matplotlib是什么？ Matplotlib是一个Python 2D绘图库，它可以在各种平台上以各种硬拷贝格式和交互式环境生成出具有 出版品质的图形。 Matplotlib可用于Python脚本，Python和IPython shell，Jupyter笔记本，Web应用 程序服务器和四个图形用户界面工具包。 Matplotlib试图让简单的事情变得更简单，让无法实现的事情变得可能实现。 只需几行代码即可生成绘图，直方图，功率谱，条形图，错误图，散点图等。 为了简单绘图，pyplot模块提供了类似于MATLAB的界面，特别是与IPython结合使用时。 对于高级用户，您可以通过面向对象的界面或MATLAB用户熟悉的一组函数完全控制线条样式，字体属性，轴属性等。 那么，我们为什么要学习matplotlib呢？ 可视化是在整个数据挖掘的关键辅助工具，可以清晰的理解数据，从而调整我们的分析方法。 能将数据进行可视化,更直观的呈现 使数据更加客观、更具说服力 例如下面两个图为数字展示和图形展示： 以上两个图形中，第一组是完全的数据。我们基本很难看出这组数据到底谁大谁小，当然，从位数上我们还是可以比较容易辨认，但是比起第二张图呢？是不是第二张图就非常清晰的展示了数据的大小，一目了然？ 那既然我们要学习的是数据可视化，我们首先要做的，必定是要先了解一下常见数据图表，知道其种类和意义。 常见图形种类及意义 我们首先要先了解具体的图形，才能知道我们在什么情况下使用什么图形来表示。 折线图：以折线的上升或下降来表示统计数量的增减变化的统计图。特点是：能够显示数据的变化趋势，反映事物的变化情况。(变化) 散点图：用两组数据构成多个坐标点，考察坐标点的分布,判断两变量之间是否存在某种关联或总结坐标点的分布模式。特点是：判断变量之间是否存在数量关联趋势,展示离群点(分布规律) 柱状图：排列在工作表的列或行中的数据可以绘制到柱状图中。特点是：绘制连离散的数据,能够一眼看出各个数据的大小,比较数据之间的差别。(统计/对比) 直方图：由一系列高度不等的纵向条纹或线段表示数据分布的情况。 一般用横轴表示数据范围， 纵轴表示分布情况。特点是：绘制连续性的数据展示一组或者多组数据的分布状况(统计) 饼图：用于表示不同分类的占比情况，通过弧度大小来对比各种分类。特点是：分类数据的占比情况(占比)。 matplotlib画图实现 首先我们要知道，Python的第三方库几乎全部都需要额外安装才行。我们之前在讲Python环境的时候有提到如何创建虚拟环境以及如何安装第三方库。那我们现在，就来直接安装一下，还记得么？我用的环境是conda， 所以我的安装命令都是使用conda的，不过你将其咱们课程中还是要使用最普遍的方式，所以以下我都会替换成pip。 1pip install motplotlib 执行完毕后，motplotlib就安装到您的Python环境内了。 建议在Python环境内安装好Jupyter来进行学习，你会发现简直太方便了。如果是在Jupyter内，那么会在代码执行之后的下方直接显示出结果，而如果我们是使用python xx.py来执行，那Python会调用内部的绘图器来进行显示。 Jupyter for vscode Python绘图器 当然，你也可以自己在命令行内起一个Jupyter notebook服务，那就可以直接在浏览器上进行操作了。 Jupyter Notebook 当然几种方法中，我还是最推崇在VSCode中进行。毕竟我们还是需要代码提示的。 在正式开始之前，让我们对matplotlib的图像结构建立一个认识： 现在让我们来具体的实现一下，做一个简单的图形： 12345678# 导入模块import matplotlib.pyplot as plt# 这段代码会让之后的代码在Jupyter内执行的时候显示图片%matplotlib inline# 传入x,y, 通过plot画图plt.plot([1,0,9],[4,5,6])# 在执行程序的时候显示图形plt.show() 在这段代码中，我们使用plot来进行了绘制，x和y分别是plot的两个参数，代表了x轴和y轴。那么这两个轴分别接受了一个列表，那就是有三个点，第一个点是(1,4), 第二个点是(0,5)， 第三个点是(9,6)。最后，我们使用show()函数来进行最终呈现。 绘图折线图 首先，我们来绘制一个折线图，这次我们用变量存储数据的方式： 1234567# 绘制折线图import matplotlib.pyplot as pltx = range(1, 8) # x轴的位置y = [17, 17, 18, 15, 11, 11, 13]# 传入x,y，plot绘图plt.plot(x, y)plt.show() 然后，我们对这个折线图进行一下设置，修改颜色和形状： 1234567# 绘制折线图import matplotlib.pyplot as pltx = range(1, 8) # x轴的位置y = [17, 17, 18, 15, 11, 11, 13]# 传入x,y，plot绘图plt.plot(x, y, color='red', alpha=0.5, linestyle='--', linewidth=3)plt.show() 我们对plot函数传递了几个参数修改了折线的样式，其中color是折线的颜色，alpha是折线的透明度（0-1）， linestyle是折线的样式， linewidth是折线的宽度。 linestyle的几个值分别是：-实线(solid)，这个也是默认值；--短线(dashed)； -.短点相间线(dashdot)； :虚线点(dotted)。 关键点样式 让我们接着进行修改这段折线样式： 1234...# 传入x,y，plot绘图plt.plot(x, y, color='red', alpha=0.5, linestyle='--', linewidth=3, marker='o')plt.show() 我们这次只增加了一个参数，很明显，marker就是关键点的样式。 折线和关键点到底有哪些值呢？我们看一下下面这个表： 值 描述 - solid line style -- dashed line style -. dash-dot line style : dotted line style . point marker , pixel marker o circle marker v triangle_down marker ^ triangle_up marker &lt; triangle_left marker &gt; triangle_right marker 1 tri_down marker 2 tri_up marker 3 tri_left marker 4 tri_right marker s square marker p pentagon marker * star marker h hexagon1 marker H hexagon2 marker + plus marker x x marker D diamond marker d thin_diamond marker | vline marker _ hline marker 如果看不懂描述的小伙伴，最直接的办法就是放到代码里直接运行一下看看。 当然，我们还可以改变关键点的大小等参数： 1plt.plot(x, y, color='red', alpha=0.5, linestyle='--', linewidth=3, marker='o', markersize='10',markeredgecolor='blue', markeredgewidth=3) 那这里面，markersize是表示关键点的大小，markeredgecolor是关键点边框的颜色，markeredgewidth就是关键点边框的宽度。 既然图片渲染出来了，那我们总是需要进行保存的。那么下面，我们就看看如何将图片保存下来，在保存之前，我们还会根据需要设置一下图片的大小。 设置图片大小和保存 1234567891011from matplotlib import pyplot as pltimport randomx = range(2, 26, 2) # x轴的位置y = [random.randint(15, 30) for i in x]# 设置图片大小plt.figure(figsize=(20, 8), dpi=80)plt.plot(x,y)# plt.show()# 保存plt.savefig('./data/img/t1.png') 我们依次来看这段代码，里面有我们认识的也有我们不认识的。其中的random是为了生成随机数，这个我们就先不管了。直接看设置部分。 figsize这参数是为了指定figure的宽和高，单位为英寸。 dpi参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为80，1英寸等于2.5cm, A4纸为21*30cm的纸张。 然后我们继续往下看，savefig就是指定目录进行保存。这里我们需要注意两点： 如果保存的目录有路径不存在，则会报错无法保存。 我们需要savefig的时候尽量不要使用show方法，因为savefig也具备了展示图片的功能，并且，如果show存在的话，在展示完图片之后，会释放figuer资源，那么savefig保存下来的图片将会是空白的。就好比我们open('file', 'a+') as fp一个文件，在使用fp.write的时候没有往里面写入内容。因为这些内容被上面一个方法清空了，但是文件我还是会保存的，只是文件内没有任何内容。 然后我们回过头来继续看我们写的这段代码，其实savefig能存储的文件格式很多，包括能够存储svg格式的矢量图。只需要plt.savefig('./t1.svg')，保存的时候换一下后缀名就可以了。 现在让我们看下保存好的图片： 绘制轴上的刻度 有没有发现，虽然我们折线图是正常的，但是似乎x轴上的刻度区间太大了，并不是所有关键点都明显展示出来了。那现在我们就来设置一下x轴和y轴的刻度。 123456789...# 设置x轴的刻度plt.xticks(x)# # 设置y轴的刻度plt.yticks(y)# 设置图片大小plt.plot(x,y)plt.show() 这段代码中我们将保存文件的代码去掉了。因为主要是进行设置，所以我们展示一下看看正确与否就行了。 我们看到现在的图片，刻度上x轴遵循了我们之前对x的设定，(2, 26, 2)， 从2开始，到25， 并且步进值为2.y轴呢？因为是随机数，所以关键点的分布并不均匀。 这个时候，我就又需要进行修改了，一个是y轴的刻度要分配的更均匀，再有就是x轴上，我希望区间为1，而不是2。让我们来对其进行下修改： 123456789...# 设置x轴的刻度plt.xticks(range(1, 25))# 设置y轴的刻度plt.yticks(range(min(y), max(y)+1))# 设置图片大小plt.plot(x,y)plt.show() 似乎图不太一样，原因是因为我在代码中使用的是随机数函数random.randint来差生y轴的数据，所以每次生成的图片都会有些不同。 我们来好好看看轴线上的刻度。确实和关键点都对应上了。并且比起关键点来说更密集一点。原因就在于，我们将x的刻度点范围改为(1, 25)，无步进值。y轴在绘制的时候也做了定义，范围设置为(最小的y值, 最大的y值+1)，同样，也是没有步进值。这样，两个轴上的刻度分布就非常均匀了。 这里的关键知识点就是：我们可以使用xticks和yticks来生成x轴刻度或者y轴刻度，并且，在其中可以传递参数来对x轴上的刻度和y轴上的刻度进行定义。 不过我们现在这个还是无法满足需求，原因就在于我们这个折线图是为了显示不同时间点上的温度变化。那么我们就必须要让x轴显示时间，而y轴显示温度。 让我们继续修改一下,这里我们就只展示轴线代码的修改： 12345678# 构造x轴刻度标签x_ticks_label = [&quot;{}:00&quot;.format(i) for i in x]# 让字旋转45度plt.xticks(x, x_ticks_label, rotation=45)# 构造y轴的刻度标签y_ticks_label = [&quot;{}℃&quot;.format(i) for i in range(min(y), max(y)+1)]plt.yticks(range(min(y), max(y)+1), y_ticks_label) 最后生成的图片： 这里，我们使用了x_ticks_label来设置了x轴的刻度上显示的信息。当然，y轴也是相同的方式。然后将label传入轴刻度生成方法xticks中进行刻度生成，在生成的时候，我们还使用了xticks的参数rotation设置为45来完成了label的旋转。 设置显示中文 不过这并未结束，matplotlib默认是只显示英文的，无法显示中文。但是我们无论是刻度，图标题，很多时候都必须显示中文。该怎么办呢？ 接下来就让我们来看看如何修改matplotlib的默认字体。这一段，我们重新写一个需求，来看看2个小时内每分钟跳动变化。 1234567891011121314151617181920from matplotlib import pyplot as pltfrom matplotlib import font_managerimport randomx = range(0, 120)y = [random.randint(10, 30) for i in range(120)]plt.figure(figsize=(20, 8), dpi=80)plt.plot(x,y)# 设置字体和Labelmy_font = font_manager.FontProperties(fname='/System/Library/Fonts/PingFang.ttc', size=18)plt.xlabel('时间', fontproperties=my_font)plt.ylabel('次数', fontproperties=my_font)# 设置标题plt.title('每分钟跳动次数', fontproperties=my_font, color='red')plt.show() 我们引入了font_manager，然后利用它设置了我们需要用到的字体（必须是中文字体）给到一个变量my_font内。最后在设置label的时候，将字体设置为这个变量。 这样，我们就完成了中文字体的显示。 作为对比，我们来看看这样设置的是什么样： 123456...plt.xlabel('时间')plt.ylabel('次数')plt.title('每分钟跳动次数',color='red')plt.show() 一图多线 大多数时候，我们的一张图表上可能不仅需要一条线。而是两条线相互交错。这就形成了两组数据的对比，我们打个比方来说：我们正在和一位同事竞争销售额，需要查看去年（2022年）全年的数据对比： 12345678910111213141516171819202122232425# 销售额数据对比y1 = [20,10,10,20,40,30,40,40,50,60,50,40]y2 = [10,30,20,30,40,20,10,30,30,80,30,20]x = range(1, 13)# 设置图形plt.figure(figsize=(20, 8), dpi=80)plt.plot(x, y1, color='orange', label=&quot;茶桁&quot;)plt.plot(x, y2, color='green', label='同事')# 设置x轴刻度xtick_labels = ['{}月'.format(i) for i in x]my_font = font_manager.FontProperties(fname='/System/Library/Fonts/PingFang.ttc', size=24)plt.xticks(x, xtick_labels, fontproperties=my_font, rotation=45)# 绘制网格plt.grid(alpha=0.4) #网格也可以设置样式，这里透明度为0.4# 添加图例(注意：只有在这里需要添加prop参数是显示中文，其他的都用fontproperties)# 设置位置loc : upper left、 lower left、 center left、 upper centerplt.legend(prop=my_font, loc='upper right')plt.show() 我们在代码中设置两两组数据，分别为y1, y2。这两组数据一共12个，对应了12个月份。然后我们在plot方法中设置了线条的颜色，设置了这条线条对应的数据，并且添加了label。意在对这个线条写个说明。 在之后，我们添加了grid网格，意图让线条上的关键点更明显。 最后使用legend来完成plot中设置的label的显示，并在其中设置了显示所用字体。这里需要注意的是，在legend方法中设置字体所用的形参是prop而非fontproperties。在最后，我们使用loc设置了这两个label显示的位置，其中的关键字upper right代表的是两个方位「上，右」，来确定显示位置为右上角。 最后，我们绘制的图片显示如图： 从图上能明显看出来，我的销售数据是在稳步上升的，而同事起伏比较大。大部分时候我占优势，可是旺季时顶峰数据同事比我高很多，所以说基本上是各有千秋。 基于折线图，我们再来看几个拓展的部分。 首先，我们在绘图的时候，实际上是支持多个坐标系绘制在一张图上的。这也经常是数据图对比经常用到的方式： 123456789101112131415161718# 多个坐标系子图 add_subplot方法，给figure新增子图import matplotlib.pyplot as pltimport numpy as npx = np.arange(1, 100)fig = plt.figure(figsize=(20, 10), dpi=80)# 子图1ax1 = fig.add_subplot(2,2,1)ax1.plot(x,x)# 子图2ax2 = fig.add_subplot(2,2,2)ax2.plot(x, x**2)ax2.grid(color='r', linestyle='--', linewidth=1, alpha=0.3)# 子图3ax3 = fig.add_subplot(2,2,3)ax3.plot(x, np.log(x))plt.show() 我们利用add_subplot方法，在一个figure上添加了三个子图。 其次，有一些时候，我们需要对坐标轴范围进行设定。 123456789101112131415# 设定坐标轴范围x = np.arange(-10, 11, 1)y = x ** 2plt.plot(x,y)# plt.xlim([-5,5])# 单边调整# plt.xlim(xmin=-4)# plt.xlim(xmax=4)plt.ylim(ymin=0)plt.xlim(xmin=0)plt.show() 在这段代码中，我展示了三个调整范围的方式。 第一个是使用数组划定范围来进行调整。 第二个方式是分别设定一边的值（最小值或者最大值）。 最后一个方式是只设定x轴和y轴的最小值。下图展示的是第三个方式绘制的图： 当然，坐标轴并不会是一成不变的。有的时候我们可能需要y轴在x轴的正中间。所以我们需要改变坐标轴的默认显示方式 我们先来看看原本的图默认样式是什么样： 1234567# 改变坐标轴的默认显示方式y = range(0, 14, 2)x = [-3, -2, -1, 0, 1, 2, 3]plt.figure(figsize=(20,8), dpi=80)plt.plot(x,y)plt.show() 然后我们对这张图进行修改，获取图像之后设置四周边线，并且移动底边，移动到y轴的0位置 1234567891011121314151617181920# 改变坐标轴的默认显示方式y = range(0, 14, 2)x = [-3, -2, -1, 0, 1, 2, 3]plt.figure(figsize=(20,8), dpi=80)# 获得当前图表的图像ax = plt.gca()# 设置图形四周的边线ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.spines['bottom'].set_color('blue')ax.spines['left'].set_color('red')# 设置底边的移动范围，移动到y轴的0位置，`data`: 移动轴的位置到交叉轴的指定坐标ax.spines['bottom'].set_position(('data', 0))ax.spines['left'].set_position(('data', 0))plt.plot(x,y)plt.show() 代码中，我们使用gca()获取了图像赋值给变量ax，然后对其进行修改，spines可以修改四周边线，包括颜色和位置等。分别设置完颜色之后，我们分别对底边和左边使用了set_position进行了位移。移动到0点位置。那其实，底边对应的就是x轴，左边对应的就是y轴。 绘制散点图 我们拿到了一组数据，就是今年3月份每天的最高气温，现在我想在图表上进行展示。为了展示气温的分布，我们准备使用散点图进行展示。 12345678# 绘制散点图y = [11,17,16,11,12,11,12,6,6,7,8,9,12,15,14,17,18,21,16,17,20,14,15,15,15,19,21,22, 22,22,23]x = range(1,32)plt.figure(figsize=(20,8),dpi=80)# 使用scatter绘制散点图plt.scatter(x, y, label='3月份')plt.show() 本来这样就已经完成绘制了。不过我们看一下图表，虽然散点是绘制完成了，但是整张图上我们看不出太多信息，包括刻度值，月份等等，另外，我记得咱们之前加过图例，现在我们都加上： 1234567891011121314151617181920from matplotlib import font_manager# 绘制散点图y = [11,17,16,11,12,11,12,6,6,7,8,9,12,15,14,17,18,21,16,17,20,14,15,15,15,19,21,22, 22,22,23]x = range(1,32)plt.figure(figsize=(20,8),dpi=80)# 使用scatter绘制散点图plt.scatter(x, y, label='3月份')my_font = font_manager.FontProperties(fname='/System/Library/Fonts/PingFang.ttc', size=12)_xticks_labels = ['3月{}日'.format(i) for i in x]plt.xticks(x[::3], _xticks_labels[::3], fontproperties=my_font, rotation=45)plt.xlabel('日期', fontproperties=my_font)plt.ylabel('温度', fontproperties=my_font)# 图例plt.legend(prop=my_font)plt.show() 以上代码中的内容，基本都是咱们之前学过的内容，我就不多做解释了。其中，我们可以看到，plt.之后的方法就是绘制不同的图形，之前我们看到的plot是折线图，这次我们学到scatter是散点图。在图形绘制之后，剩下的就是对其进行修饰和设置。 绘制条形图 今天刷到一条新闻，说目前2023年暑期档电影的票房基本已经定型，目前全部累计票房已经超过2019年称为历史最高暑期档，而最引人瞩目的是，国产电影全线压制好莱坞大片。这真是一个值的骄傲的事情。那现在，我们就来展现一下暑期档电影票房的对比吧,我们一步一步来，最开始当然是拿到数据。 本数据来源于猫眼，2023年8月20日14:00的实时数据，后期未下线的电影数据可能会有变化。 12a = ['消失的她','碟中谍7:致命清算','芭比','八角笼中','茶啊二中','热烈','长安三万里','巨齿鲨2:深渊','封神第一部','孤注一掷']b = ['35.22','3.46','2.47','21.92','3.66','8.69','17.56','7.53','22.36','26.48'] 然后我们就可以开始绘制柱状图了： 123plt.figure(figsize=(20,8), dpi=80)plt.bar(range(len(a)), [float(i) for i in b], width=0.3)plt.show() 现在图形是绘制出来了，但是完全无法让人满意。我们并不知道哪个柱子是哪个电影的，并且对比之下，我们只能看出大概高低，并不知道具体的票房。那我们现在来进行修改, 先加上x轴和y轴上的刻度标识，并且将y轴刻度范围放大。 12plt.xticks(range(len(a)), a, fontproperties=my_font)plt.yticks(range(0,51,5), range(0,51,5)) 目前的图我们是能看出谁是谁了，而且比起刚才看起来y轴上也舒服了很多，没有顶天立地。 接下来，我们继续修改。给柱子加上颜色，好做区分。并且将绘制的图形赋值给到变量rects，在下面我们好对每一根柱子上写一个数字，将票房数值写上去。这样，具体的票房我们就能一目了然。 1234567891011121314a = ['消失的她','碟中谍7:致命清算','芭比','八角笼中','茶啊二中','热烈','长安三万里','巨齿鲨2:深渊','封神第一部','孤注一掷']b = ['35.22','3.46','2.47','21.92','3.66','8.69','17.56','7.53','22.36','26.48']plt.figure(figsize=(20,8), dpi=80)rects = plt.bar(range(len(a)), [float(i) for i in b], width=0.3, color=['r','g','b','r','g','b','r','g','b','r'])plt.xticks(range(len(a)), a, fontproperties=my_font)plt.yticks(range(0,51,5), range(0,51,5))# 加标注(水平居中)for rect in rects: height = rect.get_height() plt.text(rect.get_x()+rect.get_width() / 2, height+1, str(height), ha='center')plt.show() 这段代码中其他的都好理解，就是加标注这一段。我们使用for从rects中分别获取到每一根柱子，然后设定了一个高，这个高度就是柱子的实际高度。再设定一个text给到这根柱子，text的高度为柱子的定位为每根柱子的x轴位置+其宽度的二分之一，高度为柱子高度+1， 字符串为柱子高度本身，设置水平居中。 来，让我们看看效果： 现在，我们的显示虽然还是不完美，但是已经非常清晰了。 当然，柱状图除了竖向的，还有横向，但是横向就不能称之为柱状图，而是条形图： 123456789101112131415# 横向条形图a = ['消失的她','碟中谍7:致命清算','芭比','八角笼中','茶啊二中','热烈','长安三万里','巨齿鲨2:深渊','封神第一部','孤注一掷']b = ['35.22','3.46','2.47','21.92','3.66','8.69','17.56','7.53','22.36','26.48']plt.figure(figsize=(20,8), dpi=80)rects = plt.barh(range(len(a)), [float(i) for i in b], height=0.5, color=['r','g','b','r','g','b','r','g','b','r'])plt.xticks(range(0,51,5), range(0,51,5))plt.yticks(range(len(a)), a, rotation=45, fontproperties=my_font)# 加标注(水平居中)for rect in rects: width = rect.get_width() plt.text(width+1, rect.get_y()+0.5/2, str(width)+' 亿', va='center', fontproperties=my_font)plt.show() 其方法和原理都和柱状图是一样的，不同的点就在于柱状图的方法是bar，横向条形图的方法为barh。另外，别忘了切换x,y轴。 在这里，恭喜《封神》破22亿，加油... 我们平时看到的柱状图或者条形图，一定不只是这样一根一根的，还有那种两根或两根以上并列的对吧？其形式也很简单，就是在当前的柱子旁边多加一根而已： 12345678910# 多条并列index = np.arange(4)BJ = [50,55,53,60]SH = [44,66,55,41]# 并列plt.bar(index,BJ, width=0.3)plt.bar(index+0.3, SH, width=0.3, color='green')plt.xticks(index+0.3/2, index)plt.show() 直方图 这个图形的学习，我们还是拿电影数据来做，但是这次我们不拿刚才用过的数据了，我们拿到一个250部电影的时长数据，现在我们要统计处这些电影时长的分布状态。（比如，市场为100分钟到120分钟的电影数量，出现频率等），我们该如何去做呢？ 来，让咱们尝试一下，还是老样子，先落位数据： 12345times = [131,98,125,131,124,139,131,117,128,108,135,138,131,102,107,114,119,128,121,142,127,130,124,101,110,116,117,110,128,128,115,99,95,138,117,111,78,132,124,113,150,110,117,136,126,134,95,144,105,126,130,126,130,126,116,123,106,112,138,123,99,136,123,117,119,105,137,123,128,125,104,109,134,125,127,105,120,107,129,116,108,132,103,136,118,102,120,114,105,115,132,145,119,121,112,139,125,138,109,132,134,156,106,117,127,144,139,139,119,140,83,110,102,123,107,143,115,136,118,139,123,112,118,125,109,119,133,112,114,122,109,106,123,116,131,127,115,118,112,135,115,146,137,116,103,144,83,123,111,110,111,100,154,136,100,118,119,133,134,106,129,126,110,111,109,141,120,117,106,149,122,122,110,118,127,121,114,125,126,114,140,103,130,141,117,106,114,121,114,133,137,92,121,112,146,97,137,105,98,117,112,81,97,139,113,134,106,144,110,137,137,111,104,117,100,111,101,110,105,129,137,112,120,113,133,112,83,94,146,133,101,131,116,111,84,137,115,122,106,144,109,123,116,111,111,133,150,134,76,104] print(len(times))---250 没问题，250个数据齐了。现在让我们来开始绘图： 123456789101112plt.figure(figsize=(20,8), dpi=80)# 设置组距distance = 2# 计算组距group_num = int((max(times) - min(times)) / distance)plt.hist(times, bins=group_num)plt.xticks(range(min(times), max(times))[::2])plt.grid(linestyle='--', alpha=0.5)plt.xlabel('电影时长大小', fontproperties=my_font)plt.ylabel('电影的数据量', fontproperties=my_font)plt.show() 从代码中看，直方图的方法是hist, 其中参数为需要展示的数据和组距。 x轴刻度上，我们将最小的电影时长和最大的电影时长顺序分布，步进值为2。 现在我们从图中能看到，110 ～ 112这个时间段的电影数量是最多的，其次就是116 ~ 118分钟的。 现在，我们更清晰的感受了i库55起1422 直方图的作用。 饼状图 最后我们来看看饼状图，饼状图相信大家平时看的也很多。基本上，我们遇到比例，份额对比的时候会使用这个图形。 来，让我们先绘制一个基本图形： 12345size = [55, 35, 10] # 各部分大小plt.figure(figsize=(20,8), dpi=100)plt.pie(size)plt.show() 现在让我们在这个基础的饼状图上进行设置和修改，首先，我们需要这个图形各个区域都有个名称，然后是我们自己设定下他们的颜色，而不是用默认的，接着，我们需要将其中一部分突出显示。 12345# 修改生成饼图代码plt.pie(size, explode=[0, 0.05, 0], colors = ['r','g','b'], labels = ['第一部分','第二部分','第三部分']) 现在是按照我刚才希望的去变化了，但是并不让人满意。我们需要将文字设置成中文显示，然后整个图形旋转一下，让突出的部分部要向下，接着我们希望显示出百分比，并且有一个图例，分别标识出颜色和区域的文字。 12345678910111213141516171819202122232425262728size = [55, 35, 10] # 各部分大小plt.figure(figsize=(20,8), dpi=100)label_list = ['第一部分','第二部分','第三部分'] # 各部分标签color = ['red', 'green', 'blue'] # 各部分颜色explode = [0, 0.05, 0] # 各部分突出值patches, l_text, p_text = plt.pie(size, explode = explode, colors = color, labels = label_list, labeldistance=1.1, autopct='%1.1f%%', shadow=False, startangle=150, pctdistance=0.6)for t in l_text: print(dir(t)) t.set_fontproperties(my_font)for t in p_text: t.set_size(18)for i in patches: i.set_color('pink') breakplt.legend(prop=my_font)plt.show() 我们在这段代码中，将之前在pie方法中的三组列表数据拿出来赋值给了三个变量，然后增加了一些其他参数用于标签距离、设置旋转、百分比距离等。 然后将生成的图赋值给到三个变量，patches, l_text, p_text，这三个变量分别接受的参数为「饼的区域」、「饼的外部说明文字」、「饼的内部标识文字」。 然后，对这三个变量分别进行设置。 最后，将图例的字体设置一下，然后显示出来。 总结 今天的课程到这里也就结束了。最后让我们来总结一下 最后，我们留个作业吧，好久没留作业了。将咱们之前做的这个饼图加上阴影，扩大分离的那块区域的分离距离。如图： 小伙伴们，记得认真学习并且完成作业。 好，下课。","link":"/AI-Python-matplotlib/"},{"title":"26. NumPy","text":"Hi，大家好。我是茶桁。 上一节课中，我们学习了matplotlib. 实际上，我们已经进入了数据可视化阶段。 可是在上一节课中，所有的数据都是我们固定写好的，包括两个电影的数据展示的案例（柱状图和直方图），都是我们将数据手动写成了数据列表，然后直接使用。 在我们平时的工作中，不太有那么多的机会使用现成的数据，除非你跟我一样是一个数据产品经理，那倒是会有程序员们会将数据整理好递到你手上。可是即便如此，很多时候我还是需要自己亲手去处理数据，因为我不能为了一次数据验证或者一个什么想法就去惊动程序员为你服务。 更何况，我们现在的课程面向的是人工智能，那么处理数据就成了必须要有的手段，也是家常便饭常有的事。 那么今天，我们就来学习一下Python中科学计算的基础第三方库：「numpy」。 简单介绍一下NumPy Numpy的全称是：Numerical Python, 是一个开源的Python科学计算库，用于快速处理任意维度的数组。 Numpy支持常见的数组和矩阵操作，对于同样的数值计算任务，使用Numpy比直接使用Python要简洁的多。其中的ndarray对象用于处理多维数组，该对象是一个快速而灵活的大数据容器。 相对于直接使用Python，NumPy具有一下优势： 对于同样的数值计算任务，使用NumPy要比直接编写Python代码便捷得多； NumPy中的数组的存储效率和输入输出性能均远远优于Python中等价的基本数据结构，且其能够 提升的性能是与数组中的元素成比例的； NumPy的大部分代码都是用C语言写的，其底层算法在设计时就有着优异的性能，这使得NumPy 比纯Python代码高效得多. 说那么多我们不如直接来一次对比显得更直接一点，让我们来计算100000000个数字的加法运算。 123456789101112131415161718192021222324import randomimport timeimport numpy as npa = []for i in range(100000000): a.append(random.random())t1 = time.time()# Python 处理sum_py = sum(a)t2 = time.time()b = np.array(a)t4 = time.time()# NumPy 处理sum_np = np.sum(b)t5 = time.time()print(f'Python:{t2-t1}, NumPy:{t5-t4}')---Python:1.782839059829712, NumPy:0.2144303321838379 在以上代码中，t2-t1为使用python自带的求和函数消耗的时间，t5-t4为使用numpy求和消耗的时间。我们看到了时间对比，是不是为NumPy效率的提升感到惊讶？实际上，在一些更早一些配置差一点的电脑上，这个差距还会更大。记得我以前曾经做过一样的事情，时间大概为Python: 5s, NumPy:0.5s。差了有10倍左右。那么我们也能看出来了，ndarray的计算速度快了很多，为我们节约了大量时间。 ndarray对象 N维数组对象ndarray可以说是NumPy中最重要的一个特点了，它是一个系列同类型数据的集合，以0下标为开始进行集合中元素的索引。ndarray对象是用于存放同类型元素的多维数组。 让我尝试创建一些一维数组，Numpy创建数组有三种不同的方式，1. 直接传入列表的方式； 2. 传入range生成序列； 3. 直接使用np.arange()生成数组。让我们一一来实现下： 1234567891011121314151617181920212223# 创建数组的多种形式# 1. 直接传入列表的方式list1 = [1, 2, 3, 4]oneArray = np.array(list1)print()print(f'oneArray: {oneArray, type(oneArray)}')t1 = np.array([1, 2, 3, 4])print(f't1: {t1, type(t1)}')# 2. 传入range生成序列t2 = np.array(range(10))print(f't2: {t2, type(t2)}')# 3. 使用numpy自带的np.arange()生成数组t3 = np.arange(0, 10, 2)print(f't3: {t3, type(t3)}')---oneArray: (array([1, 2, 3, 4]), &lt;class 'numpy.ndarray'&gt;)t1: (array([1, 2, 3, 4]), &lt;class 'numpy.ndarray'&gt;)t2: (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), &lt;class 'numpy.ndarray'&gt;)t3: (array([0, 2, 4, 6, 8]), &lt;class 'numpy.ndarray'&gt;) 无论用哪种方式，我们可以看到最终打印的类型都是numpy.ndarray。 那以上是一维数组的创建，我们再来看看二维数组： 123456789# 二维数组list2 = [[1,2],[3,4],[5,6]]twoArray = np.array(list2)print(twoArray)---[[1 2] [3 4] [5 6]] 对于二位数组的理解，我们可以将其看成行跟列。如果是在Excel中，那么1,2就是一行，3,4是一行，5,6是一行。而[1,3,5]就是一列，[2,4,6]也是一列。 那么，我们在看到数据之前怎么知道这组数据的维度呢？可以使用方法ndim， 顺便，我们来学习一下一些常用属性。 12345678910111213# 获取数组的维度print(twoArray.ndim)# 获取数据的形状（行、列）print(twoArray.shape)# 获取数组的元素个数print(twoArray.size)---2(3, 2)6 这样，我们就可以看到数组的一些相关属性，拿来在处理数据前做参考。其中，获取到的shape是一个元组数据，而我们获取到的size可以看成是元组内的数据相乘。 现在我们来看，我们能否在numpy中调整数组的形状呢？来，尝试一下，当前我们有一组二维数组： 1[[1,2,3],[4,5,6]] 将其转变为ndarray并赋值给一个变量arr_1， 既然我们前面知道获取数据的形状是用shape， 那我们尝试直接更改它的shape看看是否可行： 12345678arr_1 = np.array([[1,2,3],[4,5,6]])arr_1.shape = (3, 2)print(arr_1)---[[1 2] [3 4] [5 6]] 居然可以。 不过虽然这样能够对数组形状进行修改，不过在NumPy中正确的修改方式应该是使用reshape： 123456789# 返回一个新的数组arr_1 = arr_1.reshape(arr_1.shape)print(f'\\narr_1:\\n{arr_1}')---arr_1:[[1 2] [3 4] [5 6]] 这次我们使用了数组第一次修改后的形状，所以整个和之前没有差别，让我们再试试其他的： 123456789101112# 将多维变成一维数组arr_2 = arr_1.reshape((arr_1.size), order='F')print(f'\\narr_2:\\n{arr_2}')arr_3 = arr_1.flatten(order='F')print(f'\\narr_3:\\n{arr_3}')---arr_2:[1 3 5 2 4 6]arr_3:[1 3 5 2 4 6] 后方那个形参order是一个可选参数，是读取元素的索引顺序，有C, F, A三个固定值。C为行有限，F为列有限, A为按数据存储顺序。 如果只是转为一维数组，使用reshape还需要知道数组的元素个数，不如flatten来的方便。这是一个专门用于将数组折叠成一维数组的方法。 让我们来看看reshape转换数组形状的其他几个例子： 12345678910111213141516171819202122232425262728293031323334353637# 数组的形状t = np.arange(24)print(f't:\\n{t}')print(t.shape)# 转换成二维t1 = t.reshape((4,6))print(f'\\nt1:\\n{t1}')print(t1.shape)# 转换成三维t2 = t1.reshape((2, 3, 4))print(f'\\nt2:\\n{t2}')print(t2.shape)---t:[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23](24,)t1:[[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]](4, 6)t2:[[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19][9, 12, 88, 14, 25] &lt;class 'list'&gt; [20 21 22 23]]](2, 3, 4) 在数据转为了ndarray之后，方便了我们在NumPy中进行计算等操作，不过很多时候，最终我们还是要转回Python内的List。如果需要进行转换，直接使用tolist()就可以了。 1234567# 将数组转为lista = np.array([9, 12, 88, 14, 25])items = a.tolist()print(items, type(items))---[9, 12, 88, 14, 25] &lt;class 'list'&gt; NumPy的数据类型 在之前的Python基础课程中，我用几节课给大家讲了一遍Python中的数据类型。同样的，在NumPy中也有一些不同的数据类型，大多数时候，他们都可以进行转换。 来直接上代码看几个例子： 1arr = np.array([1, 2, 3, 4, 5], dtype=np.int16) 这样，我们就在Numpy中生成了一个int16类型的数组。我们来看看这组数据的元素长度和类型： 12345678# 返回数组中每个元素的直接单位长度print(arr.itemsize)# 获取数据类型print(arr.dtype)---2int16 当然，就如我们之前说的，数据的类型之间是可以进行转换的，转换也十分方便，直接使用类型方法就可以了。 123456# 调整数据类型arr_2 = arr.astype(np.int64)print(arr_2.dtype)---int64 这里给大家多讲一个小技巧，让我们看看如何生成随机小数： 123456789# 使用Python语法，保留两位print(round(random.random(), 2))# Numpy生成数组arr_3 = np.round([random.random() for i in range(10)],2)print(arr_3)---0.47[0.97 0.81 0.1 0.23 0.66 0.98 0.06 0.44 0.33 0.14] 既然我们知道了dtype是numpy中的数据类型，那么对于数组来说，都有哪些类型呢？这里给大家一个表： 名称 描述 简写 np.bool 用一个字节存储的布尔类型（True或False） 'b' np.int8 一个字节大小，-128 至 127 （一个字节） 'i' np.int16 整数，-32768 至 32767 （2个字节） 'i2' np.int32 整数，-2 31 至 2 32 -1 （4个字节） 'i4' np.int64 整数，-2 63 至 2 63 - 1 （8个字节） 'i8' np.uint8 无符号整数，0 至 255 'u' np.uint16 无符号整数，0 至 65535 'u2' np.uint32 无符号整数，0 至 2 ** 32 - 1 'u4' np.uint64 无符号整数，0 至 2 ** 64 - 1 'u8' np.ﬂoat16 半精度浮点数：16位，正负号1位，指数5位，精度10位 'f2' np.ﬂoat32 单精度浮点数：32位，正负号1位，指数8位，精度23位 'f4' np.ﬂoat64 双精度浮点数：64位，正负号1位，指数11位，精度52位 'f8' np.complex64 复数，分别用两个32位浮点数表示实部和虚部 'c8' np.complex128 复数，分别用两个64位浮点数表示实部和虚部 'c16' np.object_ python对象 'O' np.string_ 字符串 'S' np.unicode_ unicode类型 'U' 数组的计算 numpy的广播机制在运算过程中，加减乘除的值被广播到所有的元素上面： 1234567891011121314151617181920212223242526272829303132333435t1 = np.arange(24).reshape((6,4))print('原数组：\\n', t1)print('加2:\\n', t1+2)print('乘2:\\n', t1*2)print('除2:\\n', t1/2)---原数组： [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15] [16 17 18 19] [20 21 22 23]]加2: [[ 2 3 4 5] [ 6 7 8 9] [10 11 12 13] [14 15 16 17] [18 19 20 21] [22 23 24 25]]乘2: [[ 0 2 4 6] [ 8 10 12 14] [16 18 20 22] [24 26 28 30] [32 34 36 38] [40 42 44 46]]除2: [[ 0. 0.5 1. 1.5] [ 2. 2.5 3. 3.5] [ 4. 4.5 5. 5.5] [ 6. 6.5 7. 7.5] [ 8. 8.5 9. 9.5] [10. 10.5 11. 11.5]] 除了和数字进行计算之外，同种形状的数组之间也是可以进行计算（对应位置进行计算操作）。 1234567891011121314151617181920t1 = np.arange(24).reshape(6,4)t2 = np.arange(100, 124).reshape(6,4)print('相加:\\n',t1+t2)print('相乘:\\n',t1*t2)---相加: [[100 102 104 106] [108 110 112 114] [116 118 120 122] [124 126 128 130] [132 134 136 138] [140 142 144 146]]相乘: [[ 0 101 204 309] [ 416 525 636 749] [ 864 981 1100 1221] [1344 1469 1596 1725] [1856 1989 2124 2261] [2400 2541 2684 2829]] 那么，不同形状的多维数组能否可以计算呢？来，一起试试看： 12345678910111213141516t1 = np.arange(24).reshape((4,6))t2 = np.arange(18).reshape((3,6))print(t1)print(t2)print(t1-t2)---[[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]][[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17]]ValueError: operands could not be broadcast together with shapes (4,6) (3,6) 做相加操作的时候报错了，所以我们平时在处理数据的时候一定要多注意这种情况发生。不同形状的多维数组是不能进行计算的。 我们继续来往后做实验看看，行数或者列数相同的一维数组和多维数组可以进行计算吗？ 先看看行形状相同的情况： 123456789t1 = np.arange(24).reshape(4,6)t2 = np.arange(0, 6)print(t1 - t2)---[[ 0 0 0 0 0 0] [ 6 6 6 6 6 6] [12 12 12 12 12 12] [18 18 18 18 18 18]] 看到结果我们就明白了，多维数组中的每一行数组都分别和一维数组中的数据进行操作，也就是会与每一行数组的对应位相操作。 那么列形状相同是否也是相同情况？ 123456789t1 = np.arange(24).reshape(4,6)t2 = np.arange(4).reshape(4,1)print(t1-t2)---[[ 0 1 2 3 4 5] [ 5 6 7 8 9 10] [10 11 12 13 14 15] [15 16 17 18 19 20]] 就跟预料的一样，每一列的数组的对应位都进行了操作。 数组的轴 在理解了一维数组和二维数组之后，我们来看看数组中的轴。 什么是轴？在Numpy中，我们可以讲轴理解为方向，使用0,1,2数字来表示，对于一个一维数组，只有一个0轴。二维数组（shape(2,2)）呢，就有0轴和1轴，那同理向上推断，三维数组(shape(2,2,3))会有0，1，2三个轴。 那么我们到底为什么要了解和学习轴呢？有了轴的概念之后，我们计算讲会更加方便，比如计算一个二维数组的平均值，必须制定是计算哪个方向上面的数字的平均值。 下图中，我列出了不同数组的轴，看着图相信会好理解很多： 一维数组： 二维数组： 三维数组： 看完图之后，让我们再放到代码里去理解一下轴的概念,我们先创建一个二维数组，然后用数组内的数据分别从0轴和1轴进行相加计算总和，得出的结果如下： 1234567a = np.array([[1,2,3], [4,5,6]])print(np.sum(a, axis=0))print(np.sum(a, axis=1))---[5 7 9][ 6 15] 也就是说，我们按0轴算总和的时候，是列相加(1+4,2+5,3+6)，我们按1轴相加算总和的时候，是行相加(1+2+3, 4+5+6) 再让我们看看后面形参上我们不给行列值，计算的总和会是多少？ 1234print(np.sum(a))---21 也就是这个数组内的所有数字相加得到的结果。 在理解了二维数组行列相加的不同之后，我们再来看看三维数组，让我们先创建一个三维数组： 123456789101112131415a = np.arange(27).reshape(3,3,3)print(a)---[[[ 0 1 2] [ 3 4 5] [ 6 7 8]] [[ 9 10 11] [12 13 14] [15 16 17]] [[18 19 20] [21 22 23] [24 25 26]]] 接着，我们来分别按不同数轴进行计算来看看： 1234567# axis = 0print(np.sum(a, axis=0))---[[27 30 33] [36 39 42] [45 48 51]] 1234567# axis = 1print(np.sum(a, axis=1))---[[ 9 12 15] [36 39 42] [63 66 69]] 1234567# axis = 2print(np.sum(a, axis=2))---[[ 3 12 21] [30 39 48] [57 66 75]] 虽然最后相加之后得到的都是3X3的二维数组，但是计算结果大相径庭。我们就不一一分析，只拿计算结果的第一个数字来看，就能明白，其他数字都是一样的计算方法： axis=0, 第一行的第一个数字得到过程为：0+9+18； axis=1, 第一行的第一个数字得到的过程为： 0+3+6； axis=2, 第一行的第一个数字得到的过程为:0+1+2； 那现在，我们是不是对于轴相加的计算过程就比较理解了？ 最后总结一下，在计算的时候可以想象成是每一个坐标轴，分别计算这个轴上面的每一个刻度上的值，或者在二维数组中记住0表示列1表示行。 索引和切片 在学习Python基础课程的时候，我们应该已经了解过索引和切片的概念，在Numpy数组中这个概念也并无不同，一样也是开始值，结束值，步进值(start:stop:step)来进行切片操作。 12345a = np.arange(10)print(a[2:7:2])---[2 4 6] 我们创建了一个一维数组，然后从索引2开始到索引7停止，间隔为2。 和Python中的下标索引方式相同，在这个以为数组中，我们使用[2]一样可以获取到下标为2的那个元素： 1234print(a[2], a)---2 [0 1 2 3 4 5 6 7 8 9] 还记得[2:]代表的含义吗？不太记得的话，我们来看看在Numpy的数组中操作会怎么样，其结果和Python对列表进行操作一样： 1234print(a[2:])---[2 3 4 5 6 7 8 9] 那以上这些操作我们都是对一维数组进行操作，大家可能还相对比较好理解。因为我们可以将其代入成一个Python列表，那对列表进行索引和切片的操作我们之前已经学过。 可是如果是多维数组进行操作，又和一维数组有什么不同呢？我们先来创建一个4行6列的二维数组： 12345678t1 = np.arange(24).reshape(4,6)print(t1)---[[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]] 然后我们现在来一样对其使用[2]来进行操作： 1234print(t1[2])---[12 13 14 15 16 17] 是不是和一维数组不太一样？在t1数组中，我们打印的数据应该是它按行往下数第3行，那其实就是行下标的2, 按行数从上往下数的下标分别为[0, 1, 2] 那么我们如果使用切片的话，以这种方式去思考，应该是从当前下标的行往后取，对吧？对不对我们直接来试试就知道了： 12345print(t1[2:])---[[12 13 14 15 16 17] [18 19 20 21 22 23]] 这是从当前行开始向下连续取多行。 那如果是取中间的连续部分呢？ 12345print(t1[1:3])---[[ 6 7 8 9 10 11] [12 13 14 15 16 17]] 这样，我们就取到了第二和第三行。 有的时候，我们可能需要隔行取值： 123456print(t1[[0,2,3]])---[[ 0 1 2 3 4 5] [12 13 14 15 16 17] [18 19 20 21 22 23]] 我们现在是知道了如何按行来获取数组中的数据，同样，我们还可以按列来获取。 在学习如何按列获取数据之前，我们先来看看如何获取到某行某列的那单个数据： 1234print(t1[2,3])---15 这样，表示的意思就是第二行第三列，我们如愿取到了15这个值。 而从这种方式我们基本就可以看的出来，在获取数据的时候，我们用到了[axis1,axis0]这样的形式去准确的定位。知道了这个，就简单了。 既然,号之前的数值代表行下标，那,之后的数值应该就是列下标。一样，从0开始。和取行不同的地方在于，我们在取行的时候可以在,号之后加:这个符号来表示所有列，当然也可以不加，这是一种默认的方式。而取列的时候，我们必须在前面加上行的下标，如果是全取值，得加上:。 比方说，我们之前按行取值的时候： 12print(t1[1:3])print(t1[1:3,:]) 这两种方式实际上是一样的。因为默认方式列上 :的取值方式。 就如我们刚才说到的，按列取值的时候，行就必须加上:才行。 1234print(t1[,2])---SyntaxError: invalid syntax 看，报错了。我们只有在前面将取所有行加上才行，又或者加上某一行的值： 1234print(t1[:,2])---[ 2 8 14 20] 这样，我们就取到了第三列的值。 和取行一样，列也可以连续取值： 1234567print(t1[:, 2:])---[[ 2 3 4 5] [ 8 9 10 11] [14 15 16 17] [20 21 22 23]] 我们如愿从第三列开始向后连续取值。 那如果是不连续的呢？也和行是一样的形式： 1234567print(t1[:,[0,2,3]])---[[ 0 2 3] [ 6 8 9] [12 14 15] [18 20 21]] 有没有发现，当我们了解了行列关系和取值方式代表的意义之后，那这个多维数组就任我们拿捏了。其原理显得非常简单。 好，让我们多加一个难度稍微高一点的取值，我们需要从行，从列都取不连续的值。那还不简单，不就是[[行,行,行...],[列,列,列...]]的方式吗。 1234print(t1[[0,1,1],[0,1,3]])---[0 7 9] 这，其实就是一种定点取值的方式了。 修改数组的值 在了解完如何进行查询查找之后，我们现在在来学习一下如何对其中的值进行修改。在平时我们处理数据的时候，替换修改数值是常有的事情,我们每次修改之后，都会将之前的值复原。 我们从修改行的值开始，因为这个比较好理解： 123456789# 修改一行的值t1[1] = 0print(t1)---[[ 0 1 2 3 4 5] [ 0 0 0 0 0 0] [12 13 14 15 16 17] [18 19 20 21 22 23]] 就是如此简单，定位好数值后，直接用=赋值就可以了。 那同理，修改某一列的数据也是如此操作。 123456789t1 = np.arange(24).reshape(4,6)t1[:,1] = 0print(t1)---[[ 0 0 2 3 4 5] [ 6 0 8 9 10 11] [12 0 14 15 16 17] [18 0 20 21 22 23]] 修改连续多行和修改连续多列的方式也是一样，这次让我们来一次联动操作，将其中的连续多行多列一起修改掉： 123456789t1 = np.arange(24).reshape(4,6)t1[1:3,1:4] = 0print(t1)---[[ 0 1 2 3 4 5] [ 6 0 0 0 10 11] [12 0 0 0 16 17] [18 19 20 21 22 23]] 当然，既然可以连续多行多列进行修改，那我们肯定也能够进行定点修改值 123456789t1 = np.arange(24).reshape(4,6)t1[[0,1],[0,3]] = 0print(t1)---[[ 0 1 2 3 4 5] [ 6 7 8 0 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]] 那基本上，如何按照行列寻找值并修改也就这么多内容，大家多操作完全可以轻易掌握。 别着急，还没结束，按行按列并不能完全满足我们平时的要求，有的时候，我们是要对数值按条件进行修改的。打个比方说，我们需要对所有大于2小于12的值进行修改，那么这又该如何实现？总不会在数组中可以用比较运算符来判断吧？嗯，没错，就是要用比较运算符符来判断，完全可以： 12345t1 = np.arange(24).reshape(4,6)t1[2&lt;t1 and t1&lt;12] = 0---ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() 报错了，怎么回事，不是说好的可以用比较运算符判断吗？ 原因其实就是，我们不能使用and,or这样的方式，我们来改变一下逻辑运算符的方式： 123456789t1 = np.arange(24).reshape(4,6)t1[(2&lt;t1)&amp;(t1&lt;12)] = 0print(t1)---[[ 0 1 2 0 0 0] [ 0 0 0 0 0 0] [12 13 14 15 16 17] [18 19 20 21 22 23]] 成功了，从3到11我们全部替换成了0。 其实，在NumPy中，也有相应的方法名来替代&amp;, |, ~这种逻辑运算符，我们来看： 123456789t1 = np.arange(24).reshape(4,6)t1[(np.logical_and(t1&gt;2, t1&lt;12))] = 0print(t1)---[[ 0 1 2 0 0 0] [ 0 0 0 0 0 0] [12 13 14 15 16 17] [18 19 20 21 22 23]] 这就是在NumPy中特殊的逻辑运算方法。「与、或、非」对应的方法如下： np.logical_and: 与&amp; np.logical_or: 或| np.logical_not: 非~ 大家可以依次去试试，我这里另外给大家拓展一个方式,就是三目运算方式，这种方式在今后的数据处理中，我们会经常看到，其形式为： 1三目运算 np.where(condition, x, y)，满足条件(condition)输出x, 不满足输出y 我们来看个例子： 12345678score = np.array([[80,88],[82,81],[75,81]])result = np.where(score&gt;80, True, False)print(result)---[[False True] [ True True] [False True]] 在我们平时处理数据的过程当中，数据并不是如此完美，除了修改之外，更多的时候是需要我们去添加、删除以及去重。接下来，就让我们看看这部分该如何操作： 数组的添加 numpy.append函数在数组的末尾添加值。 追加操作会分配整个数组，并把原来的数组复制到新数组中。 此外，输入数组的维度必须匹配否则将生成ValueError。 在其中的参数意义如下： arr: 输入数组 values：要向arr添加的值，需要和arr形状相同（除了要添加的轴）。 axis：默认为 None。当axis无定义时，是横向加成，返回总是为一维数组！当axis有定义的时候，分别为 0和1的时候。当axis有定义的时候，分别为0和1的时候（列数要相同）。当axis为1时，数组是加在右边（行数要相同）。 1234567891011121314151617181920212223# 数组的添加a = np.array([[1,2,3], [4,5,6]])print(f'第一个数组:\\n{a}\\n')print(f'向数组添加元素:\\n{np.append(a, [7,8,9])}\\n')print(f'沿轴0添加元素:\\n{np.append(a, [[7,8,9]], axis=0)}\\n')print(f'沿轴1添加元素:\\n{np.append(a, [[5,5,5],[7,8,9]], axis=1)}\\n')---第一个数组:[[1 2 3] [4 5 6]]向数组添加元素:[1 2 3 4 5 6 7 8 9]沿轴0添加元素:[[1 2 3] [4 5 6] [7 8 9]]沿轴1添加元素:[[1 2 3 5 5 5] [4 5 6 7 8 9]] 除了append之外，我们还可以使用insert进行添加。 numpy.insert：函数在给定索引之前，沿给定轴在输入数组中插入值。如果值的类型转换为要插入，则它与输入数组不同。 插入没有原地的，函数会返回一个新数组。 此外，如果未提供轴，则输入数组会被展开。 12345678910111213141516171819202122232425262728a = np.array([[1,2],[3,4],[5,6]])print(f'第一个数组:\\n{a}\\n')print(f'未传递Axis参数,在插入之前输入数组会被展开:\\n{np.insert(a,3,[11,12])}\\n')print('\\n传递了Axis参数,会广播值数组来配输入数组')print(f'沿轴0广播:\\n{np.insert(a,1,[11], axis=0)}\\n')print(f'沿轴1广播:\\n{np.insert(a,1,11, axis=1)}\\n')---第一个数组:[[1 2] [3 4] [5 6]]未传递Axis参数,在插入之前输入数组会被展开:[ 1 2 3 11 12 4 5 6]传递了Axis参数,会广播值数组来配输入数组沿轴0广播:[[ 1 2] [11 11] [ 3 4] [ 5 6]]沿轴1广播:[[ 1 11 2] [ 3 11 4] [ 5 11 6]] 从代码中我们可以看出来，insert相比较而言在arr和values之间多了一个形参，用于指示插入数据的下标位置。 接着，我们来看看如何在数组中操作删除。 npmpy.delete: 函数返回从输入数组中删除指定子数组的新数组。 与 insert()函数的情况一样，如果未提供轴参数，则输入数组将展开。 参数说明： arr：输入数组 obj：可以被切片，整数或者整数数组，表明要从输入数组删除的子数组 axis：沿着它删除给定子数组的轴，如果未提供，则输入数组会被展开。 123456789101112131415161718a = np.arange(12).reshape(3,4)print(f'第一个数组:\\n{a}\\n')print(f'未传递Axis参数。在删除之前输入数组会被展开:\\n{np.delete(a, 5)}\\n')print(f'删除每一行中的第二列：\\n{np.delete(a,1,axis=1)}\\n')---第一个数组:[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]未传递Axis参数。在删除之前输入数组会被展开:[ 0 1 2 3 4 6 7 8 9 10 11]删除每一行中的第二列：[[ 0 2 3] [ 4 6 7] [ 8 10 11]] 接下来，估计是我们清洗数据最常用到的操作，就是「去重」。 numpy.unique: 函数用于去除数组中的重复元素。 参数说明： arr：输入数组，如果不是一维数组则会展开. return_index：如果为true，返回新列表元素在旧列表中的位置（下标），并以列表形式储. return_inverse：如果为true，返回旧列表元素在新列表中的位置（下标），并以列表形式储 . return_counts：如果为true，返回去重数组中的元素在原数组中的出现次数. 12345678910111213141516171819202122232425262728293031323334353637a = np.array([5,2,6,2,7,5,6,7,2,9])print(f'第一个数组:\\n{a}\\n')print(f'第一个数组的去重值：\\n{np.unique(a)}\\n')print('去重数组对应原数组的索引下标数组：')u,indices = np.unique(a, return_index = True)print(indices)print('\\n我们可以看到每个和原数组下标对应的数值：')print(u)print('\\n原数组对应去重数组的下标数组：')u,indices = np.unique(a, return_inverse = True)print(u)print(indices)print ('\\n返回去重元素的重复数量：')u,indices = np.unique(a,return_counts = True) print (u)print (indices)---第一个数组:[5 2 6 2 7 5 6 7 2 9]第一个数组的去重值：[2 5 6 7 9]去重数组对应原数组的索引下标数组：[1 0 2 4 9]我们可以看到每个和原数组下标对应的数值：[2 5 6 7 9]原数组对应去重数组的下标数组：[2 5 6 7 9][1 0 2 0 3 1 2 3 0 4]返回去重元素的重复数量：[2 5 6 7 9][3 2 2 2 1] 我用图形来解释一下「第一个数组的去重值」和「去重数组的索引数组」，至于其他的，也是类似的分析方式。 在去重的方法中，我们都有两个变量去接收返回值，也就是方法返回了两个值，其中u这个变量就代表的是改变之后的数组，而indices则是重新组成的数组中的数值在原数组中第一次出现的下标位置，又或者是出现次数。以方法不同，但是u肯定是去重后的重新组成的数组。 我标识了一下原数组中的下标，并且用颜色区分了一下。在重新组成的去重的数组中，可以看到已经没有重复的数值了，然后对比原数组，每一个不重复的数值也都还在。而默认的排序方式就是从小到大的排序。所以重新组成的数值中的排序和原数组中不太一样。 然后我们再来看最下面「去重下标」部分，虽然这也是一个数组，但是实际上它是一个完全由下标位置组成的数组，然后我们来看去重数组，我们是拿第一次出现的下标来算。其中2对应原数组就的下标就是1， 5第一次出现是0的位置，6是2, 7是4，9这是最后一位，也就是下标9。 所以对重新组成的数组稍微一分析，就能明白每个方法的返回值说代表的意义。 相信到这里，大家应该也都能理解了。 接下来，我们要来看看重头戏，NumPy的计算。 NumPy的计算 NumPy原本就是一个科学计算的第三方库，所以计算能力应该算是NumPy里的重点。在原始方法中，有许许多多的方法用于数据的计算。包括求最大值，求最小值，平均值，累计和等等。除了计算整体之外，还支持在不同轴上进行计算。下面我们来看看，NumPy到底为我们都提供了哪些好用的方法，最开始，还是让我们来生成一组新的数据： 1234567score = np.array([[80,88],[82,81],[75,81]])score---array([[80, 88], [82, 81], [75, 81]]) 获取所有数据最大值 12345result = np.max(score)print(result)---88 获取某一个轴上的数据最大值 12345result = np.max(score,axis=0)print(result)---[82 88] 获取最小值 12345result = np.min(score)print(result)---75 获取某一个轴上的数据最小值 12345result = np.min(score,axis=1)print(result)---[80 81 75] 数据的比较 第一个参数中的每一个数与第二个参数比较返回大的 12345result = np.maximum([-2, -1, 0, 1, 2], 0)print(result)---[0 0 0 1 2] 第一个参数中的每一个数与第二个参数比较返回小的 12345result = np.minimum([-2, -1, 0, 1, 2], 0)print(result)---[-2 -1 0 0 0] 以上两组代码中，当方法内接受两个参数，当然也可以大小一致; 当第二个参数只是一个单独的值时，其实是用到了维度的广播机制。如果第二个参数是一个同样长度的数组，会分别比较不同位置。 12345result = np.maximum([-2, -1, 0, 1, 2], [1,2,3,4,5]) print(result)---[1 2 3 4 5] 咱们在这里稍微讲一下NumPy中的广播机制，其实是有点晦涩难懂。大家尝试理解一下看看。 广播机制是Numpy让两个不同shape的数组能够做一些运算，需要对参与运算的两个数组做一些处理或者说扩展，最终是参与运算的两个数组的shape一样，然后广播计算(对应位置数据进行某运算)得到结果。 以我们做数据比较的第一组为例，当我们去对比第一个参数和第二个参数返回大的，这个时候我们给到的两个参数分别是： [-2,-1,0,1,2]和0，但是由于广播机制的存在，在NumPy中实际上是这么处理的，第一个参数还是[-2,-1,0,1,2]， 而第二个参数实际上是[0,0,0,0,0]。 获取所有数据的平均值 12345result = np.mean(score)print(result)---81.16666666666667 获取某一行或一列的平均值 12345result = np.mean(score, axis=0)print(result)---[79. 83.33333333] 返回给定axis上的累计和 123456789t1 = np.array([[1,2,3],[4,5,6]])print(t1)print(t1.cumsum(0))---[[1 2 3] [4 5 6]][[1 2 3] [5 7 9]] 在来看值为axis=1时： 12345print(t1.cumsum(1))---[[ 1 3 6] [ 4 9 15]] 我们可以这样认为，当cumsum(0)，也就是axis=0时，是这样计算的： 12[1 2 3] --------&gt; |1 |2 |3 |[5 7 9] --------&gt; |5=1+4 |7=2+5 |9=3+6| 当cumsum(1)，也就是axis=1时，是这样计算的： 12[ 1 3 6] ------&gt; |1 |3=2+1 |6=3+2+1 |[ 4 9 15] ------&gt; |4 |9=4+5 |15=4+5+6 | argmin求最小值索引 12345result = np.argmin(score, axis=0)print(result)---[2 1] 我们看score这组数据中，75是第一列最小值，81是第二列最小值。当然，第二列的数据中有两个81。现在让我们讲数组的值换一下看看： 123456score[2,1] = 64result = np.argmin(score, axis=0)print(result)---[2 2] 很显然，我们第二列的最小值变成了64， 在那一列中，它的坐标为2。 求每一列的标准差 标准差是一组数据平均值分散程度的一种度量。一个较大的标准差，代表大部分数值和其平均值之间差异较大；一个较小的标准差，代表这些数据较接近平均值反应出数据的波动稳定情况，越大表示波动越大，越不稳定。 12345result = np.std(score, axis=0)print(result)---[ 2.94392029 10.07747764] 从结果中可以看出来，我们第二列的波动较大。其原因正是因为我把[2,2]这个位置的值替换成了64。和其他值拉大了差距。 极值 12345result = np.ptp(score,axis=None) print(result)---24 计算极值，其实也就是最大值和最小值的差。在score中，最大值为88，最小值为64。 除了我们目前测试的这些方法之外，NumPy中还有很多其他方法。比如，计算反差的var， 协方差cov，平均值average， 中位数median。在这里，我们就不一一测试了，方法都比较简单，拿来直接用的那种。 我将通用函数和解释在这里列一个表： 通用函数 解释 numpy.sqrt(array) 平方根函数 numpy.exp(array) e^array[i]的数组 numpy.abs/fabs(array) 计算绝对值 numpy.square(array) 计算各元素的平方 等于 array**2 numpy.log/log10/log2(array) 计算各元素的各种对数 numpy.sign(array) 计算各元素正负号 numpy.isnan(array 计算各元素是否为NaN numpy.isinf(array) 计算各元素是否为NaN numpy.cos/cosh/sin/sinh/tan/tanh(array) 三角函数 numpy.modf(array) 将array中值得整数和小数 分离，作两个数组返回 numpy.ceil(array) 向上取整,也就是取比这个 数大的整数 numpy.ﬂoor(array) 向下取整,也就是取比这个 数小的整数 numpy.rint(array) 四舍五入 numpy.trunc(array) 向0取整 numpy.cos(array) 正弦值 numpy.sin(array) 余弦值 numpy.tan(array) 正切值 numpy.add(array1,array2) 元素级加法 numpy.subtract(array1,array2) 元素级减法 numpy.multiply(array1,array2) 元素级乘法 numpy.divide(array1,array2) 元素级除法 array1./array2 numpy.power(array1,array2) 元素级指数 array1.^array2 numpy.maximum/minimum(array1,aray2) 元素级最大值 numpy.fmax/fmin(array1,array2) 元素级最大值，忽略NaN numpy.mod(array1,array2) 元素级求模 numpy.copysign(array1,array2) 将第二个数组中值得符号复 制给第一个数组中值 numpy.greater/greater_equal/less/less_equal/equal/not_equal (array1,array2) 元素级比较运算，产生布尔 数组 numpy.logical_end/logical_or/logic_xor(array1,array2) 元素级的真值逻辑运算 数组的拼接 有的时候我们需要将两个数据加起来一起研究分析，我们就可以将其进行拼接然后分析。 我们先创建两组数组： 12a = np.array([[1,2],[3,4]])b = np.array([[5,6],[7,8]]) 然后我们现在先根据轴连接的数组序列 先沿轴0连接两个数组： 1234567print(np.concatenate((a,b), axis=0))---[[1 2] [3 4] [5 6] [7 8]] 再沿轴1连接两个数组： 12345print(np.concatenate((a,b), axis=1))---[[1 2 5 6] [3 4 7 8]] 根据轴进行堆叠 沿轴0堆叠两个数组： 12345678print(np.stack((a,b), axis=0))---[[[1 2] [3 4]] [[5 6] [7 8]]] 沿轴1堆叠两个数组： 12345678print(np.stack((a,b), axis=1))---[[[1 2] [5 6]] [[3 4] [7 8]]] 矩阵垂直拼接 12345678910v1 = [[0,1,2,3,4,5], [6,7,8,9,10,11]]v2 = [[12,13,14,15,16,17],[18,19,20,21,22,23]]result = np.vstack((v1, v2))print(result)---[[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]] 矩阵水平拼接 123456result = np.hstack((v1, v2))print(result)---[[ 0 1 2 3 4 5 12 13 14 15 16 17] [ 6 7 8 9 10 11 18 19 20 21 22 23]] 数组的分割 将一个数组分割为多个子数组 参数说明： ary：被分割的数组 indices_or_sections：果是一个整数，就用该数平均切分，如果是一个数组，为沿轴切分的位置（左开右 闭） axis：沿着哪个维度进行切向，默认为0，横向切分。为1时，纵向切分 12345678arr = np.arange(9).reshape(3,3)print('将数组分成三个大小相等的子数组：')b = np.split(arr,3)print(b)---将数组分成三个大小相等的子数组：[array([[0, 1, 2]]), array([[3, 4, 5]]), array([[6, 7, 8]])] numpy.hsplit函数用于水平分割数组，通过指定要返回的相同形状的数组数量来拆分原数组。 1234567891011121314harr = np.floor(10 * np.random.random((2,6)))print(f'原array:\\n{harr}')print(f'\\n水平分割后:\\n{np.hsplit(harr, 3)}')---原array:[[1. 1. 8. 2. 3. 9.] [3. 1. 6. 6. 5. 6.]]水平分割后:[array([[1., 1.], [3., 1.]]), array([[8., 2.], [6., 6.]]), array([[3., 9.], [5., 6.]])] 这里我们说一下floor(), 这个方法会返回数值的下舍整数（舍去小数点求整型）。 numpy.vsplit会沿垂直轴分割 123456789101112131415a = np.arange(16).reshape(4,4)print(f'第一个数组：\\n{a}')print(f'\\n垂直分割之后：\\n{np.vsplit(a,2)}')---第一个数组：[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]]垂直分割之后：[array([[0, 1, 2, 3], [4, 5, 6, 7]]), array([[ 8, 9, 10, 11], [12, 13, 14, 15]])] nan和inf C 语言中表示最大的正整数值是0x7FFFFFFF，最小的负整数是0x80000000。inf 表示无穷大，需要使用 ﬂoat(‘inf’)函数来转化，那么对应的就有 ﬂoat('-inf')表示无穷小了。这样你就可以使用任意数来判断和它的关系了。 那什么时候会出现inf呢？ 比如一个数字除以0，Python中会报错，但是numpy中会是一个inf或者-inf 另外还有 nan，这种写法在pandas中常见，表示缺失的数据，所以一般用nan来表示。任何与其做运算结果都是nan。 12345678a = np.nanb = np.infprint(a, type(a))print(b, type(b))---nan &lt;class 'float'&gt;inf &lt;class 'float'&gt; 判断数组中为nan的个数(注意：float类型的数据才能赋值nan) 1t = np.arange(24,dtype=float).reshape(4,6) 可以使用np.count_nonzero()来判断非零的个数 1234print(np.count_nonzero(t))---23 将三行四列的数改成nan 12345t[3,4] = np.nanprint(t[3,4] != np.nan)---True 注意到没有，np.nan != np.nan居然是True。难道我们更改数据失败了？我们打出来看看： 1234567print(t)---[[ 0. 1. 2. 3. 4. 5.] [ 6. 7. 8. 9. 10. 11.] [12. 13. 14. 15. 16. 17.] [18. 19. 20. 21. nan 23.]] 没错，t[3,4]确实被改变了，那只能说明np.nan != np.nan是确实存在的。 所以，我们就可以使用这两个结合使用判断nan的个数： 1234print(np.count_nonzero(t != t))---1 我们之前讲过，nan和任何数计算都为nan 1234print(np.sum(t,axis=0))---[36. 40. 44. 48. nan 56.] 接下来，让我们做一个具体的练习，在练习中，我们将处理数组中的nan 123456789101112# 练习，处理数组中的nant = np.arange(24).reshape(4,6).astype('float')# 将数组中的一部分替换nant[1, 3:] = np.nanprint(t)---[[ 0. 1. 2. 3. 4. 5.] [ 6. 7. 8. nan nan nan] [12. 13. 14. 15. 16. 17.] [18. 19. 20. 21. 22. 23.]] 现在我们得到了一组包含nan的数组，接着我们来处理这组数据： 1234567891011121314151617181920212223# 尝试便利每一列，然后判断每一列是否有`nan`for i in range(t.shape[1]): # 获取当前列数据 temp_col = t[:, i] # 判断当前列的数据中是否含有nan nan_num = np.count_nonzero(temp_col != temp_col) # 条件成立说明含有nan if nan_num != 0: # 将这一列部位nan的数据拿出来 temp_col_not_nan = temp_col[temp_col == temp_col] # 将nan替换成这一列的平均值 temp_col[np.isnan(temp_col)] = np.mean(temp_col_not_nan)print(t)---[[ 0. 1. 2. 3. 4. 5.] [ 6. 7. 8. 13. 14. 15.] [12. 13. 14. 15. 16. 17.] [18. 19. 20. 21. 22. 23.]] 这样，我们就处理了这组数据中的nan，至于替换成平均值填补空缺数据，这个是清洗数据的通用做法。 二维数组的转置 针对二维数组的转置，也就是对换数组的维度。说的直白一点，就是行转列，列转行。 这在处理数据的时候，也是我们经常要做的操作： 123456789101112131415a = np.arange(12).reshape(3,4)print (f'原数组：\\n{a}') print (f'\\n对换数组：\\n{np.transpose(a)}') ---原数组：[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]对换数组：[[ 0 4 8] [ 1 5 9] [ 2 6 10] [ 3 7 11]] 让我们再来看一种处理方式，与transpose方法一致： 123456789101112131415a = np.arange(12).reshape(3,4)print (f'原数组：\\n{a}') print (f'\\n转置数组：\\n{a.T}') ---原数组：[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]转置数组：[[ 0 4 8] [ 1 5 9] [ 2 6 10] [ 3 7 11]] 接着我们尝试一个函数用于交换数组的两个轴 1234567891011121314151617181920t1 = np.arange(24).reshape(4,6)re = t1.swapaxes(1,0)print (f'\\n原数组：\\n{t1}') print (f'\\n调用 swapaxes 函数后的数组：\\n{re}') ---原数组：[[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]]调用 swapaxes 函数后的数组：[[ 0 6 12 18] [ 1 7 13 19] [ 2 8 14 20] [ 3 9 15 21] [ 4 10 16 22] [ 5 11 17 23]] 这几种方式都完成了转置操作，平时工作中，我们可以都尝试一下。 总结 最后，我们还是对NumPy整个的做一个总结： 然后，让我们留点作业吧： 练习矩阵相乘 练习数组索引 练习数组形状修改 大家要好好的完成作业。那本节课就到这里了，下课。","link":"/AI-Python-numpy/"},{"title":"27. Pandas","text":"Hi, 大家好。我是茶桁。 先跟小伙伴们打个招呼，今天这节课呢，就是我们Python基础课程的最后一节课了。 在本节课之前，我们讲解了Python的基础，介绍了两个第三方库。而今天一样会介绍一个第三方库：Pandas。 虽然是最后一节课了，但是本节课的任务却是一点也不轻松。相比较而言，如果你以后从事的是数据治理和分析工作，那么本节课的内容可能会是你在今后工作中用到的最多的内容。我们需要学习行列索引的操作，数据的处理，数据的合并，多层索引，时间序列，数据的分组聚合（重点）。最后，我们会有一个案例的展示。 听着是不是很兴奋？那我们就开始吧。 在开始讲解pandas之前，我们讲解一些关于数据的小知识。 我们大部分人应该都用过Excel表格，而我们从数据库中拿到的数据，也基本上和Excel上的数据差不多，都是由行列组成的。可以直接导出为csv文件。 也就是说，我们大部分时候要处理的数据，基本上都一组二维数据。例如，我们今天最后案例要用到的一个电影数据(部分)，如图： 这里面，我们就将数据通过行列来展示和定位。 了解了这一点之后，我们来开始学习pandas。 Pandas简介 在之前的介绍中，我们发现很多的操作似乎都似曾相识，在NumPy中我们好像都接触过。 有这种感觉很正常，Pandas本身就是基于NumPy的一种工具，该⼯具是为了解决数据分析任务⽽创建的。Pandas 纳⼊了⼤量库和⼀些标准的数据模型，提供了⾼效地操作⼤型数据集所需的⼯具。pandas提供了⼤量能使我们快速 便捷地处理数据的函数和⽅法。 Series对象 Pandas基于两种数据类型：series与dataframe。 Series是Pandas中最基本的对象，Series类似⼀种⼀维数组。事实上，Series基本上就是基于NumPy的数组对象来的。和 NumPy的数组不同，Series能为数据⾃定义标签，也就是索引（index），然后 通过索引来访问数组中的数据。 Dataframe是⼀个⼆维的表结构。Pandas的dataframe可以存储许多种不同的数据类型，并且每⼀个坐标轴都有⾃⼰的标签。你可以把它想象成⼀个series的字典项。 在开始基于代码进行学习之前，我们需要应用一些必要项。 1234import numpy as npimport pandas as pdfrom pandas import Seriesfrom pandas import DataFrame as df 现在我们来看看Series的一些基本操作： 创建Series对象并省略索引 index 参数是可省略的，你可以选择不输⼊这个参数。如果不带 index 参数，Pandas 会⾃动⽤默认 index 进⾏索引，类似数组，索引值是 [0, ..., len(data) - 1] 123456789sel = Series([1,2,3,4])print(sel)---0 11 22 33 4dtype: int64 我们之前在NumPy中学习了dtype, 以及它的相关数据类型。所以现在的dtype: int64我们应该能理解是什么意思了。 我们看打印的结果，在1,2,3,4前面，是Series默认生成的索引值。 通常我们会⾃⼰创建索引 123456789sel = Series(data=[1,2,3,4], index= list('abcd'))print(sel)---a 1b 2c 3d 4dtype: int64 这个时候，我们可以对这个Series对象操作分别获取内容和索引： 123456print(f'values: {sel.values}')print(sel.index)---values: [1 2 3 4]Index(['a', 'b', 'c', 'd'], dtype='object') 又或者，我们可以直接获取健值对（索引和值对）。 1234print(list(sel.iteritems()))---[('a', 1), ('b', 2), ('c', 3), ('d', 4)] 那么这种健值对的形式让你想到了什么？是字典对吧？ 我们完全可以将字典转为Series: 12345678910dict={&quot;red&quot;:100,&quot;black&quot;:400,&quot;green&quot;:300,&quot;pink&quot;:900}se3=Series(dict) print(se3)---red 100black 400green 300pink 900dtype: int64 Series数据获取 在Series拿到数据转为Series对象之后，诺大的数据中，我们如何定位并获取到我们想要的内容呢？ Series在获取数据上，支持位置、标签、获取不连续数据，使用切片等方式。我们一个个的看一下： Series对象同时⽀持位置和标签两种⽅式获取数据 123456print('索引下标',sel['c'])print('位置下标',sel[2])---索引下标 3位置下标 3 获取不连续的数据 1234567891011121314print('位置切⽚\\n',sel[1:3])# 左包含右不包含print('\\n索引切⽚\\n',sel['b':'d'])# 左右都包含---位置切⽚b 2c 3dtype: int64索引切⽚b 2c 3d 4dtype: int64 我们看到结果，发现两组数据数量不对，但是其实我们获取的位置都是一样的。这是因为，位置切片的方式会是「左包含右不包含」的，而索引切片则是「左右都包含」。 重新赋值索引的值 除了获取数据之外，我们还可以对数据进行重新索引。其实在之前我们将索引的时候，第二种自己赋值的方式实际上就是一个重新赋值了，将自己定义的值替换了默认值。这里让我们再来看一下： 123456789sel.index = list('dcba')print(sel)---d 1c 2b 3a 4dtype: int64 还有一种重新索引的方法reindex， 这会返回一个新的Series。调用reindex将会重新排序，缺失值这会用NaN填补。 123456789print(sel.reindex(['b','a','c','d','e']))---b 3.0a 4.0c 2.0d 1.0e NaNdtype: float64 在重新索引的时候，我们特意多增加了一个索引。在最后一位没有数据的情况下，缺失值被NaN填补上了。 丢弃指定轴上的项 123456789101112131415sel = pd.Series(range(10, 15))print(sel)print(sel.drop([2,3]))---0 101 112 123 134 14dtype: int640 101 114 14dtype: int64 使用drop，会丢弃掉轴上的项，例子中，我们将2，3进行了丢弃。 Series进⾏算术运算操作 对 Series 的算术运算都是基于 index 进⾏的。我们可以⽤加减乘除（+ - * /）这样的运算符对两个 Series 进⾏运算，Pandas 将会根据索引 index，对响应的数据进⾏计算，结果将会以浮点数的形式存储，以避免丢失精度。如果 Pandas 在两个 Series ⾥找不到相同的 index，对应的位置就返回⼀个空值 NaN。 1234567891011121314151617181920212223242526272829303132333435363738# Series 算数运算series1 = pd.Series([1,2,3,4],['London','HongKong','Humbai','lagos'])series2 = pd.Series([1,3,6,4],['London','Accra','lagos','Delhi'])print('series1-series2:')print(series1-series2) print('\\nseries1+series2:')print(series1+series2) print('\\nseries1*series2:')print(series1*series2)---series1-series2:Accra NaNDelhi NaNHongKong NaNHumbai NaNLondon 0.0lagos -2.0dtype: float64series1+series2:Accra NaNDelhi NaNHongKong NaNHumbai NaNLondon 2.0lagos 10.0dtype: float64series1*series2:Accra NaNDelhi NaNHongKong NaNHumbai NaNLondon 1.0lagos 24.0dtype: float64 除此之外，Series的算术运算操作同样也支持NumPy的数组运算 12345678910111213141516171819sel = Series(data = [1,6,3,5], index = list('abcd'))print(sel[sel&gt;3]) # 布尔数组过滤print(sel*2) # 标量乘法print(np.square(sel)) # 可以直接加⼊到numpy的数学函数---b 6d 5dtype: int64a 2b 12c 6d 10dtype: int64a 1b 36c 9d 25dtype: int64 DataFrame DataFrame（数据表）是⼀种2维数据结构，数据以表格的形式存储，分成若⼲⾏和列。通过 DataFrame，你能很⽅便地处理数据。常见的操作⽐如选取、替换⾏或列的数据，还能重组数据表、修改索引、多重筛选等。我们基本上可以把 DataFrame 理解成⼀组采⽤同样索引的 Series 的集合。调⽤ DataFrame()可以将多种格式的数据转换为DataFrame对象，它的的三个参数data、index和columns 分别为数据、⾏索引和列索引。 DataFrame的创建 我们可以使用二维数组 123456789df1 = df(np.random.randint(0,10,(4,4)),index=[1,2,3,4],columns=['a','b','c','d']) print(df1)--- a b c d1 9 6 3 02 6 2 7 03 3 1 6 54 6 6 7 1 也可以使用字典创建 行索引由index决定，列索引由字典的键决定 12345678910dict={'Province': ['Guangdong', 'Beijing', 'Qinghai','Fujian'],'pop': [1.3, 2.5, 1.1, 0.7], 'year': [2022, 2022, 2022, 2022]}df2= df(dict,index=[1,2,3,4]) print(df2)--- Province pop year1 Guangdong 1.3 20222 Beijing 2.5 20223 Qinghai 1.1 20224 Fujian 0.7 2022 使用from_dict 123456789dict2={&quot;a&quot;:[1,2,3],&quot;b&quot;:[4,5,6]}df6=df.from_dict(dict2) print(df6)--- a b0 1 41 2 52 3 6 索引相同的情况下，相同索引的值会相对应，缺少的值会添加NaN 1234567891011121314data = { 'Name':pd.Series(['zs','ls','we'],index=['a','b','c']), 'Age':pd.Series(['10','20','30','40'],index=['a','b','c','d']), 'country':pd.Series(['中国','⽇本','韩国'],index=['a','c','b'])}df = pd.DataFrame(data)print(df)--- Name Age countrya zs 10 中国b ls 20 韩国c we 30 ⽇本d NaN 40 NaN 看了那么多DataFrame的转换方式，那我们如何将数据转为字典呢？DataFrame有一个内置方法to_dict()能将DataFrame对象转换为字典： 12345dict = df.to_dict()print(dict)---{'Name': {'a': 'zs', 'b': 'ls', 'c': 'we', 'd': nan}, 'Age': {'a': '10', 'b': '20', 'c': '30', 'd': '40'}, 'country': {'a': '中国', 'b': '韩国', 'c': '⽇本', 'd': nan}} DataFrame对象常⽤属性 让我们先来生成一组数据备用： 123456789101112df_dict = {'name':['James','Curry','Iversion'],'age':['18','20','19'], 'national':['us','China','us'] }df = pd.DataFrame(data=df_dict,index=['0','1','2'])print(df)--- name age national0 James 18 us1 Curry 20 China2 Iversion 19 us 获取⾏数和列数 1234print(df.shape)---(3,3) 获取⾏索引 1234print(df.index.tolist())---['0', '1', '2'] 获取列索引 1234print(df.columns.tolist())---['name', 'age', 'national'] 获取数据的类型 1234567print(df.dtypes)---name objectage objectnational objectdtype: object 获取数据的维度 1234print(df.ndim)---2 values属性也会以⼆维ndarray的形式返回DataFrame的数据 123456print(df.values)---[['James' '18' 'us'] ['Curry' '20' 'China'] ['Iversion' '19' 'us']] 展示df的概览 1234567891011121314print(df.info())---&lt;class 'pandas.core.frame.DataFrame'&gt;Index: 3 entries, 0 to 2Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 name 3 non-null object 1 age 3 non-null object 2 national 3 non-null objectdtypes: object(3)memory usage: 96.0+ bytesNone 显示头⼏⾏,默认显示5⾏ 123456print(df.head(2))--- name age national0 James 18 us1 Curry 20 China 显示后⼏⾏ 12345print(df.tail(1))--- name age national2 Iversion 19 us 获取DataFrame的列 1234567print(df['name'])---0 James1 Curry2 IversionName: name, dtype: object 因为我们只获取⼀列，所以返回的就是⼀个Series 1234print(type(df['name']))---&lt;class 'pandas.core.series.Series'&gt; 如果获取多个列，那返回的就是⼀个DataFrame 类型： 123456789print(df[['name','age']])print(type(df[['name','age']]))--- name age0 James 181 Curry 202 Iversion 19&lt;class 'pandas.core.frame.DataFrame'&gt; 获取一行 12345print(df[0:1])--- name age national0 James 18 us 去多⾏ 123456print(df[1:3])--- name age national1 Curry 20 China2 Iversion 19 us 取多⾏⾥⾯的某⼀列（不能进⾏多⾏多列的选择） 123456print(df[1:3][['name','age']])--- name age1 Curry 202 Iversion 19 ⚠️ 注意：df[]只能进⾏⾏选择，或列选择，不能同时多⾏多列选择。比如在NumPy中的data[:,1:3]这种是不行的。当然，并不是没有办法获取，我们接着往下看: df.loc通过标签索引⾏数据;df.iloc通过位置获取⾏数据 获取某⼀⾏某⼀列的数据 1234print(df.loc['0','name'])---James ⼀⾏所有列 1234567print(df.loc['0',:])---name Jamesage 18national usName: 0, dtype: object 某⼀⾏多列的数据 123456print(df.loc['0',['name','age']])---name Jamesage 18Name: 0, dtype: object 选择间隔的多⾏多列 123456print(df.loc[['0','2'],['name','national']]) --- name national0 James us2 Iversion us 选择连续的多⾏和间隔的多列 1234567print(df.loc['0':'2',['name','national']])--- name national0 James us1 Curry China2 Iversion us 取⼀⾏ 1234567print(df.iloc[1])---name Curryage 20national ChinaName: 1, dtype: object 取连续多⾏ 123456print(df.iloc[0:2])--- name age national0 James 18 us1 Curry 20 China 取间断的多⾏ 123456print(df.iloc[[0,2],:])--- name age national0 James 18 us2 Iversion 19 us 取某⼀列 1234567print(df.iloc[:,1])---0 181 202 19Name: age, dtype: object 某⼀个值 1234print(df.iloc[1,0])---Curry 修改值 12345678df.iloc[0,0]='panda'print(df)--- name age national0 panda 18 us1 Curry 20 China2 Iversion 19 us dataframe中的排序⽅法 12345678df = df.sort_values(by='age', ascending*=False) print(df)--- name age national1 Curry 20 China2 Iversion 19 us0 panda 18 us ascending=False是降序排列，默认为True，也就是升序。 dataframe修改index、columns 一样，让我们先创建一组新的数据供我们使用： 12345678df1 = pd.DataFrame(np.arange(9).reshape(3, 3), index = ['bj', 'sh', 'gz'], columns=['a', 'b', 'c'])print(df1)--- a b cbj 0 1 2sh 3 4 5gz 6 7 8 修改 df1 的 index df1.index可以打印出df1的索引值，同时也可以为其赋值。 12345678910print(df1.index) # 可以打印出print的值，同时也可以为其赋值df1.index = ['beijing', 'shanghai', 'guangzhou']print(df1)---Index(['bj', 'sh', 'gz'], dtype='object') a b cbeijing 0 1 2shanghai 3 4 5guangzhou 6 7 8 ⾃定义map函数（x是原有的⾏列值） 123456789101112# ⾃定义map函数（x是原有的⾏列值） def test_map(x): return x+'_ABC'print(df1.rename(index=test_map, columns=test_map, inplace=True))print(df1)---None a_ABC b_ABC c_ABCbeijing_ABC 0 1 2shanghai_ABC 3 4 5guangzhou_ABC 6 7 8 其中的inplace传入一个布尔值，默认为False。指定是否返回新的DataFrame。如果为True，则在原df上修改， 返回值为None。 rename 可以传⼊字典，为某个 index 单独修改名称 12345678df3 = df1.rename(index={'beijing_ABC':'beijing'}, columns = {'a_ABC':'aa'})print(df3)--- aa b_ABC c_ABCbeijing 0 1 2shanghai_ABC 3 4 5guangzhou_ABC 6 7 8 列转化为索引 12345678910df1=pd.DataFrame({'X':range(5),'Y':range(5),'S':list(&quot;abcde&quot;),'Z': [1,1,2,2,2]})print(df1)--- X Y S Z0 0 0 a 11 1 1 b 12 2 2 c 23 3 3 d 24 4 4 e 2 指定⼀列为索引 (drop=False 指定同时保留作为索引的列) 1234567891011result = df1.set_index('S',drop=False)result.index.name=Noneprint(result)--- X Y S Za 0 0 a 1b 1 1 b 1c 2 2 c 2d 3 3 d 2e 4 4 e 2 ⾏转为列索引 1234567891011result = df1.set_axis(df1.iloc[0],axis=1,inplace=False)result.columns.name=Noneprint(result)--- 0 0 a 10 0 0 a 11 1 1 b 12 2 2 c 23 3 3 d 24 4 4 e 2 添加数据 先增加一组数据： 123456789df1 = pd.DataFrame([['Snow','M',22],['Tyrion','M',32],['Sansa','F',18], ['Arya','F',14]],columns=['name','gender','age'])print(df1)--- name gender age0 Snow M 221 Tyrion M 322 Sansa F 183 Arya F 14 在数据框最后加上score⼀列，注意增加列的元素个数要跟原数据列的个数⼀样。 123456789df1['score']=[80,98,67,90] print(df1)--- name gender age score0 Snow M 22 801 Tyrion M 32 982 Sansa F 18 673 Arya F 14 90 在具体某个位置插⼊⼀列可以⽤insert的⽅法, 语法格式：列表.insert(index, obj) index ---&gt;对象 obj 需要插⼊的索引位置。 obj---&gt; 要插⼊列表中的对象（列名） 将数据框的列名全部提取出来存放在列表⾥ 12345col_name=df1.columns.tolist() print(col_name)---['name', 'gender', 'age', 'score'] 在列索引为2的位置插⼊⼀列,列名为:city 12345col_name.insert(2,'city')print(col_name)---['name', 'gender', 'city', 'age', 'score'] 刚插⼊时不会有值，整列都是NaN,我们使用DataFrame.reindex()对原⾏/列索引重新构建索引值 123456789df1=df1.reindex(columns=col_name)print(df1)--- name gender city age score0 Snow M NaN 22 801 Tyrion M NaN 32 982 Sansa F NaN 18 673 Arya F NaN 14 90 给city列赋值 123456789df1['city']=['北京京','⼭⻄西','湖北北','澳⻔门'] print(df1)--- name gender city age score0 Snow M 北京京 22 801 Tyrion M ⼭⻄西 32 982 Sansa F 湖北北 18 673 Arya F 澳⻔门 14 90 df中的insert是插⼊⼀列。语法和关键参数为： df.insert(iloc,column,value) iloc:要插⼊的位置 colunm:列名 value:值 刚才我们插入city列的时候省略了value,所以新建列值全部为NaN，这次我们加上再看： 123456789df1.insert(2,'score2',[80,98,67,90]) print(df1)--- name gender score2 city age score0 Snow M 80 北京京 22 801 Tyrion M 98 ⼭⻄西 32 982 Sansa F 67 湖北北 18 673 Arya F 90 澳⻔门 14 90 插⼊⼀⾏ 1234567891011# 插⼊⼀⾏row=['111','222','333','444','555','666'] df1.iloc[1]=rowprint(df1)--- name gender score2 city age score0 Snow M 80 北京京 22 801 111 222 333 444 555 6662 Sansa F 67 湖北北 18 673 Arya F 90 澳⻔门 14 90 插入行的时候，列个数必须对应才可以，否则会报错。 目前这组数据已经被我们玩乱了，我们再重新生成一组数据来看后面的： 123456789df1 = pd.DataFrame([['Snow','M',22],['Tyrion','M',32],['Sansa','F',18],['Arya','F',14]],columns=['name','gender','age'])print(df1)--- name gender age0 Snow M 221 Tyrion M 322 Sansa F 183 Arya F 14 再继续创建一组数据，我们将尝试将两组数据进行合并, 新创建的这组数据⽤来增加进数据框的最后⼀⾏。 123456new=pd.DataFrame({'name':'lisa','gender':'F','age':19},index=[0])print(new)--- name gender age0 lisa F 19 在原数据框df1最后⼀⾏新增⼀⾏，⽤append⽅法 12345678910df1=df1.append(new,ignore_index=True) print(df1)--- name gender age0 Snow M 221 Tyrion M 322 Sansa F 183 Arya F 144 lisa F 19 ignore_index=False,表示不按原来的索引， 从0开始⾃动递增。 objs:合并对象 axis:合并⽅式，默认0表示按列合并，1表示按⾏合并 ignore_index:是否忽略索引 12345678910111213df1 = pd.DataFrame(np.arange(6).reshape(3,2),columns=['four','five'])df2 = pd.DataFrame(np.arange(6).reshape(2,3),columns=['one','two','three'])print(df1)print(df2)--- four five0 0 11 2 32 4 5 one two three0 0 1 21 3 4 5 按行合并 12345678result = pd.concat([df1,df2],axis=1)print(result)--- four five one two three0 0 1 0.0 1.0 2.01 2 3 3.0 4.0 5.02 4 5 NaN NaN NaN 按列合并 123456 four five one two three0 0.0 1.0 NaN NaN NaN1 2.0 3.0 NaN NaN NaN2 4.0 5.0 NaN NaN NaN3 NaN NaN 0.0 1.0 2.04 NaN NaN 3.0 4.0 5.0 看结果我们能看出来，在合并的时候，如果对应不到值，那么就会默认添加NaN值。 DataFrame的删除 12345678910111213141516df2 = pd.DataFrame(np.arange(9).reshape(3,3),columns=['one','two','three'])print(df2)df3=df2.drop(['one'],axis=1, inplace=True)print(df2)print(df3)--- one two three0 0 1 21 3 4 52 6 7 8 two three0 1 21 4 52 7 8None lables：要删除数据的标签 axis：0表示删除⾏，1表示删除列，默认0 inplace:是否在当前df中执⾏此操作 最后的返回值为None，原因是我们设置inplace为True，在当前df中执行操作。如果我们将其设置为False，则会将操作后的值进行返回，生成一个新的对象。 1234567891011df3=df2.drop([0,1],axis=0, inplace=False)print(df2)print(df3)--- one two three0 0 1 21 3 4 52 6 7 8 one two three2 6 7 8 数据处理 在我们查看完DataFrame的基础操作之后，我们现在来正式开始数据处理。 我们可以通过通过dropna()滤除缺失数据，先让我们创建一组数据： 12345678910se=pd.Series([4,NaN,8,NaN,5]) print(se)---0 4.01 NaN2 8.03 NaN4 5.0dtype: float64 尝试清除缺失数据，也就是NaN值： 1234567print(se.dropna())---0 4.02 8.04 5.0dtype: float64 在清除数据之前，我们有两个方法可以判断当前数据中是否有缺失数据，不过这两个方法的判断方式是相反的，一个是判断不是缺失数据，一个判断是缺失数据： 12345678910111213141516print(se.notnull())print(se.isnull())---0 True1 False2 True3 False4 Truedtype: bool0 False1 True2 False3 True4 Falsedtype: bool 那既然有方法可以进行判断当前数据是否为缺失数据，那么我们用之前的方法与其配合，一样可以做滤除操作： 1234567print(se[se.notnull()])---0 4.02 8.04 5.0dtype: float64 当然，除了Series对象之外，我们还需要进行处理DataFrame对象 123456789df1=pd.DataFrame([[1,2,3],[NaN,NaN,2],[NaN,NaN,NaN],[8,8,NaN]]) print(df1)--- 0 1 20 1.0 2.0 3.01 NaN NaN 2.02 NaN NaN NaN3 8.0 8.0 NaN 默认滤除所有包含NaN： 12345print(df1.dropna())--- 0 1 20 1.0 2.0 3.0 传⼊how=‘all’滤除全为NaN的⾏： 1234567print(df1.dropna(how='all')) --- 0 1 20 1.0 2.0 3.01 NaN NaN 2.03 8.0 8.0 NaN 可以看到，除了下标为2的那一行之外，其余含NaN值的行都被保留了。 之前操作最后只留下一行，原因是how的默认值为how='any'。只要是nan就删除 传⼊axis=1滤除列： 12345678print(df1.dropna(axis=1,how=&quot;all&quot;))--- 0 1 20 1.0 2.0 3.01 NaN NaN 2.02 NaN NaN NaN3 8.0 8.0 NaN 为什么没有变化？按列来查看，没有一列是全是NaN值的。 除了how值外，我们还可以可以使用thresh来精准操作，它可以传入一个数值n，会保留至少n个非NaN数据的行或列： 123456print(df1.dropna(thresh=2))--- 0 1 20 1.0 2.0 3.03 8.0 8.0 NaN 仅有一个非NaN的行和全部为NaN的行就都被滤除了。 那NaN是不是就只能被删除了呢？并不是，还记得我们之前操作的时候我提到过，我们大多数遇到NaN值的时候，基本都是用平均值来进行填充，这是一个惯例操作。 那么，我们来看看如何填充缺失数据 123456789df1=pd.DataFrame([[1,2,3],[NaN,NaN,2],[NaN,NaN,NaN],[8,8,NaN]]) print(df1)--- 0 1 20 1.0 2.0 3.01 NaN NaN 2.02 NaN NaN NaN3 8.0 8.0 NaN ⽤常数填充fillna 12345678print(df1.fillna(0))--- 0 1 20 1.0 2.0 3.01 0.0 0.0 2.02 0.0 0.0 0.03 8.0 8.0 0.0 传⼊inplace=True直接修改原对象： 123456789df1.fillna(0,inplace=True) print(df1)--- 0 1 20 1.0 2.0 3.01 0.0 0.0 2.02 0.0 0.0 0.03 8.0 8.0 0.0 通过字典填充不同的常数 12345678print(df1.fillna({0:10,1:20,2:30}))--- 0 1 20 1.0 2.0 3.01 10.0 20.0 2.02 10.0 20.0 30.03 8.0 8.0 30.0 还有我们之前提到过的，填充平均值： 12345678print(df1.fillna(df1.mean()))--- 0 1 20 1.0 2.0 3.01 4.5 5.0 2.02 4.5 5.0 2.53 8.0 8.0 2.5 当然，我们可以只填充某一列 12345678910print(df1.iloc[:,1].fillna(5,inplace = True)) print(df1)---None 0 1 20 1.0 2.0 3.01 NaN 5.0 2.02 NaN 5.0 NaN3 8.0 8.0 NaN 传⼊method=” “会改变插值⽅式，先来一组数据，并在其中加上NaN值 123456789101112df2=pd.DataFrame(np.random.randint(0,10,(5,5))) df2.iloc[1:4,3]=NaNdf2.iloc[2:4,4]=NaNprint(df2)--- 0 1 2 3 40 3 5 9 9.0 3.01 0 1 2 NaN 8.02 6 5 8 NaN NaN3 5 6 5 NaN NaN4 5 3 5 8.0 2.0 现在，我们用前面的值来填充， method ='ffill' 123456789print(df2.fillna(method='ffill'))--- 0 1 2 3 40 3 5 9 9.0 3.01 0 1 2 9.0 8.02 6 5 8 9.0 8.03 5 6 5 9.0 8.04 5 3 5 8.0 2.0 用后面的值来填充method='bfill': 123456789print(df2.fillna(method='bfill',limit=1))--- 0 1 2 3 40 7 1 8 0.0 0.01 6 8 4 NaN 5.02 6 2 5 NaN NaN3 4 8 0 1.0 3.04 8 0 2 1.0 3.0 以上代码中，我们还传入了limit， 用于限制填充行数。 当我们传入axis的时候，会修改填充方向： 123456789print(df2.fillna(method=&quot;ffill&quot;,limit=1,axis=1))--- 0 1 2 3 40 0.0 8.0 9.0 4.0 7.01 2.0 8.0 0.0 0.0 9.02 1.0 8.0 5.0 5.0 NaN3 0.0 0.0 3.0 3.0 NaN4 2.0 9.0 4.0 6.0 3.0 接着，我们再来看看如何移除重复数据，俗称「去重」： DataFrame中经常会出现重复⾏，利⽤duplicated()函数返回每⼀⾏判断是否重复的结果（重复则为 True) 12345678910111213141516171819202122df1=pd.DataFrame({'A':[1,1,1,2,2,3,1],'B':list(&quot;aabbbca&quot;)})print(df1)print(df1.duplicated())--- A B0 1 a1 1 a2 1 b3 2 b4 2 b5 3 c6 1 a0 False1 True2 False3 False4 True5 False6 Truedtype: bool 去除全部的重复⾏ 12345678print(df1.drop_duplicates())--- A B0 1 a2 1 b3 2 b5 3 c 指定列去除重复行 1234567print(df1.drop_duplicates(['A']))--- A B0 1 a3 2 b5 3 c 保留重复⾏中的最后⼀⾏ 12345678df1=pd.DataFrame({'A':[1,1,1,2,2,3,1],'B':list(&quot;aabbbca&quot;)})print(df1.drop_duplicates(['A'],keep='last'))--- A B4 2 b5 3 c6 1 a 去除重复的同时改变DataFrame对象 123456789df1.drop_duplicates(['A','B'],inplace=True) print(df1)--- A B0 1 a2 1 b3 2 b5 3 c 数据合并 我们平时会与拿到的数据可能存储在不同的数据表中，这需要我们对数据进行合并，然后再进行操作。 使⽤join合并，着重关注的是⾏的合并。 简单合并（默认是left左连接,以左侧df3为基础） 123456789101112131415161718192021df3=pd.DataFrame({'Red':[1,3,5],'Green':[5,0,3]},index=list('abc'))df4=pd.DataFrame({'Blue':[1,9,8],'Yellow':[6,6,7]},index=list('cde')) print(df3)print(df4)df3.join(df4,how='left')--- Red Greena 1 5b 3 0c 5 3 Blue Yellowc 1 6d 9 6e 8 7 Red Green Blue Yellowa 1 5 NaN NaNb 3 0 NaN NaNc 5 3 1.0 6.0 右链接 123456789df3.join(df4,how='outer')--- Red Green Blue Yellowa 1.0 5.0 NaN NaNb 3.0 0.0 NaN NaNc 5.0 3.0 1.0 6.0d NaN NaN 9.0 6.0e NaN NaN 8.0 7.0 合并多个DataFrame对象 123456789df5=pd.DataFrame({'Brown':[3,4,5],'White':[1,1,2]},index=list('aed')) df3.join([df4,df5])--- Red Green Blue Yellow Brown Whitea 1.0 5.0 NaN NaN 3.0 1.0b 3.0 0.0 NaN NaN NaN NaNc 5.0 3.0 1.0 6.0 NaN NaN 使⽤merge，着重关注的是列的合并。 我们先来构建两组数据： 12345678910111213df1=pd.DataFrame({'名字':list('ABCDE'),'性别':['男','⼥','男','男','⼥'],'职称': ['副教授','讲师','助教','教授','助教']},index=range(1001,1006))df1.columns.name='学院⽼师'df1.index.name='编号'df1---学院⽼师 名字 性别 职称编号 1001 A 男 副教授1002 B ⼥ 讲师1003 C 男 助教1004 D 男 教授1005 E ⼥ 助教 12345678910111213df2=pd.DataFrame({'名字':list('ABDAX'),'课程':['C++','计算机导论','汇编','数据结构','马克思原理'],'职称':['副教授','讲师','教授','副教授','讲师']},index= [1001,1002,1004,1001,3001])df2.columns.name='课程'df2.index.name='编号'print(df2)---课程 名字 课程 职称编号 1001 A C++ 副教授1002 B 计算机导论 讲师1004 D 汇编 教授1001 A 数据结构 副教授3001 X 马克思原理 讲师 默认下是根据左右对象中出现同名的列作为连接的键，且连接⽅式是how=’inner’ 12345678print(pd.merge(df1,df2)) # 返回匹配的--- 名字 性别 职称 课程0 A 男 副教授 C++1 A 男 副教授 数据结构2 B ⼥ 讲师 计算机导论3 D 男 教授 汇编 指定列名合并 123456789pd.merge(df1,df2,on='名字',suffixes=['_1','_2']) # 返回匹配的--- 名字 性别 职称_1 课程 职称_20 A 男 副教授 C++ 副教授1 A 男 副教授 数据结构 副教授2 B ⼥ 讲师 计算机导论 讲师3 D 男 教授 汇编 教授 连接⽅式，根据左侧为准 12345678910pd.merge(df1,df2,how='left')--- 名字 性别 职称 课程0 A 男 副教授 C++1 A 男 副教授 数据结构2 B ⼥ 讲师 计算机导论3 C 男 助教 NaN4 D 男 教授 汇编5 E ⼥ 助教 NaN 根据右侧为准 123456789pd.merge(df1,df2,how='right')--- 名字 性别 职称 课程0 A 男 副教授 C++1 B ⼥ 讲师 计算机导论2 D 男 教授 汇编3 A 男 副教授 数据结构4 X NaN 讲师 马克思原理 所有的数据 123456789101112pd.merge(df1,df2,how='outer')---名字 性别 职称 课程0 A 男 副教授 C++1 A 男 副教授 数据结构2 B ⼥ 讲师 计算机导论3 C 男 助教 NaN4 D 男 教授 汇编5 E ⼥ 助教 NaN6 X NaN 讲师 马克思原理 根据多个键进⾏连接 12345678pd.merge(df1,df2,*on*=['职称','名字'])--- 名字 性别 职称 课程0 A 男 副教授 C++1 A 男 副教授 数据结构2 B ⼥ 讲师 计算机导论3 D 男 教授 汇编 除此之外，我们还有一种轴向连接的方式：Concat Series对象的连接 1234567891011121314151617181920s1=pd.Series([1,2],index=list('ab'))s2=pd.Series([3,4,5],index=list('bde')) print(s1)print(s2)pd.concat([s1,s2])---a 1b 2dtype: int64b 3d 4e 5dtype: int64a 1b 2b 3d 4e 5dtype: int64 横向连接 12345678pd.concat([s1,s2],*axis*=1)--- 0 1a 1.0 NaNb 2.0 3.0d NaN 4.0e NaN 5.0 ⽤内连接求交集(连接⽅式，共有’inner’,’left’,right’,’outer’) 12345pd.concat([s1,s2],axis=1,join='inner')--- 0 1b 2 3 创建层次化索引 123456789pd.concat([s1,s2],keys=['A','B'])---A a 1 b 2B b 3 d 4 e 5dtype: int64 当纵向连接时keys为列名 12345678pd.concat([s1,s2],keys=['A','D'],axis=1)---- A Da 1.0 NaNb 2.0 3.0d NaN 4.0e NaN 5.0 DataFrame对象的连接 123456789101112df3=pd.DataFrame({'Red':[1,3,5],'Green':[5,0,3]},index=list('abd')) df4=pd.DataFrame({'Blue':[1,9],'Yellow':[6,6]},index=list('ce'))pd.concat([df3,df4])--- Red Green Blue Yellowa 1.0 5.0 NaN NaNb 3.0 0.0 NaN NaNd 5.0 3.0 NaN NaNc NaN NaN 1.0 6.0e NaN NaN 9.0 6.0 ⽤字典的⽅式连接同样可以创建层次化列索引 12345678910pd.concat({'A':df3,'B':df4},axis=1)--- A B Red Green Blue Yellowa 1.0 5.0 NaN NaNb 3.0 0.0 NaN NaNd 5.0 3.0 NaN NaNc NaN NaN 1.0 6.0e NaN NaN 9.0 6.0 多层索引（拓展） 创建多层索引 1234567891011s = Series(np.random.randint(0,150,size=6),index=list('abcdef'))print(s)---a 40b 122c 95d 40e 35f 27dtype: int64 123456789101112s = Series(np.random.randint(0,150,size=6),index=[['a','a','b','b','c','c'],['期中','期末','期中','期末','期中','期末']]) print(s)---a 期中 132 期末 145b 期中 33 期末 149c 期中 10 期末 145dtype: int64 DataFrame也可以创建多层索引 1234567891011121314# DataFrame创建多层索引df1 = df(np.random.randint(0,150,size=(6,4)), columns = ['zs','ls','ww','zl'], index =[['python','python','math','math','En','En'],['期中','期末','期中','期末','期中','期末']])print(df1)--- zs ls ww zlpython 期中 123 3 98 95 期末 9 36 15 126math 期中 86 86 73 115 期末 3 130 52 89En 期中 75 21 84 98 期末 56 46 111 147 特定结构 1234567891011121314class1=['python','python','math','math','En','En']class2=['期中','期末','期中','期末','期中','期末']m_index2=pd.MultiIndex.from_arrays([class1,class2])df2=df(np.random.randint(0,150,(6,4)),index=m_index2) print(df2)--- 0 1 2 3python 期中 94 36 6 19 期末 24 41 108 120math 期中 79 69 144 32 期末 138 100 42 38En 期中 110 90 123 75 期末 69 59 72 109 1234567891011121314class1=['期中','期中','期中','期末','期末','期末']class2=['python','math','En','python','math','En']m_index2=pd.MultiIndex.from_arrays([class1,class2])df2=df(np.random.randint(0,150,(6,4)),index=m_index2) print(df2)--- 0 1 2 3期中 python 96 15 135 5 math 66 78 143 93 En 70 27 120 63期末 python 147 77 92 97 math 121 81 137 102 En 18 12 134 113 product构造 1234567891011121314class1=['python','math','En']class2=['期中','期末']m_index2=pd.MultiIndex.from_product([class1,class2])df2=df(np.random.randint(0,150,(6,4)),index=m_index2) print(df2)--- 0 1 2 3python 期中 12 72 115 59 期末 36 51 94 111math 期中 44 14 9 61 期末 115 121 65 93En 期中 29 23 16 70 期末 30 73 77 53 多层索引对象的索引 让我们先来看看Series的操作 123456789101112s = Series(np.random.randint(0,150,size=6),index=[['a','a','b','b','c','c'],['期中','期末','期中','期末','期中','期末']])print(s)---a 期中 31 期末 4b 期中 101 期末 95c 期中 54 期末 126dtype: int64 取⼀个第⼀级索引 123456print(s['a'])---期中 31期末 4dtype: int64 取多个第⼀级索引 12345678print(s[['a','b']])---a 期中 31 期末 4b 期中 101 期末 95dtype: int64 根据索引获取值 1234print(s['a','期末'])---4 loc⽅法取值 1234567891011121314print(s.loc['a'])print(s.loc[['a','b']]) print(s.loc['a','期末'])---期中 31期末 4dtype: int64a 期中 31 期末 4b 期中 101 期末 95dtype: int644 iloc⽅法取值(iloc计算的事最内层索引) 123456789print(s.iloc[1])print(s.iloc[1:4])---4a 期末 4b 期中 101 期末 95dtype: int64 然后再让我们来看看DataFrame的操作 123456789101112131415# dataframeclass1=['python','math','En']class2=['期中','期末']m_index2=pd.MultiIndex.from_product([class1,class2])df2=df(np.random.randint(0,150,(6,4)),index=m_index2) print(df2)--- 0 1 2 3python 期中 88 69 82 28 期末 110 60 130 133math 期中 64 103 24 49 期末 23 41 10 61En 期中 124 139 65 115 期末 114 13 117 79 获取列 12345678910print(df2[0])---python 期中 88 期末 110math 期中 64 期末 23En 期中 124 期末 114Name: 0, dtype: int64 ⼀级索引 1234567print(df2.loc['python'])--- 0 1 2 3期中 88 69 82 28期末 110 60 130 133 多个⼀级索引 12345678print(df2.loc[['python','math']])--- 0 1 2 3python 期中 88 69 82 28 期末 110 60 130 133math 期中 64 103 24 49 期末 23 41 10 61 取⼀⾏ 12345678print(df2.loc['python','期末'])---0 1101 602 1303 133Name: (python, 期末), dtype: int64 取⼀值 1234print(df2.loc['python','期末'][0])---110 iloc是只取最内层的索引的 12345678print(df2.iloc[0])---0 881 692 823 28Name: (python, 期中), dtype: int64 时间序列 ⽣成⼀段时间范围 该函数主要⽤于⽣成⼀个固定频率的时间索引，在调⽤构造⽅法时，必须指定start、end、periods中的两个参数值，否则报错。 时间序列频率 解释 D ⽇历⽇的每天 B ⼯作⽇的每天 H 每⼩时 T或min 每分钟 S 每秒 L或ms 每毫秒 U 每微秒 M ⽇历⽇的⽉底⽇期 BM ⼯作⽇的⽉底⽇期 MS ⽇历⽇的⽉初⽇期 BMS ⼯作⽇的⽉初⽇期 12345678910111213date = pd.date_range(start='20190501',end='20190530')print(date)---DatetimeIndex(['2023-05-01', '2023-05-02', '2023-05-03', '2023-05-04', '2023-05-05', '2023-05-06', '2023-05-07', '2023-05-08', '2023-05-09', '2023-05-10', '2023-05-11', '2023-05-12', '2023-05-13', '2023-05-14', '2023-05-15', '2023-05-16', '2023-05-17', '2023-05-18', '2023-05-19', '2023-05-20', '2023-05-21', '2023-05-22', '2023-05-23', '2023-05-24', '2023-05-25', '2023-05-26', '2023-05-27', '2023-05-28', '2023-05-29', '2023-05-30'], dtype='datetime64[ns]', freq='D') req：⽇期偏移量，取值为string, 默认为'D'， periods：固定时期，取值为整数或None freq: 时间序列频率 12345678date = pd.date_range(start='20230501',periods=10,freq='10D')print(date)---DatetimeIndex(['2023-05-01', '2023-05-11', '2023-05-21', '2023-05-31', '2023-06-10', '2023-06-20', '2023-06-30', '2023-07-10', '2023-07-20', '2023-07-30'], dtype='datetime64[ns]', freq='10D') 根据closed参数选择是否包含开始和结束时间closed=None，left包含开始时间，不包含结束时间， right与之相反。 1234567data_time =pd.date_range(start='2023-08-09',end='2023-08-14',closed='left') print(data_time)---DatetimeIndex(['2023-08-09', '2023-08-10', '2023-08-11', '2023-08-12', '2023-08-13'], dtype='datetime64[ns]', freq='D') 时间序列在dataFrame中的作⽤ 可以将时间作为索引 12345678910111213141516index = pd.date_range(start='20230801',periods=10)df = pd.Series(np.random.randint(0,10,size = 10),index=index) print(df)---2023-08-01 72023-08-02 22023-08-03 52023-08-04 52023-08-05 22023-08-06 02023-08-07 22023-08-08 32023-08-09 62023-08-10 5Freq: D, dtype: int64 truncate这个函数将before指定⽇期之前的值全部过滤出去,after指定⽇期之前的值全部过滤出去. 12345678910111213after = df.truncate(after='2023-08-8')print(after)---2023-08-01 72023-08-02 22023-08-03 52023-08-04 52023-08-05 22023-08-06 02023-08-07 22023-08-08 3Freq: D, dtype: int64 12345678long_ts = pd.Series(np.random.randn(1000),index=pd.date_range('1/1/2021',periods=1000)) print(long_ts)---2021-01-01 -0.482811...2023-09-27 -0.108047Freq: D, Length: 1000, dtype: float64 根据年份获取 12345678result = long_ts['2022']print(result)---2022-01-01 -0.600007...2022-12-31 0.097874Freq: D, Length: 365, dtype: float64 年份和⽇期获取 12345678result = long_ts['2023-07'] print(result)---2023-07-01 -1.797582...2023-07-31 0.687787Freq: D, dtype: float64 使⽤切⽚ 123456789101112# 使⽤切⽚result = long_ts['2023-05-01':'2023-05-06']print(result)---2023-05-01 -2.3382182023-05-02 -2.1307802023-05-03 0.5829202023-05-04 -0.1825402023-05-05 0.1273632023-05-06 -0.032844Freq: D, dtype: float64 通过between_time()返回位于指定时间段的数据集 123456789index=pd.date_range(&quot;2023-03-17&quot;,&quot;2023-03-30&quot;,freq=&quot;2H&quot;)ts = pd.Series(np.random.randn(157),index=index) print(ts.between_time(&quot;7:00&quot;,&quot;17:00&quot;))---2023-03-17 08:00:00 -0.532254...2023-03-29 16:00:00 0.437697Length: 65, dtype: float64 这些操作也都适⽤于dataframe 12345678910111213141516index=pd.date_range('1/1/2023',periods=100)df = pd.DataFrame(np.random.randn(100,4),index=index) print(df.loc['2023-04'])--- 0 1 2 32023-04-01 -0.220090 0.335770 -0.086181 -0.0460452023-04-02 -1.046423 -0.347116 0.367099 -0.9793542023-04-03 -0.720944 -1.478932 0.220948 0.8018312023-04-04 1.359946 -1.239004 0.309747 -0.0479592023-04-05 -0.256502 2.224782 0.494740 -1.3224902023-04-06 1.488119 0.244942 0.614101 -0.1562012023-04-07 -1.815019 -1.935966 0.239024 -1.3885022023-04-08 1.106623 1.148805 2.120405 -0.7992902023-04-09 -1.902216 0.625965 -0.102506 -0.4305502023-04-10 -0.876382 -2.034205 -0.060846 2.442651 移位⽇期 123456789101112131415ts = pd.Series(np.random.randn(10),index=pd.date_range('1/1/2023',periods=10)) print(ts)---2023-01-01 -0.9769582023-01-02 -0.4874392023-01-03 0.1431042023-01-04 -0.9642362023-01-05 0.7583262023-01-06 -1.6508182023-01-07 0.7092312023-01-08 0.1987142023-01-09 -1.0434432023-01-10 0.220834Freq: D, dtype: float64 移动数据，索引不变，默认由NaN填充 periods: 移动的位数 负数是向上移动 fill_value: 移动后填充数据 freq： ⽇期偏移量 1234567891011121314ts.shift(periods=2,fill_value=100, freq='D')---2023-01-03 -0.9769582023-01-04 -0.4874392023-01-05 0.1431042023-01-06 -0.9642362023-01-07 0.7583262023-01-08 -1.6508182023-01-09 0.7092312023-01-10 0.1987142023-01-11 -1.0434432023-01-12 0.220834Freq: D, dtype: float64 通过tshift()将索引移动指定的时间： 1234567891011121314ts.tshift(2)---2023-01-03 -0.9769582023-01-04 -0.4874392023-01-05 0.1431042023-01-06 -0.9642362023-01-07 0.7583262023-01-08 -1.6508182023-01-09 0.7092312023-01-10 0.1987142023-01-11 -1.0434432023-01-12 0.220834Freq: D, dtype: float64 将时间戳转化成时间根式 1234pd.to_datetime(1688570740000,unit='ms')---Timestamp('2023-07-05 15:25:40') utc是协调世界时,时区是以UTC的偏移量的形式表示的,但是注意设置utc=True,是让pandas对象具有时区性质,对于⼀列进⾏转换的,会造成转换错误。 unit='ms'设置粒度是到毫秒级别的。 时区名字 12345import pytzprint(pytz.common_timezones)---['Africa/Abidjan', ..., 'US/Pacific', 'UTC'] 1234pd.to_datetime(1688570740000,unit='ms').tz_localize('UTC').tz_convert('Asia/Shanghai')---Timestamp('2023-07-05 23:25:40+0800', tz='Asia/Shanghai') 一个处理的例子： 12345678df = pd.DataFrame([1688570740000, 1688570800000, 1688570860000],columns = ['time_stamp'])pd.to_datetime(df['time_stamp'],unit='ms').dt.tz_localize('UTC').dt.tz_convert ('Asia/Shanghai')---0 2023-07-05 23:25:40+08:001 2023-07-05 23:26:40+08:002 2023-07-05 23:27:40+08:00Name: time_stamp, dtype: datetime64[ns, Asia/Shanghai] 先赋予标准时区,再转换到东⼋区。 处理中⽂ 1234pd.to_datetime('2023年7⽉5⽇',format='%Y年%m⽉%d⽇')---Timestamp('2023-07-05 00:00:00') 分组聚合 123456789101112131415161718df=pd.DataFrame({'name':['BOSS','Lilei','Lilei','Han','BOSS','BOSS','Han','BOSS'], 'Year':[2016,2016,2016,2016,2017,2017,2017,2017],'Salary':[999999,20000,25000,3000,9999999,999999,3500,999999],'Bonus':[100000,20000,20000,5000,200000,300000,3000,400000]})df--- name Year Salary Bonus0 BOSS 2016 999999 1000001 Lilei 2016 20000 200002 Lilei 2016 25000 200003 Han 2016 3000 50004 BOSS 2017 9999999 2000005 BOSS 2017 999999 3000006 Han 2017 3500 30007 BOSS 2017 999999 400000 根据name这⼀列进⾏分组 12345group_by_name=df.groupby('name') print(type(group_by_name))---&lt;class 'pandas.core.groupby.generic.DataFrameGroupBy'&gt; 查看分组 12345678910print(group_by_name.groups) # 分组后的数量print(group_by_name.count())---{'BOSS': [0, 4, 5, 7], 'Han': [3, 6], 'Lilei': [1, 2]} Year Salary Bonusname BOSS 4 4 4Han 2 2 2Lilei 2 2 2 查看分组的情况 1234567for name,group in group_by_name: print(name) ---BOSSHanLilei 组的名字 123456print(group) # 组具体内容--- name Year Salary Bonus1 Lilei 2016 20000 200002 Lilei 2016 25000 20000 可以选择分组 12345678print(group_by_name.get_group('BOSS'))--- name Year Salary Bonus0 BOSS 2016 999999 1000004 BOSS 2017 9999999 2000005 BOSS 2017 999999 3000007 BOSS 2017 999999 400000 按照某⼀列进⾏分组, 将name这⼀列作为分组的键，对year进⾏分组 123456789group_by_name=df['Year'].groupby(df['name'])print(group_by_name.count())---nameBOSS 4Han 2Lilei 2Name: Year, dtype: int64 按照多列进⾏分组 123456789101112131415161718192021222324group_by_name_year=df.groupby(['name','Year'])for name,group in group_by_name_year: print(name) # 组的名字 print(group) # 组具体内容 ---('BOSS', 2016) name Year Salary Bonus0 BOSS 2016 999999 100000('BOSS', 2017) name Year Salary Bonus4 BOSS 2017 9999999 2000005 BOSS 2017 999999 3000007 BOSS 2017 999999 400000('Han', 2016) name Year Salary Bonus3 Han 2016 3000 5000('Han', 2017) name Year Salary Bonus6 Han 2017 3500 3000('Lilei', 2016) name Year Salary Bonus1 Lilei 2016 20000 200002 Lilei 2016 25000 20000 可以选择分组 12345print(group_by_name_year.get_group(('BOSS',2016)))--- name Year Salary Bonus0 BOSS 2016 999999 100000 将某列数据按数据值分成不同范围段进⾏分组（groupby）运算 123456789101112df = pd.DataFrame({'Age': np.random.randint(20, 70, 100), 'Sex': np.random.choice(['M', 'F'], 100), })age_groups = pd.cut(df['Age'], bins=[19,40,65,100])print(df.groupby(age_groups).count())--- Age SexAge (19, 40] 35 35(40, 65] 54 54(65, 100] 11 11 按‘Age’分组范围和性别（sex）进⾏制作交叉表 123456789pd.crosstab(age_groups, df['Sex'])---Sex F MAge (19, 40] 18 22(40, 65] 25 27(65, 100] 3 5 聚合 我们先来看聚合函数的表格 聚合函数 解释 mean 计算分组平均值 count 分组中⾮NA值的数量 sum ⾮NA值的和 median ⾮NA值的算术中位数 std 标准差 var ⽅差 min ⾮NA值的最⼩值 max ⾮NA值的最⼤值 prod ⾮NA值的积 first 第⼀个⾮NA值 last 最后⼀个⾮NA值 mad 平均绝对偏差 mode 模 abs 绝对值 sem 平均值的标准误差 skew 样品偏斜度（三阶矩） kurt 样品峰度（四阶矩） quantile 样本分位数（百分位上的值） cumsum 累积总和 cumprod 累积乘积 cummax 累积最⼤值 cummin 累积最⼩值 12345678910111213df1=pd.DataFrame({'Data1':np.random.randint(0,10,5), 'Data2':np.random.randint(10,20,5), 'key1':list('aabba'), 'key2':list('xyyxy')})print(df1)--- Data1 Data2 key1 key20 4 17 a x1 4 13 a y2 0 12 b y3 5 16 b x4 8 10 a y 按key1分组，进⾏聚合计算 ⚠️ 注意：当分组后进⾏数值计算时，不是数值类的列（即麻烦列）会被清除 1234567print(df1.groupby('key1').sum())--- Data1 Data2key1 a 16 40b 5 28 只算data1 123456789101112print(df1['Data1'].groupby(df1['key1']).sum()) print(df1.groupby('key1')['Data1'].sum())---key1a 16b 5Name: Data1, dtype: int64key1a 16b 5Name: Data1, dtype: int64 使⽤agg()函数做聚合运算 1234567print(df1.groupby('key1').agg('sum'))--- Data1 Data2key1 a 16 40b 5 28 可以同时做多个聚合运算 12345678print(df1.groupby('key1').agg(['sum','mean','std']))--- Data1 Data2 sum mean std sum mean stdkey1 a 16 5.333333 2.309401 40 13.333333 3.511885b 5 2.500000 3.535534 28 14.000000 2.828427 可⾃定义函数，传⼊agg⽅法中 grouped.agg(func) 123456789101112def peak_range(df): &quot;&quot;&quot; 返回数值范围 &quot;&quot;&quot; return df.max() - df.min()print(df1.groupby('key1').agg(peak_range))--- Data1 Data2key1 a 4 7b 5 4 同时应⽤多个聚合函数 1234567891011121314print(df1.groupby('key1').agg(['mean', 'std', 'count', peak_range])) # 默认列名为函数名--- Data1 Data2 \\ mean std count peak_range mean std count key1 a 5.333333 2.309401 3 4 13.333333 3.511885 3 b 2.500000 3.535534 2 5 14.000000 2.828427 2 peak_range key1 a 7 b 4 通过元组提供新的列名 12345678print(df1.groupby('key1').agg(['mean', 'std', 'count', ('range', peak_range)])) --- Data1 Data2 mean std count range mean std count rangekey1 a 5.333333 2.309401 3 4 13.333333 3.511885 3 7b 2.500000 3.535534 2 5 14.000000 2.828427 2 4 给每列作⽤不同的聚合函数 123456789101112dict_mapping = { 'Data1':['mean','max'], 'Data2':'sum'}df1.groupby('key1').agg(dict_mapping)--- Data1 Data2 mean max sumkey1 a 5.333333 8 40b 2.500000 5 28 拓展apply()函数 apply函数是pandas⾥⾯所有函数中⾃由度最⾼的函数 123456789101112df1=pd.DataFrame({'sex':list('FFMFMMF'),'smoker':list('YNYYNYY'),'age': [21,30,17,37,40,18,26],'weight':[120,100,132,140,94,89,123]})print(df1)--- sex smoker age weight0 F Y 21 1201 F N 30 1002 M Y 17 1323 F Y 37 1404 M N 40 945 M Y 18 896 F Y 26 123 抽烟的年龄⼤于等18的 1234567891011121314151617def bin_age(age): if age &gt;=18: return 1 else: return 0 print(df1['age'].apply(bin_age))---0 11 12 03 14 15 16 1Name: age, dtype: int64 123456789101112df1['age'] = df1['age'].apply(bin_age) print(df1)--- sex smoker age weight0 F Y 1 1201 F N 1 1002 M Y 0 1323 F Y 1 1404 M N 1 945 M Y 1 896 F Y 1 123 取出抽烟和不抽烟的体重前⼆ 1234567891011def top(smoker,col,n=5): return smoker.sort_values(by=col)[-n:]df1.groupby('smoker').apply(top,col='weight',n=2)--- sex smoker age weightsmoker N 4 M N 1 94 1 F N 1 100 Y 2 M Y 0 132 3 F Y 1 140 按理来说，我们最后展示数据的时候，在用完age上0,1作为判断之后，要恢复成原本的年龄的数据。不过...就这样吧。因为马上，我们要做一个完整的分组案例，从一个csv文件获取数据，然后分组，最后进行数据可视化展示： 分组案例 我们先来读取数据，案例中使用到的数据会上传到我的Github仓库中。 12345678910111213141516171819202122232425262728293031data = pd.read_csv('./data/movie_metadata.csv')print('数据的形状：', data.shape) print(data.head())---数据的形状： (5043, 28) movie_title language country \\0 Avatar English USA 1 Pirates of the Caribbean: At World's End English USA 2 Spectre English UK 3 The Dark Knight Rises English USA 4 Star Wars: Episode VII - The Force Awakens ... NaN NaN content_rating title_year color duration genres \\0 PG-13 2009-02 Color 178.0 Action|Adventure|Fantasy|Sci-Fi 1 PG-13 2007-09 Color 169.0 Action|Adventure|Fantasy 2 PG-13 2015-11 Color 148.0 Action|Adventure|Thriller 3 PG-13 2012-08 Color 164.0 Action|Thriller 4 NaN NaN NaN NaN Documentary plot_keywords budget ... \\0 avatar|future|marine|native|paraplegic 237000000.0 ... 1 goddess|marriage ceremony|marriage proposal|pi... 300000000.0 ... 2 bomb|espionage|sequel|spy|terrorist 245000000.0 ... 3 deception|imprisonment|lawlessness|police offi... 250000000.0 ... 4 NaN NaN ... actor_2_facebook_likes actor_3_name actor_3_facebook_likes \\0 936.0 Wes Studi 855.0 1 5000.0 Jack Davenport 1000.0 ... 然后让我们来处理缺失值： 123456789101112131415161718192021222324252627282930data = data.dropna(how='any')print(data.head())--- movie_title language country content_rating \\0 Avatar English USA PG-13 1 Pirates of the Caribbean: At World's End English USA PG-13 2 Spectre English UK PG-13 3 The Dark Knight Rises English USA PG-13 5 John Carter English USA PG-13 title_year color duration genres \\0 2009-02 Color 178.0 Action|Adventure|Fantasy|Sci-Fi 1 2007-09 Color 169.0 Action|Adventure|Fantasy 2 2015-11 Color 148.0 Action|Adventure|Thriller 3 2012-08 Color 164.0 Action|Thriller 5 2012-07 Color 132.0 Action|Adventure|Sci-Fi plot_keywords budget ... \\0 avatar|future|marine|native|paraplegic 237000000.0 ... 1 goddess|marriage ceremony|marriage proposal|pi... 300000000.0 ... 2 bomb|espionage|sequel|spy|terrorist 245000000.0 ... 3 deception|imprisonment|lawlessness|police offi... 250000000.0 ... 5 alien|american civil war|male nipple|mars|prin... 263700000.0 ... actor_2_facebook_likes actor_3_name actor_3_facebook_likes \\0 936.0 Wes Studi 855.0 1 5000.0 Jack Davenport 1000.0 2 393.0 Stephanie Sigman 161.0 ... 接着，我们来查看一下票房收入统计 导演vs票房总收⼊ 1group_director = data.groupby(*by*='director_name')['gross'].sum() ascending升降序排列，True升序 12345678910111213141516171819result = group_director.sort_values() print(type(result))print(result)---&lt;class 'pandas.core.series.Series'&gt;director_nameEkachai Uekrongtham 1.620000e+02Frank Whaley 7.030000e+02Ricki Stern 1.111000e+03Alex Craig Mann 1.332000e+03Paul Bunnell 2.436000e+03 ... Sam Raimi 2.049549e+09Tim Burton 2.071275e+09Michael Bay 2.231243e+09Peter Jackson 2.592969e+09Steven Spielberg 4.114233e+09Name: gross, Length: 1660, dtype: float64 电影产量年份趋势 1234567891011from matplotlib import pyplot as plt import randomfrom matplotlib import font_managermovie_years = data.groupby('title_year')['movie_title']print(movie_years.count().index.tolist()) print(movie_years.count().values)---['1927-02', ..., '2016-12'][ 1 ... 6] 最后，我们利用之前学过的matplotlib进行数据可视化展示： 12345x = movie_years.count().index.tolist()y = movie_years.count().valuesplt.figure(figsize=(20,8),dpi=80)plt.plot(x,y)plt.show() 结尾 那小伙伴们，到今天为止，我们《AI秘籍》系列课程中的「Python部分」也就完全讲完了。就如我一开始向大家承诺的，这部分内容将完全免费。 而我们的《AI秘籍》课程也才刚刚开始，未来很长一段时间内，我们都将继续和这些基础内容频繁打交道。用它们来呈现我们之后要用到的所有课程。包括AI基础，CV，BI，NLP等课程。 不过在结束了一个阶段之后，我需要花点时间休整一下，也是为了好好的备课，找到最好的结构和顺序为大家编写后面的课程。本次课程的最后这两节课我都有些疲劳，为了赶快完成进度，编写过程当中可能有些粗糙或者遗漏，也请大家多包涵。日后，我可能会对这两节课进行更新，将一些细节的讲解补充完整。 免费部分结束了，日后的收费课程，也期望大家能一如即往的支持。 在这里，我也给大家推荐一些比较好的书籍，希望看到小伙伴们能够快速成长起来。 感谢大家，好了。本节课到此结束，下课了。","link":"/AI-Python-Pandas/"},{"title":"茶桁的 AI 秘籍 - 人工智能数学基础篇 导言","text":"Hi, 大家好。又见面了，我是茶桁。 在之前的一个多月前，我有了一个写一个 AI 系列的想法，起名为《茶桁的 AI 秘籍》，简单规划之后，于 7 月 27 日发出预告，然后历时二十多天将近一个月，完成了其中《Python 篇》的写作。 不知道其中的内容对大家是否有帮助呢？ 那么今天我又回来了，根据规划，Python 以及相关第三方科学计算库只是我们基础学习的一小部分，而很大一部分基础学习都还未进行。 那么这次，我依然给大家带来的是另外一篇基础部分，「人工智能数学基础篇」。 数学对于计算机编程来说重要性是毋庸置疑的，更何况我们现在不仅仅是编程，而是走在「人工智能」的路上。可以说，数学应该是最重要的基础。 我们在学习 AI 的过程当中可能会遇到的一些关于数学方面的一些东西，比如说线性代数里面的矩阵运算，比如说求导，还有一些概率统计，图论方面的一些东西。 如果您觉得自己对于微积分，线性代数，概率统计这些内容自认为掌握的还不错的同学，其实是可以不用看了。如果大家是从文科转过来或者说以前上的数学很多年了也忘的差不多了，那可以来学习一下这套课程。 你将会学到的 ✓ 掌握数据科学领域必备数学知识点 ✓ 掌握机器学习算法中常用数学 ✓ 通俗理解各项数学公式的作用 ✓ 掌握数学知识点应用领域与方法 ✓ 掌握高等数学 ✓ 掌握线性代数 ✓ 掌握概率论 ✓ 掌握统计分析方法 ✓ 掌握结合 Python 进行数学操作 课程内容 数学导论 数学导论概述 微积分基础（导数） 线性代数基础（矩阵） 概率&amp;统计基础（随机变量） 图论（图的概念） 微积分 函数 极限&amp;连续 导数 微分 链式法则 偏导数 梯度 积分 牛顿 - 莱布尼兹公式 泰勒展开 线性代数 线性方程组 行列式与克拉默法则 矩阵及其运算 神经网络中的矩阵/向量 矩阵的性质 矩阵与线性变换 线性变换的几何意义 特征值与特征向量 NumPy 中矩阵的操作 概率&amp;统计 概率是什么 古典概型&amp;几何概型 条件概率&amp;联合概率 期望&amp;方差&amp;协方差 二项分布 高斯分布 中心极限定理 泊松分布 贝叶斯先验分布&amp;后验分布 机器学习分类指标 图论 图的由来 图的构成 图的表示 邻接矩阵 图的种类 最短路径问题 Dijkstra 算法 树 最小生成树 图与人工智能 要求 有一定的数学基础学习起来更顺手 熟悉 Python 将更快上手进行统计分析 说明 本篇是系列《茶桁的 AI 秘籍》中的《基础数学篇》，旨在帮助同学们快速打下数学基础，通俗讲解其中每一个知识点。课程内容涉及高等数学，线性代数，概率论与统计学，同学们在学习过程中应当以理解为出发点并不需要死记每一个公式，快速掌握核心知识点。课程章节内容较多，零基础同学按顺序学习即可，有基础的同学们可以按照自己的需求来有选择的学习！ 此课程面向哪些人： 数据科学方向的同学们； 准备继续学习机器学习，深度学习等方向的同学； 准备面试及就业 AI 相关方向的同学 对此有需求的小伙伴，赶紧如下方式订阅起来： 扫码并关注微信号「坍缩的奇点」，发送消息「AI 数学」，后台将为您推送订阅。 本文中所有代码：https://github.com/hivandu/AI_Cheats/tree/main/math","link":"/Math-Introduction/"},{"title":"13. 微积分 - 牛顿 - 莱布尼兹公式、泰勒展开","text":"[TOC] Hi，大家好。我是茶桁。 上一节课中，我们留了一个小尾巴，就是我让大家注意了一下这个式子： \\[ \\begin{align*} x^2\\vert _{x=6}-x^2 \\vert _{x=1}=6^2 - 1^2 = 35 \\end{align*} \\] 这段含义就是\\(x^2\\)(在\\(x\\)等于 6 的时候) 减去\\(x^2\\)(当\\(x\\)等于 1 的时候)。它的结果就是 6 的平方减 1 的平方，它也等于 35，好神奇。那难道说这\\(x^2\\)和\\(2x\\)之间有什么关联吗？或者说我们把\\(\\int_a^b2xdx\\)的积分写成\\(x^2\\vert_{x=b}-x^2 \\vert_{x=a}\\)?, 难道这两者是相等的吗？ 在这里引出了微积分里面的非常重要的一个定理了：牛顿 - 莱布尼兹公式。 牛顿 - 莱布尼兹公式 牛顿 - 莱布尼兹公式，我们通过一个运动学的问题去说，它表述了怎么样的一个场景呢？ 首先，物体的初速度大小是 0，它做一个匀加速直线运动，其加速度是\\(2m/sec^2\\), 那么它的位移时间方程 s 就应该是\\(s=\\frac{1}{2}at^2 = t^2\\)，我们物理课上学过的对吧？如果有不清楚的同学大家可以自己搜索一下，很好理解。我们把 a 的数值给它带入之后，它就等于 t 平方。 然后我们还可以考虑，不是做匀加速吗，那速度和时间的方程它应该就是： \\[ \\begin{align*} v = (s)' = 2t \\end{align*} \\] 我们打个比方，比如说我初速都是 0，一直做匀加速，每过一秒我的速度就增加 2。所以它的函数方程就应该是 2t。其实也就相当于对位移求导。就可以得到速度方程，是我们物理学上面的一个常识。 我们先看一下这两个图像： 上面一张图是我们位移时间方程的图像，下面是速度时间的函数图像。分别是\\(s=t^2\\)以及\\(v=2t\\)。 我们再来看一下这两个图所对应的几何意义。比如说我从 0 秒一直到 5 秒，上面位移时间图像里面它代表什么意义呢？0 秒的时候在原点，距原点的距离为 0，在 5 秒的时候就应该在如图的位置，\\(5^2\\), 应该是 25 米： 我从 0 秒到 5 秒总共走了多远是不是就是直接拿纵坐标直接减就行了？25-0，那不就是 25 嘛。 同样的问题再放在速度时间方程上面来看。横坐标是时间 t，纵坐标是速度 v，我们知道速度乘以时间就是等于路程，或者说等于位移。函数曲线和\\(x\\)轴围成的面积几何意义就是位移。如果我们想通过速度时间图像去求解从 0 秒到 5 秒总共走了多远的话，求 0 秒到 5 秒形成的三角形的面积就行。 我们已经知道面积代表着\\(v \\times t\\),不管是矩形还是三角形，两个数值乘在一起代表的物理含义就是位移。所以对于速度图像而言，我们求它从 0 秒到 5 秒走过的路程就是把从原点出发，一直到 5 秒，围成的这么一个三角形的面积求出来。如图： 三角形对应的面积就是我们在这 5 秒内走过的位移。 那按照几何意义来说的话，面积怎么求呢？边长分别是 5 和 10，所以面积等于\\(\\frac{1}{2} \\times 5 \\times 10\\), 等于 25。和我们用位移时间函数所求到的结果是一模一样的，唯一的区别就是在位移图像里面它是直接纵坐标结束值和初始值相减，在速度图像里面是求面积。 那我们就发现了一个非常神奇的事实：当我们对速度函数求积分的时候，其实就是求被积函的原函数。 其实就是对速度函数做了一个反向求导，从位移到速度它是一个求导过程。从速度到位移我们暂且把它叫做反向求导，就是回到里面逆向求导一下，得到函数之后再减去相应的纵坐标值就可以了。 牛顿 - 莱布尼兹公式非常非常的重要。因为它告诉了我们对于积分的形式我们应该怎么样去求解，我们来看一下定积分公式： \\[ \\begin{align*} \\int_a^b f(x)dx = F(x) \\vert_a^b = F(b) - F(a) \\end{align*} \\] 如果我们要求积分的话，只需要找出\\(f(x)\\)对应的原函数，然后把它在 a 点和 b 点的值求出来相减，就可以得到\\(f(x)\\)从 a 到 b 之间积分的值。 这是非常重要的一个事情，因为它被发现之前呢其实人类是不知道怎么样处理。虽然我知道\\(\\int_a^b\\)的意义，知道怎么求导，但是关键我不知道怎么积。正是因为公式的发明，我们才知道，原来求它原函数就行了。 原函数的意思就是说，比如这里\\(f(x)\\)是\\(2x\\)的话，那它的原函数是什么样的函数求导得到了\\(2x\\)呢？就是\\(x^2\\)。 所以我们可以看到积分其实是分成两种，一种称为「定积分」一种称为「不定积分」。定积分有具体的积分上下界以及具体的函数值相减，对应的这么一个结果。而不定积分只需要求出一个原函数就可以了。不定积分公式如下： \\[ \\begin{align*} \\int f(x)dx = F(x) + C \\end{align*} \\] 不管是定积分还是不定积分，核心都是逆向求导，找出原函数。 细心的小伙伴会注意到，在不定积分这里求出来原函数还加了一个常数\\(C\\)。为什么要加\\(C\\)呢？难道说\\(f(x) = 2x\\), \\(F(x)\\)是\\(x^2\\), 又或者是\\(x^2 + 1\\)都行吗？我们可以尝试求导试一下，确实是这样没错。这其实是一个之前课程中教过的问题，就是常数求导等于多少？等于 0 对吗？这项没有了不会体现在这里。 所以我们反向求导的时候会给它加上一个常数\\(C\\)，表示它的原函数不是只有一个，是有无穷多个，根据\\(C\\)来做区别，但是核心的\\(F(x)\\)这一部分是一样的。 很多小伙伴求不定积分，反向求导的时候把\\(F(x)\\)求出来就万事大吉了，忘了加一个常数\\(C\\)。也有的不太会加，其实就直接写上\\(+C\\)就行。因为你也并不知道常数是什么，\\(C\\)可以代表任何常数。不能就光写一个\\(F(x)\\)。 在我们定积分和不定积分的两个公司中， \\(F(x)\\)是\\(f(x)\\)的原函数， \\(C\\)为常数， \\(a, b\\)是积分区间。 我们来看一个例子： 求积分\\(\\int_1^6(x^3 + 3x^2+cos x)dx\\)的值。 我们从 1 到 6 积的积分，这被积函数看着似乎很复杂的样子，如果我们不知道牛顿 - 莱布尼兹公式的话就束手无策，但是因为这两位大神特别给力，所以现在呢，我们只需要找出这个被积函数的原函数是什么： \\[ \\begin{align*} F(x) = \\frac{1}{4}x^4 + x^3 + sinx \\end{align*} \\] 大家可能一开始会比较习惯了求导，但是反向求导或者说求原函数的过程可能还得有一个适应的过程，多练几遍其实就熟悉了。 打比方说， \\(x^3\\), 我们想一下，三次方之前肯定是四次方，四次方要求导拿下来肯定会有一个数字 4, 那在被积函数里数字 4 没有了，所以原函数肯定是有一个常数 1/4, 当求导的时候，拿下来的 4 和 1/4 抵消了，所以在被积函数里系数才是 1，求出来导数是\\(x^3\\)。 当前我们举的这个例子还是比较去好判断它的原函数，有一些函数的原函数比较难去判断，会比较复杂。还有一点，就是我们要知道，这是被积函数的一个原函数是这样子，因为被积函数原函数有无穷多个，只要加上一个常数\\(C\\), 就无穷多个。我在这里我只是写出这么一个，就是常数为 0 的情况。 \\[ \\begin{align*} &amp; \\therefore \\int_1^6(x^3 + 3x^2+cosx)dx = F(x)\\vert_1^6 \\\\ &amp; = (\\frac{1}{4}\\times 6^4 + 6^3 +sin6) - (\\frac{1}{4} \\times 1^4 + 1^3 + sin1) \\\\ &amp; \\cong 1510.84 \\end{align*} \\] 所以呢积分值就对应着原函数在 1 和 6 处，这两个点的取值的一个差。算了一下，近似值呢是1510.84，是一个无限小数。 这里还有一个地方要给大家说明一下，大家看到\\(sin6\\), 还有后面的\\(sin1\\), 并不是代表着 6 度，它并不是代表着我们平时理解的角度。我们可以注意到无论是 6 还是 1，右上角都没有代表读书人的小圆圈，因为它本身就不是角度，代表的是弧度。 弧度和角度怎么样去换算呢？就是\\(2\\pi\\), \\(2\\pi\\)等于 360 度，\\(\\pi\\)就等于 180 度，换算的话，\\(1弧度=180 / \\pi 度，1 度=\\pi / 180 弧度\\)。大家这里可以了解一下。 牛顿 - 莱布尼兹公式的意义 那牛顿 - 莱布尼兹公式对于我们有什么样的意义呢？ 首先，他提供了求积分问题的一个非常有效的办法，我们可以通过方向求导得出被积函数的原函数；并且将复杂的积分运算转化为简单的加减运算。 为什么说积分运算是一个非常复杂的运算，就是因为如果我们没有牛顿 - 莱布尼兹公式的话，如果要去计算积分唯一的办法只有一开始说的，通过画无限多个矩形去逼近它，积分这一种方式去做。而划分的过程就太复杂，这跟你划分的多精细还有关。而且关键最后得到结果还不是一个很准确的值，只是一个近似值，永远只是一个近似值，你永远达不到它积分的值。而公式将复杂的积分运算化解成了简单的这种加减运算。 第二个，它架起了微分学和积分学之间的一个桥梁。历史上其实微分学和积分学一开始是独立发展的。那个时候科学家是不知道这两个东西是有联系的，并没有意识到一个正向一个反向过来。自从牛顿 - 莱布尼兹公式提出之后，微分学和积分学才被称为了微积分。 正因为上面的这些重要性，牛顿 - 莱布尼兹公式也被称为了微积分基本定理。这是一个很高的荣誉，整个微积分的基本定理。 接下来，再给大家一个小知识点： 我们考虑，一个直恒为正的函数，也就是不存在图像在\\(x\\)轴下方出现负面积的情况。 比如上图这样，函数值恒为正。如果是函数的话，考虑它的几何意义是什么。就是函数图像以及\\(x\\)轴所围成的面积，通过积分求出来的就是面积。 那问题来了，我们通过积分运算求出来的值是它和 x 轴围成的面积的准确值，还是说它是一个极度逼近的一个近似值？准确值什么意思，就是面积被求出来了，一点点都不差，正好是这么多。而极度逼近的近似值就是可以无限的逼近，误差非常非常小，但是还是不相等，还是有一点点误差。 函数是连续的，就考虑连续的函数。 其实答案是准确值。 为什么我在这里提出这么一个问题？因为我还记得上高中的时候，发现我周围的同学很多人以为是一个极度逼近的近似值，而不是一个准确值。就跟那个极限的概念一样，很多人觉得它是一个无限逼近但永远达不到的这么一个过程。 积分求出来的，就是一个准确值。它面积是多少，通过积分求出来就是多少，大家这点要记住。 泰勒展开 接着，是我们微积分里最后一部分：泰勒展开。是不是看到这里大家都舒一口气？万里长征终于来到最后一站了，当然了还是会有些公式。 首先，我们通过微积分基本定理可知下面的式子： \\[ \\begin{align*} f(x) - f(a) = \\int_a^xf'(t_1)dt_1 \\implies f(x) = f(a) + \\int_a^xf'(t_1)dt_1 \\end{align*} \\] 这里被积函数用\\(f’(x)\\)表示了，原函数用\\(f(x)\\)表示。然后我们稍微做一下变形，就能得到\\(f(x) = f(a) + \\int_a^xf'(t_1)dt_1\\)。 然后我们再对\\(f’(t_1)\\)做同样的处理，就是\\(f’(t_1)\\)是关于\\(t_1\\)的一个函数，导函数它也是一个函数，就像我们之前说过的，导数可以求很多阶，只要你 n 阶可导的话，你一直可以求到 n 阶导数。所以求了一次导之后，它仍然可能是一个量的函数。 我们再做同样的处理： \\[ \\begin{align*} f(x) = f(a) + \\int_a^xf'(a)dt_1 + \\int_a^x \\int_a^{t_1} f''(t_2)dt_2dt_1 \\end{align*} \\] 就是把上一个式子中结果里的\\(f’(t_1)\\)看作成它所在式子中最前方的\\(f(x)\\)，其实做的就是这么一个事情。如果大家式子觉得看着脑壳疼可以不用去看。我就告诉你，就是把\\(f'(t_1)\\)看作了\\(f(x)\\)一样，然后套用一下$f(x) - f(a) = _a^xf'(t_1)dt_1 $这个式子去做。 头晕了吗？别急，往下看，往下看你就会更晕了，接着，我们在对上面得到的式子中的\\(f''(t_2)\\)继续做同样的处理： \\[ \\begin{align*} &amp; f(x) \\\\ &amp; = f(a)+\\int_a^xf'(a)dt_1 + \\int_a^x \\int_a^{t_1} f''(a)dt_2dt_1+\\int_a^x\\int_a^{t_1}\\int_a^{t_2}f'''(t_3)dt_1dt_2dt_3 \\end{align*} \\] 这里呢，其实就是吧\\(f''(t_2)\\)同样看作\\(f(x)\\)，然后得到结果，就是这么一大长串。双重积分、三重积分；你想要几成都可以，看你自己乐意。 那我们做这些有啥意义呢？别急，接着往下看。 首先，我们先来看一下形式，先别管这后面这一项，后面一项它是跟\\(t_3\\)有关的，\\(t_3\\)是一个自变量，它不是一个常数。 我们来看一下前面，\\(f(a)\\)是一个常数了，\\(f’(a)\\)是一个常数了，\\(f''(a)\\)是一个常数了。那不就是\\(f(x) = a\\)处函数值、一阶导数、二阶导数，不都是常数吗？所以\\(f(a), f'(a), f''(a)\\)这三个均为常数，我们可以得到什么？直接对它做积分： \\[ \\begin{align*} \\int_a^xf'(a)dt_1 = \\frac{f'(a)}{1}(x-a) \\end{align*} \\] 这里，可以把\\(f’(a)\\)用\\(C\\)来代替，但我们还是先给它保留一下:\\(\\frac{f'(a)}{1}\\)。 我们可以这么看，先别管\\(f'(a)\\)，把这项给拿掉，就看做它系数是 1，就是积分 a 到\\(x\\), 乘上 1，然后乘上\\(dt_1\\)。 什么东西求出来它的导数是 1 呢？不就是\\(f(x) = x\\)，\\(t_1 = x\\)的时候、\\(t_1 = a\\)的时候，它两相减乘出来的结果，再乘上常数\\(f’(a)\\)，在这里它除以 1 是我人为加的。如果计算的话，不会有除以 1 这一步，那我为什么要人为加这么一个呢？我们接着往下看： \\[ \\begin{align*} \\int_a^x\\int_a^{t_1}f''(a)dt_2dt_1 &amp; = \\int_a^x \\frac{f''(a)}{1}(t_1 - a)dt_1 \\\\ &amp; = \\frac{f''(a)}{1 \\times 2}(x-a)^2 \\end{align*} \\] 求二重积分怎么求？我们从里到外一步一步的求，我们先看这部分：\\(\\int_a^{t_1}f''(a)dt_2\\)，不然两个一块看你绝对脑壳疼，而且也反应不过来。 这一部分求积分不就和上方的式子类似吗，求出来结果就是\\(\\frac{f''(a)}{1}(t_1 - a)\\), 和上面类似，只不过就是表现形式上面，符号上面有一些差别。 当我们做完了这些之后，我们再来求这部分：\\(\\int_a^x \\frac{f''(a)}{1}(t_1 - a)dt_1\\), 双重积分我们就一层一层来，别想一口吃个胖子，一下子就把两层积分全给做了。那这个时候，\\(\\frac{f''(a)}{1}\\)是一个常数，然后\\(t_1\\)是自变量的一次，\\(a\\)是一个常数。所以我们求出来最终结果。\\(t_1-a\\)这部分的原函数，写成\\(\\frac{1}{2}t_1^2 - at_1\\)。再往里面带，当它等于 a 的时候什么值，当\\(t_1\\)等于\\(x\\)的时候什么值。 注意，这里的\\(x\\) 它就不再是我们惯常表示的那个自变量了，这里\\(x\\)我们可以把它当成一个常数。 通过我们牛顿 - 莱布尼兹公式可以算出来双重积分求出来结果:\\(\\frac{f''(a)}{1 \\times 2}(x-a)^2\\)。 然后我们接下来再来看一下，我们一直这样做下去会怎样。 我们一直做下去，那他四重积分又出来了，之后是五重、六重，一直到 n 重都能出来。在 n 重之前的这些积分，不管多少重，它这里面都是一个常数。所以我们在这里就可以得到： \\[ \\begin{align*} &amp; f(x) \\\\ &amp; = f(a) + \\frac{f'(a)}{1!}(x-a) + \\frac{f''(a)}{2!}(x-a)^2+...+\\frac{f^{(n)}(a)}{n!}(x-a)^n+R_n(x) \\end{align*} \\] 只要我们一直做下去，就会发现\\(f(x)\\)可以表示成这种形式。它在某一点的函数值，这一点的一阶导、二阶导、三阶导，都乘上一个多项式，一直到\\(n\\)次的多项式，然后再除以 n 的阶乘。 阶乘是什么意思大家都应该知道吧？我们拿 5 举例，5 的阶乘就是\\(1\\times 2 \\times 3 \\times 4 \\times 5\\)，多少的阶乘就是从 1 一直乘到多少个数。阶层的增长率其实是超过指数函数的，它的增长速度是远超过指数函数的，阶乘是一个很可怕的数。 当然，在数学上面如果你要表示非常大的数，可以用高德纳数来表示，这里就不展开说了。 最后，我们还拖了一个小尾巴，\\(R_n(x)\\)为什么没有写了，如果它写的话它是套了 n 重积分的一个形式。 这里我们就把函数用这种形式来表示，那形式有什么意义？其意义非常大，我们一个一个来看： 不管是什么样形式的函数，都可以通过多项式函数来拟合。多项式函数：易于计算，可以快速获得结果。 比如\\(cosx\\)，当\\(x\\)为 30 度，45 度，90 度的时候，我们都可以进行计算。但是当 x 为 37 度，38.89 度，二分之派，你还能算出来吗？人和机器都不行。算这些函数都是通过「泰勒展开」来计算的。把他们化成这种形式：就是我们上面最终的\\(f(x)\\)得到的形式去计算的，通过计算多项式函数来做。 所以大家不要以为计算机啊很牛逼，其实它在智能程度远逊于人类，它只能做一些非常简单的二进式的加减法，什么0+1、1+0、1*1这种东西。但是它可以通过把复杂函数转换成多项式，通过强大的算力快速的告诉我们结果，计算机就是这样去计算。 包括大家在中学的时候应该用过一个中学数学用表，在上面会看到不同的函数，在\\(x\\)等于几的时候都会有一个近似值。那时候是不是很好奇值咋算的，我那时候就好奇过。那时候我反正是想不明白这东西怎么算。后来查资料就发现是通过泰勒展开算出来的，就是通过这种方式。 那么所有的复杂函数都是用泰勒展开转换成多项式函数计算的。 泰勒展开也是有一个适用范围的。我们举一个例子：\\(\\frac{1}{1-x} = 1+x+x^2+x^3+o(x^3)\\) 这个函数，不是对任意的\\(x\\)都成立，比如如果我\\(x\\)等于 10 的话，左边是一个负数了，但是你看右边是一个非常大的一个正数，这根本是不可能的。这还引发过数学史上非常有名的一个悖论。所以泰勒展开它是有一个适用范围的。对于上面这个例子而言，它的适用范围就是 1，它的收敛半径就是 1。 有一种泰勒展开比较特殊，就是你令多重积分里这些a 等于 0 的时候，这种泰勒展开被称为麦克劳林展开。 我们来看一个例子，来看一下它到底是否像说的那么厉害。 例：求函数\\(f(x) = e^x\\)在\\(x=1\\)时的值。 当我们拿到这个函数的时候，感觉都没办法去做，e 的值都不知道。其实就是让我们求 e 的值。 由函数的麦克劳林展开式，我们可以得到： \\[ \\begin{align*} f(x) = e^x \\cong 1 + \\frac{1}{1!}x^1 + \\frac{1}{2!}x^2 + \\frac{1}{3!}x^3 +\\frac{1}{4!}x^4 + \\frac{1}{5!}x^5 \\end{align*} \\] 我们这里就先精确到\\(x\\)的 5 次方。 我们最终得到结果： \\[ \\begin{align*} f(1) = e^x &amp; \\cong 1 + \\frac{1}{1!}x^1 + \\frac{1}{2!}x^2 + \\frac{1}{3!}x^3 +\\frac{1}{4!}x^4 + \\frac{1}{5!}x^5 \\\\ &amp; = 2.7167 \\end{align*} \\] 通过查表，e 的值是多少？是\\(e^1 = e = 2.71828...\\), 一直循环下去。我们会发现仅仅是通过简单的四则运算，再加上简单的乘方运算，乘方当然也可以转化为乘法运算。就计算出了一个非常复杂函数的值，泰勒展开的威力可见一般。 我们在计算机去处理的时候，其实它没有我们聪明，人做不到事情它也做不到，只能转换成这些多项式然后去计算。靠的是它强大的算力，可能把小数点的位数给精确到多少位多少位，再交给我们去查，仅此而已。 那我们如何理解泰勒展开？其实泰勒展开是可以用一系列的多项式函数来拟合任意一个函数，它拟合的道理是什么？我们看下面的图： 比如红色的线代表了我们要拟合的函数，锯齿状的绿色曲线代表了我们拟合的效果。泰勒展开就是想用一个函数模拟另外一个。模拟不仅仅代表着在某些点的坐标，或者说函数值相同；更代表了在这些点的一阶变化率相同，二阶变化率一直到 n 阶变化率都相同。 也就是两个函数如果非常接近了，非常趋于相同了。那应该说不光在某些点函数值相同，在这些点的各阶的变化率对应的情况也相同。 就像两个赛车在比赛一样，不是哪些位置他们俩的速度相同，还得比他们在哪些位置的加速度是否也相同，这样他们运动曲线才是一样的。也就是说，如果两个函数\\(f(x)\\)和\\(g(x)\\)相同，那么： \\[ \\begin{align*} &amp; f(x) = g(x) \\\\ &amp; f'(x) = g'(x) \\\\ &amp; f''(x) = g''(x) \\\\ &amp; ... \\\\ &amp; f^{(n)}(x) = g^{(n)}(x) \\\\ &amp; 各阶的变化情况相同 \\end{align*} \\] 结束语 以泰勒展开为结束点，我们今天的课程到这里也就结束了。微积分相关的内容到这里就全部讲完了，当然，我所讲的这些内容，在熟悉之后应付人工智能是能应付了，但是远远不是微积分的所有内容，并且讲的也并不细致，所以不要将我所讲的这些内容当作正规的数学教材。如果您是为了去理解和学习数学，那么就当作我给你开了一个窗，入门的话，还需要正规的数学教材去系统的好好学一下。 对于只是希望进入人工智能大门的小伙伴，要记得我一开始所说的，微积分这部分内容体系虽然很庞大，但是最核心的还是导数和链式法则。当然在神经网络里面导数大多数情况是以偏导数的形式存在。我们用梯度下降算法去优化神经网络。 那么，这一段时间大家辛苦了。可以先好好休息一下，下节课估计会晚一点放出来，我先去好好的备下课。下节课开始，我们来学习线性代数的相关内容。还记得我规划的相关目录吗？我再把线性代数部分贴出来大家看看： 线性代数： 线性方程组 行列式与克拉默法则 矩阵及其运算 神经网络中的矩阵/向量 矩阵的性质 矩阵与线性变换 线性变换的几何意义 特征值与特征向量 NumPy 中矩阵的操作 当然，最后还是需要根据内容多少来确定一篇文章内包含多少。 好，我是茶桁，咱们下节课再见。","link":"/calculus-8/"},{"title":"AI核心能力基础 - 规划和概要","text":"Hi，你好。又见面咯，我是茶桁。 在之前，我花了两个来月的时间撰写了「Python篇」和「数学篇」，希望小伙伴们在正式进入AI之前能够打好一个基础。那么今天开始，我们将正式开始AI基础的学习。 这一节课咱们先不着急直接开始课程，而是聊一下本次课程的一个规划。 在整个课程规划中，我们将会直接从机器学习开始入手，进入深度学习，然后开始接触RNN、CNN以及三大方向：NLP、CV和BI。核心能力将会分成四大部分进行展开精讲。 目录规划 基础能力 人工智能导论 机器学习初探 机器学习进阶（这部分会比较长） 深度学习进阶 RNN CNN 自然语言处理基础（NLP） 计算机视觉基础（CV） 商业智能（BI） BI精讲 预测全家桶与机器学习四大神器 Fintech数据分析 数据可视化与DashBoard ALS算法与推荐系统 SVD矩阵分解与基于内容的推荐 PageRank、图论与推荐系统 Graph Embedding 强化学习 NLP精讲 自然语言处理的基本过程 向量空间模型 自然语言处理初步 语言模型和概率图模型 词向量模型Word2Vec Transformer与BER,大规模预训练问题 自然语言生成 自然语言处理与人工智能前沿 CV精讲 初阶计算机视觉：图像处理 中阶计算机视觉：图像描述 中阶到高阶的关键：CNN方法 计算机视觉中的图像分类 深度学习之单阶段目标检测 深度学习之两阶段目标检测 计算机视觉中的图像分割 计算机视觉中的目标跟踪 内容输出方式 以上目录中的四个部分都属于核心部分，每一个部分都会单独开一个专栏目录。一个是因为收费课程，拆散之后大家可以按照自己的需要进行购买，再一个也是将四部分区分的清晰一点。 虽然每一张专辑都是收费的，但是也并不是所有内容都需要进行购买才可查看。有的时候为了吸引流量，即便没有购买专辑，部分章节会开放阅读全部。 以上目录仅供参考，目录是按照内容概要进行规划的，并不等于实际章节。就像我在写数学篇的时候，本来就只规划了4个知识点，但是其中一个知识点可能会讲7、8个章节，也可能3、4篇就讲完了。所以届时的内容，会比从目录上看要多的多，起码就基础部分的机器学习这一知识点，可能就要十几、二十节课才能讲完。 代码库 在咱们的整个讲解过程，演示代码是不可避免的，并且其中还会包含很多数据。这部分内容基本上都会在咱们的《茶桁的AI秘籍》的代码仓库中找到，地址为：https://github.com/hivandu/AI_Cheats 其中部分数据集可能因为太大会上传到百度网盘并分享出来，分享一般都会放在文末，大家可以自取。 其他 如果您阅读时感觉文章不完整，那应该是该网站我暂时无法发布收费专栏，所以我仅提供了部分内容。如果需要阅读完整内容，请关注「坍缩的奇点」后自取。","link":"/ai-core-competence-outline/"},{"title":"Python 批量修改文件名","text":"仅个人需求，有需要的可以自取。 前段时间为家里孩子下载了一批课程，但是文件命名就很奇怪也很乱，就想着将文件名修改掉便于查看。 这批视频下载下来后前边都给了诸如001之类的编号，当然是序列。可是这批序列又非常的乱，比如，「数列」和「导数」给的是考前的编号，而课本上要先学习的「集合」，「逻辑」，「不等式」等又编号又很靠后。不仅如此，就算是同一部分，其中的编号也是混乱的。 那么就有了批量修改文件名的需求，当然我第一时间想到的是Better rename，已经是一个很古老的版本了： 可是当我使用的时候才发现并不能满足我的个人需求，也许是我不太会用吧。起码，我是想删掉开头的那些序列以及其中重复不必要的内容。但是这玩意并不能支持正则或者相关的功能。没办法，眼见有几种方式去做，一种是Mac自带的「自动操作」，一种是「捷径」，还有就是干脆用Python写个脚本。 所以，我使用了自己觉得最简便的方式，写了这样一个脚本： 12345678910111213141516171819202122232425262728293031323334import osimport tkinter as tkfrom tkinter import filedialogimport reroot = tk.Tk()root.withdraw()folderPath = filedialog.askdirectory() # 获得选择好的文件夹# filePath = filedialog.askopenfilename() # 获得选择好的文件print(folderPath)# print(filePath)files = os.listdir(folderPath)fileList = []newFileList = []oldStr = input('请输入要修改的内容或正则:')newStr = input('请输入要替换的内容，不修改只删除可留空:')for i in files: if i[0] != '.': portion = os.path.splitext(i) newname = re.sub(r'{}'.format(oldStr), r'{}'.format(newStr), portion[0]) # os.chdir(folderPath) # 测试完毕后要正式修改文件名取消这里注释 # os.rename(portion[0]+portion[1], newname+portion[1]) # 测试完毕后要正式修改文件名取消这里注释 fileList.append(portion[0]+portion[1]) newFileList.append(newname+portion[1])print('修改前，一共有{}个文件\\n {}'.format(len(fileList), fileList))print('修改后，一共有{}个文件\\n {}'.format(len(newFileList), newFileList)) 有需要的小伙伴可以自取了。代码执行后会让你选取你要修改的文件的目录，然后会让你输入你要修改的内容，可以是正则，然后输入你要修改成的内容。 比如，我需要修改标题： 12# 输入需要替换的内容\\d{3} - 要替换的内容我直接留空回车，打印结果： 1234修改前，一共有16个文件 ['001 - 不等式 1.1 不等式的基本性质 高中数学.mp4', '016 - 不等式 4.4 恒成立与存在性问题 高中数学.mp4', '007 - 不等式 3.3 高次不等式 高中数学.mp4', '008 - 不等式 3.4 含参讨论 高中数学.mp4', '003 - 不等式 1.3 比大小思路 高中数学.mp4', '002 - 不等式 1.2 不等式证明思路 高中数学.mp4', '006 - 不等式 3.1 二次不等式 高中数学.mp4', '015 - 不等式 1.4 复杂证明题 高中数学.mp4', '005 - 不等式 2.3 基本不等式变式 高中数学.mp4', '011 - 不等式 2.4 不要过度放缩三元不等式 高中数学.mp4', '010 - 不等式 2.2 使用条件误区 高中数学.mp4', '009 - 不等式 4.1 对勾型函数最值 高中数学.mp4', '013 - 不等式 4.2 对称与均值 高中数学.mp4', '004 - 不等式 2.1 基本不等式均值不等式 高中数学.mp4', '014 - 不等式 4.3 齐次与均值 高中数学.mp4', '012 - 不等式 3.2 穿针引线法 高中数学.mp4']修改后，一共有16个文件 ['不等式 1.1 不等式的基本性质 高中数学.mp4', '不等式 4.4 恒成立与存在性问题 高中数学.mp4', '不等式 3.3 高次不等式 高中数学.mp4', '不等式 3.4 含参讨论 高中数学.mp4', '不等式 1.3 比大小思路 高中数学.mp4', '不等式 1.2 不等式证明思路 高中数学.mp4', '不等式 3.1 二次不等式 高中数学.mp4', '不等式 1.4 复杂证明题 高中数学.mp4', '不等式 2.3 基本不等式变式 高中数学.mp4', '不等式 2.4 不要过度放缩三元不等式 高中数学.mp4', '不等式 2.2 使用条件误区 高中数学.mp4', '不等式 4.1 对勾型函数最值 高中数学.mp4', '不等式 4.2 对称与均值 高中数学.mp4', '不等式 2.1 基本不等式均值不等式 高中数学.mp4', '不等式 4.3 齐次与均值 高中数学.mp4', '不等式 3.2 穿针引线法 高中数学.mp4'] 打印内容中查看自己修改前和修改后的文件对比，感觉没问题了，把其中注释的两行代码打开注释，就可以完成文件修改了。","link":"/Python-change-file-names-in-batches/"},{"title":"Mac上挂载APFS移动硬盘","text":"自用文，有需要的自取。 百度网盘同步会认为移动硬盘是系统盘，所以无法进行同步。当然，也有例外的，之前我也是不知怎么同步的。 这次设置的时候被警告了，不允许设置。 好吧，那就只能将移动硬盘挂载到我的用户目录里了，我的移动硬盘是APFS类型，执行下面命令： 1234567891011# 查看当前硬盘IDENTIFIERdiskutil apfs list# 或者下面这段命令diskutil list# 然后需要进行解锁，恢复键值(recovery_key):diskutil apfs unlockVolume /dev/apfs_volume_id -passphrase recovery_key# 接着进行装载到自己期望的目录diskutil mount -mountPoint Path apfs_volume_id 假定我的硬盘apfs_volume_id为disk5s1, 希望挂载到~/mount则： 12diskutil apfs unlockVolume /dev/disk5s1 -passphrase recovery_keydiskutil mount -mountPoint ~/mount /dev/disk5s1 本是留待自用的，有需要的有缘人自行取走。","link":"/mount-apfs-on-mac/"},{"title":"03. 人工智能核心基础 - 导论（2）","text":"[TOC] Hi，你好。我是茶桁。 上一章中，我们谈论了人工智能在时间维度上的不同时间不同的侧重点，这只是一个片面的方面。当然除此之外，我们还要从其他方向来认识人工智能，才能更加的全面。 那下面，我们就分别从方法论，问题范式和研究对象来分别认识一下人工智能，看看有没有什么不一样的心得。 从方法论上来讲 接下来大家来讲从方法论上来看待人工智能。 方法论就是一件问题其实用不同的方法都可以解决，一件问题用不同的方法都可以解决。同样的一个问题，不同的人说的是不一样的。 在整个人工智能的历史上，此起彼伏的一直有两种方法在做：一种叫做基于统计的，一种叫做基于逻辑分析的。 Statistical vs Logic analysis 上节课给大家举的李开复的那个例子，当时人们是希望通过语法树，希望能够写程序去分析语法术来解决，结果发现越做越难。 后来人们就把它变成了基于统计去做，统计就简单了，只要我数一下出现的频率就可以了。 这就是这两个学派，现在是统计学派占优势。 从 1990 年开始到现在，基本上是统计学派占优势。我们之后要学习的，从第 5 节课开始，基本上学习的最主流的 80%-90% 的内容都是基于统计的方法。 但是逻辑的分析方法依然存在，而且如果你以后要做 NLP 自然语言处理，或者做计算机视觉等等，都会遇到这种逻辑类。 待会咱们会来出一道题，我们一起来动手去完成它。就是我们怎么样来生成一个句子。 逻辑和统计的区别非常大，我们如何让计算机生成语言。这个问题其实是当时要做图灵测试非常非常重要的一个能力。包括现在做什么小爱机器人，大家可以拿这个电话去打你最近的海底捞，你会发现海底捞现在基本上 100% 都是机器人在回答了。 那么现在我们摆出来了一个问题，就是机器怎么生成语言？ 今天就跟大家来讲解，这是一个非常古老人工智能方法。但是是一个非常非常具有代表性的，大家可以通过他来看到人工智能的多样性。 我们可能之后学习的时候，从第 5 课学的都是基于统计的方法，但是我们今天要给大家说的是，历史上有很多种方法非常有用。最近虽然不是占主流，但是大家在需要的时候一定要想起它，它能解决你很多很重要的问题。 而且在这个过程中为什么我们提出来了这些新的深度学习方法，也能知道到底哪些东西是解决不了的。 OK，我们现在来看一下这个问题：计算机如何能够生成像人类一样的语言。 当时人们想了很多种方法，有很多人知道，现在用什么 LSTM，GPT-4 等等去做。 它们来做语言生成很好，但是我们来跟大家看看当年人们是怎么样解决这个问题的，曾经一度非常重要的一种方法。 有人他定义了一个东西，比方说我们现在知道了一个grammar 123456grammar = &quot;&quot;&quot;句子 = 主 谓 宾谓 = 吃 | 喝 | 玩宾 = 皮球 | 桃子主 = 你 | 我 | 他&quot;&quot;&quot; 假设我们现在有一个东西叫做「句子」，大家都知道这个句子是主、谓、宾生成的，主语、谓语、宾语。 然后我们这个谓语就是动词，比方说吃|喝|玩,宾语比如说皮球|桃子，然后主语就是你|我|他。 曾经有一段时间，人们为了解决这个问题，写了这样的东西。 1234567891011import randomdef generate_verb(): return random.choice('吃 | 喝 | 玩|'.split('|'))def 生成宾语(): return random.choice('皮球 | 桃子'.split('|'))print(生成宾语()+generate_verb())---桃子玩 大家看，这是非常非常原始，非常非常古老的一种写法。当时人们想利用模板解决问题，最简单的方法。但是人们发现一个问题，就是假如说我们这里的谓语动词有个变化，比如说我们要再加一个动词，或者我们要在这里的宾语再加一个东西： 123456grammar = &quot;&quot;&quot;句子 = 主 谓 宾谓 = 吃 | 喝 | 玩 | 打宾 = 皮球 | 桃子 | 饺子主 = 你 | 我 | 他 | 她&quot;&quot;&quot; 这样的话，就意味着我们每修改一下这个语法，是不是原代码也要操作？ 1234567891011import randomdef generate_verb(): return random.choice('吃 | 喝 | 玩 | 打'.split('|'))def 生成宾语(): return random.choice('皮球 | 桃子 | 饺子'.split('|'))print(generate_verb()+生成宾语())---吃饺子 如果这样的话，代码整合起来就很复杂。而且我们有可能还要说别的话，例如我们在这里的说一个招待，招待的场景就等于打招呼。 123456grammar2 = &quot;&quot;&quot;招待 = 打招呼 玩 活动 吗？打招呼 = 你好 | 您好 | 好久不见玩 = 需要玩 | 喜欢玩 | 想玩活动 = 骑马 | 打球 | 喝茶&quot;&quot;&quot; 你看，我们如果还有另外的一个句子，想把它生成这样的话，意味着又得重新再写一套程序。才能解决 grammar2 的这个问题。 这个东西看起来很简单，但其实它会越来越难。如果说我们现在有个 grammar2，又要再重新写很多句子。 这个时候最古老的一批程序员们就希望：“我能不能不要每次都改写代码，每次代码都不要改了？你给我不同的输入我能接受，代码不经过改动、就能解决这个问题。” 这个思想其实是我们站在程序员视角，程序员思考的人工智能其实是最接近这种想法。 假如说把一张图片要做到识别率 99% 那是很简单的，要把两张图片做到 90% 也是很简单的，专门写一个程序就行。但是怎么样做到每次改了图片之后，还能做到 99% 呢？这是我们需要讨论问题。这个朴素的想法一直延伸到现在。 怎么解决这个问题呢？这个时候就需要抽象了。怎么抽象呢？ 我们现在把它变成一行一行： 123456789for line in grammar.split('\\n'): if not line.strip(): continue print(line)---句子 = 主 谓 宾谓 = 吃 | 喝 | 玩宾 = 皮球 | 桃子主 = 你 | 我 | 他 然后再留一个 grammar，我们把它变成一个表达式： 1234567891011121314151617grammar_gen = dict()for line in grammar.split('\\n'): if not line.strip(): continue print(line) stmt, expr = line.split('=') grammar_gen[stmt] = exprprint(grammar_gen)---句子 = 主 谓 宾谓 = 吃 | 喝 | 玩宾 = 皮球 | 桃子主 = 你 | 我 | 他{'句子 ': ' 主 谓 宾', '谓 ': ' 吃 | 喝 | 玩', '宾 ': ' 皮球 | 桃子', '主 ': ' 你 | 我 | 他'} 就是 statement，期望表达的东西，和 expression。 首先我们把每个 grammar 变成了一个 Dictionary，那么如果能够有一种东西通过 Dictionary 来生成的话，它是不是就可以输入不同的 grammar street 形式。 12345678910111213141516171819grammar_gen = dict()for line in grammar.split('\\n'): if not line.strip(): continue print(line) stmt, expr = line.split('=') expressions = expr.split('|') grammar_gen[stmt.strip()] = [e.strip() for e in expressions]print(grammar_gen)---句子 = 主 谓 宾谓 = 吃 | 喝 | 玩宾 = 皮球 | 桃子主 = 你 | 我 | 他{'句子': ['主 谓 宾'], '谓': ['吃', '喝', '玩'], '宾': ['皮球', '桃子'], '主': ['你', '我', '他']} 谓语后面跟的是一个 list, 宾语也是这样。 现在来写一个很有趣的一个代码。 123456789101112def generate_sentence(gram, target='句子'): if target not in gram: return target exp = random.choice(gram[target]) return ''.join([generate_sentence(gram, e) for e in exp.split()])print('generated: \\n'+generate_sentence(grammar_gen))---generated: 我吃桃子 我们来看一下这段代码，如果我们现在要根据这样的一个 Dictionary 来生成句子，怎么来生成句子呢？这是一个当年很经典的一个程序。 如果我们的一个 target 它在 grammar 的 key 里边，那么我们就证明现在需要去扩展，如果他不在我们的 key 里边，我们就说他不需要扩展，直接返回就可以了。 1if target not in garm: return target 如果他在这个 key 里边我们怎么扩展呢？比方说我们在这里收一个句子，它等于主谓宾，先要扩展主语，再扩展谓语，再扩展宾语。 扩展它的时候，就需要随机去选择一个去扩展的表达式。选择这个表达式之后，按照空格分割生成很多个小的表达式，再把这个小的表达式再次进行生成。 1generate_sentence(gram, e) for e in exp.split() generate_sentence(grammar_gen)就会生成一句话，当然是随机的，这次我们生成的是：“我吃桃子”。 那么这样的好处是什么？ 我们现在来封装一个方法get_grammar, 将之前对 grammar 的处理封装进去，然后我们再修改下generate_sentence方法，之后再打印的时候，我们可以尝试将之前定义的 grammar2 代入进去，只不过我们需要将之前写的招待修改为句子, 因为我们判断 target 是句子，完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142grammar = &quot;&quot;&quot;句子 = 主 谓 宾谓 = 吃 | 喝 | 玩宾 = 皮球 | 桃子主 = 你 | 我 | 他&quot;&quot;&quot;grammar2 = &quot;&quot;&quot;句子 = 打招呼 玩 活动 吗？打招呼 = 你好 | 您好 | 好久不见玩 = 需要玩 | 喜欢玩 | 想玩活动 = 骑马 | 打球 | 喝茶&quot;&quot;&quot;def get_grammar(grammar_string): grammar_gen = dict() for line in grammar_string.split('\\n'): if not line.strip(): continue stmt, expr = line.split('=') expressions = expr.split('|') grammar_gen[stmt.strip()] = [e.strip() for e in expressions] return grammar_gendef generate_sentence(gram, target='句子'): if target not in gram: return target exp = random.choice(gram[target]) return ''.join([generate_sentence(gram, e) for e in exp.split()])print('generated: \\n'+generate_sentence(get_grammar(grammar)))print('generated: \\n'+generate_sentence(get_grammar(grammar2)))---generated: 你吃桃子generated: 你好需要玩骑马吗？ 在这里就可以做到，写了一个 grammar, 打印结果为「你吃桃子」，接下来变成 grammar2，打印出来就是「你好需要玩骑马吗？」 是不是很有意思？我们做到改变了输入，程序没有改，他能接受新的输入，都是随机的。 这就是站在程序员视角，当时研究人工智能 AI 最渴望的就是不需要改变程序了。 下面我们再修改一下 grammar, 给他加上复合句子，起名为句子，将之前的句子修改为 Single 句子，简写为s_句子 12345678grammar = &quot;&quot;&quot;句子 = s_句子 , 连词 句子 | s_句子连词 = 而且 | 但是 | 不过s_句子 = 主语 谓语 宾语主语 = 你| 我 | 他 谓语 = 吃| 玩 宾语 = 桃子| 皮球&quot;&quot;&quot; 这样修改之后，大家觉得会发生什么事情呢？我们还是使用之前的代码，代码不变，然后我们可以得到： 12345---generated: 你吃桃子,不过你吃皮球,不过你玩桃子,但是你吃皮球,但是他玩皮球,而且他玩皮球generated: 您好喜欢玩打球吗？ 他可以产生更加长、更加复杂的句子。 如果我们再加一个判断句子概率的东西，就能够生成更像句子的句子了。 就是之前讲的，李开复在一九八几年探索出来的一种东西，能够去判断一个句子出现的概率。那么当这个程序出现很多句子的时候，我们就能做个判断，给他做个排序、做个打分，就能够知道哪个句子的概率最大了。 这个程序厉害的点不在于它采用了什么非常高科技的方法，而是在这个时候提出来的一种思想，这个思想就是问题的输入变化了，但是我们的代码希望不要变，我们的程序不希望变。这个是这个程序最主要的一个特点。 以后咱们整个课程上，机器学习等等代码，其实都是冲着这一个目标去的。 当时基于这个规则的程序其实也没有那么好写。大家看一下上面这段程序，其实看起来好像很短，但是一个人如果想靠自己的脑力思考出来这样的程序，你会发现并不简单。 这段代码当然不是我原创的，如果你想原创的去解决这个问题其实很复杂。就这也是为什么后来通过这种逻辑分析规则方法没有再进行下去的原因，因为它太复杂了。 这是咱们的第一个案例，目的是希望大家知道 AI 写程序的目的是什么，也希望大家知道有一种方法，只不过最近几年他不太火了，但是曾经产生过很多很重要的方法。 刚刚给大家讲过基于统计的方法，我们判断一句话的概率，会把它变成一组词的概率去统计，也可以变成一个数形的东西。 那么大家现在想想，基于统计和基于逻辑推理这两种方法，他们有什么特点？有什么优缺点呢？ 统计永远达不到 100% 的准确，基于逻辑的灵活性差、代码很复杂。 从问题范式上来讲 好，接下来呢，我们再从另外一个问题的角度来分析一下：从问题范式上。 什么叫做问题范式？ 方法论，就是准备用什么东西去解决它，到底用统计，还是用分析的方法去解决它。 所谓的问题范式就是 Paradigm，其实和方法论类似。只不过它又把问题分成了不同的类型。 第一种类型叫做 Relax based。 解决起来对于人类来说很轻松，比方说这张图里，公交车在哪。 我们一眼就指出来了对不对？或者问左边亮的是红灯还是绿灯？我们一眼就看出来了。 大家一定要清楚一个概念，准则就是人觉得越简单的问题，对于计算机来说越复杂。让计算机去判断这个地方的车在哪里，这个地方的红灯到底表示什么意思，其实是很复杂的一个事情。 这张图里，哪边是狗，哪边是猫，对我们人类来说是 So easy 的一个问题。那这种问题，就叫做 Relax based。 还有一类情况叫做 State based。 在这局对局中，假如你是红方，你该怎么走？ 这个问题我们要思考的时候，和前面就会发现不一样了。前面是看一眼就能懂，而这个你得经过思考，思考你现在在什么状态，走了之后会怎么样。在脑子里面要思考一系列问题。 现在 AI 主要解决的问题是第一种问题，是 Relax based。后面这种 State based 问题解决的不多，但是其实是现在迫切急需解决的问题。AlphaGo 其实解决了一部分，但是它也是需要大量的算力，大量的训练样本。 再比如，从齐齐哈尔到鄂尔多斯这两个地方我开车怎么去。 你要看到地图也不能一下子就知道，也得一个一个点去看。这种问题就是一个典型的我们需要考虑现在在什么情况，做了之后，下一个状态会是什么情况的问题。 再下一种情况叫做优化模型。 什么叫优化模型？比方说一个工厂就这么多时间这么多人力，我怎么样让这个工厂现在产出最大的金额，在一个固定的范围之内，固定的约束条件之下要得到一个最优质的答案。 这也是一种很典型的 AI 问题，给一个复杂函数，怎么样找到这个函数的最优点？这也是一类情况。 美团外卖的小哥，怎么样能够让他最快的接到单子，怎么样给他策划一个线路，让他最快的能够送达，这就是属于这样的一个问题。 除此之外，还有一类情况叫做纯推理问题。 这个其实也是人们一直在期望解决，但是现在解决的情况也不好。 比方说，下过雨天地面就会湿，那么现在我告诉你地面湿了，问你下过雨没。机器就应该自动知道下过雨。 这个问题现在难点是难在，首先人类社会中有大量的场景需要这个，在医学上、在法律上、保险上。但是用机器学习的方法现在不好做，效果很差。人们各种探索做的也不太好。 斯坦福大学为了掩饰这个问题写了一个简单程序，但是也只能解决非常非常基础的问题。 注释：此截图来自于地址：https://stanford-cs221.github.io/autumn2020/assignments/logic/index.html 在这些问题的类型里边，Relax based model 是现在 AI 解决的最多的，图像分类识别。接下来就是优化问题，应用的也比较多。State based model 会用到一些，比如说滴滴打车，其实我们常用的就有高德导航。最难的是逻辑推理，逻辑推理的问题很难做。 接下来，讲解一下什么叫做 State based model。出这么一道问题，一道很典型的智力题。很多人不太理解我们 State besed model 是解决什么问题，我们先来做一道智力题。 现在有两个杯子，左边的杯子是 90 毫升，右边的杯子是 40 毫升的。 现在这边有个水池，水池有个水龙头可以接水。现在我问个问题，如何得到 50 毫升的水？ 很简单对吧？拿 90 的杯子去给它装满，然后我们再不断的倒水，把 90 往 40 里倒， 40 的倒满之后，90 里剩下的就是 50。 那我再问一下，如何得到 60 毫升的水？或者我们问，通过这样的方法能得到 60 毫升的水吗？ 这个问题是不是看着就很难？那能不能得到 65？能不能得到 70？能不能得到 30 呢？ 或者，现在告诉你大杯子是 60，小杯子是 40，我们能不能得到 70 的水呢？假设我们这里有 5 个杯子，一个杯子是 3，一个杯子是 7，一个杯子是 13，一个杯子是 17，然后还有一个杯子是 25，我们如何得到 18？ 是不是很奇怪？想问解决这个问题有什么意义？这个问题其实代表了很多所谓的需要推理的智力题。就是今天要跟大家说的，是一种通用的方法。 那么我们再思考一下，我们再变个问题。假如说两边中间有条河，有有五个人，现在是晚上，有一个手电筒，要把这五个人从左边全部运到右边。现在想让计算机告诉我们一次过。 我们标好 ABCDE，第一次把 a 和 b 过去，然后呢 a 再过来，再把 c 接回去等等。让计算机解决这样的问题。 再来一个经典的问题，传教士和食人族。就是有一群传教士带了一群食人族的人，他们要过一条小河，左边和右边必须满足一种情况，传教士的人数必须大于等于食人族的人数，否则食人族就会把传教士给吃了，船上也是。 传教士的人数在岸边和船上都大于等于食人族的人数，要不然就会被吃了。那问你，如果现在给你 n 个食人族 m 个传教士，你怎么样制定一个策略把它移过去？ 现在的问题是，我们要让计算机来解决这两个问题。这种问题就是典型的 State based model。 这几个问题，看起来都有类似的过程。都是我们现在在一个情况下，要达到另外的一个情况下，我怎么样到达？我们现在要知道的是它的路径是什么。就这几个问题，共同点都是要知道路径是什么。 那我们来分析一下，怎么解决这个问题。 如果大家想到一个数学方法来解决这非常的好，但是我想告诉大家，我们现在是学习人工智能的课程，以后遇到这种的问题的思考方向一定是你要想一种办法让机器自动能解决。就是你要想的是机器自动解决这个问题的方法，而不是我们要花心思去找到这个答案。 这个是你和一个参加奥数的学生的区别，我们要让机器自动解决这个问题，这是咱们学习这个的工作。 我们来看看咱们怎么样解决这个问题。 这个问题抽象一下就是我们在一种状态 a 下，要达到另外一种状态 b。我们期望能解决问题现在最简单的就是，首先我们需要用一种方法来表示状态，如果是两杯水的问题，可以用一个二元数组 (B1, B2) 来表示现在这个状态。 假如说我们这里是 (0,0), 要变到 (x,6) 或者 (6, x)。 这个就和下棋，走地图一摸一样。如果从 a 这一步开始能够知道下一步有哪些状态，然后在下一步我们又知道再下一步有哪些状态。我们继续这样扩展下去，机器就可以找到一点。如果我们让机器去把这个图做个遍历，就能从 a 点找到 b 点。 倒水问题和下棋、地图，以及我们做决策都很类似。都是有一个状态 之后，要看到接下来的状态是哪些，再接下来之后再去思考一步一步的。 现在来看一下，如果对于任意的一个状态 (x,y) 我们已知，已知这两个杯子的大小是 (X,Y)，对于任意一个状态 (x,y)。它会有哪些状态呢？ 第一个状态可能是 (0, y)，也就是 x 清零。 第二个状态可以是 (x,0)。 也就是就是我从 (X,Y) 可以到 (0,y) 和 (x,0) 这两种状态。 第三种还可以把 X 灌满，就是 (X,y)。 第四种可以把 y 灌满，就是 (x,Y). 那第一种就是把 x 清空了，第二种就是把 y 清空了，第三种是把 x 灌满，第四种是把 y 灌满。 除了这几个状态之外，还有那些状态我们没有说？我们没说互相倒水的情况。 那第五个状态就是 (0, x+y)，也就是将 x 的水倒入 y 中。不知道大家发觉没有，这第五个状态有个坑，把 x 倒给 y，能这样做是因为 x+y 是小于等于 Y 才可以。如果不是的，这里其实就变成了 (x+y-Y, y)。 同理第六个状态是我们把 y 倒给 x，也就是 (x+y,0)，需要 x+y &lt;= x，如果不是的，这里把 X 倒满了，y 的状态就应该是 x+y-X，那就变成 (X, x+y-X)。 每一个状态就和走象棋很像，每个象棋后边都有一个状态。坐车也是一样，每到一站下一步有不同的路线。 在这里，其实就是每个状态会接 6 个状态。然后让机器自动去找一遍就可以了。 那咱们来解决一下这个问题，把它实现一下。 1234567891011121314def successors(x, y, X, Y): return{ (x, 0): '倒空 y', (0, y): '倒空 x', (x, Y): '装满 y', (X, y): '装满 x', (0, x+y) if x+y &lt;= Y else (x+y-Y, Y): 'x =&gt; y', (x+y, 0) if x+y &lt;= X else (X, x+y-x): 'y =&gt; x', }print(successors(30,40,90,40))---{(30, 0): '倒空 y', (0, 40): '倒空 x', (30, 40): 'x =&gt; y', (90, 40): '装满 x', (70, 0): 'y =&gt; x'} 这样，我们就写出了返回状态的一个方法。其中 x,y 是当前杯子的状态，X，Y 是我们已知的杯子状态，也就是最大可装多少水。 然后我们将当前的 x，y 设置为 (30,40)，然后我们就可以输出在这种情况下会有怎样的一些状态。 我们就能够找到它的解法了，就能够找到它的答案了。也知道我们需要写一个程序搜索。 我们来看一下这个问题的输入，输入分别是capacity1，capacity2，表示两个杯子的容量。然后还有期望得到的目标goal, 以及它初始状态时候的值start, 如果不专门的声明，就给他默认成是 0。 如果目标已经在初始状态了，那我们就把这个初始状态返回就行了，这是最简单的一种状态。 1if goal in start: return [start] 如果这个目标没有在初始状态下，我们在定义一个 set，这个是表示我们所有探测过的点。 1explored = set() 然后，我们还有一个 paths，这个 paths 就是我们现在已经观测到的所有的点，已经观测到的所有路线。现在有一个 start 状态，然后要去给他扩展很多路径。 1paths = [['init', start]] 现在就定义一个变量 paths，这个 paths 就是指各种各样的路径，但现在只有一条路，现在只知道一个初始节点，这个初始节点就是我们的 start。 那么有了这个之后，我们发现还有路径需要去扩展，咱们现在就取出来这个路径，在这些所有的路线里面取出来一条路。 这条路最后的状态就是这条路的末端，表示的是此时此刻的状态 大家想象一下这个探索的过程，这个路假如已经探索了很多次了，采取了最前面这条路，这条路里面的 -1 就是现在的这个状态。 123while paths: path = paths.pop(0) (x, y) = path[-1][-1] x 和 y 现在是把最末端的状态拿出来了。这个时候，我们刚刚写的函数就有用了： 12for state, action in successors(x, y, capacity1, capacity2).items(): pass 我们有了此时此刻的 (x,y), 还知道这个杯子的 capacity，这段代码就让我们知道最末端的 (x, y) 接下来的状态了。 那么，如果state在explored中，这个点我们已经探索过了，那么就continue。 1if state in explored: continue 如果不这样做，则可能造成前面探索过的点包含下一步的状态中，就会造成形成一个环，从而无限循环。所以我们要把每一个探索过的点加到explored里边。这样我们才可以不兜圈子。 1explored.add((x,y)) 如果它在 explored 里边我们就跳过这个循环。它不在这个里边，我们就先定义一个新的 path: new_path，这个新的 path 就等于之前的老的 path 再加上 action。 1new_path = path + [(action, state)] 这个 action 和 state，就是我们刚刚写的那个求接下来的这个 action 和 state。 那现在，如果我们的目标在 state 里边，我们就把现在这个new_path给它返回出来。如果不在，那这个 paths 里边，探索路线里面，就再给它加上一个现在的这个路线。 1234if goal in state: return new_pathelse: paths.append(new_path) 最后返回一个空，就说没有这个路径。 这个问题乍一看要用计算机编程的方法去解决会觉得比较复杂，但是我们写完整个也就这么多： 1234567891011121314151617181920212223def search_solution(capacity1, capacity2, goal, start=(0,0)): if goal in start: return [start] explored = set() paths = [[('init', start)]] while paths: path = paths.pop(0) (x, y) = path[-1][-1] for (state, action) in successors(x, y, capacity1, capacity2).items(): if state in explored: continue new_path = path + [(action, state)] if goal in state: return new_path else: paths.append(new_path) explored.add(state) return [] 下面呢，我们来求解一下。假如我们定义两个杯子，第一个杯子是 9，第二个杯子是 4，我们的目标刚开始是 5，start 是空的。来看看我们该怎么解决： 12345if __name__ == '__main__': print(search_solution(9,4,5))---[('init', (0, 0)), ('装满 x', (9, 0)), ('x =&gt; y', (5, 4))] 我们来修改一下这一段代码，让其看着跟清晰一点，我们将步骤和结果分别取出来并且进行打印： 123456789solutin = search_solution(9, 4, 5)for i, s in enumerate(solution): print('step:{},{}'.format(i,s))---step:0,('init', (0, 0))step:1,('装满 x', (9, 0))step:2,('x =&gt; y', (5, 4)) step0 刚开始是 (0,0)，然后第一步装满 x，第二步把 x 倒给 y，把 y 装满。因为我们知道 y 是 4，所以就可以得到容量是 5 的水。 那我们现在再来看一下，如果我们现在把这个问题再换一下，我们还是一样 (9,4)，假如求解是 6，如果无解的话会返回一个空，不过最终我们还是得到了解，虽然步骤比较多。 123456789101112solution = search_solution(9, 4, 6)---step:0,('init', (0, 0))step:1,('装满 x', (9, 0))step:2,('x =&gt; y', (5, 4))step:3,('倒空 y', (5, 0))step:4,('x =&gt; y', (1, 4))step:5,('倒空 y', (1, 0))step:6,('x =&gt; y', (0, 1))step:7,('装满 x', (9, 1))step:8,('x =&gt; y', (6, 4)) 那我们每次要求一个新解，都需要重新定义一遍solution，这样十分不方便，那我们现在就需要将其修改成一个函数，方便我们随意传入不同的值： 12345def get_solution(c1, c2, goal, start=(0, 0)): solution = search_solution(c1, c2, goal, start) for i, s in enumerate(solution): print('step:{},{}'.format(i,s)) 接着，我们如果求一个.5，就返回一个空。 1get_solution(9, 4, 6.5) 返回空的就就是没有路径。 我们再回过头看看刚才的求解 (9, 4, 6), 这个时候已经能够解决这个问题了。它不是经过了复杂的数学运算得出来的，是自己找到的结果。那完整的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243def successors(x, y, X, Y): return{ (x, 0): '倒空 y', (0, y): '倒空 x', (x, Y): '装满 y', (X, y): '装满 x', (0, x+y) if x+y &lt;= Y else (x+y-Y, Y): 'x =&gt; y', (x+y, 0) if x+y &lt;= X else (X, x+y-x): 'y =&gt; x', }def search_solution(capacity1, capacity2, goal, start=(0,0)): if goal in start: return [start] explored = set() paths = [[('init', start)]] while paths: path = paths.pop(0) (x, y) = path[-1][-1] for (state, action) in successors(x, y, capacity1, capacity2).items(): if state in explored: continue new_path = path + [(action, state)] if goal in state: return new_path else: paths.append(new_path) explored.add(state) return []def get_solution(c1, c2, goal, start=(0, 0)): solution = search_solution(c1, c2, goal, start) for i, s in enumerate(solution): print('step:{},{}'.format(i,s))if __name__ == '__main__': get_solution(9, 4, 6.5) 就这段代码，虽然并不长，但是难度稍微有点大，你们在看完之后最好是多敲几遍。 这就是人们当时讨论 AI 程序的作用，人们不希望每次都自己去解决，希望能够让程序解决。 我们来试试 7： 123456789101112131415161718get_solution(9, 4, 7)---step:0,('init', (0, 0))step:1,('装满 x', (9, 0))step:2,('x =&gt; y', (5, 4))step:3,('倒空 y', (5, 0))step:4,('x =&gt; y', (1, 4))step:5,('倒空 y', (1, 0))step:6,('x =&gt; y', (0, 1))step:7,('装满 x', (9, 1))step:8,('x =&gt; y', (6, 4))step:9,('倒空 y', (6, 0))step:10,('x =&gt; y', (2, 4))step:11,('倒空 y', (2, 0))step:12,('x =&gt; y', (0, 2))step:13,('装满 x', (9, 2))step:14,('x =&gt; y', (7, 4)) 7 也是可以的，给一个 9 的杯子和一个 4 的杯子，要得到 7 升水也是可以的。只不过过程比较复杂，那如果现在给你人工来解，你觉得 7 你可以吗？ 假如要让你的最短路径优先，你可以在paths.append(new_path)这里做一些优化，就是减枝、排序等等。 7这个问题乍一看感觉是不行的，完不成。但是这倒腾来倒腾去竟然可以，好神奇对不对？ 回头我给大家说一个比较容易误解的地方： 1(x, y) = path[-1][-1] 就这段代码解释一下，这个[-1]是你找到一条路径的最后一个节点，并不是值为 -1, 因为我们最后一个节点存了两个东西，一个是他的action，一个是他的状态。所以这个就是他最后那个节点的状态。 这个问题的特点在于咱们从始至终都没有去写数学方程或者找规律，然后求解，而是让程序自动解决的。 同样问题，像刚才传教士和食人族过河，还有那个手电筒、下棋的问题。其实你会发现都是类似的问题。 从研究对象来讲 接着，我们再从最后一个维度上来区分一下人工智能的这个问题，它有什么区别。 从他研究的对象上来看，首先，会有一群人在研究我们怎么样能够创建解决 AI 的问题和方法，就是研究如何创建能够解决 AI 问题的方法与模型。 有一部分人是创造模型和方法的人，但还有一些是研究如何使用这些模型和方法的人。 那以上两者，这些人都是 AI 工作者。有些同学可能是发明算法的，有些同学可能是使用算法的。 因为用现有的方法去解决问题，其实也是很难的一个事情。 好，那么本节课就到这里，这节课还是有一些难度的，希望大家能好好的消化一边，课内代码多敲几遍去理解。 下节课就没这么难了，也就吹吹牛逼聊聊天就过了。不过大家要做好准备，下一节课是咱们最后一节轻松的课程了，之后的课程将会... 大家做好心理准备吧。 好，我是茶桁，咱们下节课见。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/03.%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A0%B8%E5%BF%83%E5%9F%BA%E7%A1%80%20-%20%E5%AF%BC%E8%AE%BA2/"},{"title":"04. 人工智能核心基础 - 导论（3）","text":"[TOC] Hi，你好。我是茶桁。 基于上一节课咱们的整体强度有点大，而且咱们马上也要进入高强度内容了，那么这一篇咱们就稍微水一篇吧。来聊聊天，讲讲学习人工智能的一些事项。 人工智能和其他学科的关系 经常有同学会问我：人工智能和其他学科有什么关系？有什么区别？ 有人说人工智能就是博弈论、梯度下降，人工智能就是贝叶斯。 这都不对，大家要这样想：人工智能是我们刚刚说的解决各种各样的问题的一个研究课题。你可以把它想象成研究课题，这些数学方法，统计学方法是我们来解决这个问题所用到的方法。 千万不要把自己变成一个数学系的人，数学是你的工具。搞人工智能的人，绝对不是一个纯数学家。 比方说贝叶斯定理，其实在一七几几年就提出来了，以前一直被广泛应用于概率学。再比如最小二乘法，也是很早之前就提出来，最早用在天文学上；一阶逻辑是被用在逻辑学中；最大似然也是被用在统计学中；我们的神经网络最早是在神经科学中提出来的；MiniMax game，解决下棋问题的时候经常会用到的一个方法，其实最早是经济学里面的一个问题，博弈论用到的问题。 梯度下降最早叫做 optimization research，是运筹学里面的一个内容，也就是我上一节课中讲到的 search 方法。最早是起源于算法与数据结构； 之后还有强化学习，它其实是指的迭代，是控制论当中的东西。 所以你看到这些就应该知道，人工智能与其他学科一个很大的区别，它是博采众长，为了解决问题，可以使用各种各样的方法。 AI 是融汇了各个学科用来解决某种问题的一个学科。所以不要把它局限在某个东西，不管是啥只要能搞定问题都可以。只要能像人一样，要解决的问题能解决就可以。 为什么学习人工智能 给大家讲了这么多人工智能是什么，还讲了两个很典型的案例，我来讲一下为什么要学习人工智能。之后的课程中可以感受一下，这就和小马过河一样，代码其实不难，你会发现代码其实都挺短的，一二十行、二三十行。发现困难的时候正好是你能力提升的时候。 特斯拉、iPhone、抖音、淘宝这些的成功其实都是因为我们处在一个数字智能时代。这个数字智能时代所有东西几乎都要变成数字化的东西，大家要做的时候希望能够自动化的去处理这些东西，AI 就会因此而生。 人工智能是一种力量，是一种能够使用这种数据能源的力量。 现在有很多很多的数据，你要把这些数据解决，人工智能就是这样的一个力量来解决他们。而且我们还能够实现职业的可持续成长，从薪资待遇上、发展的瓶颈上都会好过其他的场景。 比方说做传统的开发岗位可能会面临 35 岁下岗，人工智能这个行业整体上就要好很多。 怎么学好人工智能？ 怎么样去学好 AI 呢？ 上一节课中，咱们演示两段代码，讲了两个问题，有没有觉得好难？咱们的整个课程内容量其实很大，尤其是到咱们之后的课程。 要学好人工智能，首先你要知道 AI 它之所以工资高一定不简单，但是既然有那么多人都在做，也就证明它一定是能学会的。所以你不要觉得它会特别难，但也不要看成好像听一听就可以会了。 看我课程的这些同学们，我要郑重的说一下，不要指望在地铁上啊什么的碎片时间看看就可以学会了，这必须是不可能的。我一直想跟大家说，碎片化时间是绝对学不好的，不管是什么东西，你背单词可能还行，但是如果是一个系统性很强的学科，想要碎片化时间学习，这几乎是在害自己。 人工智能永远是一个高级职位，第一个点就是要有心力，要相信你能学好，能学会。 第二点，编程能力一定要强，有很多时候其实理论很简单，但是你就是实现不出来。编程能力一定要多多练习。 第三点是数学能力，我们很多问题其实都是要抽象成数学问题，但和数学的区别是我们还要把这个数学问题变成计算机能运行的，这个就更难了。 这也就是为什么我前面要花 2 个月的时间来写两篇专题，一篇Python 基础，一篇数学基础，都是为了让大家能提高基础能力，应付之后的课程。 还有第四点，建模能力，你要把你见到的新问题变成你学过的模型。建模能力其实是这个行业中非常重要一个能力，是你需要不断的去练习的能力。 再有就是交流能力，为什么说这个重要，作为一个算法工程师，和做 Java，做 Web 前端开发的区别是什么？算法工程师永远做不到 100% 正确，永远会有一些 bad case，永远会有一些做的不好的地方。 这个时候如何去沟通让别人知道，这件事情暂时只能做成这样，或者如果我们要做的更好需要哪些数据，需要哪些支持。这些都是需要交流能力的。 建模能力是接下来课程会教给大家的，遇到一个问题我都会讲。那我的这个课程和其他课程最大的区别，就是很多课程都是告诉大家这有个方法你把它记住，而我是给大家讲为什么提出来这样的方法。 我把思考过程告诉大家之后，建模能力就会逐步提升。建模能力就是把看到的问题变成解决方法的一个能力。 再接下来第五点，我们的质疑能力。大家要学好，一定要有质疑能力。就是你现在看到的所有论文，你记住都有可能是错的。也不说都有可能绝对是错的，但肯定在某些场景下是不能运行的，结果很糟糕。 一定要知道这一点，知道这一点之后才能去提出新的方法，才能不断的前进。 以上这几种能力非常重要。 学习过程中，肯定会遇到各种各样的问题，当你遇到问题的时候，该怎么办呢？ 如果遇到相关的名词术语、方程、公式或者代码看不懂的时候该怎么办呢？ 不要放弃继续，也不要立马找人问，更不要怀疑自己。你应该去 Google，维基百科等高质量的搜索引擎去搜索，自己学会去查找相关的问题解决方法。这里我要着重说一下，当然可以借助 ChatGPT 等人工智能，不过不要过分依赖，别想都没想，还没思考就赶紧去问，有的时候 GPT 给你的答案不一定准确，而因为你没有去思考过，可能就这样全盘接收了，那就走上歧路了。 那如果感到代码困难，做不出来，也是，不要轻易放弃和怀疑自己，要想尽一切办法，比如去 Stack Overflow, Github 等等上找找相似的问题代码。 那当然有感到困难的就有觉得过于简单的，觉得简单也不要放弃继续往后学，你可以跳过，不过终归还是有内容是你期望了解的。 那我现在这门课程和市面上大部分的人工智能都不相同，那些课程实际上就是跟风，将一些已经通用的解决方案整合一下打包给你，卖你个几千几万的。主打的就是一个信息差。而我现在给大家讲的，是思维，是基础，是如何去理解人工智能，属于底层建设。 AI 的岗位动不动四五十万、五六十万年薪。那其实面向的也是工程师级别，而直接拿到解决方案去凑数的人，终究最后还是会被机器淘汰。 先把能力练起来，不要让自己很被动。我要给大家编撰的这门课程，首先收获是能力，就是常见的人工智能问题自己能够提出解决方案，并且自己写代码实现。 第二，潜力。新出现的人工智能模型能够理解，而且能够自学最先进的模型与代码，这是潜力。 按照我的理解，如果大家学到这门课程，应该会是你最后一次付费的 AI 课程，我期望达到这样的水平。 你以后不要去再去学习这些了，已经具备了继续学习的潜力，这门课程可以培养出来至少符合年薪 25 万及以上公司的 ASM 岗位要求。 在这个过程中，大家其实增强了适应性，能够在这个变化的世界中更加的适应。 如果你有前面这些能力的话，记得要多多使用你自己的 Github 账号，如果你没有一定要注册一个。课程上的练习代码，作业全部都同步上去。 第二个，用你的 Linkedin，打造你的个人主页，把你的项目能力写上去。不要在 58 同城上工作，到 Linkedin 上寻找机会，基本上都是那种比较高级的职位。国内外这些大厂，都在上面去找人。 另外，你自己要能够在你的生活中，公司中，自己的实验室环境中找到能解决的项目。自己解决了之后，那么你下次要转行，换工作的时候别人这些项目就能给你加很多分。 一些问题 最后呢，给大家留下一些思考题。 你学习这门课的目的是什么？希望达到什么结果？ 你达到的期望结果，有什么需要克服的苦难/劣势？ 你达到的期望结果，有什么可以发挥的长处/优势？ 你认为课程有什么地方可以改进？ 再思考下下面几个问题： - 人工智能、机器学习和深度之间有什么关系和异同？ - 人工智能和数学、算法与数据结构有什么关系？ - 你能给出 5 个人工智能实例，这些实例是使用了哪种方法论、范式吗？ - 你能复现出我们上一节课上的代码实例吗？ 好，那我们本节课就到这里了，也到了假期了。大家好好休息，放个长假，之后，咱们下一节课要开始加强强度了。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/04.%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A0%B8%E5%BF%83%E5%9F%BA%E7%A1%80%20-%20%E5%AF%BC%E8%AE%BA3/"},{"title":"05. 机器学习入门 - 动态规划","text":"[TOC] Hi, 你好。我是茶桁。 咱们之前的课程就给大家讲了什么是人工智能，也说了每个人的定义都不太一样。关于人工智能的不同观点和方法，其实是一个很复杂的领域，我们无法用一个或者两个概念确定什么是人工智能，无法具体化。 我也是要给大家讲两个重要的概念，要成为一个良好的 AI 工作者，需要了解两个概念，一个是什么是优化问题，第二个呢就是什么是继续学习。 这一节开始，我们要开始进入机器学习的入门，这一部分只是机器学习的初级部分，我将其分成了三个部分，分别是： 如何衡量向量的相似性（metric for tensor) k-means 算法进行聚类 线性回归与梯度下降 但是整体课程的排程并不是严格按照这三个部分来排的，所以大家能看到，我本篇课程的标题也并不是「如何衡量向量的相似性（metric for tensor)」。以上三个部分仅仅是我们当前这一部分会讲到的内容，但是也是拆散了放入课程中的。 从一个案例开始 那这节课最初，让我们从一个实际案例来引入，开始我们的深度学习初级之旅。 那要给大家讲的第一个问题，我们叫做 optimization, 也叫做优化问题。那这里，我们拿一个实际的项目来。 我们所在的城市，可能很多人会经常看到运钞车。这个运钞车其实也是银行花钱雇的，运钞车其实也不是银行的。 每次要使用运钞车的时候是需需要花钱，还比较贵。这样就会带来一个结果，专门有人来策划运钞车的运行路径。为什么要策划计划运钞车的运行路径呢？因为如果我们有一台 ATM 机，ATM 机满了，那么面对的一个问题就是别人存不进去钱了， 另外很重要一点是，如果一个 ATM 机满了而且没有把钱取出来，那这个钱就相当于是浪费了。加入一个 ATM 机能存一百万，那晚拿出来一天、两天，对银行来说损失就比较大。 还有一种情况就是这个 ATM 机空了，那客户去取钱的时候也取不到钱，也就起不到 ATM 机的作用。而银行对 ATM 机所在地的房租也就相当于白费了。 试想一下，如果同时有两张卡，一张农行卡，一张工行的卡，在农行准备想取钱的时候取不出来，就到工行去取了。而每次农行都取不出来，那之后就用工行用的多了。 在这样的情况下，一个城市有很多个点，就需要有很多个运钞车。 比方说在城市中有很多个 ATM 机的站点，现在我们有 k 个运钞车。 我们要策划一条线路，这个线路要使得所有的车，每个车每个点只走一遍，还要回到他出发的地方，而且这 k 个车运行的时间加起来是最小的。 那怎么样计划出这样一条路径呢？就这是一个非常典型的优化问题。对于此类的问题其实还有很多很相似的。 比如对于一家公司而言，手里的钱是有限的。如何把这些金额分配到不同的项目组中； 另外一个比方就是，对于一个公司来说如何选取合理的仓库的存货点，还有哪些仓库放哪些商品，能够让运输的车辆花的时间最少，能快速的去把这货物运输到不同的地方；这种物流问题是我最喜欢的问题之一。 或者我们制造一部手机，我们能花的钱就这么多，那怎么样能够在我们可以花的这个金额的范围内，如何选取最合理的元器件成本，让我们的手机达到最大的利润； 有很多约束条件，很多约束我们做决策的东西，总量不能大于多少。比方电的成本，水的成本等等，都要满足一定的关系才可以。这种东西我们就把它叫做约束条件。 要解决在约束条件下达成目标的这个问题，我们就把它叫做优化问题。 优化问题就是，我们可能会有不少的函数，这些函数去限定了之间的一种关系，也就是函数之间可能会有一些约束条件。比如图中的 g_i(x)。 假设我们要造很多仓库，仓库加起来所花费的成本是什么样的，花费的人力是什么样的。 最后要优化出来一个最小值就是我们的最小花费，或者所需要最少的时间。这种问题其实广泛存在于我们各个地方。 你点外卖每天在地图上做一个地图的规划，公司里做一笔投资，其实都是使用了类似这样的东西。 动态规划 我们要解决优化问题，其实有比较多的方法，今天来介绍最著名的一种，也是可以说是最重要的一种，动态规划的思想。 动态规划是一个什么样的情况？以这个问题引入一下。 想象一下有这么一个工厂，这个工厂是卖木头的，长度是一米的木头卖一块钱，长度 2 米的卖 5 块钱。 那么所以除非拿了一个一米的木头，否则不会有一个人像傻子一样说：把这个两米的木头截成两段。 图中我们可以看到，3 米的就卖 8 块等等，到后面 10 米的卖 30，11 米的卖 33。 那么现在你拿到了一个长度为 8 的一个木头，那你想想这个 8 米的木头是该直接卖，还是它切分掉去卖。切分掉去卖的话，如果用 1 和 7，加起来就是 18 块钱， 2 和 6 加起来就 22 块钱了，比 8 是不是就多了？ 给你任意的一个长度 n，然后我们要得出来这个 n 到底该怎么切分能够使得价格最大。怎么样能够让卖出去的价格最大。 思考一下咱们怎么解决这个问题。计算机有一个很很简单的方法，就是计算机有一个非常好的优点就是它可以做穷举。你可以让他去找什么，让他把所有东西全部都运行一遍就可以了。这是计算机的一个好处。 我们来思考一下咱们怎么样能够解决这个问题呢？ 我们现在输入了一个长度是 n 的一个木头。它可以变成 n 和 0，就是不切分。可以变成 1 和 n-1, 变成 2 和 n-2。一直截断到多少呢？可以变成 n-n/2。 (0,n),(1, n-1),(2, n-2)...(n-n/2, n-n/2) 在这个情况下，它分成 1 和 n-1, n-1 也面临了类似的问题。n-1 是你直接返回 n-1 呢，还是变成 (1, n-2)，变成 (2, n-3)。对于 2，其实也有也会面临这样一个问题，是返回 2 呢，还是 (2-1, 2-1)。如果我们要遍历的话，遍历的就应该是一个树。把这个树里面所有情况都找一遍，然后返回那个最大值就可以了。 我们可以把所有的情况循环一遍，那我们现在来实现一下，你会发现其实一点也不难。 让我们来先定义一下所有的价格： 1prices = [1, 5, 8, 9, 10, 17, 17, 20, 24, 30 , 33] 我们要知道它的每一个商品直接的价钱，就是 complete，我们来定义一个complete_price, 12345complete_price = { i+1: p for i, p in enumerate(prices)}print(complete_price[9])---24 那如果这个时候是 30，就会出问题。 1234print(complete_price[30])---KeyError: 30 遇到这种问题的时候不要非要去判断这个在不在里边，可以直接用detaultdict, 它是是一个带有默认值的 Dictionary，如果这个值不存在的话，给它赋予一个默认值。 12345678from collections import defaultdictcomplete_price = defaultdict(int)for i, p in enumerate(prices): complete_price[i+1] = pprint(complete_price[9])print(complete_price[30]) 这个时候如果是 30, 它就有一个结果了。现在有了这样的一个基础数据，我们假如要写一个 revenue,revenue 就是营收，输入长度是 n。 现在做一个很简单的方法： 12def r(n): candidates = [complete_price[n]] candidates 等于价钱完全不切割是多少钱。然后我们写一个 for 循环： 12for i in range(1, n): pass 接下来我们做个拆分，拆分左边就等于 i, right 就等于 n-i: 12left = iright = n-i 这个时候整体的价格就等于complete_price[left], 再加完整的右边，complete_price[right]。 1total_r = complete_price[left] + complete_price[right] 这里其实是有问题的，如果我们在这里假设 n 现在是 100, 假设运行到 i 是 50, 那么complete_price[50] 其实是一个没有值的，值是 0。 就是数据里没有 50 的长度的价格，所以这个左边和右边其实是需要变成一个递归问题。 12total_r = r(left) + r(right)candidates.append(total_r) 现在写法写的稍微巧妙一些，这个就是我们刚才所说的遍历一遍，把所有的情况给拿出来，找到最大的返回出来。目前为止这个方法完整的代码如下： 1234567891011def r(n): candidates = [complete_price[n]] for i in range(1, n): left = i right = n-i total_r = r(left) + r(right) candidates.append(total_r) return max(candidates) 这个时候我们来调用一下： 1234print(r(8))---22 这个 22 是怎么来的呢？我们来看一下上面这段代码，首先 8 进方法之后，第 2 行告诉我们 8 完全不切割的话是多少，并赋值给candidates。 紧接着把它变成了 1 和 7，2 和 6，3 和 5，4 和 4 这样的问题。 假如到 6 的时候，又会去求一下这个 6。右边是要变成 6，那么 6 最大应该是多少。 这个代码其实可以写成更加 Python 化的方式，我先写一遍上面的，是希望能让大家知道这个代码是怎么来的，现在让我们来用 Python 的方式来解决： 12345678# 优化为 Python 向def r(n): return max([complete_price[n]] + [r(i) + r(n-i) for i in range(1, n)])print(r(8))---22 也就是，以上方法里定义的内容，完全可以压缩成一句话解决。你们可以仔细的研究一下这两段的相同点和差别。 现在的问题就是我们缺了一个记录他中间分割步骤的内容，这个也简单，我们稍微改变一下代码，将 return 改为赋值，然后给他加一个分割方法： 1candidates = [(complete_price[n], (n, 0))] + [(r(i) + r(n-i), (i, n-i)) for i in range(1, n)] 其中我们添加了 (n, 0), 还有 (i,n-i)。 然后我们还要给他加上最优值： 1optimal_price, split = max(candidates) 如果这其中你要是看不出逻辑过程，那你需要一些更多的练习。好好的再去回去看看我之前写的 AI 秘籍中的 Python 基础教程篇。 对于熟练的 Python 的工程师来说，应该熟悉这种方式才对。第一次咱们实现的方法，其实是 C 和 Java 的一种方法。如果是 Python 的话，你需要熟悉 Python，直接会写成这个样子。 我们定义了一个candidates, 后面将风格方式定义进去并赋值给它。 然后我们使用optimal_price来接收它的最优价格。 接着，我们添加一个solution变量，solution是 n 的时候就找到了它的分割过程。 12345678# 在方法外定义一个 solutionsolution = {}# 在方法内赋值 solution[n]def r(n): ... solution[n] = split 最后，我们还是将最优价格给返回出去。 1return optimal_price 来看一下solution[8] 1234print(solution[8])---(6, 2) 我们如果在这里 debug 一下这个 solution，运行完之后 solution 如下： 是 8 的情况下变成 6 和 2，现在就要看一下 6 的时候怎么切分？6 的时候变成 6 和 0。 好，现在我们来看一个更复杂的问题，我们将它变成 33, 33 你会发现他运行的时间很久。刚刚我们运行 8，或者我们重新运行 9 的时候，速度都还可以，很快就能得到结果。但是运行 33 的时候就会非常的缓慢。知道这是为什么吗？ 在这段代码中，现在有一个 n，n 接下来会拆分成了 n-1 个问题，其实是要进行 n-1 个循环。那每个 n-1 的循环里面又有两个调用了这个运算。所以在整个计算的次数的复杂性，对于一个 n 来说，它整个的运行时间应该是：2 * (n-1) * 2 * (n-2) * ... = 2^n * n!，就等于 2 的 n 次方再乘上 n 的阶乘。 这样的话，结果就是它的运行时间会非常非常的久。那我们就需要对代码进行优化了。 如果现在仔细分析一下的话，会发现之所以运行的慢，问题在于很多重复的值其实被重复运算了。同样是 n-3 这个事，不仅在 n-1 向下分支中会计算一次，在其他的分支也会再计算很多次。有很多的值是被重复运算了。 为了解决重复运算的问题，我们可以做一个非常简单的方法。既然很多计算是重复的，那我定义一个变量去记录这些曾经计算过的，再遇到的时候就不要去计算不就完了： 12345678910111213cache = {}def r(n): if n in cache: return cache[n] ... cache[n] = optimal_price return optimal_priceprint(r(33))---99 如果 n 在 cache 里面，直接return cache[n], 如果它不在里边的话就往下继续运行，每次求完解的之后，我们让cache[n]等于optimal_price。这样就简单多了。 我们最后求了一下 33，瞬间就得到了 99 的答案。 这个 cache 其实是很简单的一个东西，但是它其实是很重要的一种思想。在一九二几年、三几年的时候，当时有一个数学家叫 Richard Bellman。Bellman 发现在整个数学计算中有相当的一类问题，都牵扯到了一个相似的问题：over-lapping sub-problem，就是子问题的重复。 Bellman 就提出来了一种方法，他就说我们解决这种问题其实有一个很简单的方法，用一张表格，不断地记录不断地查表，就是不断地写表查表。 就把值和结果都一一对应的写入表中，我们发现问题的时候，在这个表里面重新查一遍，看一下有没有这个值，存不存在。 当时 Bellman 把这种方法叫做 Dynamic，不断地、持续地、变化的、动态的。programming 在我们现在意思是编程，在一九二几、一九三几年的时候，是指把东西写到表格里以及从表格里面拿出来。 那为什么后来演化成编程的意思，因为早些时候，冯诺伊曼当年制作的那个机器写的是纸带，就是给你一个一个的纸带，这个纸带每一行会打八个洞，就是当时冯诺伊曼最早的计算机。这 8 个洞里边有一些是透光的，有一些是不透光的，其实就是一条一条计算机命令。 这其实也是一个写表读表的过程，后来 programming 就有了编程的意思。但是 Bellman 当年提出来的这个意思，Dandep programming 就是不断地写表和查表。它针对的是所有一切有这个over-lapping sub-problem的问题，都可以用这种简单的方法，可以大幅度的减少计算性。 在很多地方学动态规划的时候，很多书上教动态规划的时候往往是直接给一个解法。不知道您有没有看过那些算法的书，在动态规划里往往会有一个表，这个表格很重要。 如果不用动态规划的话，这个问题也是能解决的。世界上几乎所有的这种优化问题不用动态规划都是可以解决的。但是理论上是要给你一个运算能力无限强，存储空间无限大的计算设备。 显然不太可能，所以我们就需要这样写到一个表格里边，记一个记录表格。这个就是我们动态规划的核心思想。 关于「动态规划的详解」之后有机会我去专门写个算法的相关课程，在这里就把动态规划的核心思想给大家介绍了。 我们现在来继续看代码，刚才我们算了一下 33 这个值，得到了 99。但现在我们知道了能卖多少钱，但是我们还不知道到底怎么个拆分法。 要拆分的话我们就要把这个solution给它 parse 一下，再继续定义一个方法： 1234567891011121314def parse_solution(n, cut_solution): left, right = cut_solution[n] if left == 0 or right == 0: return [left+right, ] else: return parse_solution(left, cut_solution) + parse_solution(right, cut_solution)print(r(18))print(parse_solution(18, solution))---52[10, 6, 2] 我们看一下 left 和 right，将分割分别传进来，那如果 left 和 right 是 0 的话，比如 10，写成 (10, 0)，其实意思就是 10 就不用切分了，直接返回 left 和 right 就可以了。 那如果说它里边不是 0，假如说是 37，切分 17 和 20，我们就要知道 17 该怎么分，20 该怎么分。所以就返回： 1return parse_solution(left, cut_solution) + parse_solution(right, cut_solution) 最后我们看到了 18 的切分结果，就被切分成[10, 6, 2]。 具体的切割过程也可以获得。 在 Python 里边呢这个是一个非常非常经典的动态规划问题。经过一个比较简单的方法，把重复问题给解决了。 再跟大家普及一个知识，在 Python 里面有一个好处就是它把很多常见的功能其实都已经想到，做了切分了。 比方说 Python 自带的库里面就有一个 functools, 它有一个lru_cache，就是 least recent used, 最近最少使用。 我们如果在直接写一个 cache，它会存在一个问题。当 n 特别大的时候，那么 cache 的 size 也会变得特别的大。这个时候我们就需要一种机制，来让 cache 保留最需要保留的东西，保留那些最重要的东西，不要把所有的信息全部弄进去。 这个lru_cache帮我们实现了这个功能，它会把最近最少使用过的这些值删去，cache 的这个 size 会保持在一个比较固定的范围内。 这个函数是一个装饰器，小伙伴们是否还记得我在 Python 课程中介绍过装饰器以及其使用？ 我们来使用一下这个装饰器： 1@lru_cache(maxsize=2**10) 这里定义了一下 maxsize 等于 2 的 10 次方，就是最多可以存 2 的 10 次方个值。 那这里和我们方法里的这一段内容其实是一摸一样： 123if n in cache: return cache[n]...cache[n] = optimal_price 当我们调用这个函数的时候，如果函数的参数曾经被计算过，那么就直接返回这个值。如果没有被计算过，就计算一遍，再把这个值写下来。 那我们使用了这个装饰器之后，我们对 cache 的使用的这两句代码就可以删掉了。 lru_cache的作用都懂了吧？以后遇到一个程序很慢的时候，就可以用它来做优化。 那其实今天这节课程把这个函数看懂，几乎所有的动态规划的问题的主体思路就都懂了。因为所有的动态规划的问题都包含了以下几个问题： 识别子问题 sub-problems dividing 识别子问题中的重叠特点 over-lapping sub-problem 存储子问题的答案 cache sub-solutions 合并问题答案 combine solutions 解析答案 parse solutions 这是所有的动态规划的特点，当你发现一个问题可以被分解成子问题，而且子问题有重复的时候，就可以用这种方法去优化它。 以后大家面试，大概率会遇到这种问题。只要按照这个方法来思考的话问题基本上就不大了。 但是动态规划其实也是有一个极限，曾经有一段时间人们以为动态规划可以解决很多问题，几乎可以解决所有常见的问题。但是后来人们发现，当限制条件特别多，或者问题已经复杂到非常难的去识别此问题了。可能它是包含了子问题的关系，但是因为这个问题太复杂了，所以我们没办法去把它分割出来，我们没有办法去识别它。 动态规划是一种思维方法，一种思维类型。比方计算机里面的图论问题，并不是只能把它变成图论问题，不用图论完全可以解决，但是不太好弄。 所以，动态规划其实也有它的局限性，人们为了解决更多问题就提出了更多的方法，其中一种方法就叫做机器学习。 好，我们下节课，就好好的来讲讲机器学习。本节课的最后，我将之前咱们写的那段代码完整版贴在这里： 123456789101112131415161718192021222324252627282930from collections import defaultdictfrom functools import lru_cacheprices = [1, 5, 8, 9, 10, 17, 17, 20, 24, 30 , 33]complete_price = defaultdict(int)for i, p in enumerate(prices): complete_price[i+1] = psolution = {}cache = {}@lru_cache(maxsize=2**10)def r(n): candidates = [(complete_price[n], (n, 0))] + [(r(i) + r(n-i), (i, n-i)) for i in range(1, n)] optimal_price, split = max(candidates) solution[n] = split return optimal_pricedef parse_solution(n, cut_solution): left, right = cut_solution[n] if left == 0 or right == 0: return [left+right, ] else: return parse_solution(left, cut_solution) + parse_solution(right, cut_solution)print(r(18))print(parse_solution(18, solution)) 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/05.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20-%20%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"title":"07. 机器学习入门 3 - 了解 K-means","text":"Hi，你好。我是茶桁。 我们在机器学习入门已经学习了两节课，分别接触了动态规划，机器学习的背景，特征向量以及梯度下降。 本节课，我们在深入的学习一点其他的知识，我们来看看 K-means. 当然，在本节课我们也只是浅尝即止，关于这些内容，后面我们还有更详细的内容等着我们去深入学习。 淘宝的商品问题 上节课的最后，我们学习的内容是梯度下降，在这里不得不再次强调一下，梯度下降是咱们以后非常非常重要的一个内容。 那关于今天要讲的 K-means 呢，我们还是按照惯例，用一个问题来引入。这个问题也是一个实际问题，并非乱举例。 在淘宝国际上经常会有一些人，基本是境外人员，会从国外出售违禁商品，国家是不许卖的，这些东西会被要求全部下架。 这些人就有一个很聪明的方法，他会更换物品的名字。 比方枪支又叫狗子，或者叫什么野狗。赌博账号叫米科，毒品账号可以叫野狼等等。 我们把这些话称为黑话、黑词。我们在明处、他们在暗处。整个淘宝假设现在就只知道十几个、二十几个人黑话，想屏蔽他们就很难屏蔽。 有人就说，我们可以像拦截垃圾邮件一样，我们可以去做记忆训练。 但是这个时候就有一个情况，你做那个垃圾邮件的时候会发现有很多很多的垃圾邮件可以供你训练，但是在互联网上，这种暗语可能有几千上万，但是在整个淘宝里边，他特别的少，比例特别低，其次经常还会变。也就是我们经常会说到的时效性。 后来人们想了一个这样的方法： 第一步选取部分淘宝的商品描述，第二步将文本向量化，也就是把一个一个的文本给它变成一个一个向量。接下来对文本进行聚类。 之前我们说过，聚类其实没有标准答案。我们就让它聚三类、聚四类、聚五类，可以规定聚多少类。把非常非常多的向量，距离接近的这些向量归为一类。所以对应出来把文本向量化之后，就是把文本接近的词归为同样一类。 然后我们按照已知的暗语定位商品的类别，获得该商品下词汇频率远高于正常词汇分布的单词。 比如我们知道“野狗”这个单词是一个黑词，首先让在淘宝里的这些商品进行自由分类，让文本相似的聚集到一块，然后我们看一下包含了”野狗“这个单词这个商品的那一大类里边哪些单词出现的频率比正常情况下出现的高。 就是说，如果这个人爱说黑话，那么他所说的别的话也更有可能是黑话。 假如说有一些街头帮派的人，他们爱说一些你听不懂的话，那么你听到了一句这样的话就找到了这样的这个人。把这一类人找到，再每天去监听一下他们说了什么话，他们说的有些话的频率其实比正常情况人群说的高，大概率就是黑话。 看这个过程，其实对应了我们上一节说说的，机器学习的基本框架。有观察的部分，有提取特征的部分，还有让机器进行聚类，让他去学习的过程。 这个时候我们又发现，让机器自动去聚类之后我们并没有用那个最终求解出来的 f。也就是我们并没有让它去预测新数据，是用老数据，然后让老数据去给它聚类，并没有让这个东西去预测新数据。 其实我说两个非常重要的工程经验。 第一个工程经验是，所谓算法工程师并不是说他记住了多少算法，而是他要能把一个实际问题抽象成一个算法问题。 就比方说淘宝的这个问题，如果没人告诉你的话，你并不知道他是一个聚类问题。另外一些人，就能想到可以用聚类这个方法去解决。这个其实才是最重要的。 我有很多同学，记住了很多但是不会应用，这就不行。 第二个重要的点是在真正的工作中，我们的机器学习方法很多时候是作为整个项目的一部分，单靠机器学习很难解决完整项目。 整个项目它可能很长，有十多步，其中有几步用到了机器学习。而且用到的还是机器学习的中间的一些部分。 假如我们现在选 10 万个淘宝的商品，对他进行了聚类。其实按照标准的机器学习过程，聚类完成之后得再学些新的商品，让现在这个模型对这些新的去进行分类。 但是我们没有进行这一步，我们把这 10 万给它分完类，自动聚完类之后就停了，就进行下面的工作了。这就说明机器学习这个完整的步骤完全可以只使用其中的某几个部分，而不是一定要把这个全程跑完。 这就是要做一个算法工程师很重要的点。 K-means 接下来，来看第一个机器学习算法问题。我们把第一个机器学习问题叫做 K-means。 如果给出了很多个点，给出了很多个向量。我们拿比较方便看的二维向量举例。 K-means 原理很简单，假如有这么些数据。虽然我不知道最终的分类的结果是什么样的，但是可以告诉你我想把这些数据分成几类。 假如现在我要把这个数据分成三类，那么就随机产生 3 个点。 第一步是随机产生 k 个点。 第二步，判断其余待分类的点，离我们随机的 K 点哪一个更近一点。每一个待分类点一定能找到一个离的更近的 k 点。 然后把每一个分类的里面点再重新求一下它的中间值。 求得了新的中间值之后，把这个新的中间值再做为刚才的 k 点，让里边的所有的点去选择到底离哪个 k 最近。 然后再产生一轮新的 k，当一轮新的 k 和上一轮的 k 的距离很接近的时候，我们就说找到了，这个中心点基本上就不变了。我们把这个方法叫做 K-means。 这个 means 就是平均值的意思，首先有 k 个点，然后把这个图像里面的所有的点分配到这 k 个点，找到离 k 个点里面最近的。然后把这些隶属于每个 k 点最近的点求平均值，也就是求他的中心点。 求完中心点，这个中心点又变成了新的 k 值。同样的我们再执行，找离 k 最近的那些点，再求平均值。 这样一轮一轮的，把 k 点这样求出来之后，当上一次的 k 和这一次的 k 不怎么变的时候，我们就找到了它的中心值，就把它聚类起来。我们就把它叫做 K-means。 以上是 K-means 的原理，接下来，咱们来看一个实际的案例。比如说，咱们想这样一个场景，我们给中国建几个能源中心，给咱们中国的省会设置能源中心。 OK，我们来看一下，那首先呢，我们需要一组数据。 12345coordination_source = &quot;&quot;&quot;{name:'兰州', geoCoord:[103.73, 36.03]},...{name:'澳门', geoCoord:[113.54, 22.19]}&quot;&quot;&quot; 完整数据请查看我源代码Core foundations/07.ipynb，为了篇幅我这里就不放完整的了。 这组数据就是每个省的城市和坐标，我们用正则表达式把这个城市里的名字和数字地址全部找出来。 1234567891011121314151617181920212223242526import re# 创建变量保存city_location = { '香港': (114.17, 22.28)}test_string = &quot;{name:'兰州', geoCoord:[103.73, 36.03]},&quot;pattern = re.compile(r&quot;name:'(\\w+)',\\s+geoCoord:\\[(\\d+.\\d+),\\s(\\d+.\\d+)\\]&quot;)for line in test_string.split('\\n'): city_info = pattern.findall(line) if not city_info: continue # following: we find the city info city, long, lat = city_info[0] long, lat = float(long), float(lat) city_location[city] = (long, lat)city_location---{'香港': (114.17, 22.28), '兰州': (103.73, 36.03)} 测试字段证明我们这一段正则生效了，现在可以将内容替换成我们之前写的数据变量： 123456789for line in test_string.split('\\n'): ...city_location---{'香港': (114.17, 22.28),... '澳门': (113.54, 22.19)} 找出来之后，就可以求距离了。但是我们需要注意一下，就是城市之间是有一个球面距离，就是在球面同样都是那个纬度，东经 60 度和东经 45 度，在北纬的这个位置不一样的时候距离差的是挺大的。最极限的时候在两极是 0，最宽的是在赤道上，就会很大。所以有一个专门求经纬度距离的式子，就是经纬度在实际中球面距离的一个式子。 除此之外，还有一些距离公式，比方说余弦距离 (Cosine Distance)、欧几里德距离 (Euclidean Distance)、曼哈顿距离 (Manhattan distance or Manhattan length): 余弦距离 \\[ cos(p,q) = \\frac{x\\times y}{|x|\\times|y|} = \\frac{\\sum_{i=1}^{n}x_iy_i}{\\sqrt{\\sum_{i=1}^nx_i^2}\\sqrt{\\sum_{i=1}^ny_i^2}}, \\] 欧几里德距离 \\[ \\begin{align*} 2 维空间中：&amp; \\\\ d(p,q) &amp; = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2} \\\\ \\\\ n 维空间中：&amp; \\\\ d(p,q) &amp; = \\sqrt{\\sum_{i-1}^n(p_i-q_i)^2} \\\\ &amp; = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2+...+(p_n - q_n)^2} \\end{align*} \\] 曼哈顿距离 \\[ \\begin{align*} d(p,q) = |p_1-q_1| + |p_2 - q_2| \\end{align*} \\] 上图中红线代表曼哈顿距离，绿色代表欧氏距离，也就是直线距离，而蓝色和黄色代表等价的曼哈顿距离。曼哈顿距离——两点在南北方向上的距离加上在东西方向上的距离。 这些都是不同的求距离的方法。就之前给大家说过，我们可以把对象变成向量，向量要求解距离方法不仅仅只有一种。就像这里，咱们将会使用地理上的球面距离： 12345678910111213141516import mathdef geo_distance(origin, destination): lon1, lat1 = origin lon2, lat2 = destination radius = 6371 # km dlat = math.radians(lat2 - lat1) dlon = math.radians(lon2 - lon1) a = (math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon / 2) * math.sin(dlon / 2)) c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)) d = radius * c return d 这里，咱们涉及到了几个参数，一个是origin, 一个是destination，我们是要求这两个参数的距离。都是经度、纬度，类型为tuple of float。然后我们返回一个float。 比如慕尼黑到柏林的例子： 1234&gt;&gt;&gt; origin = (48.1372, 11.5756) # Munich&gt;&gt;&gt; destination = (52.5186, 13.4083) # Berlin&gt;&gt;&gt; round(distance(origin, destination), 1)504.2 city_location就是每一个城市的位置，我们就可以通过第三方库networkx画出来。 不过这里我们需要注意一点，就是如果是现实中文，我们还需要做一项工作，否则会出现乱码： 12345# 正常显示中文from pylab import mplmpl.rcParams['font.sans-serif'] = ['SimHei'] # 默认字体mpl.rcParams['axes.unicode_minus'] = False # 解决保存图像负号'-'显示为方块的问题 然后我们来将图画出来： 1234567import networkx as nx%matplotlib inlinecity_graph = nx.Graph()city_graph.add_nodes_from(list(city_location.keys()))nx.draw(city_graph, city_location, with_labels=True, node_size=30) 看，一个“公鸡”的骨架是不是就出现在图上了？ 接下来，咱们就需要实现 K-means 的部分了。 我们需要将经纬度分别放在 x,y 里面： 1234567all_x = []all_y = []for _, location in city_location.items(): x, y = location all_x.append(x) all_y.append(y) 然后我们随机找了 k 个 center: 12345678910def get_random_center(all_x, all_y): r_x = random.uniform(min(all_x), max(all_x)) r_y = random.uniform(min(all_y), max(all_y)) return r_x, r_yget_random_center(all_x, all_y)K = 5centers = {'{}'.format(i+1): get_random_center(all_x, all_y) for i in range(K)} all_x和all_y是刚刚所有的这些城市的经纬度，让这些城市的经纬度来和每个 k 去求距离，现在是让所有的 x,y 去一个一个和 k 去做对比，找到离它最近的 k。 123456closet_points = defaultdict(list)for x, y, in zip(all_x, all_y): closet_c, closet_dis = min([(k, geo_distance((x, y), centers[k])) for k in centers], key=lambda t: t[1]) closet_points[closet_c].append([x, y]) 找到了 k 之后，我们在所有的图上找到了离 k 最近的这些点，然后再求一个 means，在离 k 最近的这些点中求出新的平均值。 1234567891011121314151617def iterate_once(centers, closet_points, threshold=5): have_changed = False for c in closet_points: former_center = centers[c] neighbors = closet_points[c] neighbors_center = np.mean(neighbors, axis=0) if geo_distance(neighbors_center, former_center) &gt; threshold: centers[c] = neighbors_center have_changed = True else: pass ## keep former center return centers, have_changed 如果新的中心点和原有的中心点距离大于一个阈值，就把这个 central 改成新的值，否则就不改变它。 然后不断的去监测它，不断的去根据已知的这个 k 来求解。离每个 k 最近的这些点，找到这些最近的点，就求解出来新的 means 的 center，就是平均的 center。当我们发现这个 center 没有改变的时候就停了。 123456789101112131415161718192021def kmeans(Xs, k, threshold=5): all_x = Xs[:, 0] all_y = Xs[:, 1] K = k centers = {'{}'.format(i+1): get_random_center(all_x, all_y) for i in range(K)} changed = True while changed: closet_points = defaultdict(list) for x, y, in zip(all_x, all_y): closet_c, closet_dis = min([(k, geo_distance((x, y), centers[k])) for k in centers], key=lambda t: t[1]) closet_points[closet_c].append([x, y]) centers, changed = iterate_once(centers, closet_points, threshold) print('iteration') return centers 然后我们就可以得出来我们的 K-means 迭代出来的情况： 123456789101112131415kmeans(np.array(list(city_location.values())), k=5, threshold=5)---iterationiterationiterationiterationiterationiteration{'1': array([118.14307692, 37.97923077]), '2': array([115.528, 25.643]), '3': array([106.22 , 28.16333333]), '4': array([99.518, 38.86 ]), '5': array([91.11, 29.97])} 我们可以将这些点画出图来，得到五个能源中心的地点： 12plt.scatter(all_x, all_y)plt.scatter(*zip(*centers.values())) 在内蒙有一个，东南沿海有一个，重庆有一个，西藏有一个。 感觉咱们这个迭代的次数应该是还不够，如果迭代次数再多一些，能源站应该会放到珠三角这个地方。 这段代码再运行的时间长一点，几个能源中心基本上它会自然而然的变到京津冀江浙沪，还有珠三角和成都重庆这些地方，可能还会有一个在嘉峪关和乌鲁木齐这个地方。 咱们会发现国家的经济中心的形成，其实是有很强的地理关系，非常的神奇。 我们这段代码是咱们从头到尾的把原理实现了一遍，其实是我们手写的一个 K-means。这样的话，大家就对这个原理就了解多了。当你以后要用到它或者要用它一部分的时候才能知道它的用途。 其实 K-means 也是有一个最大的缺点的，就是最终计算出来的结果会受到初始选取中心的影响。当然，除此之外还有一个缺点，就是它的计算时间复杂度比较高。 尤其是当每次图形越变化越大的时候，就是越斜越细长就会有越受影响，可以设定一个同样的 random seed。 这个代码，大家之后一定要自己去敲，去运行。这样就会更加彻底，比绝大多数人更加理解。本节课的相关代码可以去课程仓库中去找，课程的相关问题可以留言提问（不负责免费解答其他无关问题）。 好，那我们机器学习的入门部分，也就随着本节课结束了。下节课开始的很长一段时间之内，咱们可以慢慢的来消化机器学习的相关知识点。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/07.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A83%20-%20%E4%BA%86%E8%A7%A3K-means/"},{"title":"06. 机器学习入门 2 - 理解特征和向量","text":"[TOC] Hi, 你好。我是茶桁。 上一节课，咱们用一个案例引入了机器学习的话题，并跟大家讲了一下「动态规划」。 那这节课，我们要真正进入机器学习。 机器学习初探 在正式开始之前，我们来想这样一个问题：我为什么要先讲解「动态规划」，然后再引入机器学习呢？ 原因其实是这样：曾经有一度时间，差不多一九七几年开始，大概有三十四年，动态规划其实可以变成图和树的问题。计算机科学里，图和树其实是占主流的，人们去解决图像的分割，图像的分类，文本的识别，文本的分类问题，有很大一部分都会将其优化为图和树的问题去解决。包括上两节课中我们提到的李开复去解决语音识别的问题，也是拆分成语言树。 为什么要优化成图和树来解决呢？因为这个技术在当时非常的成熟。但是，因为有了图和树，那么依照我们上一节的分析，树会继续向下细分成更小的树，也就是会形成子图。 所以，人们就会发现，当我们将问题优化成图和树之后，再使用动态规划，就能让问题加速解决。也就是像我们上节课中所讲的一样。有时候，甚至不用动态规划都无法解决。 可是动态规划是有局限的，当问题过度复杂的时候，使用动态规划也开始解决不了了。 这个问题的一个非常经典的案例是一九九几年的时候，当时电子邮件开始兴起，也就催生了一个非常重要的产业，就是垃圾邮件。因为垃圾邮件成本很低，只要有一个服务器，然后不断的发送就可以了。 比如邮件内容可以写： 因为你经常上网，我获得了你的一些账户密码，我已经将你的一些见不得人的浏览记录都记录下来了，需要你在三天之内，向某个账户转多少多少钱，否则我将公布你的所有记录。 我知道，你们肯定会有人觉得：这么弱智的诈骗邮件都能得逞啊？但其实是能得逞的，垃圾邮件不像诈骗电话，还需要人拨。当然，我知道现在诈骗电话都不需要人值守了，只要有一个电脑连上电话服务，然后 AI 会自动打电话，通过 AI 合成语音就可以。但是当时那个年代可没有这个，发邮件相对就简单很多，只需要有一个服务器在那不断的发送就行了。 家在之前的数学课中应该都学过概率了。这里也就涉及到了一个概率问题，我发 10000 个邮件，哪怕只有一个人会上钩，那我也会挣钱。所以当时发送垃圾邮件是一个很大的产业。 既然有人为这个东西所困扰，就会有人想着用通过正当的方式去挣钱。当时网易的 163，还有美国最早的各种邮箱比如 Hotmail 等等，都提供了一个功能是付费提供拦截垃圾邮件的功能。 当时垃圾邮件可以多到整个互联网上收到的 99.8% 的邮件都是垃圾邮件。如果不花钱，基本上都用不了邮箱，因为邮箱地址也是可以随机生成的。 结果像什么 163，还有 Hotmail 等等，要去攻克垃圾邮件，而垃圾邮件要去绕开他们的防锁，要能诈骗到钱。就进行了这样的反复的斗争。 那这个时候的程序员是怎么做的呢？很简单，他们用的想法也是一样，就是要分析文字，分析语法，把它变成文字树，语法树，变成文本关系。 我们想想，垃圾邮件规律是不是基本上找不到？只要找到一种规律它马上可以变。就算找到了一种规律，把它写成代码了，但是做垃圾邮件的人很快就可以攻克。 当时人们就很头疼，完全没有办法。用这种分析的方法，用类似于动态规划等的分析方法解决不了。 当时哈佛大学有一个老师用了一种方法，叫做基于统计的文本：贝叶斯分析方法，来判断一个邮件是不是垃圾邮件。 他说，不要人工去定规则，不要人工去分析，去找规则。假如在这里找到 2 万个垃圾邮件，然后现在来了一条新的邮件，我不知道内容，但是可以根据以前这 2 万个垃圾邮件，根据它里边的这个文本的内容，文字，看一下之前的垃圾邮件里出现的次数到底是多少，就可以进行贝叶斯分类。 也就是说，里面的单词分别在垃圾邮件里出现了多少次，出现次数多不多等等。这个时候就可以给他一个概率，比方说是垃圾邮件的概率是 0.7，非垃圾邮件的概率是 0.3，那就可以判定是垃圾邮件。 在以前，人们都是写一个方法来判断是或不是，而现在则是变成了一种概率。 结果人们就发现这样非常好做，这样做其实也做不到 100% 正确。虽然做不到 100% 正确，但是可以做一个比较高的准确度，可以拦截大部分垃圾邮件。 而且它可以自动更新，只要把这个程序放这，不断的有垃圾邮件进来，样本库越来越多，接下来再收到新的邮件，就能够知道这个是不是垃圾邮件了。 当时大家还会融入统计分析方法，后来人们就发现根据原来的这些信息提炼出一些数据，让机器自动或半自动的提炼出一些信息，然后去预测新问题。这个过程就特别像小孩学习的时候，你给他很多知识他自己去学，学完之后去解决没有见过的问题。 这种解决问题的方法，后来就叫做机器学习。我们就把解决这种问题的整个方法就叫做机器学习。 之前的这一些内容，也就是咱们机器学习产生的背景。 特征和向量 对于整个世界上的所有东西来说，都是可以被量化的。在管理学上有一个东西叫做 if one thing cannot be measure， it cannot be managed。就是一个事情如果不能被量化，它就不能被管理。 在科学上其实也有一个，笛卡尔当年就说过，如果一个东西不能被量化，那么它就不能被分析。 比方说一个人，要衡量这个人，要刻画这个人的特点，你可以给出特点。例如说身高一米 73，月收入 18000。假如我们用 0 和 1 来表示，到底是男还是女，200910 表示的可能是住址编码，28 可能是年龄。 这个时候我们就会得到一个东西，如果我们把一位男士的信息抽象成这样一个向量。 假如有另外一个向量，这个向量我们把叫做 man2。如果 man2 和 man 的向量的距离是接近的，我们就知道其实它里边的数值是接近的。因为它的向量的计算值比较小，意味着每一个对应的两个数字之间比较小。 所以当我们把一个一个的对象能够量化，变成一个向量之后，我们就能够知道哪些向量之间是相似的，哪些对象之间是相似的。 除此之外，不仅是人，还可以把邮件也处理一下。 假如说一封邮件里面包含了 213 个字符，包含了 1 个关键字，标题长度为 27，0 个抄送地址。那么邮件也是可以向量化，各种东西都可以被向量化。 向量化之后就可以有一个什么样的结果呢？假如存在一种函数： \\[ \\begin{align*} f_1 \\begin{pmatrix} \\overrightarrow {man} = [ \\\\ \\\\ 1.75 \\\\ \\\\ 18000 \\\\ \\\\ 1 \\\\ \\\\ 200910 \\\\ \\\\ 28 \\\\ \\\\ ] \\end{pmatrix} \\end{align*} \\] 我们来看上面这个式子，这一串数字代表的是我们现实生活中的一个对象。这个函数现在我们虽然不知道它是啥，但是我们知道输入一个向量给到一个函数，这个函数可以产生出不同的东西。 假如这个函数返回 0.75，或者是 1.38，负一点几等等。只要是个连续的数字，属于 R。这个函数就是一个回归函数，Regression。 一个函数，它返回出来的只有 +1 和 -1，或者只有 0 和 1 两种结果。我们就把它叫做二分类。 如果返回 3 个值，这三个值加起来等于 1，每一个表示的是某个东西的概率，那我们把这个叫做多分类。 分类函数和回归函数是机器学习里边最典型的两个函数。 分类函数大家好理解，它给出来的结果表示的是类别的概率。例如说是 1 就表示可能是 a 类别，-1 可能是 b 类别。 0.2、0.7、0.1 表示三类，第二类的概率最大。 Regression，回归是什么意思呢？ 回归这个词当年其实是一个生物学概念，一个遗传学概念。指的是生物的下一代的特征会更偏向于群体的平均值。 比方说姚明两米多，一个正常人的身高是 1 米 75，那么姚明的儿子的身高大概率会向着 1 米 75 这个方向变，而不会变得更高。也就是说姚明的儿子大概率会比姚明低。一个人个子特别矮，他儿子大概率呢会比爸爸高。这个就叫做回归现象。 与此同时，其实在我们的整个职业发展中也有这样的情况。假如说一个人特别优秀，大概率他儿子不会像他那么优秀，生物学上把这个遗传线叫做回归。 后来呢生物学家、包括心理学家就发现回归其实本质上是我们的平均值，整体趋势的平均值。所以当时统计学家也用了这个词，他们把群体趋势就叫做 Regression，就叫做回归。 后来在机器学习里，所谓的群体趋势其实就是，假设我们现在有这么多点： 现在这个群体的趋势假设是这样，我们拟合了一个函数 f(x)（红色直线）。那么我们输入一个 f(x) 输入到这条直线里面，就可以得到一个实数的输出，这样的过程就叫做 Regression。 当我们输入一个数字的时候，不仅可能会输出概率，可能还会输出一连串东西。 例如我们现在是一个决策问题，输入了一个情况到一个函数里面，要预测接下来我们该怎么办。 我们输入了行动 1、行动 2、行动 3、行动 4...，我们把这种学习问题叫做 sequence，就是序列问题。 比如输入的是 1、2、3、4 这四个人的信息，输出是 3、2、4、1，给这四个人排了个序。这种排了个序的事情我们就把它叫做 rank，尤其是在推荐系统，在搜索引擎里面用的非常非常多。同理，输入一个 Email 其实也可以做这样的事情。 那我们一起来想一下，假设我们有一个 f(x)，f(x) 具体怎么实现先不管。假设存在一个 f(x), 如果我们要让 f(x) 执行一个 Regression 的任务，可以想一下，这个 f(x) 可以是在什么场景下。 \\[ \\begin{align*} f(x) \\begin{pmatrix} \\overrightarrow {email} = [ \\\\ \\\\ 213 \\\\ \\\\ 1 \\\\ \\\\ 27 \\\\ \\\\ 0 \\\\ \\\\ ] \\end{pmatrix} \\end{align*} \\] 举个例子，比方说要让 f(x) 执行一个 rank 任务，它的场景是这里有十封未读邮件，要排个序。要输出哪些邮件最紧急，然后去回复。这就是一个 rank 的场景。 再举一个例子，如果这个 f(x) 要执行的任务是一个多分类任务，可以是在什么场景下。就是邮件分组对吧？所以我们可以看到，只要我们可以把一个一个对象表示成向量，当我们有一个函数的时候，就可以执行各种各样的任务了。 那我现在问，如果这个 f(x) 要做 Regression，可能是哪个场景呢？ 机器学习其实就是反反复复的在做这么一件事情，就是让机器半自动的得到这个 f。注意是半自动，它并不能全自动。 在求解 f 的过程中，我们需要输入一个 x，真正的值是多少是有标准答案的。f 算的对还是错，是有标准答案的。 比方说贝叶斯、SBM、决策树、神经网络，这些其实都是一种 f。里边这些关键参数是机器自动获得的，但是到底这个函数类型是什么，是概率式、还是 if else 的，还是神经网络，这种形式得人来定。 f 有标准答案，是有对错的。我们把这种有对错的求解函数的方法叫做监督学习。 为什么有对错就要监督学习呢？就是在整个学习过程中，也就是整个获得 f 的过程中，我们会不断的监督他，看他学对了还是学错了。如果学对了就给他沿着正确的方向继续走，如果学错了就要换一个方向。这就叫监督学习。 除了监督学习之外，还有一种机器学习的方法，是让机器自动去归类。 \\[ \\begin{align*} \\overrightarrow {man01} \\qquad \\overrightarrow {man02} \\qquad \\overrightarrow {man03} \\qquad \\overrightarrow {man04} \\\\ \\overrightarrow {man05} \\qquad \\overrightarrow {man06} \\qquad \\overrightarrow {man07} \\qquad \\overrightarrow {man08} \\\\ \\overrightarrow {man09} \\qquad \\overrightarrow {man10} \\qquad \\overrightarrow {man11} \\qquad \\overrightarrow {man12} \\\\ \\overrightarrow {man13} \\qquad \\overrightarrow {man14} \\qquad \\overrightarrow {man15} \\qquad \\overrightarrow {man16} \\\\ \\end{align*} \\] 假如有很多人，我们希望机器能自动的把这些人根据某些特征自动的进行一个分类。在这个过程中，其实是没有标准答案的，是机器根据这些向量自动分类的。我们把这种学习方式叫做非监督学习，也叫做聚类。 非监督学习的难点就是我们不太好衡量，到底是不是对的，还是错的。万一这个分类不是我们需要的分法，就只能改变参数让它再分一遍了。 所以，非监督学习不好量化，它的结果只能作为参考，没有标准答案。 机器学习的通用框架 不管是做什么机器学习，不管是在小公司还是大公司，还是在航空航天局。不管是在哪里，我们都有一个通用的方法。 首先，observed data, 会有一些观察到的数据。观察到的数据之后我们就要进行一件叫做特征提取的事情。特征提取就是把我们观察到的这些数据变成一个一个向量。 观察到的是路边上的一个一个的人，我们要把这一个一个的人变成一条一条的向量。变成向量之后就要进行所谓的学习。这个学习就是需要根据原来的向量，在人的指导下去优化一些函数，得到一些函数的参数。 然后这个函数要能预测新的没有见过的，也就是 New Data 部分，是没有见过的一些新数据，要能得到结果，图中就是得到 y。这是我们整个学习的过程，整个机器学习基本上都是这样的一套流程。 那什么叫做监督学习呢？监督学习就是在学习的这个过程中，每次为了获得 f，会输入一对一对的 x 和 y。y 就指的是我们在训练的时候，我们已知的这些数据的 x 以及对应的值。 通过这些大量对应的值，机器自动去总结规律，抽象规律得到 f。 监督学习就是在学习的时候我们会给到机器 x 和 y, y 就指的是这个 x 对应的值。而非监督学习就不提供这些东西。 非监督学习只提供 x，经过 x 之间的向量的距离近不近等等，自动的去获得 x 的分类。 梯度下降 在这个求解 f 的过程中，监督学习的时候有一个非常非常重要的方法叫做「梯度下降」。 梯度下降是一个非常重要的点，之后的课程中咱们会讲到。 假如我们有一组 k 和 b，输入一个 x 可以得到一个 y。假如就是 kx+b = y，我们现在其实是想求一组 k 和一组 b，能够使得我们输入任意的 x 的时候得到的值都任意的和 y 接近。 还是拿上边这个函数来讲，比如 f(x) = kx + b, 我们现在想求一组 k 和一组 b，让它和 y 的值越接近越好。我们怎么来评价它越接近越好呢？写一个函数 \\(\\sum [(kx+b)-y]^2\\)，这个我们把它叫做 loss 函数：\\(loss =\\sum [(kx+b)-y]^2\\) ，表示这个值如果越大我们信息差的越多，这个值越小就表示我们信息保留的越好，丢失的越少。 其实原理就是要获得一组 k 和 b，然后使得 loss 取最小值。为了求得一组 k 和 b，让这组 k 和 b 能够使我们的函数最接近于我们真实的值，可以给他一个随机值，然后让 loss 去给 k 求偏导。 如果此时此刻求出来的偏导是大于 0 的，就是随着 k 的减小，loss 值要减小。如果 loss 对 k 的偏导小于 0，意味着随着 k 的增大，loss 要减小。 那新获得的 k 就等于原来的 k 加上 loss 给 k 求偏导的相反数。 \\[ k_{2} = k_{1} + (-1) \\frac{\\partial loss}{\\partial k} \\times \\propto \\] 当然我们最后乘上了一个系数，这个系数必须是一个很小的数字，比如说是 0.001。这个系数的作用是什么遇到的一些函数，偏导特别大，但是此时我们其实已经很接近那个最优点了，可是偏导特别的垂直。那在这里就要加一个很小的系数控制一下。 这里要说一下，这个部分不能死记公式，没什么所谓的公式，都是一些比较基础的数学知识。这也就是为什么我之前花那么久来写数学基础的原因。 另外就是，在数学基础之上，要拿出你的笔和纸，当然平板也可以，要多画画，然后你就懂了。如果这个东西不多动笔，觉得要背下来，劝你趁早别干这行了，也别学了，可以去做个文职的工作，就天天背书就可以了。现在学的这些东西一定是要内化的，一定要拿着笔多练，多敲代码。 与此类似的，b 也可以做这样的运算，\\(b_{2} = b_{1} + (-1) \\frac{\\partial loss}{\\partial b} \\times \\propto\\)。这样，经过我们不断地输入 x 和 y，就能够慢慢地找到一组最优的 k 和 b 了。这个，就是梯度下降所做的事情。 接下来，咱们就演示一下梯度下降的意义。 我们现在有一个 loss 函数，这个函数会返回一个运算结果如下： 12def loss(k): return 3 * (k ** 2) + 7 * k - 10 现在对于 k 的偏导，我们把 2 放下来，那就是 6*k，再加上 7: 12def partial(k): return 6 * k + 7 这个就是它的偏导。 现在给他随机出一个值，为了让数据更明显，我们将范围定在 (-10, 10) 之间。顺便给一个很小的系数alpha 123import randomk = random.randint(-10, 10)alpha = 1e - 3 # 0.001 接着，我们来做循环。之前咱们分析过整个式子，直接将其写出来就可以了，最后是打印出 k 和 loss(k)： 12k = k + (-1) * partial(k) * alphaprint(k, loss(k)) 将这一段代码扔到循环里，为了更明显，我们让它循环 100 次，完整代码如下： 123456789101112131415161718192021222324import randomdef loss(k): return 3 * (k ** 2) + 7 * k - 10def partial(k): return 6 * k + 7k = random.randint(-10,10)alpha = 1e-3 # 0.001for i in range(100): k = k + (-1) * partial(k) * alpha print(k, loss(k))----9.947 217.19942699999993-9.894317999999998 214.43236005537193-9.841952091999998 211.69839829966944-9.789900379447998 208.99714566241215...-6.064345358065952 57.87843635922653-6.034959285917557 57.01748574662477-6.005749530202052 56.16683554715212 我们可以看到它的值一直在下降，虽然不能直接求解出最好的那个 k 是什么，但是通过梯度下降这样的方法，一步一步的慢慢的就找到了这个函数的最小值。 当我们把循环次数再次提升到 100000 的时候，我们来看看最后的结果： 123456789for i in range(100000): k = k + (-1) * partial(k) * alpha print(k, loss(k))---...-1.1666666666666852 -14.083333333333332-1.1666666666666852 -14.083333333333332-1.1666666666666852 -14.083333333333332 最后几次打印出的结果基本趋于一致了，k 的值就是 -1.1666，那数学里边我们学过，这个二次函数最优值应该是-b/2a，应该是-7/6，我们计算一下看看： 可以看到，和我们梯度下降所求的值很接近，几乎一致。 这个例子说明通过靠梯度下降，是能够找到一个变量让这个函数取得最小值。 既然咱们刚才面对这个问题能直接能计算出来它的值是-b/2a = -1.16666..., 为什么要用梯度下降的方法来得到这个不精确的值呢？ 我们的这个例子是一个简单函数，可是当函数很复杂的时候，很多复杂的函数我们是求解不出来的。 好，到这里就是我们这节课的内容。下节课就是我们机器学习入门的最后一节课，我们来谈谈 K-means。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/06.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A82%20-%20%E7%90%86%E8%A7%A3%E7%89%B9%E5%BE%81%E5%92%8C%E5%90%91%E9%87%8F/"},{"title":"02. 人工智能核心基础 - 导论（1）","text":"Hi，你好。我是茶桁。 非常开心能够和再次见面，能够和大家一起来学习人工智能的这个课程部分。接下来我会给大家一个既内容充实又包含了一点点难度的这样的一个课程。 我希望上完我的课程之后，能够有所收获。能够收获到自己看本课程时期望的目标。 我们首先要有一个信任，你相信我基本上常见的我都是遇到过的。我能帮你解决这些问题，我的这个课程比起市面上其他的这个课程有什么特点呢？首先是我自己有 11 年的 Python 编程经历，就今年 2023 年正好是 11 年。 Python 当时在我开始学的时候，是一个非常非常小众的语言。我还记得，但是似乎 Ruby 都比 Python 稍微要热一点，现在 Ruby 和 Python 比当然差了很多，可是那个时候，因为一样的特性这两者被对比的很多。 第二，从我开始系统的学习，到做研究，到做项目开始，有 9 年的时间，在企业里的工作经历 6 年时间。 什么是人工智能 What is artificial intelligence? 接下来是一些关于学习的一些建议。这第一节课，我要跟大家带来一个很很重要的内容，就是我们这个课程叫做人工智能课程，那么咱们需要花费一些篇幅，来搞定的一个问题，就是人工智能到底是什么。 你有没有想过人工智能是什么？这个世界上现在对于人工智能到底是什么 几乎可以说每一个人的定义都不一样。不同的人，每一个同学其实都有一个自己的认识，每一个人其实对于人工智能的定义其实都不太一样。 我们可能在整个生活中会看到很多和人工智能相关的东西，例如 openAI 的 ChatGPT，Google 的 Bard，百度的文心一言，微软的 New Bing。再比如说美国 NASA 发射火星车。 日本，还有法国人经常会做这种拟人的机器人，当然有些电影里面会有类似于终结者这样的东西。 波士顿 Dynamic 他们会出这个机器狗，还有 AlphaGo、无人驾驶、语音识别，还有面部识别。 而目前，人工智能的其中一个分之，也就是生成式人工智能火的一塌糊涂是吧。 就这些东西，你会发现我们都把它叫做人工智能。但是你仔细看起来，好像这些东西的区别每个又很大。为什么我们把它都叫做人工智能呢？他们之间的共同性是什么呢？ 有些科学家用 iPad 去自动监测水稻的整个生长，你会发现在这个过程中好像没有任何机器人相关的东西。但是我们也把它叫做人工智能。那人工智能到底是什么呢？ 首先我想给大家一个很客观的说法，大家一定要知道你现在学的是一个前沿知识，所以大家一定要抱着一个很开放的心理。目前没有一个公认的对于人工智能的定义，整个人工智能，整个 AI 其实是有不同的学派，是有不同的争论的。 这个对于我们整个东亚教育体系的学生来说可能比较难以接受，这个就是咱们从小学的东西往往都是有标准答案的，基本上课本上的东西不容置疑。学一个东西往往刚开始的时候学的是什么什么的定义。 比方生态循环定义是什，动量的定义是什么。但是当我们如果说现在学的是一个研究生课程或者博士课程，你要特别习惯的一个点就是有很多很多很重要的内容，其实这个世界上是没有标准化定义的。 例如说很火的基因工程。那么什么是基因，其实不同的学派他们的定义是不一样的，在有些学派里，这个东西是基因，有些就不是。 现在的复杂系统。什么东西是系统？在不同的学派都不一样。 所以今天我们会带着大家从各个维度来理解一下人工智能，但是不会给他一个精确的完整的定义。因为人工智能在不同的时间段，在不同的学派，它的定义是不一样的。 There is no definition accepted by all the researchers or engineers. 当你上完课程，如果你发现你听到了两个关于 AI 的定义或说法，你只要知道他们其实是不同的人在不同的时刻，不同的问题场景下对于这个东西所做的定义，你要理解他们。 在咱们整个学习的过程中，从今天开始，你以后要学习到的这些知识，大家一定要知道。 我第一节课要给大家知道的一点就是，你学的东西很有可能之后是会变的，之后是会过时的。而且你学的这个东西很有可能再过几年他会被人认为提出来是错的。 之所以会产生这样的问题不是因为你当时学错了，而是因为你现在学的是一个比较前沿的东西，整个科学的进步。 大家可以去看一下有一段关于 Steven Hawking 当时还活着的时候的一段采访。他说我们做的事情就是 不断的推翻前人的工作，并且等着我们的工作被后人推翻。这就是做前沿的事情所面临的一个问题。 所以大家一定要保持心态的开放，我发现我们做程序员出身的同学，很多同学比较固执。比方说有的同学会争到底学 Java 好还是学 C++ 好，到底学 PHP 好还是 JS 好啊等等。什么 Python 就怎么怎么样。 就互相争，这个争论其实完全没有必要啊。你们不知道 PHP 宇宙无敌吗？ 如果想成为一个真正优秀的人，一定要保持一个开放的心。我给大家分享一个概念，是道德经的前两句话，这个可能很多同学都听过，但是呢多同学不理解他是什么意思。 所谓「道可道 非常道」，这个道可道是什么意思呢？就是如果一个道理它可以被说出来，那么它就不是一个固定的道理。如果一个道理可以被人说出来，那么他肯定就有另外一种说法，肯定不是永久的道理。 如果一个东西我们给他给了一个名字，我们是可以说出来的，这个名也是可以说出来的意思。如果有一个可以讲出来的名字，那么它也不会是一个固定的名字。就好比一个人，你能够给他起一个名字，一个东西你能够给他起一个名字，就能够给他换另外一个名字。 同学在这个过程中，第一节课我想多次跟大家强调的就是：要理解并接受这个世界的复杂性，以多维度多角度观察问题，切忌固步自封。 我为什么要跟大家来讲这些事情呢？有的同学说，你讲这些东西但没有用啊。我要年薪十几万二十几万，我想年薪四五十万，我想年薪赶快过百万，年薪千万。你给我讲讲怎么年薪四五十万，百万千万别扯这些闲的。 其实我想跟大家说的是，一个人如果你想突破自己现有的边界，想从一个月一万多、两万多到一个月四、五万，五、六万，甚至一个月平均下来能挣十几万二十几万，年薪百、万千万来说，这个时候重要的是你一定要能博采众长。 在我们 AI 的这个行业更是这样。就是你要能知道你学的这个东西是会变的，不要死板的去动他。那么这个时候就会有一个问题，我们在这个课程上学什么呢？ 我们在课程上学的大家更应该关注的是那些不变的东西，例如我给大家就是讲 CNN 结构的时候，大家就一定要跟着我去思考，为什么当时人们提出来了 CNN，CNN 比起其他网络它到底解决了之前哪些不能解决的问题？它到底它的原理是什么？ 只有你理解了为什么，那么当他之后出现新的东西的时候你也知道他改了哪些东西，你也更能跟上这个时代的变化。切记死记硬背。 我们把基础课上完之后，把基础东西掌握了。现在我跟你说很多东西可能你也不理解其中到底是什么意思，先把基础课上完。然后会有专门的时间给大家介绍。 虽然现在整个人工智能没有一个统一的定义，但是我们可以从不同的维度来给大家做一个分析。 从时间维度来讲 首先从时间维度上来分析。 最早最早的时候提出来人工智能，有的同学说可以把这个时间都扯到中国的伏羲啊、阴阳啊、八卦算盘。就有人说 AI、人工智能最早是中国的伏羲氏当时提的。什么阴阳变化啊各种定制。这个无从考证。 因为伏羲确实自己也没有留下什么可验证的东西，我们说一下可验证的东西。最接近的，最早是一个叫做图灵的人，当时提出来了一个问题叫做 Can Machines think，机器们能思考吗？ 他在这个过程中讨论了一个问题，就是如何实现机器的智能化与自动化。在这篇文章里面他做了一个定义，机器如果有智能，如果能思考我们怎么去检测它。 在此之前其实图灵还做了一件很重要的事情，它提出来一个东西叫做图灵机，图灵机是一个什么东西呢？他是图灵在数学上证明了，如果我们能够在一个有限的带子上有一个状态，然后我们还有一个东西可以去读取他的状态。我们还有一些固定的操作，注意是固定的操作，这样的一台机器可以执行所有的运算，加减乘除，积分、微分，平方、开方等等。他证明了这样的一个简单东西可以执行所有的运算。除了他不能自动判断是否停机问题。 就是除了机器不能自动判断它是不是要停，除此之外它能够计算所有的计算。这是当年图灵提出来了一种机器叫做图灵机，他在理论上奠定了现代计算机的基础。 有的同学看完原始的文章后说这图灵机也太简单了吧，但图灵之所以厉害的地方就在于，你看一看我们学过的数学书、物理书、化学书，你就知道我们学过的计算有多少了，他把那么多复杂的计算变成了一个极其简单的模型。 把极其复杂的东西变成非常非常简单的东西，这才是需要非常大的智慧的。后来做完了图灵机，图灵又开始去思考机器能计算，到底能不能思考呢？当时这个文章写的很长，大家去去可以读他的原文。 图灵就提出一个很著名的测试，我们如何去测试机器有没有智慧。他给出了一个比较可行的方案，当时是说一个人在屋子外边，对面有两个屋子，其中一个屋子里也是一个人，另外一个屋子是一个机器。但是对于外边的这个人他不知道里边到底哪边是人，哪边是机器。屋外这个人可以给门缝里面去递一些那个纸条，给 1 和 2 递一些纸条。 当然那个图灵并不知道后来人们发明了互联网，可以直接打字。他当时还不知道有互联网这个东西。如果外边这个人他分不清楚里边这个人到底是人还是机器，我们就可以说对面的机器是有智慧的。 比方说他可以问对面啊一个非常复杂的数字。如果机器有智慧的话，按照图灵的理解是他会假装很复杂，虽然瞬间可以算出来，但是他会假装一下。或者他会问一些问题，比方说你吃了吗？你今天觉得这个天气怎么样啊？对或者问他一些哲学问题，比方说你觉得我们有自由意识吗等等。结果就是如果当外面的人发现他不能判断里边到底是人还是机器的时候，我们就说是有智慧的。 图灵当时提出来了图灵机和图灵测试，大家发现图灵想的东西都非常的高屋建领似得。他没有造出来任何机器。 在整个人工智能及计算机的历史上，有一个非常著名的人叫做冯诺伊曼。这个人把图灵提出来的计算模型，纸带加上一个读取物，再加上有线操作员，变成了一台机器。 这台机器通过电子管，电线连接起来，变成了一种真的能运行的机器。这种机器呢就叫做冯诺伊曼体系结构的机器。 这台机器当时在整个第二次世界大战中起到了非常著名重要的作用，世界第一台原子弹和它有密不可分的关系。就是刚才那个纸带，如果说我们要图灵机能够做出来真正可运行的，其实就是变成冯诺伊曼这样的结构，它变成了我们日后真正能运行的机器的基础。 后来到一九五几年，第二次世界大战结束，有一群很年轻的科学家就集中在一起提出来了一个单词，这 个时候呢就有一个单词在一个词组，在人类的历史长河中被提出来就叫做：Artificial Intelligence，当时叫做 Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it. 什么意思呢？只要一种 intelligence 能够被表述，那么这个机器就能够去模仿它。当时一九五几年的时候，人们对人工智能，对于我们的这种 intelligence feature，比方说我们的眼睛看东西、人说话。大家觉得这种能力可能一二十年机器就可以做好。 因为当时人们制造出来原子弹，就觉得人类已经厉害的不行了。还有准备阿波罗登月等等。那个时候，就觉得人类已经非常非常厉害了。但是呢人们发现这件事情非常非常的困难，直到 2016 年才有了一些进展。 当年的这个会议，提出人工智能的会议，大家可以看到都是很年轻的科学家，都是二十几岁哎 30 岁左右的。 我一出镜有同学说，好年轻啊。其实我比起他们来说年龄已经算大了。大家一定要知道，所有的科学技术，高产期高分期一定是很年轻的时候，那个时候人的思维最活跃。在这些科学家里边，每一个人都成为了世界顶尖的计算机科学家。而且里面还有一个非常著名的人，克劳德·艾尔伍德·香农就在这里，香农后来是成为了我们信息论整个的开山鼻祖。 我们现在每天说 3G、4G、5G，我们每天用的互联网，其实都离不开香农的工作。 后来到一九六几年，人们提出来了一个非常简单的模型，这个模型叫做 Perceptron, 感知器。 Perceptron book showed that linear models could not solve XOR (Minsky / Papert) 1969, first AI winter. 感知器是最早的类似于现在的深度学习，但是他和现在深度学习还是有比较大的区别。他提出来了这样的一种东西，就是通过我们的简单的线性和非线性，简单线性变化，能够去拟合出一些函数。而且能够对我们的一些问题进行分割。 但是有数学家和计算机科学家就证明了，这种感知器模型连最简单的逻辑上的异或关系都解决不了。 什么叫做逻辑上的异或关系呢？ 我们来看一下这张图左边的坐标轴，坐标轴 X1 轴是 0，1，X2 也有 0，1。 假如我们现在有逻辑上的 and，逻辑上的 or，逻辑上的 not。 如果是 and 的话，0 和 0 是不是就是 0，0 和 1 的话就是 0，1 和 1 就是 1，0 和 1 就是 0。 在数学上这种 Perceptron 其实它是类似于 \\(\\Sigma_{W_i X_i}\\)，其实它是一种线性变化。 线性变化的话它可以做线性分割，比方说 and，就可以线性的去把这个数据分割了。or 其实也可以，not 其实也可以。大家可以下来之后自己查一下。 但是人们就发现像这种异或关系，怎么样机器都是不能去解决的。就是你不能画一条线，把加号和减号给他分割开。在这是非常基本的一个逻辑操作。 人们就说这玩意连这个都解决不了，你还能干啥。后来人们就发现这个其实是希望通过函数拟合来解决我们的问题，希望通过函数拟合来解决类似于分类等等这种问题，类似于通过函数拟合来解决逻辑问题。 这部分听不懂没关系，因为这个方法现在已经不用了。你只要能知道一九六几年的时候人们已经提出来了一个和现在的深度学习模型比较相似的东西。但是后来他就不行了，因为他解决不了一些很基本的问题，就可以了。 后来从一九六几年开始，人们就把 AI 的这个问题转向了通过符号和逻辑运算，这是一九七几年八几年的时候最重要的一个方法。 Changed AI research to Symbols and Logics. 然后在 1980 年到 1990 年的时候，开始有一个人，这个人是 2019 年获得图灵奖的一个作者，叫做 Hinton。 Backpropagation, statistical learning, 1980s ~ 1990s Hinton 当时在美国加利福尼亚圣地亚哥分校的脑科学认知科学实验室，当时就发现我们可以通过一种叫做反向传播的东西，再加上非线性变化来拟合复杂函数。 我们梳理一下时间段，一九三几年的时候图灵提出来了我们可以有一种机器，他可以执行各种各样的运算。一九四几年、五几年冯诺依曼就做出来了真的能计算的机器，一九五几年的时候提出来人工智能到底是不是模仿我们各种可以定义的 intelligence 的特征。后来人们就提出来了一种 Perceptron 能够拟合简单函数。但是后来发现它其实连一些最基本的逻辑运算都做不了。到一九六几年，AI 有一个很重要的一个学派，现在这个学派其实也有很多人，只不过现在它不活跃了。就是此起彼伏，整个学派不活跃了。这个学派就是基于 Symbols 和 Logics 演算的。 到 1980 年有一个人叫做 Hinton，他当时就提出来可以通过一种叫做反向传播的方法来拟合复杂函数。 而同期，也是一九八几年，有一个很年轻的博士生，现在呢也是一个大佬了。这个人估计国内很多人都会比较熟悉，这个当时很年轻的博士生叫做李开复，他提出来一个东西，他的论文叫做一种能够自动识别语音的系统。 当时人们发现用 Perceptron，类似于函数拟合这种东西是解决不了 AI 的问题。人们把它变成了逻辑和推演问题，一九七几年、八几年的时候人们解决 AI 的问题是怎么解决的呢？比方说咱们现在输入了一段语音，机器要把这段语音到底是什么内容给他翻译出来。比方说这段语音我们听起来可能是「你好吗」，也有可能呢机器可能认为说是「泥耗马」，也有可能是「你好嘛」。 当时摆在这些科学家面前的问题是，人们其实已经能够通过语音来把它转变成以上三种的内容，就这些文字都可以转化出来。但是大家不知道，我让机器怎么能够「自动」，怎么样能够自动的去识别出来到底是 1、2 还是 3。 当时大家真的很恼火，就是做不出来。在全世界来说，大家都是在做什么语法分析，还定义了各种语法。大家学过那个英语语法吗？就是定义了很多语法规则，语法术。大家希望通过这些去解决，结果当时这个语法规则是越做越复杂，语言的特例实在是太多了。 后来人们就发现李开复提出来一个方法，就是他基于概率来做这件事情。怎么基于概率来做这件事情呢？ 在李开复之前人们如何判断这句话对还是错，这句话为什么错，因为他不符合语法规范，因为他怎么怎么样一大堆。 但对于李开复来说他并不会说这个话真的就错了，其实这个话人家也有可能对。 举个例子，你像这个「泥耗马」，那现在很多网络用语里一些奇奇怪怪的用语和这个「泥耗马」很类似，也许其实这个「泥耗马」就是对的。 所以说，这些话其实并不能就给他打个对错，而是要判断它的概率。类似于我们在基于语境，放在网络上一句话是正确的，放在考试里大概率就是错误的。 李开复当时是怎么判断的？他的核心思想很简单，咱们不是有很多书籍吗？有很多文章，你给他统计一下，比方说，你可以统计出来这么多书里边「你好」出现了多少次，「吗」出现了多少次，「你好吗」出现多少次。 那么这个你好吗，这句话的概率它就等于：P(你好 | 吗)P(吗)。这是不是就是一个条件概率啊？之前仔细看了我数学篇的同学对于这个公式应该就非常熟悉了对吧？ 「你好吗」的概率可以通过在文本中数个数来知道，你给我 10 万本书我给你数一遍，「吗」字也可以数出来。这样做结果就是我们把之前非常复杂的计算方式变成了数数就能解决的问题。其实数数就是我们的统计问题，统计本质上就是数数。 就上面这两篇论文，把我们的整个 AI 从以前的符号分解，带到了我们的神经网络加上统计学这样的一个过程中。 后来发展到了 1998 年，有了一个真正的能解决实际问题的 AI 程序了。看起来很复杂的一个视觉上的程序，解决自动识别问题。 0123456789，手写字母识别。当时是杨立坤使用一种叫做 CNN 的网络解决了 0-9 之间，识别手写数字的问题。大家就对它越来越有信心了，但是当时是又由于我们的计算能力和我们的数据量，之后大家知道这种 CNN，神经网络其实特别需要计算力和训练数据的数据量，导致没有大规模去应用。 时间到了 2015 年，准确的说最重要的是 2012 年。当时有一个叫做 image net 这样的比赛，image net 有 1,000 多万张图片，有 1,000 多类，让大家自己去做图片分类。 当时 2010 年的时候图片分类的 TOP5 的错误率有将近 30%，TOP5 的错误率是什么意思？就是我也不指望你全说对，你给出 5 个可能的选项，其中有一个对了就算你对，这叫 TOP5。 就是你给 5 个，只要里边有一个对了就算你对。当时的正确率也只有 72%。 如果你要按 TOP0 啊 TOP1 的话，那估计就错误率能超过 50%。这是咱们 2010 年时候的情况，就很差。 其实从 08 年之前大家一直也都在尝试，基本上是每一年能前进 1%，最多 2%。所以在 2010 年的时候，当时有一些科学家预言人工智能能在实际生活中落地大约在 2040 年左右。为啥呢？你看这个错误率是不是 30%，2010 加 30 就是 2040 年代。就按照这个速度，都是每一年前进一个百分点，两个百分点。 结果到了 2012 年这个错误率基本上相当于是拦腰折断了。 是一个科学家的学生用了一种模型，这个模型就叫做深度神经网络。他的错误率直接下降到了 16%，拦腰折断。后来从 2012 年之后每一年的错误率都迅速下降，到了 2015 年的时候错误率已经低于人类、低于人眼了。人其实也会犯错误。2015 年的时候准确度已经达到 99.7%，低于人类了。 那这个时候是不是就意味计算机在识别物体的这件事情上已经低于人类？是不是就能意味着他有可能能商用了？真正的计算机在我们国家开始大规模应用落地，开始蓬勃爆发其实也就是产生在 2016、17 年。 你现在回忆一下，在 8 年前，如果是春运的时候，比如像北京西站杭州东站成都东站这种大站，郑州站这种，是不他临时会在外面安排很多检票员，二三十个。 北京机场很夸张，每年那个时候会有 30 多个临时检票窗口，有 30 多个人在那里检票检测。 但现在，坐高铁的话全部都是身份证自动比对了。大家知道，就是对于我们的孩子来说，他就觉得这是很正常的一件事情，身份证放到这边然后人脸识别直接通过。但其实就在 8 年前，这件事情还是做不到的，还得有一个人在那里吭哧吭哧在那检查。 这就是我们其实我们是亲眼见证、感受了这个时代，从 2012 年到现在真的产生这样大的变化。 当时，2012 年带队的这个科学家叫做 Alex，他就是刚才我们提过的 Hinton 的学生。Alex 做了什么事情呢？他把我们刚才提到的 1998 年的那个网络做了优化，做了分布式。 然后在这个时候，硬件和数据已经都 OK 了，所以他改动之后取得了当时的世界第一，在人类的科学进步史上留下了不可磨灭的作用。 2016 年又发生了一件很重要的事情，当时一个叫做 Deepmind 的公司要挑战 当时的职业围棋九段。 围棋这件事情在整个人类的所有的游戏中是公认的难度最大的，就当年的 IBM 可以下赢国际象棋，但他就是下不赢围棋。甚至有科学家证明了围棋这件事情在现有的计算体系下是无法被解决的。 但是 Deepmind 这个公司的首席科学家就说，为什么人类要这么痴迷于游戏呢？是因为游戏其实是对我们现实世界问题的一种抽象和简化，而围棋是所有的游戏中对人类的这个世界抽象简化程度最为相似的，所以它的难度也是最大的。 在 Deepmind 之前，世界上最先进的人工智能程序都不能打败职业的初段的选手，结果 AlphaGo 要挑战。Deepmind 要挑战的是李世石，一个职业九段。这个我不得不佩服一下，我们不得不佩服谷歌的这个公关能力、市场能力。 他为什么当时这个比赛要去选李世石呢？其实当时李世石不是世界排名第一的棋手，当时排名世界第一的是柯洁，也是中国的年轻人。但是李世石是柯洁之前的 世界第一，李世石当时年龄已经大了，但是因为李世石是柯洁之前当了好多年的世界第一，所以李世石在全世界来说名气更大。 围棋选手都是黄金期就那么几年，他现在年龄稍微大了，脑子又跟不上了，人又是职业九段，名气又大，难度又不是最大的。所以说，谷歌当时就搞了这么个东西。结果赢了，赢了之后他们就发现，就说这东西都是很机械的，都不是智慧。 人们一直说围棋是智慧，因为里面有推理，感觉有分析、有逻辑。AlphaGo 当时的成功其实就证明了我们现有的这个计算机科学，整个人工智能能解决推理问题。这也是非常非常重要的一个操作。 后来到了 2019 年开始，我们会发现一个叫做 Alot 的概念被广泛接受，所有的工厂，所有的东西都有了 AI 的智能化。 在十几年前我们有人提物联网，大家会觉得是个很遥远的事情。但是大家想一下，你最近有没有骑那个共享单车啊？有没有用滴滴打车去打车？你会发现现在的这个智能化，其实它不仅仅是在以前的什么下个棋什么的，所有常用的硬件它都联网了，车的调度、自行车的安排等等。 从 2019 年之后，各个领域都开始做人工智能的东西。这是我们说的第一个维度，就是从时间维度上来说，不同时间段的人工智能有不同的这个侧重点，有不同的影响世界的东西出现。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/02.%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A0%B8%E5%BF%83%E5%9F%BA%E7%A1%80%20-%20%E5%AF%BC%E8%AE%BA1/"},{"title":"1. 数学导论 - 概述","text":"因为部分自媒体上无法显示公式，为了方便，有的地方我是直接整段截图。和文章字体不一致的部分还望见谅。 Hi, 大家好。又见面了，我是茶桁。 在之前的一个多月前，我有了一个写一个AI系列的想法，起名为《茶桁的AI秘籍》，简单规划之后，于7月27日发出预告，然后历时二十多天将近一个月，完成了其中《Python篇》的写作。 不知道其中的内容对大家是否有帮助呢？ 那么今天我又回来了，根据规划，Python以及相关第三方科学计算库只是我们基础工作的一小部分，而很大一部分基础工作都还未进行。 那么这次，我依然给大家带来的是基础部分，让我们进入《Math - 数学篇》。 数学对于计算机编程来说重要性是毋庸置疑的，更何况我们现在不仅仅是编程，而是走在「人工智能」的路上。可以说，数学应该是最重要的基础。那从本节课开始，就让我们进入「人工智能的数学基础课」。 我们在学习AI的过程当中可能会遇到的一些关于数学方面的一些东西，比如说线性代数里面的这个矩阵运算，比如说这个求导，还有一些概率统计，图论方面的一些东西。 如果您觉得自己对于微积分，线性代数，概率统计这些内容自认为掌握的还不错的同学，其实是可以不用看了。如果大家是从文科转过来或者说以前上的数学很多年了也忘的差不多了，那可以来听听这些课。 本节课的内容是「数学篇」的第一节课。本节是一个导论。我会为大家介绍一下这门课一些理论，还有微积分、线性代数、概率统计里面一些比较基础、和AI结合非常紧密的一部分，难度并不会很大。觉得自己数学能力还是不错的，这第一节课呢可以不用看，可以直接去看后面的课程。第一节课，我们放松一点。这节课难度真的不会特别的大，主要是带大家了解一些比较有趣味性以及AI方面一些基础性的数学知识。 此文章为公众号收费文章，更多内容请阅读原文：1. 数学导论 - 概述， 更多「AI秘籍」内容请查看「茶桁的AI秘籍」 欢迎关注「坍缩的奇点」，解锁更多高质量人工智能原创文章。","link":"/1-Introduction-to-Mathematics-Overview/"},{"title":"08. 机器学习 - 线性回归","text":"[TOC] 从本次课程开始，大部分时候我将不再将打印结果贴出来了，因为太占用篇幅。小伙伴可以根据我的输出执行敲一遍代码来进行学习和验证。 同样是为了节省篇幅，我也不会再一行行那么仔细的解释代码了，一般只会告诉你我代码做了什么。其中的逻辑关系和关键词，小伙伴们自行好好的琢磨一下。 Hi, 你好。我是茶桁。 前几节课咱们主要是了解到了什么是人工智能，整个机器学习的工作路径等等。不过可以说，前面的几节课，从机器学习导论到上一节课介绍 K-means。 咱们快速回顾一下，前几节课里的内容都有什么。之前咱们学习了什么是优化问题，然后还有什么是动态规划，什么是机器学习问题，以及监督学习和非监督学习的区别，我们还学习了一个非常著名的非监督学习方法：K-means。 这些都还是一个铺垫。真正的内容，从这节课才算事正式开始。 从这一节课开始，我会给大家开始系统的学习监督学习，监督学习其实内容比较多，我们可能需要多花多一点时间。 在最开始，我还是要更大家强调一下上一节课上更大家讲的问题：要学习算法，不仅仅是要学习很多很多的这个算法模型，更重要的是什么要能够把问题抽象成一个一个的算法。工作场景中的问题并不能使用一个单独的算法模块能够解决。 机器学习算法只是人工智能其中的一个部分，或者说人工智能的某个部分，比方说取它的特征等等。它的某一个部分用在真正的工作中，用在项目中是很杂揉、很混合的一个状态。 咱们篇幅短内容多，密度比较大，短短几节课，可能是大家研究生课程一个月时间的内容。可以说，咱们课程还是有点难，信息量也比较大。 OK，开始第一个问题。 线性回归 LINEAR REGRESSION 咱们第一个要讲的，也是非常重要的一个方法，就是线性回归。 线性回归非常的简单，也非常的基础。但是它作为我们整个人工智能，整个深度学习中要讲的第一课，里面蕴含了非常多的机器学习的基本思想。所以大家一定要把它学清楚。如果能把它学好，其实对于咱们以后学习帮助非常大。 咱们来看一下，什么是线性回归。 在生活中有很多这样的问题，比方说咱们的血糖，往往在吃饭的时候，吃的糖分、碳水化合物等比较多，饭后的半个小时、一个小时内血糖会更高。 还有一种情况，抽烟抽的越多的人，得肺病的概率往往会越高。并不是一定说抽烟抽的多的人就一定会生病，但是抽烟抽的多的人得肺病的概率往往会越高。 那么还有一种情况，比方在咱们工作的时候，随着工作时间的增加，收入往往也会越来越多。尤其是在日本，是一个非常典型的情况。在日本基本上一个人的收入和他的工作年限是最相关的。他们在一个固定的时间内的薪资变化不会非常大。 比方说都是 29 岁、都是 32 岁、都是 35 岁，假如都在同一家公司，那么薪资待遇也会很接近。 往往这个其实反映的是我们现实生活中一个非常基本的一个关系，随着有一些值的增大另外一些值也随着变化。 假如说我们把它变成自变量和因变量，所谓的自变量就是它的变化会引起因变量的变化。也就是说在现实生活中，我们最基本的关系是随着一个变量的变化另外一个变量要么增加，要么减小。当然不变可以算是一种特殊情况，就是变化为 0。 那么吃糖的多少、抽烟和工作年限就是整个自变量和因变量的一种关系。现在希望让机器来找到这个关系。 如果把这种关系画出来的话，我们用一个图表画出来就会发现其相关性。 就比如下方这三张图表： 这三个图是非常非常典型的，随着一个量的变化另外的一个量要么减少，要么增加。 这个时候工程师就希望我们能够对现实生活中这种随着一个变量的增加或减少导致另外一个量增加或减小，能够找到一种关系来刻画。 当然之间的这种关系可能会很多，可以是一种线性结构，y = kx+b, 也可以是 其他的什么结构。最简单的一种其实就是线性关系。 \\[ \\begin{align*} \\vec {x} = [x_0, x_1, x_2, ..., x_n] \\\\ f(x) = \\sum_{i \\in N} w_i \\times x_i +b \\end{align*} \\] 我们把这种关系称为是线性关系。 为什么称为线性关系呢？假设 x 现在是一维空间，\\(x \\in R^1\\), 那么 f(x) 就是一条直线。如果 x 是二维空间的话，\\(x\\in R^2\\), f(x) 就是一个平面。这种关系其实是自然界中最简单的一种关系。 除了这种关系之外，你还可以想象一下，如果我们要刻画 x 和 f(x) 之间的关系，你还能想到哪些函数呢？ 比如咱们可以有： 二次函数 \\(f(x) = ax^2 + bx + c\\)， 三角函数\\(f(x) = sin(x)\\), 幂函数\\(f(x) = a^x\\), 反函数\\(f(x) = tanh(x)\\), 对数函数\\(f(x) = log(x)\\)， 咱们整个来了个数学回顾。这个时候你会发现好像有非常多种函数，甚至我们还可以在这个基础之上做一些复杂的函数，比如说$f(x) = x{bx3+cx2+dlogx}+elog^x_m $。 理论上，我们可以做有无数种可能的关系。 其实就是如何找到因变量和自变量之间的关系，长久以来，这其实是我们整个自然科学界一直在思考探索的一个问题。 像牛顿，笛卡尔、爱因斯坦、波尔这些人其实都是在研究这件事情。当观察到了很多事情，然后期望用一种函数关系能够把它来表证出来。 后来，在 14 世纪一个非常著名的哲学家就提出来了这样的一个理论，叫做奥卡姆剃刀原理。奥卡姆剃刀原理说的是对于一件事情，你要解释它的关系的话，最简单的： The explanation requiring the fewest assumptions is most likely to be correct. 这个 fewest assumptions 指的是什么意思？就是最少的假设，你可以把它理解成是几个假设，假如对应到函数上，就有很多变量。 举个例子，你们单位上有一个同事经常迟到，有三个人对这个同事为什么迟到有不同的说法。 第一个人说这个同事迟到大概率是因为他前一天晚上吃坏了肚子，导致一晚上没睡好，一大早还因为拉肚子迟到了。 第二个人说同事昨天和一个男生出去了，可能玩太晚导致没起来。 第三个说这个同事可能对昨天法的工资有抱怨，去找领导议论没有得到结果，昨天找男朋友安慰了。今天也是因为赌气故意迟到。 那么对于一个女孩子第二天早上迟到，人们就有 3 种不同的说法。那么大家仔细思考一下，对于你来说，你要相信一个的话，在我们日常生活中你会发现把一件事情想的越复杂往往就错了。 因为比方说第三个条件的，首先昨天发没发工资是一种可能性，然后发了工资她有没有去找老板？找了老板老板有没有怼她？就算怼了她，那她有没有找男朋友？这一系列下来你会发现这个事情如果把它变得因素很多，你对这个事情的估计可能会更错误。 奥卡姆就提出：若无必要，勿增实体。如果对于同一现象有几种不同的假说，我们应该采取最简单的哪一种。那么在我们抽象的函数上，拟合也是这样，我们要拟合一种关系，一种最简单的关系其实往往是最有用的。 比方说在这种关系上： 在这种关系上，你可以说他是一条直线。 比方说我们看到绿色这条线，比较弯，可以说这条线就是这样的一个函数。但是最简单的一种方法，假设它就是一条直线，就像红色这条线。 我们如果把绿色这条定义为 a，把红色这条定义为 b。a 好像拟合的程度更高一些，b 其实是做了一个特别简单的假设，它假设就是一个简单的线性关系。线性关系就是随着一个变量的增多，另外一个也成比例的增多或者减小。 a 看起来更复杂，但 b 在整个状态上看更稳定。a 这个函数在没有看到的地方，其实按照趋势就有可能差的特别大。而 b 虽然在观察到的地方有一些差别，但是因为它这个模型很简单，假设很简单，所以在这些没有观测到的地方你会发现还是和我们整体的趋势会比较接近。 a 复杂，在做函数拟合的时候，不管这个关系看起来有多复杂，先假设它是最简单的一种线性关系。当线性关系实在不好的时候，再把它变复杂。 这就是为什么学习机器学习监督式学习要先学习线性拟合的原因。对简单的假设不一定对，但是除非这个简单的假设不行，否则我们就不要给他更复杂的假设。 比方说下面这个图： 这四张图中，都可以用一个直线去拟合。当然有的时候，比如说 (x2,y2), 当它数据量很多的时候用一个直线效果就会不太好，包括 (x4, y4) 效果可能也不会太好。 在这个时候，当我们发现它效果很差的时候，再去给他换一个模型。那像 (x1,y1)，还有 (x3, y3)，还有如下图这种： 这些其实都可以用一种线性关系来拟合。所谓的线性拟合，就是把函数写成自变量 x 和它的权重相乘相加，然后再加上一个 b。就是我们刚才所描述的：\\(f(x) = \\sum_{i\\in N} w_i \\times x_i + b\\)；这种形式。这个就是我们的线性模型。 如果 Regression 输出是一个实数，利用一种线性关系，就是我们图中下方的公式： \\[ \\begin{align*} y_i = \\beta_0+\\beta_1x_{i1}+...+\\beta_px_{ip}+\\varepsilon_i = x_i^T\\beta + \\varepsilon_i, \\qquad i = 1, ..., n, \\end{align*} \\] 我们把它的关系假设成是一种线性关系，输出是一系列的实数，这个是一种回归现象，我们就把这个叫做线性回归。 图下方的式子是线性关系，Regression 是回归现象。我们就把要拟定的 f(x) 叫做线性回归。 假如 y 是一个向量，x 是一个矩阵，y 等于 x 矩阵和\\(\\beta\\)矩阵做相乘运算。 \\[ \\begin{align*} y &amp; = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots y_n\\end{bmatrix}, y = X\\beta + \\varepsilon; \\\\ \\\\ X &amp; = \\begin{bmatrix} x_1^T \\\\ x_2^T \\\\ vdots \\\\ x_n^T \\end{bmatrix} = \\begin{bmatrix} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1p} \\\\ 1 &amp; x_{21} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{np} \\\\ \\end{bmatrix}, \\\\ \\\\ \\beta &amp; = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}, \\varepsilon = \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\\\ \\end{bmatrix} \\end{align*} \\] 这个也是咱们之后做深度学习的时候之所以会经常接受矩阵的一个原因。 线性回归，刚给大家把原理讲了。现在咱们来演示一个非常基本的例子。 我给大家演示这个线性回归，用了一个非常经典的数据集。这个数据集叫做波士顿房价问题，很久前我在学习大数据的时候也成用过这个数据集。 其实我还曾经做过一个一线城市房价的研究，包括北京、上海等地区。但是为什么我没有用这些数据集，而是使用了一个很古老的波士顿地区房价数据集？ 因为波士顿这个房价，和 room size，地点，地铁，高速路，周围的犯罪率等等有一个比较明显的关系。所以观察关系比较容易。但是北京的房价有个特点，它和远近没有关系，就是五环六环，也有些房子会很贵，三环也有房子会比较便宜。 和房屋的状况关系也不大，就是有的很老但是也是很贵。他唯一一个有关系的就是学区，这是最重要的一个决定因素。基本上周围有学区，这个房子就会非常贵，尤其是在海淀区。 波士顿数据集虽然很老，但是我们主要是为了学习他背后的这个线性回归原理。 北京这个房价要预测其实很简单，就是你用关键字来预测一下，看一下它里边包不包含学区两个字，然后再看一下那个学区排名就可以了。也并不是说说简单用学区就可以，而是学区对于北京房价的影响是最大的，别的因素都没有那么明显。 不过这个数据集在 scikit-learn 的 1.0 版本中被弃用，更甚的是在 1.2 版本中已经删除，所以我们要想使用这个数据集还需要费一番功夫。 我们可以使用公开库 openml 来进行下载： 123import pandas as pdfrom sklearn.datasets import fetch_openmldataset = fetch_openml(name='boston', version=1, as_frame=True, return_X_y=False, parser='pandas') 这其中，name就是数据集的名称，version为版本， return_X_y是下载拆分的特征和标签还是字典，False 是默认值，下载的会是字典，如果设定为 True，这需要两个变量分别接收特征和标签。 1data_x, data_y = fetch_openml(name=&quot;boston&quot;, version=1, as_frame=True, return_X_y=True, parser=&quot;pandas&quot;) 然后我们来处理一下数据： 123456data = dataset['data']target = dataset['target']columns = dataset['feature_names']dataframe = pd.DataFrame(data)dataframe['price'] = targetdataframe.head() 在我们获取的数据dataframe中，我们可以使用一个corr()， show the correlation of dataframe variables， correlation是相关系数。 那么相关系数的关系就是，如果一个值的增大，会引起另外一个值一定增大，而且是定比例增大，相关系数就越接近于 1。如果是 0，就是两者之间没有任何关系。那如果是 -1 呢，就是一个值增大，另外一个值就一定见效，而且减小是成相等比例的。 那我们来看一下dataframe的correlation之间的关系： 1dataframe.corr() 当然，我截图不全。小伙伴们下去自己去执行看看。 现在我们得到了一个 14 乘 14 的一个矩阵，我们可以通过seaborn的heatmap来图形化，方便我们更直接的看到其相关性。 在这张图中，越接近于黑色就越呈负相关，越接近于这个浅色就越是正相关，越接近于 1. 比方说 prime 和 prime 是 1, 也就说把 price 当成因变量，再把 price 也当成自变量的时候这两个值是一个增加另外一个一定增加。 price 除了自己本身之外，最亮的是 RM，这是和 price 相关性最大的一个。我们去查询数据源，RM 就是小区平均的卧室个数。 换句话说这个小区如果卧室越多，就意味着房子可能越大，就越是有钱人住的。 再接着找一下影响最负相关的是什么？就是哪一个值的增大会引起房价的降低。 最下面的 LSTAT 是最负面影响的，只要 LSTAT 增大，房价就会明显的随之下降。LSTAT 是什么呢？LSTAT 就是一个小区中的低收入人群在周围的额比例，比例越高，那么房价就会越低。 咱们现在把这两个数值给它全部拿出来： 12rm = dataframe['RM']lstat = dataframe['LSTAT'] 现在我们发现，RM 是最房价正向影响最多的，LSTAT 是对房价负面影响最多的。现在我们要通过这两个值，因为这两个是影响房价最明显的特征，所以我们现在要建立一个模型，要根据我们已知的 RM 和 LSTAT 来预测房价是多少。 我们要假设一个关系，要建立一个模型。所谓模型其实就是假设关系。很多模型其实都是现实世界中的一种抽象和简化。 高等数学概率统计，第一册的后半部分专门有一个地方就讲相关系数的。有兴趣的回过头再去看看咱们「AI 秘籍」的数学篇。 这里，大家要知道相关系数的意义是什么就行。 我们现在假设和房价之间是一种最简单的线性关系。先从最简单的线性关系开始，假设它是线性关系的话： 12def model1(rm, lstat, w1, w2, b): return w1 * w2 * lstat + b 这样，我们就用代码简单的实现了一个典型的线性关系。 这个时候，我们通过前面所讲的内容： \\[ \\begin{align*} \\vec {x} = [x_0, x_1, x_2, ..., x_n] \\\\ f(x) = \\sum_{i \\in N} w_i \\times x_i +b \\end{align*} \\] 我们知道 x 是一个向量，wi 也是一个向量。我们来重新定义一下这个model，你会发现更简单一些： 123456def model2(x, w, b): ''' if x = (rm, lstat) w = (w1, w2) ''' return np.dot(x, w.T) + b 我们把模型重新写一下，如果每一个 x 就等于 (rm, lstat), 然后 w 就等于 (w1,w2)，就是 x 和 w 是两个向量，那么model1()就可以简写model2()的形式。 就是 x1 乘以 w1 加 x2 乘以 w2，再加上 b。 那我们写成向量形式，和model1有什么区别，或者说有什么好处呢？它的好处其实就是在后续需要添加数据的时候函数是不需要改动的。 有了这个 model，我们的目标是要获得一组 w 和 b，要能够使得对于我们的值的预测最好。 怎么预测呢？ $$ \\[\\begin{align*} loss(\\theta) &amp; = \\frac{1}{2} \\sum(f_{\\theta}(x^i)-y^i)^2 = \\frac{1}{2}\\sum(\\theta ^T - y^i)^2 \\\\ loss(\\theta) &amp; = \\frac{1}{2} \\sum|f_{\\theta}(x^i)-y^i| = \\frac{1}{2}\\sum|\\theta ^T - y^i| \\end{align*}\\] $$ 做一个 loss 函数，这个 loss 函数里，\\(\\theta\\)指的是我们所有的参数。就是在这一组参数下，xi 送到 f(x) 里面，它产生的估计的值，然后再计算和 y 之间的差别。 也就是为了获得最优的参数集合，比方说是 (w, b)，我们定义了一个 loss 函数，这个 loss 函数在\\(\\theta\\)下，我们输入一组 x: \\(loss(\\theta; \\vec{x})\\)，然后它就等于求和，i 属于所有的 i:\\(\\sum_{i \\in N}\\)， \\(f_{\\theta}(x_i)\\)减去\\(y_i\\)之后的平方： \\[ \\begin{align*} loss(\\theta; \\vec {x}) = \\sum_{i \\in N}(f_{\\theta}(x_i) - y_i)^2 \\end{align*} \\] 如果这个\\(f_{\\theta}\\)对 x 的预测值越好，就说给的 x 都能非常准确的预测出来值是多少，预测值和实际值完全一样，那么这个时候 loss 就等于 0。因为我们的\\(f_\\theta(x_i)\\)和\\(y_i\\)完全相等。两者相减必定等于 0。 当 loss 特别大的时候，其实是意味着预测值就和真实值差得很远。 在统计学里预估值往往会写成\\(\\hat y\\)，那我们就可以将式子变成如下这种形式： \\[ \\begin{align*} loss(x) = \\frac{1}{n}\\sum_{i \\in N}(\\hat y_i - y_i)^2 \\end{align*} \\] 之前的课程里咱们讲过，为了找出变量让 loss 能够取得最小值，我们可以使用梯度下降的方法。那么我们上面的式子就也可以是如下这种形式： \\[ \\begin{align*} loss(x) = \\frac{1}{n}\\sum(w_1 \\times x_1 + w_2 \\times x_2 +b - y_i)^2 \\end{align*} \\] 现在为了获得一组 (w,b), 使得 loss 最小。那写出 loss 对 w1,w2 的偏导，对 b 的偏导，就能求解出来了。 那么 loss 对于 W1 的偏导等于多少呢？ \\[ \\begin{align*} \\frac{\\partial{loss}}{\\partial{w_1}} \\end{align*} \\] 这个都不用手算，眼睛都能看出来。我们将 2 放下来， 把后边的指数 2 放下来，然后再把 X1 提出去。如果你还不会算这个，可以去复习一下导数怎么求。可以找一本高数去好好看看，也可以去我之前写的《数学篇》里去好好看一下。 \\[ \\begin{align*} \\frac{\\partial{loss}}{\\partial{w_1}} = \\frac{2}{n}\\sum_{i \\in N}(w_1 \\times x_{i1} + w_2 \\times x_{i2} + b - y_i) \\times x_{i1} \\end{align*} \\] 与此类似，loss 对于 W2 的偏导就等于： \\[ \\begin{align*} \\frac{\\partial{loss}}{\\partial{w_2}} = \\frac{2}{n}\\sum_{i \\in N}(w_1 \\times x_{i1} + w_2 \\times x_{i2} + b - y_i) \\times x_{i2} \\end{align*} \\] 对于 b 的偏导，直接就乘以 1 了： \\[ \\begin{align*} \\frac{\\partial{loss}}{\\partial{b}} = \\frac{2}{n}\\sum_{i \\in N}(w_1 \\times x_{i1} + w_2 \\times x_{i2} + b - y_i) \\end{align*} \\] 我们之前是要求什么？求 rm 和 lstat 对吧？那我们现在就可以将其中的 x1 和 x2 替换掉就可以了： \\[ \\begin{align*} \\frac{\\partial{loss}}{\\partial{w_1}} &amp; = \\frac{2}{n}\\sum_{i \\in N}(w_1 \\times rm_i + w_2 \\times lstat_i + b - y_i) \\times rm_i \\\\ \\frac{\\partial{loss}}{\\partial{w_2}} &amp; = \\frac{2}{n}\\sum_{i \\in N}(w_1 \\times rm_i + w_2 \\times lstat_i + b - y_i) \\times lstat_i \\\\ \\frac{\\partial{loss}}{\\partial{b}} &amp; = \\frac{2}{n}\\sum_{i \\in N}(w_1 \\times rm_i + w_2 \\times lstat_i + b - y_i) \\end{align*} \\] 写成这样之后，接下来只要在编程的时候实现出来就行了。也就是，把这一段数学翻译成代码。 我们现在来翻译一下 loss 函数： 1234567def loss(yhat, y): loss_ = 0 for y_i, yhat_i in zip(y, yhat): loss_ += (y_i - yhat_i) ** 2 return loss_ / len(yhat) 我们有一个 loss, loss 的值先等于 0，我们循环一下 (y, yhat)，然后 loss 就加等于 y_i - yhat_i 结果的平方。然后 loss 再除以 len(yhat)。 这样就是最简单的一种翻译，之前的 loss 函数就可以翻译成这样。 不过我们要知道另外一种方法，就是 NumPy 里提供了一个方法mean()，意思是求平均值。 假如说里面有 12345：mean(1,2,3,4,5)，就是把 12345 这些数字全部加起来，再求它的平均值。 那我们之前的内容就可以写成mean((yhat-y)**2), 其实就是y_i和yhat_i加起来求个平均值。 所以，我们就可以将代码写成如下这样： 12def loss(yhat, y): return np.mean((yhat - y) ** 2) 这个写法就是 NumPy 的广播方法。咱们在之前的 Python 篇中有讲到这部分内容，不记得小伙伴可以回头去翻看一下，应该是第 26 章，大家可以去看一下，为了顺利进行下去，我们这里再提一下。 比如说，我们有两个 list： 12vec1 = [1, 2, 3]vec2 = [4, 5, 6] 那么我要进行计算，你会发现会报错： 1234vec1 - vec2---unsupported operand type(s) for -: 'list' and 'list' 当然，我们可以使用for进行循环，但是这样的方法未免太过笨重。而 NumPy 中就提供了一种广播的方法： 123456vec1 = np.array([1, 2, 3])vec2 = np.array([4, 5, 6])vec1 - vec2---array([-3. -3. -3]) 将数据改变成 NumPy 的 array，其实就是把它进行向量法，这样去做减法就直接可以减了，这就是咱们代码这样改的一个原因。 接着，咱们要对 w 求偏导： 12def partial_w(x, y, yhat): return np.array([2 * np.mean((yhat - y) * x[0]), 2 * np.mean((yhat - y) * x[1])]) 这里，我们的yhat也就是\\(\\hat y\\)，其实是相当于\\(w_1 \\times x_{i1} + w_2 \\times x_{i2} + b\\)这一部分，也就是我们预计的值。 预计的值减去实际的值，再乘上 xi。假如把 rm 和 lstat 一起输入进来的话，x 是一个向量，第一个是 rm, 第二个是 x0。 接下来，对 b 求导也就很好写了： 12def partial_b(x, y, yhat): return 2 * np.mean((yhat - y)) 现在已经定义好了线性模型，定义好了 loss，定义好了偏导。我们现在就初始化一个 w，一个一行两列的数组，b 默认可以是 0，也可以是一个随机值。 12w = np.random.random_sample(size = (1, 2))b = np.random.random() 不过一般来说，w 初始化成一个 normal, 但是 b 一般要初始化成 0。至于为什么，咱们大概讲到深度学习的时候详细的来讲。 然后现在来得到 yhat，这个时候我们就要得到一个 x，w 是有的，b 是有的。 1yhat = model(x, w, b) x 怎么求解呢？就需要带一个知识点了，这个知识点就叫做batch training。就是在做机器学习的时候每次取一个或者少数几个数字来进行学习。 这个是为什么呢？其实理论上是可以把所有的数据一起放进去的，但是在真正的工作中，比方说阿里云里面那个数据那么多，在做模型的时候一下放进去，既存不下，速度还会很慢。所以随机的找几个。那这样，你就可以得到不同的yhat了： 123456789for i in range(50): for batch in range(len(rm)): # batch training index = random.choice(range(len(rm))) rm_x = rm[index] lstat_x = lstat[index] x = np.array([rm_x, lstat_x]) yhat = model(x, w, b) print(yhat) 可以得到不同的yhats，因为每次随机取的值不一样。因为他的 x 不一样，所以估计出来的 y 也不一样。 可以加一个 loss(yhat, y)，我们来看一下它的 loss。 1234567891011y = target[index]loss_ = loss(yhat, y)print(loss_)---976.1638310150673...1.7070546224396366...121.30523219624953 loss 一直比较大，偶尔会出现一个比较小的值，也是昙花一现，因为 w 是随机的，就是 loss 一直在随机波动。 现在咱们要做一件事： 1234learning_rate = 1e-5w = w + -1 * partial_w(x, y, yhat) * learning_rateb = b + -1 * partial_b(x, y, yhat) * learning_rate 然后我们每 100 下来打印一下： 12if batch % 100 == 0: print('Epoch: {} Batch: {}, loss: {}'.format(i, batch, loss_)) 讲到这，线性回归基本上原理就已经讲完了，咱们下节课再见。下节课，咱们先来总结一下本节课内容，然后咱们开讲「逻辑回归」。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/08.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"title":"09. 机器学习 - 逻辑回归","text":"[TOC] Hi，你好。我是茶桁。 上一节课，在结尾的时候咱们预约了这节课一开始对上一节课的内容进行一个回顾，并且预告了这节课内容主要是「逻辑回归」，那我们现在就开始吧。 线性回归回顾 在上一节课中，我们定义了 model，loss 函数以及求导函数。最后我们用 for 循环来完成了求导过程。本节课一开始，咱们先来对上一节课的代码做一次优化，优化后的代码也会上传到课程代码仓库内。 此部分代码依然在 08.ipynb 中。 首先，我们将之前的 model 重新更名为linear，以便知道我们这个函数是要做什么的。接着，我们把 for 循环内对 w 和 b 的偏导封装为一个函数，便于我们之后调用： 12345def optimize(w, b, x, y, yhat, pw, pb, learning_rate): w = w + -1 * pw(x, y, yhat) * learning_rate b = b + -1 * pb(x, y, yhat) * learning_rate return w, b 然后我们将整个 for 循环封装一下： 1234567891011121314151617181920212223242526def train(model_to_be_train, target, loss, pw, pb): w = np.random.random_sample(size = (1, 2))# w normal b = np.random.random() learning_rate = 1e-5 epoch = 200 for i in range(epoch): for batch in range(len(rm)): # batch trainning index = random.choice(range(len(rm))) rm_x, lstat_x = rm[index], lstat[index] x = np.array([rm_x, lstat_x]) y = target[index] yhat = model_to_be_train(x, w, b) loss_v = loss(yhat, y) batch_loss.append(loss_v) w, b = optimize(w, b, x, y, yhat, pw, pb, learning_rate) if batch % 100 == 0: print('Epoch: {} Batch: {}, loss: {}'.format(i, batch, loss_v)) return model_to_be_train, w, b 在最后呢，我们可以在调用函数之前，导入所需第三方库，然后将之前的数据处理在执行函数前获取并处理一遍： 1234567891011121314151617import matplotlib.pyplot as pltimport randomimport pandas as pdimport numpy as npfrom sklearn.datasets import fetch_openmldataset = fetch_openml(name='boston', version=1, as_frame=True, return_X_y=False, parser='pandas')data = dataset['data']target = dataset['target']columns = dataset['feature_names']dataframe = pd.DataFrame(data)dataframe['price'] = targetrm = dataframe['RM']lstat = dataframe['LSTAT']model, w, b = train(linear, target, loss, partial_w, partial_b) 为什么我们每次都要随机取一个数字呢？ 你也可以把所有的 x 全部输入进去，所有的 y 全部输入进去。但是实际上在整个场景下，比方说我们有很多的训练数据，每个训练数据都有一个 x，有一个 y。 loss 函数本来写的是 i 属于所有的 N，y_i 减去 yhat_i 的平方。但是现在如果把所有的 x 和所有的 y 在真实的场景下输入进去的话，假设现在 x 有 100 万个或者 200 万个，输入进去之后整个求解过程可能 loss 函数这个程序加载都加载不出来，会非常非常慢。 所以在实际的工作中，假如说 i 属于 D:\\(\\sum_{i \\in D}\\)，D 就是 distribution 的意思，就是随机取一些数据，然后再把随机取的一些数据求解。这样的话每一次就可以保证它可以运行。 但是这样的一个区别是什么？ 每次把所有的 x 和 y 都输入进去，这种梯度下降方式中 loss 下降是一个很顺滑的样子，这个叫做 BGD。 还有一种情况就是咱们课上用的这种剃度下降方式，每次随机取了一个随机值，叫随机剃度下降，就随机取一个数字做梯度下降，简称 SGD。这个 loss 下降就会上下波动很厉害。如我们上面展示的图。 再下来呢还有一种，它是取这两者之间，每次不是取一个，是取了多个。我们把这个叫做 MBGD。 这是三种梯度下降方式。在实际的工作中 SGD 用的最多，因为可以快速的进行梯度下降学习。 我们可以将代码修改一下： 123456789batch_size = 10for i in range(epoch): ... for batch in range(len(rm) // batch_size): indices = np.random.choice(range(len(rm))) rm_x, lstat_x = rm[indices], lstat[indices] x = np.array([rm_x, lstat_x]) y = target[indices] ... 关于这一部分内容，这里仅仅是提一下，在后面的课程中，我们还会更详细的来讲解。 我们在循环中，将原来的次数 50 替换成了epoch, epoch在机械学习里边指的是运行了整整一遍。 在第二个循环内，里面是 rm 个东西，每次都是随机取，我们随机取了多少次呢？取了 rm 个。也就是说平均每个样本会被取样一次。 这就是数据量大的好处，当数据量很大的时候，有个别的点没有取到或者说有个别的点取了多次其实对最终的效果是不影响的。 也就是说因为数量很大，所以一两次的变化，一两个数值取的少了或者取的多了，其实不是非常影响。 我们把每次epoch的 batch 打出来，我们来看一下： 1234567891011121314151617def train(...): ... losses = [] for i in range(epoch): batch_loss = [] for batch in range(len(rm)): ... batch_loss.append(loss_v) ... losses.append(np.mean(batch_loss)) return model_to_be_train, w, b, lossesmodel, w, b, losses = train(linear, target, loss, partial_w, partial_b)plt.plot(losses)plt.show() 那如果是上面我们更改的代码，使用了batch_size控制之后，图形就完全不一样。 可以看到，这个 loss 下降还是挺明显的。 这个时候，我们假设知道一组的 rm 等于 19,lstat 等于 7。而此时其实已经有了 w 和 b，求到最终的 w 和 b，就能够有一个预测值了。 1234567predicate = model(np.array([19, 7]), w, b)print(predicate)---Epoch: 0 Batch: 0, loss: 46.17245060319155...Epoch: 199 Batch: 0, loss: 0.2053457975383563 我们在这个实例中，只用了两个最显著的特征，如果把 x 的维度变多一些，其实就会更加接近了。 好，这个线性回归的过程，其中包括线性函数的定义，为什么要用线性函数，loss 函数的意义，梯度下降的意义就都讲完了。 这个内容是我从斯坦福大学的参考书上弄过来的。 除了定义一个这样一个平方值的 loss，可以定一个绝对值 loss，都是一样的，都可以实现找到最优值。 只不过这个二次方的这个 loss 对于结果，它的惩罚会更大一些。 经过这一段代码的洗礼，对于之前的那个数学式子应该能看的更明白一些了。 逻辑回归 我们讲完了线性回归，下面再跟大家来讲一下逻辑回归。 逻辑回归是什么？假如还是如上那个问题，前面代码都没变。当然，库需要再导入一遍： 123456import randomimport pandas as pdimport numpy as npfrom sklearn.datasets import fetch_openmldataset = fetch_openml(name='boston', version=1, as_frame=True, return_X_y=False, parser='pandas') 现在咱们要变一个问题场景，我们先打印一下np.percentile()，这是要求百分位，比方说我们填入一下target，其实我们数据预处理的时候知道，就是dataframe['price']: 1print(np.percentile(target, 66)) 我们写入一个target, 其实就是price，然后我们在后面写了一个 66，也就是说，我们将这里所有的 price，也就是房价，做了一次排序，然后，我取从 0 到 100 中的第 66% 个位置的数值，就是大于 2/3 的房价。同样的，如果我这里填了一个 50，那么就是取最中间的那个值。 输出的结果为 23.53, 是 23 万美金。还是比较便宜，23 万美金折合 100 多万。 好，现在我们来做一个判断： 1234greater_then_most = np.percentile(target, 66)dataframe['expensive'] = dataframe['price'].apply(lambda p: int(p &gt; greater_then_most))print(dataframe[:20]) 我们定义了一个expensive，在 dataframe 中加入了这个特征。这个特征在房价大于 2/3 房价的时候 int 为 1，否则为 0。 做了这样一件事之后，就是问这个房子是不是贵房子，如果是 1，就是贵房子，0 就不是贵房子。根据我们添加的特征来进行判断。 那接着呢，问题发生了改变。我们不知道这个房子的price，现在需要进行预测这个房子是不是属于一个高档小区。在预测中，假如是 1，就表示是高档小区，0 就表示不是高档小区。现在要根据它的一些特征来猜测它是不是高档小区。 我们刚刚其实已经知道，所谓的高档小区其实是和价格有一定关系的。 假如说现在咱们有一个问题要求解，现在要有一个模型能够预测它到底是 1 还是 0，或者我们要预测是开心还是难过，咱们现在只要做一件事情就可以，就是把我们期望目标标成 1，把另外那个相对的目标标成 0。 如果我们能够拟合一个函数，这个函数的输出要么是 1，要么是 0，我们让这个模型的值越接近于实际的值就可以了。 比方说刚刚回顾完的线性回归，给定的 (x, y) 里边，y 这个值它是一个实数。如果现在变成了 0、1。比方说 1 就是 happy，0 就是 sad。或者还是用咱们之前定义的：1 就是 expensive，0 就是 not expensive。 把 1 和 0 认为是概率，如果是概率的话，1 就是 100% 是，0 就 100% 不是。 那么咱们之前的 model 输出的是实数R, 这次需要的 model 就是输出的是 0~1。这个模型的任务就变成了如果 x 给定的是 1，那么 model 输出最后要尽可能的接近 1。 怎么样才能让我们的 model 输出是 0 到 1 之间呢？有一个方法，一个函数叫做 logistic 函数，logistic function: \\[ \\begin{align*} J(\\theta)=-\\sum_i(y^{(i)}log(h_{\\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))) \\end{align*} \\] 这个函数其实在复杂系统里面是一个很重要的函数，人们其实是期望获得一种导数，数学家们研究的是这个：y’ = y(1-y), 就是 y 的导数等于 y 乘以 1-y。研究完了之后发现有一种函数就满足这个特征： \\[ \\begin{align*} f(x) = \\frac{1}{1+e^{-x}} \\end{align*} \\] 这个函数的值画出来，就是这个样子： 值全部是从 0 到 1 之间，中间与 y 轴交点为 0.5。 给大家讲一下这个原理，逻辑函数原本是想研究 y’=y(1-y)，求解出来有这样一个函数满足这样的特征：f(x) = 1/1+e^{-x}。那我们这里需要注意一下，这个特征以后会有大用。把它的图形画出来呢，就是如上图这样的一种函数，这个函数值就是在 0～1 之间。 为什么我们要用逻辑函数来做概率预测呢？首先第一个原因就是因为它的值本身输出就是 0～1 之间，天然的适合做概率这块，第二，他还处处可导，逻辑函数它是处处可导的。 所以我们就可以用这个函数来进行分类，可以把原来的模型 f(x)=wx+b, 这整个模型写成： \\[ \\begin{align*} f(x) = \\frac{1}{1+e^{-(wx+b)}} \\end{align*} \\] 原来的 f(x) 拟合的是等于 wx+b, 现在把这个 f(x) 变成如上式的样子。这个输出的就变成 0-1 了，就能够让它的值在 0 到 1 之间变换。 这就是为什么我们把这种方法叫做逻辑回归的原因。就是它是在回归曲线上加了一个逻辑函数，所以我们称其为逻辑回归。 加上逻辑函数虽然输出的值是 0～1 之间，但其实是在做分类。越接近于 1 就越近于一类，越近于 0 就越近于另一类。逻辑回归本质上就是在做分类。 接着，咱们来上代码给大家详细的讲解一遍。 其实我们整个代码和之前实现的线性回归非常的像，唯一的区别是我们需要一个叫做 sigmoid 的函数，也就是逻辑函数。 12def sigmoid(x): return 1/(1+np.exp(-x)) 我们来把这个函数画出来看看是什么样的： 12plt.plot(sigmoid(np.linspace(-10, 10)))plt.show() 其实，机器学习是个很简单的问题。机器学习其实是计算机里面最简单的几个部分，哪些比这更复杂呢？第一个、编译器原理，还有程序设计语言与自动机，还有计算机图形学，复杂系统，还有计算复杂性，操作系统。其实这些都比 深度学习复杂的多。 为什么我们现在深度学习用的多，就是因为深度学习简单。所以我说这些题外话是想告诉大家，在学习这个的时候不要有什么顾虑和负担，放松一点，放开膀子撸起袖子干就完了。 好，我们现在把 model 写出来： 12def model(x, w, b): return sigmoid(np.dot(x, w.T) + b) 那么来看，我们现在如果要预测，给一个 rm 和 lstat 输入进去，一个 RM 和 LSTAT 的值输入进去。 我们先来看一下真实的值是怎样的： 123456789101112rm = dataframe['RM']lstat = dataframe['LSTAT']target = dataframe['expensive']epoch = 200for i in range(epoch): for b in range(len(rm)): index = random.choice(range(len(rm))) x = np.array([rm[index], lstat[index]]) y = target[index] print(x, y) 这个呢就是我们在训练时候每个给的数据，每次给他给一组数据，然后给它的这个值到底是 0 还是 1。我们期望的是求解一组 (w,b)，能够让它输入 x 的时候也能得到 0 或者 1。就它真实的时候是 0，期望的是这一组输入进去之后，根据 (w,b) 运行完了之后也是 0。这个就是我们的目标。 假如已经获得线性回归了，然后要通过线性回归加一些东西想实现 0 和 1 的分类。之前我们在线性回归那里是不是先定义了一个 loss 函数？把 loss 函数定义清楚之后再对 loss 求偏导就可以了。那这里也是一样，需要定义 loss，只要把 loss 定义出来之后给 loss 求偏导就可以了，和之前一模一样。 现在的问题就转变成，咱们怎么求 loss 呢？ 我们的目标给定如果是 0 那么 yhat 也要是 0。y 是 1 的时候 yhat 也得是 1。如果 y 等于 1 的情况下，yhat 等于 0，就意味着错的很厉害啊。相对的，y 等于 1，yhat 等于 0 也同样是错的很厉害。 那么，如果 y 等于 1 的时候，yhat 等于 0.9, 错的就比较少。yhat 等于 1 的时候，错误就是 0，也就是没错误。 那么我们就可以写-log(yhat), 把这个写出来就是这样一个函数：当它越接近于 0 的时候，loss 值会接近于无穷大，当它接近于 1 的时候，loss 会接近于 0。 当 y 等于 1 的时候，loss 可以等于-log(yhat)。如果 y 等于 0，lose 值就越接近于无穷大。这个时候 loss 就可以写成-log(1-yhat)。 那么现在这里就出现一个问题，也是通常面试时候的一个高频题：为什么在逻辑回归里，loss 函数不直接写成 1-yhat？ 就是，如果 y 等于 1 的情况下，loss 函数不直接写成 1-y, 当 y 等于 0 的时候，loss 不直接写成 y。 因为这样会导致这条线呈现出一个直线，所有的偏导结果都是一致没有发生变化。 就好比有一个孩子考试成绩特别差，假如现在的目标是等于 1，他的成绩特别特别差，0.001。现在的这个梯度还是比较小，他考特别好的时候这个梯度还是一样。 但是我们知道，梯度代表了接下来的变化方向和力度。这个就是我们为什么要用这个的原因。 当然，其实还有一些概率上的解释，这里就不继续延展着讲了。 对于上面讲的，当 y=1 和 y=0 的两个不同的 loss 函数，可以做一个归纳，写成一个 loss 函数： \\[ loss = - (ylog\\hat y + (1-y) log(1-\\hat y)) \\] 为什么能够变成这样呢？我们来分析一下，如果 y=0 的时候，那 ylog(yhat) 就等于 0，也就是说仅剩下 (1-y)log(1-yhat)，反过来，当 y=1 的时候，等式后面部分就等于 0，仅剩下 ylog(yhat)。 那下面，我们就来完成代码来实现： 12def loss(yhat, y): return -np.sum(y*np.log(yhat) + (1-y)*np.log(1-yhat)) lose 函数求解出来之后，对于 (w,b) 怎么求偏导呢？ 那么其实式子就可以变成： \\[ \\begin{align*} &amp; -(ylog \\sigma (wx+b) + (1-y)log \\sigma (1-(wx+b))) \\\\ &amp; -(ylog \\sigma (w_1x_1+w_2x_2+b) + (1-y)log \\sigma (1-(w_1x_1+w_2x_2+b))) \\end{align*} \\] 那么对其求偏导，一系列推导完成后就可以变成： \\[ \\begin{align*} \\frac{\\partial loss}{\\partial w_i} &amp; = \\sum(\\hat y - y) x_i \\end{align*} \\] 我们来完成其函数代码，和线性部分一样，包含对 w 和 b 求导两部分： 12345def partial_w(x, y, yhat): return np.array([np.sum((yhat-y) * x[0]), np.sum((yhat-y) * x[1])]) def partial_b(x, y, yhat): return np.sum((yhat - y)) 那接下来我们干嘛？上节课的内容还有印象吗？接下来我们要给 w,b 随机值对吧？ 12w = w = np.random.random_sample((1,2))b = np.random.random() 接着我们修改上面实现过的对真实值的实现代码，删掉我们曾经打印的 (x,y)，然后利用我们实现的 loss 函数和偏导函数来计算预测值。 123456789101112131415161718192021222324252627rm = dataframe['RM']lstat = dataframe['LSTAT']target = dataframe['expensive']learning_rate = 1e-5epoch = 200losses = []for i in range(epoch): batch_loss = [] for batch in range(len(rm)): index = random.choice(range(len(rm))) x = np.array([rm[index], lstat[index]]) y = target[index] # print(x, y) yhat = model(x, w, b) loss_v = loss(yhat, y) w = w + -1 * partial_w(x, y, yhat) * learning_rate b = b + -1 * partial_b(x, y, yhat) * learning_rate if batch % 100 == 0: print('Epoch: {}, Batch: {}, loss:{}'.format(i, batch, loss_v)) losses.append(np.mean(batch_loss)) 执行完之后，我们可以看到 loss 在慢慢的变小。 现在我们在数据中随机取一些数据，比如说我们去 100 个吧，用于去预测，检验我们的模型： 1234567891011121314random_test_indices = np.random.choice(range(len(rm)), size=100)for i in random_test_indices: print('RM:{}, STAT:{}, TARGET:{}, PRE:{}'.format(rm[i], lstat[i], target[i], model(np.array([rm[i], lstat[i]]), w, b))) ---RM:6.425, STAT:12.03, TARGET:0, PRE:[0.15662289]...RM:5.0, STAT:31.99, TARGET:0, PRE:[4.87033539e-06]...RM:8.247, STAT:3.95, TARGET:1, PRE:[0.9623407]...RM:7.686, STAT:3.92, TARGET:1, PRE:[0.95077171] 我随机展示了一些数据，我们从这里能看到，预测值内有的值偏向 0，有的值甚至比 1 还要大。对比前面的 TARGET 真实值来看，预测的大部分还是准确的。 不过这个时候还是有问题，我们做这个预测的初衷是为了要做分类，也就是到底是 0 还是 1，那 PRE 值到底是什么，怎么分类呢？咱们就要牵扯到一个东西：dicision boundary。 也就是决策的边界，咱们假定为 0.5，让我们拿到的预测值去和这个边界值做对比，大于它的就是 1，小于的就是 0: 12dicision_boundary = 0.5predicate_label = int(predicate &gt; decision_boundary) 有了这个之后，我们需要更改下我们之前的代码： 123456789random_test_indices = np.random.choice(range(len(rm)), size=100)decision_boundary = 0.5for i in random_test_indices: x1, x2, y = rm[i], lstat[i], target[i] predicate = model(np.array([x1, x2]), w, b) predicate_label = int(predicate &gt; decision_boundary) print('RM:{}, LSTAT:{}, EXPENSIVE:{}, Predicated:{}'.format(x1, x2, y, predicate_label)) 更改完之后我们执行，和真实值进行对比，我们发现整个预测的还算事准确。当然也有部分预测错误的。 现在这个模型能够预测出来了，根据两个值能够预测出来它到底属于一个高档房子，还是不属于一个高档房子。但是我们会发现其实还有算错的地方。那么现在要问，如何衡量模型的好坏？以下就是我们要继续研究的问题： accuracy 准确度 precision 精确度 recall 召回率 f1, f2 score AUC-ROC 曲线 这些就是我们用于衡量模型的一些指标，通过这个，我们要引出一个非常重要的概念，就是过拟合和欠拟合 (over-fitting and under-fitting)。我们可以说，整个机器学习的过程，就是在不断的进行过拟合和欠拟合的调整。那么这些呢，就是我们下面课程的内容了。 目前来讲，我们学习了监督学习里面最重要的线性回归和逻辑回归，接下来什么我们要去学的 LSTM，CNN 等等，其实都是为了提高这个准确度所要做的事情。 也就是，现在我们发现虽然模型还是稍微有一些错误，这个时候就需要一起来再研究一下如何衡量模型的好坏。只有知道了如何衡量模型的好坏，才知道怎么样去调整它，怎么去优化它。 好，那下节课记得不见不散。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/09.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"title":"11. 机器学习 - 评价指标 2","text":"Hi, 你好。我是茶桁。 上一节课，咱们讲到了评测指标，并且在文章的最后提到了一个矩阵，我们就从这里开始。 混淆矩阵 在我们实际的工作中，会有一个矩阵，这个矩阵是分析结果常用的。 我们来看看具体是什么意思。 所谓的True condition, 指的是真实值，Predicted condition，指的是预测值。 其中行表示，Predicted condition positive表示预测值是 1，Predicted condition negative表示预测值是 0。 列表示则为：Condition positive表示真实值是 1， Condition negative表示真实值是 0。 这样行列交叉就组成了这样一个矩阵。这个矩阵叫做混淆矩阵，英文名字叫做 Confusion Matrix. 这个混淆矩阵是什么意思呢？ True Positive 意思就是预测值是 1，预测对了，True negative意思是预测值是 0，预测对了。那相对的， False positive意思就是预测值是 1，预测错了， False negative意思就是预测值是 0，预测错了。 混淆矩阵在常见的机器学习里边是一个很重要的分析工具： 123456from sklearn.metrics import confusion_matrixconfusion_matrix(true_labels, predicated_labels)—array([[59, 6], [ 6, 29]]) 我们可以直接看看这个方法的源码里有相关说明： 12345678910??confusion_matrix---def confusion_matrix( ... the count of true negatives is :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is :math:`C_{1,1}` false positives is :math:`C_{0,1}`. ... tp 实际上是1预测值是1，tn 实际是0预测是0, fp 实际是0预测是1 fn 实际是1预测是0。 這個時候我們再回頭來看上节课结尾处的那个公式： \\[ \\begin{align*} Precision &amp; = \\frac{tp}{ tp + fp} \\\\ Recall &amp; = \\frac{tp}{tp + fn} \\end{align*} \\] 很多人看到这个就有点晕，其实很简单。切换成我们刚才查看源码时查询到的就就成了这样： \\[ \\begin{align*} Precision &amp; = \\frac{C(0, 0)}{ C(0, 0) + C(1, 0)} \\\\ Recall &amp; = \\frac{C(0, 0)}{C(0, 0)+ C(0, 1)} \\end{align*} \\] tp 是实际上是 positive, 预测也是 positive. fp 就是实际上并不是 positive，但是预测的值是 positive. 那么 tp+fp 就是所有预测为 positive 的值。所以 precision 就是预测对的 positive 比上所有预测的 positive. fn 指的是实际上是 positive, 但是预测值并不是 positive 的值。所以 tp+fn 就是所有实际的 positive 值，recall 就是预测对的 positive 比上所有实际的 positive 值。 我们这样对比着矩阵和公式来理解 Precision 和 Recall 是不是就清晰了很多？这就是 position 和 recall 根据混淆矩阵的一种定义方式。 刚刚讲了 baseline, baseline 是在做评估的时候要知道结果一定要比什么好才行。如果是个二分类问题，基本上是一半一半，准确度是 50%, 那基本上就没用。 Precision 和 recall 这两个是针对于分类问题进行评价，那我们怎么解决回归问题的评价呢？ 回归问题，它也有一个 accuracy 如下： \\[ acc(y, \\hat y) = \\sum_{i \\in N}|y_i - \\hat y_i| \\\\ acc(y, \\hat y) = \\sum_{i \\in N}|y_i - \\hat y_i|^2 \\\\ acc(y, \\hat y) = \\sum_{i \\in N} \\frac{|y_i - \\hat{y_i}|}{|y_i|} \\] 除此之外，regression 问题里面有一个比较重要的评价方式叫做R2-scoree: \\[ R^2(y, \\hat y) = 1 - \\frac{\\sum_{i=1}^n(y_i - \\hat y_i)^2}{\\sum_{i=1}^n(y_i - \\bar y)^2} \\] 第一种情况：如果所有的 y_i 和 yhat_i 的值都相等，那么 R2(y, yhat) = 1 第二种情况：如果所有的 yhat_i 是 y_i 的平均值，那么 R2(y, yhat) = 0 第三种情况：如果 R2 的值比 0 还小，就意味着它还不如我们做统计求平均值，瞎猜的结果。也就是连 baseline 都没达到。 R2-scoree 之所以常常会被用于进行回归问题的评测，主要的原因就是它防止了机器作弊。 比方说我们现在有一组数据，这组数据实际都是 0.99, 0.97, 0.98..., 这些数字都很小，而且都很密集。那么给机器使用的时候随便做一个平均值，感觉到准确度还挺高，那就被骗了。 F-score 在 precision 和 recall 之外，还有一个比较重要的内容，叫做 F-score. 首先我们要知道，precision 和 recall 这两个值在实际工作中往往是相互冲突的。为了做个均衡，就有了 F-score. \\[ \\begin{align*} F-score &amp; = \\frac{(1+\\beta^2) * precision \\times recall}{\\beta^2 * precision + recall} \\end{align*} \\] \\(\\beta\\)是自行定义的参数，由这个式子可见 F-score 能同时考虑 precision 和 recall 这两种数值。分子为 precision 和 recall 相乘，根据式子，只要 precision 或 recall 趋近于 0，F-score 就会趋近于 0，代表着这个算法的精确度非常低。一个好的算法，最好能够平衡 recall 和 precision，且尽量让两种指标都很高。所以有一套判断方式可以同时考虑 recall 和 precision。当\\(\\beta \\to 0\\), F-score 就会退化为 precision, 反之，当\\(\\beta \\to \\infty\\), F-socre 就会退化为 recall. 我们一般说起来，F-score 没有特别定义的话，就是说\\(\\beta\\)为 1, 一般我们写成 F1-score. \\[ \\begin{align*} F1-score &amp; = 2 \\times \\frac{precision \\times recall}{precision + recall} \\end{align*} \\] F1-score 是仅当 precision 和 recall 都为 1 的时候，其值才等于 1. 而如果这两个值中任意一个不为 1 时，其值都不能等于 1. 也就是说，当 2*1/2 = 1 时，F1-score=100%, 代表该算法有着最佳的精确度。 AUC-ROC 除了 F-score 之外，还有比较重要的一个概念：AUC-ROC. 这个也是为了解决样本不均衡提出来的一个解决方案。 首先我们要先了解 ROC 曲线 (receiveroperating characteristic), ROC 曲线上的每一个点反映着对同一信号刺激的感受。AOC(Area under Curve), 是 ROC 曲线下的面积，取值是在 0.1 ~ 1 之间。 我们直接来看看，它在实际场景下是怎么用的。 还记得咱们在之前设定的阈值decision_boundary = 0.5, 我们就拿这个阈值来看。threshold:0.5. 在我们二分类问题中，当预测值大于 0.5 的时候，也就等于 1 了。也就是说，只要超过 0.5, 我们就判定为 positive 值。 好，现在还是的请我们劳烦了无数次的警察 a 同志来帮帮我们。当警察 a 去抓罪犯的时候，盘但一个人是不是犯了罪，他的决策很重要。在事实清晰之前，警察 a 的决策只有超过 0.5 的时候，才能判定这个人是 positive，也就是罪犯。这个时候呢，我们假设 precision 是 0.7. 现在又需要警察 b 出场了，这个警察 b 的 threshold 为 0.1 的时候，其 precision 就为 0.7. 也就是说，他预计出的值，只要大于 0.1, 就判定为 positive, 这种情况下，警察 b 判定的 precision 为 0.7. 别急，这次需要的演员有点多，所以，警察 c 登场了。那么警察 c 的 threshold 为 0.9. 也就是说，警察 c 比较谨慎，只有非常确定的时候，才能判定 positive. 警察 c 的情况，判定的 precision 也是 0.7. 好，现在我们来用脑子思考下，这三个警察哪个警察能力最强？ 必须是警察 b 最厉害。 就如我们上面的那四个坐标轴，X 轴代表 threshold, Y 轴表实 positive, 当 threshold 轴上的取值还很小的时候，positive 已经很大了。那明显紫色线条和 threshold 轴圈住的区域面积越大，这个面积就是越大越好。 这就是 AUC for ROC curves, 这个主要就是为了解决那些样本及其不均衡的问题。因为样本非常不均衡的时候，position 和 recall 你有可能都会很低，这个时候就不好对比。AUC 曲线对于这种情况就比较好用一些。 其实在真实情况下，绝大多数问题都不是很均衡的问题。比方说预测病，找消费者，找高潜力用户。换句话说，如果高潜用户多就不用找了。 我们在研究 ROC 曲线实际应用的时候，依然会用到上面给大家所讲的 tp, fp, fn, tn. 这里会引出另外两个东西，TPR 和 FPR, 如下： \\[ \\begin{align*} TPR &amp; = \\frac{tp}{tp+fn} \\\\ FPR &amp; = \\frac{fp}{fp+tn} \\end{align*} \\] 我们来看看咱们之前的这组数据的 AUC 值： 123456789from sklearn.metrics import roc_curve, aucfpr, tpr, thresholds = roc_curve(true_labels, losses)roc_auc = auc(fpr, tpr)print('AUC: {}'.format(roc_auc))---AUC: 0.9300356506238858 下一节课，咱们来说一个非常重要的概念：拟合和欠拟合。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/11.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%872/"},{"title":"10. 机器学习 - 评测指标","text":"Hi，你好。我是茶桁。 之前的课程中，我们学习了两个最重要的回归方法，一个线性回归，一个逻辑回归。也讲解了为什么学习机器学习要从逻辑回归和线性回归讲起。因为我们在解决问题的时候，有限选择简单的假设，越复杂的模型出错的概率也就越高。 本节课中，我们要继续我们未完成的内容。 还记得，咱们上一节课中最后所说的吗？在完成了基本回归之后，该如何去判断一个模型的好坏，以及如何调整和优化。 好，我们开始本节课程。 PICKLE 本节课中，会重点的给大家做一件事，叫「评测指标」。 在这之前，我们发现了一个麻烦事。就是我们现在需要去观测我们的分类结果，我们不得不再去执行一遍我们之前的训练程序，拿到最后的分类结果： 123RM:6.38, LSTAT:24.08, EXPENSIVE:0, Predicated:0...RM:6.319, LSTAT:11.1, EXPENSIVE:1, Predicated:0 这很麻烦，训练结果每次要使用的时候都需要运行一次，这样非常的麻烦。现在我想要把这个 model 不要每一次都训练一下，而是要把它做一个保存，下次用的时候不需要从头到尾再训练一次。 现在现在，可以给他做一个 persistence，做一个留存。现在就是要做这么一件事情。 123456789101112131415import picklewith open('logistic_regression.model', 'wb') as f: pickle.dump(model, f)with open('w.model', 'wb') as f: pickle.dump(w, f)with open('b.model', 'wb') as f: pickle.dump(b, f)print('pickle finished')---pickle finished 并且最后我得到了三个文件，分别是logistic_regression.model, w.model以及b.model。 现在就可以把训练完成的 model 做保存了。之后我们用 Pytorch, tenserflow 之类的做，它都有这样的功能。 到这一步之后，我们上一节上所写的代码就可以暂时不用了。不过为了整个代码的完整性，我仍然将其又在本节课的10.ipynb内些了一遍。 那么，我们要用的时候怎么办呢？如果要用这个对象的时候，将我们之前对文件操作的代码拿过来，然后将其中的wb参数改成rb，然后再将二进制文件读取一遍： 12345678910with open('logistic_regression.model', 'rb') as f: model_r = pickle.load(f)with open('w.model', 'rb') as f: w_r = pickle.load(f)with open('b.model', 'rb') as f: b_r = pickle.load(f)print('pickle read finished') rb的意思是read binary，也就是读取二进制文件。然后，为了在测试的时候避免混乱，让我接下来所使用的文件使用的是我重新读取的模型而不是之前训练时生成的的，我将重新读取的这几个文件命名为model_r，w_r,b_r。 那再之后，虽然不用重新训练了，但是数据还是要读取一遍的，并且，按照训练数据的规则重新整理好，都完善了之后，就可以开搞进行分类了。 12345678910111213141516171819202122232425262728import pandas as pdfrom sklearn.datasets import fetch_openmldataset = fetch_openml(name='boston', version=1, as_frame=True, return_X_y=False, parser='pandas')data = dataset['data']target = dataset['target']dataframe = pd.DataFrame(data)rm = dataframe['RM']lstat = dataframe['LSTAT']dataframe['price'] = dataset['target']greater_then_most = np.percentile(dataframe['price'], 66)dataframe['expensive'] = dataframe['price'].apply(lambda p: int(p &gt; greater_then_most))expensive = dataframe['expensive']random_test_indices = np.random.choice(range(len(rm)), size=100)decision_boundary = 0.5for i in random_test_indices: x1, x2, y = rm[i], lstat[i], expensive[i] predicate = model_r(np.array([x1, x2]), w_r, b_r) predicate_label = int(predicate &gt; decision_boundary) print('RM:{}, LSTAT:{}, EXPENSIVE:{}, Predicated:{}'.format(x1, x2, y, predicate_label)) 评测指标 好，解决了模型的重复使用之后，我们再回到课程中继续。 很多人在学习过程中，会觉得「评测指标」是一个没有那么有趣的事情。比方说，咱们学模型，学算法，就可以去写程序，可以运行，写出来的时候会感觉还蛮酷的。但是评测指标呢，很多同学就觉得不是那么有趣。 其实，我想告诉大家，评测指标是一个非常重要的东西。好比完成任何一个任务，不管你现在是完成普通的编程任务，还是要完成一个公司的市场行为、运营行为。一般来说，越复杂的任务，只要把评价指标，评价方式做对，这个任务基本上就已经完成了一半了。 对于我们来说，工作的时候要知道，对于一个机器学习任务，能找到正确的评测指标，这个机器学习任务就已经成功一半了。 首先，来看一个问题：Losses 持续下降，到底是意味着什么呢？ 123import matplotlib.pyplot as plt%matplotlib inlineplt.plot(losses) loss 持续下降意味着误差越来越小？方向是对的？测试值更加接近真实值？更精确的说法是，它在逼近最优解，但是效果是不是特别好，还不知道。 接下来这个问题是一个比较复杂的问题，是一个难点： 1-np.sum(y * np.log(yhat) + (1 - y) * np.log(1 - yhat)) 这段代码是我们写的 loss 函数，我们现在来假设有一组数据： 1true_label = np.array([1, 0, 1, 0, 1]) # 二分类 再假设有一个模型，在执行的时候，它会知道咱们做的是一个二分类问题，那么结果就是不是 1，就是 0。这个时候模型有可能偷懒，那给到的数据就会是随机的，好吧，开个玩笑，其实就只是因为数据不足造成给到的数据过于随机： 1predicate_1 = np.array([0.8, 0.7, 0.8, 0.3, 0.8]) 然后我们执行算法来拿到结果： 123456def test_lose(y, yhat): return -np.sum(y * np.log(yhat) + (1 - y) * np.log(1 - yhat))test_lose(true_label, predicate_1)---2.2300784022072975 现在我们拿到的值为 2.23，不过要记得，咱们这只是一个假设值。那这个时候引入我们刚才谈到的 loss 的曲线，loss 是持续下降的，当它下降到最低的值的时候依然比这个 2.23 还要高，那就说明这个模型都还没有随机猜测的准确度高。 这个情况其实是经常会遇到的一个问题，你会看到你的的模型一直在下降，下降的非常好，但是一做实际测试的时候效果就特别差。 再换个说法就是，这个模型跑的时候，瞎猜的值都有 2.23 的准确，但是 loss 虽然一只在下降，一只下降到了 3。虽然 loss 看起来在下降，但是这整个结果都不是太好。 瞎猜的时候的准确度，loss 值，我们称为这个模型的 Baseline。你的值最起码要比这个好。 所以就如之前所的，loss 持续下降意味着模型在向着最优的方向在寻找，但并不意味着结果就会很好，因为有可能连瞎猜都不如。 好，以上是第一点，我们接着来看第二点。 loss 一直在下降，但是我们现在想知道的是有多少个 label 预测对了。先建立两个变量来分别存储数据： 12345678true_labels, predicated_labels = [], []...for i in random_test_indices: ... true_labels.append(y) predicated_labels.append(predicate_label) 然后分别获得了两组数据，一个是true_labels，一个是predicated_labels。有了这两组数据之后，我们来定义一个accuracy，这个是预测的值和相似的值一共有多少个是一样的。 1234567def accuracy(ytrues, ylabels): return sum(1 for yt, y1 in zip(ytrues, ylabels) if yt == y1) / len(ytrues)accuracy(true_labels, precicated_labels)---0.89 0.89, 就是说有 89% 的 label 都是猜对了。 最早的时候其实只有这一个标记，但这个标记很容易出错。 假设有一个警察局，要在 100 个人里边判断谁是犯罪分子。现在我们知道有 3 个是犯罪分子，然后警察说这 100 个人全部都是犯罪分子。那么现在准确度有多少？ 然后又有一个警察站出来说，这 100 个人都不是犯罪分子，那他的准确度又是多少？ 我们现在让第一个警察是 a，第二个警察是 b。 警察 b 有 97 个标签都说对了，这会给人一种错觉，好像他预测的很准确的。但是其实，a 和 b 两个人都判断的不准确。那我们这个时候就需要引出一个定义：Precision。 precision 也是准确度的意思，和 accuracy 不同点是，accuracy 的对比是对比目标和现有值是否匹配，匹配的就算正确。而 precision 除了看是否匹配之外，还要目标值，也就是 positive。 这里举个例子说明一下，比如我们去检测是否有新冠病毒，那么目标是为了检测出有新馆病毒的人，那么呈阳性的人就是我们的 positive，那么我们 precision 除了预测出有新冠和没有新冠的人之外，有新冠的人也需要一一对应上，也就是 positive 要正确。 如果是写代码的话，也就是将之前的 accuracy 拿过来改改就可以直接用了： 12345678910def precision(ytrues, yhats): # 预测标签是 1 的里面，正确的比例是多少 positives_pred = [y for y in yhats if y == 1] return sum(1 for yt, y in zip(ytrues, yhats) if yt == y and y == 1) / len(positives_pred)precision(true_labels, predicated_labels)---0.8333333333333334 先将预测为 1，也就是预测呈阳性的目标放到positives_pred中，再来检测一下在这些预测出来的目标中，预测对的有多少。 除此之外之外，还有一个值叫做recall，它的意思是在实际的positive里，有多少比例被找到了。 123456789def recall(ytrues, yhats): true_positive = [y for y in ytrues if y == 1] return sum(1 for yt, y in zip(ytrues, yhats) if yt == y and yt == 1) / len(true_positive)recall(true_labels, predicated_labels)---0.8064516129032258 好，我们再来复盘一下这三个值，一个是accuracy, 一个是precision，一个是recall。 accuracy就是预测值和实际值有多少是一样的。但是有可能会在实际场景都不是很均衡。 precision是拿到预测后的目标值，然后拿这些目标的实际值去比较看有多大比例是一样的。 recall是先拿到实际的目标值，然后拿目标预测值比较看有多大比例是一样的。 根据我们之前说的警察抓坏人的那个假设，我们现在来做一个测试，假设我们现在好人有 90 个，坏人有 10 个。 123people = [0] * 90 + [1] * 10import randomrandom.shuffle(people) 现在警察 a 来了，就判断说：全部都是好人，把他们全部都放了吧。这样的话，它的 accuracy 是多少呢？accuracy 就是预测的，只要是实际值的那个 label 就行。我们来看看： 12345a = [0] * 100accuracy(people, a)---0.9 我们看这个准确度就会很高，这个也能理解，因为警察 a 将这 100 个人中的 90 个好人全部判断准确了对吧？ 让我们来看看其他两个： 12345678910precision(people, a)---ZeroDivisionError: division by zero======recall(people, a)---0 precision 警告我们分母为 0，报错了。那分母为什么为 0 呢？因为 a 说了，所有都是好人，那么预测的目标值，也就是分母上的坏人就为 0。 而 recall 呢，结果为 0。这是因为分母上的坏人实际值虽然为 10，但是预测的目标值，也就是分子上为 0。那结果肯定是为 0。 本来 a 的 accuracy 是 0.9，别人还以为准确度很高，结果一个坏人都没抓住。这肯定不行。 那 b 的情况又如何呢？之前说过，b 说所有的都是坏人，统统抓起来。 1234567891011121314151617b = [1] * 100accuracy(people, b)---0.1========precision(people, b)---0.1=========recall(people, b)---1.0 虽然accuracy和precision都不高，但是似乎目标都被找出来了。颇有一种「宁可错杀 1000，不可放过一个」的感觉。 那以上这些，就是为什么要有这 3 个非常重要的指标的原因。 好，那下一节课中，我们要来看看关于precition和recall的一个矩阵，这个矩阵呢，将会是我们工作中分析结果常用的。 \\[ \\begin{align*} Precision &amp; = \\frac{tp}{ tp + fp} \\\\ Recall &amp; = \\frac{tp}{tp + fn} \\end{align*} \\] 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/10.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/"},{"title":"13. 机器学习 - 数据集的处理","text":"[TOC] Hi，你好。我是茶桁。 上一节课，咱们讲解了『拟合』，了解了什么是过拟合，什么是欠拟合。也说过，如果大家以后在工作中做的就是机器学习的相关事情，那么欠拟合和过拟合就会一直陪伴着你，这两者是相互冲突的。 现在，让我们一起来思考一个问题：overfitting，过拟合产生的原因是什么？ 如果这是在模型层面的话，参数过多还是过少？如果从数据层面来看，是过多还是过少呢？ 好，我们来揭晓答案。如果模型层面思考，那是就是参数过多。如果从数据层面来看，那是数据过少。 现在我们需要理解一件事情，这两个事情其实是一回事，数据量多和模型复杂其实是一回事。它背后的原因就是因为任何一个 f(x) 如果有很多的参数，拟合的时候随着这个参数数量越多，那么我们所需要的训练数据集也要增多。也就是说当模型非常复杂，参数特别多，只要数据量特别大，那就不算多。就说现有的数据量对于参数不够，训练力度不够。 这就好比是有一个天才的孩子，脑子极其聪明，就跟茶桁一样。哎，这个孩子呢智商极其高，但是他想事情想的特别的复杂，结果他现在见到的事情都是太过于简单的东西。那么就不能把他的这个潜力发挥出来。 好，我们接着下一个问题：如何判断一件事情有没有发生过拟合或者欠拟合呢？ 我们看这张图，假如这是一个 2 分类问题，咱们训练时候结果的准确度是 0.7 左右。那么大家想一下，这个是过拟合还是欠拟合呢？ 如果模型训练的时候效果还不错，快接近于 1 了，达到了百分之九十几。但是实际上用 validation 数据集去测的时候发现准确度下到百分之八十几，或者百分之七十几，总之就是比在训练的时候那个效果要差。这个就叫作过拟合。 咱们上节课给大家说的就是这个问题，机器学习的整个流程最终的目的不是为了把 loss 函数降到最低，我们要关心的是像 recall，precition，这种信息才是最关键的。 Training data split 接下来，咱么要再讲几个机器学习里面极其重要的几个概念，第一个是数据集的切分 (Training data split)。第二个是 Normalization。第三个，Standardized。 其实上节课，咱们已经说过了数据集的切分问题。数据集切分最主要的原因是因为我们经常会遇见过拟合的情况，为了避免我们把所有的数据拿来不断的做 training, 然后在使用的时候效果变得不好，那我们不如自己找一些数据出来做 test sets，为了可以反复多次的去检验效果好不好，就增加了一个 validation sets。 在真实环境下我们是怎么去做这样一件事呢？我们来简单的演示下： 1234567891011121314from sklearn.model_selection import train_test_splitimport numpy as npsample_data = np.random.random(size(100, 5))train, test = train_test_split(sample_data, train_size=0.8)train---array([[1.55582066e-01, 8.19437761e-01, 3.54628257e-02, 5.53248385e-01, 4.23785508e-01],... [7.24889349e-01, 1.23458057e-01, 9.74101303e-01, 1.72605427e-01, 6.59164912e-01]]) 非常的简单，我们来看，sklearn里自带了这种分割方法。我们随机了 100 行 5 列的数据，然后使用train_test_split将其分割成train和test两份，在后面的参数内设置了百分位。 这样，这个数据就做了一个拆分。值得注意的是，给大家教一个小技巧，这是第一种方法：split。其实不只是 sklearn，pytorch 和 keras 也都有 split 方法。 但是我们去看一下源码会发现，这个 split 方法是没有 validation，它的输出只有 train 和 test 两部分。 为了解决这个问题，我们可以用一个简单的方法。这次我们使用 Numpy。 12345678910indices = np.random.choice(range(len(sample_data)), size=int(0.8*(len(sample_data))), replace=True)indices---array([39, 65, 5, 13, 69, 8, 49, 2, 16, 16, 28, 28, 99, 13, 64, 76, 55, 96, 12, 87, 81, 55, 96, 54, 94, 15, 44, 23, 17, 76, 98, 84, 21, 50, 62, 58, 21, 95, 22, 3, 6, 35, 93, 34, 68, 49, 29, 81, 58, 45, 95, 26, 21, 97, 43, 30, 40, 52, 93, 34, 17, 71, 76, 38, 92, 62, 21, 98, 56, 28, 54, 39, 15, 17, 62, 81, 61, 4, 51, 71]) 这里我们等于是把它的整体的顺序打乱，后面的 replace 就是可以重复的去取。这样我们就随机的取了一些下标。 这是一个比较简单的方法，那么我们为什么要设置replace=True呢？当数量特别大的时候，多取几个少取几个其实不是很影响，另外 replace 的话，他内部的那个随机的算法其实是不一样的，速度会快的多。以后如果遇到类似的事情，你也可以去用这个方法去做它。 Normalization 除了这个以外，做机器学习的时候，要做数值的归一化 (Normalization) 和标准化 (Standardized) 这样一个动作。 我们这么做的目的是什么呢？假设我们现在有多个特征的数据集，不过我们注意到一点，就是这些特征值跨越的范围是无法进行比较的。 比如，一个特征在 1 和 10 之间变化，但是另外一个实在 1 和 1000 之间变化。如果我们忽略了这一点而直接进行建模，模型分配给这些特征的权重将会受到严重影响，模型最终会为较大的变量分配较高的权重。 现在要解决这个问题，将这些特征置于相同或者至少是可比较的范围内，那就需要对数据做一个数据归一化。 归一化的目标是讲数据缩放到特定范围内，一般来说是[0,1]或者[-1,1]之间。这有助于消除不同特征之间的尺度差异，确保它们对模型的权重贡献大致相等。 数据归一化对于每个特征 x，归一化后的值Xnormalized计算如下： 其中 min 是特征的最小值，max 是特征的最大值。这个操作确保了数据的最小值映射到 0，最大值映射到 1. 在数据预处理过程中，首先计算每个特征的最小值和最大值，然后使用上述公式对数据进行归一化。这通常通过一次遍历数据来实现。 在进行归一化的时候，我们所使用的那个公式会有一个缺点，就是它并不能很好的去处理异常值。比方说，如果有 0 到 40 之间的 99 个值，其中一个值为 100，则这 99 个值讲全部转换为 0 到 0.4 之间的值。这些数据和以前一样被压缩！下图就是个示例： 这些数据在进行归一化之后，解决的是 y 轴上堆集的问题，但是 x 轴上的问题依然存在，就像途中橙色点那个异常值： 关于这个知识点，我们来看一个极其简单的例子： 1234567some_large_number = [23421421,42155151,25531238,21826139, 32189732, 32103721]def normalize(x): return (x - np.min(x)) / (np.max(x) - np.min(x))ic(normalize(np.array(some_large_number)))---array([0.07847317, 1., 0.18225672, 0., 0.50979325, 0.5055623 ]) 我手动定义了 6 个比较大的数字，在进行处理之后我们看到了，都变成了一些特别小的数字。 同样的，对于特别小的数字，它一样可以进行处理： 12345some_small_number = [0.00000231213, 0.0005600321, 0.0000041412892, 0.000987890576, 0.0000578921764]ic(normalize(np.array(some_small_number)))--- array([0., 0.56588085, 0.00185592, 1., 0.05639333]) Standardized 那么还有就是标准化，对于标准化，其目标是讲数据转化为均值为 0，标准差为 1 的分布，也就是标准正态分布。这有助于处理偏斜分布的数据，并确保数据的均值和方差在模型中起到合适的作用。 那对于每一个特征 x，标准化的值z计算如下： \\(\\mu\\)是特征的均值，\\(\\sigma\\)是特征的标准差。这个操作使数据的均值为 0，标准差为 1。 在数据预处理的过程中，首先计算每个特征的均值和标准差，然后使用上述公式对数据进行标准化处理。标准化后的数据具有均值 0 和标准差 1，这有助于模型更好的理解和捕捉数据之间的关系。 无论是归一化还是标准化，其实依据来源都是基于线性代数的变化理论，这确保了归一化和标准化后的数据分布具有特定的属性，这些属性对于机器学习算法的表现非常有帮助。 我们来看一个标准化的例子，为了让大家更为明显的了解其意义，我做了一些非常大的数据，但是每一个都不相同。这些数据有一个特点，就是相对于数值本身的大小来说，几个数值之间的差距可以说是非常微小的： 12345678some_dense_number = [47238941, 47238946, 47238951, 47238931, 47238949, 47238936]def standarlize(x): return (x - np.mean(x))/ np.std(x)ic(standarlize(np.array(some_dense_number)))---array([-0.18752289, 0.51568795, 1.2188988 , -1.59394459, 0.93761446, -0.89073374]) 我们定义的数据实际上是非常密集，但是使用 standarlize 公式之后，就变得比较的分散，比较的均匀了。这个情况还是很多的。 ONE-HOT 在讲完 training data split, normalization, Standardized 之后，我们来看下面一点：ONE-HOT。 为什么要用 ONE-HOT？我们都直到，咱们计算机里其实都是数字，包括视频，图片，声音，文字等其实都是数字。 数字和数字其实是不一样的。比如，有一群人分成了4组： 然后有一个女生的 GPA 是4: 那么分组的4和 GPA 的4有什么区别？最明显的一个区别就是，分组的4只是一个组名，那么假如和1组交换组名并没有太大的关系，但是 GPA 的这个4如何和1交换一下，那就从 4 分变成 1 分了，那这两个是不能相互变换的。本质上，其区别就是一个可比一个不可比。 我们也就发现了，数字其实是有区别的。这个世界中，数字其实可以分成两类： 第一类叫作 Categorical，叫作分类数据，也被称为离散数据或名义数据。它们之间不能被比较，也不能被排序，这些数字也仅仅是表示一个和另外一个不一样。就我们刚才讲人群分为 1、2、3、4 组，其实分成 A、B、C、D 组也是一样的，只是表示区别。 第二类是 Numerical，数值数据，也被称为连续数据。这个是可以比较的，也可以进行排序。这种数据包括可以用来进行数学运算的实数值。 Numerical 还可以进一步分为整数和浮点数。 知道了这一点之后，那我们以后遇到类似的情况不要随便的做加减乘除。 那我们有了 Categorical 和 Numerical 这两种类型之后，会对我们有一些什么比较重要的影响？ 如果现在有一个函数，这个函数输入一个 x 向量，它输出就是分为一个 Categorical 和 numerical。 输出是 0-1 这样一个数字，是一个典型的逻辑回归。 假如有一个人在北京，年龄 27，性别男，月入一万二。然后还有一个人，生活在安徽，年龄 28，性别女，月入 8,000。第三个住在上海，年龄 28，性别男，月入一万三。 北京，27, 12000 安徽，28, 8000 上海，28, 13000 我们注意这三组数据，如果现在做一个向量表证。 关于地域，我们常常使用的方法包括邮编排序，或者使用拼音排序。假如这里我们就使用拼音首字母来进行排序，安徽假如是 1，北京是 2，上海是 27。 我们的数据进行向量化可能就会变成下面这样： 123456# 1. 北京vec(2, 27, 12000)# 2. 安徽vec(1, 28, 8000)# 3. 上海vec(27, 28, 13000) 然后我们定义一个函数： 12def f(x): return (0, 1) 非常简单一个函数，返回表示对某一样东西买还是不买。 函数的实现过程就是类似于wi * xi + b这种形式。 我们观察向量发现，就向量值而言，北京这个人和安徽这个人之间的向量差比北京和上海这两人之间的向量差还要小。 \\[ |v_1 - v_2| &lt; |v_1 - v_3| \\] 我们假如说经过函数f(x)之后，输出的结果分别为 Y1, Y2, Y3。因为 v1 和 v2 离的更近，就会有一个结果，Y1 和 Y2 的结果其实会更相似。但是其实呢，这种结果完全不对。这样乱比其实会出问题，会让程序出错。 我们现在知道，这其实是一个 Categorical 的问题。为了解决 Categorical 的这种问题，我把 Categorical 改成这样： 123北京: [1, 0, 0]安徽: [0, 1, 0]上海: [0, 0, 1] 改成这样之后这个向量就变成了这样： 123456# 1. 北京vec(1, 0, 0, 27, 12000)# 2. 安徽vec(0, 1, 0, 28, 8000)# 3. 上海vec(0, 0, 1, 28, 13000) 向量变成这样之后，就解决了我们刚刚说的那个问题。不会导致因为分类过于相似让北京和安徽向量相似度大于北京和上海的相似度。 对于这样一个向量，三组数据中改变的那个值向量值就都为\\(\\sqrt 2\\)，这一种方式就被称为 ONE-HOT。 那这种方式也是存在问题的，目前我们只去考虑三个城市。可是当存在成百上千个城市的时候，比如说 Google 地图等等这些应用。 当城市越来越多的时候，那它的维度就会变得很高： 12345Beijing = [1, 0, 0, 0, 0, 0, ..., 0]Shanghai = [0, 1, 0, 0, 0, 0, ..., 0]Chengdu = [0, 0, 1, 0, 0, 0, ..., 0]Shenzhen = [0, 0, 0, 1, 0, 0, ..., 0]... 我们想想一下，这样得有多少个地址？可能空间会极其的大，你这样的话数字光存起来得上亿个存储单元。 ONE-HOT 就有这样的问题： 耗费空间 数据量大，更新起来，效率极低 遗漏了很多重要新息 就比如，我们再增加几个人如下： 123- 重庆 27 9000- 成都 26 8500- 呼和浩特 26 8500 在这三个城市中，我们脑子里其实就直到，重庆和成都是非常接近的。但是在 ONE-HOT 里是体现不出来，其向量值依然是根号 2。 为了解决这些问题，人们就用到了更先进的一种方法：embedding， 叫作嵌入。 嵌入就是把东西放在固定的位置，这个就是嵌入的意思。在这里，就我们空间中如果有几个实体 NTT1 NTT2 NTT3，我们把这些实体放到这个空间中，要达到一个结果就是如果实体 1 和实体 2 的相似度小于实体 1 和实体 3 的相似度，这个相似度我们可以自己来定义，比如成都和重庆的生活方式，再比如重庆和北京都是直辖市。 在这个问题场景下，我们期望达到的结果是如果这两个实体相似那么他们在空间中的距离也接近。 如何实现 Embedding, 这本身是一个研究领域，是现在非监督学习，表证学习里面非常重要的一个研究领域，属于比较高级的一个知识点。 第二就是如果之后咱们学 NLP，那么一定会讲到这个，因为要把文本单词进行嵌入，到时候会学到。如果是学推荐系统的，大家也会学什么 Graph embedding，基于图的用户行为。 那之后咱们学习 NLP，其基础就是 Embedding。关于这个问题，我们其实目前了解到这里就行了。再往下延展下去，又是一个专门的研究话题。延展后的这个问题解决方案，在我们后面的课程中会等着大家去学习。 我们再来看看 ONE-HOT 的实际展示： 1234567891011121314151617181920212223array = ['北京','上海','广州','宁夏','成都','上海','北京']def one_hot(elements): pure = list(set(elements)) vectors = [] for i in elements: vec = [0] * len(pure) vec[pure.index(i)] = 1 vectors.append(vec) return vectorsic(one_hot(array))---one_hot(array): [[0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0]] 其实 ONE-HOT 非常简单，但是基本上很多面试官都喜欢问这个问题。这个问题主要就是一个可以考察一下你的 Python 编程能力，其次他可以去问一下你 one hot 的作用是什么，再者还可以往后问你 one hot 有什么缺点，怎么解决等等。一个问题就可以问你半个小时。 补充：SOFTMAX 和 CROSS-ENTROPY 好，在本节课最后，我们来做一个前面课程的补充，在今天才想起来，有一个相关的点遗漏了没有讲到。 之前我们讲过逻辑回归的 loss 函数： 假如 y=1，loss 可以等于-log(yhat), 如果 y 等于 0，loss 就可以写成-log(1-yhat)。两个合并后就组成了最终的 loss 函数： \\[ loss = -(ylog\\hat y + (1-y)log(1- \\hat y)) \\] 那么，这个是解决二分类的，结果才不是 0 就是 1。现在的问题就是如果我们要解决多分类的问题怎么办。 如果要解决多分类的话，需要把 x 变成一种能预测多分类的东西。那最终 yhat 可以表示成 \\(\\hat y = (0.25, 0.20, 0.75)\\)。 也就是，现在要表示三个类别，那我们可以用三个小数来表示。这个向量经过各种计算，如果能够变成一个三维的向量，然后再去优化里边的参数就可以做到。 那这也就代表的是类别 1、类别 2、类别 3 的概率。ytrue 就可以写成 yhat 的形式，就变成 (1, 0, 0)。 就是我们给定一个\\(\\vec x\\), 它实际的 y 是 (1, 0, 0)，那么 yhat 就是估计值等于 0.25、0.20 和 0.75。然后对比一下两组数据之间的差别，这样我们就可以优化其中的形成参数 (w, b)。 通过不断优化，就可以计算到更接近于 (1, 0, 0) 这样的值。 首先就是怎么样把 x 向量变成 3 维的。 这个其实不难，如果 x 是 10 维的，110。那么给他再乘以一个 103 的矩阵，它最后就会变成一个 1 行乘 3 列的矩阵。 那么现在假如说现在有这样一个 x: 1x = [1231, 12314, 4341, 1542, 4123, 4512, 3213, 1241, 1231, 6842] 然后我们来做这样一件事： 123456x = np.array(normalize(x))weights = np.random.random(size=(10, 3))np.dot(x, weights)---array([0.86907231, 1.32234548, 0.88170994]) 这样，我们就生成了一个维度是 3 的一串数字。在机器学习里面，我们把这个叫做算子：logits。 现在我们将一个 10 维的 x 变成了一个 3 维的 logit，下一步我们就要考虑，怎么将这个 logit 变成一个概率分布呢？ 我们就要用到一个和逻辑函数特别像的一个函数，Softmax： 我把它写出来： 123456789logits = np.dot(x, weights)def softmax(x): return np.exp(x) / np.sum(np.exp(x))ic(softmax(logits))---array([0.27884889, 0.43875588, 0.28239524]) 这样，我们输入的是 logits，输入到 Softmax，输出的就是概率了。 输出成概率之后，我们定义一个依然和逻辑函数很像的一个函数，叫做 Cross-entropy。 我们刚才使用 softmax 输出的数组就是概率，也就是估算的 yhat。这个 Cross-entropy 的 loss 就是： 求得 loss，然后再对 x 求偏导，就可以通过梯度下降让输入的 x 得到和真正的 y 相近的 yhat。 那我们将 cross-entropy 也写一下： 12def cross_entropy(yhat, y): return -np.sum(y_i * np.log(yhat_i) for y_i, yhat_i in zip(y, yhat)) 现在我们需要一组真正的 y，也就是真实值，和我们预测房价时所使用的真实值是一样的东西，只是现在我们的 y 的维度不太一样： 1y = [0, 1, 0] 接着我们使用cross_entropy将我们之前使用 softmax 计算的概率分布和真实的 y 放进去： 1234ic(cross_entropy(softmax(logits),y))---1.040664959870481 这个时候我们就得到了一个 loss 值。 我们现在去给 weights 求偏导。然后通过不断的迭代，就能找到一组 wi，和 x 进行点乘就能够生成和 y 接近的值。 以上这些就是 softmax 和 cross-entropy 的作用。 cross-centropy 就是用来衡量产生的 yhat 和 y 之间的相似程度差距的。Softmax 是把任意的一组数字变成概率分布，然后这个概率分布就可以送到 loss 函数里面和实际上的 y 进行对比。 Softmax 有这么几个特性，它的结果是一个典型的概率分布。还有就是 Softmax 中有 e 的 n 次方，可以把 Max 变得更大。除了把 Max 变得更大，还保留原来小的数字。 理论上完全可以找别的函数代替，计算机里边很多东西，只要好用就行。这就是放大特征，正是面对多分类任务的一个做法。 Softmax 在实现的时候有个坑稍微要注意一下，在实现的时候我们多加一句： 123456def softmax(x): x = np.array(x) x -= np.max(x) # 多加这么两句 return np.exp(x) / np.sum(np.exp(x))ic(softmax(logits)) 首先，如果 x 的输入是一个 array 就不用管了，但是如果不是，我们就要强制转换一下。 下一句代码是因为 e 的 x 次方可能非常的大，但是我们计算机的存储是有限的，最大只能表示 2^63 的数字，再大就表示不了了。所以我们就需要这样一段代码来处理一下，让最后结果的数字不要那么大。 好，那这一节课的内容到这里也就结束了。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/13.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%84%E7%90%86/"},{"title":"12. 机器学习 - 拟合","text":"Hi, 你好。我是茶桁。 这一节课一开始我们要说一个非常重要的概念：拟合。 拟合 相信只要你关注机器学习，那么多少在某些场合下都会听到拟合这个概念。 什么叫做拟合，什么叫做过拟合或者欠拟合呢？ 假如有一个模型，这个模型在训练数据的时候效果很好，体现在 loss 很小，或者说 precision 很高，accuracy 也比较好，但是在实际情况下，用到没有见过的数据的时候，效果就很差，那么这个就过拟合了。 在这个过程中，要主一的是仅当数据 label 比较均衡的时候，才有必要使用 acc. 我们来看三条曲线： 第一个图是比较合理的参数模型，第二个图就是过拟合的参数模型。 因为对数据过度拟合，当有新的点出现的时候，函数的趋势和新的点并不匹配。那过度拟合就会对于未来的点就预测不对了。为了在训练的时候效果很好，在见过的数据里效果特别好，结果在新的未见过的数据里，效果就很差。 第三个图就是欠拟合的状态。训练的时候这个效果就不好，整个接近程度就不高。 比较好的场景就是第一张图的拟合状态，其形成了一个合理的参数模型。在训练的时候拟合也没有那么高，实际中的结果会发现结果也没那么差。这其实也就暗合了我们前几节课里所讲的[奥卡姆剃刀原理]. 过拟合和欠拟合这两个概念，在我们平时的工作中会是每天都要一直取解决的问题。遇到一个问题，训练的时候效果很差这个欠拟合，经过了很多调试结果发现效果还不错，结果在实际问题中发现效果很差，这个就是过拟合。 这两件事情其实它是互相冲突的，这个可以通过 loss 来判断，也可以通过 percision 来判断，只不过在计算新问题的时候，不存在 lose 函数这回事儿。就是当你把模型已经训练完了，去用真实数据做测试了，那个时候是不存在 loss 函数的。 在整个机器学习的发展历程中，我们一直在不断的做的事情就是怎么样提高欠拟合的准确度，同时降低过拟合。 影响过拟合和欠拟合原因有很多，既和数据有关系，也和模型有关系。但是在这个过程中有一点大家需要注意。所有的机器学习任务里边，在我们收集数据的时候，有一个很重要的问题就是异常值对过拟合和欠拟合影响会很大。 OUTLINER 有一本书就叫《Outliner》(异类), 大家有空可以去看一下。 outliner 为什么会对我们整个值影响很大呢？ 咱们来看这个图，本来正常的线性走向应该是右边这张图。可是因为存在异常值的情况，所以导致线性偏向左边这张图的情况。可是在我们的数据中，这种极端的异常值属于少数，并且因为数值偏差过大，就导致整体趋势的偏斜。 那么我们怎么样去判断异常值呢？为了把模型做好，从一开始收集数据以及清洗数据的时候就要把那些异常值给它去掉。 所谓异常值是没有一个标准定义的，但是在数学上会有一个比较常见的去除方法，就是利用百分位，常见的方法就是按百分位来解决数值型问题。 numpy 里有一个persontile，它接受一个 array，和一个浮点值。 1np.percentile(np.array([]), number) 那这个 percentile 是干嘛的呢？比如下面这张图： 把所人从左到右排排队，比方说第 80 分位，就是第 80% 的是那一个。而我们如果是要处理数据的话，会让这些人按数值大小来排队，比如按身高，那就是最矮的在最左边。 一般统计学上常见的几个数字，一个是 0.5，还有 0.25 和 0.75。 比如我们之前的 lstat 数据，我们来找一下其中的异常值： 12345import numpy as npnp.where(np.array(lstat) &lt; np.percentile(np.array(lstat), 0.25) / 1.5)---(array([], dtype=int64),) 12345np.where(np.array(lstat) &gt; np.percentile(np.array(lstat), 0.75) * 2.5)---(array([ 1, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...500, 501, 502, 505])) 在寻找极大值的时候，我们找到了一堆的数字。如果当异常值比较多的时候，很难把它们定义成异常值。我们总结异常值规律的时候，其实它的和周围的信状它很不一样。 BIAS AND VARIANCE 好，再接下来我们来一起看看 BIAS 和 VARIANCE。 在整个机器学习过程中，我们持续的有一个问题就是它的过拟合和欠拟合一直在互相 PK。那么不管是欠拟合比较严重还是过拟合比较严重，这都是问题。但这两种问题在统计学里有两个名字。 The bias is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between feature and target outputs. (underfitting); 我们把欠拟合这种问题叫做偏见，BIAS 叫做偏见。 假如说对于一个人来说，你对一件事情的判断判断错了，有 BIAS，就是有偏见。这个就是你的脑子对这件事情的抽象程度不够，脑子的判断模型就错了。所以效果就不好。 就比方说分明是一个二次函数，你硬是要拿直线去怼出来，那你怎么怼？这就叫 BIAS。 而 VARIANCE 是什么呢？它指的是你的那个变化太大。 The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). 也就是说，这个训练集对未来比较敏感，实际的值稍微有一点不一样就会产生很差的结果。高 VARIANCE 最后会导致模型产生的结果都很随机，效果很差。 产生 VARIANCE 的背后有一个很重要的特性，就是模型复杂度。 随着模型越来越复杂，它的 BIAS 会越来越低，就是模型越来越复杂。 就比方下面这个图中的模型： 右边的这个曲线就是用了一个很复杂的模型，它的 BIAS 就会很低，BIAS 低的时候它的 VARIANCE 就会变大。 因为模型复杂，所以之后问题稍微变化了，这个模型就会产生出来变化比较大的输出值。而模型越简单，BIAS 就会越高，对于未来也会变化没有那么大。 这是一个非常重要的一个 Dilemma，是一个两难问题，进退两难的一个问题叫 Dilemma。 这两类错误背后其实都是和我们的模型复杂程度有关。 那么讲到这里我们就可以来谈谈，BIAS 和 VARIANCE，过拟合和欠拟合背后的原因有哪些。 过拟合 overfitting 欠拟合 Underfitting 训练数据占总体数据过少 训练数据占总体数据过多 模型过于复杂 模型过于简单 采样过于不均衡 没有正则化... 模型过于简单可能会产生欠拟合情况，模型过于复杂呢有可能会产生一个过拟合的情况。如果产生了过拟合，还有一个很重要的特点就是有可能模型采样非常不均衡。 模型复杂不复杂，单不简单，采样均衡不均衡。其实背后都有一个重点叫做『训练数据占总体数据的比例』，就是训练数据是不是够多。 所谓的采样均衡不均衡，异常值的出现最终都指向的是一个问题，就是我们的训练数据不够多。 为什么训练数据不够多会引起模型过于复杂之类的情况呢？在整个机器学习里是个非常重要的概念，叫做维度灾难。 我们举例来说明下，就比如，在一个平面坐标轴上有几百个点，我们用两点去确定了一根直线，但是这根直线并不能代表这几百个点确定的走向，我们需要去确定更多的点来调整这根线。 但是如果是三维轴上，我们为了去确定一个平面，就需要更多的点，比二维轴上确定直线的点多的多。为了能更加精确地确定一个平面，需要更多的数据才行。 在机器学习中算是一个经验，当机器学习的维度每增加一个，那么所需要的样本量基本上要增加一个数量级。 举个例子，假如班上有一个老师，这个老师要预测同学能不能考上重点大学。假如现在是高一，还没有开始考试，他预测这个同学有这么几个情况： 第一个情况是这个同学做作业的情况； 第二个情况是这个孩子每天上课时回答问题的活跃程度 假如是这两个点，如果他靠这两个 features 来预测这个孩子能不能考上大学，准确度能够做到不错，假如到百分之八九十，他需要 50 个学生能够预测。 那么现在又加一个 features，这个 features 还是和前两个值不相关的，又加了一个孩子的家庭收入情况。那么它得变成上百个学生数据才能预测对。因为每增加一个 features，在这个世界中就会增加很多不确定的情况。 所以模型之所以过于复杂，其实背后本质上还是数据量太少。 就在我们今天大数据的情况下，模型其实进步的并没有非常大，但是数据量变大之后，整个的效果就好多了。在机器学习里有一个点就是算法再好，模型再好，抵不过不过数据量大。 真正工作、学习的时候，一定要想办法提前检测出来过拟合、欠拟合的情况。为了提前检测出这些，有一个很简单的方法。 假如现在给了许多的训练数据，不要把这训练数据全部拿上做完。而是选一部分，拿其中的一部分数据做训练，比方 80%。然后，剩下 20% 不给模型去看，然后把模型拿到这 20% 上去看一下结果。这样，我们就可以测试出结果。这就叫做训练集和数据集。 为什么要有训练集和测试集呢？有一个非常极端的情况，如果不做训练集和数据集想获得好的结果，直接把所有 label 对应的值记下来，也就是小时候我们背诵古诗散文。 你的模型只会阅读并背诵全文，那想想训练的时候，acc、precision、recall 等评测指标就非常高，但是效果就很差，过拟合的情况就会很严重。 在实际的工作中，除了train_set和test_set，还有一个值叫validation_set。 早些年的时候，做测试数据集只会分成训练集和测试集。然后在训练集上去训练，完事在测试集上去测试。但是人们发现一个情况，比方说在测试集上训练完了之后，在测试集上发现效果不太好，过拟合有点严重，于是分析 test 数据哪里做错了，找到错误之后修改代码或者修改数据。 就好比一个同学做题的时候有十套卷子，他做完 8 套，留了两套。再做这两套之后发现哪里不太对，然后反反复复去观察后面这两套，也就是我们的 test 数据。这个时候其实是在做针对性的调整，在有针对性的解决问题，整体的能力并没有提升。 为了解决这个问题，在实际的工作中我们会把数据集分成三个数据集。 训练数据集不断的去训练，然后结果给到 validation set 这样一个小数据集里。我们去观察 validation 的结果，再去调整，当 validation 的结果很不错的时候我们拿一套完全没做过的题来检验。这个完全没见过的题就是我们 test set 了。 当 test set 用过之后，如果考试成绩太差，那只能把数据集打乱，再重新取一份新的 test 数据了。因为这个时候如果再去有针对性的调整模型结果，那其实是在手动过拟合了。 在这个过程中，假设一共有 100 个数据，test 里有 20 个，validation 有 10 个，train 里有 70 个。为了尽可能多的把所有的数据都用上，把它的效率都发挥上，有一个很简单的操作：cross validation，也叫做交叉验证。 如图，假如我把数据分成很多份，我让其中一部分做 validation 数据集，下一次训练的时候，我再让另外一部分做 validation 数据集。 再回过头来看我们之前见过的机器学习的通用框架，这几节课学习了评价指标之后，我们就应该知道，在这个背后多了一个 acc 和 precision，我们要持续的去观测它的结果。 在这个过程中有了数据，然后定义一个𝜃，这个𝜃就是我们的参数。然后根据 loss function，gradient descent 不断优化这个参数。结果并不是要一个低的 loss 参数，是期望有一个好的 acc 和 precision。这个是在之前的基础上完善的整个学习过程。 好，下节课呢，咱们来看看 FEATURE SCALING，特征缩放。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/12.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%8B%9F%E5%90%88/"},{"title":"16. 机器学习 - 决策树","text":"Hi，你好。我是茶桁。 在上一节课讲 SVM 之后，再给大家将一个新的分类模型「决策树」。我们直接开始正题。 决策树 我们从一个例子开始，来看下面这张图： 假设我们的 x1 ~ x4 是特征，y 是最终的决定，打比方说是买东西和不买东西，0 为不买，1 为买东西，假设现在 y 是[0,0,1,0,1]。 那么，我们应该以哪个特征为准去判断到底 y 是 0 还是 1 呢？ 如果关注 x3，那么 x3 为 A 的时候，即有 0 也有 1，我们先放一边找找看有没有更合适的。 如果是 x4 的话，肉眼可见的，区分度是最准确的对吧？B 的都是都是 0，C 的时候都是 1，那么 x4 也就是区分度最大的。 我们现在换成人的思维过程来说，肯定是期望先找到那个最能区分它的，就是最能识别的特征。这个最能识别的特征在数学里面有一个专门的定义：Salient feature, 就是显著特征。 如果我们更改一下 x4 的值，变成[B,C,C,B,B]，那 x4 也就不那么显著了。这个时候最能区分的就是 x2 了，只在x2[1]的位置上判断错了一个。 这个时候，我们就需要客观的衡量一下，什么叫做最能区分，或者说是最能分割，最显著的区别。这就需要一个专门的数值来计算。 那我们就来看看，到底怎么样来做区分。我们现在根据一个值，将我们的数据分成了两堆，那咱们的期望就是这两堆数据尽可能是一样的。比如极端情况下，一堆全是 0，一堆全是 1，当然，中间混进去一个不一样的也行，但是尽可能的「纯」： 好，那我们该怎么定义这个「纯」呢？ 我们大家应该都知道物理里有一个定义：「熵」，那熵在物理里是衡量物体的混乱程度的。 比如说一个组织、一个单位、公司或者个人，其内部的熵都是在呈现越来越混乱，而且熵的混乱具有不可逆性。这个呢，就是熵增定律，也叫「热力学第二定律」。是德国人克劳修斯提出的理论，最初用于揭示事物总是向无序的方向的发展、以及“孤立系统下热量从高温物体流向低温不可逆”的热力学定律（要不说看我的文章涨学问呢是吧）。 信息熵 好，说回咱们的机器学习。后来有一个叫「香农」的人要衡量一堆新息的混乱程度，就起了一个名字「信息熵」，也就成了衡量信息的复杂程度。 那么信息熵怎么求呢？ \\[ \\begin{align*} Entropy = \\sum_{i=1}^n-p(c_i)log_2(p(c_i)) \\end{align*} \\] 这个值越大，就说明了这个新息越混乱，相反的，越小就说明越有秩序。来，我们看一下代码演示，定义一个熵的方法： 1234567891011import numpy as npfrom icecream import icfrom collections import Counterdef pr(e, elements): counter = Counter(elements) return counter[e] / len(elements)# 信息熵def entropy(elements): return -np.sum(pr(e, element) * np.log(pr(e, elements)) for e in elements) 然后我们具体的来看几组数据： 1234567891011121314ic(entropy([1,1,1,1,1,0]))ic(entropy([1,1,1,1,1,1]))ic(entropy([1,2,3,4,5,8]))ic(entropy([1,2,3,4,5,9]))ic(entropy(['a','b','c','c','c','c','c']))ic(entropy(['a','b','c','c','c','c','d']))---ic| entropy([1,1,1,1,1,0]): 1.05829973151282ic| entropy([1,1,1,1,1,1]): -0.0ic| entropy([1,2,3,4,5,8]): 1.7917594692280547ic| entropy([1,2,3,4,5,9]): 1.7917594692280547ic| entropy(['a','b','c','c','c','c','c']): 1.7576608876629927ic| entropy(['a','b','c','c','c','c','d']): 2.1130832934475294 我们可以看到，最「纯」的是第二行数据，然后是第一行，第三行和第四行是一样的。5，6 行就更混乱一些。 那接下来的知识点是只关于 Python 的，我们看上面的代码是不是有点小问题？这个代码里有很多的冗余。一般情况下，会将 counter 改成全局变量，但是一般如果想要代码质量好一些，尽量不要轻易定义全局变量。我们来简单的修改一下： 123456789101112def entropy(elements): # 信息熵 def pr(es): counter = Counter(es) def _wrap(e): return counter[e] / len(elements) return _wrap p = pr(elements) return -np.sum(p(e) * np.log(p(e)) for e in elements) 这样写之后，我们再用刚才的数据来进行调用会看到结果完全一样。不过如果这样写之后，如果我们数据量很大的情况下，会发现会快很多。 Gini 系数 除了上面这个信息熵之外，还有一个叫 Gini 系数，和信息熵很类似： \\[ \\begin{align*} Gini = 1 - \\sum_{i=1}^np^2(C_i) \\\\ \\end{align*} \\] 假如说 probability 都是 1，也就是最纯的情况，那么 1 减去 1 就等于 0。如果它特别长，特别混乱，都很分散，那 probability 就会越接近于 0，那么 1 减去 0，那结果也就是越接近于 1。 那么代码实现一下就是这样： 12def geni(elements): return 1-np.sum(pr(e) ** 2 for e in set(elements)) 好吧，这个时候我发现一个问题，之前我们将 probability 函数定义到熵函数内部了，为了让 Gini 函数能够调用，我们还得拿出来。在我们之前修改的初衷不变的情况下，我们来这样进行修改： 1234567891011def pr(es): counter = Counter(es) def _wrap(e): return counter[e] / len(es) return _wrapdef entropy(elements): # 信息熵 p = pr(elements) return -np.sum(p(e) * np.log(p(e)) for e in elements) 哎，这样就对了。优雅... 然后我们来修改 Gini 函数，让其调用 pr 函数： 123def gini(elements): p = pr(elements) return 1 - np.sum(p(e) ** 2 for e in set(elements)) 然后我们写一个衡量的方法： 1pure_func = gini 然后我们将之前的调用都修改一下： 1234567891011121314ic(pure_func([1,1,1,1,1,0]))ic(pure_func([1,1,1,1,1,1]))ic(pure_func([1,2,3,4,5,8]))ic(pure_func([1,2,3,4,5,9]))ic(pure_func(['a','b','c','c','c','c','c']))ic(pure_func(['a','b','c','c','c','c','d']))---ic| pure_func([1,1,1,1,1,0]): 0.2777777777777777ic| pure_func([1,1,1,1,1,1]): 0.0ic| pure_func([1,2,3,4,5,8]): 0.8333333333333333ic| pure_func([1,2,3,4,5,9]): 0.8333333333333333ic| pure_func(['a','b','c','c','c','c','c']): 0.44897959183673464ic| pure_func(['a','b','c','c','c','c','d']): 0.6122448979591837 我们可以看到，Gini 系数是把整个纯度压缩到了 0~1 之间，越接近于 1 就是越混乱，越接近 0 呢就是越有秩序。 其实除了数组之外，字符串也是一样可以衡量的： 12345678ic(pure_func(&quot;probability&quot;))ic(pure_func(&quot;apple&quot;))ic(pure_func(&quot;boom&quot;))---ic| pure_func(&quot;probability&quot;): 0.8760330578512396ic| pure_func(&quot;apple&quot;): 0.72ic| pure_func(&quot;boom&quot;): 0.625 在能够定义纯度之后，现在如果我们有很多数据，就比方说是我们最之前定义数据再增多一些：[x1, x2, x3, ..., xn]，那么我们决策树会做什么呢？ 根据 x1 对 y 做了分类，根据 x2 对 y 做了分类，做了分类之后，通过 x1 把 y 分成了两堆，一堆我们称呼其为m_left, 另外一堆我们称呼其为m_right，然后我们来定义一个 loss 函数： \\[ \\begin{align*} loss = \\frac{m_{left}}{n} \\cdot G_{left} + \\frac{m_{right}}{m} \\cdot G_{right} \\end{align*} \\] 现在要让这个 loss 函数的值最小。在整个式子中，G 代表的是纯度函数，这个纯度函数可以是 Entropy，也可以是 Gini。 loss 函数的值最小的时候，就可以实现左右两边分的很均匀。我们把这个算法就叫做 CART。 CART 算法其实就是classification and regression tree Algorithm, 也称为「分类和回归树算法」。 上面我们讲的所有内容可以实现分类问题，那么回归问题怎么解决呢？CART 里可是包含了 Regression 的。 好，还是我们最之前给的数据，我们现在的 y 不是[0,0,1,0,1]了，我们将其更改为[1.3,1.4,0.5,0.8,1.9]。我们人类大脑中的直觉会怎么分类？会将[1.3,1.4,1.9]分为一类，而[0.5,0.8]分为一类对吧？（我说的是大部分人，少部分「天才」忽略）。 那么为了完成这样的一个分类，我们将之前公式里的纯度函数替换成 MSE，那么函数就会变成如下这个样子： \\[ J(k, t_k) = \\frac{m_{left}}{m} \\cdot MSE_{left} + \\frac{m_{right}}{m}MSE_{right} \\] MSE 是什么呢？其实就是均方误差。 \\[ \\begin{align*} MSE_{node} &amp; = \\sum_{i \\in node}(\\hat y_{node} - y^{(i)})^2 \\\\ \\hat y_{node} &amp; = \\frac{1}{m_{node}}\\sum_{i\\in node}y^{(i)} \\end{align*} \\] 我们要取的，就是最小的那一个 MSE。 决策树最大的优点就是在决策上我们需要更大的解释性的时候很直观，决策树可以将其分析过程以树的形式展现出来。一般在商业、金融上进行决策的时候我们都需要很高的解释性。就比如下面这个例子： 第二呢，它可以来提取重要特征。决策术可能某一类问题上效果假设最多只能做到 85% 的准确度，我们期望换一种模型，希望用到的维度少一点。 比方说要做逻辑回归，我们期望的w.x+b乘以一个 Sigmoid，原来的 x 是 10 维的，我们期望把它降到 5 维。那这个时候决策树的构建过程就从最显著的特征开始逐渐构建，我们就可以把它前五个特征给他保留下来，前五个特征就是最 salience 的 feature。我们假如把它用到逻辑回归上，直接用这五个维度就可以了。 接着我们来个问题：本来是 10 维的我们期望把它变成 5 维，为什么我们希望降维呢？ 还记得我们「拟合」这一节么？我们说过，过拟合最主要的原因是数据量过少或者模型过于复杂，那为什么数据量过少呢？不知道是否还记得我讲过的维度灾难。 多个维度就需要多个数量级的数据。在仅有这么多数据的情况下，维度越多需要更多数据来拟合，但是大部分时候我们并没有那么多数据。 这个问题其实是一个很经典的问题，为什么我们做各种机器学习的时候期望降维。如果能把这个仔仔细细的想清楚，其实机器学习原理基本上已经能够掌握清楚了。 接下来我们再说说它的缺点，其实也很明显，它的分类规则太过于简单。所以它最大的缺点就是因为简单，所以好解释，但正是因为简单，所以解决不了复杂问题。 和 SVM 一样，也可以给决策树加核函数，曾经有一段时间这也是很重要的一个研究领域。 好，在结束之前我们预告一下下一节课内容，我们还是讲决策树，讲讲决策树中的 Adaboost 等，以及决策树的升级版：随机森林。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/16.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E5%86%B3%E7%AD%96%E6%A0%91/"},{"title":"15. 机器学习 - 支持向量机","text":"Hi, 你好。我是茶桁。 逻辑回归预测心脏病 在本节课开始呢，我给大家一份逻辑回归的练习，利用下面这个数据集做了一次逻辑回归预测心脏病的练习。 本次练习的代码在「茶桁的 AI 秘籍」在 Github 上的代码库内，数据集的获取在文末。这样做是因为我的数据集都是和百度盘同步的，很多数据集过大了，所以也就不传 Github 了。而且，我直接获取盘内同步数据也更方便。 还有一个原因，有些数据集可能以后会收费获取。 好，让我们进入今天的正课。 因为未来几节课的内容比较多。「核心基础」的这部分内容已经超出我原本的预计，咱们「核心基础」的部分刚刚过半，可是已经写到 15 节了，本来这部分内容我是想在 21 节左右结束的，所以，我们还是要压缩一下内容了。 这节课咱们还是继续讲解经典的机器学习。 支持向量机 接下来，要讲解一个非常有趣的方法：支持向量机。 支持向量机的原理其实可以很复杂，但它是一个很经典的思想方法。咱们就把它的核心思想讲明白就行了。其实我们平时在工作中用的也比较少。但是面试中有一些老一代的面试官会比较喜欢问这个问题。 支持向量机的核心思想，假如我们有两堆数据，希望找一根线去把它做分类，那么咱们找哪一根线呢？ 上图中，我们假设黑色的那根线定义为 l，把离这根线最近的点，也就是直线距离最小的点，找到两个这样的点定义为 P1、P2。 现在我们是希望离这个 l 最近的点，假如说是 d1,d2，那么我们希望这两个距离加起来最大：max|d1+d2|。 现在再定义蓝色的线为直线 b，那直线 b 做分类就比直线 l 要好。为什么直线 b 就比是直线 l 好呢？因为直线 b 离 d1,d2 普遍都比较远。 现在这里的演示是一个二维平面中用一根线来分割，如果是在多维空间中，SVM 的目标就是找到一个最佳的超平面来最大化间隔，同时确保正确分类样本。 假设我们有一组训练样本，每个样本用特征向量 x 表示，并且标记为正类别 +1 或负类别 -1。 我们可以表示为以下凸优化问题： \\[ \\begin{align*} min_{w, b}\\frac{1}{2}||w||^2 \\end{align*} \\] 其中对所有样本 \\[ y_i(w \\cdot x_i+b) \\ge 1 \\] w 是超平面的法向量，b 是截距项，yi 是样本 xi 的标签，也就是 +1 或者 -1。 为了解决这个优化问题，我们引入拉格朗日乘子\\(a_i\\)来得到拉格朗日函数： \\[ L(w,b,a) = \\frac{1}{2}||w||^2 - \\sum_{i=1}^Na_i[y_i(w\\cdot x_i +b) - 1] \\] 然后我们要最小化拉格朗日函数，首先对 w 和 b 求偏导数，令它们等于 0，然后代入拉格朗日乘子条件： \\[ a_i[y_i(w\\cdot x_i + b)-1] = 0 \\] 然后我们就可以得到如下这个式子 \\[ w = \\sum_{i=1}^Na_iy_ix_i \\\\ sum_{i=1}^N a_iy_i = 0 \\] 使用某种优化算法（例如，SMO 算法），求解拉格朗日乘子\\(a_i\\)。我们就可以使用求解得到的\\(a_i\\)计算超平面参数 w 和 b。 对于新样本 x，使用超平面\\(w\\cdot x + b\\)的符号来预测其类别。 那我们讲了这么半天，都是一个支持向量机的数学演示过程，下面我们来看看具体的代码实现。 我们先来生成两组数据，这两组数据咱们让他距离更大： 123import numpy as nplabel_a = np.random.normal(6, 2, size=(50, 2))label_b = np.random.normal(-6, 2, size=(50, 2)) 我们现在来观察以下生成的这些点： 12345import matplotlib.pyplot as pltplt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))plt.show() 然后我们继续： 12label_a_x = label_a[:, 0]label_b_x = label_b[:, 0] 我们就将这两组数据的第一列分别取出来了。 接着我们随机的定义一些 w 和 b 12for i in range(100): w, b = (np.random.random(size=(1, 2)) * 10 - 5)[0] 然后我们按照之前讲的数学演示来定义一个函数 12def f(x): return w*x+b 然后我们之前从数学演示里已经知道，\\(y_i(w\\cdot x+b) \\ge 1\\), 而我们也知道这个说的是距离，也就是说，同样的$y_i(wx+b) $。 也就是说，我们要让函数 f 小于等于 -1，并且大于等于 1。当然，为了保证其被分到两边，我们将函数的最大值定义为小于等于 -1，将函数的最小值定义为大于等于 1。这样就保证 (-1,1) 之间是不存在任何函数值： 1np.max(f(label_a_x, w, b)) &lt;= -1 and np.min(f(label_b_x, w, b)) &gt;= 1 只有同时满足这两个条件的值，我们才会留下来进行保存。我们可以定义一个变量将其保存 12345w_and_b = []for i in range(100): w, b = (np.random.random(size=(1, 2)) * 10 - 5)[0] if np.min(f(label_a_x, w, b)) &gt;= -1 and np.min(f(label_b_x, w, b)) &gt;= 1: w_and_b.append((w, b)) 在得到这些 w,b 之后，我们将这些 w,b 连起来进行画图： 12345for w, b in w_and_b: x = np.concatenate((label_a_x, label_b_x)) plt.plot(x, f(x, w, b))plt.show() 这样，我们就拟合出来了很多的曲线。这些个曲线到底哪一个是最好的那一个呢？ 现在根据刚刚得到的那个结论，现在所有的\\(y_i(w\\cdot x_i + b)\\), 那么现在其实就是\\(margin = \\frac{2}{||w||}\\)。 那我们现在就找这个 w 最小的这个值就可以了。 12345w, b = min(w_and_b, key = lambda w_b: w_b[0])all_x = np.concatenate((label_a_x, label_b_x))plt.plot(all_x, f(all_x, w, b), 'r-o')plt.show() 现在我们就可以看到那个最优的直线了，就是众多红色的点连接起来的那根线。 当然，最后代码执行顺序和讲解顺序有一些不一样，为了避免数据每次重新生成造成的差别，所以最开始是生成数据，之后是定义函数、过滤参数以及生成图像。 这个就是支持向量机的原理，我们找到离它所有的点的一个距离，让它这个边距最大，最后得到一个简化结果。 核函数 然后我们再来看另外一个点：「核函数」： 核函数是支持向量机里面非常重要的一个东西。 如果支持向量机只要数据是线性可分的，那么我们一定能够找到它的分割线。但是在实际的现实生活中有很多点并不是线性可分的。 举个例子，我们来画一张图： 就比如图中的这种数据，是无论如何用一条直线无法分割的，不管怎么画，都无法把蓝色和红色的点分割开。 就像我们下面这张图： 但是，我们我们可以做这样一件事情，假设我们在一个坐标轴上拥有 8 个点，A、B、C、D 为一组，a,b,c,d 为一组。如下图： 分别为 A(-1,1), B(1,1), C(1, -1), D(-1,-1)；a(-0.5, 0.5), b(0.5, 0.5), c(0.5, -0.5), d(-0.5, -0.5)。 现在我们 ABCD 和 abcd 是无法用一根直线来分割的，然后我们令： \\[ \\begin{align*} f(x) =&gt; \\begin{Bmatrix} x^2 \\\\ y^2 \\end{Bmatrix} \\end{align*} \\] 那在这种情况下，八个点分别就变成了 A(1, 1),B(1, 1),C(1, 1),D(1, 1)，a(0.25, 0.25),b(0.25, 0.25),c(0.25, 0.25),d(0.25, 0.25)。 那这样的情况下，我们就完全可以用一根直线去分割了： 那现在找到这根线是 w2 = wx+b，那我们遇到新数据应用到这个函数里边，再应用到这个线里面做分割就可以了。我们把原本线性不可分的东西，变成线性可分的。那么这个就是核函数神奇的地方。 支持向量机通过某非线性变换 φ(x) ，将输入空间映射到高维特征空间。特征空间的维数可能非常高。如果支持向量机的求解只用到内积运算，而在低维输入空间又存在某个函数 K(x, x′) ，它恰好等于在高维空间中这个内积，即 K(x, x′) =φ(x)⋅φ(x') ; 。那么支持向量机就不用计算复杂的非线性变换，而由这个函数 K(x, x′) 直接得到非线性变换的内积，使大大简化了计算。我们就将这种函数函数 K(x, x′) 称为核函数。 \\[ \\varphi (x) = \\begin{bmatrix} x \\\\ x^2 \\\\ x^3 \\end{bmatrix} \\] 那其实，就类似的事情，已经有人总结了一些相应的公式来使用： 这些是一些常见的核函数。 一般在使用的时候调用它就可以，如果在用 SVM 的时候，它会有一个参数。可以自己定义一个核函数，但一般不自己定义，调用现有的就够了。 SVM 其实也有弊端，当数据量很复杂的时候，现有的核函数就没有作用了。因为它会失效，所以我们需要很多的人工分析，整个效率很低。 但是在整个机器学习的发展史上，它曾经有非常重要的一段历史。有一段时间它的论文量非常的多，做科研的非常爱做 SVM，不是因为快速，是因为可以提出来各种各样的 Kerno 函数。 假如有一组数据不好分割，但是你提出了一种新的核函数，这个函数量可以比较复杂啊 然后提升了分割率，提高了效果。 但是这种方法其实曾经一度让机器学习非常不受人待见，在学术圈非常不受人待见。搞机器学习的人就是每天就是发论文，说我的曲线比你的曲线强，这就是他们干的事。 所以 10 年左右，做机器学习、做人工智能的人都不说自己是做机器学习，做人工智能的。都换个名字，说做文本挖掘等等。 SVM 因为要做各种升维，当数据量比较大的时候，计算量非常的复杂，计算需求量非常的大。 但是 SVM 它有个好处，就是它比较直观，还有就是 SVM 对于不平衡的数据比较有用。 好，这节课我们就讲到这里，下一节课我们来看「决策树」。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/15.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"},{"title":"18. 深度学习 - 从零理解神经网络","text":"[TOC] Hi, 你好。我是茶桁。 我们终于又开启新的篇章了，从今天这节课开始，我们会花几节课来理解一下深度学习的相关知识，了解神经网络，多层神经网络相关知识。并且，我们会尝试着来打造一个自己的深度学习框架。 以前很多时候都会被人问到很多问题，其中比较多的就包括现在各种各样的框架应该用到哪一个，在学习人工智能的时候，对于深度学习框架有比较多的问题。那在这里我就希望能帮助各位小伙伴彻底的去理解一下什么是学习框架。 对于我们来说，就像小孩子去学一个东西，最好的就是从头到尾能把它拆了，然后再重建起来。 从今天开始往后的几节课里，我们都会去好好了解「如何从零构建一个深度学习框架」。 本文目标 我们基本的核心目的就是来讲明白，什么是神经网络，以及神经网络的原理是什么。 我们要知道，人工智能有很多方法，但是神经网络是现代人工智能里面一个非常核心的内容。 咱们现在就是要先去了解神经网络的原理是怎么回事，然后在这个过程中我们来讲解清楚神经网络的框架到底是什么样的。 如我们之前学习过的几节机器学习课程，会发现它有很多的概念。 比方说非监督学习、监督学习、强化学习，监督学习里面又分了回归和分类等等。 很多人看到这些，在初次接触、初次学习的时候就觉得人工智能很复杂，很难学会。除此之外，我们在学到人工智能目前比较核心的一个内容是关于深度学习神经网络。好多人不知道深度学习神经网络到底是什么原理。 在整个学习过程会发现有很多很多的问题，概念很多，变体也很多，学习很困难。 那这里要跟大家强调一点，就是千万别成为「马保国」，为什么这里会提到这个人呢？在我看来，这其实是一类人，他是一类人的代表。就是整很多的概念，假装子集很厉害。 就是我们脑子里不要总是去提很多概念，或者说很多很花哨的东西，最重要的还是基本功修炼好。我一直都强调一个观念，就是基础学科，基本功才是所有学科的基石。过多的概念其实并没有什么卵用。 早些时候，我上班的地方有一个叫「李雨晨」（匿名🙄）的产品经理，各种概念信手拈来，都是一些高大上的东西。也是将面试官唬的一愣一愣的。当时大家也是没多想，心想人家既然是个牛逼人物，那就多配合人家呗，结果是没过 3 个月就原形毕露，当然是下面干事的人最先觉察出来的。 没办法，为了继续装下去只能是利用自己的职权和谎言去盗用别人的成果，比如设计稿啊，文档啊啥的，拿着当自己的东西向上汇报。 再然后，基本人人都开始防着他了，就开始恼羞成怒，一直打压那个最开始说他不行并防着他的产品。不过不行就是不行，其实最开始就能看出端倪，因为基本没有一家公司干活超过 6 个月，那肯定是有问题的。就这资质也能忽悠成高级产品经理，也能看出来那会儿产品这个行业的水份多大，门槛多低。不过终归潮水退了之后，裸泳的王八都要现行是吧。 好，说这么多吐槽的话其实也是想说一个道理，不要去搞花里胡哨的玩意，踏踏实实的把基本功练扎实，否则一时唬的了人，但是终归是走不远。 那这也是咱们这节课的目的，让大家去除掉背后这些繁杂的表象，那么它背后到底是什么，这就是咱们这三天的目的。 这些年，人工智能已经应用到我们各个地方了。先不说现在大火的 AIGC，人工智能还应用到其他各个地方。 比方说在商场购物的时候，它的楼宇灯光，自动停车都是在做这些事情。买票的时候，机场，火车站都有人脸识别。每天给你推荐的各种商品，以及我们做物流配送等等这些东西，背后都有人工智能。 而这些人工智能背后有一个很重要的东西，就是用到了神经网络框架。 比方说众所周知的 TensorFlow, 我们每次调用的时候，框架背后调用了很多东西。 123456789101112131415161718192021222324252627282930313233# Store layers weight &amp; bias# A random value generator to initialize weights.random_normal = tf.initializers.RandomNormal()weights = { 'h1': tf.Variable(random_normal([num_features, n_hidden_1])), 'h2': tf.Variable(random_normal([n_hidden_1, n_hidden_2])), 'out': tf.Variable(random_normal([n_hidden_2, num_classes]))}biases = { 'b1': tf.Variable(tf.zeros([n_hidden_1])), 'b2': tf.Variable(tf.zeros([n_hidden_2])), 'out': tf.Variable(tf.zeros([num_classes]))}...# Create model.def neural_net(x): # Hidden fully connected layer with 128 neurons. layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) # Apply sigmoid to layer_1 output for non_linerity. layer_1 = tf.nn.sigmoid(layer_1) # Hidden fully connected layer with 256 neurons. layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # Apply sigmoid to layer_1 output for non_linerity. layer_2 = tf.nn.sigmoid(layer_2) # Output fully connected layer with a neuron for each class. out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # Apply softmax to normalize the logits to a probability distribution return tf.nn.softmax(out_layer) 我们现在想把这些框架搞清楚，就需要知道它背后这些东西到底是什么原理、什么原因。 那这几节课之后，就希望我们能从 0 到 1 学会创建一个深度学习框架，从底层来理解这个神经网络的原理，理解现代人工智能的核心。 一开始的课程，内容也会稍微比较简单一些，越往后咱们就越难一点。最后，彻底理解深度学习神经网络原理。 预测趋势与关系 我们以一个趋势预测的问题为引入。 如果对于自然哲学或者说科学研究这些，就是对科学研究方法论感兴趣的话，你会知道我们整个科学研究其实分为三个层面。 不管是牛顿、爱因斯坦，还是伽利略、图灵等等，所有的科学研究，所有的 research，不管是关于数据还是别的，它都是三个层面。 第一个层面叫做描述性的，第二个叫做因果推理，第三个叫做未来的预测。 就说我们所有的科学活动，所有的研究活动都可以归为这三类。 描述性的东西，比方说你又长胖了多少，然后又增加了多少重量。今天的体重，明天的体重等等。 除此之外第二个层面是我们要看出来它们之间的相关性。比方吃的多和你长胖，它们之间是呈正相关的。还有其他的一些关系，比方说是呈负相关的等等。 那我们最重要也是最难的一个科学活动是要对它进行未来的预测，对于未来的预测。这个未来它不仅是 predict。 比方说现在你知道的是几组数据，知道每个对应的结果。然后你看到了一组没有见过的数据，你去预测它。 就好比一个孩子做题，他见过的题都能做，没见过的题他也要会做。这个其实就是属于对未来的一种预测能力。 关于预测，我们最关心的预测是关于我们的身体健康，能活多久；还有就是关于挣钱的问题。 我们看一下这个例子，你的性别和你的吸烟的频率，跟一种疾病（可能是肺癌），它会有一个相对应的概率。 性别不同，年龄不同，抽烟频率不同。我们会发现，得病概率随着年龄的增大并不会有多少增加，此时男性得病概率反而比女性还小。 但是随着抽烟频率越多，得病概率上升的非常快。其中呢，同样的年龄和抽烟频率下，男性得病的概率则会更高。 假设存在一个人 p，男性，年龄是 72 岁，他每天抽三根：P{age:72, sex: male, rate: 3/day}。那他得这种病的概率大约是多少？那我们就先在图上随意画一个，假如说就如图的位置一样的概率： 那么这个概率到底是多少？我们就需要用到数据去做预测，此时我们就得去做个拟合。 除此之外，我们再来看 BMI，也就是身体指数。身体指数就是体重除以身高的平方：BMI = kg/h^2，越大就表示你越胖。 当你到某一个值的时候，可以看到得病的概率。 我们假设有一个人 180 斤，身高一米 73，我们来预测他得肾病的概率是多少。这个时候我们还是需要去做预测。 波士顿房价预测 现在就来看一个非常经典的预测案例：波士顿房价案例。这个波士顿房价的数据，我们曾经在机器学习的线性回归里有用到，不知道小伙伴们有没有去看过。 波士顿地区是在美国东北部，房地产的价钱也比较稳定，那这个数据也是比较老的数据了，通过这些数据来考察，希望机器能够根据输入的内容来预测它的房价。 现在就以波士顿房价问题为例，来讲讲计算机怎么去预测。然后在预测的过程中我们来讲解实现深度学习的原理。最终把它封装成我们所需要的一个深度学习框架。 第一步自然是加载和分析数据。 之前的课程我提到过，这个数据由于一些原因，sklearn 的 datasets 中已经删除了，那我们要想加载数据，就需要用到其中的 fetch_openml： 123from sklearn.datasets import fetch_openmldataset = fetch_openml(name='boston', version=1, as_frame=True, return_X_y=False, parser='pandas') 在我们第一次获取到这个数据不知道怎么处理的时候，我们可以使用 dir 来看看这个数据里面的内容： 1234dir(dataset)---['DESCR', 'categories', 'data', 'details', 'feature_names', 'frame', 'target', 'target_names', 'url'] 我们看到这个 dataset 里有一个feature_names，直觉上这个应该是一些特征名称，我们来查看一下这个的内容： 1234dataset['feature_names']---['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'] 这里要说明一下，因为我是用的 Jupyter，所以我可以这样直接打印出变量的具体内容，如果小伙伴们不是在 Jupyter 里，而是在 Python 文件中去编写代码，不要忘了使用print函数。 在拿到数据之后，我们先来定义一下问题。就是假设你现在要买一个房子，那么你就要根据他的这个房子的相关数据，来判断这个房子到底应该能卖到多少钱。所以我们的任务就是给定一组房屋的数据，然后要能够预测售价是多少。 定义完问题之后，我们来分析一下数据。 首先，要做数据，我们会先把它装载到一个表格里边。这里，我们使用 Pandas。 Pandas 在 Python 基础课里我有详细的讲过，它是做数据科学非常常用的一个东西。不要把它认为是熊猫啊，它是 panel data set 的缩写，就是「面板数据集」，可以理解为一个 Excel。可是它比 Excel 更方便编程。 12345import pandas as pddata = dataset['data']dataframe = pd.DataFrame(data)print(len(dataframe))dataframe.head(5) 为了节省篇幅，打印结果我就不贴出来了。 有的小伙伴在处理这里data的时候，会发现头部没有特征名，会呈现 1，2，3，4 这样的数字。我们就需要将名称给它加上，之前我们说过，feature_name 是特征名，于是： 1dataframe.columns = dataset['feature_names] 这个时候我们就能看出来每一个特征到底是什么。不过这组数据里因为只是特征数据，并没有相关的价格。价格原本是目标数据，也就是最初始数据里的target，所以我们这里给这组特征数据里加上一列。 1dataframe['price'] = dataset['target'] 然后我们要想看看到底什么因素对房价的影响是最大的。「What's the most significant（salient）feature of the house price」。 对于决定一个东西最重要的特征我们就叫做 significant，或者 silence，显著特征。 在 pandas 里边有一个很简单的东西，correlation。correlation 就是两组变量的相关性。 关于特征相关性，我们在机器学习里面有详细的讲过，这里我们就粗略带过就行了，在使用corr()找到特征之间的相关性数据之后，可以使用 seaborn 来将热图可视化出来： 123import seaborn as snssns.heatmap(dataframe.corr()) 这里我们着重来看和价格相关的特征，除了它本身之外，正相关性最大的就是 RM，负相关性最大的是 LSTAT。 我们来看一下这两个特征的说明： 1234567print(dataset['DESCR'])---...RM average number of rooms per dwellingLSTAT % lower status of the population... RM 是一套住宅的房间数量，一个是低收入人群的人口比例。也就是说，房间越多的房子越贵，小区内低收入人群的比例越低，小区内的房子越贵。那小区内低收入人群的比例居然比犯罪率的影响还要大一些，似乎有点让人难以接受，但是这个确实是事实。 基于以上分析，我们需要把房屋里边卧室的个数和房屋价格最成正相关。 把问题简单化：如何依据房屋里边卧室的数量来估计房子的面积？ 在一九七几年的时候啊，当时有过这样一种想法，首先，我们将所有的 RM 数据存下来，还有目标数据，也就是 price 也存下来： 12X_rm = dataframe['RM'].valuesy = dataframe['price'] 存下来之后我们把做一个字典映射： 1234567rm_to_price = {r: y for r, y in zip(X_rm, y)}---{6.575: 24.0, 6.421: 21.6, ... 6.976: 23.9} 这样之后，问题也就相应的做了一个简化。假如有人在销售那里要求买房子，那销售就可以拿出一个字典，里面都是这样的对应关系，然后我们就可以去查一下就知道了。 这个时候假如有人告诉你有一个小区，他平均里边房屋平均是 6.421。那一查就发现这个 6.421 的基本上卖 21 万。那如果小区里房屋数量是 5.57 的时候我卖多少钱？卖 13 万。这都是一一对应的关系。 1234rm_to_price[6.421]---21.6 不过这个时候有一个人说我们那个小区里面平均是 7 个房间，那是多少呢？我们发现，我们的字典里没有超过 7 的数字，也就是没有这么一个对应关系。 那么找不到的时候怎么办呢？我们大部分时候解决问题都会找一个近似值，也就是最接近的数据来做参考。也可以根据以前的数据来做计算，其实也就是一句话的事： 123def find_price_by_simila(history_price, query_x, topn=3): return np.mean([p for x, p in sorted(history_price.items(), key=lambda x_y: (x_y[0] - query_x) **2)[:topn]]) 要根据以前的数据来做计算的话，我们定义了一个方法，传入了参数历史价格以及查询特征。然后我们返回的内容稍微有点复杂，首先给这个房屋进行排序，排序依据是按照 x 和 query 之间的距离来给他排序。排序的时候我们取最接近的这几个数字，这样就能够得到最接近的 x 和 y。然后在 x 和 y 里面我们取它的 price，这就是最接近的 price。 然后我们来看看它给咱们算的如果房间数是 7 的情况是什么价格： 1234find_price_by_simila(rm_to_price, 7)---29.233333333333334 关于排序那里看不懂的小伙伴，我们这里额外花点篇幅开个小灶。这样，假如说我们有下面一组数据： 12345person_and_age = { 'A 张学友': 62, 'C 周杰伦': 44, 'B 毛不易': 29} 然后我们将这组数据改成列表并进行排序： 12345l = list(person_and_age.items())sorted(l)---[('A 张学友', 62), ('B 毛不易', 29), ('C 周杰伦', 44)] 我们可以看到它是按照数据的首字母进行排序的，可是这个时候我们不想以首字母来排序，而是想根据年龄大小进行排序该怎么办？这个时候我们就可以给排序方法的 key 里面定规则，这个规则就是按照元素的第二个下标进行排序。 1234567def get_first_items(element): return element[1]sorted(l, key=get_first_items)---[('B 毛不易', 29), ('C 周杰伦', 44), ('A 张学友', 62)] 我们这里定义了一个函数get_first_items, 其实做了一件很简单的事情，就是获得了element的第二个下标。 那么这里我们其实可以不用这样定义函数，而是直接用匿名函数。关于匿名函数，我在 Python 基础课里也有详细的讲到，大家可以回头去翻看一下。 1sorted(l, key=lambda element: element[1]) 那其实，element 是一个输入参数，是一个变量，所以我们完全可以就简写一下就行： 1sorted(l, key=lambda e: e[1]) 然后我们再在后面多加一个切片操作： 1234sorted(l, key=lambda e: e[1], reverse=True)[:2]---[('A 张学友', 62), ('C 周杰伦', 44)] 不用在意那个reverse=True, 只是打开了反向排序，因为个人情感上不想去掉张学友。 好，那这个时候呢我们在前面加一个for，就可以拿到名字和 age，而我们只需要 age: 1234[age for name, age in sorted(l, key=lambda e: e[1], reverse=True)[:2]]---[62, 44] 这样我们就可以只取两个排序最靠前的年龄值，当然最后，就是mean，取平均值。 1234np.mean([age for name, age in sorted(l, key=lambda e: e[1], reverse=True)[:2]])---53.0 那我们之前所写的函数内容就是这样一段话，拆解之后是不是就能明白了？ 那么刚才讲到的这种方法，你会发现它是在找相似的东西，其实我们定义的这种方法，后来给它起个名字叫做：发现 K 个最相近的邻居，K-Neighbor-Nearest, 简称KNN。 12def knn(history_price, query_x, topn=3): return np.mean([p for x, p in sorted(history_price.items(), key=lambda x_y: (x_y[0] - query_x) **2)[:topn]]) 这种算法之前机器学习的章节里咱们也详细讲过，这是一个非常经典的机器学习算法。关于 KNN 的有优点和缺点，我们之前也讲的很详细。那大家可以回过头取看我关于机器学习 KNN 的部分来学习，这里就不再继续赘述 KNN 的内容了，在这里，我们就了解之前我们所做的这么多内容，其实就是 KNN，就可以了。 好，那这节课的内容就到这里，下一节课，咱们会继续写这一篇未完成的代码，来找到 X_rm 和 y 之间的函数关系。那么代码文件就依然还是18.ipynb。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/18.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E4%BB%8E%E9%9B%B6%E7%90%86%E8%A7%A3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"19. 深度学习 - 用函数解决问题","text":"Hi，你好。我是茶桁。 上一节课，我们从一个波士顿房价的预测开始写代码，写到了 KNN。 之前咱们机器学习课程中有讲到 KNN 这个算法，分析过其优点和缺点，说起来，KNN 这种方法比较低效，在数据量比较大的时候就比较明显。 那本节课，我们就来看一下更加有效的学习方法是什么，A more Efficient Learning Way. 接着我们上节课的代码我们继续啊，有不太了解的先回到上节课里去看一下。 我们X_rm和y如果能够找到这两者之间的函数关系，每次要计算的时候，输入给这个函数，就能直接获得预测值。 那这个函数关系怎么获得呢？我们需要先观察一下，这个时候就到了我们的拟合函数关系。 那既然要观察，当然最好就是将数据可视化之后进行观察： 12import matplotlib.pyplot as pltplt.scatter(X_rm, y) 可以看到，它们之间的关系大体应该这样一种关系： 那这个样子的图我们熟悉不？是不是在线性回归那一张里我们见过？也就是用一根直线去拟合了这些点的一个趋势。 我们把它写出来： \\[ f(x) = k \\cdot rm + b \\] 那我们现在就会把这个问题变成，假设现在的函数k*rm+b，那我们就需要找到一组 k 和 b，然后让它的拟合效果最好。这个时候我们就会遇到一个问题，拟合效果怎样算是好？ 比方说我们现在有一组数据，一组实际的值，还有一组预测值。 123real_y = {3, 6, 7}y_hats = {3, 4, 7}y_hats2 = {3, 6, 6} 问哪个值更好。 我们会发现这两个预测都挺好的，那哪个更好？这个时候我们需要搬出我们的 loss 函数了。 loss 函数就是在我们进行预测的时候，它的信息损失了多少，所以我们称其为损失函数，loss 函数。 \\[ loss(y, \\hat y) = \\frac{1}{N}{\\sum_{i \\in N}}(y_i - \\hat {y_i})^2 \\] y_i - yhat_i 这个值越接近于 0。等于 0 的意思就是每一个预测的 y 都和实际的 y 的值是一样的。那么如果这个值越大指的是预测的 y 和实际的 y 之间差的越大。 那我们在这个地方就可以定义一个函数： 12def loss(y, yhat): return np.mean((np.array(y) - np.array(yhat))** 2) 然后我们直接将两组 yhat 和真实的 real_y 代入进去比对： 123456loss(real_y, y_hats)loss(real_y, y_hats2)---1.33333333333333330.3333333333333333 所以它这个意思是说 yhats2 的效果更好一些。 那我们将上面这个 loss 函数就叫做 Mean Squared Error，就是均方误差，也简称 MSE。咱们现在有了 loss，就有了是非判断的标准了，就可以找到最好的结果。 有了判断标准怎么样来获得最优的 k 和 b 呢？早些年的时候有这么几种方法，第一种是直接用微积分的方法做计算。 \\[ \\begin{align*} loss &amp; = \\frac{1}{N}\\sum_{i\\in N}(y_i - \\hat y)^2 \\\\ &amp; = \\frac{1}{N}\\sum_{i\\in N}(y_i - (kx_i + b))^2 \\\\ \\end{align*} \\] 此时我们是知道 x_i 和 y_i 的值，N 也是常数。那么其实求偏导之后它就可以变化成下面这组式子： \\[ Ak^2 + Bk +C \\\\ A'b^2+B'b+C' \\] A、B、C 是根据我们所知道的 x_i 和 y_i 以及常数 N 来计算出来的数。这个时候 loss 要取极值的时候，我们令其为 loss’，那 loss’就等于-A/2B，或者-A’/2B’。那么这种方法我们就称之为最小二乘法，它是为了最小化 MSE，对 MSE 求偏导数并令其等于零，来找到使 MSE 最小的参数值。 但是为什么后来人们没有用微积方的方法直接做呢？是因为这个函数会变得很复杂，当函数变得极其复杂的时候，学过微积分的同学就应该知道，你是不能直接求出来他的导数的。也就是说当函数变得极其复杂的时候，直接用微积分是求不出来极致点的，所以这种方法后来就没用。 第二种方法，后来人们想了可以用随机模拟的方法来做。 我们首先来在 -100 到 100 之间随机两个值：k 和 b 12VAR_MAX, VAR_MIN = 100, -100k, b = random.randint(VAR_MIN, VAR_MAX), random.randint(VAR_MIN, VAR_MAX) 只拿到一组当然是无从比较的，所以我们决定拿个 100 组的随机值： 123total_times = 100for t in range(total_times): k, b = random.randint(VAR_MIN, VAR_MAX), random.randint(VAR_MIN, VAR_MAX) 然后定义一个值，叫做最小的 loss。这个最小的 loss 一开始取值为无穷大，并且再给两个值，最好的 k 和最好的 b，先赋值为None 12min_loss = float('inf')best_k, best_b = None, None 之后我们要拿预测值来赋值给新的 loss，我们来定义一个函数，它要做的事情很简单，就是返回k*x+b 1234def model(x, k, b): return k*x + bloss_ = loss(y, model(X_rm, k, b)) 接着我们就可以来进行对比了，就会找到那组最好的 k 和 b： 123if loss_ &lt; min_loss: min_loss = loss_ best_k, best_b = k, b 完整的代码如下，当然我们是接着之前的代码写的，所以loss函数和y，还有X_rm都是在之前代码中有过定义的。 1234567891011121314151617181920212223VAR_MAX, VAR_MIN = 100, -100min_loss = float('inf')best_k, best_b = None, Nonedef model(x, k, b): return x * k +btotal_times = 100for t in range(total_times): k, b = random.randint(VAR_MIN, VAR_MAX), random.randint(VAR_MIN,VAR_MAX) loss_ = loss(y, model(X_rm, k, b)) if loss_ &lt; min_loss: min_loss = loss_ best_k, best_b = k, b print(&quot;在{}时刻找到了更好的 k: {}, b: {}，这个 loss 是：{}&quot;.format(t, k, b, loss_))---在0时刻找到了更好的k: 12, b: 89， 这个loss是：20178.46882444269在8时刻找到了更好的k: 2, b: 2， 这个loss是：131.87000511462452在21时刻找到了更好的k: 11, b: -48， 这个loss是：47.340357088932805 如果我们将寻找的次数放大，改为 10**3, 那我们会发现，开始找的很快，但是后面寻找的会越来越慢。 就类似于你现在在一个公司，假设你从刚进去的时候，要达到职位很高，薪水很高。小职员你想一直升职，你可以随机的去做很多你喜欢做的事情，没有人指导你。一开始的时候，你会发觉自己的升职加薪似乎并没有那么困难，但是随着自己越往上，升职的速度就降下来了，因为上面职位并没有那么多了。这个时候你所需要尝试和努力就会越来越多。到后面你每尝试一步，你所需要的努力就会越来越多。 那么这个时候我们就要想，我们怎么样能够让更新频率更快呢？而不要像这样到后面基本上不更新了。 不知道我们是否还记得大学时候的数学知识，假设现在这个 loss 和 k 在一个二维平面上，我们对 loss 和 k 来求一个偏导： \\[ \\frac{\\partial loss}{\\partial k} \\] 这个导数的取值范围就会导致两种情况，当其大于 0 的时候，k 越大，则 loss 也越大，当其小于 0 的时候，k 越大，loss 则越小。 那我们在这里就可以总结出一个规律： \\[ p' = p + (-1)\\frac{\\partial loss}{\\partial p} * \\alpha \\] \\(\\alpha\\)就是一个很小的数，因为我们每次要只能移动很小的一点，不能减小很多。 那有了这个，我们就可以将我们的 k 和 b 应用上去，也就可以得到： \\[ \\begin{align*} k' = k + (-1)\\frac{\\partial loss}{\\partial k} \\cdot \\alpha \\\\ b' = b + (-1)\\frac{\\partial loss}{\\partial b} \\cdot \\alpha \\\\ \\end{align*} \\] 那我们如何使用计算机来实现刚刚讲的这些内容呢？我们先把上面的式子再做一下变化： \\[ k_{n+1} = k_n + -1 \\cdot \\frac{\\partial loss(k, b)}{\\partial k_n} \\\\ b_{n+1} = b_n + -1 \\cdot \\frac{\\partial loss(b, b)}{\\partial b_n} \\] 这个就是所谓的梯度下降。 那现在的问题就变成，如何使用计算机来实现梯度下降。我们就来定义两个求导函数，并且将之前的代码拿过来做一些修改： 12345678910111213141516171819202122232425262728293031323334353637def loss(y, yhat): return np.mean((np.array(y) - np.array(yhat)) ** 2)def partial_k(x, y, k_n, b_n): return 2 * np.mean((y - (k * x + b))*(-x))def partial_b(x, y, k_n, b_n): return 2 * np.mean((y - (k * x + b))*(-1))k,b = random.random(), random.random()min_loss = float('inf')best_k, best_b = None, Nonetotal_times = 500alpha = 1e-3k_b_history = []for t in range(total_times): k = k + (-1) * partial_k(X_rm, y, k, b) * alpha b = b + (-1) * partial_b(X_rm, y, k, b) * alpha loss_ = loss(y, model(X_rm, k, b)) if loss_ &lt; min_loss: min_loss = loss_ best_k, best_b = k, b k_b_history.append([best_k, best_b]) print(&quot;在{}时刻找到了更好的 k: {}, b: {}，这个 loss 是：{}&quot;.format(t, k, b, loss_))---在0时刻找到了更好的k: 0.8391888851738278, b: 0.44333100376779605， 这个loss是：360.000103176194在1时刻找到了更好的k: 1.0586893752129705, b: 0.474203003102507， 这个loss是：312.7942150454931...在498时刻找到了更好的k: 3.587603582169745, b: 0.40777844839877003， 这个loss是：58.761172062586965在499时刻找到了更好的k: 3.587736446932306, b: 0.4069332804559017， 这个loss是：58.760441520932375 其实关于这个内容，我们在机器学习 - 线性回归那一章就介绍过。看不懂这一段的小伙伴可以回过头取好好看一下那一章。 那这样，我们可以发现，之前是间隔很多次才作一词更新，而现在是每一次都会进行更新，一直在减小。这个是因为我们实现了一个「监督」。 在这样的情况下结果就变得更好了，比如我们再将次数调高一点，在全部运行完之后，我们来画个图看看： 123plt.scatter(X_rm, y)plt.scatter(X_rm, best_k * X_rm + best_b, color='orange')plt.plot(X_rm, best_k * X_rm + best_b, color='red') 我们可以看到它拟合出来的点和连接成的直线，和我们上面手动去画的似乎还是有很大差别的。 在刚才的代码里我还做了一件事情，定义了一个k_b_history, 然后将所有的 best_k 和 best_b 都存储到了里面。然后我们随机取几个点，第一个取第 10 个测试点，第二个取第 50 次测试点，第三个我们取第 5000 次，第四个我们取最后一次： 1test_0, test_1, test_2, test_3, test_4 = 0, 10, 50, 5000, -1 然后我们分别画一下这几个点的图： 123456plt.scatter(X_rm, y)plt.scatter(X_rm, k_b_history[test_0][0] * X_rm + k_b_history[test_0][1])plt.scatter(X_rm, k_b_history[test_1][0] * X_rm + k_b_history[test_1][1])plt.scatter(X_rm, k_b_history[test_2][0] * X_rm + k_b_history[test_2][1])plt.scatter(X_rm, k_b_history[test_3][0] * X_rm + k_b_history[test_3][1])plt.scatter(X_rm, k_b_history[test_4][0] * X_rm + k_b_history[test_4][1]) 我们就可以看到，刚开始的时候和最后的一次拟合的线的结果，还有中间一步步的拟合的变化。这条线在往上面一步一步的走。这样我们相当于是透视了它整个获得最优的 k 和 b 的过程。 那这个时候我们来看一下，咱们怎么怎么预测呢？我们可以拿我们的best_k和best_b去输出最后的预测值了： 1234model(7, best_k, best_b)---28.718752244698216 预测出来是 28.7 万。那房间数目为 7 的时候，我们预测出这个价格是 28.7 万，还记得咱们上节课中用 KNN 预测出来的值么？ 1234find_price_by_simila(rm_to_price, 7)---29.233333333333334 是 29 万对吧？现在我们就能看到了，这两种方式预测值基本很接近，都能预测。 那么我们使用函数来进行预测的原因还有一个，就是我们在使用函数在进行学习之后，然后拿模型去计算最后的值，这个计算过程速度会快很多。 好，咱们下节课将会学习怎样拟合更加复杂的函数，因为这个世界上的函数可不仅仅是最简单线性，还得拟合更加复杂的函数。 然后再后面的课程，我们会讲到激活函数，开始接触神经网络，什么是深度学习。 然后我们要来讲解一个很重要的概念，就是反向传播，会讲怎么样实现自动的反向传播。实现了自动的反向传播，我们会基于拓普排序的方法让计算机能够自动的计算它的梯度和偏导。 在讲完这些之后，基本上我们就有了构建一个深度学习神经网络框架的内容了。 好，希望小伙伴们在今天的课程中有所收获。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/19.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E7%94%A8%E5%87%BD%E6%95%B0%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/"},{"title":"14. 机器学习 - KNN &amp; 贝叶斯","text":"Hi，你好。我是茶桁。 咱们之前几节课的内容，从线性回归开始到最后讲到了数据集的处理。还有最后补充了 SOFTMAX。 这些东西，都挺零碎的，但是又有着相互之间的关系，并且也都蛮重要的。并且是在学习机器学习过程当中比较容易忽视的一些内容。 从这节课开始呢，我要跟大家将一些其他的内容。 虽然最近几年用到的方法主要都是深度学习的方法，但是机器学习并不代表就只有深度学习这一种方法。 当然现在的深度学习其实是从线性回归演化来的，都是用一种梯度下降的方式来做。但是呢其实有很多机器学习方法用的不是这种思想。 那接下来就给大家要讲的，就是曾经非常有名，也非常有用的一些方法。这些方法的思想和用法和线性回归的机器学习不太一样。 为什么咱们现在主要用深度学习呢？之所以深度学习很火，原因就是我们的整个机器学习的模型可以像搭乐高积木一样。 比方有一个线性变化，是 sigmoid，然后有一个 Softmax，还有之后大家要学到什么 LSTM，RNCN，还有 Linear regression。他可以互相去连接，可以像玩乐高积木或者说像做电路一样，可以做出来非常复杂的模型。 那么深度学习的模型变得极其复杂之后，上节课我讲过，如果模型特别复杂，可以表证比较复杂的情况，但是需要比较多的数据去拟合它。 如果模型很复杂，就需要比较多的数据。而在现在，最近十几年互联网产生的数据量啊大了很多，所以深度学习这种复杂模型的特点就可以被释放出来了。 而在以前数据量特别小，类似深度学习这种方法，参数也特别多，效果不太好。 那我们为什么要去学习这些老的经典的学习方法的原因，就是咱们有些时候经常会遇到一个情况就是训练数据其实没有那么多，可能也就一两千个，或者说三五千个。就这几千几万个数据，用深度学习模型其实它是很复杂的，效果也不好。因为模型太复杂了，需要的数据比较多。 所以，当数据量比较小的时候，问题比较简单的时候，其实用一些比较经典的方法是比较好的。 第二个原因，像贝叶斯、KNN、还有决策树，这些方法现在虽然慢慢不是主流了，但是他背后的思想其实可以帮助我们做很多事情。 假如说要判断物体是不是相似等等类似的这些情况，这些方法可以很好的启发你。可以去用在其他场景下去解决问题。 第三个原因，传统的机器学习有比较好的可解释性。函数 f(x) 到底是怎么样求得的 y，里边的每一步可以解释的很清楚。 举个例子，现在我们给任何一个人一台计算机，只要给足够的时间，也不用多就一个月，用深度学习模型去做股市的预测，都可以拟合一个函数拟合这个股市的预测程度非常高。 按照这个做法来做的话，我们去股市投一年可以赚 200%。但是你用这个模型去预测未来的时候就不行了。 就是你去预测过去的事情，做训练可以用，不代表未来也可以用。 所以巴菲特如果问你，为啥这个东西可以？你说因为我在收集到的历史数据上做的效果比较好。你觉得巴菲特会信你吗？ 但是老一代的机械学习模型，尤其是 KNN、贝叶斯，可以把它的决策逻辑，为什么决策，为什么得到这个结果的过程给大家讲清楚。 除此之外要说的是，如果你想成为一个技术很厉害的技术达人，或者一个技术专家，那你要注意一件事，心态一定要开放。 深度学习不能从逻辑上解释，只能凭感觉去解释，就是只能人去解释。 就好比一个占卜的人去解释，只能靠人去去阐述。就好比牧师去解读圣经一样。科学的尽头都是玄学是吧？ 之前给大家说过监督学习，监督学习有一个比较数学，比较形式化的定义： 对于一些数据，如上图，数据 D={D1, D2, ..., Dn}。对于这些数据有 x 和 y，y 就是它 label，是 desire 的 output，是期望的输出。 然后我们希望能够学习到一种映射，f:x -&gt; y, 从 x 能够到 y，只要能够实现从 x 到 y 这样的一种映射，那么它就是一种监督学习。 如果这个 y 输出是连续的，那么它就是回归。如果它是 discrete，那么我们就说它是分类。 所以这个 mapping，这种映射关系可以是各种各样的一种映射关系，可以是很多种。 KNN 我们现在要来讲的，就是第一种。除了深度学习，线性回归和逻辑回归之外，咱们要讲的第一种，就是 KNN，又称 K 近临。 KNN 几乎可以说是最简单、最直接、最古老的一种机器学习方法了。 他的原理很简单。 比方说我们现在有这么些点，然后问 X 轴红色五角星的位置向上对应的点在什么位置？ 那 KNN 无法告诉你准确的这个点的位置是多少，但是我们可以利用周围确定的点，也就是圆圈圈定的范围内的这些点，用它们来求一个平均值。 也就是说，离的最近的 k 个值是多少，然后求个平均值就行了。这个好像很有道理的样子。 好，那我们看到，这个其实是解决了回归问题。那现在我们来看看分类问题： 比方说上面这张图，有一些红色的点和一些蓝色的点。那么现在问题就是，?号所在的这些点是什么呢？ 那左边的圈里，离的最近的是一个蓝色，四个红色，那？的这个点就是红色。在右边的这个圈里，红色比蓝色更多，那这个点也是红色。 这样的话，你会发现整个求解起来就很简单。 假如咱们给一组数据 x， 那么我们有一组\\(x_i\\), y 有可能是 Numerical, 也有可能是一个类别。 现在来了一个新的\\(x_i\\), 假如要知道他的值，我们要找到离他最近的。 现在我们有 n 个 x，打比方说有 n=100，在这里 k 假设先定义成 30000, k 也是一个参数可以自己改。 那对应到表格内，如果这个问题是一个回归问题，我们就那最近的点求一个平均值，如果是分类问题，我们就看附近的所有点哪一个分类出现的最多就可以了。 那按照表格内的数据，假如新出现的这个\\(x_i\\)附近是 x1, x2, x3, x4，回归问题就是 (0.38+1.27+3.56+3.19)/4, 分类问题就是 (0, 1, 0)。 这个就是 KNN 的原理，如果要手动去写的话，按照大家水平不到 5 分钟就能把这个分类和回归的全部写完。 它实现起来真的特别简单，而且解释起来也很好解释。比方说它预测出来是红色，为什么是红色呢？因为离我最近的 3 个或者 5 个占大多数的是红色。 它也有一些缺点，比较大的缺点是什么呢？ 一个显著的缺点就是当我们要求的这个点附近完全没有值，离他最近的那个值都离的特别远，那这个时候我们要去求解，它的值就会跑到很远但是离它最近的那些点之间。 KNN 找的是和自己最类似的，但是如果他找不到和自己最类似的，他就傻了。 总结一下 KNN 的优缺点： 优点 缺点 容易实现、容易理解 运行时间长 模型调整容易，可以方便的改变 k 的数量，或者给不同距离的 k 进行加权 容易被异常值影响 适合解决各种复杂问题（分类、回归、高纬、低纬、复杂关系、简单关系） 所需空间大 高纬空间的距离区分度不大 关于这个时间久，我们来看一个问题：KNN 的时间复杂度是多少？ 如果是 2.7Ghz 英特尔 i7 处理器，训练数据有 1 千万哥数据是 300 维，定义的 k=11，则预测 100 个实例需要多久？ 几毫秒、几秒、几分钟、几个小时还是几天？ 2.7Ghz，Ghz 就是一秒钟可以运行一个 G，一个 G 就是 2 的 11 次方。 那么估算下来其实应该是几个小时的操作。 那在一些大厂，阿里，蚂蚁，微信等等，那随随便便都是几千万的数据。 所以 KNN 更合适数据量小的情况。 还有就是我们说一个数学上的概念，当一个向量的维度很高的时候，比如说几百几千维，那个时候的向量就会有一个特点，基本上任意两个向量之间的距离都差不多，区别不是很大。 所以对于 KNN 这种在高纬向量里面做检索，整个距离差的也不大。 我们需要了解一点，机器学习里面分了两种方式，第一种叫 lazy-learning，第二种叫 eager learning。 lazy-learning 就是懒惰的学习，KNN 就是这种方式。KNN 是典型的一种 lazy-learning。 KNN 只是简单的内容记下来，然后去找了一个最接近的东西。lazy-learning 最大的问题就是所观测到的是它周围的这些结果。 Data site 是比较少的维度，其实效果倒也可以。 为什么要用 Lazy 呢，这个其实和东西方的教育观念的一个差别，在我们传统观念中的那种勤奋，就是死记硬背的埋头苦读其实就是一种 Lazy，其含义是思维上比较 Lazy。 然后比较擅长总结归纳，然后分析预测这种我们叫做的 eager。 这种方法看到的更加全面，要看到更加广阔的问题，然后抽象出更高层次的函数。我们把这种叫做 eager learning。 基本上在咱们整个课程里面除了 KNN 算法，别的全部都是 eager。 贝叶斯 接着我们来讲一下贝叶斯，首先我们来看一段文本： 对于某种商品，根据以往购买的数据，在任意投放广告，未进行特点渠道优化的时候，点击广告到购买商品的比率为 7%，自然形成的用户中，本科及以上学历的用户占 15%，本科以下学历占 85%；现在有一笔广告预算，本科及以上学历渠道的获客成本市场 100 元每人，本科以下人群投放广告的成本是 70 元每人。问，该广告投放到本科及以上学历专门的人群还是本科以下人群？（本科及以上学历占总国人的比例约为 5%，2016 年国家统计局数据） 假如遇到这种问题的时候，一般都会有两拨人互相 PK，互相撕扯。一波人认为顾客里面买东西的有 15% 的是本科生，本科生比例还挺高，应该大力发展本科生这部分用户，应该把广告主要往本科生这边投。另外一波人会说，有 85% 的人是本科以下学历的人，而且国家有 95% 的人是本科以下学历，所以这个市场更大，应该去投本科以下的学历。 这是非常实际的一个问题，假如你以后在公司里的遇到这个问题的时候怎么样去估计呢？我们可以做一个比较简单的数学式子，其实现在我们要估算两个概率，第一个概率是本科及以上的人看见了广告买东西的概率是多少，另外一个就是本科以下的人看见广告买东西的概率是多少。 但是我们现在没有这个数据，只有购买的人里边有多少个人是本科以上，有多少是本科以下。 也就是说，我们现在不知道一个本科生看见广告之后有多少概率会买，但是可以通过一个方法来解决。就是本科及以上的人购买的概率，其实等于如下这个方程：P(A|B) = P(AB) / P(B) = P(B|A)P(A) / P(B)。 对应我们现在面对的这个案例，那其实就等于是： Pr(购买 | 本科及以上) = Pr(本科及以上 | 购买) * Pr(购买) / Pr(本科及以上) = 15% * 7% / 5% = 21% Pr(购买 | 本科以下) = Pr(本科以下 | 购买) * Pr(购买) / Pr(本科以下) = 85% * 7% / 95% = 6.26% 贝叶斯其实也就是就是这个方法。 我们根据得出的结论，本科及以上每个人的广告成本为 100 块钱，有 21% 的人会买，所以平均成交一个人需要花 476，100/21 = 476。 同理本科以下的人每个广告成本是 70%, 但是最终会购买东西的概率只有 6%, 所以说最终成本是 1,180 块。70 / 6% = 1167。 那假如咱们的产品卖 2,000，花 100 万广告费只投放给本科以上的人群，那公司就可以收入 420 万，投给本科以下的人，就只能卖 116.7 万。 这个例子是一个非常典型的商业决策例子，这种决策都有个特点，未来的事情谁都说不上。过去的事情板上钉钉的已经发生了。 我们再说一个很典型的例子，一个骰子连续丢出 4 次 6，那第 5 次出现 6 的概率到底是大于 1/6 还是小于 1/6，还是等于 1/6？ 等于 1/6 的说法，因为骰子出不出 1/6 每一次事件之间是独立的，所以说第五次也是 1/6。小于 1/6 的说法，已经出了那么多 6 了，接下来不应该出 6 了。而对于有些人来说，这色子现在明显就是有问题，一个骰子一般来说很少会出现这样的情况，所以这个骰子容易出 6，概率肯定是比 1/6 大。 所以一般来说，比较聪明的决策是把未来看成是过去的再次发生。贝叶斯分类其实就是做这件事情的。 贝叶斯定理表示了在给定先验概率和条件概率的情况下，如何计算后验概率。 \\[ \\begin{align*} P(A|B) = \\frac{P(B|A)*P(A)}{P(B)} \\end{align*} \\] P(A|B) 是后验概率，表示在给定观测数据 B 后事件 A 发生的概率。 P(B|A) 是条件概率，表示事件 A 发生的情况下事件 B 发生的概率。 P(A) 是先验概率，表示事件 A 在没有观测数据 B 的情况下的概率。 P(B) 是边际概率，表示事件 B 发生的总概率。 在朴素贝叶斯分类中，我们使用特征向量 X=(x1,x2,...,xn) 来表示一个样本，其中 x_i 是第 i 个特征的取值。我们希望根据这些特征来分类样本为不同的类别 C。 \\[ \\begin{align*} p(C_k,x_1,...,x_n) &amp; = p(x_1,...,x_n, C_k) \\\\ &amp; = p(x_1|x_2,...,x_n,C_k)p(x_2,...,x_n,C_k) \\\\ &amp; = p(x_1|x_2,...,x_n,C_k)p(x_2|x_3,...,x_n,C_k)p(x_3,...,x_n,C_k) \\\\ &amp; = ... \\\\ &amp; = p(x_1|x_2,...,x_n,C_k)p(x_2|x_3,...,x_n,C_k)...p(x_{n-1}|x_n,C_k)p(x_n|C_k)p(C_k) \\end{align*} \\] 我们要做这个决策，在以上的式子基础上做了一个很重要的假设，假设什所有特征之间的条件是独立的，即给定类别 C 下，特征之间的关系是独立的。这个假设使得计算变得简单，但通常并不成立，尤其实在自然语言处理任务中。 因为这个原因，所以它被称为 Naive Bayes，就是我们通常所称的「朴素贝叶斯」。其实 Naive 真正的翻译应该是「幼稚的」。 简化后，朴素贝叶斯分类器的数学公式： P(C|X) 是后验概率，表示在给定特征向量 X 的情况下，样本属于类别 C 的概率。 P(X|C): 是似然度，表示在类别 C 下观测到特征向量 X 的概率。基于朴素独立性假设，可以将它分解为各个特征的条件概率的乘积：\\(P(X|C) = P(x_1|C)*P(x_2|C)*...*P(x_n|C)\\) P(C) 是先验概率，表示样本属于类别 C 的概率。 P(X) 是归一化常数，用于确保后验概率的总和为 1。 为了进行分类决策，朴素贝叶斯分类器计算每个类别的后验概率，然后选择具有最高后验概率的类别作为分类结果。数学上，这可以表示为： \\[ \\begin{align*} C_{MAP} = \\arg max_cP(C|X) \\end{align*} \\] 其中\\(C_{MAP}\\)是最可能的类别。 比方下面这个列表： Example No. Color Type Origin Stolen? 1 Red Sports Domestic YES 2 Red Sports Domestic NO 3 Red Sports Domestic YES 4 Yellow Sports Domestic NO 5 Yellow Sports Imported YES 6 Yellow SUV Imported NO 7 Yellow SUV Imported YES 8 Yellow SUV Domestic NO 9 Red SUV Imported NO 10 Red Sports Imported YES 列表中分别有 Color，车的颜色，Type，车的型号，Origin，车的产地，以及 Stolen，是否被偷。 我们令\\(x_1\\) = color, \\(x_2\\) = type, \\(x_3\\) = origin，那么\\(x_1,x_2,x_3\\)现在就是特征。 假如基于上面朴素贝叶斯的数学公式，我们得到： \\[ \\begin{align*} P(C_1 |x) = \\frac{分子}{P(x)} \\\\ P(C_2 |x) = \\frac{分子}{P(x)} \\\\ \\end{align*} \\] 我们可以发觉，它们的分母其实都是一样的，都是\\(P(x)\\)。虽然我们并不知道\\(P(x)\\)是多少，但是我们要比较的是\\(P(C_1|x)\\)和\\(P(C_2|x)\\)的大小，所以我们完全可以不考虑一样大小的分母，就只比较分子大小就可以了。 那我们就将式子变为： \\[ \\begin{align*} P(C_1|x) = P(x_1|C_1)P(x_2|C_1)P(x_3|C_1)P(C_1) \\\\ P(C_2|x) = P(x_1|C_2)P(x_2|C_2)P(x_3|C_2)P(C_2) \\end{align*} \\] 我们拿一个来分析，其中的\\(P(x_1|C_1)\\), 假设现在\\(x_1 = Red, C_1 = Stolen(YES)\\), 那其实就是\\(P(Red|Stolen(YES))\\)，也就是所有被偷的里面，Red 占多少。那么现在就简单的数数就行了。同理，各个特征的值我们都可以直接数数就能数出来。 \\(P(C_1)\\)和\\(P(C_2)\\)的概率是多少？\\(P(C_1)\\)和\\(P(C_2)\\)是我们所有的汽车里面，被偷的占比多少，没偷的占比多少，也是数数就可以数出来。 贝叶斯方法就是我们只需要有一张表格，然后通过数数，通过简单的计算就能够把概率给求出来。黄色的车被偷的概率大还是红色车被偷的概率大，直接就可以数出来。 我们现在看到的贝叶斯都是一个一个类别，但是有的时候，假如说我们所面对的不是 0，1 这种类别，而是：3，2，2.5, 1.6 这种实数该怎么办呢？我们就要用到「高斯贝叶斯分布」。 高斯贝叶斯分布是在处理连续值的时候一个非常典型的做法，就是把连续值做一个分布，做一个离散化处理： \\[ \\begin{align*} p(x = v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}e^{-\\frac{(v-\\mu k)^2}{2\\sigma^2_k}} \\end{align*} \\] 我们只需要知道这一点就行了，贝叶斯解决连续值的问题用了高斯分布。 那么贝叶斯公式的优点是什么呢？首先它非常容易被实现，基本上写代码都能写到，就是不断的数数。 而且它的预测其实很快，它其实做出了一个概率函数，不像 KNN 那样还要把所有的数据都存下来，就是存下来了一个判别的数字，直接一乘就可以，特别快。而且在数据量很大的时候效果也比较好。 但是它也有缺点，因为做了一个 if 的假设，就是 X1 和 X2、X3 之间都没有关系，但是其实很多时候 x 之间是有关系的，这其实是很错误的。 所以当问题变得复杂的时候，贝叶斯往往就不行了。比方说要解决复杂的自然语言处理问题、图像识别问题就不行了。 贝叶斯案例 - 预测广告 咱们来看一个贝叶斯的题目。 假设我们现在有三段短信内容：第一段是一段广告：\"快来抢购，这是最大的优惠\"；第二段也是一段广告：\"今天抢购最优惠\"；第三段不是广告，就是一个正常的短信内容：\"今天什么时候回家\" Ad: 快来抢购，这是最大的优惠 Ad: 今天抢购最优惠 Text: 今天什么时候回家 现在我们有一个问题：“今天回家抢购”，我们现在要做的是一个垃圾短信拦截，那么这一段内容是属于广告还是不属于广告？ 那这个问题，我们实际上就可以用贝叶斯来进行解决。 我们假设 S = 今天回家抢购 那么我们要做的事情就是比较这两个概率的大小:Pr(Ad|S) ~ Pr(Text|S)。 根据贝叶斯的公式，我们就可以得到下面这一步： 12Pr(Ad | S) = Pr(Ad | w1 w2 w3)Pr(Text | S) = Pr(Ad | w1 w2 w3) 然后我们可以将其转化为： \\[ \\begin{align*} Pr(Ad|w_1 w_2 w_3) = \\frac{Pr(w_1|Ad)Pr(w_2|Ad)Pr(w_3|Ad)Pr(Ad)}{Pr(w_1w_2w_3)} \\end{align*} \\] 同理呢，Text 和 Ad 一样可以进行转化： \\[ \\begin{align*} Pr(Text|w_1 w_2 w_3) = \\frac{Pr(w_1|Text)Pr(w_2|Text)Pr(w_3|Text)Pr(Text)}{Pr(w_1w_2w_3)} \\end{align*} \\] 其中这些 w1, w2, w3 就是相关的特征，我们将 S 分词成 w1 = 今天，w2 = 回家，w3 = 抢购。 现在我们来看一下，Pr(Ad) 等于多少？等于 2/3，也就是我们这三句话中，已知的广告概率是多少。相对的，Pr(Text) 就是 1/3。 接着我们可以知道。Pr(w1|Ad) 就是“今天”在所有广告里出现了几次， 广告一共是 2 次，“今天”出现了 1 次，所以应该是 1/2。 那 Pr(w2|Ad) 呢，“回家\"没有出现过，不过这里我们要注意，虽然它没有出现过，但是我们不能让它的概率为 0，我们要给它一个估计值，这个叫做 OOV，out of vocabulary。因为直接为 0 的话，这个词有比较奇怪就没法去做了，判断不了。 经过简单的分词，我们一共有 13 个单词，13 个单词里边 w2 出现了一次，所以咱们可以给他一个估计值 1/13。 如果没有这个估计值的话，可能只要在内容里面随机加一些生僻字，就能够躲过系统的检测。 w3 是“抢购”，Pr(w3|Ad) 就是 1。那么 Pr(Ad|S) 的分子部分就是 Pr(Ad) = 2/3, Pr(w1|Ad) = 1/2, Pr(w2|Ad)=1/13, Pr(w3|Ad) = 1。 那我们上节课说过，分母我们其实不用管，也就是 Pr(w1 w2 w3) 在过程中其实是不用计算的。 我们同理再来看以下 Pr(Text|S) 的分子，Pr(Text)=1/3, Pr(w1|Text)=1, Pr(w2|Text)=1, Pr(w3|Text)=2/13。 那我们最后可以得到简单的一个小学式子： \\[ \\begin{align*} Pr(Ad|S) &amp; = 1/2 * 1/13 * 2/3 = 1/13 * 1/3 \\\\ Pr(Text|S) &amp; = 2/13*1/3 \\end{align*} \\] 明显 Pr(Text|S) 更大一点，所以“今天回家抢购”这句话更大的概率下不是广告。 这个就是贝叶斯的一个应用案例，也是非常常见的一个面实题。 好，那这节课的内容就到这里了，要记得复习。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/14.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20KNN%20-%20%E8%B4%9D%E5%8F%B6%E6%96%AF/"},{"title":"17. 机器学习 - 随机森林","text":"[TOC] Hi，你好。我是茶桁。 我们之前那一节课讲了决策树，说了决策树的优点，也说了其缺点。 决策树实现起来比较简单，解释解释性也比较强。但是它唯一的问题就是不能拟合比较复杂的关系。 后来人们为了解决这个问题，让其能够拟合更加复杂的情况，提出来了一种模型，这种模型就叫做随机森林。 随机森林 随机森林之所以叫随机森林，是因为它是由多棵树组成。它结合了决策树和随机性的概念，用于解决分类和回归问题，随机森林由多个决策树组成，每棵树都是随机构建的。 随机森林其核心组成部分是决策树，为了提高模型的性能和泛化能力，所以引入了两种主要形式的随机性。 第一种就是随机选择样本，对于每棵决策树的构建，随机森林从训练数据中随机抽取一部分样本（有放回地抽样），这称为自助采样（Bootstrap Sampling）。这就使得每棵树都在不同的样本子集上进行训练，增加了模型的多样性。 第二种是随机选择特征，在每个节点上，随机森林只考虑特征的一个子集来进行分割决策，而不是考虑所有特征。这确保了每棵树的分裂过程是不同的，增加了多样性。 对于分类问题，随机森林中的每棵决策树都会对输入数据进行分类，那对于回归问题，就会变成是每棵决策树都会对输入数据进行预测了。最后的预测结果是通过对所有树的投票或平均值来获得的。这种集成方法可以减小过拟合奉先，提高模型的稳定性和泛化能力。 使用随机森林来预测。 在预测之前呢，我们使用 Out-Of-Bagging 样本来评估我们的模型。这个 bagging 就是袋子，就是我们从袋子里随机取东西去衡量。 使用评估结果，我们可以选择合适的变量数。 随机森林的原理其实很简单，是一个非常简单但是非常好用的一个方法。基本上，除了深度学习之外，也是企业用的最多的方法之一。咱们在这里就来演示一下随机森林的作用以及效果： 123456from sklearn.datasets import load_irisiris = load_iris()x = iris.datay = iris.targetprint(x, y) 这个是我们用 sklearn 里面鸢尾花分类的数据做个简单例子，快速的展现一下它的效果。我们将数据拿到以后，x 是鸢尾花的四个维度，四个维度对应了它的一个类别。 123456789from sklearn.tree import DecisionTreeClassifiertree_clf = DecisionTreeClassifier()tree_clf.fit(x, y)tree_clf.feature_importances_---array([0.02666667, 0. , 0.05072262, 0.92261071]) 我们 fit 完之后就可以看到，这四个 feature 中，最终要的是第四个 feature。然后是第三个，第二个根本就没用。 12345678910111213from sklearn.model_selection import train_test_splittrain_x, test_x,train_y, test_y = train_test_split(x, y, test_size=0.3, random_state=0)tree_clf = DecisionTreeClassifier()tree_clf.fit(train_x, train_y)print(tree_clf.score(train_x, train_y))print(tree_clf.score(test_x, test_y))---1.00.9777777777777777 看结果我们其实可以看到，这个拟合度有点太高了。我们换个数据再来看，还是之前的课程中我们用到的 Boston 房价的数据，不过因为这个是一个回归问题，所以我们需要用回归预测的方法： 123456789101112131415161718from sklearn.tree import DecisionTreeRegressorfrom sklearn.datasets import fetch_openmldataset = fetch_openml(name='boston', version=1, as_frame=True, return_X_y=False, parser='pandas')data = dataset['data']target = dataset['target']x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2)tree_reg = DecisionTreeRegressor()tree_reg.fit(x_train, y_train)print('whole dataset train acc: {}'.format(tree_reg.score(x_train, y_train)))print('whole dataset test acc: {}'.format(tree_reg.score(x_test, y_test)))---whole dataset train acc: 1.0whole dataset test acc: 0.6606392933985246 现在我们来看，它的 train 上的 score 准确度是 1.0，在 test 上是 0.81，这个是全数据量测试的情况。 然后我们来定义一个函数： 123456789101112131415161718def random_forest(x_train, y_train, x_test, y_test, drop_n=4): features_random = np.random.choice(list(x_train.columns), size=len(x_train.columns)-drop_n) x_sample = x_train[features_random] y_sample = y_train reg = DecisionTreeRegressor() reg.fit(x_sample, y_sample) score_train = reg.score(x_sample, y_sample) score_test = reg.score(x_test[features_random], y_test) print('sub sample :: train score: {}, test score: {}'.format(score_train, score_test)) y_predicated = reg.predict(x_test[features_random]) return y_predicated 咱们随机的从data里面取一些数据，之后我们来看一下单个树的结果： 123456789with_feature_names = pd.DataFrame(data)with_feature_names.columns = dataset['feature_names']x_train, x_test, y_train, y_test = train_test_split(with_feature_names, target, test_size=0.3, random_state=0)random_forest(x_train, y_train, x_test, y_test, 4)---sub sample :: train score: 1.0, test score: 0.5171643497313849 单个的结果显然是要比整个的数据量要差。那么咱们现在看一下最终的结果，把它变成一个森林： 123456789101112tree_num = 4predicates = []for _ in range(tree_num): predicated, score = random_forest(x_train, y_train, x_test, y_test) predicates.append((predicated))print('the mean result is: {}'.format(np.mean(predicates), axis=0))print('the score of forest is: {}'.format(r2_score(y_test, np.mean(predicates, axis=0))))---the mean result is: 21.614144736842107the score of forest is: 0.7194989474162439 从一开始到现在完整的打印结果为： 12345678910whole dataset train acc: 1.0whole dataset test acc: 0.6606392933985246ssub sample :: train score: 1.0, test score: 0.5885292814825753sub sample :: train score: 1.0, test score: 0.559086368163823sub sample :: train score: 1.0, test score: 0.6119989116140754sub sample :: train score: 1.0, test score: 0.21831688326567122the mean result is: 21.614144736842107the score of forest is: 0.7194989474162439 这是个很典型的例子，使用全量的数据集，它的结果最终的在 test 集上是 0.66，然后基本上每个的结都比它要差一些。但当我们用了森林的值做了平均之后，这个值就变得更好了。 当然其实这个值并不是每次都是如此，在我们进行计算的时候，因为数据什么的都是随机的，偶尔也会出现取均值之后变的更差的情况。不过大部分时候，都会更好一些。 我们现在再将结果稍微改一改： 123456789101112131415161718192021222324252627def random_forest(x_train, y_train, x_test, y_test, drop_n=4): ... return y_predicated, score_testtree_num = 4predicates = []for _ in range(tree_num): predicated, score = random_forest(x_train, y_train, x_test, y_test) predicates.append((predicated, score))predicates_value = [v for v, s in predicates]forest_scores = [s for v, s in predicates]print('the score of forest is: {}'.format(r2_score(y_test, np.mean(predicates_value, axis=0))))weights = np.array(forest_scores) / np.sum(forest_scores)score_weights = np.zeros_like(np.mean(predicates_value, axis=0))for i, v in enumerate(predicates_value): score_weights += v * weights[i]print('the score of weighted forest is: {}'.format(r2_score(y_test, score_weights)))---the score of forest is: 0.7049603534192553the score of weighted forest is: 0.7204901503020483 后面这段代码呢，其实就是人们发现用了随机森林之后，效果明显要好了，那一些人就想如果在知道每一次的test_score之后，能不能给test_score比较高的值加一个比较大的权重。 也就是说，当我知道test_score比较好，那在最后做决策的时候给它加的权重大一些。 最后我们打印了常规状态下森林的结果和加权之后的结果。加权之后的结果又变得好了一些。 Adaboost 然后人们沿着这个思路，就做了一件事情，就是 Adaboost(Adaptive Boosting)： Adaboost 就是在随机森林的权重思路上做了一个优化，它的示意图也是有多个 weak classifier, 然后最后有一个 Weighted Voter, 这是一个权重的投票，这个就和我们上面加权的那部分代码非常的类似。只不过它在这里做了个细化： 我们来注意看最后一个公式： \\[ H(x) = sign(\\sum_{t=1}^T\\alpha_th_t(x)) \\] 公式里的\\(\\alpha_t\\)就是它的权重，最终的 H(x) 就是很多\\(\\alpha_t \\cdot h_t(x)\\)加在一起的结果。这里的这个\\(\\alpha_t\\)就是每一次小的数的权重：v * weights[i]。这个权重就不是像咱们刚才代码里那样根据score的大小简单的做个加权。 我们看上图中间又一个\\(\\alpha_t\\)的公式： \\[ \\begin{align*} \\alpha_t = \\frac{1}{2}ln(\\frac{1-\\varepsilon_t}{\\varepsilon_t}) \\end{align*} \\] 然后我们再往上倒腾，\\(\\varepsilon_t\\)是当你预测出来这个值和实际值错的越多，越趋近于 1。如果完全没有错，一个错都没有的情况下，那么\\(\\varepsilon_t=0\\)。 \\[ \\begin{align*} \\varepsilon_t = Pr_{i\\sim D_t}[h_t(x_i)\\ne y_i] \\end{align*} \\] 如果\\(\\varepsilon_t = 1\\)的话，那就是\\(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\)就是：1-1/1=0。ln0 等于什么呢？它等于负的无穷大，那么\\(\\alpha_t\\)等于就没有。如果\\(\\varepsilon_t = 0\\)，\\(ln(\\frac{1-\\varepsilon_t}{\\varepsilon_t})\\)就是无穷大。 也就是说，随着\\(\\varepsilon_t\\)越大，那\\(\\alpha_t\\)会越大，随着\\(\\varepsilon_t\\)越小，\\(\\alpha_t\\)也会越小。而且在这个地方是呈指数变化的，就是误差会对\\(\\alpha_t\\)的变化影响的很大。 除了用指数的东西来做，它还有一个很重要的特性，这个特性才在我们整个 Adaboost 里非常重要： \\[ D_{t+1}(i) = \\frac{D_t(i)exp(-\\alpha_ty_ih_t(x_i))}{Z_t} \\] 我们先来看\\(y_ih_t(x_i)\\)这部分，假设 ht(xi)=1，yi 预测对了等于 1，yi 预测错了等于 -1。那如果预测错了，这整个部分都等于 -1，如果预测对了，这里就是 1。 前面有一个负号： \\(-\\alpha_ty_ih_t(x_i)\\)，那肯定是要变号的。也就是说，如果预测错了，那么这一串东西应该是正的，如果预测对了这一串东西应该是负的。 前面是什么，是\\(D_{t+1}(i)\\), 这里其实就是第 i 个训练元素在\\(D_{t+1}\\)被取到的概率。那么我们最前面有表示\\(D_1(i) = \\frac{1}{m}\\)，也就是说，所有元素被取到的概率都是一样的，是平均的。那第二次的概率就是：\\(D_1(i)\\cdot exp(...)\\), exp 就是 e 的多少次方。 那我们现在知道，如果预测对了，这里是 -1，预测错了这里是 1, 都要再乘以\\(\\alpha_t\\)。那么如果预测对了，这里是\\(-\\alpha_t\\)，那 exp 这里就是小于 1 的。那如果预测错了呢，exp 就是大于 1 的。 如果 exp 大于 1，那么\\(D_{t+1}(i)\\)概率就会被\\(D_t(i)\\)的概率要更大，反之就会更小。 这个就是我们 Ada 的含义，Ada 就是 Adaptive，就是动态调整的意思。也就是通过这种方法实现的。 如果此时此刻\\(x_i\\)算对了，那下一次就更不容易被取到，如果算错了，那下一次训练就会更有可能被取到。 觉得绕的小伙伴去理解这样一个例子：如果你是个学生去做卷子，那么你作对的题还会反复去做吗？肯定是不会的题才会反复刷，刷到自己会为止。 Gradient Boosting 除了 Adaboost 之外，后来人们又提出来了一个新的方法：Gradient Boosting。 Gradient Boosting 和 Adaboost 的核心原理很像： \\[ loss(p, q) = -\\sum_{i\\in output classes}p(x)logq(x) \\] Gradient Boosting 主要用于解决回归和分类问题。它基于决策树（通常是浅层决策树）构建模型，通过迭代改进预测的准确性。 其最核心的就是梯度提升，是一种集成学习方法。将多个弱预测模型，也就是决策树组合在一起，以提高整体性能。每个决策树在不同的数据子集上训练，然后进行组合以生成最终的预测。其核心原理就是通过迭代优化损失函数来构建模型。 在每一步中，模型的更新方向就是损失函数的负梯度。假设我们有一个损失函数 L(y, f(x))，其中 y 是真实标签，f(x) 是当前模型的预测，梯度提升的目标是找到一个新的模型 h(x), 使得损失函数 L(y, f(x) + h(x)) 最小化。 梯度提升使用负梯度方向的决策树 h(x) 来拟合当前模型的残差，因此可以通过以下方式迭代更新模型： \\[ f(x) = f(x) + learning\\_rate \\cdot h(x) \\\\ \\] 也就是说，它其实要变成这样一个式子： \\[ Boosted Ensemble = First Tree + \\eta \\cdot Second Tree \\\\ loss(Boosted Ensemble) &lt; loss(First Tree) \\] 也就是说，我们第二波的 h_t(x)，也就是 h_2(x)，前面乘以一个\\(\\eta\\)，这个\\(\\eta\\)是等于 Learning Rate 的，然后 h_2(x) 加上 h_1(x) 最后得到的结果，要比 h_1(x) 的 loss 值更小。 那么我们现在要做的就是改变\\(\\eta\\)的权重，这个东西的权重就是和之前我们在随机森林里调整权重不同。 一开始，梯度提升初始化一个简单的模型，通常是一个常数，用来拟合目标变量的平均值。 对于每一个训练样本，计算模型的梯度。这表示模型对于每个样本的预测误差。 使用新的决策树来拟合梯度的负梯度，也就是模型的残差。这意味着构建一个决策树，其目标是减小之前模型的误差。 将新构建的决策树与之前的模型相加，以形成一个新的模型。这个过程重复进行多次，每次都会减小误差。 重复 2 到 4 步，直到满足某个停止条件，理入达到最大迭代次数或误差足够小。 最终模型是所有决策树的组合，可以用来进行预测。 那我们之前谈到的\\(\\eta\\)，也就是 learning_rate，其实就是学习率，用于控制每次更新的幅度。 数学上，梯度提升通过迭代不断减小损失函数来逼近最优模型，这是一种梯度下降的优化方法，因此它的核心原理与梯度下降算法是密切相关的。 Grading Boost 和 AdaBoost 的整个区别不大，它们都是属于 Ensemble Learning，中文翻译是合唱团。 这个 Ensemble Learning 我们可以取很多个分类、回归，然后我们把它做好之后给它求一个平均值。 比如这样， 123456789101112131415from sklearn.ensemble import RandomForestClassifierfrom sklearn.ensemble import VotingClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import SVClog_clf = LogisticRegression()rnd_clf = RandomForestClassifier()svm_clf = SVC()voting_clf = VotingClassifier( estimators = [('lr', log_clf),('rf', rnd_clf),('svc', svm_clf)], voting='hard')voting_clf.fit(x_train, y_train)... 在 sklearn 的 ensemble 中本身就有一个 VotingClassifier，也有 RandomForestClassifier，我们可以直接用几个分类器可以实现。 AdaBoost 和 Gradient Boost 也是属于一个典型的 ensemble Learning。 那还有两个比较重要的东西，一个叫做 Xgboost，一个叫做 LightBGM，这两个是 Grading Boost 的升级版。它们被广泛的使用于机器挖掘，推荐系统等等。 当然这两块内容就不放在「核心基础」里讲了，将会在后面讲到 BI 专业课的时候专门的去讲，这两个是很重要的点。 那本节课讲完之后呢，咱们核心基础的部分，关于机器学习就跨过一个小阶段了。下一节课开始，我们要讲「深度学习」了。属于向前要跨一大步。 好，那咱们经典机器学习模型到今天就讲完了。各位看文章的小伙伴，自己去把这个课程再好好巩固一下，咱们下节课开始，就进入深度学习了。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/17.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"},{"title":"21. 深度学习 - 拓朴排序的原理和实现","text":"[TOC] Hi，你好。我是茶桁。 上节课，我们讲了多层神经网络的原理，并且明白了，数据量是层级无法超过 3 层的主要原因。 然后我们用一张图来解释了整个链式求导的过程： 那么，我们如何将这张图里的节点关系来获得它的求导过程呢？ 假如我们现在定义一个函数get_output: 123456789def get_output(graph, node): outputs = [] for n, links in graph.items(): if node == n: outputs += links return outputsget_output(computing_graph, 'k1')---['L1'] 我们可以根据 k1 获得 l1。 来，让我们整理一下思路，问：如何获得 k1 的偏导： 获得 k1 的输出节点 获得 k1 输出节点的输出节点 ...直到我们找到最后一个节点 123456789101112131415computing_order = []target = 'k1'out = get_output(computing_graph, target)[0]computing_order.append(target)while out: computing_order.append(out) out = get_output(computing_graph, out) if out: out = out[0]computing_order---['k1', 'L1', 'sigmoid', 'L2', 'loss'] 我们从 k1 出发，它可以获得这么一套顺序。那么现在如果要计算 k1 的偏导，我们的这个偏导顺序就等于从后到前给它求解一遍。 12345678order = ''for i, n in enumerate(computing_order[:-1]): order += '*∂{} / ∂{}'.format(n, computing_order[i+1])order---'*∂k1 / ∂L1*∂L1 / ∂sigmoid*∂sigmoid / ∂L2*∂L2 / ∂loss' 现在 k1 的求导顺序计算机就给它自动求解出来了，我们把它放到了一个图里面，然后它自动就求解出来了。只不过唯一的问题是现在这个 order 是反着的，需要把它再反过来。 12345678for i, n in enumerate(computing_order[:-1]): order.append((computing_order[i + 1], n)) # order += ' * ∂{} / ∂{}'.format(n, computing_order[i+1])' * '.join(['∂{}/∂{}'.format(a, b) for a, b in order[::-1]])---'∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂k1' 这个过程用计算机实现之后，我们就可以拿它来看一下其他的参数，比如说b1: 123456789101112131415161718192021computing_order = []target = 'b1'out = get_output(computing_graph, target)[0]computing_order.append(target)while out: computing_order.append(out) out = get_output(computing_graph, out) if out: out = out[0]order = []for i, n in enumerate(computing_order[:-1]): order.append((computing_order[i + 1], n)) # order += ' * ∂{} / ∂{}'.format(n, computing_order[i+1])' * '.join(['∂{}/∂{}'.format(a, b) for a, b in order[::-1]])---'∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂b1' k2: 123456...target = 'k2'...---'∂loss/∂L2 * ∂L2/∂k2' 到这里，我们能够自动的求解各个参数的导数了。 然后我们将其封装一下，然后循环一下每一个参数： 1234567891011121314def get_paramter_partial_order(p): ... target = p ... return ...for p in ['b1', 'k1', 'b2', 'k2']: print(get_paramter_partial_order(p))---∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂b1∂loss/∂L2 * ∂L2/∂sigmoid * ∂sigmoid/∂L1 * ∂L1/∂k1∂loss/∂L2 * ∂L2/∂b2∂loss/∂L2 * ∂L2/∂k2 到这一步你就能够发现，每一个参数的导数的偏导我们都可以求解了。而且我们还发现一个问题，不管是['b1', 'k1', 'b2', 'k2']中的哪一个，我们都需要求求解∂loss/∂L2。 所以现在如果有一个内存能够记录结果，先把∂loss/∂L2的值求解下来，把这个值先存下来，只要算出来这一个值之后，再算['b1', 'k1', 'b2', 'k2']的时候直接拿过来就行了。 也就是说我们首先需要记录的就是这个值，其次，如果我们把 L2 和 sigmoid 的值记下来，求解 b1 和 k1 的时候直接拿过来用就行，不需要再去计算一遍，这个时候我们的效率就会提升很多。 首先把共有的一个基础∂loss/∂L2计算了，第二步，有了∂loss/∂L2，把∂L2/∂sigmoid再记录一遍，第三个是∂sigmoid/∂L1,然后后面以此就是∂L1/∂b1, ∂L1/∂k1，∂L2/∂b2, ∂L2/∂k2。 现在的问题就是就是怎么样让计算机自动得到这个顺序，计算机得到这个顺序的时候，把这些值都存在某个地方。 这个所谓的顺序就是我们非常重要的一个概念，在计算机科学，算法里面非常重要的一个概念：「拓朴排序」。 那拓朴排序该如何实现呢？来，我们一起来实现一下： 首先，我们定义一个方法，咱们输入的是一个图，这个图的定义方式是一个 Dictionary，然后里面有一些节点，里面的很多个连接的点： 12345678def topologic(graph): ''' graph: dict { node: [node1, node2, ..., noden] } ''' return None 因为我们要把它的结果存在一个变量里边，当我们不断的检查看这个图，看看它是否为空，然后我们来定义两个存储变量： 12345678def topologic(graph): sorted_node = [] while graph: all_inputs = [] all_outputs = list(graph.keys()) return sorted_node 这里的两个变量，all_inputs和all_outputs, 一个是用来存储所有的输入节点，一个是存储所有的输出节点。 我们还记得我们那个图的格式是什么样的吗？ 1234567891011computing_graph = { 'k1': ['L1'], 'b1': ['L1'], 'x': ['L1'], 'L1':['sigmoid'], 'sigmoid': ['L2'], 'k2': ['L2'], 'b2': ['L2'], 'L2': ['loss'], 'y': ['loss']} 我们看这个数据，那所有的输出节点是不是就是其中的key啊？ 打比方说，我们拿一个短小的数据来做示例： 123456789simple_graph = { 'a': [1, 2], 'b': [3, 4]}list(simple_graph.keys())---['a', 'b'] 那我们这样就拿到了输出节点，并将其放在了一个列表内。 这里说点其他的，Python 3.9 及以上的版本其实都实现了自带拓朴排序，但是如果你的 Python 版本较低，那还是需要自己去实现。这个也是 Python 3.9 里面一个比较重要的更新。 那为什么我们的 value 定义的是一个列表呢？这是因为这个 key，也就是输出值可能会输出到好几个函数里面，因为我们现在拿的是一个比较简单的模型，但是在真实场景中，有可能会输出到更多的节点中。 这里，就获得了所有有输入的节点， simple_graph中，a 输出给了[1,2], b 输出给了[2,3]。 那我们怎么获得所有输入的节点呢？那就应该是 value。 1234list(simple_graph.values())---[[1, 2], [3, 4]] 这样就获得所有有输入的节点。然后就是怎么样把这两个 list 合并。可以有一个简单的方法，一个叫做 reduce 的方法。 1234reduce(lambda a, b: a+b, list(simple_graph.values()))---[1, 2, 3, 4] 这样就把它给它连起来了。 那我们还需要找一个，就是只有输出没有输入的节点，这些该怎么去找呢？其实也就是我们的[k1, b1, k2, b2, y]这些值。 来，我们还是拿刚才的simple_graph来举例，但是这次我们改一下里面的内容： 123456789101112simple_graph = { 'a': ['a', 2], 'b': ['b', 4]}a = list(simple_graph.keys())b = reduce(lambda a, b: a+b, list(simple_graph.values()))print(list(set(b) - set(a)))---[2, 4] 我们没有用循环，而是将其变成了一个集合，然后利用集合的加减来做。 我们的实际代码就可以这样写： 12345678910111213def topologic(graph): sorted_node = [] while graph: all_inputs = reduce(lambda a, b: a+b, list(graph.values())) all_outputs = list(graph.keys()) all_inputs = set(all_inputs) all_outputs = set(all_outputs) need_remove = all_outputs - all_inputs return sorted_node 那现在我们继续往后，如果找到了这些只有输出没有输入的节点之后，我们做一个判断，然后定义一个节点，用来保存随机选择的节点： 12if len(need_remove) &gt; 0: node = random.choice(list(need_remove)) 这个时候 x, b, k, y 都有可能，那么我们随机找一个就行。然后将这个找到的节点从 graph 给它删除。并且将其插入到sorted_node中去，并且返回出来。 123456 if ...: node = random.choice(list(need_remove)) graph.pop(node) sorted_node.append(node)return sorted_node 然后这里还会出一个小问题，我们还是拿一个示例来说： 1234567891011simple_graph = { 'a': ['sigmoid'], 'b': ['loss'], 'c': ['loss']}simple_graph.pop('b')simple_graph---{'a': ['sigmoid'], 'c': ['loss']} 看，我们在删除 node 的时候，其所对应的 value 也就一起删除了，那这个时候，我们最后的输出列表里会丢失最后一个 node。所以，我们在判断为最后一个的时候，需要额外的将其加上，放在 pop 方法执行之前。那我们整个代码需要调整一下先后顺序。 12345678910111213def topologic(graph): sorted_node = [] while graph: ... if len(need_remove) &gt; 0: node = random.choice(list(need_remove)) sorted_node.append(node) if len(graph) == 1: sorted_node += graph[node] graph.pop(node) return sorted_node 现在其实这个代码就已经 OK 了，我们来再加几句话： 12345678...if len(need_remove) &gt; 0: ... for _, links in graph.items(): if node in links: links.remove(node)else: raise TypeError('This graph has circle, which cannot get topological order.')... 我们把它的连接关系，例如现在选择了 k1，我们要把 k1 的连接关系从这些里边给它删掉。 遍历一下 graph，遍历的时候如果删除的 node 在它的输出里边，我们就把它删除。 加上else判断，如果图不是空的，但是最终没有找到，也就是这两个集合作减法，但是得到一个空集，没有找到，那我们就来输出一个错误：This graph has circle, which cannot get topological order. 现在我们可以来实验一下了： 1234567891011121314x, k, b, linear, sigmoid, y, loss = 'x', 'k', 'b', 'linear', 'sigmoid', 'y', 'loss'test_graph = { x: [linear], k: [linear], b: [linear], linear: [sigmoid], sigmoid: [loss], y: [loss]}topologic(test_graph)---['x', 'b', 'k', 'y', 'linear', 'sigmoid', 'loss'] 好，现在让我们来声明一个class node: 12class Node: pass 然后我们先来抽象一下这些节点： 123456789## Our Simple Model Elementsnode_x = Node(inputs=None, outputs=[node_linear])node_y = Node(inputs=None, outputs=[node_loss])node_k = Node(inputs=None, outputs=[node_linear])node_b = Node(inputs=None, outputs=[node_linear])node_linear = Node(inputs=[node_x, node_k, node_b], outputs=[node_sigmoid])node_sigmoid = Node(inputs=[node_linear], outputs=[node_loss])node_loss = Node(inputs=[node_sigmoid, node_y], outputs=None) 现在咱们就把图中每个节点已经给它抽象好了，但是我们发现节点写成这个样子代码是比较冗余。打比方说：node_linear = Node(input=[node_x, node_k, node_b], outputs=[node_sigmoid])，既然我们已经告诉程序node_linear这个节点的输入是[node_x, node_k, node_b]，那其实也就是告诉程序这些节点的输出是node_linear。 好，我们接下来要在class Node里定义一个方法： 123def __init__(self, inputs, outputs): self.inputs = inputs self.outputs = outputs 现在我们根据上面对代码冗余的分析，可以加上这样简单的一句： 123456def __init__(self, inputs=[]): self.inputs = inputs self.outputs = [] for node in inputs: node.outputs.append(self) 把这句加上之后，就可以只在里面输入 inputs 就行了，不用再输入 outputs，代码就变得简单多了： 123456789## Our Simple Model Elements### version - 02node_x = Node()node_y = Node()node_k = Node()node_b = Node()node_linear = Node(inputs=[node_x, node_k, node_b])node_sigmoid = Node(inputs=[node_linear])node_loss = Node(inputs=[node_sigmoid, node_y]) 我们是把每个节点给它做出来了，那么怎么样能够把这个节点给它像串珠子一样串起来变成一张图呢？ 其实我们只要去考察所有的边沿节点就可以了，把所有的 x，y，k 和 b 这种外层的函数给个变量： 1need_expend = [node_x, node_y, node_k, node_b] 咱们再生成一个变量，这个变量是用来通过外沿这些节点，把连接图给生成出来。 12345678910computing_graph = defaultdict(list)while need_expend: n = need_expend.pop(0) if n in computing_graph: continue for m in n.outputs: computing_graph[n].append(m) need_expend.append(m) while里面，当外沿节点的 list 不为空的时候，我们就在里面来取一个点，我们就取第一个吧，取出来并删除。 然后如果这个点我们已经考察过了，那就continue，如果没有，我们对于所有的这个 n 里边的outputs，插入到 computing_graph 的 n 的位置。再插入到外沿节点的 list 内。因为我们现在多了一个扩充节点，所以我们需要给插入进去。 比方说我们这次找出来了 linear，把 linear 也加到这个需要扩充的点一行，然后就可以从 linear 再找到 sigmoid 了。 来，我们看下现在的这个computing_graph: 12345678910computing_graph---defaultdict(list, {&lt;__main__.Node at 0x12053e080&gt;: [&lt;__main__.Node at 0x12053dc30&gt;], &lt;__main__.Node at 0x12053e9b0&gt;: [&lt;__main__.Node at 0x12053ef50&gt;], &lt;__main__.Node at 0x12053d510&gt;: [&lt;__main__.Node at 0x12053dc30&gt;], &lt;__main__.Node at 0x12053c280&gt;: [&lt;__main__.Node at 0x12053dc30&gt;], &lt;__main__.Node at 0x12053dc30&gt;: [&lt;__main__.Node at 0x1202860e0&gt;], &lt;__main__.Node at 0x1202860e0&gt;: [&lt;__main__.Node at 0x12053ef50&gt;]}) 这样就获得出来了，其实是把它变成了刚刚的那个图。这样呢，我们就可以应用topologic来进行拓朴排序了。 12345678910topologic(computing_graph)---[&lt;__main__.Node at 0x12053c280&gt;, &lt;__main__.Node at 0x12053d510&gt;, &lt;__main__.Node at 0x12053e080&gt;, &lt;__main__.Node at 0x12053e9b0&gt;, &lt;__main__.Node at 0x12053dc30&gt;, &lt;__main__.Node at 0x1202860e0&gt;, &lt;__main__.Node at 0x12053ef50&gt;] 但是我们打出来的内容都是一些内存地址，我们还需要改一下这个程序。我们在我们的class Node里多增加一个方法，用于 return 它的名字： 123456def __init__(self, inputs=[], name=None): ... self.name = namedef __repr__(self): return 'Node:{} '.format(self.name) 这样之后，我们还需要改一下节点，在里面增加一个变量name=''。 123node_x = Node(name='x')...node_loss = Node(inputs=[node_sigmoid, node_y], name='loss') 每一个都需要加上，我用...简化了代码。 然后我们再来看： 1234topologic(computing_graph)---[Node:k , Node:x , Node:b , Node:linear , Node:sigmoid , Node:y , Node:loss ] 然后我们来将这段封装起来，变成一个函数： 123456789101112feed_dict = { node_x: 3, node_y: random.random(), node_k: random.random(), node_b: 0.38}def convert_feed_dict_to_graph(feed_dict): need_expend = [n for n in feed_dict] ... return computing_graph 一般来说，很多大厂在建立代码的时候，x, y, k, b这种东西会被称为placeholder，我们创建的need_expend会被称为是feed_dict。所以我们做了这样一个修改，将need_expend拿到方法里取重新获取。 这些节点刚开始的时候没有值，那我们给它一个初始值，我这里的值都是随意给的。 这样，就不仅把最外沿的节点给找出来了，而且还把值给他送进去了，相对来说就会更简单一些。所有定义出来的节点，我们都可以把它变成图关系。 1234topologic(convert_feed_dict_to_graph(feed_dict))---[Node:k , Node:y , Node:b , Node:x , Node:linear , Node:sigmoid , Node:loss ] 咱们现在再定一个点，我们用一个变量存起来： 1sorted_nodes = topologic(convert_feed_dict_to_graph(feed_dict)) 那么咱们现在来模拟一下它的计算过程，模拟神经网络的计算过程。 12345678910111213141516class Node: ... def fowward(self): print('I am {}, I calculate myself value!!!'.format(self.name))for node in sorted_nodes: node.forward()---I am y, I calculate myself value!!!I am x, I calculate myself value!!!I am b, I calculate myself value!!!I am k, I calculate myself value!!!I am linear, I calculate myself value!!!I am sigmoid, I calculate myself value!!!I am loss, I calculate myself value!!! 我们在Node里定义了一个方法forward，从前往后运算，这个时候我们在每个里面加一个向前运算。 这个就是拓朴排序的作用，经过排序之后，那需要在后面计算的节点，就一定会放在后面再进行计算。 好，那我们现在需要区分两个内容，一个是被赋值的内容，一个是需要计算的内容。 刚才我们说过，在大厂的这些地方，x,y,k,b这种东西都被定义为占位符，那我们来修改一下代码： 12345678910111213141516171819202122232425class Node: def __init__(self, inputs=[], name=None): ... def forward(self): print('I am {}, 我需要自己计算自己的值。'.format(self.name)) ... class Placeholder(Node): def __init__(self, name=None): Node.__init__(self, name = name) def forward(self): print('I am {}, 我已经被人为赋值了。'.format(self.name)) def __repr__(self): return 'Node:{} '.format(self.name)### version - 02node_x = Placeholder(name='x')node_y = Placeholder(name='y')node_k = Placeholder(name='k')node_b = Placeholder(name='b')node_linear = Node(inputs=[node_x, node_k, node_b], name='linear')node_sigmoid = Node(inputs=[node_linear], name='sigmoid')node_loss = Node(inputs=[node_sigmoid, node_y], name='loss') 我们创建了一个 Placeholder 类，继承了 Node, 然后我们取修改初始化方法，它是是没有 input 的，只有一个 name。 然后 forward 我们改一下，改成打印已经被赋值的语句。父类 Node 里的 forward 也改一下，改成需要自己计算自己的值。 那我们这个时候将赋值的四个节点改成调用 Placeholder。 接下来，我们需要修改convert_feed_dict_to_graph方法了： 1234567def convert_feed_dict_to_graph(feed_dict): ... while need_expend: ... if isinstance(n, Placeholder): n.value = feed_dict[n] ... ... 我们来检查这个节点是否是 Placeholder，如果是的话，将当前的 feed_dict 赋值给 n.value。来看下结果： 1234567891011for node in sorted_nodes: node.forward()---I am b, 我已经被人为赋值了。I am x, 我已经被人为赋值了。I am k, 我已经被人为赋值了。I am y, 我已经被人为赋值了。I am linear, 我需要自己计算自己的值。I am sigmoid, 我需要自己计算自己的值。I am loss, 我需要自己计算自己的值。 好，到现在为止，咱们只是打了一段文字，问题是对于linear, sigmoid和loss, 到底是怎么计算的呢？ 这个问题，咱们放到下一节课里面去讲，现在咱们这篇文章已经超标了，目测应该超过万字了吧。 好，下节课记得来看咱们具体如何在实现拓朴排序后将计算加进去。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/21.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E6%8B%93%E6%9C%B4%E6%8E%92%E5%BA%8F%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/"},{"title":"20. 深度学习 - 多层神经网络","text":"Hi，你好。我是茶桁。 之前两节课的内容，我们讲了一下相关性、显著特征、机器学习是什么，KNN 模型以及随机迭代的方式取获取 K 和 B，然后定义了一个损失函数（loss 函数），然后我们进行梯度下降。 可以说是又帮大家回顾了一下深度学习的相关知识，但是由于要保证整个内容的连续性，所以这也没办法。 那么接下来的课程里，咱们要来看一下神经网络，怎么样去拟合更加复杂的函数，什么是激活函数，什么是神经网络，什么是深度学习。 然后我们还要来学习一下反向传播，以及如何实现自动的反向传播，什么是错误排序以及怎么样自动的去计算元素的 gradients。梯度怎么样自动求导。 从简单的线性回归函数到复杂的神经网络，从人工实现的求导到自动求导。那我们现在来跟大家一起来看一下。 上一节课结束的时候我们说过，现实生活中绝大多数事情的关系都不是线性的。 比方说，我工作的特别努力，然后就可以升职加薪了。但是其实有可能工作的努力程度和升职加薪程度之间的关系可能并不是一条直线的函数关系。 可能一开始不管怎么努力，薪水都没有什么大的变化，可是忽然有了一个机会，薪水涨的幅度很大，但是似乎没怎么努力。再之后，又趋于一条平行横轴的线，不管怎么努力都无法往上有提升。这是不是咱们这些社畜的真实写照？ 在现实生活中有挺多这样的问题，这样的对应关系。比如艾宾浩斯曲线，再比如细菌生长曲线，很多很多。 经过刚刚的分析我们知道了除了线性函数 (kx+b)，还有一种常见的函数关系式，是一种 s 型的一种函数，这种 s 形的函数在我们整个计算机科学里我们称呼它为sigmoid函数： \\[ \\begin{align*} Sigmoid: f(x) = \\sigma (x) = \\frac{1}{1+e^{-x}} \\end{align*} \\] 这是一个非常常见的函数。我们可以节用 NumPy 库来用代码将它实现出来： 12def sigmoid(x): return 1 / (1 + np.exp(-x)) 把它的图像描述出来： 12sub_x = np.linspace(-10, 10)plt.plot(sub_x, sigmoid(sub_x)) 然后我们来利用一下这个函数，我们定义一个随机线性函数，然后和 sigmoid 函数一起应用画 5 根不同的线： 123456def random_linear(x): k, b = random.random(), random.random() return k*x + bfor _ in range(5): plt.plot(sub_x, random_linear(sigmoid(random_linear(sub_x)))) 这个里面起变化的就是 k 和 b 这两个参数，那我们来调节 k 和 b 的画，就可以变化这条曲线的样式。 除了以上这些函数，我们生活中还会遇到更复杂的函数，甚至很有可能是一个复杂的三维图像。 那这个时候，我们该如何去拟合这么多复杂的函数呢？ 一个比较直接的方法，当然就是我们人为的去观察，比如这个类似于 sin(x): 还有这个 k*sin(kx+b)+b: 当然，这样理论上是可以的，每当我们遇到一个场景之后就自己去提出来一个函数模型，然后把函数模型让机器去拟合出来。 但是这样就会有一个问题，大家就会发现假如你是那个工作者，那你熊猫眼会很严重，因为我们要看到很多这样的场景。现实生活中的问题实在太多了，每一天我们都可能会遇到新的函数。 如果我们每观察一个情况就要去考察，去思考它的这个函数模型是什么，你就会发现你的工作量无穷无尽。而且你会发现一个问题：现在函数能够可视化的，但是如果在现实生活中有很多场景的函数是无法可视化的。 那么这个时候我们就需要其他的一些方法能够拟合更加复杂的函数，这个时候我们怎么样不通过去观察它就能够拟合出复杂的函数呢？其实很简单。 有一个老头子叫做 Hinton，他是 2018 年图灵奖的获得者。 他在一九八几年的时候发了一篇文章，经过他多年的研究，发现人的脑能够做出非常复杂的一些行为，其实我们这个神经元的类型都是很有限的，并没有很多奇怪的东西，就是有很多不同的节点，其实就那么几种。人类就能够进行复杂行为，背后其实就是一些基本的神经元的一些组合。 只不过这些基本的在组合还有一种形式，就是输入进来的会经过一个叫做 activate neurons，就是激活单元，去做一个非线性变化。然后经过不断的这种非线性变化，最后就拟合出来非常复杂的信号。 那非线性变化的这些函数其实都是一样的，就他们背后的逻辑都是一样。只不过有的时候非线性变化的多，有的时候非线性变化的少。 讲了这么多不直观的东西，我们来看点实际的。 1234for _ in range(5): i = random.choice(range(len(sub_x))) output_1 = np.concatenate((random_linear(sub_x[:i]), random_linear(sub_x[i:]))) plt.plot(sub_x, output_1) 然后我们来做两件事，第一个是将 k,b 随机方式改成normalvariate()，第二个在上面的基础上再做一次拆分： 1234567891011def random_linear(x): k, b = random.normalvariate(0,1), random.normalvariate(0,1) return k * x + bfor _ in range(5): i = random.choice(range(len(sub_x))) linear_output = np.concatenate((random_linear(sub_x[:i]), random_linear(sub_x[i:]))) i_2 = random.choice(range(len(linear_output))) output = np.concatenate((sigmoid(linear_output[:i_2]), sigmoid(linear_output[i_2:]))) plt.plot(sub_x, output) 我们来看，这个时候你会发现他生成的这个图像比较奇怪。它生成了很多奇怪的函数，每一次根据不同的参数就形成了不同的函数图像。 迄今为止就两个函数，一个 sigmoid，一个 random linear，但是它生成了很多奇怪的函数。 面对这种函数，这么多层出不穷列举都列举不完的函数，我们怎么样能够每次遇到一个问题就得去提出它这个函数到底是什么样。而且关键是有可能函数维度高了之后都观察不到它是什么关系。 所以我们就会去考虑，怎么样能够让机器自动的去拟合出来更复杂的函数呢？ 我们可以用基本模块经过组合拼接，然后就能够形成复杂函数。而在组合拼接的过程中，我们只需要让机器自动的去把 K1、K2、B1、B2 等等这些参数给它拟合出来就行。 也就是说我们可以通过参数的变化来拟合出来各种各样的函数。 这其实就是深度神经网络的一个核心思想。就是用基本的模块像大家玩积木一样，并不会有很多积木类型给你，只有一些基本的东西，但是通过这些基本的可以造出来特别多复杂的东西。 这个就是背后的原理，通过函数的复合和叠加。这种变化的引起都是由一个线性函数加上一个非线性函数。 其实很大程度上由我们大脑里边这种简单东西可以构成复杂东西得到了启示。只不过人的大脑里边，在脑神经科学里面把这种非线性变化呢叫做 activate neurons，叫做激活神经元。在程序里，我们把这种非线性函数叫 activation function，激活函数。 激活函数的作用就是为了让我们的程序能够拟合非线性关系。如果没有激活函数，咱们的程序永远只能拟合最简单的线性关系，而现实生活中绝大多数关系并不是并不是线性关系。 让机器来拟合更加复杂函数的这种方法，和我们的神经网络很像，就是咱们现在做的这个事情，我们就把它命了个名叫做神经网络。 早些年的时候科学家们有一个理论，人们把一组线性和非线性变化叫做一层。在以前的时候科学家们发现这个层数不能多于三层，就是神经网络的层数不能多于三层。 为什么不能多于 3 层？其实最主要的不是计算量太大的问题，最核心的原因是什么？ 假设我们有一个 f(x) 和一个 x 组成一个平面坐标系，在其中有无数的点，当我们在做拟合的时候，发现了一条直线可以拟合，但是实际上呢，当我们将数据量继续放大的时候，才发现我们的拟合的直线偏离的非常厉害。 我们之前在机器学习的课程里说过，我们要有高精度，就需要有足够的数据量。如果这个时候变成一个三维问题，就需要更多的数据量。没有更多的数据的话，就好比有一个平板在空中，它会摇来摇去，你以为拟合了一个正确的平板，但其实完全不对。 那这个时候，每当我们所需要拟合的参数多一个，多少数据量认为是足够的？这个不一定。这个和整个问题的复杂程度有关系。 后来科学家们发现一个规律，在相似的问题下，我们需要拟合的参数多一个，需要的数据就要多一个数量级。 当变成三层的时候，会发现参数就更多了，而参数变得特别多就会需要特别多的数据量。而早在一九八几年、一九九几年的时候并没有那么多的数据量，就会产生数据量不够的情况，所以模型在现实生活中没法用。 但是随着到二零零几年，再到二零一几年之后，产生了大量的数据。就给我们做函数拟合提供了数据资源，所以数据量是最重要的，数据量决定了这个东西能不能做。而其他的一些，比方说计算、GPU 啊等等，它是加速这个过程的，是让它更方便。 那么后来我们把层数超过 3 层的就叫深度神经网络，机器学习就简称深度学习。这是为什么深度学习在二零一几年的时候才开始火起来。 那现在我们把上一节课的这个问题再拿过来，现在来想想，如果我们把房价的函数关系也写成类似的，linear 和 sigmoid 之间的关系，那会怎么样呢？ 首先，我们的 k 和 b 就会多加一组： 12345678for t in range(total_times): k1 = k1 + (-1) * loss对k1的偏导 b1 = b1 + (-1) * loss对b1的偏导 k2 = k2 + (-1) * loss对k2的偏导 b2 = b2 + (-1) * loss对b2的偏导 loss_ = loss(y, model(X_rm, k1, b1, k2, b2)) ... 然后我们的 model 也会多接受了一组参数，并且我们要将其内部函数关系做一个叠加： 123456def model(x, k1, k2, b1, b2): linear1_output = k1 * x + b1 sigmoid_output = sigmoid(linear1_output) linear2_output = k2 * sigmoid_output + b2 return linear2_output 在这个时候我们要求解的时候就会发现有个问题，我们现在的 loss 是这样的： \\[ \\begin{align*} loss &amp; = \\frac{1}{N}(y_i - \\hat y)^2 \\\\ &amp; = \\frac{1}{N} \\begin{bmatrix} y_1 - (k_2 \\frac{1}{1+e^{-(k_1x+b_1)}}+b_2) \\end{bmatrix} ^2 \\end{align*} \\] 似乎变得有点过于复杂。当前情况下，我们是可以复杂的去求导，但是当函数继续复杂下去的时候，怎么把这个导数求出来呢？ 函数还可以继续叠加，层数还可以写的越来越多。那么怎么样才能给它求出它的导数呢？ 我们再将上面的式子做个变化： \\[ \\frac{1}{N}[l_2(\\sigma(l_1(x))) -y_1]^2 \\] 这样我们就可以将问题进行简化，我们上面代码里loss对k1的偏导就可以写成： \\[ \\frac{\\partial loss}{\\partial l_2} \\cdot \\frac{\\partial l_2}{\\partial \\sigma} \\cdot \\frac{\\partial \\sigma}{\\partial l_1} \\cdot \\frac{\\partial l_1}{\\partial k_1} \\] 同理，loss对b1的偏导就是： \\[ \\frac{\\partial loss}{\\partial l_2} \\cdot \\frac{\\partial l_2}{\\partial \\sigma} \\cdot \\frac{\\partial \\sigma}{\\partial l_1} \\cdot \\frac{\\partial l_1}{\\partial b_1} \\] 这个时候，问题就变成一个可解决的了。 \\(\\frac{\\partial loss}{\\partial l_2}\\)其实就等于\\(\\frac{2}{N}(l_2 - y_1)\\)。 我们继续往后看第二部分，那么这个时候我们可以得到\\(l_2 = k_2 \\cdot \\sigma + b_2\\)，那\\(\\frac{\\partial l_2}{\\partial \\sigma}\\)就等于\\(k_2\\)。 再来看第三部分，\\(\\sigma'(x) = \\sigma(x) \\cdot (1- \\sigma(x)\\), 所以\\(\\frac{\\partial \\sigma}{\\partial l_1} = \\sigma \\cdot (1 - \\sigma)\\)。 最后第四部分，\\(\\frac{\\partial l_1}{\\partial k_1} = x\\)。 这样，我们整个式子就应该变成这样： \\[ \\begin{align*} \\frac{2}{N}(l_2 - y_1) \\cdot k_2 \\cdot \\sigma \\cdot (1 - \\sigma) \\cdot x \\end{align*} \\] 这样的话，我们就把 loss 对于 K1 的偏导就求出来了，这里算是一个突破。本来看起来是很复杂的的一个问题，我们将其拆分成了这样的一种形式。那这种形式，我们把它称作「链式求导」。 但是现在其实还有个问题，这整个一串链式求导的东西是我们通过眼睛求出来的，但是现在怎么样让机器自动的把这一串东西写出来？就是机器怎么知道是这些数字乘到一起？ 换句话说，我们现在把这个问题再形式化一下，定义一个问题。 给定一个模型定义，这个模型里边包含参数{k1, k2, b1, b2}，我们要构建一个程序，让它能够求解出 k1,k2,b1,b2 的偏导是多少。 如果我们想解决这个问题，我们首先要思考一下，\\(k_1, k_2, b_1, b_2, l_1, l_2, \\sigma, y_{true}, loss\\), 它们之间是一种什么样的关系。 观察一下我们会发现它们之间的关系是这样的： \\[ \\begin{align*} &amp; \\{k_1, b_1, x\\} \\to l_1 \\to \\sigma, \\\\ &amp; \\{\\sigma, k_2, b_2\\} \\to l_2, \\\\ &amp; \\{l_2, y_{true}\\} \\to loss \\\\ &amp; \\to 表示的是'输出到'的关系。 \\end{align*} \\] 要用计算机去表示这种关系，是典型的一个数据结构问题，怎么样让计算机合理的去存储它，你会发现这个是一个图案。 这种节点和节点之间通过关系连接起来的就把它叫做图，我们把它先表示成图的样子。 12345678910111213computing_graph = { 'k1': ['L1'], 'b1': ['L1'], 'x': ['L1'], 'L1':['sigmoid'], 'sigmoid': ['L2'], 'k2': ['L2'], 'b2': ['L2'], 'L2': ['loss'], 'y': ['loss']}nx.draw(nx.DiGraph(computing_graph), with_labels = True) 这个就是我们要表达的一个关系，我们把这个变成图。 现在我们将给定的一个 model，这样一个函数变成了这样一张图。计算机里有现成的各种各样的图算法，我们就可以来计算这个图之间的关系了。 现在我们就要根据这个图的表示来思考我们如何求 loss 对 K1 的偏导。那其实，我们可以发现 k1 在末尾出，一直在向前输入直到 loss。换句话说，我们可以通过 k1 一直往图的终点去寻找来找到它求导的这个过程。 也就是说，只要我们的能把模型变成一个图，然后我们就可以根据这些点去找到它们之间节点的对应关系，我们就可以通过这个节点关系来获得它的求导过程了。 那下一节课呢，我们就继续来看一下，如何将这个图的关系，变成一个自动求导的过程。 关注「坍缩的奇点」，第一时间获取更多免费 AI 教程。","link":"/20.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"22. 深度学习 - 自动求导","text":"Hi，你好。我是茶桁。 咱们接着上节课内容继续讲，我们上节课已经了解了拓朴排序的原理，并且简单的模拟实现了。我们这节课就来开始将其中的内容变成具体的计算过程。 linear, sigmoid和loss这三个函数的值具体该如何计算呢？ 我们现在似乎大脑已经有了一个起比较模糊的印象，可以通过它的输入来计算它的点。 让我们先把最初的父类 Node 改造一下： 1234567class Node(): def __init__(self, inputs=[], name=None): ... self.value = None ... 然后再复制出一个，和Placeholder一样，我们需要继承 Node，并且改写这个方法自己独有的内容： 123456789class Linear(Node): def __init__(self, x, k, b, name=None): Node.__init__(self, inputs=[x, k, b], name=name) def forward(self): x, k, b = self.inputs[0], self.inputs[1], self.inputs[2] self.value = k.value * x.value + b.value print('我是{}, 我没有人类爸爸，需要自己计算结果{}'.format(self.name, self.value)) ... 我们新定义的这个类叫Linear, 它会接收 x, k, b。它继承了 Node。这个里面的 forward 该如何计算呢？我们需要每一个节点都需要一个值，一个变量，因为我们初始化的时候接收的 x,k,b 都赋值到了 inputs 里，这里我们将其取出来就行了，然后就是线性方程的公式k*x+b，赋值到它自己的 value 上。 然后接着呢，就轮到 Sigmoid 了，一样的，我们定义一个子类来继承 Node: 123456789101112class Sigmoid(Node): def __init__(self, x, name=None): Node.__init__(self, inputs=[x], name=name) self.x = self.inputs[0] def _sigmoid(self, x): return 1/(1+np.exp(-x)) def forward(self): self.value = self._sigmoid(self.x.value) print('我是{}, 我自己计算了结果{}'.format(self.name, self.value)) ... Sigmoid 函数只接收一个参数，就是 x，其公式为 1/(1+e^{-x})，我们在这里定义一个新的方法来计算，然后在 forward 里把传入的 x 取出来，再将其送到这个方法里进行计算，最后将结果返回给它自己的 value。 那下面自然是 Loss 函数了，方式也是一模一样： 12345678910111213class Loss(Node): def __init__(self, y, yhat, name=None): Node.__init__(self, inputs = [y, yhat], name=name) self.y = self.inputs[0] self.yhat = self.inputs[1] def forward(self): y_v = np.array(self.y.value) yhat_v = np.array(self.y_hat.value) self.value = np.mean((y.value - yhat.value) ** 2) print('我是{}, 我自己计算了结果{}'.format(self.name, self.value)) ... 那我们这里定义成 Loss 其实并不确切，因为我们虽然喊它是损失函数，但是其实损失函数的种类也非常多。而这里，我们用的 MSE。所以我们应该定义为MSE，不过为了避免歧义，这里还是沿用 Loss 好了。 定义完类之后，我们参数调用的类名也就需要改一下了： 1234...node_linear = Linear(x=node_x, k=node_k, b=node_b, name='linear')node_sigmoid = Sigmoid(x=node_linear, name='sigmoid')node_loss = Loss(y=node_y, yhat=node_sigmoid, name='loss') 好，这个时候我们基本完成了，计算之前让我们先看一下sorted_node: 12345678910sorted_node---[Placeholder: y, Placeholder: k, Placeholder: x, Placeholder: b, Linear: Linear, Sigmoid: Sigmoid, MSE: Loss] 没有问题，我们现在可以模拟神经网络的计算过程了： 1234567891011for node in sorted_nodes: node.forward()---我是x, 我已经被人类爸爸赋值为3我是b, 我已经被人类爸爸赋值为0.3737660632429008我是k, 我已经被人类爸爸赋值为0.35915077292816744我是y, 我已经被人类爸爸赋值为0.6087876106387002我是Linear, 我没有人类爸爸，需要自己计算结果1.4512183820274032我是Sigmoid, 我没有人类爸爸，需要自己计算结果0.8101858733432837我是Loss, 我没有人类爸爸，需要自己计算结果0.04056126022042443 咱们这个整个过程就像是数学老师推公式一样，因为这个比较复杂。你不了解这个过程就求解不出来。 这就是为什么我一直坚持要手写代码的原因。c+v大法确实好，但是肯定是学的不够深刻。表面的东西懂了，但是更具体的为什么不清楚。 我们可以看到，我们现在已经将 Linear、Sigmoid 和 Loss 都将值计算出来了。那我们现在已经实现了从 x 到 loss 的前向传播 现在我们有了 loss，那就又要回到我们之前机器学习要做的事情了，就是将损失函数 loss 的值降低。 之前咱们讲过，要将 loss 的值减小，那我们就需要求它的偏导，我们前面课程的求导公式这个时候就需要拿过来了。 然后我们需要做的事情并不是完成求导就好了，而是要实现「链式求导」。 那从 Loss 开始反向传播的时候该做些什么？先让我们把“口号”喊出来： 1234567class Node: def __init__(...): ... ... def backward(self): for n in self.inputs: print('获取∂{} / ∂{}'.format(self.name, n.name)) 这样修改一下 Node，然后在其中假如一个反向传播的方法，将口号喊出来。 然后我们来看一下口号喊的如何，用[::-1]来实现反向获取： 12345678910for node in sorted_nodes[::-1]: node.backward()---获取∂Loss / ∂y获取∂Loss / ∂Sigmoid获取∂Sigmoid / ∂Linear获取∂Linear / ∂x获取∂Linear / ∂k获取∂Linear / ∂b 这样看着似乎不是太直观，我们再将 node 的名称加上去来看就明白很多： 1234567891011121314for node in sorted_nodes[::-1]: print(node.name) node.backward()---Loss获取∂Loss / ∂y获取∂Loss / ∂SigmoidSigmoid获取∂Sigmoid / ∂LinearLinear获取∂Linear / ∂x获取∂Linear / ∂k获取∂Linear / ∂b... 最后的k, y, x, b我就用...代替了，主要是函数。 那我们就清楚的看到，Loss 获取了两个偏导，然后传到了 Sigmoid，Sigmoid 获取到一个，再传到 Linear，获取了三个。那现在其实我们只要把这些值能乘起来就可以了。我们要计算步骤都有了，只需要把它乘起来就行了。 我们先是需要一个变量，用于存储 Loss 对某个值的偏导 12345class Node: def __init__(...): ... self.gradients = dict() ... 然后我们倒着来看，先来看 Loss: 1234567class Loss(Node): ... def backward(self): self.gradients[self.inputs[0]] = '∂{}/∂{}'.format(self.name, self.inputs[0].name) self.gradients[self.inputs[1]] = '∂{}/∂{}'.format(self.name, self.inputs[1].name) print('[0]: {}'.format(self.gradients[self.inputs[0]])) print('[1]: {}'.format(self.gradients[self.inputs[1]])) 眼尖的小伙伴应该看出来了，我现在依然还是现在里面进行「喊口号」的动作。主要是先来看一下过程。 刚才每个 node 都有一个 gradients，它代表的是对某个节点的偏导。 现在这个节点 self 就是 loss，然后我们self.inputs[0]就是 y, self.inputs[1]就是 yhat, 也就是node_sigmoid。那么我们现在这个self.gradients[self.inputs[n]]其实就分别是∂loss/∂y和∂loss/∂yhat，我们把对的值分别赋值给它们。 然后我们再来看 Sigmoid： 123456class Sigmoid(Node): ... def backward(self): self.gradients[self.inputs[0]] = '∂{}/∂{}'.format(self.name, self.inputs[0].name) print('[0]: {}'.format(self.gradients[self.inputs[0]])) 我们依次来看哈，这个时候的 self 就是 Sigmoid 了，这个时候的sigmoid.inputs[0]应该是 Linear 对吧，然后我们整个self.gradients[self.inputs[0]]自然就应该是∂sigmoid/∂linear。 我们继续，这个时候self.outputs[0]就是 loss, loss.gradients[self]那自然就应该是输出过来的∂loss/∂sigmoid，然后呢，我们需要将这两个部分乘起来： 123def backward(self): self.gradients[self.inputs[0]] = '*'.join([self.outputs[0].gradients[self], '∂{}/∂{}'.format(self.name, self.inputs[0].name)]) print('[0]: {}'.format(self.gradients[self.inputs[0]])) 接着，我们就需要来看看 Linear 了： 1234567def backward(self): self.gradients[self.inputs[0]] = '*'.join([self.outputs[0].gradients[self], '∂{}/∂{}'.format(self.name, self.inputs[0].name)]) self.gradients[self.inputs[1]] = '*'.join([self.outputs[0].gradients[self], '∂{}/∂{}'.format(self.name, self.inputs[1].name)]) self.gradients[self.inputs[2]] = '*'.join([self.outputs[0].gradients[self], '∂{}/∂{}'.format(self.name, self.inputs[2].name)]) print('[0]: {}'.format(self.gradients[self.inputs[0]])) print('[1]: {}'.format(self.gradients[self.inputs[1]])) print('[2]: {}'.format(self.gradients[self.inputs[2]])) 和上面的分析一样，我们先来看三个inputs[n]的部分，self 在这里是 linear 了，这里的self.inputs[n]分别应该是x, k, b对吧，那么它们就应该分别是linear.gradients[x], linear.gradients[k]和linear.gradients[b]，也就是∂linear/∂x,∂linear/∂k, ∂linear/∂b。 那反过来，outputs就应该反向来找，那么self.outputs[0]这会儿就应该是 sigmoid。sigmoid.gradients[self]就是前一个输出过来的∂loss/∂sigmoid * ∂sigmoid/∂linear, 那后面以此的[1]和[2]我们也就应该明白了。 然后后面分别是∂linear/∂x,∂linear/∂k, ∂linear/∂b。一样，我们将它们用乘号连接起来。 公式就应该是： \\[ \\begin{align*} \\frac{\\partial loss}{\\partial sigmoid} \\cdot \\frac{\\partial sigmoid}{\\partial linear} \\cdot \\frac{\\partial linear}{\\partial x} \\\\ \\frac{\\partial loss}{\\partial sigmoid} \\cdot \\frac{\\partial sigmoid}{\\partial linear} \\cdot \\frac{\\partial linear}{\\partial k} \\\\ \\frac{\\partial loss}{\\partial sigmoid} \\cdot \\frac{\\partial sigmoid}{\\partial linear} \\cdot \\frac{\\partial linear}{\\partial b} \\\\ \\end{align*} \\] 那同理，我们还需要写一下Placeholder： 12345def Placeholder(Node): ... def backward(self): print('我获取了我自己的 gradients: {}'.format(self.outputs[0].gradients[self])) ... 好，我们来看下我们模拟的情况如何，看看它们是否都如期喊口号了，结合我们之前的前向传播的结果，我们一起来看： 12345678910111213141516171819202122232425262728293031for node in sorted_nodes: node.forward() for node in sorted_nodes[::-1]: print('\\n{}'.format(node.name)) node.backward()---Loss[0]: ∂Loss/∂y[1]: ∂Loss/∂SigmoidSigmoid[0]: ∂Loss/∂Sigmoid*∂Sigmoid/∂LinearLinear[0]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂x[1]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂k[2]: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂bk我获取了我自己的gradients: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂kb我获取了我自己的gradients: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂bx我获取了我自己的gradients: ∂Loss/∂Sigmoid*∂Sigmoid/∂Linear*∂Linear/∂xy我获取了我自己的gradients: ∂Loss/∂y 好，观察下来没问题，那我们现在还剩下最后一步。就是将这些口号替换成真正的计算的值，其实很简单，就是将我们之前学习过并写过的函数替换进去就可以了： 1234567891011121314151617181920212223class Linear(Node): ... def backward(self): x, k, b = self.inputs[0], self.inputs[1], self.inputs[2] self.gradients[self.inputs[0]] = self.outputs[0].gradients[self] * k.value self.gradients[self.inputs[1]] = self.outputs[0].gradients[self] * x.value self.gradients[self.inputs[2]] = self.outputs[0].gradients[self] * 1 ...class Sigmoid(Node): ... def backward(self): self.value = self._sigmoid(self.x.value) self.gradients[self.inputs[0]] = self.outputs[0].gradients[self] * self.value * (1 - self.value) ...class Loss(Node): ... def backward(self): y_v = self.y.value yhat_v = self.y_hat.value self.gradients[self.inputs[0]] = 2*np.mean(y_v - yhat_v) self.gradients[self.inputs[1]] = -2*np.mean(y_v - yhat_v) 那我们来看下真正计算的结果是怎样的： 12345678910111213141516171819202122232425262728for node in sorted_nodes[::-1]: print('\\n{}'.format(node.name)) node.backward()---Loss∂Loss/∂y: -0.402796525409167∂Loss/∂Sigmoid: 0.402796525409167Sigmoid∂Sigmoid/∂Linear: 0.06194395247945269Linear∂Linear/∂x: 0.02224721841122111∂Linear/∂k: 0.18583185743835806∂Linear/∂b: 0.06194395247945269ygradients: -0.402796525409167kgradients: 0.18583185743835806bgradients: 0.06194395247945269xgradients: 0.02224721841122111 好，到这里，我们就实现了前向传播和反向传播，让程序自动计算出了它们的偏导值。 不过我们整个动作还没有结束，就是我们需要将 loss 降低到最小才可以。 那我们下节课，就来完成这一步。","link":"/22.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"title":"23. 深度学习 - 多维向量自动求导","text":"Hi, 你好。我是茶桁。 前面几节课中，我们从最初的理解神经网络，到讲解函数，多层神经网络，拓朴排序以及自动求导。可以说，最难的部分已经过去了，这节课到了我们来收尾的阶段，没错，生长了这么久，终于到迎接成果的时候了。 好，让我们开始。 我们还是用上一节课的代码：21.ipynb。 我们上一节课中，实现了自动计算的部分。 123for node in sorted_nodes[::-1]: print('\\n{}'.format(node.name)) node.backward() 结果我就不打印了，节省篇幅。 那我们到这一步之后，咱们就已经获得了偏导，现在要考虑的问题就是去更新它，去优化它的值。 1234learning_rate = 1e-5for node in sorted_nodes: node.value = node.value + -1 * node.gradients[node] * learning_rate node 的值去更新，就应该等于它本身的值加上一个 -1 乘以它的偏导在乘以一个learning_rate, 我们对这个是不是已经很熟悉了？我们从第 8 节线性回归的时候就一直在接触这个公式。 只不过在这个地方，x, y 的值也要更新吗？它们的值是不应该去更新的，那要更新的应该是 k, b 的值。 那么在这个地方该怎么办呢？其实很简单，我们添加一个判断就可以了： 123for node in sorted_nodes: if node.is_trainable: node.value = node.value + -1 * node.gradients[node] * learning_rate 然后我们给之前定义的类上加一个变量用于判断。 12345class Node: def __init__(..., is_trainable=False): ... self.is_trainable = is_trainable 在这里我们默认是不可以训练的，只有少数的一些是需要训练的。 然后我们在初始化的部分把这个定义的值加上： 12node_k = Placeholder(name='k', is_trainable=True)node_b = Placeholder(name='b', is_trainable=True) 对了，我们还需要将 Placeholder 做些改变： 12345class Placeholder(Node): def __init__(..., is_trainable=False): Node.__init__(.., is_trainable=is_trainable) ... ... 这就意味着，运行 for 循环的时候只有 k 和 b 的值会更新，我们再加几句话： 123456789for node in sorted_nodes: if node.is_trainable: ... cmp = 'large' if node.gradients[node] &gt; 0 else 'small' print('{}的值{}，需要更新。'.format(node.name, cmp))---k的值small，需要更新。b的值small，需要更新。 我们现在将 forward, backward 和 optimize 的三个循环封装乘三个方法： 12345678910111213141516171819def forward(graph_sorted_nodes): # Forward for node in sorted_nodes: node.forward()def backward(graph_sorted_nodes): # Backward for node in sorted_nodes[::-1]: print('\\n{}'.format(node.name)) node.backward()def optimize(graph_sorted_nodes, learning_rate=1e-3): # optimize for node in sorted_nodes: if node.is_trainable: node.value = node.value + -1 * node.gradients[node] * learning_rate cmp = 'large' if node.gradients[node] &gt; 0 else 'small' print('{}的值{}，需要更新。'.format(node.name, cmp)) 然后我们再来定义一个 epoch 方法，将 forward 和 backward 放进去一起执行： 123def run_one_epoch(graph_sorted_nodes): forward(graph_sorted_nodes) backward(graph_sorted_nodes) 这样，我们完成一次完整的求值 - 求导 - 更新，就可以写成这样： 12run_one_epoch(sorted_nodes)optimize(sorted_nodes) 为了更好的观察，我们将所有的 print 都删掉，然后在 backward 方法中写一个观察 loss 的打印函数： 123456def backward(graph_sorted_nodes): # Backward for node in sorted_nodes[::-1]: if isinstance(node, Loss): print('loss value: {}'.format(node.value)) node.backward() 然后我们来对刚才完整的过程做个循环： 12345678910111213141516# 完整的一次求值 - 求导 - 更新：for _ in range(10): run_one_epoch(sorted_nodes) optimize(sorted_nodes, learning_rate=1e-1)---loss value: 0.12023025149136042loss value: 0.11090709486917472loss value: 0.10118818479676453loss value: 0.09120180962480523loss value: 0.08111466190584131loss value: 0.0711246044819575loss value: 0.061446239826641165loss value: 0.05229053883349982loss value: 0.043842158831920566loss value: 0.036239620745126 可以看到 loss 在一点点的下降。当然，这样循环 10 次我们还能观察出来，但是我们如果要成百上千次的去计算它，这样可就不行了，那我们需要将 history 存下来，然后用图来显示出来： 123456789loss_history = []for _ in range(100): ... _loss_node = sorted_nodes[-1] assert isinstance(_loss_node, Loss) loss_history.append(_loss_node.value) optimize(sorted_nodes, learning_rate=1e-1)plt.plot(loss_history) 我们现在可以验证一下，我们拟合的 yhat 和真实的 y 之间差距有多大，首先我们当然是要获取到每个值的下标，然后用 sigmoid 函数来算一下： 1234sorted_nodes---[k, y, x, b, Linear, Sigmoid, Loss] 通过下标来进行计算，k 是 0，x 是 2，b 是 3，y 是 1： 12345678910111213def sigmoid(x): return 1/(1+np.exp(-x))# k*x+bsigmoid_x = sorted_nodes[0].value * sorted_nodes[2].value + sorted_nodes[3].valueprint(sigmoid(sigmoid_x))# yprint(sorted_nodes[1].value)---0.8911654796019810.8988713384533658 可以看到，非常的接近。那说明我们拟合的情况还是不错的。 好，这里总结一下，就是我们有了拓朴排序，就能向前去计算它的值，通过向前计算的值就可以向后计算它的值。那现在其实我们已经完成了一个 mini 的深度学习框架的核心内容，咱们能够定义节点，能够前向传播运算，能够反向传播运算，能更新梯度了。 那接下来是不是就结束了呢？很遗憾，并没有，接着咱们还要考虑如何处理多维数据。咱们现在看到的数据都是 x、k、b 的输入，也就是都是一维的。 然而咱们真实世界中大多数场景下其实都是多维度的，其实都是多维数组。那么多维数组的还需要更新些什么，和现在有什么区别呢？ 我们来接着往后看，因为基本上写法和现在这些几乎完全一样，那我也就不这么细致的讲了。 为了和之前代码做一个区分，所以我将多维向量计算的代码从新开了个文件，放在了23.ipynb里，小伙伴可以去下载到本地研习。 那么多维和现在最大的区别在哪里呢？就在于计算的时候，我们就要用到矩阵运算了。只是值变成了矩阵，运算变成的了矩阵运算。好，我们从 Node 开始来改动它，没什么变化的地方我就直接用...来省略了： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162class Node: def __init__(self, input=[]): ... def forward(self): raise NotImplemented def backward(self): raise NotImplementedclass Placeholder(Node): def __init__(self): Node.__init__(self) def forward(self, value=None): ... def backward(self): self.gradients = {self:0} for n in self.outputs: grad_cost = n.gradients[self] self.gradients[self] = grad_cost * 1class Linear(Node): def __init__(self, x, k, b): ... def forward(self): ... def backward(self): self.gradients = {n: np.zeros_like(n.value) for n in self.inputs} for n in self.outputs: grad_cost = n.gradients[self] self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T) self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost) self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)class Sigmoid(Node): def __init__(self, node): Node.__init__(self, [node]) def _sigmoid(self, x): ... def forward(self): ... def backward(self): self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x)) self.gradients = {n: np.zeros_like(n.value) for n in self.inputs} for n in self.outputs: grad_cost = n.gradients[self] self.gradients[self.inputs[0]] = grad_cost * self.partialclass MSE(Node): # 也就是之前的 Loss 类 def __init__(self, y, a): Node.__init__(self, [y, a]) def forward(self): y = self.inputs[0].value.reshape(-1, 1) a = self.inputs[1].value.reshape(-1, 1) assert(y.shape == a.shape) self.m = self.inputs[0].value.shape[0] self.diff = y - a self.value = np.mean(self.diff**2) def backward(self): self.gradients[self.inputs[0]] = (2 / self.m) * self.diff self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff 类完成之后，我们还有一些其他的方法： 1234567891011121314151617def forward_and_backward(graph): # run_one_epoch for n in graph: n.forward() for n in graph[::-1]: n.backward()def toplogic(graph): ...def convert_feed_dict_to_graph(feed_dict): ...# 将 sorted_nodes 赋值从新定义了一个方法def topological_sort_feed_dict(feed_dict): graph = convert_feed_dict_to_graph(feed_dict) return toplogic(graph)def optimize(trainables, learning_rate=1e-2): for node in trainables: node.value += -1 * learning_rate * node.gradients[node] 这样就完成了。可以发现基本上代码没有什么变动，变化比较大的都是各个类中的 backward 方法，因为要将其变成使用矩阵运算。 我们来尝试着用一下这个多维算法，我们还是用波士顿房价的那个数据来做一下尝试： 123456789101112131415161718192021222324252627282930313233343536373839404142X_ = data['data']y_ = data['target']# Normalize dataX_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)n_features = X_.shape[1]n_hidden = 10W1_ = np.random.randn(n_features, n_hidden)b1_ = np.zeros(n_hidden)W2_ = np.random.randn(n_hidden, 1)b2_ = np.zeros(1)# Neural networkX, y = Placeholder(), Placeholder()W1, b1 = Placeholder(), Placeholder()W2, b2 = Placeholder(), Placeholder()l1 = Linear(X, W1, b1)s1 = Sigmoid(l1)l2 = Linear(s1, W2, b2)cost = MSE(y, l2)feed_dict = { X: X_, y: y_, W1: W1_, b1: b1_, W2: W2_, b2: b2_}epochs = 5000# Total number of examplesm = X_.shape[0]batch_size = 16steps_per_epoch = m // batch_sizegraph = topological_sort_feed_dict(feed_dict)trainables = [W1, b1, W2, b2]print(&quot;Total number of examples = {}&quot;.format(m)) 我们在中间定义了 l1, s1, l2, cost, 分别来实例化四个类。然后我们就需要根据数据来进行迭代计算了，定义一个 losses 来保存历史数据： 12345678910111213141516171819202122232425262728293031losses = []epochs = 100for i in range(epochs): loss = 0 for j in range(steps_per_epoch): # Step 1 X_batch, y_batch = resample(X_, y_, n_samples=batch_size) X.value = X_batch y.value = y_batch # Step 2 forward_and_backward(graph) # set output node not important. # Step 3 rate = 1e-2 optimize(trainables, rate) loss += graph[-1].value if i % 100 == 0: print(&quot;Epoch: {}, Loss: {:.3f}&quot;.format(i+1, loss/steps_per_epoch)) losses.append(loss/steps_per_epoch)---Epoch: 1, Loss: 194.170...Epoch: 4901, Loss: 3.137 可以看到它 loss 下降的非常快，还记得咱们刚开始的时候在训练波士顿房价数据的时候，那个 loss 下降到多少？最低是不是就下降到在第一节课的时候我们的 lose 最多下降到了多少 47.34 对吧？那现在呢？直接下降到了 3，这是为什么？因为我们的维度多了，维度多了它就准确了。这说明什么？说明大家去谈恋爱的时候，不要盯着对象的一个方面，多方面考察，才能知道这个人是否合适。 好，现在看起来效果是很好，但是我们想知道到底拟合出来的什么函数，那怎么办？咱们把这个维度降低成三维空间就可以看了。 现在咱们这个波士顿的所有数据实际上是一个 15 维的数据，15 维的数据你根本看不了，咱们现在只要把 x 这个里边取一点值，在这个里边稍微把值给它变一下。 12X_ = dataframe[['RM', 'LSTAT']]y_ = data['target'] 在咱们之前的课程中对其进行计算的时候就分析过，RM 和 LSTAT 是影响最大的两个特征，我们还是来用这个。然后我们将刚才的代码从新运行一遍： 123456789losses = []for i in tqdm_notebook(range(epochs)): ...---Epoch: 1, Loss: 150.122...Epoch: 4901, Loss: 16.181 这次下降的就没上次好了。 现在我们可视化一下这个三维空间来看看： 1234567891011121314151617181920212223from mpl_toolkits.mplot3d import Axes3Dpredicate_results = []for rm, ls in X_.values: X.value = np.array([[rm, ls]]) forward_and_backward(graph) predicate_results.append(graph[-2].value[0][0])%matplotlib widgetfig = plt.figure(figsize=(10, 10))ax = fig.add_subplot(111, projection='3d')X_ = dataframe[['RM', 'LSTAT']].values[:, 0]Y_ = dataframe[['RM', 'LSTAT']].values[:, 1]Z = predicate_resultsrm_and_lstp_price = ax.plot_trisurf(X_, Y_, Z, color='green')ax.set_xlabel('RM')ax.set_ylabel('% of lower state')ax.set_zlabel('Predicated-Price') 然后我们就能看到一个数据的三维图形，因为我们开启了 widget，所以可以进行拖动。 从图形上看，确实符合房间越多，低收入人群越少，房价越高的特性。 那现在计算机确实帮我们自动的去找到了一个函数，这个函数到底怎么设置咱们都不用关心，它自动就给你求解出来，这个就是深度学习的意义。咱们经过这一系列写出来的东西其实就已经能够做到。 我觉得这个真的有一种数学之美，它从最简单的东西出发，最后做成了这样一个复杂的东西。确实很深其，并且还都在我们的掌握之中。 好，大家下来以后记得要多多自己敲代码，多分析其中的一些过程和原理。","link":"/23.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%20%E5%AE%8C%E6%88%90%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A1%86%E6%9E%B6/"},{"title":"24. 深度学习进阶 - 矩阵运算的维度和激活函数","text":"Hi，你好。我是茶桁。 咱们经过前一轮的学习，已经完成了一个小型的神经网络框架。但是这也只是个开始而已，在之后的课程中，针对深度学习我们需要进阶学习。 我们要学到超参数，优化器，卷积神经网络等等。看起来，任务还是蛮重的。 行吧，让我们开始。 矩阵运算的维度 首先，我们之前写了一份拓朴排序的代码。那我们是否了解在神经网络中拓朴排序的作用。我们前面讲过的内容大家可以回忆一下，拓朴排序在咱们的神经网络中的作用不是为了计算方便，是为了能计算。 换句话说，没有拓朴排序的话，根本就没法计算了。Tensorflow和PyTourh最大的区别就是，Tensorflow在运行之前必须得把拓朴排序建好，PyTorch是在运行的过程中自己根据我们的连接状况一边运行一边建立。但是它们都有拓朴排序。 拓朴排序后要进行计算，那就要提到维度问题，在进行机器学习的时候一定要确保我们矩阵运算的维度正确。我们来看一下示例就明白我要说的了。 12345x = torch.from_numpy(np.random.random(size=(4, 10)))print(x.shape)---torch.Size([4, 10]) 假如说，现在我们生成了一个4，10的矩阵，也就是4行10列。 123456from torch import nnlinear = nn.Linear(in_features=10, out_features=5).double()print(linear(x).shape)---torch.Size([4, 5]) 然后我们来给他定义一个线性变化，in_features=10，这个就是必须的, 然后，out_features=5，假如把它分成5类。 这个时候，你看他就变成一个四行五列的一个东西了。 刚才我们说了，in_features=10是必须的，如果这个值我们设置成其他的，比如说8，那就不行了，运行不了。会收到警告：mat1 and mat2 shapes cannot be multiplied (4x10 and 8x5) 我们再给它来一个Softmax 12nonlinear = nn.Softmax()print(nonlinear(linear(x))) 这样，我们就得到了一个4*5的概率分布。 我们把这个非线性函数换一下，换成Sigmoid, 之前的Softmax赋值给yhat, 咱们做一个多层的： 12345yhat = nn.Softmax()nonlinear = nn.Sigmoid()linear2 = nn.linear(in_features=n, out_features=8).double()print(yhat(linear2(nonlinear(linear(x))))) 好，这个时候，我还并没有给in_features赋值，我们来想想，这个时候应该赋值是多少？也就是说，我们现在的linear2到底传入的特征是多少？ 我们这里定义的linear和linear2其实就是w*x+b。 那这里我们来推一下，第一次使用linear的时候，我们得到了4*5的矩阵对吧？nonlinear并没有改变矩阵的维度。现在linear2中，那我们in_features赋值就得是5对吧？ 12345678yhat = nn.Softmax()nonlinear = nn.Sigmoid()linear2 = nn.linear(in_features=5, out_features=8).double()print(yhat(linear2(nonlinear(linear(x)))).shape)---torch.Size([4, 8]) 然后我们就得到了一个4*8的维度的矩阵。 那其实在PyTorch里提供了一种比较简单的方法，就叫做Sequential： 1234567891011model = nn.Sequential( nn.Linear(in_features=10, out_features=5).double(), nn.Sigmoid(), nn.Linear(in_features=5, out_features=8).double(), nn.Softmax(),)print(model(x).shape)---torch.Size([4, 8]) 这样，我们就把刚才几个函数方法按顺序都一个一个的写在Sequential里，那其实刚才的过程，也就是解释了这个方法的原理。 接着，我们来写一个ytrue: 123456789ytrue = torch.randint(8, (4, ))loss_fn = nn.CrossEntropyLoss()print(model(x).shape)print(ytrue.shape)---torch.Size([4, 8])torch.Size([4]) 现在ytrue就是CrossEntropyLoss输入的一个label值。 然后我们就可以进行反向传播了： 123456789loss.backward()for p in model.parameters(): print(p, p.grad) ---Parameter containing:tensor([...])... 求解反向传播之后就可以得到它的梯度了。然后再经过一轮一轮的训练，就可以把梯度稳定在某个值，这就是神经网络进行学习的一个过程。那主要是在这个过程中，一定要注意矩阵前后的大小。 激活函数 然后我们来看看激活函数的重要性。 在我们之前的课程中，我们提到过一个概念「激活函数」，不知道大家还有没有印象。那么激活函数的作用是什么呢？ 是实现非线性拟合对吧？ 打比方来说，如果我们现在要拟合一个函数f(x) = w*x+b, 你把它再给送到一个g(x)， 再比如g(x)=w2*x+b，我们来做一个拟合，那么g(f(x)), 那是不是还是一样，g(f(x)) = w2*(w*x+b)+b, 然后就变成w2*w*x + w2*b + b， 那其实这个就还是一个线性函数。 我们每一段都给它进行一个线性变化，再进行一个非线性变化，再进行一个线性变化，一段一段这样折起来，理论上它可以拟合任何函数。 这个怎么理解？其实我们如何用已知的函数去拟合函数在高等数学里边是一个一直在学习，一直在研究的东西。学高数的同学应该知道，高数里面有一个著名的东西叫做傅立叶变化，这是一种线性积分变换，用于函数在时域和频域之间的变换。 我们给定任意一个复杂的函数，都可以通过sin和cos来把它拟合出来，其关键思想是任何连续、周期或非周期的函数都可以表示为正弦和余弦函数的组合。通过计算不同频率的正弦和余弦成分的系数an和bn， 我们可以了解一个函数的频谱特性，即它包含那些频率成分。 \\[ \\begin{align*} f(x) = a_0 + \\sum_{n=1}^0(a_n cos(2\\pi nfx) + b_n sin(2\\pi n fx)) \\end{align*} \\] 除此之外，我们还有一个泰勒展开。我在数学篇的时候有仔细讲解过这个部分，大家可以回头去读一下我那篇文章，应该是数学篇第13节课，在那里我曾说过，所有的复杂函数都是用泰勒展开转换成多项式函数计算的。 之前有同学给我私信，也有同学在我文章下留言，说到某个位置看不懂了，还是数学拖了后腿。但是其实只是应用的话无所谓，但是如果想在这个方面有所建树，想要做些不一样的东西出来，还是要把数学的东西好好补一下的。 OK，那其实呢，我们的深度学习本质上其实就是在做这么一件事情，就是来自动拟合，到底是由什么构成的。 大家再来想一下，一个比较重要的，就是反向传播和前向传播。这个我们前面的课程里有详细的讲过，就是，我们的前向传播和反向传播的作用是什么。 那现在我们学完前几节了，回过头来我们想想，前向传播的作用是什么？反向传播的这个作用呢？ 现在，假如说我已经训练出来了一个模型，我要用这个模型去预测。那么第一个问题是，预测的时候需不需要求loss？第二个是我需不需要做反向传播？ 然后我们再来思考一个问题，如果我们需要求loss对于某个参数wi的偏导\\(\\frac{\\partial loss}{\\partial w_i}\\)，那么我们首先需要进行反向传播对吧？那我们在进行反向传播之前，能不能不进行前向传播？ 也就是说，我们把这个模型放在这里，一个x，然后输入进去得到一个loss。那么咱们训练了一轮之后，我们能不能在求解的时候不进行前向传播，直接进行反向传播？ 我们只要知道，求loss值需要预测值就明白了。 那我们继续来思考，loss值和precision、recall等等的关系是什么？这些是什么？我们之前学习过，这些是评测指标对吧？也就是再问，loss和评测指标的关系是什么？ 也就是说，我们能不能用precision，能不能用precision来做我们的loss函数？不能对吧，无法求导。 所以在整个机器学习的过程中，如果要有反向传播、梯度下降，必须得是可导的。像我们所说的MSE是可导的，cross-entropy也是可以求导的。 那如果上过我之前课程的同学应该记得，可求导的的函数需要满足什么条件？光滑性和连续性对吧？连续性呢，是可求导的一个必要条件，但不是充分条件，还必须在某个点附近足够光滑，以使得导数存在。 对于loss函数的设定，第一点，一定是要能求偏导的。第二呢，就是它一定得是一个凸函数:Convex functions。 那什么叫做凸函数呢？如果一个函数上的任意两点连线上的函数值都不低于这两点的函数值的线段，就称为凸函数。常见的比如线性函数，指数函数，幂函数，绝对值函数等都是凸函数。 想象一下，有一辆车，从a点开到b点，如果这个车在a点到b点的时候方向盘始终是打在一个方向的，那我们就说它是凸函数。 不过在一些情况下有些函数它不是凸函数，就在数学上专门有一个研究领域，Convex optimization，凸优化其实就是解决对于这种函数怎么样快速的求出他的基值，另外一个就是对于这种非凸函数怎么把它变成凸函数。 不同的激活函数它有什么区别呢？在最早的时候，大家用的是Sigmoid： \\[ \\begin{align*} \\sigma(z)=\\frac{1}{1+e^{-z}} \\end{align*} \\] 为什么最早用Sigmoid，这是因为Sigmoid有个天然的优势，就是它输出是0-1，而且它处处可导。 但是后来Sigmoid的结果有个e^x，指数运算就比较费时，这是第一个问题。第二个问题是Sigmoid的输出虽然是在0~1之间，但是平均值是0.5，对于程序来说，我们希望获得均值等于0，STD等于1。我们往往希望把它变成这样的一种函数，这样的话做梯度下降的时候比较好做。 于是就又提出来了一个更简单的方法，就是反正切函数：Tanh。 \\[ \\sigma(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\] 它的形式和Sigmoid很像，不同的是平均值，它的平均值是0。现在这个用的也挺多。 但是Tanh和sigmoid一样都有一个小问题，就是它的绝大多数地方loss都等于0, 那么wi大部分时候就没有办法学习，也就不会更新。 为了解决这个问题，就是有人提出来了一种非常简单的方法，就是ReLU： \\[ \\begin{align*} ReLU(z) = \\begin{cases} z, z&gt;0 \\\\ 0, otherwise \\end{cases} \\end{align*} \\] 这种方法看似非常简单，但其实非常好用。它就是当一个x值经过ReLU的时候，如果它大于0就还保持原来的值，如果不大于0就直接把它变成0。 这样大家可能会觉得x&lt;0时有这么多值没有办法求导，但其实比起sigmoid来说可求导的范围其实已经变多了。而且你会发现要对他x大于0的地方求偏导非常的简单，就直接等于1。 可以保证它肯定是可以做更新的，而且ReLU这种函数它是大量的被应用在卷积神经网络里边。 在咱们后面的课程中，会讲到卷积，它是有一个卷积核，[F1,F2,F3,F4]然后把它经过ReLU之后，可能会变成[F1，0，F3，0]。那我们只要更新F1，F3就可以了，下一次再经过某种方式，在重新把F2和F4我们重新计算一下。 也就是说现在的wx+b不像以前一样，只有一个w，如果x值等于0，那整个都等于0. 而是我们会有一个矩阵，它部分等于0也没关系。而且它的求导会变得非常的快，比求指数的导数快多了。 那其实这里还有一个小问题，面试的时候可能会问到，就是ReLU其实在0点的时候不可导，怎么办？ 这个很简单，可以在函数里边直接设置一下，直接给他一个0的值就可以了，就是在代码里面加一句话。 再后来，又有人提出了一种方法：LeakyRelU： \\[ LeakyReLU(z) = \\begin{cases} z, z&gt;0 \\\\ az, otherwise \\end{cases} \\] 它把小于0的这些地方，也加了一个很小的梯度，这样的话大于0的时候partial就恒等于1，小于的时候partial也恒等于一个值，比如定一个a=0.2, 都可以。那这样就可以实现处处有导数。 但是其实用的也不太多，因为我们事实上发现在这种卷积神经网络里边，我们每一次把部分的权重设置成0不更新，反而可以提升它的训练效率，我们反而可以每次把训练focus on在几个参数上。 好，下节课，咱们来看看初始化的内容。","link":"/24.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"},{"title":"25. 深度学习进阶 - 权重初始化，梯度消失和梯度爆炸","text":"Hi，你好。我是茶桁。 咱们这节课会讲到权重初始化、梯度消失和梯度爆炸。咱们先来看看权重初始化的内容。 权重初始化 机器学习在我们使用的过程中的初始值非常的重要。就比如最简单的wx+b，现在要拟合成一个yhat，w如果初始的过大或者初始的过小其实都会比较有影响。 假设举个极端情况，就是w拟合的时候刚刚就拟合到了离x很近的地方，我们想象一下，这个时候是不是学习起来就会很快？所以对于深度学习模型权重的初始化是一个非常重要的事情，甚至有人就说把初始化做好了，其实绝大部分事情就已经解决了。 那么我们怎么样获得一个比较好的初始化的值？首先有这么几个原则 我们的权重值不能设置为0。 尽量将权重变成一个随机化的正态分布。而且有更大的X输入，那我们的权重就应该更小。 \\[ \\begin{align*} loss &amp; = \\sum(\\hat y - y_i)^2 \\\\ &amp; = \\sum(\\sum w_ix_i - y_i)^2 \\end{align*} \\] 我们看上面的式子，yhat就是w_i*x_i, 这个时候x_i可能是几百万，也可能是几百。我们w_i取值在(-n, n)之间，那当x_i维度特别大的时候，那yhat值算出来的也就会特别大。所以，x_i的维度特别大的时候，我们期望w_i值稍微小一些，否则加出来的yhat可能就会特别大，那最后求出来的loss也会特别大。 如果loss值特别大，可能就会得到一个非常的梯度。那我们知道，学习的梯度特别大的话，就会发生比较大的震荡。 所以有一个原则，就是当x的dimension很大的时候, 我们期望的它的权重越小。 那后来就有人提出来了一个比较重要的初始化方法，Xavier初始化。这个方法特别适用于sigmoid激活函数或反正切tanh激活函数，它会根据前一层和当前层的神经元数量来选择初始化的范围，以确保权重不会过大或过小。 \\[ \\begin{align*} 均值为0和标准差的正态分布: \\sigma &amp; = \\sqrt{\\frac{2}{n_{inputs}+n_{outputs}}} \\\\ -r和+r之间的均匀分布：r &amp; = \\sqrt{\\frac{6}{n_{inputs}+n_{outputs}}} \\end{align*} \\] 然后W的均匀分布就会是这样： \\[ W \\sim U \\Bigg \\vert -\\frac{\\sqrt 6}{\\sqrt{n_j + n_{j+1}}}, \\frac{\\sqrt 6}{\\sqrt{n_j + n_{j+1}}} \\] 这个是一个比较有名的初始化方法，如果要做函数的初始化的话，PyTorch在init里面有一个方法： 1torch.nn.init.xavier_uniform_(tensor, gain=1.0) 比如，我们看这样例子： 12w = torch.empty(3, 5)nn.init.xavier_uniform_(w, gain=nn.calculate_gain('relu')) 注意: init方法里还有其他的一些方法，大家可以查阅PyTorch的相关文档：https://pytorch.org/docs/stable/nn.init.html 梯度消失与梯度爆炸 当我们的模型层数特别多的时候 就比如我们上节课用到的Sequential，我们可以在里面写如非常多的一个函数： 1234567891011model = nn.Sequential( nn.Linear(in_features=10, out_features=5).double(), nn.Sigmoid(), nn.Linear(in_features=5, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), ... nn.Linear(in_features=8, out_features=8).double(), nn.Softmax(),) 这样，在做偏导的时候我们其中几个值特别小，那两个一乘就会乘出来一个特别特别小的数字。最后可能会导致一个结果，\\(\\frac{\\partial loss}{\\partial wi}\\)的值就会极小，它的更新就会特别的慢。我们把这种东西就叫做梯度消失，也有人叫梯度弥散。 以Sigmoid函数为例，其导数为 \\[ \\begin{align*} \\sigma '(x) = \\sigma(x)(1-\\sigma(x)) \\end{align*} \\] 在x趋近正无穷或者负无穷时，导数接近0。当这种小梯度在多层网络中相乘的时候，梯度会迅速减小，导致梯度消失。 除此之外还有一种情况叫梯度爆炸，剃度爆炸类似，当模型的层很多的时候，如果其中某两个值很大，例如两个102，当这两个乘起来就会变成104。乘下来整个loss很大，又会产生一个结果，我们来看这样一个场景： 假如说对于上图中这个函数来说，横轴为x, 竖轴为loss，对于这个xi来说，这个地方\\(\\frac{\\partial loss}{\\partial xi}\\)已经是一个特别大的数字了。 假设咱们举个极端的情况（忽略图中竖轴上的数字），我们现在loss等于x^4：\\(loss=x^4\\)，然后现在\\(\\frac{\\partial loss}{\\partial x^4}\\)就等于\\(4x^3\\)，我们假设x在A点，当x=10的时候，那\\(4\\times x^3 = 4000\\)， 那我们计算新的xi，就是\\(x_i = x_i - \\alpha \\cdot \\frac{\\partial loss}{\\partial x_i}\\)，现在给alpha一个比较小的数，我们假设是0.1，那式子就变成\\(10 - 0.1 \\times 4000\\)，结果就是-390。 我们把它变到-390之后，本来我们本来做梯度下降更新完，xi期望的是loss要下降，但是我们结合图像来看，xi=-390的时候，loss就变得极其的巨大了，然后我们在继续，(-390)^4， 这个loss就已经爆炸了。 再继续的时候，会发现会在极值上跳来跳去，loss就无法进行收敛了。所以我们也要拒绝这种情况的发生。 那梯度消失和梯度爆炸这两个问题该如何解决呢？我们来看第一种解决方法： Batch normalization，批量归一化。 那这个方法的核心思想是对神经网络的每一层的输入进行归一化，使其具有零均值和单位方差。 那么首先，对于每个mini-batch中的输入数据，计算均值和方差。\\(B = \\{x_1...m\\}\\); 要学习的参数: \\(\\gamma,\\beta\\)。 \\[ \\begin{align*} \\mu_B &amp; = \\frac{1}{m}\\sum^m_{i=1}x_i \\\\ \\sigma ^2_B &amp; = \\frac{1}{m}\\sum_{i=1}^m(x_i-\\mu_B)^2 \\\\ &amp; \\mu 为均值mean， \\sigma为方差 \\end{align*} \\] 这里和咱们之前讲x做normalization的时候其实是特别相似，基本上就是一件事。 然后我们使用均值和方差对输入进行归一化，使得其零均值和单位方差，即将输入标准化为xhat。 \\[ \\begin{align*} \\hat x_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma ^2_B + \\varepsilon}} \\end{align*} \\] 接着我们对归一化后的输入应用缩放和平移操作，以允许网络学习最佳的变换。 \\[ \\begin{align*} y_i = \\gamma \\hat x_i + \\beta \\equiv BN_{\\gamma,\\beta}(x_i) \\end{align*} \\] 输出为\\(\\{y_i = BN_{\\gamma,\\beta}(x_i)\\}\\)。 最后将缩放和平移后的数据传递给激活函数进行非线性变换。 它会输入一个小批量的x值， 经过反复的梯度下降，会得到一个gamma和beta，能够知道在这一步x要怎么样进行缩放，在缩放之前会经历刚开始的时候那个normalization一样，把把过小值会变大，把过大值会变小。 我们在之前的课程中演示过，没看过和忘掉的同学可以往前翻看一下。 然后在经过这两个可学习的参数进行一个变化，这样它可以做到在每一层x变化不会极度的增大或者极度的缩小，可以让我们的权值保持的比较稳定。 那除了Batch normalization之外，还有一个方法叫Gradient clipping， 它是可以直接将过大的梯度值变小。 它其实很简单，也叫做梯度减脂。 如果我们求解出来\\(\\frac{\\partial loss}{\\partial w_i}\\)很大，假设原来等于400，我们定义了一个100，那超过100的部分，就全部设置成100。 123train_loss.backward()pt.nn.units.clip_grad_value_(model.parameters(), 100)optimizer.step() 简单粗暴。那其实梯度爆炸还是比较容易解决的，比较复杂的其实是梯度消失的问题。 梯度爆炸为什么比较容易解决？梯度爆炸起码是有导数的，只要把这个导数给它放的特别小就行了，有导数起码保证wi可以更新。 假设alpha，我们的learning_rate等于0.01，乘上一个100，可以保证每次可以有个变化。但是每次这个梯度特别小，假如都快接近于0了，那么1e-10, 就算乘上100倍，最后还是一个特别小的数字。所以相较而言，梯度爆炸就更好解决一些，方法更粗暴一些。 补充一个知识点，这个虽然现在已经用不到了，但是对我们的理解还是有帮助的。方法比较古老。 就是当我们发现梯度有问题的时候， 大概在10年前，那个时候神经网络的模块也不太丰富，很多新出的model，做神经网络的人，一些导数，传播什么的都需要自己写，就我们前几节课写那个神经网络框架的时候做的事。 有的时候导数写错了，就有一种方法叫做gradient checking，梯度检查。 这个使用场景非常的少，当你自己发明了一个新的模块，加到这个模型里面的时候会遇到。 其实很简单，就是把最终的\\(\\frac{\\partial loss}{\\partial w_i}\\)，求解出来的偏导总是不收敛，可能是这个偏导有问题，那么有可能求导的函数写错了。 那在这个时候就可以做个简单的变化： \\[ \\begin{align*} \\frac{\\partial loss(\\theta+\\varepsilon)-\\partial loss(\\theta - \\varepsilon)}{2\\varepsilon} \\end{align*} \\] 这其中\\(\\partial loss(\\theta + \\varepsilon)\\)和\\(\\partial loss(\\theta - \\varepsilon)\\)是在参数\\(\\theta\\), 其实也就是我们的wi上添加和减去微小扰动theta后的损失函数值。 然后我们计算数值梯度和反向传播计算得到的梯度之间的差异。通常这是通过计算它们之间的差异来完成，然后将其与一个小的阈值，比如1e-7进行比较。如果差异非常小（小于阈值），则可以认为梯度计算是正确的，否则可能就需要从新写一下偏导函数了。 这个比较难，但不是一个重点，当且仅当自己要发明一个模型的时候。 那接下来我们来看一下关于Learning_rate和Early Stopping的问题。 理论上，如果深度学习效果不好，那么我们可以将learning rate调小，可以让所有模型效果变得更好，它可以让所有的loss下降。 但是如果你的learning rate变得特别小，假如说是1e-9，那这样的结果就是w的变化会非常的慢，训练时间就变得很长。为了解决这个问题，就有一些比较简单的方法。 第一个，我们可以把learning rate和loss设置成一个相关的函数，例如说loss越小的时候，Learning rate越小，或者随着epoch的增大，loss越小。这个就叫learning rate的decay。 将learning rate或者训练次数和loss设置成一个相关的函数，那么越到后面效果越好的时候，learning rate就会越小。 还有，我们可能会发现loss连续k次不下降，那我们就可以提前结束训练过程，这个就是Early Stopping。 也就是当你发现loss连续k次不下降，或者甚至于在上升，那么这个时候，就可以将最优的这个值给它记录下来。 咱们可能会经常出现的情况就是值在那里震荡，本来呢已经快接近于最优点了，可是震荡了几次之后，还可能震荡出去了，loss变大了。或者就一直在这个震荡里边出不去，这个时候多学习也没有用，所以就可以早点停止，这个就是Early Stopping，中文有人称呼它为早停方法。 好，下节课，咱们要讲一个重点，也是一个难点。就是咱们做机器学习的时候，不同的优化方法。","link":"/25.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/"},{"title":"26. 深度学习进阶 - 深度学习的优化方法","text":"Hi, 你好。我是茶桁。 上一节课中我们预告了，本节课是一个难点，同时也是一个重点，大家要理解清楚。 我们在做机器学习的时候，会用不同的优化方法。 SGD 上图中左边就是Batch Gradient Descent，中间是Mini-Batch Gradient Descent, 最右边则是Stochastic Gradient Descent。 我们还是直接上代码写一下来看。首先我们先定义两个随机值，一个x，一个ytrue： 123import numpy as npx = np.random.random(size=(100, 8))ytrue = torch.from_numpy(np.random.uniform(0, 5, size=(100, 1))) x是一个1008的随机值，ytrue是1001的随机值，在0到5之间，这100个x对应着这100个ytrue的输入。 然后我们来定义一个Sequential, 在里面按顺序放一个线性函数，一个Sigmoid激活函数，然后再来一个线性函数，别忘了咱们上节课所讲的，要注意x的维度大小。 1234567linear = torch.nn.Linear(in_features=8, out_features=1)sigmoid = torch.nn.Sigmoid()linear2 = torch.nn.Linear(in_features=1, out_features=1)train_x = torch.from_numpy(x)model = torch.nn.Sequential(linear, sigmoid, linear2).double() 我们先来看一下训练x和ytrue值的大小： 123456print(model(train_x).shape)print(ytrue.shape)---torch.Size([100, 1])torch.Size([100, 1]) 然后我们就可以来求loss了，先拿到预测值，然后将预测值和真实值一起放进去求值。 1234567loss_fn = torch.nn.MSELoss()yhat = model(train_x)loss = loss_fn(yhat, ytrue)print(loss)---36.4703 我们现在可以定义一个optimer， 来尝试进行优化，我们来将之前的所做的循环个100次，在其中我们加上反向传播： 1234567891011121314optimer = torch.optim.SGD(model.parameters(), lr=1e-3)for e in range(100): yhat = model(train_x) loss = loss_fn(yhat, ytrue) loss.backward() print(loss) optimer.step()---tensor(194.9302, dtype=torch.float64, grad_fn=&lt;MseLossBackward0&gt;)...tensor(1.9384, dtype=torch.float64, grad_fn=&lt;MseLossBackward0&gt;) 可以看到，loss会一直降低。从194一直降低到了2左右。 在求解loss的时候，我们用到了所有的train_x，那这种方式就叫做Batch gradient Descent，批量梯度下降。 它会对整个数据集计算损失函数相对于模型参数的梯度。梯度是一个矢量，包含了每个参数相对与损失函数的变化率。 这个方法会使用计算得到的梯度来更新模型的参数。更新规则通常是按照一下方式进行： \\[ \\begin{align*} w_{t+1} = w_t - \\eta \\triangledown w_t \\end{align*} \\] \\(w_{t+1}\\)是模型参数，\\(\\eta\\)是学习率， \\(\\triangledown w_t\\)是损失函数相对于参数的梯度。 但是在实际的情况下这个方法可能会有一个问题，比如说，我们在随机x的时候参数不是100，而是10^8，维度还是8维。假如它的维度很大，那么会出现的情况就是把x给加载到模型里面运算的时候，消耗的内存就会非常非常大，所需要的运算空间就非常大。 这也就是这个方法的一个缺点，计算成本非常高，由于需要计算整个训练数据集的梯度，因此在大规模数据集上的计算成本较高。而且可能会卡在局部最小值，难以逃离。说实话，我上面演示的数据也是尝试了几次之后拿到一次满意的，也遇到了在底部震荡的情况。 在这里可以有一个很简单的方法，我们规定每次就取20个: 123456789for e in range(100): for b in range(100 // 20): batch_index = np.random.choice(range(len(train_x)), size=20) yhat = model(train_x[batch_index]) loss = loss_fn(yhat, ytrue[batch_index]) loss.backward() print(loss) optimer.step() 这样做loss也是可以下降的，那这种方法就叫做Mini Batch。 还有一种方法很极端，就是Stochhastic Gradient Descent，就是每次只取一个个数字: 123for e in range(100): for b in range(100 // 1): ... 这种方法很极端，但是可以每次都可以运行下去。那大家就知道，有这三种不同的优化方式。 这样的话，我们来看一下，上图中的蓝色，绿色和紫色，分别对应哪种训练方式？ 紫色的是Stochastic Gradient Descent，因为它每次只取一个点，所以它的loss变化会很大，随机性会很强。换句话说，这一次取得数据好，可能loss会下降，如果数据取得不好，它的这个抖动会很大。 绿色就是Mini-Batch, 我们刚才20个、20个的输入进去，是有的时候涨，有的时候下降。 最后蓝色的就是Batch Gradient Descent， 因为它x最多，所以下降的最稳定。 但是因为每次x特别多内存，那有可能就满了。内存如果满了，机器就没有时间去运行程序，就会变得特别的慢。 MOMENTUM 我们上面讲到的了这个式子： \\[ \\begin{align*} w_{t+1} = w_t - \\eta \\triangledown w_t \\end{align*} \\] 这个是最原始的Grady descent, 我们会发现一个问题，就是本来在等高线上进行梯度下降的时候，它找到的不是最快的下降的那条线，在实际情况中，数据量会很多，数量会很大。比方说做图片什么的，动辄几兆几十兆，如果要再加载几百个这个进去，那就会很慢。这个梯度往往可能会变的抖动会很大。 那有人就想了一个办法去减少抖动。就是我们每一次在计算梯度下降方向的时候，连带着上一次的方向一起考虑，然后取一个比例改变了原本的方向。那这样的话，整个梯度下降的线就会平缓了，抖动也就没有那么大，这个就叫做Momentum, 动量。 \\[ \\begin{align*} v_t &amp; = \\gamma \\cdot v_{t-1} + \\eta \\triangledown w_t \\\\ w_{t+1} &amp; = w_t - v_t \\end{align*} \\] 动量在物理学中就是物体沿某个方向运动的能量。 之前我们每次的wt是直接去减去学习率乘以梯度，现在还考虑了v{t-1}的值，乘上一个gamma，这个值就是我们刚才说的取了一个比例。 就像这个图一样，原来是红色，加了动量之后就变成蓝色，可以看到更平稳一些。 RMS-PROP 除了动量法之外呢，还有一个RMS-PROP方法，全称为Root mean square prop。 \\[ \\begin{align*} S_{\\frac{\\partial loss}{\\partial w}} &amp; = \\beta S_{\\frac{\\partial loss}{\\partial w}} + (1 - \\beta)||\\frac{\\partial loss}{\\partial w} ||^2 \\\\ S_{\\frac{\\partial loss}{\\partial b}} &amp; = \\beta S_{\\frac{\\partial loss}{\\partial b}} + (1 - \\beta)||\\frac{\\partial loss}{\\partial b} ||^2 \\\\ w &amp; = w - \\alpha \\frac{\\frac{\\partial loss}{\\partial w}}{\\sqrt{S_{\\frac{\\partial loss}{\\partial w}}}} \\\\ b &amp; = b - \\alpha \\frac{\\frac{\\partial loss}{\\partial b}}{\\sqrt{S_{\\frac{\\partial loss}{\\partial b}}}} \\end{align*} \\] 这个方法看似复杂，其实也是非常简单。这些方法在PyTorch里其实都有包含，我们可以直接调用。我们在这里还是要理解一下它的原理，之后做事的时候也并不需要真的取从头写这些玩意。 在讲它之前，我们再回头来说一下刚刚求解的动量法，动量法其实已经做的比较好了，但是还是有一个问题，它每次的rate是人工定义的。也就是我们上述公式中的\\(\\gamma\\), 这个比例是人工定义的，那在RMS-PROP中就写了一个动态的调整方法。 这个动态的调整方法就是我们每一次在进行调整w或者b的时候，都会除以一个根号下的\\(S_{\\frac{\\partial loss}{\\partial w}}\\)，我们往上看，如果\\(\\frac{\\partial loss}{\\partial w}\\)比较大的话，那么\\(S_{\\frac{\\partial loss}{\\partial w}}\\)也就将会比较大，那放在下面的式子中，根号下，也就是\\(\\sqrt{S_{\\frac{\\partial loss}{\\partial w}}}\\)在分母上，那么w就会更小，反之则会更大。 所以说，当这一次的梯度很大的时候，这样一个方法就让\\(\\frac{\\partial loss}{\\partial w}\\)其实变小了，对b来说也是一样的情况。 也就说，如果上一次的方向变化的很严重，那么这一次就会稍微的收敛一点，就会动态的有个缩放。那么如果上一次变化的很小，那为了加速它，这个值反而就会变大一些。 所以说他是实现了一个动态的学习率的变化，当然它前面还有一个初始值，这个\\(\\gamma\\)需要人为设置，但是在这个\\(\\gamma\\)基础上它实现了动态的学习速率的变化。 动态的学习速率考察两个值，一个是前一时刻的变化的快慢，另一个就是它此时此刻变化的快慢。这个就叫做RMS。 ADAM 那我们在这里，其实还有一个方法：ADAM。 \\[ \\begin{align*} V_{dw} &amp; = \\beta_1V_{dw} + (1-\\beta_1)dw \\\\ V_{db} &amp; = \\beta_1V_{db} + (1-\\beta_1)db \\\\ S_{dw} &amp; = \\beta_2S_{dw} + (1-\\beta_2)||dw||^2 \\\\ S_{db} &amp; = \\beta_2S_{db} + (1-\\beta_2)||db||^2 \\\\ &amp; V_{dw}^{corrected} = \\frac{V_{dw}}{1-\\beta_1^t} \\\\ &amp; V_{db}^{corrected} = \\frac{V_{db}}{1-\\beta_1^t} \\\\ &amp; S_{dw}^{corrected} = \\frac{S_{dw}}{1-\\beta_2^t} \\\\ &amp; S_{db}^{corrected} = \\frac{S_{db}}{1-\\beta_2^t} \\\\ w &amp; = w - \\alpha\\frac{V_{db}^{corrected}}{\\sqrt{S_{dw}^{corrected}}+\\varepsilon} \\\\ b &amp; = b - \\alpha\\frac{V_{db}^{corrected}}{\\sqrt{S_{db}^{corrected}}+\\varepsilon} \\\\ \\end{align*} \\] 刚刚讲过的RMS特点其实是动态的调整了我们的学习率，之前讲Momentum其实还保持了上一时刻的方向，RMS就没有解决这个问题，RMS把上一时刻的方向给弄没了。 RMS，它的定义其实就没有考虑上次的方向，它只考虑上次变化的大小。而现在提出来这个ADAM，这个ADAM的意思就是Adaptive Momentum, 还记不记得咱们讲随机森林和Adaboost那一节，我们讲过Adaboost就是Adaptive Boosting，这里的Adaptive其实就是一个意思，就是自适应动量，也叫动态变化动量。 ADAM就结合了RMS和动量的两个优点。第一个是他在分母上也加了一个根号下的数，也就做了RMS做的事，然后在分子上还有一个数，这个数就保留了上一时刻的数，比如\\(V_{dw}^{corrected}\\)， 就保留了上一时刻的V，就保留了上一时刻的方向。 所以ADAM既是动态的调整了学习率，又保留了上一时刻的方向。 那除此之外，其实还有一个AdaGrad和L-BFGS方法，不过常用的方法也就是上面详细讲的这几种。 到此为止，我们进阶神经网络的基础知识就都差不多具备了，接下来我们就该来讲解下卷机和序列，比如说LSTM和RNN、CNN的东西。在这些结束之后，我们还会有Attention机制，Transformer机制，YOLO机制，Segmentation机制，还有强化深度学习其实都是基于这些东西。 那我们下节课，就先从RNN来说开去。","link":"/26.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"},{"title":"27. 深度学习进阶 - 为什么RNN","text":"[TOC] Hi，你好。我是茶桁。 这节课开始，我们将会讲一个比较重要的一种神经网络，它对应了咱们整个生活中很多类型的一种问题结构，它就是咱们的RNN网络。 咱们首先回忆一下，上节课咱们学到了一些深度学习的一些进阶基础。 学了很多神经网络的Principles, 就是它的一些很重要的概念，比方层数维度。再然后咱们讲了Optimizer， 一些优化方式。还有weights的initialization，初始化等等。 那么大家具备了这些知识之后，那我们基本上已经能够解决常见的大概90%的机器学习问题了。 我们现实生活中绝大多数的机器学习问题，或者说识别问题都可以把它抽象成要么是分类，要么是回归问题。 一个柯基的例子 我们来一个例子，比方说一张图片里这个是什么动物，这显然是一个分类问题。 但是我们对这个图片的多个物体是什么，还有位置标注出来，那这个在里面前面会有一段是一个分类问题，后面还有一个长的向量，又会是一个回归问题。 我们只要知道分类和回归最大的区别就是一个返回的是一个类别，另外一个返回的是一个真正的数值。 那么接下来我们要正是的讲一下两种神经网络，RNN和CNN。这两个的目的是用来加速解决我们之前遇到的分类问题，或者回归问题。 在这些LSTM和CNN之类的高级的方法出现之前，其实我们用最直接的神经网络是可以解决所有的问题。 我们还是来看上面的那个例子，还是那张图片，如果要去分类看这图片里的是什么动物，我们把它形式化的表述一下。 假设我们这张图片现在是258*258的，那每一张图片进来之后，这个图片的饿背后其实都是一个向量： 1234567891011# 258 * 258from PIL import Imageimport numpy as npexample_img = Image.open('assets/Corgi.png')example_img = np.array(example_img)print(example_img.shape)---(429, 696, 3) 我们可以看到这张图片在计算机里保存的时候是(429, 696, 3)这样的一组数字。 1plt.imshow(example_img) 我们用plt展示出来，就是这样。 我们现在就可以讲整个图片变成一个向量，然后把它从立方体变的拉平： 12345example_img = example_img.reshape(1, -1)print(example_img)---[[120 150 88 ... 43 39 38]] 那现在，我们要给这个图片做分类： 12345678910class Model(nn.Module): def __init__(self, input_dimension, categorical): super(Model, self).__init__() self.linear = nn.Linear(in_features=input_dimension, out_features=categorical) self.softmax = nn.Softmax() def forward(self, x): predict = self.softmax(self.linear(x)) return predict ... 这里我们暂停一下，来说说这段代码中的super(...)，为了避免有些小伙伴Python基础不太好，这里说明一下。 如果有从我Python基础课就看过来的小伙伴，应该知道我在面向对象的时候应该是讲过这个方法。这个是为了在继承父类的时候，我们在重写父类方法的时候，依然可以调用父类方法。方式就是super().父类方法名()。有需要补Python基础的可以回头将我写的Python基础课程好好再看一遍。 好，我们继续回过头来讲，我们定义好这个Model之后，将图片数据变成一个PyTorch能够处理的一个example，当作训练数据传入train_x。 123456train_x = torch.from_numpy(example_img)print('shape:{}, \\ntrain_x:{}'.format(train_x.shape, train_x))---shape:torch.Size([1, 895752]), train_x:tensor([[120, 150, 88, ..., 43, 39, 38]], dtype=torch.uint8) 然后进入线性函数，传入in_features为train_x.shape[1]， 把它变成一个10分类，再把test_model运行一下，将我们的train_x输入进去就可以了。 12test_model = Model(input_dimension=train_x.shape[1], categorical=10)output = test_model(train_x.float()) 这样的话, 我们就可以产生出一个Softmax，有了这个Softmax，在这我们如果有很多个x，它就会对应我们很多个已知的y。 然后我们在这里定义一个loss： 1criterion = torch.nn.CrossEntropyLoss() 再之后我们在做线性的时候之前，肯定是有一些ytrue数据的，肯定是知道它的y的，写个循环它就不断的可以去训练。 接着我们可以得到这个它的权重，那么在这里这是一张图片，如果这个图片要做回归，要给这个图片打分，那么将out_features换成1就可以了。 我们在Model里不断的去改它的东西，让它的输出能够满足就可以了。 不管是用户数据还是气象数据、天文数据、图片、文字，我们都可以把它变成这样的一个x向量。变成x向量之后只要送到一个模型里面，这个模型它能够去做优化，做些调整。那么它就能够去不断的去做优化。 当然，我们这里还缺一个optimizer： 1optimizer = torch.optim.SGD(test_model.parameters(), lr=1e-3) 我们定义了一个SGD优化器，learning_rate设置了一下，给了一个初始的学习率。 然后呢再不断的去循环它就可以了： 123456789101112# 定义虚拟的ylable = np.random.randint(0,2,10)train_y = torch.from_numpy(np.array([lable])).float()for t in range(100): y_true = train_y y_predict = test_model(train_x.float()) print(y_true.shape) print(y_predict.shape) print(loss) 我们现在可以将criterion假如到循环里来计算一下loss了。 123for t in range(100): ... loss = criterion(y_predict, y_true) 就是说，我们之前学习的这些内容，不管是图片还是用户的数据、或者文字，其实都是可以变成一个向量，再把向量送入到定义好的模型里，求出它的结果。 再经过反复的运作，反复的调试来更新它的数据。 为什么RNN or CNN 那为什么我们还要学习RNN和CNN这些东西呢？我们刚开始学的wx+b的形式，可以把任意的x变成其它的一个output， 但是它在解决一些问题的时候效果就不是太好。 比方说啊，我们要识别一个图像到底是什么的时候，wx+b它是给每一个x一个权重, \\(w x_i + b\\), 然后最后产出一个值。 但是图像我们是希望给中间一个区域一个平分，可是现在是一个点一个点的。 例如我们输入是一个x，输出是一个y。x它包含了多个x:{x1, x2, x3, ..., xn}，那y的输出呢，它是和多个x有关系。如果是在一个曲线上，我们取几个点, {output1, output2, output3}, 那么这个output3就不止和\\(\\vec x_3\\)有关系，它和前面的output2, output1都有关系。 也就是说，当下这一时刻的数据其实不仅取不仅取决于今天发生的一些事情，还取决于昨天前天，甚至大前天发生的事情。 但是我们如果直接进行wxi+b的话，这里xi=x3，wx3+b我们期望输出一个output3，这样就忽略了前边的这些事情。 与此类似的还有我们写文章，当前这个字和前面是什么字应该是有依赖关系的。其实把它抽象一下的话，会发现在现实生活中其实有很多种依赖关系。 我们之前讲的wx+b，其实是一对一。 虽然x的维度可能会很大，y输出的维度也可能很大，但是它一个x就只对应输出一个y。 而除了one to one 之外，我们还有一些其他的类别： one to many，就是x输入之后，最后会输出多个y。比方说咱们输入的是一个类别，输出的是一篇文章，分别是第一个单词，第二个单词和第三个单词。 我们会发现，这三个输出的单词前后是有相关性的。这种就属于是一对多，输出的的这些内容是独立的个体，但是它们之间有相关性。 后面的many to one，典型的一个应用，你给他输入一句话，输出这个地方，这句话到底是表示正向的还是负向的。那么这句话其实每个单词之间是有依赖关系的，而输出的是一个值。 那many to many里，前边输入的这个input是一个序列，有依赖关系。输出也是一个序列，有依赖关系。那么这会是一个什么？比方我们的机器翻译，就有可能是这样一个关系，对吧？还有比方说我们会去做那个文本的阅读理解，文本的摘要。 那还有一个many to many和第一个有什么区别呢？它其实只是更加的实时，比如说同声传译。 对于这些所有的问题我们给它抽象一下，它每一步的输出就像我们之前学过递归函数一样，是和前一步的输出有关系，还和当前这一步的输入有关系, 我们其实学过最典型的一个依赖关系就是这样，就是斐波那契数列或者求阶乘： 12345678910def fib(n): if n == 0 or n == 1: return 1 else: return fib(n-1) + fib(n-2) def fac(n): if n == 0: return 1 else: return n*fac(n-1)for i in range(10): print('{}\\t{}'.format(fib(i), fac(i))) 那么这个怎么实现的？我们要实现这个有多种方法，我们可以来看一个具体的案例： 123456789101112131415class RNN(nn.Module): # implement RNN from scratch rather than ysubf nn.RNN def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(input_size + hidden_size, hidden_size) self.i2o = nn.Linear(input_size + hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input_tensor, hidden_tensor): combined = torch.cat((input_tensor, hidden_tensor), 1) hidden = self.i2h(combined) output = self.i2o(combined) 这是一个非常经典的RNN的模型，我们来一起来分析它的构成。 在构造函数内，输入了一个input_size（x向量），还有一个hidden_size。然后在下面做了一个i2h的线性变化，这个线性变化它接受一个的两个参数， in_features是input_size + hidden_size, out_features是hidden_size。 现在有一个\\(\\vec x\\)和一个\\(\\vec h\\)， 将两个向量相加输入进入，然后会输出一个\\(vec h\\)一样大小的东西。 然后下面还有一个i2o， 它是将input_size + hidden_size输入之后，输出一个output_size一样大小的东西。 在输出这两个之后，我们将output_size大小的这个向量，输入到Softmax里面，就会变成一个概率分布。 然后它继续forward的时候，继续向前运算的时候，它的输入是input和hidden，那它在这里，如果我们要求训练： 12345def train(line_tensor, category_tensor): hidden = rnn.init_hidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) 这里它有很多的tensor，比如我们的x:[x1, x2, ..., xn], 这个tensor就是这些个x。那么它在做训练的第一步会取最前面的这个x向量，这个x向量刚开始会有一个随机的hidden向量，这个时候关键的地方就来了，就是它不断的重复:output, hidden = rnn(line_tensor[i], hidden), 我们来看，这个hidden就会一次一次的送进去做更新。 hidden一开始是随机的，之后t时刻的hidden的值是由上一时刻，也就是t-1时刻的x和hidden来影响的。 1234h0 -&gt; random(x0, h0) -&gt; output1, h1(x1, h1) -&gt; output2, h2... 这样，输出的output2不仅是x1的影响，也是受到x0的影响的，这样前后的关系就被连接起来了。 就比如说我们输入的是一段文字，就比说ChaHeng，输入C的时候，我们会得到一个hidden， 然后计算h时候，我们又会得到一个hidden, 一直到最后一个g，那我们算这一步的时候，它既包含了g这个字母， 还包含了之前n的hidden向量。那n再往上，一直到C都相关，这样它就实现了传递的效果。 那这个做法有两个人分别提出来了两种。 之前，我们将神经网络建模为: \\[ \\begin{align*} y_t = \\sigma(Wx_t + b) \\\\ y_{t+1} = \\sigma(Wx_{t+1} + b) \\end{align*} \\] 现在我们将其更新为两两种方法，一个是Elman network: \\[ \\begin{align*} h_t &amp; = \\sigma_h(W_hx_t + U_hh_{t-1}+b_h) \\\\ y_t &amp; = \\sigma_y(W_yh_t + b_y) \\end{align*} \\] 还有一个是Jordan networks: \\[ \\begin{align*} h_t &amp; = \\sigma_h(W_hx_t + U_hy_{t-1}+b_h) \\\\ y_t &amp; = \\sigma_y(W_yh_t + b_y) \\end{align*} \\] 我们看一下区别，其实就是为了加上非线性变化。给h加了一个非线性变化，再给y加了一个非线性变化。 这两个人都是很著名的计算机科学家，他们提出来的模型有区别，一个是一直在传递这个h，一个是一直在传递y。但是都实现了yt时刻和xt有关，也和x_{t-1}有关。这两个都实现了这样的一种功能，只不过它们中间一直传递的东西不太一样。 这个就是RNN的内核，它的内核就是这个东西。 我们接着，就来看一个案例，这个案例中的数据是一个盈利数据, 还是老样子，数据集我就放在文末了。 我们这里是一个两个月每天的盈利指数，其中2点几的是盈利比较多，1点几的就是盈利比较少的。 12345timeserise_revenue = pd.read_csv('~/mount/Sync/data/AI_Cheats/time_serise_revenue.csv')sales_data = pd.read_csv('~/mount/Sync/data/AI_Cheats/time_serise_sale.csv')timeserise_revenue.drop(axis=1, columns='Unnamed: 0', inplace=True)sales_data.drop(axis=1, columns='Unnamed: 0', inplace=True) 数据上我就不展示了，大家自己拿到后查看一下。我们现在要做的是，是想根据它前十天的一个数据，来预测一下第11天的数据。 很简单的方法咱们可以写一个全连接的网络： 1234567891011121314151617class FullyConnected(nn.Module): def __init__(self, x_size, hidden_size, output_size): super(FullyConnected, self).__init__() self.hidden_size = hidden_size self.linear_with_tanh = nn.Sequential( nn.Linear(10, self.hidden_size), nn.Tanh(), nn.Linear(self.hidden_size, self.hidden_size), nn.Tanh(), nn.Linear(self.hidden_size, output_size) ) def forward(self, x): yhat = self.linear_with_tanh(x) return yhat 我们输入10个值对它进行线性变化，再给它进行一个非线性变化，然后重复一遍，最后再来一次线性变化，这样就是最简单的一种线性和非线性变化的网络。 然后我们处理一下数据，设置一下相关参数： 123456789101112131415161718192021sales_data.drop(axis=1, columns='Unnamed: 0', inplace=True)source_data = sales_datan_epochs = 30hidden_size = 2 # try to change this parameters n_layers = 1batch_size = 5seq_length = 10n_sample_size = 50x_size = 1fc_model = FullyConnected(x_size, hidden_size, output_size=seq_length)fc_model = fc_model.double()criterion = nn.MSELoss()optimizer = optim.SGD(fc_model.parameters(), lr=0.01)fc_losses = np.zeros(n_epochs) plt.imshow(fc_model.state_dict()['linear_with_tanh.0.weight']) 显示了一下一开始的权重。 之后我们来看一下整个的训练过程： 123456789101112131415161718data_loader = torch.utils.data.DataLoader(source_data.values, batch_size=seq_length, shuffle=True)for epoch in range(n_epochs): epoch_losses = [] for iter_, t in enumerate(data_loader): random_index = random.randint(0, t.shape[-1] - seq_length - 1) train_x = t[:, random_index: random_index+seq_length] train_y = t[:, random_index + 1: random_index + seq_length + 1] outputs = fc_model(train_x.double()) optimizer.zero_grad() loss = criterion(outputs, train_y) loss.backward() optimizer.step() epoch_losses.append(loss.detach()) fc_losses[epoch] = np.mean(epoch_losses) 传入的data_loader是每一次随机的取期望的10个数字,这个数字我们就会根据序列来取出x和y, 然后把x送到模型里边得到outputs,得到outputs之后又出现熟悉的面孔, 我们求它的loss，再通过它的loss做反向传播。 optimizer做step，就是做全程的更新。 之后我们可以将每次循环的结果打印出来看看： 12345678910111213for epoch in range(n_epochs): ... for iter_, t in enumerate(data_loader): ... if iter_ == 0: plt.clf() plt.ion() plt.title(&quot;Epoch {}, iter {}&quot;.format(epoch, iter_)) plt.plot(torch.flatten(outputs.detach()),'r-',linewidth=1,label='Output') plt.plot(torch.flatten(train_y),'c-',linewidth=1,label='Label') plt.plot(torch.flatten(train_x),'g-',linewidth=1,label='Input') plt.draw() plt.pause(0.05) 我们就不全展示了，大家可以自行去运行一下。 红色是预测值，绿色是输入值，蓝色是实际值。这里我只放了第一张和第30张，也就是本次循环的最后一张。 那一开始，预测出来值没有和我们实际的值相符，到了30的相较而言是比较相符了。 我们看看它的loss是否如预期的下降了： 1plt.plot(fc_losses) 看完全连接的模型，再来看看RNN的模型，做一个非常简单的RNN模型，那首先还是定义模型： 12345678910111213141516171819class SimpleRNN(nn.Module): def __init__(self, x_size, hidden_size, n_layers, batch_size, output_size): super(SimpleRNN, self).__init__() self.hidden_size = hidden_size self.n_layers = n_layers self.batch_size = batch_size self.rnn = nn.RNN(x_size, hidden_size, n_layers, batch_first=True) self.out = nn.Linear(hidden_size, output_size) # 10 in and 10 out def forward(self, inputs, hidden=None): hidden = self.__init__hidden() output, hidden = self.rnn(inputs.float(), hidden.float()) output = self.out(output.float()); return output, hidden def __init__hidden(self): hidden = torch.zeros(self.n_layers, self.batch_size, self.hidden_size, dtype=torch.float64) return hidden 我们输入的是x_size，然后然后定义一个hidden_size。这里注意啊，hidden_size是可以改的，越大可以表示的中间层的信息就越多，但意味着需要更多的数据去训练它。 然后在forward里，可以看到每一步会输出一个output，到最后一步的时候我们把output做一个线性变化，就可以变成期望的这个结果。 那这个RNN模型其实非常的简单，就是进了一个RNN，然后做了一个线性变化，把output做成线性变化。 然后我们来看看具体表现如何， 那首先一样的是定义参数，数据可以用上一次整理过的数据，不需要再做一次了： 123456789101112131415161718n_epochs = 30hidden_size = 2 # try to change this parameters n_layers = 1batch_size = 5seq_length = 10n_sample_size = 50x_size = 1output_size = 1hidden = Nonernn_model = SimpleRNN(x_size, hidden_size, n_layers, seq_length, output_size)criterion = nn.MSELoss()optimizer = optim.SGD(rnn_model.parameters(), lr=0.01)rnn_losses = np.zeros(n_epochs) 然后我们就可以来跑一下了。 12345678910111213141516171819data_loader = torch.utils.data.DataLoader(source_data.values, batch_size=seq_length, shuffle=True)for epoch in range(n_epochs): for iter_, t in enumerate(data_loader): if t.shape[0] != seq_length: continue random_index = random.randint(0, t.shape[-1] - seq_length - 1) train_x = t[:, random_index: random_index+seq_length] train_y = t[:, random_index + 1: random_index + seq_length + 1] outputs, hidden = rnn_model(train_x.double().unsqueeze(2), hidden) optimizer.zero_grad() loss = criterion(outputs.double(), train_y.double().unsqueeze(2)) loss.backward() optimizer.step() epoch_losses.append(loss.detach()) rnn_losses[epoch] = np.mean(epoch_losses) 那RNN模型其实从第三轮的时候效果就已经出现了，我们的x一样，改变了一个模型之后拟合的效果就不一样了。 我们来看看它的loss： RNN模型跑下来，loss是下降到了0.67左右，那我们之前的全连接模型的loss是在0.8以上，还是有一些区别的。我们可以将两个模型的loss打印到一张图上，就更能看出来两个模型的区别了。 12plt.plot(rnn_losses, c='red')plt.plot(fc_losses, c='green') 就可以看到，非常明显。 举这个例子作用是想说明，wx+b加上非线性变化这种形式其实也能解决问题，但是遇到时间相关，序列相关的问题的时候，解决效果就没有RN模型这么好。 为什么没有RNN模型好呢？因为RNN模型在这个过程中每一步把前一步的hidden的影响给它保留了下来。就是说它每一步的输出的时候不是单纯的考虑这一步的输出，把之前每一步的x的值其实都保留下来了。这个区别就是为什么要有RNN，以及大家之后什么时候用RNN。 因为我这边只是做个测试，所以仅仅做了30次epoch，那之后，大家可以尝试一下将epoch改成200或者更多，来看看具体loss会下降到什么程度。 好，文章最后，就是本文所用的数据集了： time_serise_revenue.csv 链接: https://pan.baidu.com/s/1dL9XdBgoi3nC2VOC6w_wnw?pwd=qmw6 提取码: qmw6 --来自百度网盘超级会员v6的分享 time_serise_sale.csv 链接: https://pan.baidu.com/s/12wMJHzSZk91YPFcaG-K6Eg?pwd=1kmp 提取码: 1kmp --来自百度网盘超级会员v6的分享","link":"/27.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E4%B8%BA%E4%BB%80%E4%B9%88RNN/"},{"title":"28. 深度学习进阶 - LSTM","text":"Hi, 你好。我是茶桁。 我们上一节课，用了一个示例来展示了一下我们为什么要用RNN神经网络，它和全连接的神经网络具体有什么区别。 这节课，我们就着上一节课的内容继续往后讲，没看过上节课的，建议回头去好好看看，特别是对代码的进程顺序好好的弄清楚。 全连接的模型得很仔细的去改变它的结构，然后再给它加很多东西，效果才能变好： 1234567self.linear_with_tanh = nn.Sequential( nn.Linear(10, self.hidden_size), nn.Tanh(), nn.Linear(self.hidden_size, self.hidden_size), nn.Tanh(), nn.Linear(self.hidden_size, output_size)) 但是对于RNN模型来说，我们只用了两个函数： 12self.rnn = nn.RNN(x_size, hidden_size, n_layers, batch_first=True)self.out = nn.Linear(hidden_size, output_size) 这是一个很本质的问题, 也比较重要。为什么RNN的模型这么简单，它的效果比更复杂的全连接要好呢？ 这个和我们平时生活中做各种事情其实都很类似，他背后的原因是他的信息保留的更多。RNN模型厉害的本质是在运行的过程中把更多的信息记录下来，而全连接没有记录。 对于RNN模型，还有两个点大家需要注意。 第一个，有一种叫做stacked的RNN的模型。我们RNN模型每一次输出都有一个output和hidden，把outputs和hidden作为它的输入再传给另外一个RNN模型，模型就变得更复杂，理论上可以解决些更复杂的场景。我们把这种就叫做stacked RNN。 还有一种形式，Bidirectional RNN，双向RNN。有一个很著名的文本模型Bert, 那个B就是双向的意思。 我们回过头来看上节课我们讲过的两种网络： \\[ \\begin{align*} h_t &amp; = \\sigma_h(W_hx_t + U_hh_{t-1} + b_h) \\\\ y_t &amp; = \\sigma_y(W_yh_t + b_y) \\end{align*} \\] 在这个里面，每一时刻的y_t只和y_{t-1} 有关系，如果把所有的x一次性给到模型的时候，其实我们在这里可以给它加一个东西： \\[ \\begin{align*} h_t &amp; = \\sigma_h(W_hx_t + U_hh_{t-1} + V_h * h_{t+1} + b_h) \\end{align*} \\] 还可以写成这样，那这样的话它实现的就是每一时刻的t既和前一次有关系 和后一刻有关系。这样我们每一次的值不仅和前面有关，还和后面有关。就叫做双向RNN。 对于RNN来说，它有一个很严重的问题，就是之前说过的，它的vanishing和exploding的问题会很明显, 也就是梯度消失和爆炸问题。 想一下，现在如果有一个loss，那它最终的loss是不是对于{x1, x2, ..., xn}都有关系，比方说现在要求\\(\\frac{\\partial loss}{\\partial w_1}\\), 假如说现在h是100， 那这种调用关系就是 \\[ \\begin{align*} \\frac{\\partial loss}{\\partial w_1} = \\frac{\\partial h_{100}}{\\partial h_{99}} \\cdot \\frac{\\partial h_{99}}{\\partial h_{98}} \\cdot ... \\cdot \\frac{\\partial h_{0}}{\\partial w_{1}} \\end{align*} \\] loss对于w1求偏导的时候，其实loss最先接受的是离他最近的, 假如说是h100。h100调用了h99,h99调用h98，就这个调用过程，这一串东西会变得很长。 我们之前课程说过一些情况，怎么去解决这个问题呢？对于RNN模型来说梯度爆炸很好解决，就直接设定一个阈值就可以了，起码也是能学习的。 要讲的是想一种方法怎么样来解决梯度消失的问题。这个梯度消失的解决方法，就叫LSTM。要解决梯度消失，就是要用LSTM: Long Short-Term Memory，长短记忆模型，既能保持长信息，又能保持短信息。 在之前那个很长的过程中，怎么样能够让它不消散呢？LSTM的核心思想是通过门控机制来控制信息的流动和及已的更新，包含了Input Gate, Forget Gate，Cell State以及Output Gate。这些会一起协作来处理序列数据。 其中Input Gate控制着新信息的输入，以及信息对细胞状态的影响。 Forget Gate控制着细胞状态中哪些信息应该被易王，Cell State用于传递信息，是LSTM的核心，Output Gate控制着细胞状态如何影响输出。 这里每一个门控单元都由一个Sigmoid激活函数来控制信息的流动，以及一个Tanh激活函数来确定信息的值。 \\[ \\begin{align*} Input Gate \\\\ i_t &amp; = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\\\ C't &amp; = \\tanh(W_c \\cdot [h{t-1}, x_t] + b_c) \\\\ C_t &amp; = f_t \\cdot C_{t-1} + i_t \\cdot C'_t \\\\ Forget Gate \\\\ f_t &amp; = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\\\ C_t &amp; = f_t \\cdot C_{t-1} + i_t \\cdot C'_t \\\\ Output Gate \\\\ o_t &amp; = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\\\ h_t &amp; = o_t \\cdot \\tanh(C_t) \\end{align*} \\] 其中，\\(h_{t-1}\\) 是前一个时间步的隐藏状态，\\(x_t\\) 是当前时间步的输入，\\(W_i, W_f, W_o, W_c\\) 是权重矩阵，\\(b_i, b_f, b_o, b_c\\) 是偏置。 LSTM输入的是一个序列数据，可以是文本、时间序列，音频信号等等。那每个时间步的输入是序列中的饿一个元素，比如一个单词、一个时间点的观测值等等。 假设我们有一个序列 x = [x1, x2, ..., xt]， 其中t就代表的是时间步。 xt进来的时候, 之前我们是只接收一个hidden state, 现在我们多接收了一个\\(C_{t-1}\\)，这个就是我们的Cell，这一步的\\(C_{t-1}\\)其实就是上一步的\\(C_t\\)。 在训练开始时，需要初始化LSTM单元的隐藏状态h0和细胞状态c0。通常我们初始化它们为全零向量。 最开始的时候，我们要进入Input Gate, 对于每个时间步t, 计算输入门的激活值\\(i_t\\)，控制新信息的输入。使用Sigmoid函数来计算输入门的值： \\[ i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i) \\] 然后，计算新的侯选值\\(C'_t\\)， 这是在当前时间步考虑的新信息。使用tanh激活函数来计算侯选值： \\[ C'_t = tanh(W_c \\cdot [h_{t-1}, x_t] + b_c) \\] 接下来我们就要更新细胞状态了，细胞状态\\(C_t\\)更新是通过遗忘门\\(f_t\\)和输入门\\(i_t\\)控制的。遗忘门控制着哪些信息应该被遗忘，输入门控制新信息对细胞状态的影响： \\[ C_t = f_t \\cdot C_{t-1} + i_t \\cdot C'_t \\] 那遗忘门决定哪些信息应该被遗忘，使用的就是Sigmoid函数计算遗忘门的激活值。 \\[ f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\] 接着，计算输出门\\(O_t\\), 控制着细胞状态如何影响输出和隐藏状态。一样，我们还是使用Sigmoid函数计算。 \\[ o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\] 使用输出门的值\\(o_t\\)来计算最终的隐藏状态\\(h_t\\)和输出。 隐藏状态和输出都是根据细胞状态和输出门的值来计算的： \\[ h_t = o_t \\cdot tanh(C_t) \\] 接下来就容易了，我们迭代重复上述过程，处理序列中的每一个时间步，直到处理完整个序列。 LSTM的输出可以是隐藏状态\\(h_t\\), 也可以是细胞状态\\(C_t\\)， 具体是取决于应用的需求。 后来大家就发现了一种改进的LSTM，其中门控机制允许细胞状态窥视现前的细胞状态的信息，而不仅仅是根据当前时间步的输入和隐藏状态来决定。 这个机制在LSTM单源种引入了额外的权重和连接，以允许细胞状态在门控过程中访问现前的细胞状态，我们称之为窥视孔连接: Peephole connections。 \\[ \\begin{align*} f_t = \\sigma(W_f \\cdot [C_{t-1}, h_{t-1}, x_t] + b_f) \\\\ i_t = \\sigma(W_i \\cdot [C_{t-1}, h_{t-1}, x_t] + b_i) \\\\ o_t = \\sigma(W_o \\cdot [C_{t-1}, h_{t-1}, x_t] + b_o) \\\\ \\end{align*} \\] 之前，我们是xt和x_{t-1}决定的f，那现在又把c_{t-1}加上了。就是多加了一些信息。 除此之外它有一个方法GRU，这个是2014年提出来的，Geted Recurrent Unit，它是LSTM的一个简化版本。 它最核心的内容： \\[ \\begin{align*} h_t = (1-z_t) \\cdot h_{t-1} + z_t \\cdot h'_t \\end{align*} \\] 咱们刚刚是\\(C_t = f_t \\cdot C_{t-1} + i_t \\cdot C'_t\\)，也就是遗忘加上输入，那我们对过去保留越多的时候， 输入就会越小，那对过去保留越小的时候，输入就会越大。 所以既然f也是1-0，i也是0-1，f大的时候i就小，f小的时候i就大，那么能不能写成f=(1-i)？ 于是，GRU就这样实现了, 它其实最核心的就做了这样一件事， f=(1-i)。 \\[ \\begin{align*} z_t &amp; = \\sigma(W_z \\cdot [h_{t-1}, x_t]) \\\\ r_t &amp; = \\sigma(W_r \\cdot [h_{t-1}, x_t]) \\\\ h'_t &amp; = \\tanh(W \\cdot [r_t \\cdot h_{t-1}, x_t]) \\\\ h_t &amp; = (1-z_t) \\cdot h_{t-1} + z_t \\cdot h'_t \\end{align*} \\] 这个z其实和i是一样的东西，只是原作者为了发表论文方便而改了个名称。 https://arxiv.org/pdf/1406.1078v3.pdf \\(r_t\\)是来控制上一时刻的\\(h_t\\)在我们此时此刻的重要性、影响程度。那我们可以将\\(r_t \\cdot h_{t-1}\\)看成是关于及已的，\\(1-z_t\\)也是关于记忆的。 GRU这样做之后有什么好处呢? 原来我们有三个门: f, i, o， 那现在变成了两个，z和r。为什么就更好了呢？我们在PyTorch里面往往用的是GRU。 大家想一下，是不是少了一个门其实就少了一个矩阵？我们看公式的时候，\\(W_f\\)是一个数学符号，但是在背后其实是一个矩阵，是一个矩阵的话少了一个矩阵意味着参数就少多了，运算就更快了等等。 但其实这些都不是最关键的，最关键的是减少过拟合了。我们之前的课程中一再强调，过拟合之所以产生，最主要的原因是数据不够或者说是模型太复杂。 但是在现有的数据情况下，为了让数据发挥出最大效力，你把需要训练的模型变简单，参数变少，就没有那么复杂了。 关于RNN模型，我们后面还会介绍一些具体的示例。","link":"/28.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20LSTM/"},{"title":"29. 深度学习进阶 - 卷积的原理","text":"Hi,你好。我是茶桁。 在结束了RNN的学习之后，咱们今天开始来介绍一下CNN。 CNN是现代的机器深度学习一个很核心的内容，就假如说咱们做图像分类、图像分割，图像的切分等等。 其实这些过程就是你让计算机能够自动识别，不仅能够识别图像里有什么，还能识别图像里这些东西分别是在什么地方。这种复杂操作其实都是基于啊CNN的变体。要给计算机有识别图像的能力。 再比方说大无人驾驶汽车，它要识别行人在哪里。 再比如安防的摄像头，要能够检测出来我们人在哪里。 这些事情背后都是计算机视觉的问题。 大概一九五几年、六几年的时候，哈佛大学曾经做过一个研究，给猫的大脑上装了一些电极，让这个猫去看前面的一个幻灯片，然后通过切换幻灯片的内容，然后观察猫的大脑哪些地方活跃。 就发现两个特点，第一个它有一种一层一层的特性，比方说我换了颜色，它固定的就这几层会活跃，离眼睛远的地方会活跃。如果换了线条，颜色没变，会是另外的一层区域会活跃，不同层其实对于不同的特定变化是不一样的。 第二个发现，越靠近眼睛的地方，越低级的层次的变化会越明显，比如线条颜色。眼睛越远的距离，线条和颜色没变，但是眼睛变大了或者变小了，那么这些地方它会更明显。 也就是说，第一个它是有分层的，第二个，它不同的这个层的抽象性是不一样的，对于什么东西的感受力是不一样的。 沿着这个思路，人们当时就提出来了一些方法。当时人们做计算机视觉，主流不是机器学习。但是人们提出来一个一个这样的filter： 12345filter = np.array([ [1, 0, -1], [1, 0, -1], [1, 0, -1]]) 这样的filter是人刻意的，主观的提出来的。他们把这个filter去应用到一个一个的图像上。 比方说我们的图像是a b c d e f g h i j k l，然后按4*4的矩阵相乘，再加起来，比如\\(aw+bx+ey+fz\\)，这样就得到了一个新的内容。大家把这个操作就叫做卷积操作。 看个示例： 1234567891011import numpy as npimage = np.array([ [10, 10, 10, -3, -3, -3], [10, 10, 10, -3, -3, -3], [10, 10, 10, -3, -3, -3], [10, 10, 10, -3, -3, -3], [10, 10, 10, -3, -3, -3], [10, 10, 10, -3, -3, -3],])plt.imshow(image)plt.show() 我们可以看到，这个矩阵的前三列全是10，后两列都是0，最后生成的图像有一个明显的分界，伴随着两个不同的颜色。 我们现在给这个图像矩阵加上一个filter, 然后按上面的方法进行操作： 那左上角的3*3的小矩阵的运算结果就是0。 那同理，我们以此往后算，第二个结果是39, 第三个结果是39.... 大家后面可以自行计算一下，最后的计算结果就是： 123[[0, 39, 39, 0],[0, 39, 39, 0],[0, 39, 39, 0]] 我们可以看出来，当分割的小矩阵内数据相同的时候，值为0，如果说矩阵内的这个部分图像差距不是很大，那它也是近乎接近于0，意味着差别很小。如果说分割的这个小矩阵左右两边是相反数的时候，两边的差别是最大的，不管最后相加的值是正的还是负的，绝对值下应该是最大的。这个地方其实是图像竖着的边缘。 那如果我们将filter改一下，改成下面这样： 123[[1, 1, 1],[0, 0, 0],[-1, -1, -1]] 如果是这样，计算的结果就是图像横向的边缘的绝对值最大。 基于这种原理，我们就可以找到图像所有竖向和横向的边沿，给它拿出来。 这整个的一个过程，就叫做卷积: convolution。convolution就是两个东西之间互相起作用。最早是出现在信号处理上，两个信号把它做一个合并。 卷积的操作是为了干什么呢？卷积的操作是用来提取图片的某种特征，抓取图片特征。在上个世纪后期，计算机视觉的老科学家们提出了大量的kernel，当时叫做算子，现在叫做卷积核。 卷积的操作就是给定一个图片，然后给定一个卷积核，和卷积核一样大小的窗口里边的每个值相乘，相乘之后再做相加。 假如咱们有一张图片，一般来说，咱们现实生活中图片往往是三维，通常是红绿蓝(RGB)，然后我们让这张图片和这个filter去做相乘的操作。 这三个层里面每一层都会和filter做一个相乘的操作，咱们就假设这三个层分别为: 123[[a11, a12, a13],[a21, a22, a23],[a31, a32, a33]],[[b11, b12, b13],[b21, b22, b23],[b31, b32, b33]][[c11, c12, c13],[c21, c22, c23],[c31, c32, c33]] 然后再假设filter为： 1[[f11, f12, f13], [f21, f22, f23], [f31, f32, f33]] 那这个filter会分别和这三个层进行卷积操作，产生的卷积结果为v1, v2, v3, 然后这三个结果再进行相加，最后会产生一个新的层。 我们来看一下下面这张图： 这张图显示的是一层的情况，一个filter大小的矩阵被卷积成了一个点，然后这个操作不只是针对一层的，而是对整个一个纵向体积内的所有层都做这样一个操作： 途中最底下是我们的图片的RGB分层，再经过和filter相乘之后向上会卷积成一个点，那向上之后的Map1, Map2,... 原因是每一层都是一个不同的filter计算的结果，这里存在很多个filter， 然后分别计算产生了这样一个叠加层。 再做下一次运算的时候也是一样，这些Map的纵向上经过和filter运算依然会被卷积成一个点。 就着上面那个简单的图形，咱们来做个演示： 123456789101112131415161718192021222324252627282930def conv(image, filter): h = filter.shape[0] w = filter.shape[1] for i in range(image.shape[0]): for j in range(image.shape[1]): window = image[i: i+h, j: j+w] print(window)filter = np.array([ [1, 0, -1], [1, 0, -1], [1, 0, -1]])conv(image, filter)---[[10 10 10] [10 10 10] [10 10 10]]...[[-3] [-3] [-3]]...[[10 -3 -3]][[-3 -3 -3]][[-3 -3]][[-3]] 输入一个图片的数据, 拿到filter的高宽，然后让filter沿着图片从上到下，从左到右移动。 我们打印结果能看到，运行到中间的时候会出现一串的[[-3], [-3], [-3]]。因为i会一直运行边上，那么如果要做卷积的话，大小要和filter一直一样，所以咱们在这里需要给他减去一个filter。就是不要运行后边这几个。 1234... for i in range(image.shape[0] - h+1): for j in range(image.shape[1] - w+1): ... 这样就可以了。 我们每一次其实就是从左到右，从上到下裁剪出来一个一个的window。 我们让这个window和filter相乘后再相加，我们可以得到什么结果？ 123456789101112for i ...: for j ... ... result = np.sum(filter * window) print(result)---039...390 就是计算卷积的结果。 那我们可以将其改成矩阵的形式, 然后咱们打印出来看看是个啥： 1234567891011121314151617def conv(image, filter): r_height = image.shape[0] - h+1 r_width = image.shape[1] - w+1 result = np.zeros(shape=(r_height, r_width)) for i ...: for j ...: result[i][j] = np.sum(filter * window) return resultresult = conv(image, filter)plt.imshow(result)plt.show()---array([[ 0., 39., 39., 0.], [ 0., 39., 39., 0.], [ 0., 39., 39., 0.], [ 0., 39., 39., 0.]]) 那变成这样的原因是因为原来的图像中间有一个边缘，现在这张图显示的是图片边缘的部分被高亮。 这样一张图片可能并不太能理解，我拿我的头像来做这个示例好了： 123myself = Image.open('./assets/chaheng.jpg').convert('L')...plt.imshow(result, cmap='gray') 为了更明显一点，我将图像改成灰度显示。 我们可以看到卷积之后的效果，明显边缘都被显示出来了。但是我们也注意到了，竖向的边缘都很明显，但是横向的边缘并不清楚。我们再来对横向进行一下卷积, 我们先要增加一个处理多个filter的方法，将原来的conv方法改为single_conv， 表示处理单个： 123456def single_conv(...): ...def conv(image, filters): results = [single_conv(image, f) for f in filters] return results 然后我们的调用需要改一下传递的参数： 1results = conv(image, [h_filter, w_filter]) 既然要传递两个filter, 那我们就需要再定义一个横向的filter，然后一起传进去： 123456789# 原来的filterh_filter = np.array([...])# 新定义的横向filterw_filter = np.array([ [1, 1, 1], [0, 0, 0], [-1, -1, -1]]) 接着我们将原图，竖向的卷积结果和横向的卷积结果都打印出来： 12345678plt.subplot(1, 3, 1)plt.imshow(image)plt.subplot(1, 3, 2)plt.imshow(results[0])plt.subplot(1, 3, 3)plt.imshow(results[1])plt.show() 原图变成这个颜色的原因是我在PIL读取图像的时候，将其转为了灰度。我们可以看到第二张图片和第三张图明显在边缘上的区别，一个像是灯光从左边打过来的，一个像是灯光从上面打下来的。 中间和右边这个，其实都是把边缘提出来了。因为卷积核的不同，中间这个图把竖着的边缘明显提取的比较准确，右边的把横向的提取的比较准确。 这也是为什么我们之前看得那张图里会有那么多的Map: 它的每一层都是一个不同的filter提取出来的，有这么多filter的原因则是每一个filter提取出来的特征都是不一样的。 我们来看我们刚才定义的方法： 12def single(image, filter): ... 我们把输入卷积的时候的image这个参数叫做input channel。那在此时此刻，我们这个图像如果是RGB的，它就是三维的，那么input channel就等于3。 filters的个数，就叫做output channel。原因就在于，有多少个filter，那我们的results就有多厚。比如说我们有4个filter, 那输出的result就有四层。 然后可以接着对results继续应用filter做卷积，那在这一轮的input channel就等于一次的output channel, 也就是4。 这个，就是卷积的原理。 好，这节课就到这里了，下节课咱们继续学习卷积，来看看在神经网络里如何应用。","link":"/29.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%8E%9F%E7%90%86/"},{"title":"31. 深度学习进阶 - 全连接层及网络结构","text":"Hi，你好。我是茶桁。 之前的课程咱们学习了卷积以及池化，那到底卷积是如何构成卷积神经网络的呢？我们这节课来好好讲一下。 全连接层 整个卷积的运算就是经过卷积，再经过pooling，再经过卷积。会把这个图形变的很小。然后再经过pooling，又会一直把我们的特征变得越来越小，之后有一个很重要的层，这个层叫做全连接层。 后面的几个柱状图就是它的线性变化，就是它的全连接层。 先是将图片卷积、池化变小，变成很小的高级特征，然后拉平之后进入全连接层进行线性变化。 这就是卷积操作的整个工作流，也是为什么卷积操作需要的参数少的原因。 我们在这里重点说一下全连接层。 我们做了很多pooling， 很多卷积之后，我们会生成一个很厚的一个值。把很厚的这个值给他拉平，在PyTorch里面直接就flatten, 或者用reshape直接进行，把它拉平成一个1乘以n的一个向量。然后给这个1乘以n进行熟悉的wx+b。 我们对它进行线性变化，第一是对它的维度进行了变化。假如要给它变成一个10分类，纬度进行的变化。 另外一点，我们每一层都会有不同的特征点，这些特征点代表这图像不同的位置把它抽象成的值。然后一层一层的，又是不同的filter的结果，提取出来的不同的特征。比如横向，竖向之类的。机器可能还会自动提取一些颜色，形状等等。 那么现在我们要把这些东西进行一个综合考量，要把这些信息全部拿起来综合做个判断。比如我们有三个filter, 也就是有三层，这三层里面拿出四个位置。 那么拉平的画，就变成3乘以4，这里面有12个数值。这12个数值提取出来通过不同的方式了，关注点不同，提出来的12高级特征。 现在要把这12个高级特征全盘考虑、综合考虑。我们要给这些数据加一个不同的权重。就要给它做一个wi * xi，就给它这些全盘综合做了一个权重的这个赋值。 所以说，全连接不仅对维度进行了变化，它还对之前提取出来的局部信息进行了综合，这个就是全连接层的作用。既进行了变化又进行了维度信息的综合。 所以说，大家看一下 这些不同的著名的网络结构，都是进行完之后要进行线性变化，线性变化之后把它变到我们期望的target上，就是最前面的这些东西进行综合。 算出来这个数值之后，然后用全连接层进行分类。但是全连接层不一定是只能进行分类，其实还进行特征的一个变化。 进行线性变化完了之后，通过Softmax，然后再给它进行cross-entropy，就可以求出它的loss值了。 其实最近几年，就从2019年左右开始呢，其实大家慢慢的不用Softmax和cross entropy了，当然用这个也可以。为什么不用了呢？ 比方我现在有三个图片，IMAGE1、IMAGE2和IMAGE3，对应的label分别是3、5、6。那么要做cross-entropy的时候，就要把3变成[0,0,1,0,0,0,0,0]，然后5和6都要进行变化。然后才能跟Softmax预测出来这个probability做cross-entropy。也就是说，在这里要进行一次one-hot编码。结果后来就发现可以做一个简化操作，进行了Softmax之后给它前面加个log。 假如说，Softmax之后是0.1, 0.3, 0.3, 0.2, 0.1, 给它加个log,就会是一个负的比较大的数字，越接近于1，比方0.99，越接近于1结果会越接近于0，越远离1，这个负的值会越大。 所以现在大家会有一个非线性变化，叫做log Softmax，出来的结果就是负的。然后还有一个loss叫做NLLloss, negative log likelihood loss，这个在PyTorch里边也有。 这个有趣的地方就来了，如果我们它的label是3，直接来看一下log之后的值是不是-3, 给它再取个负号，那么就直接说这个的loss是3。如果它的label是5，那么log之后是另外一个值，假如说是-0.7，那么它取5，我们发现结果是-0.7，加个负号，它的loss直接就是0.7。 这样就不需要进行one-hot编码了，而且也能达到一个效果，就是我们期望的地方越接近于1，loss越接近于0。 所以，现在在工作中，我们看大量代码都开始这么做了，相当于是一个简化板的Softmax。 那这个呢就是我们整个卷积神经网络的工作流程，全连接层的作用大家一定要知道。 好，我们做一个总结。第一节课，给大家讲解卷积的原理。那么什么是卷积神经网络呢？只要用了卷积(Conv)这个操作的网络, 它就叫卷积神经网络。所以理论上，你可以让一个图形先经过卷积，再经过RNN，再经过卷积，再经过RNN，都可以。这个你既可以叫它卷积网络，也可以叫它循环神经网络。 然后呢跟大家说了CNN可以用在很多地方，比方说分类，探测，还有分割，其实背后都是卷积神经网络在做。 还有给大家讲了filters, padding, stride和channel，它的作用。除此之外，我们讲了Parameters sharing和Location Invariant。 在整个过程中，我们哪一层做卷积，哪一层做pooling，线性变化做几层，是不是纯靠经验？说白了这个确实还是纯靠经验，所以有一个很重要的特点就是我们需要去借鉴，我们需要去借鉴前人的经验。 几种神经网络结构 我们需要看前人的网络结构是怎么搭的，有几种比较重要的结构，LE-NET5，ALEX-NET。Alex那个net结构就是2012年ImageNet取得第一名的，上面有图。 它的特点就是第一次用Relu去做了非线性变化, 作用就会进行的比较快，它还在GPU上进行运算。 Relu就是一个非线性变化，如果把它做了卷积操作之后，给它再加个Relu，可以把它值再进行一个非线性变化就可以了，就是把它卷积出来的结果做了一个非线性变化。 GPU运算的作用是什么呢？GPU为什么重要？ 假设现在有一张1万 * 1万的一张图，有3 * 3的卷积核，如果说原始的状态我们得先从左到右再从上到下的做。我们得进行998乘以998次移动。 有GPU的话，我们可以让其中一部分在GPU的某个地方进行计算，另外一部分同时在GPU的另外一个地方计算，就可以分布式的。因为GPU所做的事情就是把矩阵运算可以分布式的在不同的地方并行运算。 这就是为什么有GPU玩游戏不卡，因为加载图片的时候它一部分图片在GPU某个地方加载，另外一部分图片在GPU另外一个地方加载，这是同时一起加载的。 如果年龄在30岁以上的小伙伴应该知道，以前看网页的时候那个大的图片会一行一行显示出来，就90年代末那会儿，图片是一行一行一行显示出来的。而对于GPU的话，显示图片是一块一块一起去渲染的。 那么对于卷积神经网络来说，这一块一块的filters，也是一起渲染一起计算的。所以说在做一层的计算的时候它就快了。而且如果你的GPU足够多，你还可以让它每一层的filters也并行计算。每一层的filters在每一块上又可以快速计算。 所以有了GPU的运行速度可以快十几倍，二十几倍，甚至上百倍都可以。 然后是VGG-NET。 VGG-NET是第一个真正意义上的深度神经网络，我们看这张图，它门一层都向下做了一个下采样。不断的下采样的结果是可以获得一些非常深的feature，或者一些非常高层次的feature。 VGG当时取得的效果也非常的好，也学的非常好。但是随着VGG正式的把我们带到深度神经网络这个过程中，我们就发现当网络特别深的时候会产生一个问题。 我们回忆一下，之前的课程中有讲过，当网络特别深的时候会产生什么问题？ 我们之前课程里有说，当网络特别深的时候就会产生梯度消失。 首先做这个变化的时候它的体现倒不是说就是会梯度消失，而是和梯度消失很类似。就是这个图片在前面运行的特别长，如果这个filter有几个值比较小，那么值经过filter值会变得很小，再经过一个filter又会变得很小。 到最后，原来的图像区别还挺大的，经过几次卷积之后呢，就都变成了很小的一些数字，展示出来就近乎一张纯色的图片。 这个其实在哲学上也可以理解一下，当你的抽象层次特别特别高的时候，全世界的东西都一样。对吧，就很佛系，科学尽头是神学。当你的抽象层次极高的时候，你看全世界所有东西都一样，在CNN里也一样，当你的这个东西足够长的时候，最后得到的东西它都差不多。 所以为了解决这个问题，就提出来一个重要的神经网络叫做RES-NET。 这个叫做残差网络，这个残差网络是非常重要的，是微软亚研当年提出来的。 2015年用了RES-NET造成了计算机视觉的识别率超过了人类眼睛的识别率，所以2016年是AI在产业中开始落地的第一年。 当然它的原理并不难，但是经过这样的一个修改，使得我们计算机识别网络的准确度超过了人类，然后开始了这个产业落地。 截图中是RES-NET的一个Block。 123456789class ResBlock(nn.Module): ... def forward(self,x): out = self.conv(x) out = self.batch_normal(out) out = torch.relu(out) return out + x 向前运算的时候输入x，经过了卷积，之后再给它进行一个Batch normalization ，它的那个值就把小的变大，大的变小。然后再进行一个Relu非线性变化，输出的是out加了个x。 这句话就是我们所谓的Residual的意思,就是理论上我们只要输出out就行了，但是为啥要加x呢？因为当经过很多层之后，out可能会变成0，变成一个纯色图片。所以把x加上，就是它还是保留了它的主要的图片信息，但是它在out上又有一些小的变化。这就是RES-NET的原理。 如果我们现在想做一个深的RES-NET的话怎么办？你给它输入一个三维的图片，比方说32个filters。 然后我们进行了一个ResBlock: 12345678class NetResDeep(nn.Module): def __init__(...): ... self.resblocks = nn.Sequential( *[ResBlock(n_chans=n_chans) for _ in range(n_blocks)] ) ... ... 这个地方其实相当于是ResBlock之后，输出的x+out又给它输入到了一个ResBlock，又是一个x+out。 我们这里Sequential的意思就是做完了这个，它的输出直接给下一个做输出。 在这个过程中，先让x进来做卷积、做非线性变化、做pooling。然后把它送到一串ResBlock: 1234567891011class NetResDeep(...): ... def forward(self, x): out = F.max_pool2d(torch.relu(self.conv(x)), 2) out = self.resblocks(out) out = F.max_pool2d(out, 2) out = out.view(-1, 8 * 8 * self.n_chans) out = torch.relu(self.fc1(out)) out = self.fc2(out) ... 这一串ResBlock, 它有很多个ResBlock，一层一层运行下来。之后，做了一个pooling, 之后做拉平，拉平之后在做一个全连接，就是对个权重进行线性变化，变化完了之后再加了非线性变化，最后再做一个线性变化。 这里的fc2，我们定义的维度是10，意思就是把它要变成一个10分类的任务。 然后我们再给它做个log Softmax，或者说cross-entropy，或者是NLL，就可以给它进行反向传播了。 这整个过程就是咱们的RES-NET。 只要这个网络有ResBlock，或者类似于ResBlock的，它都叫RES-NET。就像只要有卷积这个单元的网络都叫卷积网络一样。 这句话的意思是说，RES-NET其实有很多种。比方下面这张图，就是一个非常著名的RES-NET。 咱们刚才写的那个ResBlock是最简单化的ResBlock, 这个ResBlock是x进来之后，首先有一个卷积，卷积之后又给它进行了一个Batch normalization，normalization之后又进行了一个Relu，然后又进行了一个drop out，再之后再给它进行一个Relu，然后再sum，加上x。 这个是刚才我们写的ResBlock的一个更复杂的版本。这个网络结构是RES-NET的一个经典结构。 RES-NET的经典结构一共有这么几种：ResNet-18、ResNet-34、ResNet-50、ResNet-101、ResNet-152几种。ResNet-18和ResNet-34的基本结构相同，属于相对浅层的网络，后面3种的基本结构不同于ResNet-18和ResNet-34，属于更深层的网络。 感兴趣的可以去看看这篇论文：https://arxiv.org/pdf/1512.03385.pdf。 这五种结构都可以实现，但是它们的具体实现方法不一样。 RES-NET内理论上全部是卷积，没有全链接。全连接的部分其实是放在外边的。 RES-NET它的实现过程含有一点工程上的东西，如果是想要做计算机视觉的小伙伴，就需要想起的去学习一下这个部分。之后我会有专门讲CV的部分，会更详细的讲解。 然后我们再来了解一个Inception model， 直译的话称之为「初创模型」，一般大家都把它叫做inception。它是Google在RES-NET提出来之后提出来的一个神经网络。 Google的神经网络提出来的这个Inception机制有一个很很奇怪的点，就是它提出来了一个操作叫做1*1 convolutional, 意思就是我们把之前的卷积操作的那个kernel_size变成了1*1，就是变成一个点点。变成一个点点之后再加了一个非线性变化。 这个权重也是刚开始随机的，后来是学习出来的。 它相当于是把整个前面的图形，整体每个数字乘了一个数，然后再给它进行了一个非线性变化。 也就是说，如图 1 * 1的位置是5，相当于把前面矩阵内所有的数字都乘了个5，然后再进行了一个非线性变化。 假如现在有一个8 * 8的照片, 包含RGB就是8 * 8 * 3，现在有5个1 * 1的卷积核，那么得出的结果应该是多少？ 如图，也就是说，如果A为5， 那么B、C、D应该等于多少？分别应该是8 * 8 * 5。 所以它其实起到了什么作用？首先我们知道了第一个功能就是改变通道数。 改变了通道数之后，如果是28 * 28 * 3，这个Inception的机制是对每一层的输入要用多个不同的kernel size的卷积做操作，做完之后把这些值拼起来，把它再作为下一层的输出。 这个时候padding就很有用了，保证了值都是28 * 28，就可以连起来了，否则还要做各种reshap就很麻烦。 Inception第一个操作是它有一个1 * 1，什么都没干但它改变了通道数，第二个就是它使用了多个kernel size给一层做卷积，之后把它的结果全部连起来。 那么把所有连起来它会产生这样一个结果 刚刚说了，inception里面会把多个kernel size出来的结果连起来，因为有很多个Kernel size，这个结果就很长，我们希望把它变短，就可以用1 * 1的这个操作给它变短。 用了16个1 * 1的操作，就可以把这个256层的channel 变成16层的channel，变成16层之后再用5 * 5的卷积核去，得到了一个28 * 28 * 32的channel。 而如果直接用5 * 5再patting的话也可以得到一个28 * 28的channel，但是这两个是有区别的。如果直接用，就是28 * 28个channel，有32个。那么所需要拟合的参数就是160 million。 参数之所以大是因为连在一起的值特别大，特别的深。现在如果想把它变浅的话参数就少了。这个地方叫做Bottleneck Network, 称为瓶颈网络。就是将之前连在一起而特别长的这个channel给它变得特别短，然后在这个短的channel上再做计算，所消耗的参数算下来就只有13 million。 所以1 * 1的操作其实就是因为有了inception这种机制，所以会产生出特别长的结果。如果现在要对特别长的这个结果进行卷积的话，会需要的参数特别的多，而我们可以通过1 * 1的操作把它变短，之后再进行卷积操作，它的权重就少多了。这个就是这个inseption机制。 所以Inseption还是一样的道理，减少了参数的量，减少了parameters的数量，又降低了模型的复杂度，降低了过拟合，加快了计算速度。 那么我们卷积神经网络基本上到这里就给大家讲完了。关于更多卷积神经的应用后面会讲到专门的CV方面。 在这之前，接下来会用几节课分别讲解一下CV、BI和NLP的一些基础，给大家热热场子。","link":"/31.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%8F%8A%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"},{"title":"30. 深度学习进阶 - 池化","text":"Hi，你好。我是茶桁。 上一节课，我们详细的学习了卷积的原理，在这个过程中给大家讲了一个比较重要的概念，叫做input channel，和output channel。 当然现在不需要直接去实现, 卷积的原理PyTorch、或者TensorFlow什么的其实都实现了。但我们现在如果要用PyTorch的卷积操作，它就会有一个input channel和output channel的一个写法。 这里的方法是Conv2d，表示这里所应用的filter是一个2d的。那么它如何去做3d的？ 是将每一层的结果加在一起。 与此对应的还有一个Conv3d, 这个时候filter就是很多个不一样的。 我们一般在使用的时候，应用的都是Conv2d。感觉上好像觉得Conv3d里，我们每一层的值不一样其实会更好。但其实现在得到的这个filter是咱们人工写的，但是我们为了提取出来大自然中非常非常多的特征，其实我们会让机器自动生成、自动初始化一堆filter，这些filter的结果全部都是随机的。 就是说，这个filter的结果在深度学习中其实这些结果都是随机的，然后通过训练和反向传播，这些filter会自动地学习出来一个值。也就是说它会自动地收敛到某个数值上，而这个数值在这种环境下卷积应该怎么提取特征，那么我们做成2D的话所需要拟合的参数其实就少了。 假如filters前边的input channel很多、很深，那么这个filters需要拟合的参数也很多，如果是2D的话，只需要拟合2D的这一层就可以了。这就是2D和3D的区别，以及为什么一般要用2D不用3D。 如果现在要用卷积的话，第一个参数就是input channel。第二个就是output channel。output channel其实就是有多少个filters。 那kernel size指的就是做卷积的时候这个卷积的大小。比方说是3 * 3的，那么Kernel size就是3，也可以写成(3,3)。 还有一个参数叫做stride，这个stride就是步幅。那这个步幅是干什么的？我们一般用filter去卷积图像的时候，在矩阵上是一个单位一个单位从左到右从上到下移动的，这个步幅是为了加快移动，从而设置的间隔。比如[10, 9, 8, 7, 6]， 那我就拿一行来举例，知道意思就行了。比如这样一个数列，如果filter是3列，那按顺序就应该先是[10, 9, 8]，然后是[9, 8, 7]， 但是我设置了stride就可以跳步来执行。在[10, 9, 8]之后，可以是[8, 7, 6]。stride默认为1。 下面一个参数， padding。假如是一个6 * 6的图像矩阵，有一个3 * 3的filter， 那么对这6 * 6的图像进行卷积，会先变成一个4 * 4，然后变成2 * 2。显示出来的结果就是在不断地变小，代表抽象层次越来越高。 那么因为每一次window都在不断变化，在进行下一轮的时候，如果这中间要加一些什么操作，维度发生变化，会导致每一次中间要连接什么东西的时候维度都得重新去计算。 也就说维度不断的变化，会导致写代码的时候计算会变得更复杂。 那第二，就是我们也不希望减少的太快了。举个极端情况，把1万 * 1万的图像很快就变成一个2 * 2的了。抽象层次太高信息就少了。 第三个解释起来比较复杂，我们脑子里想想一下，一个filter在图像上进行从左到右移动，那么在依次进行卷积计算的时候，最左边的一列就只计算了一次，但是中间位置就会被卷入计算多次。我们希望的是边上的的数据也能被计算多次，就是也能被反复的提取。 要解决这三个问题有一个很简单的方法，就是padding。它的意思就是在这个图形外边加了一圈或者两圈0。如果你要加一圈0的话，padding=1。如果等于2的话，就加两圈0。 接下来，dilation。这个是在我们做图形的分割的时候用的。在图形识别的时候大家现在先不用去学习它。 我们一张图片进行卷积的时候，会越来越小，这个叫做下采样， down sampling。进行完下采样之后，如果要做图像的切分，我们要把图像里边主体部分全部给它涂黑，别的地方全部涂白，需要基于这个小的采样又把它给扩大，慢慢恢复到原来大小，这个叫做上采样。上采样时，有时候会用到dilation。 重要的就是这几个参数。这几个参数给大家说完，其实基本上卷积的几个重要的特性就说明白了。 池化 除了卷积之外，还有一个比较重要的操作: pooling, 池化操作。 池化操作其实很简单，我们给定一个图片，卷积操作是选了一个window和filter，做了一个f乘w然后给它做相加. sum(f*w). pooling是一个很直接的操作, 把w这里边所有的值给它取个平均值, 也有可能取个最大值。假如是它最大值，那么值最大就代表着是在这个图形里边对他影响最重的这个点。 那么做了pooling之后，每一次这样一个操作，图形变小了，但是图像基本上保持了原来的样子。就是pooling操作前后的图像是相似的，它取了最重要的信息。 我们现在来思考一下，如果有一个图片，不管是卷积还是pooling都会让其缩小。那我们思考下， 既然两个操作都会导致图像缩小，那为什么会存在两个操作呢？ 咱们机器学习里面最头疼的事情就是所谓的过拟合，过拟合就是在训练的时候效果挺好，结果在实际中效果就不好了。 而控制过拟合最主要的就是能够减少参数，在越少的参数能达到效果的时候，我们期望参数越少越好，在同样的数据量下就越能防止过拟合。 卷积里面这些值以前的时候是人来确定，但现在其实是期望机器自动的去求，也就是说这个参数是需要自己去求解的。而pooling并不需要去设定参数，它没有参数，这样会减少参数。 用了pooling之后不仅减少了参数, 还减少了接下来x的维度。所以最核心的其实是我们减少了所需要训练的参数。 之所以用pooling是因为可以减少参数，可以让它的过拟合的问题减弱。但是如果你的数据量本身就很多，或者说模型本身就比较好训练、好收敛，那你没有这个pooling操作其实也是可以的。 权值共享和位置平移 那么这个时候就要跟大家来讲一个比较重要的概念，叫做权值共享和局部不变性: Parameters Sharing and Location Invariant.这个Loction Invariant也有人把它叫做shifting Invariant。CNN的最重要的两个特点，第一个特点就是它的权值共享。 我们给定一个图片，就之前我那个头像，假如有一个filter，它是3 * 3的，那么这3 * 3的这个网格它在每一个窗口上都是和这个filter做的运算。 那么大家想一下，假如有一个1,000乘以1,000的一个图形，我们这1,000 * 1,000的图形我们要把它写成wx+b的话，这个x是100万维的，那么这个w也是100万维的。 那么如果我们要做训练的话，就要训练100万个w。这是拟合一个线性变化，那么我们如果现在是要去拟合一个卷积，假如output channel是10，那我们需要拟合的参数是多少？ 卷积核是3 * 3, 有10个。 那就是9 * 9再乘以10。不管这个地方是1,000 * 1,000还是1万 * 1万，我们要拟合的都是卷积核里的这个参数。 我们做一层卷积，哪怕给了10个卷积核，也是九十个。如果要给它做一层线性变化，得100万个，这两个相差特别大。 为什么相差这么大？是因为不同的位置上用的filter的值是一样的。filter的参数整个图像共享了。这就是卷积神经网络的权值共享。 那么我们现在想一下，我们有了这个Parameters Sharing，它的作用是什么? 减少参数量的作用是防止过拟合，防止过拟合的最终体现就是我们在各种计算机视觉上的任务，表现就好。除此之外还有一个特性，它可以大大的提升我们的计算速度。 本来我们以前如果你有这么多参数的话，要反向传播一次要进行100万个反向传播。现在我们只要进行九十个就行了。 所以权值共享其实是卷积神经网络为什么效果特别好的原因。 2012年的时候，计算机视觉的测试效果一下子有了突飞猛进。当时就是因为用了卷积神经网络。 以前大家在实验室环境下，在训练集上的效果都挺不错，但是一拿到测试集的时候效果就很差。后来用卷积神经网络之后，这个错误率一下就下降了。 权值共享这个特性因此带来了一个特点，就是Loction Invariant。就是一个局部的东西，我们把它信息连接在一块了。 我们分别有两张图片，比如下面这张我以前画的一幅画，我把构图分别改变一下。 这两个图像数据表征上是很不一样，但是眼睛所在的位置经过卷积之后，只要用的是同一个卷积核，产生的结果是相似的。所以这个Loction Invariant指的意思是，不管这个眼睛在哪我们都能提取出来。 假设我们在train的时候, 眼睛不管在哪，只要把这个filter训练出来了，在test数据集上就算它的位置变了我们依然能够提取出来它的特征，依然能够计算出来和它相似的这个值，这个就叫做Loction Invariant。 这两个特性是极其重要的。 搭建卷积神经网络这个事，说实话最主要的还是要看经验。那么前人的总结就很值得借鉴，下节课，我们就来看看几种经典的神经网络结构。","link":"/30.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%B1%A0%E5%8C%96/"}],"tags":[{"name":"产品经理","slug":"产品经理","link":"/tags/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"CV","slug":"CV","link":"/tags/CV/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"BI","slug":"BI","link":"/tags/BI/"},{"name":"LLM","slug":"LLM","link":"/tags/LLM/"},{"name":"Math","slug":"Math","link":"/tags/Math/"},{"name":"Neural Network","slug":"Neural-Network","link":"/tags/Neural-Network/"},{"name":"Chrome","slug":"Chrome","link":"/tags/Chrome/"},{"name":"ChatGPT","slug":"ChatGPT","link":"/tags/ChatGPT/"},{"name":"Mac","slug":"Mac","link":"/tags/Mac/"},{"name":"Stable Diffusion","slug":"Stable-Diffusion","link":"/tags/Stable-Diffusion/"},{"name":"Google","slug":"Google","link":"/tags/Google/"},{"name":"javascript","slug":"javascript","link":"/tags/javascript/"},{"name":"Gmail","slug":"Gmail","link":"/tags/Gmail/"},{"name":"Photoshop","slug":"Photoshop","link":"/tags/Photoshop/"},{"name":"Model","slug":"Model","link":"/tags/Model/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"微积分","slug":"微积分","link":"/tags/%E5%BE%AE%E7%A7%AF%E5%88%86/"},{"name":"AI核心","slug":"AI核心","link":"/tags/AI%E6%A0%B8%E5%BF%83/"},{"name":"办公自动化","slug":"办公自动化","link":"/tags/%E5%8A%9E%E5%85%AC%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"categories":[{"name":"从零开始接触人工智能大模型","slug":"从零开始接触人工智能大模型","link":"/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"AI秘籍","slug":"AI秘籍","link":"/categories/AI%E7%A7%98%E7%B1%8D/"},{"name":"Python","slug":"AI秘籍/Python","link":"/categories/AI%E7%A7%98%E7%B1%8D/Python/"},{"name":"Math","slug":"AI秘籍/Math","link":"/categories/AI%E7%A7%98%E7%B1%8D/Math/"},{"name":"核心能力基础","slug":"AI秘籍/核心能力基础","link":"/categories/AI%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"}],"pages":[]}