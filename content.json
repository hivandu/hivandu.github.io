{"posts":[{"title":"2023年薪酬最高的科技工作中产品经理赫然在列","text":"在最近一段时间内，互联网以及科技公司面临了很大的挑战，工作岗位迅速减少。这让我很大程度上看衰了互联网行业，并且认为目前只是开始，更大的裁员潮还没延伸到中小企业。让我们来看看数据： 就数据（美国的数据）来看，现实非常严峻，2023年迄今为止已有809家科技公司裁员211,400名员工（这个数据一直在变化，最新数据可以看这里），但是各组织仍然在快速招聘人才，以填补云技术、区块链和网络安全等新兴领域的滋味。 看出来了么？并不是互联网和技术行业不香了，而是技术正在进行一轮行业替换，岗位需求发生了大变化。虽然早几年前大家都知道这些岗位是日后的方向，可是这一下子发生了一个断层的变化，导致很大一部分人没有完成转变，科技公司也没进行缓慢的过渡，再加上整个大环境的经济压力，才导致了如今全世界范围内（中国并未逃脱）的大裁员。如果最近一连串的科技裁员让您感到惊慌，请放心，科技作为一个行业仍然健康发展。 而事实上，83% 的美国人力资源专业人士表示，在过去 12 个月里招聘候选人一直很困难，这导致了未来缺乏必要的技术专业人员的更大问题。 麦肯锡最近的一份报告发现，43%的组织目前面临技能缺口，而来自Korn Ferry的单独数据表明，到2030年可能会出现8500 万人短缺，导致同年潜在年收入总计损失 8.5 万亿美元。 好消息是，对于那些寻找新工作的人来说，机会很多。但是坏消息是，对于35岁以上的求职者，这依然是一个难以跨过去的坎，我们不得不承认一点，对于在国内的求职者们，35岁这个节点比在国外更加明显。（为我自己默哀。） 好了，说了这么多，还是要有点干货的。接下来咱们根据国外的相关数据，详细介绍五个薪酬最高的技术职位，数据来源于Payscale，可以在VentureBeat上找到数千个职位，就算国内的求职者们碰不到这些职业，我们依然可以从这些岗位的数据来分析一下目前最吃香的相关职位，为自己的转型做个有力的参考（以下内容都是基于美国当前数据所做的分析）。 1. 云计算解决方案架构师 预计2023年云计算应用将超过6000 亿美元，并将推动人工智能和Web3等新兴技术。 平均工资： 132,700 美元 如果您有云计算方面的经验，德勤正在招聘一名云解决方案架构师，负责核心业务运营（CBO）组合的工作，以帮助C-suite和项目负责人通过新兴和颠覆性技术改造他们的组织并加速任务执行。 此外，SAIC 正在聘请一名专门从事系统工程的云解决方案架构师，以协助确定技术解决方案，解决技术差距，如在其国家情报社区（NIC）业务部门、美国政府任务和信息技术部门内的蜂窝和云服务。 2. 产品经理（软件） 产品经理负责根据数据制定策略，其角色不断发展，因此是任何销售产品或服务的组织不可或缺的一部分。 由于我本身就是一名数据产品经理，这里我不得不发表一些感慨。不过我们还是得认清一个事实，产品经理的门槛在不断变高，不要认为PRD和原型就可以胜任了，我们从数据中可以看到，数据产品、策略产品以及安全相关的产品经理更容易赢得心仪的工作。 基于此，我觉得我还是的多写点数据产品经理的相关文章了。 平均工资： $102,866 如果您正在该领域寻找职位，西门子正在寻找一名高级产品经理来领导网络安全产品的产品策略的开发和执行，与保护和自动化产品/解决方案的产品管理人员密切合作，以确保无缝集成网络安全功能。 与此同时，苹果公司正在招聘一名新产品技术项目经理。在此职位中，您将需要建立矩阵管理并监督材料预测、规划、分析和报告、物流准备、预算、采购和配置管理活动。 3. 网络安全工程师 由于数据泄露和网络威胁仍然是一个大问题——网络犯罪预计每年增长 15%，到2025 年将达到每年 10.5 万亿美元——网络安全领域迫切需要拥有保护企业及其资产的技能和经验的专业人员来自恶意软件攻击。 平均工资： $99,887 国土安全部特别投资于网络安全，因此政府和军事承包商 Booz Allen Hamilton 正在美国各地招聘各种网络安全工程师职位，包括华盛顿、圣安东尼奥和埃尔塞贡多。 在这些职位上，您将需要提供国家和国际层面的网络安全解决方案。 4. 软件工程师 美国劳工统计局预测，从 2021 年到 2031 年，软件开发人员、质量保证分析师和测试人员的就业人数预计将增长 25%，新增 411,400 个就业岗位。 平均工资： $90,777 对于那些拥有丰富经验的人，诺斯罗普·格鲁曼公司正在招聘一名软件工程师/首席软件工程师，作为其企业范围数字化转型的一部分。在此职位上，您将支持工程应用和产品的生成，例如实验室电子战 (EW) 以及靶场训练和模拟系统。 经验丰富的 Aces Incorporated 也正在招聘一名软件工程师来应对美国政府最困难的挑战。 对于金融服务行业的职位，摩根大通银行正在招聘一名全栈首席软件工程师，以安全、稳定和可扩展的方式增强、构建和交付值得信赖的市场领先技术产品。 5. 区块链工程师 虽然大多数人认为区块链的唯一功能是加密货币，但该技术目前已应用于医疗保健、房地产、抵押贷款处理和游戏等各个领域，并且是一个正在增长的领域。 平均工资： 90,000美元 cyberThink Inc 正在寻找一名区块链工程师，带领技术开发人员和云工程师团队建立 AWS 区块链集成环境并管理数据接口和链码开发。 在西海岸，Third Republic 正在与一个开发团队合作，该团队为财富 500 强公司提供创新的软件开发解决方案，以聘请区块链开发人员。","link":"/2023%E5%B9%B4%E8%96%AA%E9%85%AC%E6%9C%80%E9%AB%98%E7%9A%84%E7%A7%91%E6%8A%80%E5%B7%A5%E4%BD%9C%E4%B8%AD%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E8%B5%AB%E7%84%B6%E5%9C%A8%E5%88%97/"},{"title":"纪念","text":"纪念 1234567891011121314&lt;math xmlns='http://www.w3.org/1998/Math/MathML'&gt; &lt;mn&gt;25&lt;/mn&gt; &lt;mo&gt; &amp;#x00D7;&lt;!--multiplication sign --&gt;&lt;/mo&gt; &lt;msup&gt; &lt;mrow&gt; &lt;mn&gt;2&lt;/mn&gt; &lt;/mrow&gt; &lt;mrow&gt; &lt;mn&gt;6&lt;/mn&gt; &lt;/mrow&gt; &lt;/msup&gt; &lt;mo&gt;=&lt;/mo&gt; &lt;mo&gt;?&lt;/mo&gt;&lt;/math&gt; 以上，为了纪念！","link":"/25X2de6cifang/"},{"title":"《AI秘籍》预告","text":"Hi, 大家好，我是茶桁，这里为自己做个广告，目前打算开始写一整个系列《AI秘籍》。 这一段时间内我写过一个系列《零基础学习大语言模型》（目前还没写完）。 说实话，这个系列其实原出处并不是我，严谨的说来，有涉嫌擦边“洗稿”的嫌疑，所以最后放弃了收费的想法，仅仅对一些模型，资源以及计算结果进行了补偿性的收费。不过在写这个系列的同时，我开始有了自己的一些想法，打算真正写一个属于自己的系列文章。 因为我的个人博客并没有付费阅读的功能，所以还在看平台。第一选择自然是我的微信订阅号，有想过发到少数派里，但是并不清楚少数派对我文章的审核会是什么结果，能成为专栏发出来不太有信心。 说说这个专栏本身，参照我几个自媒体平台的数据来看，Python的基础知识还是更受欢迎一点，我想大概也是更多基础不太好的小伙伴希望能入行吧。所以这次我准备从基础开始写起，总的来说分成以下几个大的篇章： 第一卷：Python 第二卷：核心知识 第三卷：核心能力培养 第四卷：NLP 第五卷：BI 第六卷：CV 第七卷：扩展 - 数学 第八卷：扩展 - 英语 大致的算了一下，可能这个系列会耗费比较长的时间和精力，也希望小伙伴们能多多支持。 在这里，放上我的公众号订阅方式： 最后，找到合适的订阅平台之后，本篇内容应该会有更新。","link":"/AI%20Cheats%20Trailer/"},{"title":"卡尔曼滤波器的非数学介绍","text":"如果你想查看的话，本文的代码可以在我的Github上查看。 卡尔曼滤波器非常巧妙。如果你从未听说过卡尔曼滤波器，那么一种非常直观（也可以说是还原）的思考方式就是将其视为一个漏斗，在这里你可以从多个嘈杂的信息源中获取信息，并将其浓缩为一个更精确的统计数据。 如果这一切听起来含糊不清，请不要担心。稍后，我们将把这句话剥离成一个更容易理解的例子，希望能进一步加深我们的直觉。要研究和推理卡尔曼滤波器，没有比数学更好的工具了。但同样，卡尔曼滤波器的基础数学具有挑战性，包含线性代数、概率论和微积分等内容。因此，并非所有人都能轻松掌握。这篇文章的目的就是希望为你提供一个易于理解的直观印象，或许能促使你自己深入研究这个问题。现在，让我们开始吧，同时牢记这一点：\"以下内容仅提供直觉，可能并不完整\"。 让我们先问一句：\"为什么卡尔曼滤波器是必要的？对于这个问题，一个简单而又故意模糊的答案是：现实生活并不完美。请看这个激励性的例子：想象一艘船在一个维度上行驶，从港口出发（x=0）并行驶一段距离。这艘船的发动机被设定为为船提供一个恒定的速度，例如 10 米/秒。 我们首先要问的问题是，在离开港口 2 秒钟后，船到底在哪里？很自然，你会说船离港口的距离是 210=20m，因为毕竟距离 = 速度 时间。在理想世界里，这的确是正确的，根本不需要卡尔曼滤波器。但在现实世界中，情况绝非如此简单。首先，可能没有足够的发动机能够产生足够的力，使每个时间点的速度始终保持在 10 米/秒。当然，你可能会在某些时候获得 10.00001 的速度，或在其他时候获得 9.99999 m/s 或介于两者之间的某个数字，但正如所说，99.99% 的完美终究还是不完美。其次，即使你说你确实拥有这样一个完美的发动机，但当你施加一个精确测量的力时，你也不可能获得预期的完美速度。波浪运动可能会让你的船稍微慢一点，或者风可能会让它加速，或者谁也不知道什么东西会以什么方式对它产生影响。 因此，仅仅通过测量你想要的位置，你永远无法确定你的船在哪里。 那么，我们是否注定永远无法真正知道自己的位置呢？不尽然！这就是传感器的用武之地。想象一下，你，水手，随身携带一个全球定位系统。这样，GPS 就能精确地告诉您在任何给定时刻的位置！事实上，你现在甚至不需要船的速度，因为无论船如何行驶，你的全球定位系统都能准确地告诉你所在的位置。问题解决了吗？就像我说的，不完全是。在现实中，传感器经常会出现错误，而且不可靠。也就是说，它们确实能告诉你你在哪里，但测量结果可能并不精确。因此，您的 GPS 可能会告诉您，3 秒钟后您距离港口 29.998 米或 30.002 米，甚至是距离港口 100 米，但这种可能性极低。此外，您也无法确保传感器永远不会出现故障。以 GPS 传感器为例。一旦你发现自己身处没有卫星覆盖的地区，它就会失灵。事实上，如果有一个传感器能保证永远不会离线，并能以任意的精确度测量出你想知道的信息，那就根本不需要卡尔曼滤波器了。 有了这些，我们现在就可以回答为什么需要卡尔曼滤波器了。而答案与我们之前已经确定的并没有什么不同。卡尔曼滤波器是一个漏斗，它能接收两个或更多不完美、不可靠的信息源，并对你想知道的信息做出更准确的估计。在这个例子中，卡尔曼滤波器会把你在任何时间的速度估计值和 GPS 估计值（如果有的话）作为输入，然后给出比这两个信息加起来更准确的估计值！事实上，如果你有更多的信息来源，比如雷达或声纳，甚至是你目前在水中看到的鱼的种类，理论上你可以将这些测量结果结合起来，从而对你的位置做出更准确的估计。 因此，现在的问题是，如果不使用这样的数学知识（摘自维基百科），我们如何理解卡尔曼滤波器的作用和原理？ 首先，我们假设船上没有一名乘客，而是有一千名乘客，每个人都有自己的 GPS 设备。现在，每位乘客都可以通过以下方式进行基于速度的估算，从而估算出自己的位置（进而估算出船的位置）： 123456from random import gaussdef new_position(last): velocity = 10 wind = gauss(0, 2) wave = gauss(0, 0.1) return last + velocity + wind + wave 注：有关高斯函数的可选但更完整的解释，请参阅下面的附录。目前，只需说明它会产生一个随机数（正数/负数），其顺序由第二个参数指示即可。 从本质上讲，这 1000 名乘客中的每一个人都是这样做的：取上一次已知的位置（在现在之前的时间），加上速度，并且知道风和水波会轻微地改变航向，再加上一些随机的估计波动。现在，如果这些乘客真的有估算风速和水速的好方法，他们就会使用它。但因为他们没有，所以只能用随机数来估计影响。实际上，现实生活中也是如此。我们不可能测量所有的东西，所以我们只能用一些简单的方法来估计它们，就像我们上面用平均值（0）和偏差参数（0.1 和 2）所做的那样。 现在我们进入卡尔曼滤波法的第二阶段，即测量。在这一阶段，所有乘客都知道，由于风噪和水噪的影响，他们对自己的状态（所处位置）只有不完全的了解，因此，他们会利用自己的传感器来改善自己的状态： 1234567891011def sensor(t): if t == 3: # oops, passing through a thunderstorm. GPS fluctuating! sensor_noise = gauss(5, 10) elif t == 6: # uh-oh, satellite unavailable! sensor_noise = gauss(-5, 10) else: sensor_noise = gauss(0, 1) return true_position[t] + sensor_noise 请记住，传感器是一种不精确的设备，也就是说，它们返回的统计数据大多是正确的，在本例中就是变量 true_position，但它们本身也有噪声，我们再次使用高斯函数随机生成的数字来模拟这种噪声。此外，我们在这里还模拟了传感器的不稳定性，即在某些情况下（t=3 和 t=6），由于某些因素传感器基本上是不可用的，而这些因素并非完全不可想象。因此，每位乘客在使用传感器时，实际上都会得到不同的测量结果。 想象一下，这艘船现在离开港口，每秒行驶这些距离： 12true_position = [0, 9, 19.2, 28, 38.1, 48.5, 57.2, 66.2, 77.5, 85, 95.2] 也就是说，船从港口出发（x=0），第一秒行驶 9 米，第二秒行驶 10.2 米，最后到达 19.2 米，以此类推。现在，乘客们的任务是利用他们所掌握的嘈杂且不可靠的测量数据，尽可能准确地预测出每一秒的不同位置。 因此，在时间 t = 1 时，乘客可以通过上述函数得到这些读数： 1# 如果 t=0 时的新位置为 0，则 t=1 时的新位置为 0new_position(0) =&gt; 9.37 (error -0.37)# t = 1s 时的传感器读数sensor(1) =&gt; 8.98 (error +0.02) 所有乘客都是如此。现在的问题是，真相到底是什么？是我们的牛顿物理知识更可靠，还是 GPS 传感器更可靠？在这种特殊情况下，由于我们已经知道船的真实位置距离 true_position 变量 9 米，答案可能是显而易见的，但情况并非总是如此。在这种情况下，为了将这两个独立的统计数据结合起来，我们实际上采用了一种非常简单的方法：取两者的平均值！在上面的例子中，我们可以得出以下结果 1combined =&gt; (9.37+8.98)/2 =&gt; 9.17 (error -0.17) 请注意，在这个例子中，综合统计量的误差比单独的速度估计值要小，但比传感器估计值要差。但问题是，我们实际上可以做得比取平均值更好。考虑一下这样的情况：你知道你的传感器实际上是最先进的，而且非常可靠。这实际上意味着你应该更倾向于传感器的数据，而不是速度更新的数据。实际上，您可以通过使用加权平均值来做到这一点。请看这段代码 123def combine(A, B, trustA, trustB): total_trust = trustA + trustB return (A * trustA + B * trustB) / total_trust 这就综合了 A 和 B 来源的两个数字，但也考虑到了您对这些来源的信任程度。因此，如果您将其称为 12combine(9.37, 8.98, 10, 1) =&gt; 9.33 (error -0.33)combine(9.37, 8.98, 1, 10) =&gt; 9.01 (error -0.01) 在第一次调用中，您对源 A（速度）的信任度远高于源 B（传感器），即 10 比 1，因此得到的答案更倾向于源 A，即更接近 9.37。这种基于信任的加权平均法是卡尔曼滤波器的核心，也是它的数据组合能力所在。 但现在，我们遇到了一个新问题。哪个来源更可信，或者如何计算可信度？是应该优先考虑速度呢？还是应该优先考虑 GPS 测量结果？决定这一点的是偏差或方差指标。想想看，什么更值得信赖？是波动剧烈的信息源还是没有波动的信息源？试想一下，你收听 10 个气象广播电台，其中 4 个告诉你会下雨，6 个告诉你会是晴天。现在想象一下，你登录 10 个天气网站，其中 9 个告诉你会下雨，1 个告诉你会是晴天。哪个消息来源更可靠？你倾向于相信大多数气象广播电台告诉你的（晴天）？还是你倾向于相信天气网站告诉你的（下雨）？理性的做法是更倾向于网站的结论，因为许多网站的结论都是一致的，即它们的方差较小，而气象广播电台，至少在这个例子中，它们的结论似乎波动很大，所以也许不应该太相信。 这样，完整的更新步骤就变成了这样： 1234567891011121314151617181920212223242526272829303132from statistics import variance# Find updated positions per passenger at t secondsdef update(t, last): velocity_updates = [] sensor_updates = [] for p in range(1000): # for each passenger # new velocity update based on last known position # for the passenger velocity_updates.append(new_position(last[p])) sensor_updates.append(sensor(t)) # Calculate trust metrics for velocity and sensor measurements # Remember that as fluctuation increases, trust decreases # And vice-versa fluctuation_velocity = variance(velocity_updates) fluctuation_sensor = variance(sensor_updates) # calculate trust trust_velocity = 1/fluctuation_velocity trust_sensor = 1/fluctuation_sensor # combine these together for each passenger combined = [] for p in range(1000): combined.append(combine(A = velocity_updates[p], B = sensor_updates[p], trustA = trust_velocity, trustB = trust_sensor)) # Sensor updates &amp; velocity updates returned for plotting purposes return sensor_updates, velocity_updates, combined 注：有关方差函数的更多信息，请参阅附录。现在，只需将其视为数字列表波动的度量。 这段代码相对简单。对于每位乘客，它都会进行基于速度的噪声测量和基于传感器的噪声测量。根据所有乘客的这些测量结果，计算出每个测量结果的信任度指标，作为方差的倒数（因为方差增加，信任度降低），然后调用包含相关信任度参数的组合方法。值得注意的是，这里的每位乘客都在为自己进行位置更新。在这些单个更新结束后，可以根据所有乘客位置的平均值推断出船只本身的实际位置。 我们使用以下代码来连接上述整个代码。 123456789101112131415161718192021# We'll do a final plot using this listplot_data = []def update_plot(t, sensor, velocity, combined_position): # add true position at this time plot_data.append({'passenger': 'true', 'type': 'true', 'time': t, 'position': true_position[t]}) # for each passengers for p in range(1000): plot_data.append({'passenger': p, 'type': 'sensor', 'time': t, 'position': sensor[p]}) plot_data.append({'passenger': p, 'type': 'velocity', 'time': t, 'position': velocity[p]}) plot_data.append({'passenger': p, 'type': 'combined', 'time': t, 'position': combined_position[p]})update_plot(0, [0]*1000, [0]*1000, [0]*1000)estimated_positions = [0]*1000 # all estimates start from 0for t in range(1, 10): # ten seconds _sensor, _velocity, estimated_positions = update(t, estimated_positions) update_plot(t, _sensor, _velocity, estimated_positions) update_plot 函数只是做一些基本的簿记工作，以存储用于绘图的瞬时统计数据。这里的主要迭代只是最底层的 for 循环，它使用乘客当前的最佳估计值，在任何给定时间持续更新位置估计值。除此以外，代码基本上不言自明。 使用 seaborn 库绘制的结果如下： 由于目前的比例尺，这有点难以解析。让我们放大这两个区域，特别是 t=0.75 至 t=1，即传感器正常工作时，以及 t=2 至 t=4 出现故障时。 注：包络线指的是不确定性。线中的包络线越宽，我们对数字的不确定性就越大。 在第一种情况下，正如您所看到的，所有 1000 名乘客的综合位置估计值比单独的速度估计值要好（绿色），虽然在第一种情况下，我们的估计值确实比我们的传感器读数要差，但在第二种情况下，我们的估计值实际上比单独的故障传感器读数要好得多！这是因为卡尔曼滤波器会自动调整不可预见的波动造成的剧烈变化，并始终为我们提供合理可靠的指标。如下图所示，一旦我们的传感器恢复正常（t=4 到 t=5），卡尔曼滤波器就会再次偏向于传感器（由于传感器读数和真实值重叠太多，所以有点难看）。 我相信你至少对卡尔曼滤波器的工作原理有了一些直观的了解。卡尔曼滤波器的实际理论基础同样引人入胜，如果你的工作需要，我鼓励你继续深入研究。与此同时，我希望这篇文章能证明，代码作为一种形式语言，能在多大程度上帮助人们对那些乍看之下令人生畏的概念产生直觉。我也希望能够通过简单的代码，向大家传授一些我认为很有吸引力的话题的更多见解。 高斯函数 这里唯一需要知道的特殊函数是正态分布函数，即 gauss(0, 0.1) 和 gauss(0,2)。简单地说，它给你一个随机数，这个数通常在 0 附近（技术上正确的说法是以 0 为中心），而得到离 0 更远的数的几率由第二个参数控制，即 2 和 0.1。 因此，如果调用 gauss(0,0.1)，得到 0.06、-0.07、-0.06、0.02、-0.23、-0.06、0.09 等数字的可能性较大，顺序不分先后。 而如果调用 gauss(0,2)，则更有可能得到 1.05、1.03、-1.06、0.32、1.29、-0.40、-1.72 等数字，同样不分先后。 直观地说，第二个参数也叫标准偏差，控制着测量值的波动程度。在上面的代码中，这意味着您通常会认为风的偏差过大（大风天？请注意下面直方图中偏差=2 和偏差=0.1 所产生的数字的频率（特别注意 x 轴）。虽然数字的范围有很大不同，但这两个直方图的形状看起来差不多。这种钟形分布被称为高斯分布、正态分布或钟形曲线分布，在自然界中经常出现。 方差 方差是衡量一致性的标准。也就是说，如果一致性好，方差就小，反之亦然。在上图中，由于 x 轴实际上是自动调整的，所以你无法完全看到方差。如果我们在相同的坐标轴限制内绘制上图中的直方图，就会得到如下结果： 注意到第一张图片有多宽了吗？这是因为其中的数字变化很大。也就是说，你会发现其中有很多 -2、2、0 和一些 4、-4。但在第二幅图中，你会发现很多 0、0.1、-0.1 等，但你会发现-2、2 等的数量会少很多。正确地说，第一个分布的方差（准确地说是 4）大于第二个分布的方差（0.01）。有关方差的更多信息，请上网查阅。","link":"/2023_8_3_Kalman/"},{"title":"新专辑《AI秘籍》，你所感兴趣的一切","text":"Hi，大家好。我时茶桁。 最近，我花了几天时间仔细思考了一下即将要开始写的专栏《AI秘籍》，再根据自己的能力大概规划了一下。目前大致已经理出了一些相关信息可以分享给大家。 专栏形式 本次专栏应该会以文章的形式先和大家见面，后续还会根据能力以原本的文章为准录制视频版本。 专栏平台 就如前一篇文章公布的内容一致，会优先发表在我的公众号上，当然目前我还在努力寻找其他的专栏平台。 我的预想是尽量能够让大家一篇一篇的购买，不需要必须购买全部专辑，这样朋友们可以根据自己具体需求来进行购买。 而目前我的百家号收费专栏也在申请之中，不知道会不会顺利申请下来。有新的平台入驻之后，我会进行通知的。 专栏内容 在规划专栏的时候，大部分时间基本都放在了规划内容上。包括目录的编排，内容取舍等等。 目前规划中的专栏打算从基础开始，到Python开发，再到一些应用基础，比如AI数学，AI英语等。而由于这些内容都会是针对AI学习的，所以并不会是那种很全面的学习资料。 比如说Python，我们不会讲的很系统，重点会放在数据结构以及数据分析和开发方面。数学等基础当然也会是一致的。 当基础篇学完之后，接下来就是重点了，会根据三个不同的AI方向来进行讲解，分别包括：BI、NLP以及CV。 基本目录如下： 第一篇： Python基础（AI方向） 第二篇：核心基础能力 第三篇：核心知识增强 第四篇：BI 基础 第五篇：CV 基础 第六篇：NLP 基础 第七篇：BI 进阶 第八篇：CV 进阶 第九篇：NLP 进阶 拓展篇1: 数学 拓展篇2: 英语 详细目录如下（进阶部分目录未完全展开）： 一些说明和后续 本专栏暂时价格上还未进行调研，反正第一篇Python部分应该会是全免费发放。毕竟Python课程网上太多了，而且同质化严重，收费感觉没太大必要。所以，咱们写的时候再慢慢想。小伙伴们也可以留言来说说大家期望是一个什么价格，我根据大家留言再结合自己的实际情况最后定价。 另外，除了Python部分之外，数学和英语部分也是免费的。说实话，我数学和英语并不是很好，这两部分我仅仅给大家一个总结和方向，反正也是独立内容，均可以去网上找相关替代的。 专栏在完成之后，会更新一些关于算法和数据库的内容，然后会考虑整篇投放到其他平台去进行完整售卖。 结尾 好了，结束语也无需说太多。让我们一起期待吧，希望在我的课程完成的那一天，各位小伙伴们能完全入门并掌握人工智能。 本次课程的所有代码都会上传到Github上，地址：","link":"/AI-cheats-information/"},{"title":"12 AI帮你写个小插件，轻松处理Excel文件","text":"开头我就要跟各位先说对不起，本来我是很想为大家把这部分实现并完成的。但是很抱歉，因为我用的Mac，而这部分代码实现起来的时候一直会如下报错： 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/AI-create-a-excel-plugin/"},{"title":"Analysis data and research report collection","text":"The purpose is to facilitate finding specific locations when doing data analysis by yourself 1. 国内咨询机构网站数据报告列表 艾瑞研究-艾瑞网 互联网行业报告 艾瑞APP指数 移动App TOP 1000 月度活跃和日活跃 艾瑞PC指数 PC TOP 1000 月度活跃和日活跃 199IT互联网数据中心 中国互联网络信息中心 数据新知 - 易观 【友盟+】数据报告 http://www.dcci.com.cn/report/index.html 北京赛诺市场研究有限责任公司 赛诺数据，智能机出货量的专业统计 数据报告-移动观象台-TalkingData Talkingdata报告 艾媒网-全球移动互联网行业数据发布平台/iiMedia Research出品 DataEye大数据移动营销综合服务商-数据报告 手游方向 ASO100 - 专业App Store数据平台丨ASO优化专家平台丨iOS榜单排行榜查询工具 电影电视行业免费报告列表页 研究娱乐行业 旅游数据报告-旅游圈旅游行业报告 小程序报告-阿拉丁统计 爱应用：一个应用所有历史版本的产品分析截图记录 Appannie，国外下载应用 卡思数据-短视频网红分析数据分析 国金证券研究所 国家宏观经济研究数据和报告 中金研报 抖音快手的热门视频和kol的 各大媒体的每天的热门排行榜 短视频行业的数据 招商证券的电商类报告电商类的行业观察，企业研究，品牌深度报告 短视频和图文内容类的行业报告-新榜 http://www.100ec.cn/zt/wmds/ 涵盖跨境电商所有报告，行业数据和研报 镝数聚-权威数据 海量聚合 提取了报告中的数据，颗粒度比较细 东方财富研报首页 东方财富研报 2. 国家机构公开数据 中国信通院-研究成果-权威发布-权威数据 中国城市轨道交通协会城市地铁线路的流量数据 国家的便民服务查询（包括5A景区list，小微企业名录，法人信用查询，出租车信息查询） 国家宏观经济数据（GDP,CPI，总人口，社会消费品零售总额，粮食产品，PPI，各地区行政规划，各地财政收支等等，分月度季度和年度）部分数据如下 国家统计局（数据多到瞠目结舌，包括年度，季度，国家，国际，年鉴，介乎涵盖所有数据指标和历史）部分数据举例 世界银行的公开数据库（有健康，农业，公共部部门，人口分布，外债，性别，教育，环境，气候变化，能源，贫困等各种公开数据） 世界数据图册（世界和地区统计资料，各国数据，地图，排名）包含的全球的国家公开的数据 国家机关部委的公开数据（包括国家发展和改革委员会，教育部，民政部，司法部，财政部，工业和信息化不，交通运输部，文化和旅游部等） 各城市开放数据（包括浙江数据开放网，青岛数据开放网，贵阳数据开放平台，成都数据公开平台，合肥数据开放平台，河南开放数据平台等） 宏观经济查询数据（包括高校财经信息库，人民网经济数据库，香港统计处，联合国统计司，世界经合组织，欧盟统计局，国际货币基金组织等） 房价数据（包括中国房价指数，房价走势，台湾房价行情，北京房价查询，深圳楼盘成交查询等，上海地铁房价，贝壳指数等） 汽车数据（包括中国汽车工业协会数据，百度汽车网，易车汽车指数，汽车渠道直通车，中国汽车流通协会数据中心，德国汽车工业协会等） 权威发布 | 中华全国商业信息中心 3. 国内互联网公司数据报告网站列表 讲座PPT-腾讯大讲堂 Tencent 腾讯-业绩报告 腾讯大数据-腾讯云数据分析出来的行业报告 百度开放服务平台-百度云数据分析出来的行业报告 百度数据研究中心 提供行业研究报告、行业分析报告-百度数据中心报告 首页-阿里研究院-阿里行业研究报告 企鹅智酷_腾讯网-腾讯出品行业报告 腾讯CDC -腾讯交互设计报告 百度用户体验中心-百度UED用户研究报告 网易用户体验设计中心-网易UED用户研究报告 网络视频数据报告-优酷指数行业报告 PP指数_PPTV聚力-PPTV指数行业报告 360研究报告_360安全中心-360应用商店等产品出品报告 4. 国外咨询机构网站数据报告列表 国外咨询机构较多，数据详实，无论是海外出海产品，海外报告中多有亚洲和中国的重点研究，相关报告和趋势分析都可以选看 Flurry-国外app行业报告 App Annie Blog-app指数报告 https://www.appannie.com/insights/ (Appnnie的行业包括，包括app 分发行业的分发量和收入） BI Intelligence-business insider的报告 Today's Articles on Digital Marketing and Media-emarker的报告 http://www.newzoo.com/category/press-releases/-newzoo侧重于手游行业报告 Gartner Press Release Archives-gartner侧重于硬件的出货量，包括智能机和PC等 IDC - Search Results-IDC的硬件出货量全球报告 Yozzo Telecom News J.P. Morgan Home-摩根投行报告 德勤中国 | 审计, 企业管理咨询, 财务咨询, 风险管理, 税务服务及行业洞察 Precisely Everywhere-comscore的互联网行业报告 Ericsson - A world of communication（Global移动行业报告） GamesIndustry.biz（Global游戏行业报告） http://adfonic.com/（Global广告行业报告） Canalys | Insight. Innovation. Impact.（Global智能机报告） Mobile, Online &amp; Digital Market Research, Data &amp; Consultancy（通信无线报告） Home | GfK Global（终端比较专业的报告） Kantar Worldpanel（主要统计Android和ios的市场份额） PwC publications（皮尤的所有用户，市场研究报告） Fiksu | Data-fueled mobile marketing（统计app用户获取成本和应用商店下载频次的监测） https://www.weforum.org/reports（世界经济论坛的报告，揭示国内外发展的大趋势） Insights - Jampp （Jampp是国外的app 的粘性和转化漏洞的网站，在insights里还有行业的app的retention等benchmark的数据，有些类似flurry的行业数据） 罗兰贝格行业评论 战略和行业评论和报告 普华永道:blog 各个行业的主要发现和行业报告 Website Traffic &amp; Mobile App Analytics （similar web 以色列的网站分析工具，可以分析任何网站，包括用户，来源，终端，分布等等，数据非常棒） CADAS（全球航空公司研究报告）：非常支持和专业 印度互联网年报 - 竺帆 | 助力中企扬帆天竺 （印度出海报告，非常详细） GSMA： 全球移动互联网经济分析报告，全部免费下载报告和数据，从2015年到现在 商业价值研究院 -IBM（行业观点报告比较多） Home - McKinsey Greater China麦肯锡 **BCG - 波士顿咨询公司波士顿 企业管理咨询公司罗兰贝格 Accenture - China埃森哲** 5. 各大公司不定期发布的报告，比如（细分方向的时候用）** 高德地图：2015年度中国主要城市交通分析报告 微信城市服务发布《2015微信政务民生白皮书》 【报告】淘宝发布 2015 中国消费趋势数据，2015 年我们为什么买单？ 互联网增长的第一本数据分析手册-Growing IO的公开手册 移动游戏运营数据分析指标白皮书（一）-Talkingdata 运营指标分析白皮书 多多大学 （多多大学也分享了很多的拼多多运营数据还提供课程，可以看） 6. 找行业内的人事打听内部一手资料 关注一些专门打听行业内部人事的信息来源 这里先推荐一家公共号：晚点LatePost（微信搜索公共号可以 关注） 这家主要是会 看一些行业内部和重要的消息 在行上约人。在行 App 如果想知道一些企业的信息，可以在在行上找到一些行家，曾经一手经营或者运营过祥光额项目和参与过竞品和行业公司的操盘，可以约出来，从信息和方法论角度获得资讯 7. 企业信息报告** 新三板在线 - 中国最大的新三板生态平台（各行各业的新三板上市公司财务数据，高管数据等） 企查查|企业查询（查询企业的产品，品牌和法人信息） 企业注册信息查询（天眼查，同企查查） SEC.gov | Home（美国上市公司年度财务报告） 巨潮资讯网—（中国上市公司季度年度财务报告） Baidu | Investors（各大上市公司季度财报，IR.XX公司.com，比如百度这个） 天眼查（可以查到各个企业的详细信息，还可以查到员工个数） 8. 爬虫网站或者APP的数据 最近研究发现，还有一个好的行业信息获取来源，就是通过站内或者App内的爬虫抓取，这个渠道获取的数据，通常可以帮助你了解行业和竞品的站内使用情况，用户喜欢的内容，用户的分布，用户的行为和喜好等等。 爬虫，简单来说是通过程序来获取网页的信息，整理成数据库，从而进行数据挖掘的得到分析结论的过程。比如你可以爬虫购物的页面，知道哪个商品的销量好，比如你可以爬虫小红书的页面，知道哪些kol收到欢迎，你还可以爬取他们的分类，知道美妆和购物的kol表现好，并且有多少个这样的kol。如果你没有对方的数据库权限（当然你肯定没有），那么从外部爬虫是最好的了解他们业务数据的方式。 通常的搜索方式 是：你要了解的网站/App+爬虫，在搜索平台比如百度搜索 这里举例一些程序员垂直的网站， CSDN网站：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索 简书 - 创作你的创作：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索， V2EX：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索 掘金：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索 9 . 业内微信群 现在发现很多好的内部报告和难以获得报告，是通过加入一些干货群，内部群来获得的。 比如做直播电商的人自己比较关注一些直播和电商带货的详细的数据和报告趋势，大家会自己组建一些干货群，只要是市面上有的报告，自己内部发现的都会往里面扔。 这个是淘宝直播的负责人赵圆圆离开淘宝后创业，同时聚集的几个群，里面关于直播的干货非常多。 其他的关于投资的，趋势，创业的类似群也很多，获取报告也很一手，大家也可以自己开发下这样的群组织。 10. 搜索引擎 搜索引擎还是可以搜到很多你个性化想要找的报告和趋势。以前我没觉得搜索引擎很很难，后来发现也需要学习和熟练使用，才能让其为自己所用。 如何使用搜索引擎 11. 各大公司的财报 通常对于上市公司来说，财报信息包含的内容是最全面的，关于用户，商业，渠道，增长，业务策略等等。所以如果想了解一个公司，如果是上市公司最好第一手先看财报后者SEC（上市报告）。 很多同学问我财报哪里找，不知道怎么看。其实每个公司都有自己的IR（投资者页面），在上面有财报的完整的pdf下载。另外，也推荐大家听听每期的企业conference call（回答财报问题），可以听下CEO对财报的解读。 这里我列举几个大公司财报的网站 wind：金融数据库，包含财报和行业信息（wind的账号可以到闲鱼租） 百度财报 PDF：Baidu | Investors | Home 百度财报解读podcast：音频可以在线听 阿里财报pdf：阿里巴巴集团 腾讯财报pdf：Tencent 腾讯 - 投资者关系 搜狐财报pdf：http://investors.sohu.com/ 拼多多pdf：Investor Relations | Pinduoduo Inc. 拼多多财报解读：音频可以在线听。 如果大家有自己想要了解的公司，在百度or google搜索：公司名字+IR ，可以 定位到他们公司的财报网站页面。在页面上找到conference call或者webcast，可以 找到他们的财报解读音频。 12. 投资机构的统计网站（创业方向选择，投融资选择的时候用） IT桔子 | IT互联网公司产品数据库及商业信息服务（IT桔子，中国创业公司投融资数据和报告） 研究院_ChinaVenture投资中国网-（投中的每个季度的行业融资报告，不定期有专项分析报告） CB Insights - Blog （CBI insights的一系列产品，包括公司的估值，独角兽公司列表等） The Downround Tracker（公司估值下降的趋势） The Complete List of Unicorn Companies（独角兽公司列表） IPO Center: IPO Market, IPO News, IPO Calendars, IPO Pricings, IPO Voting（IPO相关新闻和趋势报告） PrivCo | Private Company Financial Intelligence（美国金融数据公司，主要关注未上市公司的所有投融资资料，目前涵盖的公司包括全世界，当然也包括中国公司） 券商行业研究报告 （国内券商的行业报告，策略报告，可以筛选行业，筛选报告类型） https://pitchbook.com/news/reports（PitchBook的PE,VC，M&amp;A行业报告） 研究院_ChinaVenture投资中国网 （IPO 投融资行业报告） Dow Jones VentureSource 2Q’16 U.S. Venture Capital Report（道琼斯旗下机构Dow Jones LP Source行业投资报告） NVCA Venture Investment（美国国家风险投资协会，每个季度和年度都会出投融资行业报告） PWC-MoneyTree Home（PWC的money tree report是每个季度美国的风险投资行业报告） https://home.kpmg.com/xx/en/home/insights.html （KPMG毕马威的insights报告，一般是每个季度的创投趋势，比较细致的分析） Mattermark - Discover, Enrich, &amp; Analyze Companies（创业公司投资并购信息一站式搜索） M&amp;A, Private Equity &amp; Venture Capital Database（创业公司投资并购信息一站式搜索） DataFox | Prospect Sales Leads with Company Signals（创业公司投资并购信息一站式搜索） CrunchBase accelerates innovation by bringing together data on companies and the people behind them.（创业公司数据库） Venture Intelligence PE/VC database Stock Market Insights | Seeking Alpha （二级市场金融分析网站） Tencent Holdings Ltd -- Trefis（各个公司的revenue model的预测和key driver的趋势，这个网站简直不能再棒） 13. 本地数据库 这个世界有很多有用的信息，搜索引擎只解决了其中20%，其他80%的信息再各个角落，包括微信群，包括口口，甚至包括直播里都有，但是都不在搜索引擎。 就搜索引擎而言，现在很多人只是使用了其中的5%还不到。搜索引擎的技巧可以提升，但是其他80%的信息获取渠道更为隐蔽和无法公开获得的。 我加了很多 群，里面都是这些报告和信息和各行各业的各种信息，这些是搜索引擎提供不了的 这些冰山下的信息才决定了信息的获取的不同和优质与否。 除了上述渠道外，能找到靠谱渠道，找到合适的报告随时存储起来，等用的时候随手打开用是最好的。分享一个我最近看的收藏的精品的报告收藏夹，也希望对你们有用（随时更新） 共享下我看过的精品报告的收藏夹list 14. 怎么提炼自己获取信息的层次和获取信息的价值 找到行业信息报告知识获取信息只是其中一个层次 ，获取信息是否更有价值更直接可用，在于基本功行业信息报告的甄别和获取，积累和提炼，这是非常重要的。 但是 越往上走，越是接近信息更有价值，更新鲜，更真实有效，更直接，有大量的渠道 可以 获得更多 的信息，这些不仅是通过 行业报告获取的，还有包括自己可以控制的方法，包括爬虫，数据挖掘，信息技术 等，还包括人脉，圈子，内幕的等渠道。大家感兴趣可以到这个答案看下详情，我对每个层级的方法的解读。 哪些渠道可以获取一般人不知道的知识和信息 15. 其他（不定期更新） IBM商业价值研究院","link":"/Analysis_data_and_research_report_collection/"},{"title":"Apple M1的AI环境搭建","text":"本文环境搭建的基础是Python3.9， 因为M1为ARM架构，所以放弃了Anaconda，使用Miniforge3。包括Tensorflow, xgboost, Lightgbm, Numpy, Pandas, Matplotlib, NGBoost等。当然，因为是Python3.9， 所以有些库实在是无法使用。 Homebrew 作为Mac的包管理神器，首先当然要先从Homebrew开始。Homebrew已经支持了ARM架构，可以直接进行安装，当然，如果你电脑里以前存在X86的brew支持，请先卸载干净。 Homebrew 卸载 1/bin/bash -c &quot;$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/uninstall.sh)&quot; Install ARM Homebrew 1/bin/bash -c &quot;$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/install.sh)&quot; 执行完毕后，Homebrew安装在/opt/homebrew路径下；在安装完毕后，命令行后会提示执行命令设置环境变量，当然，以防万一，这里也提供一下： 12echo 'eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;' &gt;&gt; ~/.zprofileeval &quot;$(/opt/homebrew/bin/brew shellenv)&quot; 如果是bash shell， 则： 12echo 'eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;' &gt;&gt; ~/.bash_profileeval &quot;$(/opt/homebrew/bin/brew shellenv)&quot; 记得source ~/.zprofile Install X86 Homebrew 1arch -x86_64 /bin/bash -c &quot;$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/install.sh)&quot; X86版本的安装执行完成后命令行未提示添加环境变量。 alias 支持多版本 在终端执行： 12alias brew='arch -arm64 /opt/homebrew/bin/brew'alias ibrew='arch -x86_64 /usr/local/bin/brew' 这里可以看出两者路径区别 设置镜像 中科大源 1234567891011# brewgit -C &quot;$(brew --repo)&quot; remote set-url origin https://mirrors.ustc.edu.cn/brew.git# coregit -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git# caskgit -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.gitbrew update 清华大学源 1234567891011# brewgit -C &quot;$(brew --repo)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git# coregit -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git# caskgit -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-cask.gitbrew update 恢复默认源 1234567891011# brewgit -C &quot;$(brew --repo)&quot; remote set-url origin https://github.com/Homebrew/brew.git# coregit -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://github.com/Homebrew/homebrew-core.git# caskgit -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://github.com/Homebrew/homebrew-cask.gitbrew update 更多源 Homebrew 其他相关 设置bottles镜像 1234567# bottles for zshecho 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles/bottles' &gt;&gt; ~/.zprofilesource ~/.zprofile# bottles bashecho 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles/bottles' &gt;&gt; ~/.bash_profilesource ~/.bash_profile cask 目前cask是从GitHub上读取软件源，而GitHub Api对访问有限制，如果使用比较频繁的话，可以申请Api Token，然后在环境变量中配置到HOMEBREW_GITHUB_API_TOKEN。 12echo 'export HOMEBREW_GITHUB_API_TOKEN=yourtoken' &gt;&gt; ~/.zprofilesource ~/.zprofile Install Miniforge3 首先需要下载安装包： Download 请下载arm64(Apple Silicon)版本： 下载完成后进入到文件目录，比如我是在~/Download/内，执行： 1bash Miniforge3-MacOSX-arm64.sh 整个执行过程会有大概三次填写yes并回车确定，最后一次会询问你是否执行conda init， 会自动在~/.zshrc内添加环境变量，如果未执行的，可以将下面语句加入文件末尾： 12345678910111213141516# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;# !! Contents within this block are managed by 'conda init' !!__conda_setup=&quot;$('/Users/xx/miniforge3/bin/conda' 'shell.zsh' 'hook' 2&gt; /dev/null)&quot;if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot;else if [ -f &quot;/Users/xx/miniforge3/etc/profile.d/conda.sh&quot; ]; then . &quot;/Users/xx/miniforge3/etc/profile.d/conda.sh&quot; else export PATH=&quot;/Users/xx/miniforge3/bin:$PATH&quot; fifiunset __conda_setupconda activate tf# &lt;&lt;&lt; conda initialize &lt;&lt;&lt; 记得自行更改/Users/xx/内的用户名 等待Miniforge3安装完成，然后设置一个专供学习Tensorflow的虚拟环境 12conda create -n tf python=3.9.5conda activate tf # 将这句添加到~/.zshrc内，每次打开shell都会自动执行 关于conda切换环境的命令，建议自行Google学习一下，很有用。 Install Tensorflow 目前网上流传的Tensorflow安装基本是两个版本，一个是安装一大堆的支持和依赖，一个是使用yml文件提前准备好环境库一键完成环境创建，比如environment.yml： 1conda env create --file=environment.yml --name=tf 其实这一步也很简单，Apple为了大力推广自家的ARM，已经为大家做好了这部分准备，我们只需要安装就行了。 假设目前在tf环境内 123conda install -c apple tensorflow-depspython -m pip install tensorflow-macospython -m pip install tensorflow-metal 好了，结束！ 可以自行利用下面一段代码测试下： 123456789101112from tensorflow.keras import layersfrom tensorflow.keras import modelsmodel = models.Sequential()model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.Flatten())model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(10, activation='softmax'))model.summary() 123456789101112131415from tensorflow.keras.datasets import mnistfrom tensorflow.keras.utils import to_categorical(train_images, train_labels), (test_images, test_labels) = mnist.load_data()train_images = train_images.reshape((60000, 28, 28, 1))train_images = train_images.astype('float32') / 255test_images = test_images.reshape((10000, 28, 28, 1))test_images = test_images.astype('float32') / 255train_labels = to_categorical(train_labels)test_labels = to_categorical(test_labels)model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])model.fit(train_images, train_labels, epochs=5, batch_size=64)test_loss, test_acc = model.evaluate(test_images, test_labels)test_acc 执行过程中可以在资源管理器中看到GPU的占用： 其他 Lightgbm 1conda install Loghtgbm 一句代码解决，完全靠谱。 xgboost xgboost稍微有点麻烦，我测试了最稳妥的安装方式，还是自行编译，那这个时候我们就需要用到brew安装并设置编译环境了： 注意，我用的都是brew而非ibrew, 目前都是在ARM环境下完成操作。 123brew install gccbrew install cmakebrew install libomp 然后下载源码并执行 1234567git clone git@github.com:dmlc/xgboost.gitcd xgboostmkdir buildcd buildCC=gcc-11 CXX=g++-11 cmake ..cd ../python-package/Users/xx/miniforge3/envs/tf/bin/python setup.py install 然后就OK了。 至于其他的，Numpy在安装Tensorflow的时候就自动作为依赖安装了，Pandas, Matplotlib, NGBoost等，执行下方： 123conda install -c conda-forge pandasconda install -c conda-forge matplotlibconda install -c conda-forge ngboost 如果conda内实在没有的，再试试pip安装，再不行，就只能自行下载源码编译了。 目前在当前环境下解决不了的几个库： CatBoost Cairo -&gt; Pycairo GraphEmbedding CV2 igraph 在整个过程中，可能会遇到各种各样的问题，大家要习惯于使用Google和查阅官方文档； 参考 Tensoflow-macos Run xgboost on Mac and Regression data Accelerating TensorFlow Performance on Mac The new Apple M1 chips have accelerated TensorFlow support M1 Mac Mini Scores Higher Than My RTX 2080Ti in TensorFlow Speed Test. GPU acceleration for Apple's M1 chip? M1芯片Mac上Homebrew安装教程 Mac mini M1使用简单体验(编程、游戏、深度学习) Installing TensorFlow 2.4 on MacOS 11.0 without CUDA for both Intel and M1 based Macs 在 M1 芯片 Mac 上使用 Homebrew Apple M1终于让MacBook变的可以炼丹了 Install XGBoost and LightGBM on Apple M1 Macs Installing TensorFlow on the M1 Mac Getting Started with tensorflow-metal PluggableDevice M1芯片mac安装xgboost和lightgbm AI - Apple Silicon Mac M1 机器学习环境 (TensorFlow, JupyterLab, VSCode) M1芯片安装tensorflow 使用MacBook pro M1搭建基于ML Compute加速的TensorFlow深度学习环境 你的Mac有了专用版TensorFlow，GPU可用于训练，速度最高提升7倍 在M1的Mac上安装Tensorflow（避坑版） 在M1芯片Mac上搭建原生适配Python环境 Conda-forge Miniforge M1 mac安装PyTorch的完整步骤指南 macOS M1(AppleSilicon) 安装TensorFlow环境 傻瓜版M1配置Tensorflow-超简单近乎一键完成 environment.yml opencv-python MAC安装Opencv以及Dlib碰到的一些问题 Jupiter Widgets 启动SparkContext报错 MacBook Pro 2020 M1芯片安装xgboost xgboost Homebrew / Linuxbrew 镜像使用帮助 镜像助手 Apple Silicon Mac 安装xgboost M1芯片mac安装xgboost和lightgbm mac安装lightgbm踩坑心得，亲测有效！ MAC 上 使用lightgbm遇到image not found 解决办法总结 杂记-Macbook Pro M1芯片能玩深度学习吗？","link":"/Apple_M1_AI_environment_construction/"},{"title":"07 AI帮你做总结","text":"Hi， 我是茶桁。 在上一节中，我们介绍了如何使用最新的ChatGPT API，注册HuggingFace账户，并将我们的聊天机器人部署出去。在这个过程中，我们学习了实际的应用开发过程，使你对聊天机器人的开发有了充足的体验。在这一讲中，我们将探讨OpenAI的各种接口提供的能力，以更深入地了解这些接口。我们将分别介绍如何利用嵌入（Embedding）进行文本聚类，并使用提示语（Prompt）对文本进行总结。此外，我们还将介绍其他的接口能力，如语言模型和自然语言生成，以帮助您更好地理解和利用OpenAI的各种功能。 基于 Embedding 向量进行文本聚类文本聚类简介 文本聚类是一种自动将未标注的文本根据相似度分成几类的方法。使用 GPT 系列模型进行文本聚类非常简单，我们可以将文本转换为向量，然后使用一些简单的聚类算法，比如最简单的 K-Means 算法。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/AI-can-help-you-summarize-your-content/"},{"title":"人工神经网络","text":"神经元、如何构建网络、高级神经网络 人工神经网络是人工智能（AI）中重要而有趣的一部分。 什么是神经网络？ 神经网络是对大脑神经过程的复制。 它是在计算机上构建的大脑模拟。 神经网络，无论是生物的还是人工的，都由大量的简单单元和神经元组成，它们相互接收和传输信号。 它由细胞体和连接神经元的导线组成。 用生物学语言来说 ： 为神经元提供输入的电线称为树突。 在某些情况下，神经元会向另一个神经元发送信号，这些向外发送信号的导线被称为轴突。 轴突可能与一个或多个树突相连，这种交叉点称为突触。 这个过程会随着我们的成长而不断调整，这种 \"调整 \"被称为记忆或学习。 什么是深度学习？ 深度学习是一种机器学习技术，由相互连接的多层简单处理单元组成。 它的灵感来源于大脑处理视觉信息的方式。 为什么要开发人工神经网络？ 开发人工神经网络（ANN）的原因之一是为了帮助神经科学（研究大脑和神经系统）。 人们相信，通过绘制人脑图谱，我们可以了解意识和智力背后的秘密。 我们已经能够识别异常功能，并帮助大脑避免异常功能。 例如--解决老年痴呆症、因受伤造成的损伤和发育障碍。 开发人工神经网络（ANN）的另一个原因是为了建立更好的人工智能和机器学习技术。 因为，大脑是一个极其复杂的信息处理系统。 人工神经网络的特点 ： ANN 由许多神经元组成，可以同时处理信息。这意味着，我们可以同时处理大量数据，从而提高了效率。 神经元可以同时存储（就像内存一样）和处理信息，因此从存储器中检索数据不会有任何延迟，因而速度很快。 是的，ANN 可以快速处理信息，但很难应用于 \"传统计算机\"（单机处理），因为它一次只能完成一项任务。这就是 GPU 的用武之地。 听说过 GPU 吗？ GPU 是图形处理单元（Graphical Processing Unit）的缩写，它可以进行并行处理，而不是像传统计算机那样进行单一处理。因此，神经网络可以快速完成工作或处理信息。 构建神经网络 ： 权重在神经网络中扮演着重要角色，它通过控制每个输入，让网络从这些数据中学习，从而做出准确的预测。 但是，什么是权重？ 权重就像可调节的旋钮，决定着每个输入对最终输出的影响程度。 例如，为了找到适当的平衡（数据），我们要给输入值加上适当的权重。 因此，通过将每个输入值（神经元）与权重相乘并相加，我们就能实现 \"线性组合\"。 线性组合公式 ： 考虑到我们有 4 个输入，因此我们也需要 4 个权重来平衡它，而且还会有一个额外的固定值，称为截距（偏差）。 截距值是一个偏置值，用作基准值，这样即使输入值为零，网络也能做出预测。 计算公式： 线性组合 = [截距 + Weight 1 × Input 1 + Weight 2 × Input 2 + Weight 3 × Input 3 + Weight 4 × Input 4］ 问：请考虑以下表达式 10.0 + 5.4 × 8 + (-10.2) × 5 + (-0.1) × 22 + 101.4 × (-5) + 0.0 × 2 + 12.0 × (-3) = -543.0 (i) 表达式中的截距（偏差）项是什么？ \"10.0 \"是截距（偏差）数，因为它没有乘以任何变量。 (ii) 这里的输入是什么？ \"8,5,22,-5,2,-3 \"是输入值，因为它是乘法中的第二个数字。 实现线性组合后，再将其传递给 \"激活函数\"。 激活功能： 激活函数就像一个开关，它决定信号是否应该通过，使神经网络能够有效地学习和解决不同的问题。 为图像识别、自然语言处理等进行预测。 激活函数示例 ： 激活函数的一些示例如下 识别函数：什么也不做，只输出线性组合（与线性回归相同，不提供任何新信息，因此很少使用） 步进功能：如果线性组合值大于 0，则通过信号，否则什么也不做 Sigmoid 函数：阶跃函数的 \"软 \"版本 通过线性组合激活函数实现的神经元输出用于预测或决策。 \"感知器--人工神经网络（ANN）之母\"： Perceptron 是一种使用阶跃激活函数的简单神经元模型。 它被用作二元分类任务中的简单分类器。 由于它是第一个正式的神经网络模型，因此被称为 \"ANN 之母\"。 \"现在，让我们回到神经网络的构建上来。 网络架构由层级组成，例如 ： 输入层：由作为输入数据的神经元组成。例如，用于图像识别的图像像素值。 隐藏层：接收输入层的输出，并将自己的输出传递给下一层。 输出层：产生网络的最终结果。例如，用于人脸识别的人的概率值。 为了在这些层中进行线性组合，我们应该能够找到合适的权重。 反向传播 - 找到合适的权重 ： 在过去（20 世纪 80 年代之前），人们曾使用过感知器算法，但寻找权重需要花费大量时间。 因此，人们引入了反向传播算法。它通过层层递进和递退来确定合适的权重，从而做出准确的预测。 现在，让我们举个例子，来识别图像。 建立分类器，对图像显示的是 \"X \"还是 \"O \"进行分类 这里是一个 5 × 5 的网格，因此每幅图像由 25 个像素组成。阴影像素为 1，其他空白像素为 0。 现在，我们应该应用权重，其中在中心位置，权重假设为-1，而在近中心像素位置，权重假设为 1： 因此，在这里，如果线性组合为负数，即激活度为零，则为 \"X\"；如果为正数，则为 \"O\"。 对第一幅图像进行线性组合 ： (忽略 0 值权重，得到） -&gt; 1 × -1 = -1 因此，我们得到 \"X” 对第二幅图像进行线性组合： (忽略 0 值权重，我们得到） -&gt; 1 × 1 + 1 × 1 + 1 × 1 + 1 × 1 = 4 因此我们得到 \"O\" 到目前为止，我们已经了解了 -&gt; 多层网络（超过一层的神经网络）、非线性函数（阶跃激活函数和 Sigmoid 激活函数）、学习规则（如反向传播）。 让我们进入高级神经网络。 卷积神经网络 ： 使用感知器或线性回归可以进行图像处理，但由于需要大量权值，而且无法有效检测图像特征，因此效果和效率都不高。 因此，为了解决这些局限性，人们引入了卷积神经网络。 CNN 或卷积神经网络由卷积层组成，可以自动学习和提取图像特征，如颜色、图案、边缘等。 例如，CNN 可用于动物检测、标志检测等。 如果我们想使用传统方法检测图像或识别图像，它将使用图像中的像素位置来检测物体。因此，我们必须有一张相似的图像才能做到这一点，但对于卷积神经网络来说，这并非必要。 例如，我们有一张位于图像中心的停车标志的训练图像，然后我们会得到一张测试图像，该图像的右上角有一个停车标志。由于训练图像和测试图像的像素值和位置不同，因此无法使用感知器进行检测。不过，通过使用卷积神经网络，它可以成功检测出图像中任何位置的停车标志。 生成式人工智能（Generative AI）： 生成式人工智能是人工智能的一种，可以生成文本、图像、音频和合成数据等各种类型的内容。 它可以是.....： 监督学习法 无监督学习法 半监督学习法 判别模型用于通过标注数据的训练进行分类或预测。 生成模型用于生成新数据，如预测序列中的下一个单词。 生成对抗网络（GAN）： 其原理是让两个神经网络相互竞争。 一个网络将生成与训练数据类似的图像。 另一个网络将对生成的图像和训练图像进行分类。 这样做是为了生成逼真的图像。 上述图像由英伟达公司开发的 GAN 生成。 将人工智能应用于现实问题比解决谜题和游戏更具挑战性。在现实世界的场景中，可能出现的状态数量之多令人目不暇接，使得穷举式搜索或巧妙的启发式方法无法奏效。此外，由于我们无法控制的因素，行动的结果并不总是可以预测的，这就引入了随机性。为了解决这些复杂问题，我们需要将不确定性和概率的概念纳入算法，同时利用先进的神经网络，使我们能够有效地解决现实世界中的人工智能问题。 康康康康恐龙康。最后，你已经掌握了基本的神经网络和高级神经网络的基本知识。","link":"/Artificial-Neural-Network/"},{"title":"Finish the search problem","text":"The code address of this article is: example_01_Assignment Please read the answer below after thinking for yourself Please using the search policy to implement an agent. This agent receives two input, one is @param start station and the other is @param destination. Your agent should give the optimal route based on Beijing Subway system. Dataflow: 1. Get data from web page. Get web page source from: https://baike.baidu.com/item/%E5%8C%97%E4%BA%AC%E5%9C%B0%E9%93%81/408485 You may need @package requests https://2.python-requests.org/en/master/ page to get the response via url You may need save the page source to file system. The target of this step is to get station information of all the subway lines; You may need install @package beautiful soup https://www.crummy.com/software/BeautifulSoup/bs4/doc/ to get the url information, or just use &gt; Regular Expression to get the url. Our recommendation is that using the Regular Expression and BeautiflSoup both. You may need BFS to get all the related page url from one url. Question: Why do we use BFS to traverse web page (or someone said, build a web spider)? Can DFS do this job? which is better? 2. Preprocessing data from page source. Based on the page source gotten from url. You may need some more preprocessing of the page. the Regular Expression you may need to process the text information. You may need @package networkx, @package matplotlib to visualize data. You should build a dictionary or graph which could represent the connection information of Beijing subway routes. You may need the defaultdict, set data structures to implement this procedure. 3. Build the search agent Build the search agent based on the graph we build. for example, when you run: 1&gt;&gt;&gt; search('奥体中心', '天安门') you need get the result: 奥体中心-&gt; A -&gt; B -&gt; C -&gt; ... -&gt; 天安门 HTTP协议 超文本传输协议（HTTP，HyperText Transfer Protocol）是互联网上应用最为广泛的一种网络协议。所有的www文件都必须遵守这个标准。 HTTP用于客户端和服务器之间的通信。协议中规定了客户端应该按照什么格式给服务器发送请求，同时也约定了服务端返回的响应结果应该是什么格式。 请求访问文本或图像等信息的一端称为客户端，而提供信息响应的一端称为服务器端。 客户端告诉服务器请求访问信息的方法： - Get 获得内容 - Post 提交表单来爬取需要登录才能获得数据的网站 - put 传输文件 更多参考： HTTP请求状态 了解200 404 503 - 200 OK //客户端请求成功 - 404 Not Found //请求资源不存在，eg：输入了错误的URL - 503 Server Unavailable //服务器当前不能处理客户端的请求，一段时间后可能恢复正常。 #### Requests 纯粹HTML格式的网页通常被称为静态网页，静态网页的数据比较容易获取。 在静态网页抓取中，有一个强大的Requests库能够让你轻易地发送HTTP请求。 在终端上安装 Requests pip install requents 123456789101112# 获取响应内容import requests# get（输入你想要抓去的网页地址）r = requests.get('https://www.baidu.com/')print('文本编码：（服务器使用的文本编码）', r.encoding)print('响应状态码：（200表示成功）', r.status_code)print('字符串方式的响应体：（服务器响应的内容）', r.text) 拓展知识： Unicode和UTF-8有什么区别?(盛世唐朝回答) 正则表达式 正则表达式的思想是你在人群中寻找你的男朋友/女朋友，他/她在你心里非常有特点。 同样，从一堆文本中找到需要的内容，我们可以采用正则表达式。 正经点说，是以一定的模式来进行字符串的匹配。 掌握正则表达式需要非常多的时间，我们可以先入门，在以后的工作中遇到，可更加深入研究。 使用正则表达式有如下步骤： 寻找【待找到的信息】特点 使用符号找到特点 获得信息 12345678910111213141516171819202122232425262728293031323334353637383940# 请先运行一下、看一下有什么参数？# 请思考，找到会返回什么？没找到会返回什么？import rehelp(re.match)# 请运行之后、思考 match 与 search 的区别?m = re.search('foo', 'seafood')print(m)print(m.group())print('-------------------------')m = re.match('foo', 'seafood')print(m)#### `search`是搜索字符串中首次出现的位置# 匹配多个字符串 |m = re.match('bat|bet|bit', 'bat')print(m.group()) if m is not None else print('None')# 匹配任意单个字符 .m = re.match('.end', 'kend')print(m.group()) if m is not None else print('None')m = re.match('.end', 'end')print(m.group()) if m is not None else print('None')# 字符串集合 []m = re.match('[cr][23][dp][o2]', 'c3p2')print(m.group()) if m is not None else print('None')# [] 与 |是不同的m = re.match('c3po|r2d2', 'c3p2')print(m.group()) if m is not None else print('None') 给大家提供一个字典，供大家查询～ 字符 描述 &lt;/th&gt; 将下一个字符标记为一个特殊字符、或一个原义字符、或一个向后引用、或一个八进制转义符。例如，“n”匹配字符“n”。“”匹配一个换行符。串行“\\”匹配“&lt;/code&gt;”而“(”则匹配“(”。 &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;^&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配“&lt;code&gt;\\n&lt;/code&gt;”或“&lt;code&gt;\\r&lt;/code&gt;”之后的位置。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt; \\* &lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配前面的子表达式零次或多次。例如，zo\\*能匹配“&lt;code&gt;z&lt;/code&gt;”以及“&lt;code&gt;zoo&lt;/code&gt;”。\\* 等价于{0,}。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;+&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配前面的子表达式一次或多次。例如，“&lt;code&gt;zo+&lt;/code&gt;”能匹配“&lt;code&gt;zo&lt;/code&gt;”以及“&lt;code&gt;zoo&lt;/code&gt;”，但不能匹配“&lt;code&gt;z&lt;/code&gt;”。+等价于{1,}。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;?&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配前面的子表达式零次或一次。例如，“&lt;code&gt;do(es)?&lt;/code&gt;”可以匹配“&lt;code&gt;does&lt;/code&gt;”或“&lt;code&gt;does&lt;/code&gt;”中的“&lt;code&gt;do&lt;/code&gt;”。?等价于{0,1}。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;}&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;是一个非负整数。匹配确定的&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;次。例如，“&lt;code&gt;o{2}&lt;/code&gt;”不能匹配“&lt;code&gt;Bob&lt;/code&gt;”中的“&lt;code&gt;o&lt;/code&gt;”，但是能匹配“&lt;code&gt;food&lt;/code&gt;”中的两个o。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,}&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;是一个非负整数。至少匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;次。例如，“&lt;code&gt;o{2,}&lt;/code&gt;”不能匹配“&lt;code&gt;Bob&lt;/code&gt;”中的“&lt;code&gt;o&lt;/code&gt;”，但能匹配“&lt;code&gt;foooood&lt;/code&gt;”中的所有o。“&lt;code&gt;o{1,}&lt;/code&gt;”等价于“&lt;code&gt;o+&lt;/code&gt;”。“&lt;code&gt;o{0,}&lt;/code&gt;”则等价于“&lt;code&gt;o*&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;}&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;和&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;均为非负整数，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&amp;lt;=&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;。最少匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;次且最多匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;次。例如，“&lt;code&gt;o{1,3}&lt;/code&gt;”将匹配“&lt;code&gt;fooooood&lt;/code&gt;”中的前三个o。“&lt;code&gt;o{0,1}&lt;/code&gt;”等价于“&lt;code&gt;o?&lt;/code&gt;”。请注意在逗号和两个数之间不能有空格。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;?&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;当该字符紧跟在任何一个其他限制符（*,+,?，{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;}，{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,}，{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“&lt;code&gt;oooo&lt;/code&gt;”，“&lt;code&gt;o+?&lt;/code&gt;”将匹配单个“&lt;code&gt;o&lt;/code&gt;”，而“&lt;code&gt;o+&lt;/code&gt;”将匹配所有“&lt;code&gt;o&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;.&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配除“&lt;code&gt;\\&lt;/code&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;&lt;code&gt;n&lt;/code&gt;&lt;/span&gt;”之外的任何单个字符。要匹配包括“&lt;code&gt;\\&lt;/code&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;&lt;code&gt;n&lt;/code&gt;&lt;/span&gt;”在内的任何字符，请使用像“&lt;code&gt;(.|\\n)&lt;/code&gt;”的模式。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配pattern并获取这一匹配。所获取的匹配可以从产生的Matches集合得到，在VBScript中使用SubMatches集合，在JScript中则使用$0…$9属性。要匹配圆括号字符，请使用“&lt;code&gt;\\(&lt;/code&gt;”或“&lt;code&gt;\\)&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?:pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配pattern但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用或字符“&lt;code&gt;(|)&lt;/code&gt;”来组合一个模式的各个部分是很有用。例如“&lt;code&gt;industr(?:y|ies)&lt;/code&gt;”就是一个比“&lt;code&gt;industry|industries&lt;/code&gt;”更简略的表达式。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?=pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，“&lt;code&gt;Windows(?=95|98|NT|2000)&lt;/code&gt;”能匹配“&lt;code&gt;Windows2000&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;Windows3.1&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?!pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如“&lt;code&gt;Windows(?!95|98|NT|2000)&lt;/code&gt;”能匹配“&lt;code&gt;Windows3.1&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;Windows2000&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?&amp;lt;=pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;反向肯定预查，与正向肯定预查类拟，只是方向相反。例如，“&lt;code&gt;(?&amp;lt;=95|98|NT|2000)Windows&lt;/code&gt;”能匹配“&lt;code&gt;2000Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;3.1Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?&amp;lt;!pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;反向否定预查，与正向否定预查类拟，只是方向相反。例如“&lt;code&gt;(?&amp;lt;!95|98|NT|2000)Windows&lt;/code&gt;”能匹配“&lt;code&gt;3.1Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;2000Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;x|y&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配x或y。例如，“&lt;code&gt;z|food&lt;/code&gt;”能匹配“&lt;code&gt;z&lt;/code&gt;”或“&lt;code&gt;food&lt;/code&gt;”。“&lt;code&gt;(z|f)ood&lt;/code&gt;”则匹配“&lt;code&gt;zood&lt;/code&gt;”或“&lt;code&gt;food&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[xyz]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;字符集合。匹配所包含的任意一个字符。例如，“&lt;code&gt;[abc]&lt;/code&gt;”可以匹配“&lt;code&gt;plain&lt;/code&gt;”中的“&lt;code&gt;a&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[^xyz]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;负值字符集合。匹配未包含的任意字符。例如，“&lt;code&gt;[^abc]&lt;/code&gt;”可以匹配“&lt;code&gt;plain&lt;/code&gt;”中的“&lt;code&gt;p&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[a-z]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;字符范围。匹配指定范围内的任意字符。例如，“&lt;code&gt;[a-z]&lt;/code&gt;”可以匹配“&lt;code&gt;a&lt;/code&gt;”到“&lt;code&gt;z&lt;/code&gt;”范围内的任意小写字母字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[^a-z]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;负值字符范围。匹配任何不在指定范围内的任意字符。例如，“&lt;code&gt;[^a-z]&lt;/code&gt;”可以匹配任何不在“&lt;code&gt;a&lt;/code&gt;”到“&lt;code&gt;z&lt;/code&gt;”范围内的任意字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\b&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个单词边界，也就是指单词和空格间的位置。例如，“&lt;code&gt;er\\b&lt;/code&gt;”可以匹配“&lt;code&gt;never&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”，但不能匹配“&lt;code&gt;verb&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\B&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配非单词边界。“&lt;code&gt;er\\B&lt;/code&gt;”能匹配“&lt;code&gt;verb&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”，但不能匹配“&lt;code&gt;never&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\cx&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配由x指明的控制字符。例如，\\cM匹配一个Control-M或回车符。x的值必须为A-Z或a-z之一。否则，将c视为一个原义的“&lt;code&gt;c&lt;/code&gt;”字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\d&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个数字字符。等价于[0-9]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\D&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个非数字字符。等价于[^0-9]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\f&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个换页符。等价于\\x0c和\\cL。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\n&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个换行符。等价于\\x0a和\\cJ。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\r&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个回车符。等价于\\x0d和\\cM。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\s&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配任何空白字符，包括空格、制表符、换页符等等。等价于[ \\f\\n\\r\\t\\v]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\S&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配任何非空白字符。等价于[^ \\f\\n\\r\\t\\v]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\t&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个制表符。等价于\\x09和\\cI。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\v&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个垂直制表符。等价于\\x0b和\\cK。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\w&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配包括下划线的任何单词字符。等价于“&lt;code&gt;[A-Za-z0-9_]&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\W&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配任何非单词字符。等价于“&lt;code&gt;[^A-Za-z0-9_]&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\x&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，“&lt;code&gt;\\x41&lt;/code&gt;”匹配“&lt;code&gt;A&lt;/code&gt;”。“&lt;code&gt;\\x041&lt;/code&gt;”则等价于“&lt;code&gt;\\x04&amp;amp;1&lt;/code&gt;”。正则表达式中可以使用ASCII编码。.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;num&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;num&lt;/span&gt;，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;num&lt;/span&gt;是一个正整数。对所获取的匹配的引用。例如，“&lt;code&gt;(.)\\1&lt;/code&gt;”匹配两个连续的相同字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;标识一个八进制转义值或一个向后引用。如果\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;之前至少&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;个获取的子表达式，则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为向后引用。否则，如果&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为八进制数字（0-7），则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为一个八进制转义值。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;标识一个八进制转义值或一个向后引用。如果\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;之前至少有&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;个获得子表达式，则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;为向后引用。如果\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;之前至少有&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;个获取，则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为一个后跟文字&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;的向后引用。如果前面的条件都不满足，若&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;和&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;均为八进制数字（0-7），则\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;将匹配八进制转义值&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nml&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;如果&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为八进制数字（0-3），且&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m和l&lt;/span&gt;均为八进制数字（0-7），则匹配八进制转义值&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;l。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\u&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;是一个用四个十六进制数字表示的Unicode字符。例如，\\u00A9匹配版权符号（©）。&lt;/td&gt; &lt;/tr&gt; 12345678910111213# 匹配电子邮件地址patt = '\\w+@(\\w+\\.)?\\w+\\.com'm = re.match(patt, 'nobody@xxx.com')print(m.group()) if m is not None else print('None')# 匹配QQm = re.search('[1-9][0-9]{4,}', '这是我的QQ号781504542,第二个qq号：10054422288')print(m.group()) if m is not None else print('None')# findall() 是search的升级版，可以找到所有匹配的字符串m = re.findall('[1-9][0-9]{4,}', '这是我的QQ号781504542,第二个qq号：10054422288')print(m) if m is not None else print('None') 了解了怎么使用，下面进入实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# get the data (subway for beijing ,from amap)# 你需要用到以下的包import requestsimport reimport numpy as npr = requests.get('http://map.amap.com/service/subway?_1469083453978&amp;srhdata=1100_drw_beijing.json')r.textdef get_lines_stations_info(text): # Please write your code here pass # Traverse the text format data to form the location data structure # Dict of all line information: key: line name; value: list of site names lines_info = {} # A dict of all site information: key: site name; value: site coordinates (x, y) stations_info = {} for i in range(len(lines_list)): # Several questions you may need to think about, get &quot;Metro line name, station information list, station name, coordinates (x, y), add data to the information dict of the station, add data to the subway line dict&quot; passlines_info, stations_info = get_lines_stations_info(r.text)# According to the route information, establish the site adjacency table dictdef get_neighbor_info(lines_info): pass # Add str2 to the adjacency list of site str1 def add_neighbor_dict(info, str1, str2): # Please write code here pass return neighbor_info neighbor_info = get_neighbor_info(lines_info)neighbor_info# Draw subway mapimport networkx as nximport matplotlibimport matplotlib.pyplot as plt# If Chinese characters cannot be displayed, please refer tomatplotlib.rcParams['font.sans-serif'] = ['SimHei']# matplotlib.rcParams['font.family']='sans-serif'# You can use recursion to find all pathsdef get_path_DFS_ALL(lines_info, neighbor_info, from_station, to_station): # Recursive algorithm, essentially depth first # Traverse all paths # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Check input site name passdef get_next_station_DFS_ALL(node, neighbor_info, to_station): pass# You can also use the second algorithm: simple breadth first without heuristic functiondef get_path_BFS(lines_info, neighbor_info, from_station, to_station): # Search strategy: take the number of stations as the cost (because the ticket price is calculated by station) # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Since the cost of each layer is increased by 1, the cost of each layer is the same, and it does not matter whether it is calculated or not, so it is omitted # Check input site name pass# You can also use the third algorithm: heuristic search with path distance as the costimport pandas as pddef get_path_Astar(lines_info, neighbor_info, stations_info, from_station, to_station): # Search strategy: the straight-line distance between the stations of the route is accumulated as the cost, and the straight-line distance from the current station to the target is used as the heuristic function # Check input site name pass As much as you can to use the already implemented search agent. You just need to define the is_goal(), get_successor(), strategy() three functions. Define different policies for transfer system. Such as Shortest Path Priority（路程最短优先）, Minimum Transfer Priority(最少换乘优先), Comprehensive Priority(综合优先) Implement Continuous transfer. Based on the Agent you implemented, please add this feature: Besides the @param start and @param destination two stations, add some more stations, we called @param by_way, it means, our path should from the start and end, but also include the @param by_way stations. e.g 123451. Input: start=A, destination=B, by_way=[C] Output: [A, … .., C, …. B]2. Input: start=A, destination=B, by_way=[C, D, E] Output: [A … C … E … D … B] # based on your policy, the E station could be reached firstly. The Answer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305# get the data (subway for beijing ,from amap)import requestsimport reimport numpy as npr = requests.request('GET', url = 'http://map.amap.com/service/subway?_1469083453978&amp;srhdata=1100_drw_beijing.json')def get_lines_stations_info(text): lines_info = {} stations_info = {} pattern = re.compile('&quot;st&quot;.*?&quot;kn&quot;') lines_list = pattern.findall(text) for i in range(len(lines_list)): pattern = re.compile('&quot;ln&quot;:&quot;.*?&quot;') line_name = pattern.findall(lines_list[i])[0][6:-1] pattern = re.compile('&quot;rs&quot;.*?&quot;sp&quot;') temp_list = pattern.findall(lines_list[i]) station_name_list = [] for j in range(len(temp_list)): pattern = re.compile('&quot;n&quot;:&quot;.*?&quot;') station_name = pattern.findall(temp_list[j])[0][5:-1] station_name_list.append(station_name) pattern = re.compile('&quot;sl&quot;:&quot;.*?&quot;') position = tuple(map(float, pattern.findall(temp_list[j])[0][6:-1].split(','))) stations_info[station_name] = position lines_info[line_name] = station_name_list return lines_info, stations_infolines_info, stations_info = get_lines_stations_info(r.text)# print(stations_info)# print(lines_info)len(lines_info)def get_neighbor_info(lines_info): def add_neighbor_dict(info, str1, str2): list1 = info.get(str1) if not list1: list1 = [] list1.append(str2) info[str1] = list1 return info neighbor_info = {} for line_name, station_list in lines_info.items(): for i in range(len(station_list) -1): sta1 = station_list[i] sta2 = station_list[i+1] neighbor_info = add_neighbor_dict(neighbor_info, sta1, sta2) neighbor_info = add_neighbor_dict(neighbor_info, sta2, sta1) return neighbor_infoneighbor_info = get_neighbor_info(lines_info)print(neighbor_info)import networkx as nximport matplotlibimport matplotlib.pyplot as pltmatplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS']matplotlib.rcParams['font.size'] = 2plt.figure(figsize = (20, 20))stations_graph = nx.Graph()stations_graph.add_nodes_from(list(stations_info.keys()))nx.draw(stations_graph, stations_info, with_labels = True, node_size = 5)stations_connection_graph = nx.Graph(neighbor_info)plt.figure(figsize = (30, 30))nx.draw(stations_connection_graph, stations_info, with_labels = True, node_size = 5)# The first algorithm: recursively find all pathsdef get_path_DFS_ALL(lines_info, neighbor_info, from_station, to_station): # # Recursive algorithm, essentially depth first # Traverse all paths # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Check input site name if not neighbor_info.get(from_station): print('起始站点“%s”不存在。请正确输入！'%from_station) return None if not neighbor_info.get(to_station): print('目的站点“%s”不存在。请正确输入！'%to_station) return None path = [] this_station = from_station path.append(this_station) neighbors = neighbor_info.get(this_station) node = {'pre_station':'', 'this_station':this_station, 'neighbors':neighbors, 'path':path} return get_next_station_DFS_ALL(node, neighbor_info, to_station)def get_next_station_DFS_ALL(node, neighbor_info, to_station): neighbors = node.get('neighbors') pre_station = node.get('this_station') path = node.get('path') paths = [] for i in range(len(neighbors)): this_station = neighbors[i] if (this_station in path): # If this station is already in the path, it means a loop, and this road is unreachable return None if neighbors[i] == to_station: # Find the end, return to the path path.append(to_station) paths.append(path) return paths else: neighbors_ = neighbor_info.get(this_station).copy() neighbors_.remove(pre_station) path_ = path.copy() path_.append(this_station) new_node = {'pre_station':pre_station, 'this_station':this_station, 'neighbors':neighbors_, 'path':path_} paths_ = get_next_station_DFS_ALL(new_node, neighbor_info, to_station) if paths_: paths.extend(paths_) return pathspaths = get_path_DFS_ALL(lines_info, neighbor_info, '回龙观', '西二旗')print('共有%d种路径。'%len(paths))for item in paths: print(&quot;此路径总计%d站:&quot;%(len(item)-1)) print('-'.join(item))# The second algorithm: simple breadth first without heuristic functiondef get_path_BFS(lines_info, neighbor_info, from_station, to_station): # Search strategy: take the number of stations as the cost (because the ticket price is calculated by station) # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Since the cost of each layer is increased by 1, the cost of each layer is the same, and it does not matter whether it is calculated or not, so it is omitted # Check input site name if not neighbor_info.get(from_station): print('起始站点“%s”不存在。请正确输入！'%from_station) return None if not neighbor_info.get(to_station): print('目的站点“%s”不存在。请正确输入！'%to_station) return None # The search node is a dict, key=site name, value is a list of sites that contain passing nodes = {} nodes[from_station] = [from_station] while True: new_nodes = {} for (k,v) in nodes.items(): neighbor = neighbor_info.get(k).copy() if (len(v) &gt;= 2): # Do not go to the previous stop pre_station = v[-2] neighbor.remove(pre_station) for station in neighbor: # Traverse neighbors if station in nodes: # Skip the nodes that have been searched continue path = v.copy() path.append(station) new_nodes[station] = path if station == to_station: # Find the path, end return path nodes = new_nodes print('未能找到路径') return Nonepaths = get_path_BFS(lines_info, neighbor_info, '回龙观', '西二旗')print(&quot;路径总计%d站。&quot;%(len(paths)-1))print(&quot;-&quot;.join(paths))# Gaode Navigation is 31 stations, only 1 transfer# The result of the code is 28 stations, but there are 5 transfers# Guess Gaode's path cost is mainly time# The third algorithm: heuristic search with path distance as the costimport pandas as pddef get_path_Astar(lines_info, neighbor_info, stations_info, from_station, to_station): # Search strategy: the straight-line distance between the stations of the route is accumulated as the cost, and the straight-line distance from the current station to the target is used as the heuristic function # Check input site name if not neighbor_info.get(from_station): print('起始站点“%s”不存在。请正确输入！'%from_station) return None if not neighbor_info.get(to_station): print('目的站点“%s”不存在。请正确输入！'%to_station) return None # Calculate the straight-line distance from all nodes to the target node, spare distances = {} x,y = stations_info.get(to_station) for (k,v) in stations_info.items(): x0,y0 = stations_info.get(k) l = ((x-x0)**2 + (y-y0)**2)**0.5 distances[k] = l # Nodes that have been searched, dict # key=site name, value is the minimum cost from a known starting point to this site # 已搜索过的节点，dict searched = {} searched[from_station] = 0 # The data structure is pandas dataframe # index is the site name # g is the path taken, h is the heuristic function value (the current straight-line distance to the target) nodes = pd.DataFrame([[[from_station], 0, 0, distances.get(from_station)]], index=[from_station], columns=['path', 'cost', 'g', 'h']) count = 0 while True: if count &gt; 1000: break nodes.sort_values('cost', inplace=True) for index, node in nodes.iterrows(): count += 1 # Search for the site that is the shortest from the destination among the neighbors neighbors = neighbor_info.get(index).copy() if len(node['path']) &gt;= 2: # Do not search in the reverse direction of this path neighbors.remove(node['path'][-2]) for i in range(len(neighbors)): count += 1 neighbor = neighbors[i] g = node['g'] + get_distance(stations_info, index, neighbor) h = distances[neighbor] cost = g + h path = node['path'].copy() path.append(neighbor) if neighbor == to_station: # Find the goal, end print('共检索%d次。'%count) return path if neighbor in searched: if g &gt;= searched[neighbor]: # Explain that the search path is not optimal, ignore it continue else: searched[neighbor] = g # Modify the node information corresponding to this site# nodes.loc[neighbor, 'path'] = path # 这行总是报错# nodes.loc[neighbor, 'cost'] = cost# nodes.loc[neighbor, 'g'] = g# nodes.loc[neighbor, 'h'] = h # I don’t know how to modify the list element in df, I can only delete and add new rows nodes.drop(neighbor, axis=0, inplace=True) row = pd.DataFrame([[path, cost, g, h]], index=[neighbor], columns=['path', 'cost', 'g', 'h']) nodes = nodes.append(row) else: searched[neighbor] = g row = pd.DataFrame([[path, cost, g, h]], index=[neighbor], columns=['path', 'cost', 'g', 'h']) nodes = nodes.append(row) # All neighbors of this site have been searched, delete this node nodes.drop(index, axis=0, inplace=True) # The outer for loop only runs the first row of data, and then re-sort and then calculate continue print('未能找到路径') return Nonedef get_distance(stations_info, str1, str2): x1,y1 = stations_info.get(str1) x2,y2 = stations_info.get(str2) return ((x1-x2)**2 + (y1-y2)**2)** 0.5paths = get_path_Astar(lines_info, neighbor_info, stations_info, '回龙观', '西二旗')if paths: print(&quot;路径总计%d站。&quot;%(len(paths)-1)) print(&quot;-&quot;.join(paths))# Gaode Navigation is 31 stations, only 1 transfer# The code result is 28 stations, which is the same as the result with the number of subway stations as the cost, but the path is different (from the first traversal algorithm, you can see that there are 3 paths for 28 stations to reach the destination)# Guess Gaode's path cost is mainly time","link":"/Assignment/"},{"title":"将 Bard API 与 ChatGPT 集成：实时数据访问","text":"在人工智能领域，很少有创新能像 OpenAI 的 ChatGPT 一样激发世界的想象力。这种非凡的对话式人工智能改变了我们看待人机交互的方式，展现出一定程度的复杂性、情境意识和创造力，而这些曾经被认为是人类智能的专属领域。 ChatGPT 基于强大的 GPT-3 模型构建，能够进行引人入胜、有意义且令人印象深刻的类人对话。它可以写诗、回答复杂的问题、辅导各种科目、翻译语言，甚至模仿著名作家的写作风格。从本质上讲，它重新定义了我们认为人工智能可能实现的界限。 然而，ChatGPT 的主要缺点是它缺乏实时互联网数据访问。这意味着，虽然 ChatGPT 可以生成高度智能且上下文准确的响应，但其知识基本上被及时冻结，截止日期为 2021 年 9 月。 那么，当出现需要通过将 Google 的 Bard API 与 ChatGPT 集成来获取超出此限制的信息的问题时，会发生什么情况呢？ 以下是使用Python将 Bard API 连接到 ChatGPT 以检索实时数据的分步指南： 第 1 步：安装非官方 Bard Python 库并检索 Cookie 值（API 密钥） 我正在使用Daniel Park使用逆向工程开发的非官方 Bard 库。这个库是一个非常用户友好的Python包。其主要目的是通过 API 从 Google Bard 获取响应。使用 Bard-API，用户可以方便地将 Bard 的自然语言响应集成到他们的 Python 项目和各种应用程序中。 1pip install bardapi 您还可以直接从 Github安装最新版本： 1pip install git+https://github.com/dsdanielpark/Bard-API.git 12345from bardapi import Bardtoken = 'xxxxxxx'bard = Bard(token=token)bard.get_answer(&lt;your query&gt;)['content'] 设置您的 API 密钥 安装 Bard-API 后，使用 Bard cookie 中的 Secure-1PSID 进行身份验证。尽管非正式地称为 API KEY（Cookie 值），但请记住对其保密以确保安全访问。 访问https://bard.google.com/ 按 F12 或右键单击并“检查” 转到应用程序 → Cookie，并将您的 __Secure-1PSID Cookie 值复制到安全位置。 步骤 2：从openai.com获取 OpenAI 密钥并安装 OpenAI 库 访问 OpenAI 网站并获取您的 OpenAI API 密钥。现在安装 OpenAI 库并导入它。 1pip install openai 12import openaiopenai.api_key = &lt;Your_API_Key&gt; 步骤3：将bard请求结果连接到gpt-3.5-turbo模型并设计提示 这里的关键部分是设计将 Bard 结果集成到 ChatGPT API 函数中所需的提示。因此，我为此制定了一个方法： 123456789101112query = input(&quot;Your query&quot;)bard_result = bard.get_answer(query)['content']completion = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Act as an AI chatbot with access to the internet.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Provide a well structured and easily readable text by analyzing this: The first content below is the user's query and the second content below is the result obtained by accessing the internet with the help of google's search alogoritm. Provide the well structured and good mannered answer by processing the user's query and the result from Google search algorithm. /n&quot;+query+' /n '+ bard_result} ])final_response = completion[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]print(final_response) 将所有代码封装在一起，得出结果： 1234567891011121314151617from bardapi import Bardimport openaiopenai.api_key = &lt;Your Key&gt;token = &lt;Your Key&gt;bard = Bard(token=token)query = input(&quot;Your query: &quot;)bard_result = bard.get_answer(query)['content']completion = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Act as an AI chatbot with access to the internet.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Provide a well structured and easily readable text by analyzing this: The first content below is the user's query and the second content below is the result obtained by accessing the internet with the help of google's search alogoritm. Provide the well structured and good mannered answer by processing the user's query and the result from Google search algorithm. /n&quot;+query+' /n '+ bard_result} ])final_response = completion[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]print(final_response) 与其使用 gpt-3.5-turbo 型号，不如试试 gpt-3.5-turbo-16k 和 gpt-4-0314，效果会更好。 通过整合像谷歌的 Bard 这样的应用程序接口，ChatGPT 可以超越目前的局限，为用户提供实时、准确的上下文信息。这将大大增强其协助、教育和与用户互动的能力，为人与人工智能的互动增添一个全新的维度。此外，这还将极大地扩展 ChatGPT 的应用范围，为企业、教育工作者、研究人员和个人带来新的机遇。 我认为这是将互联网接入集成到 ChatGPT 并从 ChatGPT 获得实时见解的最简单方法。","link":"/BardAPI-ChatGPT/"},{"title":"最佳 ChatGPT Chrome 扩展程序","text":"想要轻松访问ChatGPT吗？其中一个最佳方式是通过其一系列Chrome扩展程序。这些扩展程序还为您提供更好的使用ChatGPT的方法，包括帮助您编写更好的提示以获得更好的响应，或为ChatGPT授予搜索互联网的能力，从而提供对更多最新信息的访问。 这是您现在可以使用的最佳 ChatGPT Chrome 扩展程序。 Google 聊天 GPT 嫉妒 Bing Chat 及其在您搜索时与您聊天或使用最新的 GPT-4 语言模型的能力吗？不需要。只需从网上商店获取适用于 Google 的 ChatGPT，您就可以将 ChatGPT 与 Google 搜索一起使用。事实上，只需进行一次普通的 Google 搜索，在结果旁边，您也会收到来自 ChatGPT 的回复，这有时比 Google 结果本身更有用。 Merlin 想要 ChatGPT 在您上网的任何地方响应任何内容？Merlin 将 ChatGPT 带到任何网站，因此您可以突出显示任何文本或网页，并要求 ChatGPT 对其做出响应。您可以让它为您总结一个网页，或为您提供 YouTube 视频的纲要，这样您就不需要全部观看了。 TalkBerry 为什么要在 ChatGPT 上打字，而不是直接与 ChatGPT 对话呢？使用 TalkBerry，您可以简单地与 ChatGPT 通话。只需安装扩展程序并确保您的麦克风或耳机已插入，即可开始使用。使用 TalkBerry，您可以节省大量在输入上的时间，或者将 ChatGPT 用作语言导师，让它聆听并帮助您提高发音和语言理解能力。 TweetGPT 使用 TweetGPT 可以让您的社交媒体游戏更上一层楼。TweetGPT 是 ChatGPT 的插件，利用 AI 聊天机器人工具的强大功能，制作更有趣、更尖刻、更具吸引力或更友好的推文和回复。您可以选择要发布的主题、您的情绪基调和语言，ChatGPT 将完成剩下的工作。如果您对某些措辞不满意，您甚至可以在之后编辑该消息。 GPT-EZ 如果您不喜欢 ChatGPT 界面并想将其更改为您自己的喜好，请尝试 GPT-EZ。它允许您自定义 ChatGPT 网站的 UI，包括配色方案、字体样式和其他选项。此外，它还可以让您更轻松地复制和继续与 ChatGPT 的对话，并让您更轻松地下载对话日志。 SnackPrompt 通过使用一些评价最高的提示来充分利用 AI 聊天机器人。SnackPrompt 列出并排名全球其他聊天机器人用户的最佳提示，让您可以访问一些最新和最强大的 AI 功能。 WebChatGPT ChatGPT 的最大限制之一是它无法访问最近的信息。即使您使用的是最新的 GPT-4 语言模型，它仍然只能访问 2021 年之前的信息。使用 WebChatGPT，您可以让 ChatGPT 能够在网络上搜索更多最新的信息来源。从 Chrome 网上应用店获取扩展程序，在使用 ChatGPT 时只需将其打开即可享受这一方便的功能。 YouTube Summary 喜欢 YouTube 教程，但不想看完序言？让此 ChatGPT 扩展为您总结说明。只需从 YouTube 视频页面获取转录内容，然后将其输入到插件中，您就会立即获得摘要。它还适用于文章、电子邮件或科学论文。","link":"/Best-ChatGPT-Chrome-Extension/"},{"title":"21. 尝试制作你自己的数字人进行播报","text":"Hi， 大家好。我是茶桁。 在之前的课程中，我们接触了AI进行文字回复，语音合成。 那么将这两个组合在一起，我们基本就可以制作一个智能的语音聊天机器人了。看过电影《Her》的同学都应该清楚，AI因为用了女神斯嘉丽.约翰逊的配音，吸引到了不少的观众。 不过， 我们怎么能就满足于此呢，从文字到音频，我们似乎还缺少了一点什么。是啊，谁不希望拥有一个特定的虚拟人来发出自己特定的语音。看着自己在镜头面前侃侃而谈的样子，是不是想想就兴奋？ 把这些需求都结合在一起，那就是“数字人”了，我相信各位小伙伴或多或少都已经接触过，至少在抖音上看到过其他主播的“数字人”了。但是我们不得不说，那些都是一些商业公司的成熟方案，而咱们要实现的内容肯定比不了人家，但是作为概念演示，那是完全够用了。 好了，让我开始吧。首先呢，让我们先制作一个语音聊天机器人 语音聊天机器人的制作 第一步：文本聊天 第一步是什么？当然是需要先做一个「文本聊天机器人」，还记得咱们第六讲的内容吗？接下来，咱们就需要用到第六讲中的代码逻辑，整个UI界面也还是使用Gradio来创建。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Digital-human-broadcasting/"},{"title":"ChatGPT代码解释器：如何为我节省数小时的工作","text":"创建一个交互式世界地图，显示国家人口数量，配以简短的句子描述。 2023年7月6日，OpenAI宣布Code Interpreter将在接下来的一周内向ChatGPT Plus用户开放。它可能是增强ChatGPT的能力和功能的最佳插件之一。 Code Interpreter可以运行代码，允许上传数据，这样您就可以用它来进行数据清理、分析、可视化等许多其他任务。它就像是您指尖上的数据分析师。 听起来很棒吧？接下来我们来看看。 我在一项任务中使用了它，这项任务可能需要我花费几个小时才能完成。根据您的任务和对Python库的熟悉程度，这项任务甚至可能需要更长时间。 在使用ChatGPT时，我只需要写几句话并上传两个CSV文件。 我将逐步介绍整个过程，以及Code Interpreter在理解数据、清理和预处理数据以及创建数据可视化方面的印象深刻之处。 您需要先启用它 如果您想在聊天中使用代码解释器，需要先通过设置启用它。 如何启用代码解释器 开启之后，你在接下来开启新的聊天的时候就可以使用它了。 来个具体任务 我打算创建一个世界地图可视化，其中点的大小与国家的人口成比例。 我有两个数据集，一个是国家人口，另一个是以纬度和经度表示的位置。 您可以轻松地在网上找到这些数据集。如果没有，文末我将提供数据集（以及源代码）。 让我们继续吧 由于这是一个简单的任务，我没有花费太多时间在提示工程上。这是我向ChatGPT请求的第一句话： 你可以在提示窗口左侧使用加号上传数据集。 使用+号上传数据集 首先要做的是读取数据集，这是显而易见的事情。但这还不够令人印象深刻！ 12345678# Code Interpreter 生成的代码import pandas as pd# 将CSV数据加载到pandas DataFrame中population_data = pd.read_csv('/mnt/data/country_population_historic.csv')# 显示 DataFrame 的前几行population_data.head() 然后，ChatGPT要求提供国家位置： 由于我已经拥有相关数据了，所以我要做的就只是上传它： 123456# Code Interpreter 生成的代码# 加载国家坐标数据country_coordinates = pd.read_csv('/mnt/data/country_coordinates.csv')# 显示前几行country_coordinates.head() 数据清洗和预处理 读取数据后的下一步是清洗和预处理数据。我相信你已经听过很多次，这一步通常需要最多的时间。 在创建世界地图可视化之前，有一些问题需要处理。ChatGPT检测到它们并在不到一分钟的时间内提供了解决方案。相当令人印象深刻！ 上面代码段中的代码: 12345678910# 由代码解释器生成的代码# 检查数据框中的国家名称是否一致population_countries = set(population_data['Country Name'])coordinates_countries = set(country_coordinates['name'])# 人口数据中的国家但不在坐标数据中missing_in_coordinates = population_countries - coordinates_countries# 坐标数据中的国家但人口数据中没有missing_in_population = coordinates_countries - population_countries 在坐标数据集和人口数据集中都有一些国家缺失。 ChatGPT在上面的代码片段中做了什么： 创建了Python集合，包含两个数据集中的国家名称。 分别比较这些集合，找到它们之间的差异。 ChatGPT在不到一分钟的时间内解决了这个问题： 以上片段中的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# Code Interpreter 生成的代码# 将人口数据集中的国家名称映射到坐标数据集中country_name_mapping = { 'Bahamas, The': 'Bahamas', 'Brunei Darussalam': 'Brunei', 'Cabo Verde': 'Cape Verde', 'Congo, Dem. Rep.': 'Congo [DRC]', 'Congo, Rep.': 'Congo [Republic]', &quot;Cote d'Ivoire&quot;: &quot;Côte d'Ivoire&quot;, 'Czechia': 'Czech Republic', 'Egypt, Arab Rep.': 'Egypt', 'Gambia, The': 'Gambia', 'Hong Kong SAR, China': 'Hong Kong', 'Iran, Islamic Rep.': 'Iran', &quot;Korea, Dem. People's Rep.&quot;: 'North Korea', 'Korea, Rep.': 'South Korea', 'Kyrgyz Republic': 'Kyrgyzstan', 'Lao PDR': 'Laos', 'Macao SAR, China': 'Macau', 'Micronesia, Fed. Sts.': 'Micronesia', 'Myanmar': 'Myanmar [Burma]', 'North Macedonia': 'Macedonia [FYROM]', 'Russian Federation': 'Russia', 'Sao Tome and Principe': 'São Tomé and Príncipe', 'Slovak Republic': 'Slovakia', 'Syrian Arab Republic': 'Syria', 'Turkiye': 'Turkey', 'Venezuela, RB': 'Venezuela', 'Yemen, Rep.': 'Yemen', 'Eswatini': 'Swaziland'}# 清理人口数据集population_data_clean = population_data.replace({&quot;Country Name&quot;: country_name_mapping})# 删除在坐标数据集中不存在的行population_data_clean = population_data_clean[population_data_clean['Country Name'].isin(coordinates_countries)]# 将人口数据与国家坐标数据合并merged_data = pd.merge(population_data_clean, country_coordinates, left_on='Country Name', right_on='name')# 只保留必要的列merged_data = merged_data[['Country Name', '2022', 'latitude', 'longitude']]# 重命名列以增加清晰度merged_data.columns = ['Country', 'Population', 'Latitude', 'Longitude']merged_data.head() 清理和预处理的数据 现在我们已经有了整洁和清晰的数据格式。这个步骤通常需要很多时间和人工。ChatGPT在大约一分钟内完成了这一步。 创建可视化 ChatGPT首先尝试使用Matplotlib库进行创建。然而，由于没有世界地图图像而失败： 然后，我只是要求它使用 Plotly，这就是我一开始想到的。 以上代码段中的代码： 12345678910111213141516171819202122232425262728293031# 代码由代码解释器生成import plotly.graph_objects as go# 创建散点地理图fig = go.Figure(data=go.Scattergeo( lon = merged_data['Longitude'], lat = merged_data['Latitude'], text = merged_data['Country'] + ': ' + merged_data['Population'].astype(str), mode = 'markers', marker = dict( size = (merged_data['Population'] / 1000000) ** 0.5, # Take square root to better visualize differences sizemode = 'diameter', sizeref = 1, sizemin = 4, color = merged_data['Population'], colorscale = 'Viridis', colorbar_title = &quot;Millions&quot;, )))# 更新布局fig.update_layout( title_text = '2022 World Population', geo = dict( showframe = False, showcoastlines = True, projection_type = 'equirectangular' ))fig.show() 它不能在浏览器中显示交互式世界地图，但成功编写了生成它的代码。我所要做的就是将代码复制粘贴到Jupyter笔记本中。 这是最终结果： 互动世界地图，显示国家人口 最后的话 我们所做的事情： 读取数据集 清洗、预处理和合并它们 创建互动数据可视化 我们所要做的只是写两个句子（并告诉 ChatGPT 使用 Plotly）。我认为这非常令人印象深刻！ 数据 本文已经结束。 文章最后，我将提供数据以及一个jupyter notebook内容，和往常一样，数据将付费查看，以获取一些成本。有想要的朋友可以去我公众号内搜索本文购买：","link":"/ChatGPT%E4%BB%A3%E7%A0%81%E8%A7%A3%E9%87%8A%E5%99%A8/"},{"title":"你要做的很简单，别去想那只大象","text":"“禁止思考实验”，这是心理学一个经典的实验，它被用来研究逆反心理。有兴趣的小伙伴可以跟着我们一起做： 实验要求参与者坐下，并听取实验者说明。实验者会告诉参与者，接下来的任务非常简单，只需要尽可能长时间地不去想象一只白色的大象。 实验者开始计时，并要求参与者开始执行任务。在执行任务时，参与者需要告诉实验者当他们想起白色大象的时刻，或者当他们对大象的想象感到特别强烈的时刻。参与者开始尝试不去想象白色大象。但是，实验结果表明，大多数参与者很快就会想到大象，即使他们试图抑制这种想象，他们也很难控制。在实验的后续部分，实验者可能会告诉参与者，他们现在可以开始想象白色大象了，这时参与者的反应可能会改变，他们可能会开始主动抵制想象大象，或者会出现想象不出白色大象的情况。最后，实验者会记录下参与者在实验过程中的各种反应，并进行分析和总结。怎么样，屏幕前的你情况如何？有克制到自己不去想那只白象吗？把你的结果发在评论区告诉我。 这个实验表明了逆反心理的存在，即当人们感到被强迫或期望去做某些事情时，他们可能会反其道而行之，甚至会选择与预期相反的行为。在这个实验中，参与者被要求不去想象白色大象，但很多人最终还是想象了这只大象。这表明，人们往往会对被禁止或限制的事物产生强烈的反应，即逆反心理。 逆反心理是指人们抵制或反对他们所认为的强制性或压迫性的做法，即使这种做法实际上可能对他们有益。这种反应可能是一种自我保护机制，因为人们往往会感到自己失去了自主权或选择的权利。 例如，当父母强迫孩子吃某种食物或学习某个科目时，孩子可能会表现出逆反心理，并且刻意不吃那种食物或不学习那个科目。同样地，当政府强制人们遵守某些规定或法律时，一些人可能会表现出逆反心理，不想遵守这些规定或法律。 逆反心理并不总是一件坏事，因为它可以帮助人们保护自己的权利和自主性。然而，在某些情况下，它可能会导致人们做出不理性或不负责任的决定。因此，在处理逆反心理时，需要寻找平衡，以确保人们不会受到伤害并且可以做出明智的决定。","link":"/Do-not-imagine-a-white-elephant/"},{"title":"01 进入AI大门，学会与其交谈","text":"不用问我都知道，你们一定是被ChatGPT的火热出圈导致的开始关注人工智能，也是由于此才看到我这篇文章。 放心，大家想要的我一定会给予，既然大家都想先认识ChatGPT，那么我们就从这个主题开始。 接下来，我们学学如何利用openAI的API来和其沟通。在整个使用过程中，我们都使用的是GPT-3.5的大预言模型。 在本课程中，我们将回答许多问题，例如，OpenAI 的 API 能够实现哪些神奇的事情？OpenAI 的产品被称为已经离通用人工智能（AGI）不远了，它们长什么样子？GPT-3 这样的模型与之前基于深度学习的自然语言处理解决方案有什么不同？我们将通过逐步解释这些问题，使您深入了解这个令人兴奋的领域。 无论您是否是一名程序员，您都可以从本课程中学习如何使用 AI 技术，尤其是大型语言模型，为您的项目和业务提供价值。 基础工作 创建帐号和API Key 了开始学习本课程，您需要先注册一个可以使用 OpenAI 的 API 的账号。您可以通过注册入口进行注册。目前，OpenAI 尚未向中国大陆和香港地区开放，因此您需要自己寻找适当的解决方案进行注册。如果您有更好的解决方案，也欢迎在评论区分享。 注册账号后，您需要点击右上角的账号，然后进入 \"View API Keys\" 页面管理 API Keys。 您可以点击下方的 \"+Create new secret key\" 来创建一个新的 API Key。 您需要将此 API Key 存储在一个安全的位置，因为在后续的 OpenAI 接口调用中，需要使用此 API Key。 储存API Key留用这方便，我使用的是1Password，开了家庭版，很好用。 目前，OpenAI 为所有免费注册的用户提供了 5 美元（原来是 18 美元）的免费 API 额度，足够您体验 API 的功能并完成本课程的学习。如果您想在实际产品中进一步使用此 API，则需要考虑升级到付费账户。 注：在本文完成之时，我发现免费账户已经无法使用免费的API配额了，不仅如此，因为API配额和ChatGPT Plus是两个支付系统，所以Plus并不等同于API 配额，你需要绑定一张信用卡用于支付你的使用量。 未绑卡 已绑卡 关于绑卡这个事，可以自己在网上搜索看，办法总比问题多。不要找我，虽然我有渠道，但是我的渠道很贵，到时候说我骗人钱我可说不清楚。 搭建环境 既然是开发API应用，那必然是需要开发环境的。如果你自己会，那就最好不过了，如果不是太熟悉，可以参考一下我这篇文章： 这篇文章详细的介绍了在Mac内如何搭建AI环境，包括Tensorflow的安装等。 基本上，我们现在需要的是3.10 的Python环境，还有Conda（我习惯用这个），然后在本地安装好Jupyter lab, 如下： 123conda create --name gpt python=3.10conda activate gptconda install -c conda-forge jupyterlab ipywidgets openai 这一段命令的意思是创建一个名为 gpt的python 3.10的开发环境，然后切换到这个环境里，再安装必要的包。 在后面的使用过程中，当然你可以选择jupyter notebook, 也可以和我一样，使用VSCode。 当然，你也可以选择Colab，其实这也是一个Jupyterlab，如果你不想本地搭建环境，那就直接使用Colab吧，不过注意一点，需要科学上网。就算你本地有环境，我还是建议你有些事后使用Colab，能用到一些免费的GPU资源，我的M1没有好的显卡支持，很多时候还是需要上Colab。 使用时候，记得要安装openAI的库，并且设置自己的API Key： 12!pip install openai%env OPENAI_API_KEY=&quot;这里输入你的API Key&quot; 测试一下 让我们现在开始依次写完这段代码，虽然截图内已经有了，但是还是让我们一步步来执行起来，这一段代码，并不是出自我之手，而且直接借鉴的徐文浩的代码： 123456789101112131415161718192021222324252627282930313233import openaiimport json# 设定API Key和模型openai.api_key = &quot;输入你自己的代码&quot;COMPLETION_MODEL = &quot;text-davinci-003&quot;# 设定关键词和描述prompt = &quot;&quot;&quot;Consideration proudct : 工厂现货PVC充气青蛙夜市地摊热卖充气玩具发光蛙儿童水上玩具1. Compose human readale product title used on Amazon in english within 20 words.2. Write 5 selling points for the products in Amazon.3. Evaluate a price range for this product in U.S.Output the result in json format with three properties called title, selling_points and price_range&quot;&quot;&quot;# 写一个调用方法def get_response(prompt): completions = openai.Completion.create ( engine=COMPLETION_MODEL, prompt=prompt, max_tokens=512, n=1, stop=None, temperature=0.0, ) message = completions.choices[0].text return message# 调用方法并打印最终结果print(get_response(prompt)) 然后我们就可以看到返回了： 1234567891011{ &quot;title&quot;: &quot;Glow-in-the-Dark Inflatable PVC Frog Night Market Hot Selling Water Toy for Kids&quot;, &quot;selling_points&quot;: [ &quot;Made of durable PVC material&quot;, &quot;Glow-in-the-dark design for night play&quot;, &quot;Inflatable design for easy storage and transport&quot;, &quot;Perfect for water play and outdoor activities&quot;, &quot;Great gift for kids&quot; ], &quot;price_range&quot;: &quot;$10 - $20&quot;} 这段代码里面，我们调用了 OpenAI 的 Completion 接口，然后向它提了一个需求，也就是为一个我在 1688 上找到的中文商品名称做三件事情。 为这个商品写一个适合在亚马逊上使用的英文标题。 给这个商品写 5 个卖点。 估计一下，这个商品在美国卖多少钱比较合适。 同时，我们告诉 OpenAI，我们希望返回的结果是 JSON 格式的，并且上面的三个事情用 title、selling_points 和 price_range 三个字段返回。 神奇的是，OpenAI 真的理解了我们的需求，返回了一个符合我们要求的 JSON 字符串给我们。在这个过程中，它完成了好几件不同的事情。 第一个是翻译，我们给的商品名称是中文的，返回的内容是英文的。 第二个是理解你的语义去生成文本，我们这里希望它写一个在亚马逊电商平台上适合人读的标题，所以在返回的英文结果里面，AI 没有保留原文里有的“工厂现货”的含义，因为那个明显不适合在亚马逊这样的平台上作为标题。下面 5 条描述也没有包含“工厂现货”这样的信息。而且，其中的第三条卖点 “Inflatable design for easy storage and transport”，也就是作为一个充气的产品易于存放和运输，这一点其实是从“充气”这个信息 AI 推理出来的，原来的中文标题里并没有这样的信息。 第三个是利用 AI 自己有的知识给商品定价，这里它为这个商品定的价格是在 10～20 美元之间。而我用 “Glow-in-the-Dark frog” 在亚马逊里搜索，搜索结果的第一行里，就有一个 16 美元发光的青蛙。 最后是根据我们的要求把我们想要的结果，通过一个 JSON 结构化地返回给我们。而且，尽管我们没有提出要求，但是 AI 还是很贴心地把 5 个卖点放在了一个数组里，方便你后续只选取其中的几个来用。返回的结果是 JSON，这样方便了我们进一步利用返回结果。比如，我们就可以把这个结果解析之后存储到数据库里，然后展现给商品运营人员。 接下来，我们再看一个其他的例子： 1234567prompt = &quot;&quot;&quot;Man Utd must win trophies, says Ten Hag ahead of League Cup final请将上面这句话的人名提取出来，并用json的方式展示出来&quot;&quot;&quot;print(get_response(prompt)) 得到输出结果： 123{ &quot;names&quot;: [&quot;Ten Hag&quot;]} 看出AI干了什么吗？其实从中文中你能知道我需要AI做什么，而他完完全全输出了我想要的。 我们这里的两个例子，其实对应着很多不同的问题，其中就包括机器翻译、文本生成、知识推理、命名实体识别等等。在传统的机器学习领域，对于其中任何一个问题，都可能需要一个独立的机器学习模型。就算把这些模型都免费提供给你，把这些独立的机器学习模型组合到一起实现上面的效果，还需要海量的工程研发工作。没有一个数十人的团队，工作量根本看不到头。然而，OpenAI 通过一个包含 1750 亿参数的大语言模型，就能理解自然的语言输入，直接完成各种不同的问题。而这个让人惊艳的表现，也是让很多人惊呼“通用人工智能（AGI）要来了”的原因。 这两个例子虽然简单，但是咱们暂时先到此为止，记得课后好好练习。 请将今天课程中提供的示例代码，在你搭建的开发环境中运行一下。 你可以去看一下 OpenAI 提供的示例，找几个你感兴趣的用途，在上面的开发环境里运行体验一下，你也可以脑洞大开，尝试一些你想用 AI 解决的问题，看看 AI 能不能给出你想要的结果。 推荐阅读 推荐阅读如果你想知道 GPT 系列大模型到底是怎么回事儿，我推荐你去看一下李沐老师讲解 GPT 系列论文的视频 GPT、GPT-2、GPT-3 论文精读，这个视频深入浅出，能够让你理解为什么现在 GPT 那么火热。","link":"/Enter-the-door-of-AI-learn-to-communicate-with-it/"},{"title":"扩展欧几里德","text":"PE -05欧几里德算法 又名「辗转相除」法 迄今为止已知的最古老的算法, 距今(2017年)2317年 用于快速计算两个数字的最大公约数 还可以用于快速求解ax + by = 1方程的一组整数解 扩展欧几里德 已知算法上推导其它算法的流程, 思想过程才是最重要的;","link":"/Extended-Euclid-algorithm/"},{"title":"Exploring the Potential and Challenges of Hybrid Machine Learning Systems in AI","text":"随着机器学习和深度学习的飞速发展，人工智能（AI）正取得飞跃性进展。然而，越来越多的研究者一致认为，AI演进的下一个阶段在于开发混合机器学习系统。这篇博客文章将探讨这个新兴领域，讨论它的潜力、挑战以及对AI未来的影响。 什么是混合机器学习系统？ 混合机器学习系统结合了两个或更多的机器学习模型或技术，创建出一个更强大、更灵活的AI解决方案。这些系统可以发挥每个组成模型的优势，同时弥补它们各自的弱点。组合机器学习模型的方法有多种，例如： 集成学习：将多个基础模型结合成一个更强大的模型。这可以通过bagging、boosting和stacking等技术来实现。 多模态学习：集成不同的数据来源（例如文本、图像和音频）以创建更丰富的数据表示，并提高整体性能。 迁移学习：利用从一个领域或任务中获得的知识，以改善另一个领域或任务中的性能。 元学习：训练模型学习如何学习，使它们能够更快地适应新任务。 混合系统的潜力 混合机器学习系统有潜力彻底改变AI，为解决复杂问题开辟新的可能性。这些系统的一些关键优势包括： 改善性能：通过结合多个模型，混合系统可以实现比任何单个模型更好的性能。对于过于复杂以至于单个模型无法有效解决的问题尤为如此。 鲁棒性：混合系统可以更好地抵御噪声、过拟合和其他影响单个模型的问题。这在现实世界中的应用中尤其重要，因为数据通常是嘈杂和不完美的。 通用性：混合系统可以处理各种问题、数据类型和任务，这使它们高度适应各种行业和应用。 可迁移性：混合系统可以更轻松地利用从一个领域或任务中获得的知识，以改善另一个领域或任务中的性能，这使它们非常适合具有有限训练数据的任务。 开发混合系统的挑战 尽管有潜力，混合机器学习系统也面临着一些挑战。其中最显著的障碍包括： 复杂性：设计和实施混合系统可能比单个模型更加复杂。研究人员和实践者需要仔细考虑如何最好地组合模型和技术，以创建一个有效的系统。 可伸缩性：混合系统的增加的复杂性可能使它们更难以扩展，无论是在计算资源方面还是处理大量数据的能力方面。 可解释性：混合系统可以更具挑战性地解释和说明，因为它们涉及多个模型和技术的交互。这可能使得理解系统如何做出决策并确保其正确运行变得更加困难。 训练和适应：与训练单个模型相比，训练混合系统可能需要更多的计算资源和时间。此外，将这些系统适应到新任务或不断变化的条件可能需要大量的工作。 总之，混合机器学习系统代表了AI未来的一个有希望的方向。通过利用多个模型和技术的优势，这些系统有潜力在性能、鲁棒性和通用性方面取得重大进展。然而，实现这个潜力需要克服与复杂性、可伸缩性、可解释性和训练相关的挑战。随着研究人员和实践者继续探索这个令人兴奋的领域，我们可以期待在各种行业和应用中看到混合机器学习系统的大量进展。","link":"/Exploring-the-Potential-and-Challenges-of-Hybrid-Machine-Learning-Systems-in-AI/"},{"title":"04 GPT-3&#x2F;4对比其他模型胜在哪？","text":"大家好，我是茶桁。 在前两节课中，我们一起体验了 OpenAI 提供的 GPT-3.5 系列模型的两个核心接口。一个是获取文本的 Embedding 向量，另一个是根据提示语生成补全的文本内容。通过这两种方法，我们可以在零样本或少样本的情况下进行情感分析任务。然而，你可能会有两个疑问。首先，Embedding 不就是将文本转换为向量吗？为什么不直接使用开源模型（如Word2Vec、Bert）而要调用 OpenAI 的 API 呢？在我们的情感分析任务中，我们进行了一些简化处理。一方面，我们排除了相对中性的评分（3分）；另一方面，我们将1分、2分和4分、5分合并，将原本需要判断5个分类的问题简化了。那么，如果我们想要准确预测多个分类，是否也能如此简单呢？ 在本节中，我们将通过代码和数据来回答第一个问题，尝试使用常见的开源模型，看看是否可以通过零样本学习的方式取得良好的效果。至于第二个问题，我们将在下节课中探讨，探索如何进一步利用 Embedding 结合机器学习算法来更好地处理情感分析问题。 什么是预训练模型？ 预训练模型是指通过在大规模文本数据上进行学习而生成的模型，能够将文本转化为语义丰富的向量表示。OpenAI 的 GPT-3 是一种超大规模的预训练模型，其英文全称为“Generative Pre-trained Transformer”，即生成式预训练 Transformer。通过预训练模型，我们可以在没有见过具体问题的情况下，利用大量可用的文本数据进行学习。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/GPT-3-VS-Other-Model/"},{"title":"如何在WordPress后台中加入表情符号","text":"本文来自zengrong 这应该是比较老的话题了，请参阅Smilies in WP (wp中的表情) WordPress自带了二十多个表情符号: 这些符号是使用代码插入的，要记住这些代码可不太容易，因此如果将这些表情符号嵌入到后台就方便了。 方法也并不麻烦，首先下载 WP-Grins这个插件，按正常方式安装。由于此插件并不是针对2.0开发，因此要先修改一下。找到wp-grins.php中的下面这句（大约在34行） \\(grins .= '&amp;lt;img src=&amp;quot;'.get_settings('siteurl').'/wp-images/smilies/'.\\)grin.'\" alt=\"'.\\(tag.'&amp;quot; onclick=&amp;quot;grin(\\''.\\)tag.'');\"/&gt; '; 将其中的“/wp-images/smilies/” 修改为“/wp-includes/images/smilies/”。 安装后在插件管理器中激活它，然后编辑“/wp-admin/admin-functions.php”文件，搜索“ edToolbar();”，大约在1079行，找到下面这句 if (!strstr($_SERVER['HTTP_USER_AGENT'], 'Safari')) echo ' &lt;div id=\"quicktags\"&gt; &lt;script src=\"../wp-includes/js/quicktags.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt;if ( typeof tinyMCE == \"undefined\" || tinyMCE.configs.length &lt; 1 ) edToolbar();&lt;/script&gt; &lt;/div&gt; '; 再上面这句之下加入： if (function_exists(\"wp_grins\")) { echo \"\"; wp_grins(); } //zrong added 修改完毕上传，打开后台撰写文章即可看到，可爱的表情已经加入到编辑器中了。 如果要将这些表情也加入到评论页面中，可以在主模版的comments.php中搜索如下语句（可能不完全相同）： &lt;p&gt; &lt;textarea class=\"textform\" name=\"comment\" id=\"comment\" cols=\"100%\"rows=\"10\" tabindex=\"4\"&gt;&lt;/textarea&gt; &lt;/p&gt; 在其上加入如下PHP语句即可： &lt;?php wp_grins(); ?&gt; 完毕。记下来也方便自己查找。 顺便： 如果想在页面中加入表情，还可以借助Emotions这个插件。","link":"/How-to-add-emoticons-in-wordpress-background/"},{"title":"系列课程：从零开始接触人工智能大模型（介绍）","text":"整个系列课程内容虽然为自己所写，但是参考了bothub 创始人徐文浩的课程《AI 大模型之美》 人工智能是计算机科学领域中最具前瞻性和影响力的技术之一。它是一种智慧型算法，能够模拟人类的思维过程，处理大量的数据和信息，从而发现隐藏在其中的规律和趋势。人工智能的应用范围非常广泛，包括语音识别、图像识别、自然语言处理、机器翻译、智能推荐、智能问答、自动驾驶等等。 人工智能的发展历程可以追溯到上个世纪五六十年代。当时，计算机科学家们开始思考如何让计算机能够像人类一样思考和行动。1956年，美国达特茅斯学院举办了一次名为“人工智能”的会议，正式提出了人工智能的概念。自此以后，人工智能的研究和应用就成为了计算机科学领域的一项重要任务。 随着科技的不断进步，人工智能技术得到了长足的发展。各种机器学习算法、深度学习算法、开源的软件包以及云平台提供的解决方案不断涌现，为企业提供了各种智能化的产品和服务。例如，通过图像识别技术，我们可以将人脸识别、车牌识别、病变识别等技术应用于安防、交通、医疗等领域；通过自然语言处理技术，我们可以实现智能客服、智能翻译、智能问答等功能，提升用户体验和效率；通过机器学习技术，我们可以实现推荐系统、广告精准投放等功能，提高市场竞争力。 然而，人工智能领域仍然存在着挑战和困难。其中一个难点就是“有多少人工就有多少智能”这个诅咒。很多“智能”都来自于大量的人工数据标注和硬编码的业务规则，导致人工智能在某些特殊情况下表现得像“人工智障”。因此，如何提高人工智能的效率和性能，仍然是人工智能领域需要解决的问题之一。 去年 12 月，我第一次尝试与 ChatGPT 进行交互。一开始我并没有对这个新的 AI 聊天机器人抱有太高的期待，毕竟以前的聊天机器人总是表现得像“人工智障”。但是，ChatGPT 证明了我的想法是错误的。在与它交流了几分钟后，我决定让它帮我写一些 Bash 脚本和 SQL 代码。我很惊讶地发现，它不仅完全理解了我的需求，还精确地写出了我需要的复杂 SQL 代码。这次体验让我对人工智能的潜力有了新的认识，也让我更加期待未来人工智能的发展。 对于复杂的需要窗口函数的 SQL，ChatGPT 写得比我快多了。 从年前到目前（5月份）为止，我一直在体验市面上大部分的人工智能工具，例如最近非常火热的ChatGPT等。这样的体验让我对人工智能的潜力有了新的认识，也让我开始思考各行业未来的前景，并提出了一些担忧。当我们看到人工智能技术不断取得突破性进展的同时，也会担心它是否会取代人类的工作，进而给社会带来不稳定的因素。然而，我相信人工智能的发展是为了更好地服务人类，而非取代人类。我们需要更多人去了解和掌握人工智能技术，这样才能更好地应对未来的挑战，发挥人工智能技术的最大价值。 基于此，我想让更多人开始接触人工智能，并且学会如何利用人工智能，更甚为学习新一代AI应用编程。因此，我想介绍一门系列课程：从零开始接触人工智能大模型。该课程将介绍人工智能的基本概念、常见应用场景以及如何利用最新的AI技术构建自己的AI应用。我们将从浅入深地讲解人工智能相关的知识，帮助每个人都能够轻松上手，并且学会如何应用到自己的领域中去。不仅仅是算法工程师和机器学习研究人员，每个工程师都可以快速学习并参与开发新一代的AI应用。我相信，学会开发新一代AI应用是每个软件开发行业从业者都值得学习的，无论是产品经理还是工程师，乃至于行业之外的业务人员，都应该拥抱新的AI浪潮。 在学习的过程中，我们将讨论人工智能的应用场景，例如语音识别、图像识别、自然语言处理、机器翻译、智能推荐、智能问答、自动驾驶等等。这将有助于我们了解人工智能技术在不同领域的应用，从而更好地把握未来的发展方向。同时，我们也会学习最新的人工智能技术，例如大模型、自监督学习等等。这些技术的出现，为人工智能的应用提供了更加广阔的空间和深度。 在这个充满挑战和机遇的时代，我们需要准备好迎接未来的挑战。学习人工智能技术，是每个软件开发行业从业者都需要掌握的技能。无论是产品经理还是工程师，乃至于行业之外的业务人员，都应该拥抱新的AI浪潮，学习开发新一代的AI应用。我相信，通过学习新一代的AI应用编程，我们能够更好地应对未来的挑战，为我们的生活和工作带来更多的便利和机遇。 学习成本那么高，给我一个理由先 这个应该是普遍的一个想法，其实对于此，我将不仅给你一个理由，而是给你三个： 1. 开发门槛降低，人人可学习AI应用开发 人人都应该学习如何开发新一代 AI 应用，因为这一轮的 AI 浪潮里，开发新的 AI 应用的门槛大大降低了。过去，AI 应用开发是一个门槛比较高的领域，需要掌握大量的机器学习和深度学习的知识，了解各种基础模型，使用各种机器学习的编程框架，以及理解在实际应用里锤炼机器学习的各种实战技巧和模型。对于没有相关经验的人来说，不花上个一两年时间，你可能很难用 AI 算法做出有价值的产品。 但是现在，随着预训练好的大型基础模型的出现，以及这些模型的能力通过开放 API 的形式提供出来，即使没有任何机器学习的理论知识，你只需要一两天时间，就能做出一个能解决实际问题的 AI 应用。比如，最近在 GitHub 上就能看到很多工程师，花上 1-2 天时间就做出来的图书翻译、人工智能语音对话的应用。 这样的开发方式，让更多的人有机会参与到 AI 应用的开发中来。无论你是产品经理、UI/UE 设计师、前端开发、后端开发还是大数据团队的人员，都可以通过学习一些基本的 AI 应用开发技能，为自己的职业生涯增添新的技能和竞争力。特别是在当前的科技革命和数字化转型浪潮下，AI 技术已经逐渐渗透到各个行业中，很多企业已经开始了 AI 落地实践，而能够掌握 AI 技术的人才也逐渐成为各个行业中的稀缺资源。因此，学习如何开发新一代 AI 应用，也是提升自己职业竞争力的一种重要途径。 最后，学习如何开发新一代 AI 应用还可以让我们更好地了解 AI 技术的本质和应用，拓宽我们的知识面和视野。AI 技术正在改变我们的生活和工作方式，了解和掌握这些技术，也有助于我们更好地适应未来的发展和变化。 学习如何开发新一代 AI 应用对于个人的职业发展和未来非常重要，因为 AI 技术已经开始在各个行业得到广泛应用。无论你从事什么行业，都可以利用 AI 技术来提高效率、降低成本、提供更好的服务，并在竞争中脱颖而出。掌握 AI 技术也可以让你在未来的就业市场上更有竞争力，拥有更广泛的职业选择。因此，学习如何开发新一代 AI 应用可以为个人的职业发展和未来打下坚实的基础。 2. 站在巨人的肩膀上 随着人工智能技术的迅猛发展，AI应用开发的范围也越来越广泛，涉及到自然语言处理、计算机视觉、语音识别等多个领域。这一轮的AI浪潮已经开始让我们看到了通用人工智能（AGI）的雏形，AI应用的覆盖领域被大大扩展了，几乎任何一个问题都有机会通过AI来解决优化。 过去，机器学习模型的应用通常局限于某一个细分领域上的进步，而且对于每一个具体问题都要单独收集数据、训练单独的机器学习模型来解决里面某一个小问题。然而，随着计算能力的提高和模型规模的增加，现在拥有海量参数的大模型已经开始成为主流。例如，2020年发布的GPT-3模型拥有1750亿个参数，可以无需任何微调，就能解决情感分析、命名实体识别、机器翻译等一系列的自然语言处理问题。同时，对于很多AI没有见过的问题，只要通过自然语言给到AI几个例子，通过\"小样本学习\"，AI就能给出正确的回答。这意味着，一个大模型可以一次性解决不同类型的很多问题。 在计算机视觉上，像2021年OpenAI发布的CLIP模型也有类似的效果。通过4亿个（图片、文本）对的训练，对于图片的分类可以任意扩展，而不需要预先标注。这样的模型使得我们对于图片的分类不再局限于预先的人工数据标注的类别，而是可以扩展到任何类别上去。 这种发展趋势使得AI应用开发的门槛逐渐降低，使得普通人也能够参与到AI应用的开发中来。无论你所在的行业和领域，都有机会通过简单的AI应用开发，提升效率和产出。同时，了解和掌握AI技术也成为了一种职业竞争力，可以帮助你更好地适应未来的工作环境。 总之，AI技术的广泛应用和快速发展已经让AI应用开发成为一个非常重要的技能。了解AI技术的人，无论是在工作中还是在日常生活中，都会受益匪浅。 3. 人工智能对我们的工作的影响 人工智能（AI）已经开始以多种方式改变我们的生活。我们已经习惯了依赖AI进行日常任务，如编写代码、翻译文本，甚至为文章生成图像。然而，AI的影响超出了我们的个人生活，它将对我们的工作产生重大影响。 随着AI的不断发展，它不可避免地将接管许多以前由人类执行的任务。公司已经在使用AI来优化产品描述、搜索算法和其他曾经是人工工作者领域的任务。这无疑会导致工作的流失，并改变许多人的工作性质。 尽管存在工作流失的可能性，但那些拥抱AI的人无疑将会获得好处。使用AI的团队和公司很可能会看到更高的效率和生产力，从而导致更大的产出和成功。无论您是产品经理、工程师、运营专家还是平面设计师，AI的出现都将从根本上改变您的工作性质。 AI将作为助手，帮助我们完成简单的基于知识的任务，甚至提供创造性的灵感。事实上，有些人将AI的发展与工业革命相比，标志着我们生活和工作方式的根本变化。虽然这可能会对一些工人造成危机，但有机会拥抱这种变化并学习在以AI为驱动的未来需求的新技能。 正如2008年App Store的发布创造了对移动应用程序开发人员的需求一样，AI革命为那些愿意学习和适应的人带来了新的机会。无论是获得机器学习、数据科学还是其他与AI相关的领域的专业知识，那些积极应对不断变化的就业市场的人无疑会蓬勃发展。 AI对我们的工作的影响不容小觑。尽管有些人可能将其视为危机，但有机会拥抱这种变化并学习在以AI为驱动的未来需求的新技能。每个人都可以决定如何应对这种变化，以及他们是否会抓住它带来的机会。 如何学习呢？ 新一代AI应用开发是一个快速发展的领域，需要不断更新自己的知识和技能。而通过实践学习是最有效的方法之一，因为它可以让你在实际解决问题的过程中学习和掌握技能。 这门课程采用实践教学的方式，让学生能够亲自动手解决一系列实际问题，如情感分析、记忆聊天机器人、图像搜索等。学生们将通过编写几行或几十行的代码来解决这些问题，并在在线Notebook的环境下进行代码运行，无需搭建复杂的开发环境。即使你是产品经理或业务方，也可以轻松地体验到新一代AI应用的开发过程，从而更好地理解和掌握其工作原理。 除了OpenAI的API外，这门课程还涵盖了语音、视觉等应用场景，包括语音识别、语音合成、AI绘画等。学生们将了解到如何使用开源模型以及如何根据自己的数据微调这些模型，从而更好地满足不同场景下的需求。 此外，这门课程还将探讨AI应用的套路和方法，例如分类、搜索、推荐、问答等问题。学生们将学习如何使用现有模型的能力来解决这些问题，并将这些方法和套路应用到现有的业务系统中，以提高应用的体验和效率。 随着课程的深入，学生们还将学习如何组合多个API、开源模型和开源库来解决复杂的真实问题。例如，如果你想实现一个电商客服，不仅需要检索知识库和问答的能力，还需要连接现有的订单和物流信息的能力。学生们将学习如何在AI应用的开发过程中将复杂的业务流程串联起来，以更好地应对实际问题。 拥抱新时代，接受“通用人工智能” 随着人工智能技术的不断进步，越来越多的人开始认识到“通用人工智能”（AGI）的重要性和可能性。如今，AGI已经不再是一个遥不可及的概念，而是一个即将到来的现实。我们可以看到，各个领域的科学家和工程师正在努力推进AGI的研究和应用，希望通过人工智能技术的创新和发展来实现这一目标。 在这个变化迅速的时代，我们需要尽快拥抱AGI。AGI可以帮助我们解决许多复杂的问题，并且能够极大地改善我们的生活质量。例如，我们可以使用AGI来开发更为智能的医疗设备，提高医疗诊断的准确性和效率；我们也可以利用AGI来优化城市交通，减少交通堵塞和污染；此外，AGI还能为我们提供更好的教育和娱乐体验，让我们的生活更加丰富多彩。 因此，我们需要尽快投入时间和精力来学习和应用AGI技术。学习AGI不仅可以让我们掌握更加先进的技能和知识，还可以激发我们的创新和热情，让我们更好地适应这个变化迅速的时代。我们需要通过学习和实践，尽快将AGI技术应用到我们的工作和生活中，让其发挥最大的价值。 最后，我希望每一个人都能积极地拥抱AGI技术，努力学习和应用这项技术，为推动人工智能技术的发展贡献自己的力量。相信在不久的将来，AGI将成为我们生活中不可或缺的一部分，让我们共同期待并努力实现这一目标。 目录 注： 这里将会是我未来所有系列教程的目录，便于大家更快找到相关章节 导读：了解AI并使用它/他/她们","link":"/Gettin_started_with_large-scale_artificial_intelligence_models_from_scratch/"},{"title":"09 使用Embedding实现语义检索","text":"Hi，我是茶桁。 过去的8讲，你已熟悉Embedding和Completion接口。Embedding适合用于机器学习中的分类、聚类等传统场景。Completion接口可以用作聊天机器人，也可以用于文案撰写、文本摘要、机器翻译等工作。 然而，很多同学可能认为这与他们的日常工作无关。实际上，我们通常在搜索、广告、推荐等业务中使用自然语言处理技术。因此，我们今天来看看如何使用OpenAI的接口来帮助这些需求。 当涉及到优化搜索结果时，OpenAI的Embedding接口可以提供有价值的功能。Embedding接口能够将文本转换为表示其语义特征的向量，这些向量可以用于比较文本之间的相似性，从而优化搜索结果的排序和相关性。 首先，使用OpenAI的嵌入接口，您可以将搜索查询和搜索结果中的文本转换为嵌入向量。通过比较查询向量与结果向量之间的相似度，您可以重新排列搜索结果，以提供更相关和有用的结果。这可以帮助用户更快地找到他们想要的信息，并提供更好的搜索体验。 其次，OpenAI的嵌入接口还可以帮助您改进搜索结果的相关性。通过将用户的上下文和历史记录与搜索查询结合起来，您可以生成更具个性化和定制化的搜索结果。使用嵌入接口，您可以将用户的上下文信息转换为嵌入向量，并与查询向量进行比较，以确定最相关的结果，并在搜索结果中突出显示这些个性化的内容。 此外，OpenAI的嵌入接口还可以用于相似性搜索和聚类分析。您可以使用嵌入向量来比较不同文本之间的相似性，并将相似的文本聚集在一起。这有助于在搜索结果中提供更多相关的选项，并帮助用户发现相关但可能未被明确搜索的内容。 下面，就让我们来一步步的实现： 生成实验数据 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Implementing-semantic-retrieval-using-Embedding/"},{"title":"2. 初识Python脚本","text":"Python的系列课程是写给零基础的小白看的，如果基础比较好的小伙伴可以暂时先不关注，等待后续课程。 Hi， 大家好，我是茶桁。 之前的课程已经给大家讲解了Python相关特性和基本语法。那么这节课呢，我们尝试着从最简单的脚本来开始认识Python。 在开始这节课之前呢，我是默认大家已经安装好了VSCode，并且配置好了Jupyter环境和Python的基本环境。如果在这一步有疑问的小伙伴，可以留言私信我。 我们在终端里输入: 12$ python -VPython 3.10.11 可以看到自己的Python版本。 这个时候，我们其实已经可以在终端里进行Python的代码编辑了，如下： 123$ python&gt;&gt;&gt; print(&quot;Hello AI Cheats&quot;)Hello AI Cheats 如下图： 我们这里需要理解一下，编写python程序的文件，称为python的脚本或程序。要求我们当前的python脚本的文件后缀名必须是.py，如果是Jupyter的文件，后缀是.ipynb print() 输出函数 print可以在程序中输出一些内容，如字符串，数字等等。 函数就是为了完成一些功能，例如： print就是为了输出数据。 变量 变量就是用一个英文字符串来记录或标记一些数据，并且这个被标记的数据是可以变化的。 比如 num = 10，就是把数据10赋值给了变量num来使用，之后就可以使用num来代替这个10的数据。 命名规范 这里我们强调一下Python的命名规范，所有在教授编程的教程中最初一定都会强调规范性。 变量的命名规范如下： 变量名可以使用字母，数字，下划线_， 不能以数字开头 严格区分大小写 不要使用中文 不要使用关键字 if else True False print 当然具体关键字并不仅仅是这些，这里列出了Python的关键字，大家可以自行查看一下，命名的时候需要进行避免。 变量的定义方式 在遵循了变量命名规范之下，我们可以有以下几种定义方式： 123456# 第一种定义方式a = 10b = 20# 第二种定义方式a,b = 30, 40 这里我们来思考一个问题，如何实现两个变量的数据相互交换呢？ 123456# 定义两个变量a = 10b = 20# 交换两个变量的值... 如果使用最普通的方式完成变量数据的交换，那么我们可以使用如下步骤： 把a变量的值 赋值给c ，此时 c变量中 就是 10 把b变量的值 赋值给a ， 此时 a变量中 就是 20 把c变量的值 赋值给b ， 此时 b变量中 就是 10 123456789# 普通方式交换数据a = 10b = 20c = aa = bb = cprint(a, b) 输出结果： 120, 10 我们还可以利用python定义比变量的语法来实现变量的数据交换 12345# 定义比变量的语法方式a = 10b = 20a,b = b,aprint(a, b) 输出结果： 120,10 Python的数据类型 数据类型就是数据的表现形式，比如 “你好” 就是一个字符串，200 就是一个数字。 在程序当中除了这种常用的字符和数字外还有很多其它的数据表现形式。 在Python中，我们可以使用type()函数来返回当前数据的数据类型： 123s = 'ilovechaheng'res = type(s)print(res) 输出结果： 1&lt;class 'str'&gt; 字符串类型 单双引号都可以定义字符串 三引号也可以定义字符串 单双引号定义的字符串不能随意换行，需要在换行时指明换行符 字符串中的引号可以互相嵌套，但是不能嵌套自己（例如不能在单引号中嵌套单引号，除非转义） 字符串中可以使用转义字符，如 .. 如果不想在字符串中实现转义字符可以在字符定义时 加 love = r'\\nihao \\shijie' 123456789# 单引号和双引号进行定义love = 'iloveyou'hello = &quot;你好 世界&quot;# 使用三引号实现大字符串的定义，一般用于大文本字符串的定义，并且大字符串，可以换行s = '''比如说这一个很长很长的文章内容。。。''' 数字类型 int 整型 float 浮点类型 complex 复数 bool 布尔类型（True，False） 12345678910111213141516171819# 数字类型 Numbervarn = 521varn = -1111varn = 3.1415926varn = 0x10 # 十六进制varn = b'001100111' # bytes# 复数varn = 5+6j # complex# 布尔类型 boolvarn = Truevarn = False# print(varn,type(varn))# 数值类型可以参与运算a = 10b = 20print(a+b) # 输出结果 30 List列表类型 列表用来表示一系列数据，例如： 需要记录一组数字或其它数据 列表中存储的数据可以是任意类型的 在需要记录多个数据时，可以使用中括号进行定义 [], 并且每个数据之间使用逗号分隔 , 例如以下数据，定义了几组数字 列表中存储的每一组数据，称为元素 列表中存储的数据，可以通过下标的方式进行获取 那么列表中元素的值可不可以存储一个列表,称为 二级列表（二维列表） 或者 多级列表 （多维列表） 关于列表中的下标，正读和反读的正负号是不一样的： 12345678# 关于列表中的下标''' 0 1 2 3 4 ['a','b',521,'pai',3.1415926] -5 -4 -3 -2 -1'''a = ['a','b',521,'pai',3.1415926]print(a[-3]) 输出结果： 1521 tuple 元组类型的定义 在定义多个数据内容时，可以选择使用List列表类型 还可以使用元组类型来定义， 元组和列表非常像，都时用于存储多个数据时使用 元组使用小括号进行定义（），列表使用中括号进行定义 元组的最大特点就是值不能被改变 123vart = (1,2,3,'a','b')# 元组的其它定义方式vart = 1,2,3 注意在定义元组时，如果元组中只有一个元素，那么需要加, 不然就不是元组类型了 Dict字典类型 字典也是用于存储一组或多组数据时使用，使用大括号 {}来定义 字典是 键值对 的存储方式 name ：admin 键和值之间使用冒号进行分隔，多组键值对之间使用逗号分隔 键必须是字符串或数字类型，值可以是任意类型 键名不能重复，值可以重复 12345678910# 比如需要记录一本书的相关数据 书名，作者，价格，。。。vard = {'title':'&lt;&lt;鬼谷子&gt;&gt;','author':'鬼谷子','price':'29.99'}# print(vard,type(vard))# {'title': '&lt;&lt;鬼谷子&gt;&gt;', 'author': '鬼谷子', 'price': '29.99'} &lt;class 'dict'&gt;# 获取字典中的值print(vard['title'])# 字典中的键不能重复使用，否则会覆盖vard = {'a':10,'b':10,'c':20,'a':'aa',1:'abcdef','2':'2222'}print(vard) 输出结果： 12&lt;&lt;鬼谷子&gt;&gt;{'a': 'aa', 'b': 10, 'c': 20, 1: 'abcdef', '2': '2222'} 在python之前的版本中，字典是无序的 set集合类型 set集合是一个 无序且元素不重复的 集合的数据类型 set集合使用 中括号或者set()方法来定义 如果需要定义一个空集合时 只能使用 set()方法,因为大括号时定义的空字典 集合主要用于运算，交集，差集，并集，对称集合 123456789101112131415161718a = {1,2,3,'a'}# 给集合添加元素# a.add('b')# 无法获取集合中的单个元素，但是可以添加和删除# a.discard('a')# print(a)# 检查当前的元素是否在集合中# print(1 in a)# 集合主要用于运算，交集，差集，并集，对称集合a = {1,2,3,'a','b'}b = {1,'a',22,33}print(a &amp; b) # 交集 {1, 'a'}print(a - b) # 差集 {'b', 2, 3} a 集合有，b集合没有的print(a | b) # 并集 {1, 2, 3, 33, 'a', 'b', 22} 两个集合，放到一起，并且去重print(a ^ b) # 对称差集 {33, 2, 3, 'b', 22} 输出结果： 1234{1, 'a'}{2, 3, 'b'}{1, 2, 3, 'a', 33, 22, 'b'}{33, 2, 3, 22, 'b'} 总结 最后，让我们来进行总结一下，关于Python的数据类型可以查看如下列表： 12345678910111213141516字符串 string数字类型 Number 整型 int 浮点 float 复数 布尔 bool列表 list元组 tuple字典 dict集合 set可变数据类型：列表，字典，集合不可不数据类型： 字符串，数字，元组容器类型数据 ： 字符串，列表，元组，集合，字典非容器类型数据： 数字，布尔类型 数据类型转换 什么是数据类型转换？ 把一个数据类型转换为另一个数据类型，例如 字符串转为数字 为什么需要数据类型转换？ 因为不同的数据类型之间不能运算 数据类型转换的形式？ 自动类型转换 强制类型转换 自动类型转换 当两个不同的值进行运算时，结果会向更高的精度进行计算：True ==&gt; 整型 ==&gt; 浮点 ==&gt; 复数 12345a = 123b = True # 在和数字运算时 True转为数字1，False转为数字 0print(a+b)print(12.5+22)print(True+3.14) 输出结果： 12312434.54.140000000000001 强制类型转换 python中的每个数据类型都有对应的方法，可以对数据类型进行转换 str()可以把所有的其它数据类型转换为字符串类型 int()字符串转数字类型时，如果字符串中是纯数字，可以转换，其它容器类型不能转为数字int类型 float()浮点类型的转换和int类型一样，不过转换的结果是浮点类型 bool() 可以把其它类型转换布尔类型的True或False 以下情况转bool的结果是 False: '',0,0.0,False,[],{},(),set() list()列表 数字类型是 非容器类型，不能转换为列表 字符串 转换为列表时 会把字符串中的每一个字符当做列表的元素 集合 可以转换为 list列表类型 元组 可以转换为 list列表类型 字典 可以转换为 list列表类型,只保留了字典中的键 tuple()元组 数字类型 非容器类型，不能转换为元组 其它容器类型的数据进行转换时，和列表一样 set()集合 数字类型 非容器类型，不能转换为 集合 字符串,列表，元组 可以转为 集合 结果是无序的 字典转换为集合时，只保留了字典的键 key dict()字典 数字类型 非容器类型，不能转换为 字典 字符串不能直接转换为 字典 列表可以转换为字典，要求是一个二级列表，并且每个二级元素只能有两个值 元组可以转换为字典，要求是一个二级元组，并且每个二级元素只能有两个值","link":"/Introduction-to-Python-scripting/"},{"title":"在 Apple Silicon M1&#x2F;M2 Mac 上安装和运行Stable Diffusion","text":"说实话，我找了好多关于如何在 M1/M2 上安装和运行 Stable Diffusion 的教程和帖子，发现相互之间借鉴的不少，但是能用的确实没几个。 寻找一番后，发现其实没那么复杂。也不知道为什么网上的那么多教程搞得那么复杂，又是这个又是那个的一大堆，简单实现的方式有好几种： 1. Diffuers 这是可以在 App Store 上直接搜索并下载的一个 App，看评分和排名似乎都不太好，开发者却是「Hugging Face」，其实在官方的 Github 上就有其下载链接：‣ 这应该是体验 Stable Diffusion最简便的方式了吧。而且还支持选择 Model， 不过有点遗憾的点是没办法调整参数。 2. DiffusionBee 这是出现的比较早的一款第三方 App，使用起来也是特别简单，直接下载安装就行了： DiffusionBee - Stable Diffusion App for AI Art DiffusionBee is the easiest way to generate AI art on your computer with Stable Diffusion. Completely free of charge. https://diffusionbee.com/download 目前不止是 MacOS，还有对应 Windows 64 Bit 的版本，而且，你可以选择下载 HQ Version 版本。官方对其说的是速度慢两倍，但是图像质量更好。 以上两个 App 第一次使用的时候都是需要下载 Model 的，之后就可以直接开心的玩耍了，相比较而言，DiffusionBee 在参数选择上要多一点。支持 Text to Image, Image To Image等。 # 安装 AUTOMATIC1111 这个方法是需要有一点动手能力了，不过相比较而言，也不是那些网站上介绍的那么繁琐。其实就只需要几步而已。 1. 下载 Homebrew 一个包管理器，不太明白的朋友不需要管那些，操作就行了 打开你的终端，不明白什么是终端直接在你的搜索框里输入”终端”，或者”Terminal”, 就能看到了。 然后直接把下面的代码粘贴进去，回车，看着他跑就行了 1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; 2. 安装一些必须的软件包 等上面步骤跑完之后，再复制下面的代码，一样粘贴进去回车看着他跑： 1brew install cmake protobuf rust python@3.10 git wget 3. 下载 AUTOMATIC1111 存储库 等上面步骤跑完，在你的终端输入一下代码并回车 1cd ~ 以上命令是为了让你进入你在 Mac 电脑上的账户主目录，就是这个地址 1/User/xx/ 然后我们接下来的操作会在你这个目录下下载一个文件夹，名字叫 「stable-diffusion-webui」，这贴下面代码到你的终端里，然后回车 1git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui 命令跑完后，你就会渐渐自己名字的目录下多了一个 stable-diffusion-webui 的目录了，然后在终端进入这个目录，操作方法和上面一样 1cd ~/stable-diffusion-webui/ 在你的访达里你也进入这个目录，然后继续进入 models/Stable-diffusion， 这里是存放 Model 的地方。现在你需要下载一个 Model 存放进去，你可以直接在这里下载 1.5 model, 当然，如果你需要 2.1 的或者其他 model，可以去点击下面的链接进去自己下载一个合适的，然后扔到目录里。 Models - Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/models?sort=downloads&amp;search=stable+diffusion 然后你的目录应该会是这样： 然后你就可以尝试着跑你的 stable diffusion 了，刚才我们在终端里进入了 ~/stable-diffusion-webui/， 假设你还在这个位置，我们就可以直接输入： 1./webui.sh 然后去干些自己的事情吧，喝杯茶，看看书。要跑一会呢，特别是你网络不好的情况。 等到终端命令全部跑完后，打开你的 Safari，输入：http://127.0.0.1:7860/ 好了，可以把玩了。 4. 其他 是的，还没结束，还有一些要说的，其实在 Mac App Store 里搜索的话，你还能看到一些其他的 App 可以直接使用，比如： 反正都是免费的，尽量多试试，找到一个自己满意的。 另外，不管你用那种方法，你都需要知道一些good prompts for Stable Diffusion, 这里有一个地方可以看些别人的例子，不过不是那么容易打开： arthub.ai https://arthub.ai/ 还有啊，自己可以多测试一些 model，下下来把玩下。 祝你玩的愉快。全文完。","link":"/How-to-install-and-run-Stable-Diffusion-on-Apple-Silicon/"},{"title":"Knock 升级 -- 快速输入管理员密码","text":"本文知乎专栏 还记得之前介绍过的Knock么？只需要敲击两下就能快速解锁Mac的app。 其实这款App还是 @Rachel 介绍给我的，当时就觉得很酷，可是用下来之后，并没有觉得有很高的实用性。 不过，这次Knock升级了，除了解锁Mac之外，还可以再你需要root权限的时候免去输入管理员密码的麻烦，你所需要的，仅仅是敲击两下你的iPhone。 好吧，现在可以为自己的Mac设定一个超级复杂的密码了。","link":"/Knock-update/"},{"title":"16. Langchain让AI拥有记忆力","text":"你好，我是茶桁。 在之前的两讲中，我们深入了解了 Langchain 的 LLMChain 核心功能，它可以帮助我们链式地调用一系列命令，包括直接调用 OpenAI 的 API、调用其他外部接口或自己实现的 Python 代码。但这只是完成一个小任务所需的调用序列。除了这些，LangChain 还封装了许多其他功能，以便于我们开发 AI 应用。例如，让 AI 有“记忆力”，即记住我们的聊天上下文。我们在第 6 讲中制作的聊天机器人的例子就是这样。为了让 ChatGPT 知道整个聊天的上下文，我们需要将历史对话记录传递给它。但由于 Token 数量有限，我们只能保留最后几轮对话。最终，我们将此功能抽象为一个 Conversation 类。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Langchain%E8%AE%A9AI%E6%8B%A5%E6%9C%89%E8%AE%B0%E5%BF%86%E5%8A%9B/"},{"title":"LLMs的实用介绍","text":"在实践中使用LLMs的3个级别 这是关于在实践中使用大型语言模型（LLMs）系列文章的第一篇。在这里，我将介绍LLMs并提出三个使用它们的级别。未来的文章将探讨LLMs的实际方面，例如如何使用OpenAI的公共API、Hugging Face Transformers Python库、如何微调LLMs以及如何从头构建LLMs 什么是LLM？ LLM 是 Large Language Model 的缩写，是人工智能和机器学习中的最新创新。这种强大的新型人工智能在2022年12月随着 ChatGPT 的发布而迅速传播开来。 对于那些生活在人工智能热潮和技术新闻周期之外的人来说，ChatGPT 是运行在名为 GPT-3 的 LLM 上的聊天界面（现在在撰写本文时已升级到 GPT-3.5 或 GPT-4）。 如果你使用过 ChatGPT，显然这不是来自 [AOL Instant Messenger]（https://en.wikipedia.org/wiki/AIM_(software)) 或你的信用卡客服的传统聊天机器人。 这个聊天机器人感觉不同。 什么使得LLM“大”？ 当我听到“大型语言模型”这个术语时，我的第一个问题是，这与“常规”语言模型有何不同？ 语言模型比大型语言模型更通用。就像所有正方形都是矩形，但并非所有矩形都是正方形一样。所有LLM都是语言模型，但不是所有语言模型都是LLM。 所以LLM是一种特殊的语言模型，但是什么使它们与众不同呢? 有2个关键属性区分LLMs与其他语言模型。一个是数量上的，另一个则是质量上的。 数量上，LLM的区别在于模型中使用的参数数量。目前的LLM大约有10-1000亿个参数[1]。 质量上，当语言模型变得“大”时，会发生一些非凡的事情。它会展示出所谓的*** emergent properties***例如零-shot学习[1]。这些是当语言模型达到足够大的规模时，似乎突然出现的特性。 零样本学习 GPT-3（以及其他LLM）的主要创新在于它能够在各种情境下进行零样本学习[2]。这意味着ChatGPT可以执行一个任务，即使它没有被明确训练过。 尽管这对我们这些高度进化的人类来说可能不是什么大不了的事情，但是这种零样本学习能力与之前的机器学习范例形成了鲜明对比。 以前，为了获得良好的性能，模型需要明确地在它所要完成的任务上进行明确的训练。这可能需要1k-1M个预标记的训练示例。 例如，如果你想让计算机进行语言翻译、情感分析和识别语法错误。每个任务都需要一个专门的模型，它需要在大量标记示例的基础上进行训练。然而，现在，LLM可以在没有明确训练的情况下完成所有这些任务。 LLM如何工作？ 训练大多数最先进的LLM所使用的核心任务是单词预测。换句话说，给定一序列单词，下一个单词的概率分布是什么？ 例如，给定序列Listen to your ____，最有可能的下一个单词可能是：heart，gut，body，parents，grandma等。这可能看起来像下面显示的概率分布。 有趣的是，这是许多（非大型）语言模型过去被训练的方式（例如GPT-1）[3]。然而，由于某种原因，当语言模型超过一定大小（例如~10B个参数）时，这些（新生的）能力，例如零-shot学习，开始出现[1]。 尽管目前还没有明确的答案，解释为什么会发生这种情况（只有推测），但明显LLM是一种强大的技术，具有无数的潜在用例。 使用LLM的3个层次 现在我们来看看如何在实践中使用这种强大的技术。虽然有无数的LLM用例，但在这里，我将它们按所需的技术知识和计算资源排序为3个层次。我们从最容易使用的开始。 一级：提示工程 使用LLM的第一级别是“提示工程”，我将其定义为“任何使用LLM的开箱即用方式”，即不更改任何模型参数。虽然许多技术倾向的个人似乎对提示工程的想法不屑一顾，但这是实际中使用LLM（在技术和经济上）最可访问的方法。 有两种主要的提示工程方式： 简单方式 和 较不简单方式。 简单方式：ChatGPT（或其他方便的LLM UI） - 这种方法的关键好处是方便。像ChatGPT这样的工具提供了一种直观，免费且无代码的使用LLM的方法（没有比这更容易的方法了）。 然而，方便通常是有代价的。在这种情况下，这种方法有两个主要缺点。第一个是缺乏功能。例如，ChatGPT不容易使用户自定义模型输入参数（例如温度或最大响应长度），这些值调节LLM输出。第二，与ChatGPT UI的交互不能轻松地自动化，因此无法应用于大规模使用情况。 虽然这些缺点可能是某些用例的杀手级应用，但如果我们将提示工程向前推进一步，这两个缺点都可以得到改善。 较不简单方式：直接与LLM交互 - 我们可以通过编程接口直接与LLM进行交互来克服ChatGPT的一些缺点。这可以通过公共API（例如OpenAI的API）或在本地运行LLM（使用像Transformers这样的库）来实现。 虽然这种提示工程方式不太方便（因为它需要编程知识和潜在的API成本），但它提供了一种可定制，灵活和可扩展的使用LLM的方法。本系列文章将讨论付费和免费的方法来进行此类提示工程。 尽管提示工程（如此定义）可以处理大多数潜在的LLM应用程序，但依赖通用模型可能会导致特定用例的次优性能。对于这些情况，我们可以进入使用LLM的下一个级别。 等级 2：模型微调 使用 LLM 的第二个等级是模型微调，我定义为对现有 LLM 进行微调以用于特定用例，通过改变至少一个（内部）模型参数，即权重和偏差。在此类别中，我还将在此处将迁移学习即使用现有 LLM 的某些部分来开发另一个模型。 微调通常包括两个步骤。步骤 1：获得预先训练的 LLM。步骤 2：基于给定的特定任务更新模型参数（通常是数千个）高质量标记的示例。 模型参数是定义 LLM 对输入文本的内部表示的。因此，通过针对特定任务调整这些参数，内部表示变得针对微调任务进行了优化（或者至少是这样的想法）。 这是一种强大的模型开发方法，因为相对较少的示例和计算资源可以产生出色的模型性能。 然而，缺点是它需要比提示工程更多的技术专业知识和计算资源。在未来的一篇文章中，我将尝试通过审查微调技术并共享示例 Python 代码来缓解这种缺点。 虽然提示工程和模型微调可能可以处理 LLM 应用程序的 99％，但有时必须走得更远。 等级 3：构建自己的 LLM 在实践中使用 LLM 的第三种最终方法是构建自己的。在模型参数方面，这是您从头开始制定所有模型参数的地方。 LLM 主要是其训练数据的产物。因此，对于某些应用程序，可能需要策划自定义的高质量文本语料库进行模型训练，例如医学研究语料库，用于开发临床应用程序。 这种方法最大的优点是您可以完全自定义 LLM 以适用于您的特定用例。这是终极的灵活性。但是，通常情况下，灵活性的代价是方便性。 由于LLM 性能的关键是规模，因此从头开始构建 LLM 需要巨大的计算资源和技术专业知识。换句话说，这不会是一个个人周末项目，而是一个完整的团队工作数月甚至数年，预算达到 7-8F。 尽管如此，在我未来文章中，我希望探讨从头开始开发 LLM 的流行技术。 最后让我们来总结一下： 虽然LLM现在被吹得足够大，但它们是AI领域的一项强大创新。在这里，我提供了有关LLMs是什么以及如何在实践中使用它们的入门指南。日后我希望写一些文章提供初学者指南，帮助大家启动下一个LLM用例。 资源 链接：「个人博客」 社交：「推特」|「微博」| 「领英」|「油管」 之后我会出一些AI相关的具体视频教程，目前还未找到合适的平台托管，敬请期待。关注我，我会第一时间通知到家。 在我的公众号内的文章大部分是免费阅读的（除非有实际成本支出），如果您觉得对您有帮助，可以给我赞赏一下以表支持。 引用 [1] 大型语言模型调查。 arXiv:2303.18223 [cs.CL] [2] GPT-3论文。 arXiv:2005.14165 [cs.CL] [3] Radford，A.，&amp; Narasimhan，K。（2018）。通过生成式预训练改善语言理解。 （GPT-1论文）","link":"/LLMs%E7%9A%84%E5%AE%9E%E7%94%A8%E4%BB%8B%E7%BB%8D/"},{"title":"02 大语言模型做情感分析","text":"上一节中，我们介绍了大型语言模型的接口非常简单，仅提供了Complete和Embedding两个接口。但这样看似简单的接口，实际上可以解决很多自然语言处理问题。例如，情感分析、文本分类、文章聚类、摘要生成、搜索等问题，都可以使用大型语言模型解决。接下来的几节课中，我们将介绍如何使用这两个简单的API来解决传统的自然语言处理问题。本节我们将从最常见的自然语言处理问题“情感分析”开始介绍，看看如何使用大型语言模型。 传统的二分类方法：朴素贝叶斯与逻辑回归 朴素贝叶斯与逻辑回归可以用来解决“情感分析”问题。这些算法的基本思想是，根据给定的标记数据，学习一个分类器，用来将新的输入数据进行分类。对于情感分析问题，分类器的目标是将一段文字分为正面或负面情感。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Large-language-model-for-sentiment-analysis/"},{"title":"Foundation of Artificial Intelligence - Lecture 1","text":"Algorithm --&gt; Data Structure No obvious solution ==&gt; Algorithm engineers do it If there is a clear implementation path ==&gt; the person who develops the project will do it What's the Algorithm? {Ace of hearts, 10 of spades, 3 of spades, 9 of hearts, 9 clubs, 4 of diamonds, J} First: Hearts&gt; Diamonds&gt; Spades&gt; Clubs Second: Numbers are arranged from small to large Some people put the colors together first Some people arrange the size first, and extract the colors one by one \\[ 1024 --&gt; 10^3 --&gt; 1k \\] \\[ 1024 * 1024 --&gt; 10^6 --&gt; 1M \\] \\[ 1024 * 1024 * 1024 --&gt; 10^9 --&gt; 1G \\] 123456struction-0 00011101struction-1 00011111 struction-2 00011100struction-3 00011101struction-4 00011100struction-5 00011001 2.6G Hz 12345def fac(n): # return n! if n == 1: return 1 # 返回操作 else: return n * fac(n-1) # 乘法操作 + 返回操作 + 函数调用 12345678910fac(1)&gt; 1fac(100)&gt; 93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000fac_100 = &quot;&quot;&quot;93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000&quot;&quot;&quot;len(fac_100)&gt; 158 12345?? N --&gt; fac(n)# 乘法操作 + 返回操作 + 函数调用?? (N - 1)--&gt; fac(n-1)?? N == 100 fac(N) ??? 99 1234Object ` N --&gt; fac(n)` not found.Object ` (N - 1)--&gt; fac(n-1)` not found.Object ` N == 100 fac(N)` not found.Object `? 99` not found. \\[ Time(N) - Time(N-1) = constant \\] \\[ Time(N-1) - Time(N-2) = constant \\] \\[ Time(N-2) - Time(N-3) = constant \\] \\[ Time(2) - Time(1) = constant \\] \\[ Time(N) - Time(1) == (N-1)constant \\] \\[ Time(N) == (N-1)constant + Time(1) \\] \\[ Time(N) == N * constant + (Time(1) - constant) \\]","link":"/Lecture_1/"},{"title":"M1安装Homebrew(ARM)","text":"?&gt; 详情可见作者说明 安装 ARM版本Homebrew必须安装在/opt/homebrew路径下 123cd /optsudo mkdir homebrewsudo curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C homebrew 如果不进行sudo授权，则会报错； 环境变量 本人使用zsh, 所以编辑文件~/.zshrc. 添加如下内容： 12path=('/opt/homebrew/bin' $path) export PATH ?&gt; 如果是使用bash，请修改~/.bashrc 在终端内执行: 1source ~/.zshrc 现在可以试试执行brew install graphviz试试看能否正常安装回归树可视化模块； 软件包和迁徙 软件包依然需要使用X86版Homebrew 1arch -x86_64 启用一个X86模式中端，之后运行的命令都在X86模式下运行，再次安装Homebrew 1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; !&gt; 注意：要将ARM版本Homebrew环境变量设置到最前面，此时两个版本共存时会有限启动ARM版本，需要运行X86版本时，需要手动输入完整路径arch -x86_64 /usr/local/bin/brew 可以在配置文件中设置alias 12abrew='/opt/homebrew/bin/brew' # ARM Homebrewibrew='arch -x86_64 /usr/local/bin/brew' # X86 Homebrew 如果对已有软件包做迁徙，则： 1ibrew bundle dump 此时在目录下就得到一个名为Brewfile的备份文件，导入内容并安装 1abrew bundle --file /path/to/Brewfile !&gt; 执行之前需要编辑Brewfile文件，将cask和mas开头的记录删除掉；","link":"/M1_install_homebrew/"},{"title":"5. 模块化编程","text":"HI, 大家好。我是茶桁。 上一节中我们学习了Python基本的流程控制，并且预告了这一节的内容，就是将要学习「模块化编程」。那什么是模块化编程呢？按照维基百科的说法： 模块化编程（英语：modular programming），是强调将计算机程序的功能分离成独立的、可相互改变的“模块)”（module）的软件设计技术，它使得每个模块都包含着执行预期功能的一个唯一方面（aspect）所必需的所有东西。 说的简单一点，就是把程序进行封装（函数封装、面向对象、文件...） OK，话不多说，让我们开始吧。 函数 什么是函数？ 函数的英文单词为function, 我们将其翻译过来，就是“函数，功能”。 其实，函数就是一个具有特定功能的代码块。 函数的作用 函数的目的是封装，将特定功能的代码块进行封装，其目的是提高代码的重用性，从而提高开发效率，并且降低了后期的维护成本。 函数的定义和使用 函数的定义其实非常简单，我们用代码来写一下： 12345# 定义函数[基本结构]def 函数名([参数列表]): 当前行数的具体功能的代码 当前行数的具体功能的代码 ... 当然，函数在写完之后并不会自动执行，只是把函数定义了而已。如果想要使用定义完成的函数，需要用语法来进行函数的调用。 那么函数该如何调用呢？如下： 1函数名() 示例： 12345678# 函数的定义格式def love(): print('i') print('love') print('u')# 函数的调用love() 当前程序运行输出结果： 123iloveu 以上代码可以得到函数的第一个特征：函数定义后，不调用不执行。还记得咱们上节课强调的流程控制吗？代码最基本流程顺序是自上而下的，所以，这个时候我们如果调用放在上方，例如： 12345678# 函数的调用love()# 函数的定义格式def love(): print('i') print('love') print('u') 此时因为love()调用的时候函数还未被定义，所以会执行报错： 1NameError: name 'love' is not defined 所以我们需要注意：不能在函数定义前调用函数。 另外，我们需要注意，函数的调用不受次数的影响，比如，我们定义好函数后，这个时候在后面调用三次： 123love()love()love() 那执行后的结果应该是连着打印了三次结果。 和变量一样，函数的命名也是要遵守命名规范的： 字母数字下划线，不能以数字开头 严格区分大小写，且不能使用关键字 命名最好有意义，且不要使用中文 现在我们想想，在love()函数被定义后，我们再来定义一个同名的函数会怎么样？ 我们尝试一下，在刚才定义好的函数下方重复写一个同名的函数： 1234567891011121314# 函数的定义格式def love(): print('i') print('love') print('u')def love(): print('u') print(&quot;don't&quot;) print('love') print('me')# 函数的调用love() 直接结果： 1234udon'tloveme 那，我们得到了实验结果：同样的函数名被再次定义之后，冲突的函数会被覆盖。 所以，最后我们总结一下函数的特征及注意事项： 123456781. 函数定义后，不调用不执行2. 不能在函数定义前调用函数3. 函数的调用不受次数影响4. 函数的命名要遵守命名规范 - 字母数字下划线，不能以数字开头 - 严格区分大小写，不能使用关键字 - 命名最好有意义，且不要使用中文5. 函数名不要冲突，冲突后会被覆盖 函数的参数 在定义函数的时候，我们需要注意给函数的参数。可以在参数列表的位置进行定义，这个称为形参。如果一个函数有形参，那么在调用的时候必须传递参数（实参）。实参将值传递给实参的过程，本质上就是变量赋值操作。 函数参数概念及分类 带有参数的函数，该如何定义？ 在定义函数时，在小括号内可以定义形参（形式上的参数） 12345def love(w): print(f'i love {w}')# 调用带有形参的函数时，需要传递参数（实参）love('马户') 执行结果为： 1i love 马户 在这整个函数中，小括号内的w就是形参，在调用的时候的马户就是实参，在调用过程中将值传给了形参w。 那么，如果我在调用的时候没有传递实参，就会直接报错： 123love()TypeError: love() missing 1 required positional argument: 'w' 形参可以是多个，这就是定义带有多个参数的函数： 1234def love(m, n): print(f'{m} love {n}')love('i', 'u') 执行结果： 1i love u 如果形参是多个的话，那么有多少个形参就必须传递几个实参。并且参数都是按照顺序进行传递的。 如果少传一个参数，则同样会被错。 那，能不能多传呢？也不行，如果多传了参数，一样会报错。 至此，我们可以做如下总结： 函数参数：调用时需要传递的数据 函数参数的大类分为形参和实参 形参意思：函数定义时的参数 实参意思：函数调用时的参数 形实关系：函数调用时，形参和实参个数需要一一对应 函数中的参数类型 在确定了什么是形参和实参之后，我们来看看，这两种参数都有哪些类型。 函数参数在类型上，包括： 普通参数 默认参数 收集参数 命名关键字参数 关键字参数收集 普通参数 先来说说普通参数，其实就是位置参数，也叫顺序参数，也是必须传递的参数。 1234def love(m, n): print(f'{m} love {n}')love('i', 'u') 这段代码中，m, n就是普通参数，必须传递。 默认参数 有些函数在定义的时候，行参上就已经定义了默认值，那么这种就叫做默认参数。 在调用函数的时候，默认参数是可以不传值的。当传值之后，默认值就会被改变： 1234def func(x, y=20): print(x, y)func(2) 这段代码中的行参y就是默认参数，我们在调用函数func()只写了一个实参，也就是只传了一个值给函数。这个时候执行结果为： 12 20 我们修改一下，传两个值进去看看结果： 1234def func(x, y=20): print(x, y)func(2, 100) 执行结果： 12 100 可以看到，本来我们定义的行参y的默认值被改变了。 在定义默认参数的时候需要注意，函数中的默认参数只能全部定义在普通参数的后面，否则在调用函数的时候就会报错，比如以下这些情况： 1234567891011121314151617# 第1种错误情况def func(x=100, y=200, z): print(x, y, z)func(100,200,300)# 第2种错误情况def func(x=100, y, z=200): print(x, y, z)func(100, 200, 300)# 第3种错误情况def func(x, y=100, z): print(x, y, z)func(300,200,50) 收集参数 收集参数就是专门收集在函数调用时传递的多余的实参，或者我们可以理解为，不确定需要传递多少个实参，直接用一个行参来接收。 比如，我们现在有个需求就是需要计算用户输入的数字总和，我们按前面那个函数的定义方式为： 1234def func(x, y z=100): print(x+y+z)func(20,30) 这个函数中，我们输入2个值或者3个值都可以，但是当我们只输入一个值或者三个以上的时候，程序就会报错了。 那么有没有什么办法，不管用户输入多少个数字，我们都可以进行相加计算呢？ 1234567def func(x=&quot;+&quot;, *args): if x == '+': print('加法运算', args) else: print('减法运算', args)func(&quot;-&quot;, 2, 3, 4, 5, 6, 7, 8) 这段代码执行结果为： 1减法运算 (2, 3, 4, 5, 6, 7, 8) 虽然中间的运算代码我没有写，但是大家可以看到，已经可以接受不固定的多个参数了。 这个*args就是我们的收集参数。 在定义函数的时候，如果需要收集参数，那么这个形参前面需要加一个*号，例如*args。这里需要注意一点，*args并不是固定写法，你可以随意定义一个，只要前面有*号就可以了。比如下面这样： 1234567def func(x=&quot;+&quot;, *y): if x == '+': print('加法运算', y) else: print('减法运算', y)func(&quot;-&quot;, 2, 3, 4, 5, 6, 7, 8) 一样可以执行并得到一样的结果，这个时候，我们的*y就是收集参数。 收集参数也有两类，一种是普通的收集参数：专门用于收集多余的普通参数，形成一个新的元组。 1语法： 参数前面加*, 例如：*args 还有一种是关键字收集参数：用于专门收集多余关键字实参，形成一个新的字典： 1语法：参数前面加**, 例如：**kwargs 现在我们已经理解了普通的收集参数，那么在学习关键字收集参数之前，我们先来学习一下命名关键字参数 命名关键字参数 命名关键字是放在*号后面的参数，调用的时候强制必须传入制定参数名才能进行调用。 1234def func(a, b, c=3, *args, name): print(a, b, c, &quot;\\n&quot;, *args, &quot;\\n&quot;, name)func(1, 2, 3, 4, 5, 6, 7, 8, name='茶桁') 这段代码执行结果： 1231 2 3 4 5 6 7 8 茶桁 我们特意在中间加了换行字符来清晰的辨别*args和name。 如果在这段代码中我稍微变一下，在执行函数的时候，实参里不标明name可以吗？ 1234def func(a, b, c=3, *args, name): print(a, b, c, &quot;\\n&quot;, *args, &quot;\\n&quot;, name)func(1, 2, 3, 4, 5, 6, 7, 8, '茶桁') 执行之后我们收到了报错： 1TypeError: func() missing 1 required keyword-only argument: 'name' 这段报错明显告诉我们，确实了一个必须的关键字参数name。 那为什么会出现这种报错呢？这是因为在关键字参数之前，我们使用了*args来进行收集参数，那么无论你写多少，这些值都会被*args接收变成元组，那么后面的name自然就无法接受到值了。 让我们再来做一个实验，给命名关键字参数加上一个默认值，那么我们就能明显的看出问题： 1234def func(a, b, c=3, *args, name='_茶桁'): print(a, b, c, &quot;\\n&quot;, *args, &quot;\\n&quot;, name)func(1, 2, 3, 4, 5, 6, 7, 8, '茶桁') 这段代码执行结果： 1231 2 3 4 5 6 7 8 茶桁 _茶桁 可以看到，name给了默认值之后不再出现报错，而我们的实参也并未传到name里，而是全部被*args接收了。最后打印出了name的默认值_茶桁。 利用命名参数的这种定义参数名称接收值的特点，我们就可以打乱之前普通参数传值的顺序性，比如： 12345def func(x, y): print(x, &quot;\\t&quot;, y)func(2, 3)func(y = 2, x = 3) 执行结果为： 122 33 2 还是最开始的普通参数的写法，但是最后执行函数的时候，我们给实参指定了名称，这样传参顺序就没那么重要了。 所以，我们总结一下： 关键字参数定义在收集参数后面 关键字参数必须通过形参的名字来进行传递 关键字参数收集 前面我们在讲收集参数的结尾处提到了关键字参数收集，形式为**kwargs。 1234567def func(a, b, c=3, *args, name, age, **kwargs): print(a, b, c) print(args) # 普通收集参数，会把多余的参数收集为元组 print(name, age) print(kwargs) # 关键字参数收集，会把多余的关键字参数收集为字典func(1, 2, 4, 112, 123, 321, 541, 231, name=&quot;茶桁&quot;, age=18, sex='male', height=185, x='x', y='y') 执行结果： 12341 2 4(112, 123, 321, 541, 231)茶桁 18{'sex': 'male', 'height': 185, 'x': 'x', 'y': 'y'} 从执行结果上我们可以看到，在name和age之后的所有参数都被传递到了**kwargs里，然后作为字典打印了出来。 在声明这个函数和执行函数的时候需要注意，这些参数都是有顺序的，如果在执行函数的时候再多传一个非关键字参数，这个时候程序就会报错，如果是关键字参数，则照样会被**kwargs接收。 在我们介绍完这些参数之后，我们最后再说明一下： 形参声明的位置顺序：普通参数 -&gt; 默认参数 -&gt; 收集参数 -&gt; 命名关键字参数 -&gt; 关键字收集参数 def func(a, b, c=1, *args, d, **kw) 这段声明中，a,b为普通参数，c是默认参数，args是收集参数，d是命名关键字参数，kw是关键字收集参数 极少情况下会同时出现上面五种形参，一般情况下为：普通参数，收集参数，关键字收集参数 所有参数的摆放问题: 实参：普通实参在前，关键字参数在后 形参：关键字收集参数一定在最后出现，收集参数推荐在普通参数之后使用。 推荐顺序：普通形参、收集参数、关键字收集参数 函数的返回值 一个函数除了可以完成一定功能之外，还可以用来安需要返回一些内容。在函数中，使用return关键字来制定返回数据，可以返回任意类型的数据。 函数的返回值会把数据返回到调用的地方，可以使用变量进行接收，或者作其他处理。 函数可以分为两类： 执行过程函数：函数体内完成一定的功能即可，没有返回值 具有返回值的函数：函数体内完成一定的功能，并且返回一个结果到函数调用处。 比如： 12def func(a, b): print(f'{a} love {b}') 以上函数就是一个没有返回值的函数，这个函数只是为了完成打印这句话的功能。 那么有返回值的函数是什么样子？ 如果需要在函数中制定返回内容，我们需要使用return关键字。 12345678# 有返回值的函数def func(a, b): res = f'{a} love {b}' # 可以在函数体内，使用return返回内容 return resr = func('老鼠', '布丁')print(r) 执行结果为： 1老鼠 love 布丁 在这段代码中，我们在func()的函数体内最后利用关键字return返回了任意内容，并且使用变量r接收了这个返回值，最后讲r打印了出来。 在调用func()这个函数的时候，函数中的返回值会返回到函数调用处。 我们再来研究一下return这个关键字，我们在return的前后都加上一段打印代码，看看会发生什么。 12345678def func(a, b): res = f'{a} love {b}' print('这是return前') return res print('这是return后')r = func('老鼠','布丁')print(r) 执行结果： 12这是return前老鼠 love 布丁 看到结果我们可以清楚，return之后的的代码并未继续执行，也就是说，我们如果要在函数体内执行其他任务，必须放在return之前执行，否则根本不会执行。那么我们可以得出结论：return必须放在一个定义函数的最后面。 Tips: 其实，即便没有return关键字或者returen之后没有任何内容，也有返回值，只是返回的是None值。 None是一个特殊的数据，表示什么都没有。查询类型可以看到返回 &lt;class ‘NoneType’&gt; 变量的作用域 作用域就是当前起作用，可用的范围区域。也就是变量的有效范围。 变量按作用域可以分为： 局部变量： 在函数内部可以使用的变量 全局变量：在函数内外都可以使用的变量 局部变量 让我们尝试下，如果函数内定义的变量在函数外使用会如何： 1234def func(): a = 20 print(a) 执行结果： 1NameError: name 'a' is not defined 被告知a并未被定义。 可以看到，函数内定义的变量，在函数外连获取都做不到。这种在函数内定义的这个变量a，就是局部变量，它在函数外不能使用。 再让我们来看看将变量定义在函数外会是怎样的一种情况： 12345num = 10def func(): print(num)func() 执行结果： 110 在func函数内，我们获取到了变量num的值并打印。那说明，在函数内我们可以获取函数外部的变量。 我们继续在继续看： 123456num = 10def func(): num += 20 print(num)func() 执行后报错： 1UnboundLocalError: local variable 'num' referenced before assignment 这样我们可以看到，变量num在函数内虽然可以使用，但是无法进行更改。 那在函数外定义的所有变量都是如此吗？再让我们试试看： 123456items = [1, 2, 3, 4, 5]def func(): items[0] = 20 print(items)func() 执行结果: 1[20, 2, 3, 4, 5] 由此我们看出，并不是所有的变量都不可在函数内进行更改。 其实，变量是分为两种的： 可变数据类型：在函数外定义的变量，在函数内可以使用并改变 不可变数据类型：在函数外定义的变量，在函数内只可以访问而无法改变 可变数据类型有列表和字典，其他的都是不可变数据类型。 全局变量 之前我们介绍的都是局部变量，那怎样定义全局变量呢？ 在函数内使用global直接定义的变量，就是全局变量，函数内外都可以直接使用。 在函数外定义的变量，在函数内使用global关键字进行声明，那么也是全局变量。 例如： 1234567num = 20def func(): global num num += 10 print(num)func() 这个时候我们可以得到执行结果： 130 那有小伙伴就问了，如果我在函数外直接使用global定义全局变量可以吗？让我们来试试看就知道了： 1234567global numnum = 20def func(): num += 10 print(num)func() 执行之后得到报错： 1UnboundLocalError: local variable 'num' referenced before assignment 这样我们就得到了结果：不可以。 在整个程序中，我们可以使用两个函数方法来获取数据： globals()用来获取全局数据，locals()用来获取当前作用域的数据 讲到这里，我们再来多看一组代码： 12345678# 函数的作用域def outer(): print('this is outer func...') def inner(): print('this is inner func...')outer()inner() 这段代码执行结果为： 123this is outer func...NameError: name 'inner' is not defined 正常执行了outer()内的打印，然后又报了一个错误，提示inner函数未定义。 说明，不只是变量有作用域，函数一样也有作用域。要想inner函数内的打印也起作用，我们需要在函数内就调用执行inner()。 1234567# 函数的作用域def outer(): print('this is outer func...') def inner(): print('this is inner func...') inner() # 在函数内执行outer() 这样，我们执行的结果就是： 12this is outer func...this is inner func... 如果我们在外层函数中定义一个局部变量，能在内层函数中使用吗？ 12345678910# 函数的作用域def outer(): a = 2 print('this is outer func...') def inner(): a += 1 print('this is inner func...') print(a) inner()outer() 执行之后得到报错： 1UnboundLocalError: local variable 'a' referenced before assignment 说明并不可以。 nonlocal关键字 那么，到底有没有什么办法在内函数中使用上一层函数中的局部变量呢？答案是有办法。 在内函数中如果想要使用外层函数的变量，那么需要使用nonlocal关键字，可以引用上一层函数中定义的局部变量。 123456789101112# 定义一个外层函数def outer(): # 外函数的局部变量 num = 10 # 内函数, 局部函数, 在函数的内部定义的函数 def inner(): # nonlocal 关键字在局部函数中使用 nonlocal num # 可以引用上一层函数中定义的局部变量 num += 1 print(num) inner()outer() 执行后返回结果： 111 至此，我们通过使用nonlocal关键字，成功拿到了外层函数定义的变量num并使用。最后打印出使用的结果。 这里我们要注意，nonlocal虽然可以引用上一层函数中定义的局部变量，但是这并不代表提升为了全局变量。 既然我们有了global关键字可以提升变量为全局变量，为什么还需要一个nonlocal关键字呢？是不是有点多此一举？ 这两者的功能上并不相同。global关键字修饰变量后标识该变量是全局变量，对该变量进行修改就是修改全局变量，而nonlocal关键字修饰变量后标识该变量是上一级函数中的局部变量，如果上一级函数中不存在该局部变量，nonlocal位置会发生错误（最上层的函数使用nonlocal修饰变量必定会报错）。 关于函数的文档 我们在一个未定义任何变量和函数的空白文档中打印一下全局数据： 1print(globals()) 执行结果： 1234567891011121314151617181920212223242526272829{ '__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': &lt;module 'builtins' (built-in)&gt;, '__builtins__': &lt;module 'builtins' (built-in)&gt;, '_ih': ['', 'print(globals())'], '_oh': {}, '_dh': [PosixPath('/Users/xx/git/AI_Cheats/Python'), PosixPath('/Users/xx/git/AI_Cheats/Python')], 'In': ['', 'print(globals())'], 'Out': {}, 'get_ipython': &lt;bound method InteractiveShell.get_ipython of &lt;ipykernel.zmqshell.ZMQInteractiveShell object at 0x105be29e0&gt;&gt;, 'exit': &lt;IPython.core.autocall.ZMQExitAutocall object at 0x105be3280&gt;, 'quit': &lt;IPython.core.autocall.ZMQExitAutocall object at 0x105be3280&gt;, 'open': &lt;function open at 0x10488e710&gt;, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/Users/xx/git/AI_Cheats/Python/globals.ipynb', '_i': '', '_ii': '', '_iii': '', '_i1': 'print(globals())'} 我们来看这个打印出来的json 类似于__name__这种前后有__ __的数据，称之为“魔术变量”。我们并未定义，但是已经存在了。 如果脚本作为主程序，那么__name__值是__main__，如果是当作一个模块在另外一个脚本中引用去使用，那么值就是当前文件的命名。 __doc__当前脚本的文档说明，在当前脚本当中的第一个三引号注释就是当前脚本的说明文档。比如，我们在这个空白的文档中写一段三引号注释 12345&quot;&quot;&quot;这里是整个文档的说明部分。&quot;&quot;&quot;def func(): pass 然后我们直接将doc打印出来查看： 1print(__doc__) 可以看到输出内容： 1这里是整个文档的说明部分。 这种文档其实不止适用于python文件，对于定义的函数依然适用。比如，我们执行定义了一个函数，并在函数内部用三引号进行注释： 12345678910def func(): &quot;&quot;&quot; 这里是让你写一写函数的文档说明的。 需要说明当前函数的作用， 如果当前函数还有形参，那么也需要对形参进行一一说明。 name: 这个是一个name参数，用于接收姓名 age: 这个参数是表示年龄 :return: 此处说明当前函数的返回值 &quot;&quot;&quot; pass 这个时候，我们在下方执行： 1print(func.__doc__) 可以看到我们在注释内定义的说明文档被打印出来了： 这样，我们不仅可以在自己写函数的时候在上方清晰的写明当前函数的作用及参数，我们还可以使用此方法，查找其他人所写的函数的一些说明。 在我们平时写代码的时候，养成一个好习惯是非常有必要的。 总结一下： print(__name__): 获取当前脚本的文件名 print(__doc__): 获取当前脚本的说明文档 print(func.__doc__)： 获取当前函数的说明文档 练习：函数封装 我们上一讲中的练习中，我们打印了乘法表，矩形图形，还计算了12生肖。这里我们就将乘法表来封装成函数，实现我们上节课留的其中一道思考题：反向打印。 我们先将打印乘法表封装起来： 123456789# 定义函数，打印九九乘法表def multiply_table(): &quot;&quot;&quot; 当前函数的功能是打印出乘法表 &quot;&quot;&quot; for x in range(1, 10): for y in range(1, x+1): print(f'{x}X{y}={x*y}', end=&quot; &quot;) print() 这样，我们在其他地方执行multiply_table()函数的时候，就可以直接打印出乘法表。 现在让我们给这个函数多加一些功能： 123456789101112131415# 定义函数，打印九九乘法表def multiply_table(i=0): &quot;&quot;&quot; 当前函数的功能是打印出乘法表 i=0; i 这个参数可以用来控制正向输出和方向输出，0的时候正向，1的时候反向,默认为0 &quot;&quot;&quot; if i: rs = range(9, 0, -1) else: rs = range(1, 10) for x in rs: for y in range(1, x+1): print(f'{x}X{y}={x*y}', end=&quot; &quot;) print() 我们执行函数的时候，输入1来试试看： 1multiply_table(1) 输出结果： 1234567899X1=9 9X2=18 9X3=27 9X4=36 9X5=45 9X6=54 9X7=63 9X8=72 9X9=81 8X1=8 8X2=16 8X3=24 8X4=32 8X5=40 8X6=48 8X7=56 8X8=64 7X1=7 7X2=14 7X3=21 7X4=28 7X5=35 7X6=42 7X7=49 6X1=6 6X2=12 6X3=18 6X4=24 6X5=30 6X6=36 5X1=5 5X2=10 5X3=15 5X4=20 5X5=25 4X1=4 4X2=8 4X3=12 4X4=16 3X1=3 3X2=6 3X3=9 2X1=2 2X2=4 1X1=1 可以看到，我们的控制结果被成功打印出来。至此，这个有一些小功能的九九乘法表就封装完成了。 那么这一节课就不留思考题了，大家课后熟练掌握一下封装函数和变量的作用域，我们下节课来学习一些高阶函数， 好，下课。","link":"/Modular-programming/"},{"title":"03 提示语，做个聊天机器人","text":"大家好，我是茶桁。 在本次讲座中，我们将研究 OpenAI 提供的 Completion API 接口。你可能已经体验过与 ChatGPT 进行聊天的乐趣，或是利用它帮我们编写各种文本材料的便利。现在，我们可以从这个需求出发，进一步探索 Completion API 的具体用法，以及如何利用它来生成更加优质的文本。 AI 客服 在我了解人工智能和自然语言生成之前，我听说过智能客服，然而我并没有亲身体验过。我想象中，智能客服的回答应该是按照固定的模板进行生成的，这样的缺点就是每次回答都会是一样的。虽然可以设计多个模板来轮换着表达相同的意思，但是最多也就是三四个模板，整体的体验可能会比较呆板。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Let-s-Build-a-Chatbot/"},{"title":"Power for Mac","text":"很多人都会使用\"Pow\"来进行本地静态页面开发环境。对于其配置确实简单到有爱。 不过Yosemite上\"Pow\"都不能正常工作，之前我参照官方的办法写了一篇如何在Yosemite上设置\"Pow\"的方法。有兴趣的可以参看我原文:http://hivan.me/setting-pow-at-Yosemite/ 不过一直用的好好的\"Pow\"近期又开始出现 404 错误，短暂的解决无果以后，我开始寻找一些快速能解决的办法，便遇到了\"Power\"，和\"Pow\"一样，都是建立快速开发环境，并且经过测试，在Yosemite 10.10.1下正常工作。 使用\"Pow\"来做开发环境的，可以暂时用这样一个替代方案，毕竟开发中没有时间去多做研究了，暂时不知道两者的区别，不过\".dev\"正常访问已经没有问题。 Power 项目地址: https://github.com/HackPlan/power/","link":"/Power-for-Mac/"},{"title":"心理学知识预告","text":"我原本是一个不太知名的产品经理，在我的职业生涯中，一直致力于帮助企业创造出引人入胜的产品，吸引和留住用户。然而，我很快就发现了一个重要的事实：想要设计和推出一款成功地产品，需要对人的心理有一定的了解。 我对心理学的兴趣开始于我的工作经验。随着我不断深入地研究用户心理，我开始对这个领域的其他方面产生了兴趣。这引发了我对心理学本身的好奇心，使我开始不断学习心理学的相关概念和理论。最终，这个兴趣成为了我个人的爱好和研究方向。 我想在个人博客上开始写相关内容的目的是想与读者分享我对心理学的研究和思考，希望能够让大家对这个领域产生更深入的了解。我希望这本书能够帮助读者更好地理解自己和他人的行为和思维方式，从而更好地解决生活中的问题。在这里我不得不要提一本书，就是《津巴多普通心理学》。这本书目前应该是出到第八版了，里面的知识基本上构建了我对心理学大部分的认知，最重要的，也构建了我对所谓“科学心理学”最正确的观念。 最后，我要感谢我的导师们，包括且不仅限于：邱岳东教授，刘晓霞老师，娄丽华老师，周琼老师，李元春老师。感谢《津巴多普通心理学》这本书，其中的内容对我完成这本书起到了至关重要的作用。在完成本书的过程中，我对心理学的相关内容和概念又有了深一层次的理解。","link":"/Psychological-Trailer/"},{"title":"1. Python的特性和语法","text":"千里之行始于足下。 大家好，我是茶桁，这里是我们《AI秘籍》的第一节，让我们先从Python来开始好好的打好基础。 第一堂课，我们先从最基础的Python特性开始，当然，还有一些基本语法。 上来就开始讲特性和语法，说明我们将会遗弃惯用的“环境搭建”等更基础的内容，那些内容网上已经很丰富了，一查一大堆，并且相对来说内容都比较独立，所以希望还不太会搭建开发环境的同学可以自己去搜索看看。或者，其实从我这篇《Apple M1的AI环境搭建》也能完全搭建起一个完整的Python开发环境。（Windows和Linux的同学就只能在网上搜索一下看看了。） 总体来说，Python语言的使用率越来越高，它不仅可以用于GUI开发、Web开发，还能进行数据预处理、数据探索性分析（EDA），更是进行数据挖掘、机器学习、深度学习等任务的首选语言。 基于Python的包也越来越丰富，各种优秀的库层出不穷。根据\"Tiobe编程语言排行榜\"的最新统计，Python结束了自己攀爬的势头，已经开始长期占据榜首。Python的发展势头让人们看到了它在各个领域都有着广阔的应用前景。 得益于Python语言的简洁语法和高效开发能力，使得集成系统变得非常方便。 与此同时，Python相关的就业机会也非常丰富，待遇也相当优渥。 因此，无论从易用性、就业机会还是薪酬待遇来看，Python都是IT从业者必备的编程语言之一。 课程说明 本课程所使用语言为Python3 本课程将会有一些案例，用于辅助学习和理解知识点。 基本所有案例均使用Jupyter Notebook做演示 一些项目会用到软件工程和设计模式的思想。 本课程无任何文学色彩，重点在于简单通俗易懂。 接下来，让我们真实开始吧。 Python的两大特性 一句话总结，就是Python是一门动态的、强类型语言。 动态语言 在了解“动态语言”之前，我们先来了解一下“类型检查”。 类型检查是验证变量或表达式的数据类型是否符合语言规定的类型约束的过程。它用于确保程序在运行时不会出现类型错误，例如将一个字符串与一个整数相加或将一个数字与一个布尔值进行比较。类型检查旨在捕捉潜在的类型不匹配错误，并在编译时或运行时提供相应的警告或错误信息。 如果类型检查发生在程序运行阶段（运行时），则称为\"动态类型语言\"（dynamically typed languages）。常见的动态语言包括： Python JavaScript Ruby PHP Lua Perl Shell脚本 有动态语言，则必然会有其相对的“静态语言”，常见的“静态类语言”包含： C C++ Java C# Swift Kotlin Rust TypeScript 当然，这些都只是一部分常见的动态语言和静态类型语言，还有许多其他编程语言也属于这两个类别。在实际开发中，选择使用动态语言还是静态类型语言取决于项目的需求、开发团队的喜好和项目的规模等因素。每种类型的语言都有其独特的特点和适用场景，因此选择合适的语言是非常重要的。 强类型语言 强类型语言（Strongly Typed Language）要求在编程过程中严格定义和遵守数据类型规则。在强类型语言中，变量必须明确地声明其数据类型，并且在运行时不能隐式地改变数据类型。这意味着变量在使用之前必须进行类型转换，以确保数据的一致性和安全性。 在强类型语言中，编译器或解释器会对数据类型进行严格的检查，如果发现不符合类型规则的操作，就会报错并拒绝执行。这样可以防止一些潜在的类型错误，确保程序的稳定性和正确性。 强类型语言的主要特点包括： 静态类型检查：在编译时或解释时进行类型检查，检查数据类型是否匹配，避免类型不匹配的错误。 显式类型转换：在进行类型转换时，必须显式地指定转换的方式，例如强制类型转换。 不支持隐式类型转换：强类型语言不允许在不明确声明的情况下将一个数据类型隐式地转换为另一个数据类型。 这么说可能并不直接，我们来拿个示例，我们输入两行代码： 12a = 5a = a + 's' 然后我们会看到程序抛出TypeError异常： 这个异常意思是不支持int变量和str变量相加。 常见的强类型语言包括： Java C++ C# Python Swift Kotlin TypeScript Rust Pascal Ada Delphi 一样的，对应的就是弱类型语言。弱类型语言容易与其他类型混合计算，其代表是JavaScript。（有一说一，我还挺喜欢JS的这个特性的） 弱类型语言包括： JavaScript PHP Perl Ruby Tcl Bash AWK MATLAB (在一些操作上可以被视为弱类型) 当然，对于这个划分其实并不是所有人都一致的，有些人还是会把Python归结为弱类型语言，而通常意义上，大家会把C++划分到弱类型。这里我们不去争论，仅仅记住动态/静态、弱类型/强类型的区别就行了。 基本语法 基本语法里，我们介绍一下Python的命名规则、缩进原则、特殊关键字和特殊运算符四个方面。 命名规则 Python的变量命名规则包括以下几条： 允许包括英文、数字以及下划线（_），不能以数字开头。 名称区分大小写，例如\"myVar\"和\"myvar\"是两个不同的变量。 以单下划线（_）开头的变量通常用作受保护的变量，表示应该将其视为私有，不建议直接访问。虽然Python没有严格的访问控制，但这是一种约定俗成的做法。 以双下划线（__）开头和结尾的变量是Python中的特殊标识符，具有特殊的意义，如类的私有成员或专用标识符。 Python的变量命名习惯一般遵守蛇形命名法（snake case）： 一般变量命名使用小写字母，多个单词之间用下划线连接，例如：book_id、book_store_count。 类名首字母大写，如Python内置模块collections.abc中的Iterable类，我们自定义的Book类等。 类方法名也使用小写字母，多个单词之间用下划线连接，例如：get_store_count()。 与Java的命名方法不同，Java通常使用驼峰命名法（camel case）来命名变量和方法名，其中第一个单词首字母小写，后续单词首字母大写，例如：myVar、getStoreCount()。而Python则更倾向于使用蛇形命名法。这是因为Python社区普遍认可了蛇形命名法，使得代码在风格上更加一致和易读。 缩进原则 Python最具特色的特点之一是使用缩进来表示代码的逻辑层次，而不是像Java和C++中使用{}。Python的缩进层级结构非常重要，它代表着代码的逻辑结构。 通常情况下，Python的缩进为4个空格字符。例如，在定义一个Book类并重写__add__方法来计算两本书的库存量时，代码如下所示： 12345678910111213141516171819class Book(object): # 定义类的参数 def __init__(self, b_id, b_name, b_store_count): self.b_id = b_id self.b_name = b_name self.b_store_count = b_store_count # 重写加法操作 def __add__(self, book): return self.b_store_count + book.b_store_count # 创建两个Book类的实例python_intro_book = Book(1, '金瓶梅', 100)ml_intro_book = Book(2, '玉蒲团', 200)# 求两本书的总销量sales_cnt = python_intro_book + ml_intro_bookprint(sales_cnt) 上述代码定义了一个Book类，包括初始化方法__init__和重写的加法操作__add__。通过这种缩进结构，我们可以清晰地看到代码的层次结构和逻辑。 在Python编码中，缩进格式、行间空行数、变量和等号之间的空格等都遵循PEP8（Python Enhancement Proposal 8）规范。可以使用autopep8包来自动实现PEP8规范，保持代码的规范和易读性。 特殊关键字 Python有35个关键字，这些关键字具有特殊的含义，不能用于自定义变量名，否则会引起语法错误。以下是Python的关键字列表： 1234567False await else import passNone break except in raiseTrue class finally is returnand continue for lambda tryas def from nonlocal whileassert del global not withasync elif if or yield 这些关键字在Python编程中扮演着重要的角色。其中，True和False用于表示布尔值的真和假，类似于Java中的true和false；None表示空值，类似于Java中的null；Python使用not表示逻辑反操作，而Java使用!；Python使用and表示两个条件同时满足，Java使用&amp;&amp;；Python使用or表示两个条件满足其一，Java使用||；Python使用elif，而Java使用else if。 还有一些比较特殊的关键字，例如： del用于删除可迭代对象中的某个元素； def用于定义函数； 带有yield的关键字用于定义生成器（generator）函数； global和nonlocal是在Python函数式编程的闭包场景中使用的。 pass关键字用于占位，当你在定义函数或类时暂时不想添加具体的实现时，可以使用pass关键字。 这些关键字的具体用法将在后续文章中更详细地介绍。在此，先对它们有一个整体的认识即可。 特殊运算符 Python的运算符包括： 123+ - * ** / // % @&lt;&lt; &gt;&gt; &amp; | ^ ~ :=&lt; &gt; &lt;= &gt;= == != 大部分运算符在其他编程语言中也是常见的，不过这里重点介绍三个比较特殊的运算符：//、**和:=。 //运算符用于两个数值相除并向下取整，类似于Python的math.floor()功能： 12print(5 // 2) # 输出: 2print(5 // 4.5) # 输出: 1.0 **运算符用于进行幂运算： 1print(2 ** 3) # 输出: 8 :=运算符是在Python 3.8版本中引入的，被形象地称为“海象运算符”。它可以在表达式中同时为变量赋值和比较： 123n = len(a)if n &gt; 10: print(f&quot;{n}大于10&quot;) 可以用“海象运算符”简化为： 12if (n := len(a)) &gt; 10: print(f&quot;{n}大于10&quot;) Python的比较运算符还支持链式比较，使得编码更加方便： 123i = 3print(1 &lt; i &lt; 3) # 输出: Falseprint(1 &lt; i &lt;= 3) # 输出: True 此外，运算符@用于装饰器功能，本专栏会深入解释它的本质，并提供相关案例，帮助你学会使用装饰器。 总结 在本文中，我们一起学习了Python这门功能强大的编程语言。Python的两大特性是动态语言和强类型语言。 动态语言意味着在运行时执行类型检查，而不是在编译时。这使得Python更加灵活和易于使用，允许我们在代码中动态创建和修改变量。Python的动态特性使其成为进行数据预处理、数据探索性分析、数据挖掘、机器学习和深度学习等任务的首选语言。 另一方面，强类型语言意味着变量的类型在声明时就已经确定，并且不能进行隐式类型转换。这确保了代码的稳定性和安全性，帮助我们避免一些常见的错误。 在Python的基本语法方面，我们学习了变量命名规则，缩进原则，特殊关键字和特殊运算符。Python的命名规则允许使用英文、数字和下划线，但不能以数字开头，并且区分大小写。特殊关键字包括Python的35个关键字，如if、else、for、while等等，它们有着特定的含义和用途。特殊运算符中，//用于整数除法，**用于幂运算，:=是在Python 3.8版本中引入的“海象运算符”，使得在表达式中同时为变量赋值和比较变得更加方便。 通过学习Python的特性和基本语法，我们已经具备了编写简单到复杂的程序的基础知识。Python的易用性、丰富的库和社区支持，使其成为一个优秀的编程语言，适用于各种应用场景。无论是初学者还是有经验的开发者，Python都是一个值得深入学习和探索的语言。 希望本文能够为读者提供了一个对Python的初步认识，并激发了你继续学习和研究的兴趣。在接下来的学习过程中，我们可以更深入地了解Python的各种功能和应用领域，并用Python来解决更复杂的问题。 好了，我是茶桁，咱们下节见。","link":"/Python-features-and-syntax/"},{"title":"3. Python3 运算符","text":"Hi，大家好。我是茶桁。 前两节我们学习了基本的Python特性和语法，并且认识了一些基本的Python脚本。今天，我们来学习一下Python的运算符，而我们选择的版本为Python3。 什么是运算符 为了能让我们的学习顺利进行下去，首先我们需要先弄明白：什么是运算符。 这里举一个简单的栗子：\\(4 + 5 = 9\\), 在这个简单的数学计算栗子中，4和5倍称为操作数，+就被成为是运算符, 最后9就是它的运算结果。 到这里，我们对于运算符应该有了一个基本的认知，那么Python语言都支持哪些运算符呢？如下列表： 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 接下来，就让我们来一个个的学习Python的运算符 算术运算符 运算符 描述 实例 + 加 - 两个对象相加 a + b 输出结果 31 - 减 - 得到负数或是一个数减去另一个数 a - b 输出结果 -11 * 乘 - 两个数相乘或是返回一个被重复若干次的字符串 a * b 输出结果 210 / 除 - x 除以 y b / a 输出结果 2.1 % 取模 - 返回除法的余数 b % a 输出结果 1 ** 幂 - 返回x的y次幂 a**b 为10的21次方 // 取整除 - 向下取接近除数的整数 &gt;&gt;&gt; 9//2 4 &gt;&gt;&gt; -9//2 -5 1234567891011# 算术运算符a = 10b = 21print(&quot;a+b=&quot;, a+b)print(&quot;a-b=&quot;, a-b)print(&quot;a*b=&quot;, a*b)print(&quot;b/a=&quot;, b/a)print(&quot;b%a=&quot;, b%a)print(&quot;a**b=&quot;, a**b)print(9//2)print(-9//2) 输出结果： 12345678a+b= 31a-b= -11a*b= 210b/a= 2.1b%a= 1a**b= 10000000000000000000004-5 比较运算符 运算符 描述 实例 == 等于 - 比较对象是否相等 (a == b) 返回 False。 != 不等于 - 比较两个对象是否不相等 (a != b) 返回 True。 &gt; 大于 - 返回x是否大于y (a &gt; b) 返回 False。 &lt; 小于 - 返回x是否小于y。所有比较运算符返回1表示真，返回0表示假。这分别与特殊的变量True和False等价。注意，这些变量名的大写。 (a &lt; b) 返回 True。 &gt;= 大于等于 - 返回x是否大于等于y。 (a &gt;= b) 返回 False。 &lt;= 小于等于 - 返回x是否小于等于y。 (a &lt;= b) 返回 True。 1234567# 比较运算符print(&quot;a==b:&quot;, a==b)print(&quot;a!=b:&quot;, a!=b)print(&quot;a&gt;b:&quot;, a&gt;b)print(&quot;a&lt;b:&quot;, a&lt;b)print(&quot;a&gt;=b:&quot;, a&gt;=b)print(&quot;a&lt;=b:&quot;, a&lt;=b) 输出结果： 123456a==b: Falsea!=b: Truea&gt;b: Falsea&lt;b: Truea&gt;=b: Falsea&lt;=b: True 赋值运算符 运算符 描述 实例 = 简单的赋值运算符 c = a + b 将 a + b 的运算结果赋值为 c += 加法赋值运算符 c += a 等效于 c = c + a -= 减法赋值运算符 c -= a 等效于 c = c - a *= 乘法赋值运算符 c = a 等效于 c = c a /= 除法赋值运算符 c /= a 等效于 c = c / a %= 取模赋值运算符 c %= a 等效于 c = c % a **= 幂赋值运算符 c = a 等效于 c = c a //= 取整除赋值运算符 c //= a 等效于 c = c // a 1234567891011121314151617# 赋值运算符c = a+bprint(c)c+=aprint(c)c-=aprint(c)c*=aprint(c)c/=aprint(c)c%=aprint(c)c=aprint(c)c//=aprint(c) 输出结果： 1234567831413131031.01.0101 位运算符 按位运算符是把数字看作二进制来进行计算的。bin()函数可以把数字转为二进制。 Python中的按位运算法则如下： 下表中变量 a 为 60，b 为 13二进制格式如下： 12345678910111213a = 0011 1100b = 0000 1101-----------------a&amp;b = 0000 1100a|b = 0011 1101a^b = 0011 0001~a = 1100 0011 运算符 描述 实例 &amp; 按位与运算符：参与运算的两个值,如果两个相应位都为1,则该位的结果为1,否则为0 (a &amp; b) 输出结果 12 ，二进制解释： 0000 1100 | 按位或运算符：只要对应的二个二进位有一个为1时，结果位就为1。 (a | b) 输出结果 61 ，二进制解释： 0011 1101 ^ 按位异或运算符：当两对应的二进位相异时，结果为1 (a ^ b) 输出结果 49 ，二进制解释： 0011 0001 ~ 按位取反运算符：对数据的每个二进制位取反,即把1变为0,把0变为1。~x 类似于 -x-1 (~a ) 输出结果 -61 ，二进制解释： 1100 0011， 在一个有符号二进制数的补码形式。 &lt;&lt; 左移动运算符：运算数的各二进位全部左移若干位，由\"&lt;&lt;\"右边的数指定移动的位数，高位丢弃，低位补0。 a &lt;&lt; 2 输出结果 240 ，二进制解释： 1111 0000 &gt;&gt; 右移动运算符：把\"&gt;&gt;\"左边的运算数的各二进位全部右移若干位，\"&gt;&gt;\"右边的数指定移动的位数 a &gt;&gt; 2 输出结果 15 ，二进制解释： 0000 1111 1234567891011# 位运算符print(bin(20))a = 60b = 13print(&quot;a = &quot;, bin(a), &quot;, b = &quot;, bin(b))print(&quot;a&amp;b =&quot;,bin(a&amp;b))print(&quot;a|b =&quot;,bin(a|b))print(&quot;a^b =&quot;,bin(a^b))print(&quot;~a =&quot;,bin(~a))print(&quot;a&lt;&lt;2 = &quot;,bin(a&lt;&lt;2))print(&quot;a&gt;&gt;2 = &quot;,bin(a&gt;&gt;2)) 输出结果： 123456780b10100a = 0b111100 , b = 0b1101a&amp;b = 0b1100a|b = 0b111101a^b = 0b110001~a = -0b111101a&lt;&lt;2 = 0b11110000a&gt;&gt;2 = 0b1111 逻辑运算符 Python语言支持逻辑运算符，以下假设变量 a 为 10, b为 20: 运算符 逻辑表达式 描述 实例 and x and y 布尔\"与\" - 如果 x 为 False，x and y 返回 False，否则它返回 y 的计算值。 (a and b) 返回 20。 or x or y 布尔\"或\" - 如果 x 是 True，它返回 x 的值，否则它返回 y 的计算值。 (a or b) 返回 10。 not not x 布尔\"非\" - 如果 x 为 True，返回 False 。如果 x 为 False，它返回 True。 not(a and b) 返回 False 123456# 逻辑运算符a = 10b = 20print(&quot;a and b = &quot;, a and b)print(&quot;a or b = &quot;, a or b)print(&quot;not(a and b) = &quot;, not(a and b)) 输出结果： 123a and b = 20a or b = 10not(a and b) = False 成员运算符 除了以上的一些运算符之外，Python还支持成员运算符，测试实例中包含了一系列的成员，包括字符串，列表或元组。 运算符 描述 实例 in 如果在指定的序列中找到值返回 True，否则返回 False。 x 在 y 序列中 , 如果 x 在 y 序列中返回 True。 not in 如果在指定的序列中没有找到值返回 True，否则返回 False。 x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 12345# 成员运算符x = [0, 1, 2, 3, 4, 5, 6, 7]y = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]print(&quot;x in y :&quot;, x in y)print(&quot;x not in y :&quot;, x not in y) 输出结果： 12x in y : Falsex not in y : True 身份运算符 身份运算符用于比较两个对象的存储单元 运算符 描述 实例 is is 是判断两个标识符是不是引用自一个对象 x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False is not is not 是判断两个标识符是不是引用自不同对象 x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 注： id() 函数用于获取对象内存地址。 1234567891011# 身份运算符x = 10y = xprint(&quot;x is y:&quot;, x is y)x = 10y = 10print(&quot;x is y:&quot;, x is y)print(&quot;id(x) == id(y)&quot;, id(x) == id(y))print(&quot;x is not y:&quot;, x is not y)id(x) 输出结果： 123456x is y: Truex is y: Trueid(x) == id(y) Truex is not y: False4312793616 is 与 == 区别： is 用于判断两个变量引用对象是否为同一个， == 用于判断引用变量的值是否相等。 运算符优先级 以下表格列出了从最高到最低优先级的所有运算符： 运算符 描述 ** 指数 (最高优先级) ~ + - 按位翻转, 一元加号和减号 (最后两个的方法名为 +@ 和 -@) * / % // 乘，除，取模和取整除 + - 加法减法 &gt;&gt; &lt;&lt; 右移，左移运算符 &amp; 位 'AND' ^ | 位运算符 &lt;= &lt; &gt; &gt;= 比较运算符 == != 等于运算符 = %= /= //= -= += *= **= 赋值运算符 is is not 身份运算符 in not in 成员运算符 not and or 逻辑运算符 注意：Pyhton3 已不支持 &lt;&gt; 运算符，可以使用 != 代替 本教程相关代码可在此查看 ​","link":"/Python-operators/"},{"title":"4. Python的流程控制","text":"Hi，大家好。我是茶桁。 在前面几节课的基础之上，我们今天开始尝试在Python中控制流程。这中间，让我们来做一些实际的练习。 Python语句的分类 让我们先了解一下Python语句的分类。 在Python中，可分为单行代码和代码块/组, 顾名思义，单行代码就是一行的Python代码，而代码块是以冒号作为开始，用缩进划分相同作用域，这样的结构称之为代码块，是一个整体。 12345678# 单行代码a = 123# 代码块if a == 123: print('True')else: print('False') 以上代码中输出结果为： 1True 在输入代码块的时候，我们要注意使用缩进。在其他语言中代码块可能是{}，但是在Python中严格遵守的缩进规则就是代码块。缩进可以是一个Tab距离或者四个空格，可是注意绝对不能混合使用，必须自使用一种方式缩进。 流程控制的分类 什么是流程？流程就是计算机执行代码时候的顺序。 流程可以被分为以下几类： 顺序结构 分支结构/选择结构 循环结构 顺序结构 顺序结构是系统的默认程序结构，自上而下进行执行。 分支结构 分支结构可以让代码走向不同的方向，不同的分支区间。 分支结构中又包含了单向分支，双分支和多分支以及巢状分支。 单向分支 单向分支就是在条件满足之后，执行后续任务。条件不满足的情况下，则不执行。 比如： 123456if 条件表达式： 一条python代码 a = Trueif a: print(&quot;True&quot;) 执行结果： 1True 一个经典案例： 程序员下班前女朋友打电话：下班路上买十个包子回来，如果看到卖西瓜的买一个 123456baozi = 10mxg = Falseif mxg: baozi = 1 print(&quot;买 %s 个包子&quot; %(baozi)) 输出结果： 1买 10 个包子 正常情况下，我们是直接买了10个包子回家，那如果我们看到了卖西瓜的呢？那么这段代码中等于是我们重新赋值了mxg, 就变成： 12345678910baozi = 10mxg = False# 走在路上看到了卖西瓜的，重新赋值mxg = Trueif mxg: baozi = 1 print(&quot;买 %s 个包子&quot; %(baozi)) 输出结果： 1买 1 个包子 双分支 双分支就是在单向分支的基础之上，又多了一个“否则”的选项，当条件不满足的时候执行其他操作。 123456789101112if 条件表达式: 一条python代码else: 另外一条python代码 person = 'girl'if person == 'girl': # 真区间 print(&quot;上前搭讪：美女，能加个微信吗？&quot;)else: # 假区间 print(&quot;直接走开。&quot;) 执行结果： 1上前搭讪：美女，能加个微信吗？ 以上就是一个双向的流程控制，这里面的含义为：表达式成立则执行真区间，如果不成立则执行假区间。 多分支 多分支就是在双分支的基础之上再增加其他可能出现的判断条件，用于执行更多的其他操作。 1234567891011121314if 条件表达式： 一条python代码 ...elif 条件表达式： 一条python代码 ...elif 条件表达式： 一条python代码 ......else: 一条python代码 ... 这段代码中的elif就是可能出现的不同条件，示例如下: 1234567891011score = 59if score &gt;= 90 and score &lt;= 100: print(&quot;奖励一个手机&quot;)elif score &gt;= 80 and score &lt; 90: print(&quot;今晚吃一顿好的奖励一下&quot;)elif score &gt;= 70 and score &lt; 80: print(&quot;鼓励：下次努力加油。&quot;)elif score &gt;= 60 and score &lt; 70: print(&quot;盯紧复习，争取下次进步。&quot;)else: print(&quot;奖励一顿‘竹笋炒肉’&quot;) 执行结果： 1奖励一顿‘竹笋炒肉’ 可以看到以上代码中，是从上到下依次进行判断条件，当所有条件都没有满足的时候，最后走到了else区间。 这就是多分支，需要判断多个表达式的结果，会自行其中符合条件的一个。 巢状分支 巢状分支，也就是嵌套分支。也就是if条件语句的嵌套： 12345678if 条件表达式： 代码语句 if 条件表达式: 代码语句 else: 代码语句else： 代码语句 示例： 1234567891011121314age = 25height = 177sex = 'male'if sex == 'male': # 可以往后判断 if age &gt;= 22 and age &lt;= 35: # 年龄比较合适 if height &gt;= 175: print(&quot;处一下试试...&quot;) else: print(&quot;拉到...&quot;)else: print('当闺蜜吧。') 输出结果： 1处一下试试... 在嵌套分支中我们需要注意，3 ～ 5层嵌套就是极限了，不要再往后嵌套。如果这个层数无法解决你的问题，那么可以重新梳理一下逻辑。基本大部分时候都是逻辑上有问题了。 分支 练习：十二生肖 当用户输入一个四位数的年份，计算当前这个年份对应的生肖： 申猴 酉鸡 戌狗 亥猪 子鼠 丑牛 寅虎 卯兔 辰龙 已蛇 午马 未羊 我们先来做一个用户输入的操作 123# 获取用户输入的年份year = input(&quot;请输入四位数的年份: &quot;)print(year, type(year)) 添加一个type()函数是为了验证用户输入之后的数据类型，当我们输入2023之后，可以看到输出结果为： 12023 &lt;class 'str'&gt; 证明虽然我们输入的是数字，但是被转成了字符串，那这个时候，我们就需要处理一下了： 123# 获取用户输入的年份year = int(input(&quot;请输入四位数的年份: &quot;))print(year, type(year)) 输出结果： 12023 &lt;class 'int'&gt; 这下就对了。 原本我们是需要讲位数，以及范围都控制在合理的数据内的。因为时间关系，在这整个示例中，我就不再去做更多的验证判断了。 123456789101112131415161718192021222324252627282930313233343536# 获取用户输入的年份year = int(input(&quot;请输入四位数的年份: &quot;))#print(year%12)num = year % 12&quot;&quot;&quot;申猴 酉鸡 戌狗 亥猪 子鼠 丑牛 寅虎 卯兔 辰龙 巳蛇 午马 未羊&quot;&quot;&quot;if num == 0: print(f'{year}年是 ==&gt; 申猴')elif num == 1: print(f'{year}年是 ==&gt; 酉鸡')elif num == 2: print(f'{year}年是 ==&gt; 戌狗')elif num == 3: print(f'{year}年是 ==&gt; 亥猪')elif num == 4: print(f'{year}年是 ==&gt; 子鼠')elif num == 5: print(f'{year}年是 ==&gt; 丑牛')elif num == 6: print(f'{year}年是 ==&gt; 寅虎')elif num == 7: print(f'{year}年是 ==&gt; 卯兔')elif num == 8: print(f'{year}年是 ==&gt; 辰龙')elif num == 9: print(f'{year}年是 ==&gt; 巳蛇')elif num == 10: print(f'{year}年是 ==&gt; 午马')elif num == 11: print(f'{year}年是 ==&gt; 未羊')else: print(&quot;您为输入正常的年份&quot;) 当我输入2023的时候，程序输出结果： 12023年是 ==&gt; 卯兔 程序是正常运行了（排除我没做特殊处理可能会出现的BUG），但是我们看这段代码，已经不能用丑陋来形容了。 让我们再改动一下，还记得咱们第二节课程中所学的list吗？这段代码中我们去判断的num是不是和list的下标是一模一样？OK，让我们利用下标来重新写一下这段代码： 123456# 获取用户输入的年份year = int(input('请输入四位数的年份：'))# 定义十二生肖 列表items = ['申猴', '酉鸡', '戌狗', '亥猪', '子鼠', '丑牛', '寅虎', '卯兔', '辰龙', '巳蛇', '午马', '未羊']print(f'{year}年是%s年' %(items[year % 12])) 这段代码输出结果为： 12023年是卯兔年 是不是比起第一段代码来优雅多了？ 循环结构 在完成了分支结构之后，我们来看一下循环结构。循环结构非常重要，必须熟练掌握。 为什么我们需要循环呢？先来看一段代码： 123456print(1)print(2)print(3)print(4)print(5).... 这段代码中，我们重复做了很多次打印的工作。这种事情，其实完全没必要重复去做，交给循环就可以了。 目前在Python中常用的循环有两个，while循环和for...in循环。 while 循环 12345while 条件表达式: 代码内容 代码内容 代码内容 ... 在while循环中，我们通常都会写带有条件变化的循环 1234num = 1while num &lt;=10: print(f'num为{num}') num += 1 输出结果： 12345678910num为1num为2num为3num为4num为5num为6num为7num为8num为9num为10 在这样一段代码中，在进入循环的时候，判断了一下当前条件是否成立。我们先设定了num的值为1，满足进入循环的条件，所以就进入了循环体，然后输出了num的值。 之后，每循环一次我们都对num做一次+1的处理. 也就是更改了变量。当变量更改后，会重新走到循环体的开始去判断条件。在循环11次之后，num就变成了11，不符合进入循环的条件了，循环自然被终止。也可以说，更该变量也是在朝着循环结束的方向在前进。 那么如果我们没有设定这个num的条件变化呢，自然就是无限的循环下去，最终导致内存溢出。 for循环 通常来说，for循环是用来遍历一个容器类型的数据。 1234for 自定义变量 in 容器数据: 代码内容,可以使用自定义变量 代码内容,可以使用自定义变量 代码内容,可以使用自定义变量 使用for...in循环遍历容器类型数据，那么中间的自定义变量就是当前容器类型中的每一个元素。 示例： 123n = '123456789'for i in n: print(i) 输出结果： 123456789123456789 在整个for...in循环体内，我们经常使用range()函数来迭代输出一个范围，比如： 12for i in range(0, 10): print(i) 输出结果为： 123456789100123456789 可以看到我们输出了从0开始，一直到9结束， 一共输出了10个数字。 从结果中，我们大致可以猜到range()函数中(a, b)的含义为：从a开始循环输出，输出到b（不包含b）为止, 比如，我们将刚才的数字改为range(1,8)，那么我们最后输出的内容就会是： 12345671234567 其他流程控制语句 在循环体中，我们还经常应用一些其他的控制语句，用于程序的正常执行和中止。这其中包括 continue语句， 用于跳过当前这一次循环 break语句，用于结束或者跳出 pass语句， 用于占位 12345678num = 1while num &lt;= 10: num += 1 # 判断当前的num是否为偶数 if num % 2 == 0: continue # 跳过本次循环，执行下一次循环 print(num) 输出结果： 12345357911 可以从结果中看到，每次num为偶数的时候，打印并未执行，被跳过了。 让我们来更改一下这一段代码： 123456789num = 1while num &lt;= 10: num += 1 # 判断当前的num是否为偶数 if num % 2 == 0: continue # 跳过本次循环，执行下一次循环 if num == 7: break # 跳出并结束循环，不再继续执行。 print(num) 输出结果为： 1235 结果中我们可以看到，代码只输出到了5。我们来剖析一下整个代码，当代码为5的时候，print()函数还是正常执行了一次，然后再进来的时候，num在最前方+1变为了6，执行了continue，跳出了本次循环。再进入循环之后，num +1 变成了7，这个时候进入了第二个if判断，直接执行了break语句，跳出并结束了整个循环。这样，print()函数这无法再继续执行下去了。 特殊语句 exit() quit() 这两个特殊语句，均是用于结束程序的执行，exit()和quit()之后的代码不会执行。在单纯的循环结构中的作用与break很像，但是完全不能混为一谈。这两个语句是用于结束并退出当前python解释器的，而break仅用于结束当前的循环体。 练习 打印矩形 让我们循环出十行十列 ★ ☆ ，隔一行换色，再做一个隔一列换色。 在最开始，我们先思考一下，十行十列，那就是完成100次打印。我们先把这部分实现一下： 输出结果因为占篇幅，我就不写了，大家自行执行就可以了。 1234num = 0while num &lt; 100: print('☆', end = &quot; &quot;) num += 1 在这之后，我们需要考虑一下，既然是十行十列，那说明我们每隔10个就需要一次换行： 1234567num = 0while num &lt; 100: print('☆', end = &quot; &quot;) # 判断是否需要换行 if num % 10 == 9: print('\\n') num += 1 现在打印出了十行☆, 每一行十个。第一步我们已经实现了，那么现在，让我们来尝试一下隔一行打印一个不同的。思考一下，其实就是奇偶数的问题，想明白之后，接下来就好办了： 123456789101112# 隔列换色num = 0while num &lt; 100: # 判断当前是基数还是偶数 if num % 2 == 0: print('☆', end = &quot; &quot;) else: print('★', end = &quot; &quot;) # 判断是否需要换行 if num % 10 == 9: print('\\n') num += 1 隔列换色实现之后，我们再来考虑一下隔行换色，让我们从隔列换色上找一点灵感。既然隔列换色是奇偶数的问题，那么隔行换色，是不是就是每一行的奇偶数问题？ 那么我们如何对行数做判断呢？其实很简单，我们只要对当前数字做取整数操作：num // 10，然后获取到的整数再来取余就行了。 那么我们就可以这样来实现： 123456789101112# 隔行换色num = 0while num &lt; 100: # 以当前行数为基数，对2取余，判断奇偶 if num // 10 % 2 == 0: print('☆ ', end = &quot; &quot;) else: print('★', end = &quot; &quot;) # 判断是否需要换行 if num % 10 == 9: print('\\n') num += 1 大家可以执行去操作一下试试，建议使用Jupyter Notebook，这种实验性的代码块，很方便得到结果。 打印乘法口诀表 这也是Python教学中经常被拿来进行教学的一个经典案例，和上一个练习一样，我们一边分析，便来完善代码。 整个代码中，我们用到了刚才学到的for...in循环以及range()函数。 首先我们利用range()函数，输出1到9，每输出一个换一次行： 1234# 乘法口诀表for x in range(1, 10): # 换行 print() 然后我们在每一行内再做一次循环，输出每一行的序列, 当然还是从1开始。 12345678# 乘法口诀表for x in range(1, 10): # 第二层循环，内循环 # 内循环负责当前行的函数，第一行 1列 2行 2列....9行 9列 for y in range(1, 10): print(f'{x}x{y}={x*y}', end=&quot; &quot;) # 换行 print() 这里需要注意，就乘法表而言，我们最大列不能大于这一行的被乘数, 那么我们range()需要调整一下： 12345678# 乘法口诀表for x in range(1, 10): # 第二层循环，内循环 # 内循环负责当前行的函数，第一行 1列 2行 2列....9行 9列 for y in range(1, x+1): print(f'{x}x{y}={x*y}', end=&quot; &quot;) # 换行 print() 斐波那契数列 再来让我们多做一个练习，斐波那契数列。 在做这个练习之前，首先我们需要了解什么是斐波那契数列。我这里应用一下维基百科的解释： 斐波那契数所形成的数列称为斐波那契数列。这个数列是由意大利数学家斐波那契在他的《算盘书》中提出。在数学上，斐波那契数是以递归的方法来定义： \\(F_0=0\\) \\(F_1=1\\) \\(F_n=F_{n-1}+F_{n-2}(n&gt;=2)\\) 用文字来说，就是斐波那契数列由0和1开始，之后的斐波那契数就是由之前的两数相加而得出。首几个斐波那契数是：1、 1、 2、 3、 5、 8、 13、 21、 34、 55、 89、 144、 233、 377、 610、 987…… 了解之后，让我们来分析一下： 10, 1, 1, 2, 3, 5, 8, 13... 第0项如果是0，那么第一项是1， 第二项也是1， 之后的第三项开始，每一项都是前面两个数的和。 因为这个数列是一个无限递归下去的数列，我们不能无限的计算下去，所以需要先知道自己计算多少项： 12# 获取用户输入的数据num = int(input('你需要计算多少项？')) 之前我们分析得到，第三项开始，每一项是前面两个数的和，那么，我们需要定义两个变量用来承载相加的两个数，再多定义一个初始值，用于判断是否执行循环： 1234num = int(input('你需要计算多少项？'))n1 = 0n2 = 1count = 2 然后，让我们开始进入正题，需要先判断用户输入的数字是否正整数，我们先不搞那么复杂，只需要简单判断一下是否大于等于0，然后再判断用户输入是否为1， 因为如果是只输出1项，那么就不需要计算了，直接输出n1就好了： 1234567891011num = int(input('你需要计算多少项？'))n1 = 0n2 = 1count = 2# 从之后的数字开始计算if num &lt;= 0: print('请输入一个正整数。')elif num == 1: print(f'斐波那契数列: {n1}')else: pass # 占位 然后，让我们正式进入循环计算, 现在n1为第一项，n2就是第二项，直接输出就可以了 1234567891011num = int(input('你需要计算多少项？'))n1 = 0n2 = 1count = 2# 从之后的数字开始计算if num &lt;= 0: print('请输入一个正整数。')elif num == 1: print(f'斐波那契数列: {n1}')else: print(f'斐波那契数列: {n1}, {n2}', end = &quot;, &quot;) 之后，我们去判断count是否小于用户输入的数字，如果小于，就进入循环。然后再循环内定义一个变量n3， 用来承载相加之后得到的结果，作为当前项输出。再讲n1, n2重新赋值。不要忘了给count加值。 1234567891011121314151617num = int(input('你需要计算多少项？'))n1 = 0n2 = 1count = 2# 从之后的数字开始计算if num &lt;= 0: print('请输入一个正整数。')elif num == 1: print(f'斐波那契数列: {n1}')else: print(f'斐波那契数列: {n1}, {n2}', end = &quot;, &quot;) while count &lt; num: n3 = n1 + n2 print(n3, end = &quot;, &quot;) # 更新数据 n1, n2 = n2, n3 count += 1 当我们输入9的时候，输出结果： 1斐波那契数列: 0, 1, 1, 2, 3, 5, 8, 13, 21, 百钱买百鸡 让我们先来说明一下这个题目： 123一共有100块钱，需要买100只鸡公鸡 3元钱一只，母鸡1元钱一只，小鸡5毛钱一只。要求计算，100块钱买100只鸡，一共有多少种方案 在这个题目里，我们可以计算如果只买一种，这公鸡可以有33只，母鸡有100只，小鸡这可以买200只。 这里面我们可以思考一下，这里我们一共需要3个变量和2个常量，变量为公鸡，母鸡以及小鸡；2个常量为100块钱和总共100只鸡。 先让我们从循环体来开始写： 12345678910num = 0for gj in range(1, 34): for mj in range(1, 101): for xj in range(1, 201): # 判断是否为100只，是否话费100元 if gj + mj + xj == 100 and gj*3 + mj + xj*0.5 == 100: print(f'公鸡{gj}只，母鸡{mj}只，小鸡{xj}只， 共花费{gj*3 + mj + xj*0.5}元') num += 1print(num) 这里，我们是计算了三只都买的情况，那么其实还有一种额外的情况，就是我们一开始说的，100块钱都买母鸡的情况，也正好是100块钱100只鸡。所以，我们的num要从1开始计数 12345678910num = 1for gj in range(1, 34): for mj in range(1, 101): for xj in range(1, 201): # 判断是否为100只，是否话费100元 if gj + mj + xj == 100 and gj*3 + mj + xj*0.5 == 100: print(f'公鸡{gj}只，母鸡{mj}只，小鸡{xj}只， 共花费{gj*3 + mj + xj*0.5}元') num += 1print(num) 输出结果（只看num）： 120 也就是说，我们目前一共有20种组合方案。具体有哪些方案，有兴趣的小伙伴可以执行我所写的代码，会打印出来。 虽然解决问题了，可是这并不是最好的写法。 看看这团垃圾的效率：第一层需要计算34次， 第一层每次计算，第二层都要计算100次，第二层每跑一遍，第三层需要计算200次.... 这简直就是一堆米田共。当我们加上一个计数变量稍微统计一下到底计算了多少次 1234567count = 0... for xj in range(1, 201): count += 1 # 判断是否为100只，是否话费100元...print(count) 可以得到最后结果为： 1660000 是不是很恐怖？让我们改动一下代码，优化性能： 让我们来思考一下，100只鸡这个总数是不是固定不变的？那么公鸡，母鸡的计算得到之后，是不是小鸡的数量就得到了。还有必要在进入一次循环吗？肯定没必要了对不对？所以我们这样改动： 12345678910111213count = 0num = 1for gj in range(1, 34): for mj in range(1, 101): xj = 100 - gj - mj count += 1 # 判断是否为100只，是否话费100元 if gj + mj + xj == 100 and gj*3 + mj + xj*0.5 == 100: print(f'公鸡{gj}只，母鸡{mj}只，小鸡{xj}只， 共花费{gj*3 + mj + xj*0.5}元') num += 1print(f'一共有{num}种组合方式。')print(f'当前一共计算了{count}次') 最后得到的计算结果： 12一共有20种组合方式。当前一共计算了3300次 从660000次一下下降到了3300次，这个性能的提示是很大的了。 所以，很多问题我们不要只追求解决，要善于多思考。 那么至此，我们这节课也就结束了。让我们最后放几个思考题： 思考题 对于我们打印矩阵，完成了隔行上色和隔列上色的问题，我们思考一下该如何解决三角形和菱形； 对于乘法表，思考下我们如何完成反向打印。 解决了思考题的小伙伴，可以在评论区留言。期待看到大家的想法。我是茶桁，咱们下次见，下一节课，我们进入「模块化编程」，开始学习函数。","link":"/Python-process-control/"},{"title":"06 快速建立一个AI应用","text":"Hi，我是茶桁。 在过去的两讲中，我们已经使用 OpenAI 提供的 Embedding 接口完成了文本分类的功能。现在，我们回到 Completion 接口，这一讲将带你更深入地了解该接口的使用。除此之外，我们还将快速搭建一个有界面的聊天机器人，这将让你更好地理解 Completion 接口的应用场景。在这个过程中，你将第一次使用 HuggingFace 这个平台，它是目前最流行的深度学习模型社区。通过 HuggingFace，你可以下载到最新的开源模型，查看其他人提供的示例代码，并参与到社区的交流中。 价廉高质的ChatGPT 我们在第三讲里介绍了Completion接口，并且通过它实现了一个聊天机器人的功能。在那个时候，我们采用的是自己将整个对话拼接起来，将整个上下文都发送给 OpenAI 的 Completion API 的方式。不过，在 3 月 2 日，因为 ChatGPT 的火热，OpenAI 放出了一个直接可以进行对话聊天的接口。这个接口叫做 ChatCompletion，对应的模型叫做 gpt-3.5-turbo，不但用起来更容易了，速度还快，而且价格也是我们之前使用的 text-davinci-003 的十分之一，可谓是物美价廉了。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Quickly-build-an-AI-application/"},{"title":"去掉wp后台编辑的文章预览","text":"在网上搜索此类教程。我的目的比较简单，就是想加快的速度，没有出于安全问题考虑。不过这个问题要摆出来确实蛮吓人的啊。： 前面提到因为 WordPress 2.0 的编辑预览功能使用 iframe，与我使用的一个js 代码冲突，一进入编辑窗口就会跳出来。liyuanzao 告诉我可以使用 is_preview() 这个函数，我试了试发现不行。Google 了一下，发现还是个大问题。 因为首先搜到的是这篇文章： AdSense on WordPress 2.0，大体的中文翻译在这儿： 在 WordPress 2.0 使用 AdSense 注意之事。讲 在 WordPress 2.0 中有一個很酷的功能名叫 Post Preview，即是在 edit / view draft 時，在文章下方會顯示一個 preview 畫面，內裡會顯示這篇文章在發表時的樣子，但如果有使用 Goolge AdSense，當 preview 時都會直接讀取 Google AdSense 廣告，但因為文章還未發表，如果跟著 Google 嘗試尋找這篇文章時，就會出現 404 Not found 的情況，有可能會被 Google 暫停你的 AdSense 戶口。 这是不是个潜在的大危险？文章中提到了使用 is_preview() 来解决，但是不少人和我一样发现这其实并不行。有人已经到 WordPress 报了bug，问题的表现说得很清楚。 如果实在着急，可以去掉预览功能，WP 支持论坛上给出了方法：删除 /wp-admin/post.php 中 82 到 87 行。 原文地址 啊。。下边来说重点，不管是为了自己的编辑速度还是安全方面考虑，总之我们是要去掉文章预览功能是吧。其实这是一个鸡肋功能，本来编辑器就是所见即所得了。这样等于在打开后台编辑器的同时又打开了一次主页面。。。去掉的办法上边有说，第82到87行，其实我看了一下并不是这几行。不知道是不是因为版本不对，所以有些出入。我的wp是2.0.4的。。。第82到85行删除就OK了。。具体代码是： &lt;div id='preview' class='wrap'&gt; &lt;h2 id=\"preview-post\"&gt;&lt;?php _e('Post Preview (updated when post is saved)'); ?&gt; &lt;small class=\"quickjump\"&gt;&lt;a href=\"#write-post\"&gt;&lt;?php _e('edit &amp;uarr;'); ?&gt;&lt;/a&gt;&lt;/small&gt;&lt;/h2&gt; &lt;iframe src=\"&lt;?php echo add_query_arg('preview', 'true', get_permalink($post-&gt;ID)); ?&gt;\" width=\"100%\" height=\"600\" &gt;&lt;/iframe&gt; &lt;/div&gt;","link":"/Remove-the-wp-background-edit-the-article-preview/"},{"title":"08 改写和审核","text":"Hi, 我是茶桁。 我们已经介绍了 OpenAI 的主要接口。这是基础知识系列的最后一讲，我们将讨论 OpenAI GPT 系列模型的其他接口。你可能不会经常使用其中一些接口，但了解它们不会有任何坏处，说不定你会在某些需求中用到它们。 在这篇文章中，我们将一起探讨 OpenAI 为文本改写和内容审核提供的功能，以及 GPT 系列模型的种类、区别和应用场景。 文本改写教程 我猜你可能已经用过许多基于 AI 大型语言模型的产品了。其中很常见的一类应用是写作助手，比如 Notion AI。它可以帮助你在文章中选择一段内容，并让 AI 帮你修改它，例如缩短文本或改变语气等。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Rewriting-and-Reviewing/"},{"title":"AI ability practice","text":"Including core capabilities, BI and algorithm related, Code warehouse: 【AI Basic](https://github.com/hivandu/practise/tree/master/AI-basic)","link":"/README/"},{"title":"SQL练习1","text":"-- UPPER 是转换大写的函数 1SELECT emp_name,salary * 12, UPPER(email) FROM employee; -- 使用别名 1234SELECT e.emp_name AS &quot;姓名&quot;, salary * 12 AS &quot;12月的工资&quot;, UPPER(email) &quot;电子邮箱&quot;FROM employee AS e; -- 无表查询 1234567SELECT 1+1;-- Oracle 实现，dual只有一个字段且只包含一行数据SELECT 1+1FROM dual;SELECT * FROM employee WHERE emp_name = '刘备'; -- 比较运算符 123select * from employee where sex &lt;&gt; '男'select * from employee where salary BETWEEN 5000 and 7000select * from employee where emp_name IN('刘备') -- 一个时间段之后入职 1234567select emp_name, hire_date from employee where hire_date &gt; DATE '2018-01-01'SELECT emp_name,hire_date,manager from employee where manager IS NULLSELECT emp_name, sex, salary from employee where sex = '女' AND salary &gt; 10000SELECT emp_name, sex, salary FROM employee WHERE emp_name = '刘备' OR emp_name = '张飞' OR emp_name = '赵云' -- 短路运算 short-circuit evaluation 12345select 'AND' FROM employee WHERE 1 = 0 AND 1/0 = 1;SELECT 'OR' FROM employee where 1 = 1 OR 1/0 = 1;-- NOTselect emp_id,emp_name FROM employee WHERE emp_name NOT IN('刘备','张飞','赵云') -- 运算符优先级 123SELECT emp_name,dept_id,bonusFROM employeeWHERE (dept_id = 2 OR dept_id = 3) AND bonus IS NOT NULL; -- 去处重复值 12SELECT DISTINCT SEX FROM employeeSELECT DISTINCTROW sex from employee / 查找 2018 年 1 月 1 日之后入职，月薪小于 5000，并且奖金小于 1000（包括没有奖金）的员工。 / 12345SELECT emp_id,emp_name,salary,hire_date,bonus FROM employee WHERE hire_date &gt; '2018-01-01' AND salary &lt; 5000 AND (bonus &lt; 1000 OR bonus IS NULL) -- LIKE运算符 1234567SELECT emp_id,emp_name,sexFROM employeeWHERE emp_name LIKE '赵%'SELECT emp_name,emailFROM employeeWHERE email NOT LIKE 'dengzh_@shuguo.com'; -- 转义字符 1234567891011CREATE TABLE t_like(c1 VARCHAR(20))INSERT INTO t_like(c1) VALUES ('进度:25% 已完成')INSERT INTO t_like(c1) VALUES ('日期:2019年5月25日')SELECT c1FROM t_likeWHERE c1 LIKE &quot;%25\\%%&quot;SELECT c1FROM t_likeWHERE c1 LIKE &quot;%25#%%&quot; ESCAPE '#' -- 大小写匹配 123SELECT emp_name,emailFROM employeeWHERE email LIKE 'M%' -- 正则表达式 判断邮箱 1234567891011/*以字母或者数字开头；后面是一个或者多个字母、数组或特殊字符（ . _ - ）；然后是一个 @ 字符；之后包含一个或者多个字母、数组或特殊字符（ . - ）；最后是域名，即 . 以及 2 到 4 个字母。^[a-zA-Z0-9]+[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}$*/SELECT email FROM t_regexpWHERE REGEXP_LIKE (email, BINARY '^[a-z0-9]+[a-z0-9._-]+@[a-z0-9.-]+\\\\.[a-z]{2,4}$','i'); -- 降序排序 1234select emp_name,salary,hire_datefrom employeewhere dept_id = 4ORDER BY salary DESC; -- 多列排序(工资，入职先后) 1234select emp_name,salary,hire_dateFROM employeewhere dept_id = 4ORDER BY salary DESC, hire_date; -- 按照SELECT顺序 1234SELECT emp_name, salary, hire_dateFROM employeeWHERE dept_id = 4ORDER BY 2 desc, 3 中文排序 -- CONVERT 是一个函数，用于转换数据的字符集编码。以下转换为中文GBK字符集 1234SELECT emp_namefrom employeeWHERE dept_id = 4ORDER BY CONVERT(1 USING GBK) -- 空值排序 1234select emp_name,bonusfrom employeewhere dept_id = 2ORDER BY 2 -- 其他空值排序方法 12345-- COALESCE函数将控制转换为一个指定的值SELECT emp_name, COALESCE(bonus,0) AS bonusFROM employeewhere dept_id = 2ORDER BY COALESCE(bonus,0); / 第六节练习: 查询所有的员工信息，按照员工的总收入（年薪加奖金）从高到低进行排序，总收入相同再按照姓名的拼音顺序排序。 / 1234SELECT emp_name, salary, bonus, (salary+bonus) as sumFROM employeeWHERE dept_id &gt;= 0ORDER BY (salary+bonus) DESC, CONVERT(1 USING GBK) -- TopN 排行榜 123456789101112131415161718-- 标准FETCH语法,此语法MySQL不支持，Oracle, PostgreSQL支持SELECT emp_name, salaryFROM employeeORDER BY salary DESCOFFSET 0 ROWSFETCH FIRST 5 ROWS ONLY;-- LIMIT实现TOPN排行榜SELECT emp_name, salaryFROM employeeORDER BY salary DESCLIMIT 5 OFFSET 0;-- 第二种写法SELECT emp_name,salaryFROM employeeORDER BY salary DESCLIMIT 0,5 -- SQL 实现分页查询 1234SELECT emp_name, salaryFROM employeeORDER BY salary DESCLIMIT 10,5 -- 员工排名第3高 123456789select emp_name,salary FROM employeeWHERE salary = ( select salary from employee ORDER BY salary DESC LIMIT 2,1)SELECT emp_name,salary FROM employeeORDER BY salary desc limit 5 offset 10; / 练习：使用LIMIT和OFFSET找出员工表中月薪排名第三高的所有员工 / 123456select emp_name, salary FROM employeewhere salary = ( select salary FROM employee ORDER BY salary desc limit 1 offset 2)","link":"/SQL_ext_1/"},{"title":"SVM-based Text Classification in Practice","text":"The source code: SVM-based Text Classification in Practice 'cnews.train.txt' data cannot be uploaded because it is too large, so it needs to be decompressed and imported after compression. Use SVM to implement a simple text classification based on bag of words and support vector machine. import data 1234# importimport codecsimport osimport jieba Chinese news data is prepared as a sample data set. The number of training data is 50,000 and the number of test data is 10,000. All data is divided into 10 categories: sports, finance, real estate, home furnishing, education, technology, fashion, current affairs, games and entertainment . From the training text, you can load the code, view the data format and samples: 1234567891011data_train = './data/cnews.train.txt' # training data file name data_test = './data/cnews.test.txt' # test data file namevocab = './data/cnews.vocab.txt' # dictionarywith codecs.open(data_train, 'r', 'utf-8') as f: lines = f.readlines()# print sample contentlabel, content = lines[0].strip('\\r\\n').split('\\t')content Take the first item of the training data as an example to segment the loaded news data. Here I use the word segmentation function of LTP, you can also use jieba, and the segmentation results are displayed separated by \"/\" symbols. 123# print word segment resultssegment = jieba.cut(content)print('/'.join(segment)) To sort out the above logic a bit, implement a class to load training and test data and perform word segmentation. 123456789101112131415161718192021222324# cut datadef process_line(idx, line): data = tuple(line.strip('\\r\\n').split('\\t')) if not len(data)==2: return None content_segged = list(jieba.cut(data[1])) if idx % 1000 == 0: print('line number: {}'.format(idx)) return (data[0], content_segged) # data loading methoddef load_data(file): with codecs.open(file, 'r', 'utf-8') as f: lines = f.readlines() data_records = [process_line(idx, line) for idx, line in enumerate(lines)] data_records = [data for data in data_records if data is not None] return data_records# load and process training datatrain_data = load_data(data_train)print('first training data: label {} segment {}'.format(train_data[0][0], '/'.join(train_data[0][1])))# load and process testing datatest_data = load_data(data_test)print('first testing data: label {} segment {}'.format(test_data[0][0], '/'.join(test_data[0][1]))) After spending some time on word segmentation, you can start building a dictionary. The dictionary is built from the training set and sorted by word frequency. 12345678910111213141516171819202122def build_vocab(train_data, thresh): vocab = {'&lt;UNK&gt;': 0} word_count = {} # word frequency for idx, data in enumerate(train_data): content = data[1] for word in content: if word in word_count: word_count[word] += 1 else: word_count[word] = 1 word_list = [(k, v) for k, v in word_count.items()] print('word list length: {}'.format(len(word_list))) word_list.sort(key = lambda x : x[1], reverse = True) # sorted by word frequency word_list_filtered = [word for word in word_list if word[1] &gt; thresh] print('word list length after filtering: {}'.format(len(word_list_filtered))) # construct vocab for word in word_list_filtered: vocab[word[0]] = len(vocab) print('vocab size: {}'.format(len(vocab))) # vocab size is word list size +1 due to unk token return vocabvocab = build_vocab(train_data, 1) In addition, according to category, we know that the label itself also has a \"dictionary\": 12345678910def build_label_vocab(cate_file): label_vocab = {} with codecs.open(cate_file, 'r', 'utf-8') as f: for lines in f: line = lines.strip().split('\\t') label_vocab[line[0]] = int(line[1]) return label_vocablabel_vocab = build_label_vocab('./data/cnews.category.txt')print(f'label vocab: {label_vocab}') Next, construct the id-based training and test sets, because we only consider the bag of words, so the order of words is excluded. Constructed to look like libsvm can eat. Note that because the bag of word model 12345678910111213141516171819202122def construct_trainable_matrix(corpus, vocab, label_vocab, out_file): records = [] for idx, data in enumerate(corpus): if idx % 1000 == 0: print('process {} data'.format(idx)) label = str(label_vocab[data[0]]) # label id token_dict = {} for token in data[1]: token_id = vocab.get(token, 0) if token_id in token_dict: token_dict[token_id] += 1 else: token_dict[token_id] = 1 feature = [str(int(k) + 1) + ':' + str(v) for k,v in token_dict.items()] feature_text = ' '.join(feature) records.append(label + ' ' + feature_text) with open(out_file, 'w') as f: f.write('\\n'.join(records))construct_trainable_matrix(train_data, vocab, label_vocab, './data/train.svm.txt')construct_trainable_matrix(test_data, vocab, label_vocab, './data/test.svm.txt') Training process The remaining core model is simple: use libsvm to train the support vector machine, let your svm eat the training and test files you have processed, and then use the existing method of libsvm to train, we can change different parameter settings . The documentation of libsvm can be viewed here, where the \"-s, -t, -c\" parameters are more important, and they decide what you choose Svm, your choice of kernel function, and your penalty coefficient. 1234567891011121314from libsvm import svmfrom libsvm.svmutil import svm_read_problem,svm_train,svm_predict,svm_save_model,svm_load_model# train svmtrain_label, train_feature = svm_read_problem('./data/train.svm.txt')print(train_label[0], train_feature[0])model=svm_train(train_label,train_feature,'-s 0 -c 5 -t 0 -g 0.5 -e 0.1')# predicttest_label, test_feature = svm_read_problem('./data/test.svm.txt')print(test_label[0], test_feature[0])p_labs, p_acc, p_vals = svm_predict(test_label, test_feature, model)print('accuracy: {}'.format(p_acc)) After a period of training, we can observe the experimental results. You can change different svm types, penalty coefficients, and kernel functions to optimize the results.","link":"/SVM-based_text_classification_in_practice/"},{"title":"11 用好开源模型节约成本","text":"Hi， 大家好，我是茶桁。 直奔主题，我们来谈谈成本这件事。 大家应该都知道，ChatGPT对免费用户是有5美元的API调用额度的，说是这么说，可是那是以前，现在新注册的小伙伴应该都发现自己的API Key根本无法调用API，原因是这个免费额度似乎已经失效了。而我可以直接说，在我从第一节到第10节的课程中所用到的金额，已经超过这个数目了。也就是说，我这10节课API调用成本就已经超过了40元人民币。 看到这大家大概能理解我这个课程为什么改为付费课程了吧？ 对于 ChatCompletion 的接口来说，为了更好地使用它，我们需要传入更多的上下文信息，以便更准确地进行文本生成。不过要注意的是，实际消耗的 Token 数量可能比我们感觉的要多。此外，除了费用之外，数据安全也是我们需要考虑的一个问题。由于每个国家的数据监管要求不同，不是所有的数据都适合通过 OpenAI 的 API 来处理。因此，我们需要寻找一个除 OpenAI 以外的解决方案。幸运的是，有一些开源的大语言模型可以帮助我们解决这个问题。通过利用这些开源的模型，中小型公司也可以轻松地获得更准确、更安全的文本生成服务。 在 Colab 中使用 GPU 在本课中，我们需要使用一些开源模型。但是，并不是所有人的电脑都配备了强劲的 NVIDIA GPU。因此，我建议您使用 Colab 运行相应的笔记本，并注意将运行环境设置为 GPU。 如下图，选择 代码执行程序-&gt;更改运行时类型,然后在硬件加速器上选择 GPU 就可以了。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Save-costs-with-an-open-source-model/"},{"title":"「泰坦尼克」生存预测","text":"最好的学习就是输出,所以虽然这个预测很多人做过了,我还是在这里再做一遍,纯粹是为了自己学习. 前言 这次预测使用的是Sklearn中的决策树模型: 1clf = DecisionTreeClassifier(criterion='entropy') 其中criterion是标准,决定了构造分类树是采用ID3分类树还是CART分类树,对应的取值分别是entropy和gini entropy: 基于信息熵,也就是ID3算法, 实际结果与C4.5相差不大; gini: 默认参数,基于基尼系数. CART算法是基于基尼系数做属性划分的,所以criterion=gini时, 实际上执行的是CART算法. 其完整参数: 123456DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best') 参数代表的含义如下表: 参数表 作用 criterion 在基于特征划分数据集合时，选择特征的标准。默认是 gini,也可以是entropyo splitter 在构造树时，选择属性特征的原则，可以是best或者 random。默认是best,best代表在所有的特征中选择最 好的，random代表在部分特征中选择最好的。 max_depth 决策树的最大深度，我们可以控制决策树的深度来防止 决策树过拟合 max_features 在划分数据集时考虑的最多的特征值数量。为int或float类型。其中int值是每次split时最大特征数；float值是百 分数，即特征数=max_features * n_featureso min_samples_split 当节点的样本数少于min_samples_split时，不再继续分 裂。默认值为2 min_samples_leaf 叶子节点需要的最少样本数。如果某叶子节点数目小于 这个阈值，则会和兄弟节点一起被剪枝。 min_samples_leaf的取值可以是int或float类型。 int类型：代矗小样本数； float类型：表示一个百分比，这是最小样本数 =min_samples_leaf乘以样本数量，并向上取整。 max_leaf_nodes 最大叶子节点数。int类型，默认为None。 默认情况下是不设置最大叶子节点数，特征不多时，不 用设置。特征多时，可以通过设置最大叶子节点数，防 止过拟合。 min_impurity_decrease 节点划分最小不纯度。float类型，默认值为0。 节点的不纯度必须大于这个阈值，否则该节点不再生成 子节点。通过设置，可以限制决策树的增长。 minjmpurity_split 信息増益的阀值。信息増益必须大于这个阀值，否则不 分裂。 class_weight 类别权重。默认为None,也可以是diet或balanced。 diet类型：指定样本各类别的权重，权重大的类别在决策 树构造的时候会进行偏倚。 balanced:算法自己计算权重，样本量少的类别所对应 的样本权重会更高。 presort bool类型，默认是false,表示在拟合前，是否对数据进 行排序来加快树的构建。当数据集较小时，使用 presort=true会加快分类器构造速度。当数据集庞大 时，presort=true会导致整个分类非常缓慢。 在构造决策树分类器后,我们可以使用fit方法让他分类器进行拟合, 使用predict方法对新数据进行预测, 得到预测的分类结果, 也可以使用score方法得到分类器的准确率. fit、predict和score方法的作用如下表: 方法表 作用 fit(features, labels) 通过特征矩阵, 分类表示,让分类器进行拟合 predict(features) 返回预测结果 score(features, labels) 返回准确率 本次数据集一共两个,一个是train.csv, 用于训练, 包含特征信息和存活与否的标签, 一个是test.csv, 测试数据集, 只包含特征信息. 训练集中,包括了以下字段: 字段 描述 Passengerld 乘客编号 Survived 是否幸存 Pclass 船票等级 Name 乘客姓名 Sex 乗客性别 SibSp 亲戚数虽（兄妹、配偶数） Parch 亲戚数虽（父母、子女数） Ticket 船票号码 Fare 船票价格 Cabin 船舱 Embarked 登陆港口 流程 整个流程可以划分为三个阶段: 获取数据 准备阶段 数据探索 数据清洗 特征选择 分类阶段 决策树模型 模型评估&amp;预测 决策树可视化 获取数据 这一步还包含了引入所需依赖 123456789101112# 引入依赖import pandas as pdfrom sklearn.feature_extraction import DictVectorizerfrom sklearn.tree import DecisionTreeClassifierimport os# 准备工作path = os.path.expanduser('~/data/python/Titanic_Data/')# 获取数据train_data = pd.read_csv(path + 'train.csv')test_data = pd.read_csv(path + 'test.csv') 准备阶段 对数据进行探索,分析数据质量,并对数据进行清洗,然后通过特征选择对数据进行降维, 以便于之后进行分类运算; 数据探索 123456train_data.info() # 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度train_data.describe() # 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等train_data.describe(include=['O']) #查看字符串类型 (非数字) 的整体情况train_head(5) # 查看前几行数据 (默认是前 5 行)train_tail(5) # 查看后几行数据 (默认是最后 5 行)train_sample(5) # 查看随机几行数据 (默认是随机1行) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 运行结果&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId 891 non-null int64Survived 891 non-null int64Pclass 891 non-null int64Name 891 non-null objectSex 891 non-null objectAge 714 non-null float64SibSp 891 non-null int64Parch 891 non-null int64Ticket 891 non-null objectFare 891 non-null float64Cabin 204 non-null objectEmbarked 889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.6+ KBNone------------------------------ PassengerId Survived ... Parch Farecount 891.000000 891.000000 ... 891.000000 891.000000mean 446.000000 0.383838 ... 0.381594 32.204208std 257.353842 0.486592 ... 0.806057 49.693429min 1.000000 0.000000 ... 0.000000 0.00000025% 223.500000 0.000000 ... 0.000000 7.91040050% 446.000000 0.000000 ... 0.000000 14.45420075% 668.500000 1.000000 ... 0.000000 31.000000max 891.000000 1.000000 ... 6.000000 512.329200[8 rows x 7 columns]------------------------------ Name Sex ... Cabin Embarkedcount 891 891 ... 204 889unique 891 2 ... 147 3top Peter, Mrs. Catherine (Catherine Rizk) male ... B96 B98 Sfreq 1 577 ... 4 644[4 rows x 5 columns]------------------------------ PassengerId Survived Pclass ... Fare Cabin Embarked0 1 0 3 ... 7.2500 NaN S1 2 1 1 ... 71.2833 C85 C2 3 1 3 ... 7.9250 NaN S3 4 1 1 ... 53.1000 C123 S4 5 0 3 ... 8.0500 NaN S[5 rows x 12 columns]------------------------------ PassengerId Survived Pclass ... Fare Cabin Embarked886 887 0 2 ... 13.00 NaN S887 888 1 1 ... 30.00 B42 S888 889 0 3 ... 23.45 NaN S889 890 1 1 ... 30.00 C148 C890 891 0 3 ... 7.75 NaN Q[5 rows x 12 columns]------------------------------ PassengerId Survived Pclass ... Fare Cabin Embarked619 620 0 2 ... 10.5000 NaN S330 331 1 3 ... 23.2500 NaN Q647 648 1 1 ... 35.5000 A26 C716 717 1 1 ... 227.5250 C45 C860 861 0 3 ... 14.1083 NaN S[5 rows x 12 columns] 数据清洗 探索之后, 我们发现Age、Cabin这两个字段的数据有缺失. 其中, Cabin为船舱, 有大量的缺失值, 在训练集和测试集中的缺失率分别为77%和78%, 无法补齐, Age可以获取平均值进行补齐, 而Embarked是登陆港口, 这个字段也有少量(2个)缺失值, 可以使用最大数据进行补齐. 12345train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)test_data['Age'].fillna(test_data['Age'].mean(), inplace=True)train_data['Embarked'].fillna(train_data['Embarked'].value_counts().idxmax(), inplace=True)test_data['Embarked'].fillna(test_data['Embarked'].value_counts().idxmax(), inplace=True) 分类阶段 特征选择 需要选择有用的字段作为特征,这一步其实很重要: 123456789# 特征选择train_data.columns# 从上一句的结果中选择特征字段features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Parch', 'Embarked']train_features = train_data[features]test_features = test_data[features]train_labels = train_data['Survived'] 这中间有一些事字符串字段, 是不适合进行后续运算的, 需要在这里转变为数值类型,比如Sex字段, 男女两种取值需要转变成0和1 再比如Embarked有S, C, Q三种可能, 我们可以改成Embarked=S, Embarked=C, Embarked=Q三个字段,然后用数值0和1来表示, 其中sklearn特征选择中的DictVectorizer类(上面已引入依赖), 可以处理符号化的对象, 将符号转变为0/1进行表示: 12dvec=DictVectorizer(sparse=False)train_features=dvec.fit_transform(train_features.to_dict(orient='record')) fit_transform这个函数可以讲特征向量转化为特征值矩阵, 我们查看下: 1dvec.feature_names_ 123# 运行结果:['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp'] 我们讲Embarked转化为三列 (['Embarked=C', 'Embarked=Q', 'Embarked=S']), Sex变为了两列 ([Sex=female', 'Sex=male']) 决策树模型 1234# 构造ID3决策树clf=DecisionTreeClassifier(criterion='entropy')# 决策树训练clf.fit(train_features, train_labels) 模型预测 &amp; 评估 我们首先得到测试集的特征值矩阵, 然后使用训练好的决策树clf进行预测, 得到预测结果: 123test_features=dvec.transform(test_features.to_dict(orient='record'))# 决策树预测pred_labels=clf.predict(test_features) 模型评估中,决策树提供了score函数直接得到准确率,但是我们并不知道真实的预测结果,所以无法用预测值和真实的预测结果做比较, 需要使用训练机中的数据进行模型评估, 可以使用决策树自带的score函数计算: 123# 得到决策树准确率acc_decision_tree=round(clf.score(train_features, train_labels), 6)acc_decision_tree 12# 运行结果0.982043 其实,以上准确率评估并不准确,因为我们用训练集做了训练,再用训练集做准确率评估, 并不能代表决策树分类器的准确率. 要统计决策树分类器的准确率, 可以使用K折交叉验证, cross_val_score 函数中的参数 cv 代表对原始数据划分成多少份，也就是我们的 K 值，一般建议 K 值取 10，因此我们可以设置 CV=10 1234import numpy as npfrom sklearn.model_selection import cross_val_score# 使用K折交叉验证, 统计决策树准确率np.mean(cross_val_score(clf, train_features, train_labels, cv=10)) 12# 输出结果0.7778901373283394","link":"/Titanic/"},{"title":"导读：了解AI并使用它&#x2F;他&#x2F;她们","text":"如果你想开始学习AI应用开发，那么在学习之前，有一些学前提醒需要注意。在当今AI爆发的时代，学习AI应用开发需要的学习方法和策略也发生了变化。本课程的目标是通过多尝试、多体验、多做头脑风暴的学习方法，帮助学生在短时间内掌握AI应用开发的基本技能。我们并不会传授过于深奥的数学和理论知识，而是会通过简单易学的API，让学生能够快速上手开发实用价值的AI应用。因此，在学习本课程的过程中，我们鼓励学生多尝试、多体验、多做头脑风暴，以更加轻松、快速地掌握AI应用开发的技能。如果你不知道如何开始，你可以使用Jupyter或者Golab这样的工具来帮助自己入门。 多练习，多尝试，多交流 1. 多尝试运行和修改代码 为什么要多尝试运行和修改代码？因为这是学习AI最有效的方式之一。通过自己亲手运行代码并进行修改，可以更深入地理解算法和模型背后的原理，并从中学到许多实用的技巧。此外，尝试运行和修改代码也能够帮助你更好地掌握编程语言和工具。 而为了方便地进行代码实验，我们可以使用一些开源的工具。例如，Jupyter Notebook 是一个广泛使用的交互式笔记本工具，它支持多种编程语言，并且可以在本地运行。如果你不知道如何搭建环境， 除了本地启动Jupyter之外，你也可以直接使用微软的VSCode，可以直接调用本地Jupyter环境（推荐）。 也可以使用 Google 的 Colaboratory（简称 Colab） 这样的云端工具，只需要一个 Google 账号即可使用。 自然，这也不是全无门槛的，学会如何科学上网是必备技能。这一部分请原谅我无法教授，还是需要自行查找资料。 2. 多体验不同的AI工具 在学习 AI 应用开发的过程中，你需要了解当前市场上涌现的海量 AI 应用。通过体验这些 AI 应用，你可以更好地了解 AI 的能力和应用场景，也能够了解到当前 AI 技术的发展状况。 例如，你可以尝试使用一些人工智能工具来完成自己的工作，比如使用自然语言处理的工具来帮助你写作、使用机器学习的工具来进行数据分析、使用计算机视觉的工具来进行图像处理等等。此外，你还可以体验一些常用的 AI 应用，比如语音助手、智能家居、智能客服、智能医疗等等。通过这些体验，你可以深入了解 AI 技术在实际场景中的应用和效果，从而更好地理解 AI 技术的价值和未来发展方向。 在课程中，你还可以从我推荐中了解到一些最新的 AI 应用。你可以注册账号、下载应用，多去体验一下这些 AI 应用，这不仅能够激发你学习课程的动力，也能够打开你自己利用 AI 大模型能力的思路。 总之，多去体验各类 AI 应用能够帮助你更好地了解 AI 技术的应用和发展现状，也能够激发你对 AI 技术的兴趣和热情，从而更好地进行学习和实践。 本地搭建Stable Diffusion 这里给大家一个小小的建议，尽量不要用线上的图片生成AI去生成商业图片，会有法律隐患的。而如果是在本地架设的情况下，这种问题基本就不存在了。 3. 多交流 与周围的人以及朋友一起多做做头脑风暴，尝试寻找有趣的新产品的机会。事实上，这是一个非常好的建议。AI应用已经涌现出许多，但是有些应用只是简单地使用了现有的API，缺乏创意和创新。然而，还有很多应用具有独特的想法，有些甚至可以直接商业化。学习AI的目的在于学以致用，可以与身边对新一代AI应用有兴趣的人一起探讨，看看课程中介绍的各种方法和技巧能否用于不同的场景和角度。这才是学习这门课程的真正价值。当然，如果你对AI大模型的底层原理有兴趣，可以深入研究其中的数学原理和各种深度学习模型。现在，有能力构建大模型的人实在是太少了，而不是太多了。能够推动通用人工智能向前发展一小步，相信是所有AI从业者都梦寐以求的事情。所以，学习AI，不仅要学习知识，更要发挥创造力，发掘新的应用场景，才能真正做到学以致用。 Stable Diffution 生成的填色图，完全可以变成一项生意，生成多张图做本书：Link 使用AI工具改变你现在的工作方式 随着 ChatGPT、Whisper 和 Stable Diffusion 等强大的 AI 技术的出现，我们的学习和工作方式也需要跟着改变。现在，利用 AI 工具来改造自己的学习和工作流程已经成为一种趋势。通过将 AI 技术应用到各个方面，我们可以获得更加沉浸式的学习体验，同时也能够提高日常生活和工作的效率。在过去的几个月里，我自己也不断地研究和学习新技术，并通过 AI 工具来提高自己的效率。在这篇文章中，我想和大家分享一些我所使用的 AI 工具和优化流程。 ChatGPT，这是一种基于 GPT 技术的人工智能语言模型。我常常利用 ChatGPT 来帮助自己解决问题，比如在学习编程的过程中，我会输入一些代码，然后让 ChatGPT 来帮我检查代码的错误。ChatGPT 还可以用来进行翻译、摘要和生成文章等等。这种 AI 工具可以帮助我们更加高效地学习和工作。 Whisper，这是一种人工智能笔记工具。与传统笔记工具不同的是，Whisper 可以将我们所输入的笔记和文本转化为自然语言，并通过 AI 技术来优化笔记的布局和结构。这样一来，我们可以更加快速和方便地记录学习和工作中的重要信息，并将其整理成易于理解的形式。 Stable Diffusion，这是一种用于大规模数据处理和分析的 AI 工具。在我的研究工作中，我常常需要处理海量的数据，并对数据进行分析和建模。Stable Diffusion 的出现让我能够更加高效地处理数据，并且能够利用 AI 技术来进行数据建模和预测。 利用 AI 工具来改造学习和工作流程已经成为一种趋势。通过利用这些工具，我们可以更加高效地学习和工作，并且可以更加快速地解决问题。当然，这些工具只是 AI 技术应用的冰山一角，未来还将有更多更加强大和智能的 AI 工具出现，让我们拭目以待。 如何使用 ChatGPT 进行学习 随着 AI 技术的发展，ChatGPT 成为了许多人学习知识的“助教”。但是，有些人觉得 ChatGPT 没有多大用处，这可能是因为他们询问了过于宽泛的问题。实际上，ChatGPT 的作用是为我们提供有针对性的回答，只需询问具体问题即可。 当我们学习新知识时，我们可以请 ChatGPT 帮助我们解释我们不理解的内容。与搜索不同，ChatGPT 可以根据我们的追问提供更深入的解释，直到我们完全理解这个知识点为止。此外，ChatGPT 不仅可以解释概念，还可以解释代码。我们可以将需要解释的代码段粘贴到 ChatGPT 中，它将为我们提供详细的讲解。 此外，ChatGPT 作为一个 AI “助教”，它的知识广博、不知疲倦，极其耐心。我们不需要担心问题过于简单或产生心理压力，因为 ChatGPT 不会嫌麻烦或不耐烦。因此，与查找资料或询问他人相比，使用 ChatGPT 可以更高效地解决问题。 问题具体化，将思考的过程交给自己，而获取知识交给ChatGPT，是我最常用的方式 在 Poe 平台中，我们可以选择不同的语言模型，不仅可以使用 ChatGPT，还可以使用其他大型语言模型。这样，我们可以选择最适合自己的模型。 学会使用工具获取额外资料 在当今的数字时代，英文资料已经成为许多行业中获取第一手信息的主要来源。特别是在技术领域，大量的技术文档、API文档和博客文章都是用英文写成的。虽然英文阅读能力是每个人在学习和工作中必须掌握的技能，但对于许多人来说，英文阅读还是比较吃力的。然而，随着机器翻译技术的不断提高，人们已经越来越多地使用翻译插件，将英文资料转换为中英对照版本。 DeepL是一种在线翻译工具，它使用了深度学习技术，可以对英文文本进行准确的翻译。使用DeepL，只需将需要翻译的文本复制粘贴到工具中，它就可以快速将其翻译成目标语言。另外，DeepL还可以通过浏览器插件的形式直接嵌入到浏览器中，当您访问英文网页时，它会自动将其翻译成您的语言。 使用翻译插件可以帮助我们快速浏览英文资料，同时避免了语言障碍。当我们遇到一些翻译不准确的地方，可以快速查看英文原文，确保对资料的理解和应用。同时，翻译插件的中英对照形式也让我们能够更快速地阅读英文资料，从而提高我们的阅读效率。 除了文本资料外，现在还有越来越多的音视频资料，如播客和视频等。这些最新的资料往往只有音频或视频版，而没有文字版。但是通过语音识别和文本摘要技术，我们也可以快速将音频和视频转换为文本，并且生成一个摘要。这样，我们就可以先快速浏览一遍摘要，决定是否值得去完整地听或看。 现在市面上有许多这样的浏览器插件，例如 Glarity，可以帮助我们快速总结视频内容，再来决定是否要看。对于像约翰卡马克这样的大神的访谈，我们可以使用 ChatGPT 背后的语言模型来生成一个摘要，以快速浏览视频内容，确定是否值得花时间去看。 如何通过AI来阅读论文 在科技飞速发展的今天，不断学习新知识，跟上最新的技术进展是非常必要的。读论文是获取新知识的好方法。然而，阅读一篇论文是一项费时费力的工作，因为它通常包含大量的专业术语、公式和图表。但是，随着大型语言模型的出现，我们现在可以借助AI来阅读论文。 现在有很多工具可以帮助我们阅读论文。例如，scispace是一个网站，可以将要阅读的论文上传到其中。然后，我们可以向AI提出问题，以快速了解论文讲解了什么内容。scispace内置了许多你可能会关心的问题，并且可以直接选择回答的语言。此外，在阅读过程中，AI可以对公式、图表等内容进行详细解释，这些工具都可以大大降低阅读论文的门槛，提高掌握这些复杂知识的效率。 除了scispace之外，还有许多其他的工具可以帮助我们阅读论文。例如，ChatPDF是一个可以对PDF文件进行小结和提问的工具。将各种分析报告上传至ChatPDF中，可以快速获取所需的信息。 利用 AI 写代码 Demo：提高生产效率的新工具 GitHub Copilot 是一个利用 AI 技术帮助工程师写代码的工具。通过将需求描述给 ChatGPT，它可以快速生成可用的代码，帮助工程师节省时间和精力。使用 Copilot 写代码的体验非常棒，只需输入注释或代码的开头，Copilot 就能为你生成完整的代码。 对于一些简单的函数调用等胶水代码，Copilot 十有八九是能帮上忙的。即使有些代码不够完美，以它为基础改造比从头开始写更快。当你需要使用一些不熟悉的包时，Copilot 尤其有用。 如果你是一个工程师，安装 Copilot 并使用它写代码是提高生产效率的好方法。此外，使用 ChatGPT 和 Copilot 来帮助写 Demo 代码，可以帮助你快速实验需求，而不必费时查找文档和阅读教程。这些工具能让你更轻松地完成工作，提高你的生产力和效率。 在实际使用中，你可能需要花一些时间来熟悉和调整 Copilot 生成的代码。但是，一旦你熟悉了 Copilot，它将成为你编程工作中最有价值的助手之一。 当然，我是一个穷人，写代码并不是我的主业，所以我一般都使用ChatGPT来完成我为数不多的需求： ChatGPT和Copilot的代码正确率以及BUG率比多数工程师都要来的优秀，自然，也包括我自己。 使用AI获取灵感 现在随着人工智能技术的快速发展，越来越多的AI写作工具涌现出来，让人们在创作过程中更加得心应手。其中，最让人印象深刻的就是AI如何帮助我们获取灵感。虽然AI还没有完全替代人类的思维，但在寻找灵感方面，它们已经展现出了惊人的能力。 在实际的写作中，很多人并不依赖AI产生内容，但是当缺少灵感的时候，AI可以作为一个非常好的助手。例如，当你在围绕一个主题思考写作内容时，可以尝试使用notion.ai等工具寻找灵感。虽然这些工具的很多主意并不新颖，但它们往往可以给你带来意想不到的角度和思路。 此外，你还可以尝试通过人设的不同来让AI从另一个角度帮助你思考问题。例如，为ChatGPT设置一个不同的人设，让它模拟某个领域的专家来帮助你做头脑风暴。这个时候，虽然你只有一个人在思考，但是你却可以组织一个各路大神汇聚的团队帮助你思考问题，让你得到更多有价值的点子。 不仅如此，对于不同领域的人群，还有一些专门的AI工具可以帮助他们获取灵感。例如，Midjourney、Dall-E 2等工具可以让设计师在创作过程中更加得心应手，快速地制作出优秀的设计作品。 接受它，别被它替代 随着科技的快速发展，现代社会正在经历着巨大的变革。许多传统行业和工作岗位正在面临被自动化和数字化取代的风险。因此，我们必须积极地适应新的技术，以免被时代抛弃。尤其是人工智能的发展，不仅给我们的生活带来了诸多便利，也对我们的工作和职业规划产生了重大影响。因此，了解人工智能的基本原理和应用场景，具备使用和开发人工智能的能力，成为未来职场竞争力的关键。同时，我们也需要不断地学习和更新知识，跟上时代的步伐，不断提升自己的技能和能力，以应对快速变化的社会和市场。","link":"/Understanding_and_Utilizing_AI/"},{"title":"05 为文本分类","text":"Hi， 我是茶桁。 在前一讲中，我们观察到大型模型的确表现出良好效果。在情感分析任务中，通过使用 OpenAI API 提供的 Embedding，我们得到的结果要比能在单个机器上运行的较小模型（如T5-base）要好得多。然而，我们之前所选择的问题确实有些过于简单。我们将5个不同的评分分成了正面、负面和中性，同时还排除了相对难以判断的\"中性\"评价，因此我们判断准确率的提高相对较容易实现。但是，如果我们想要准确预测具体的分数呢？这将是我们接下来需要探索的问题。 训练机器学习模型 一种最简单的方法是利用我们获得的文本 Embedding 向量。这次我们将不再直接使用向量之间的距离，而是采用传统的机器学习方法进行分类。这样做的原因在于，仅仅使用向量之间的距离无法最大化利用已标注好的分数信息。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Text-Classification/"},{"title":"利用AI写一个『贪吃蛇游戏』","text":"在完成一些简单的任务之后，我觉得ChatGPT可以帮我完成一些更为复杂，甚至于可能我并不是特别擅长的任务，而我想到了，也许，可以帮我写一个简单的游戏。 贪多嚼不烂的道理我还是明白的，于是，我给自己提前就设定了一个目标： 游戏可以运行实现简单的规则减少BUG的存在可以开始和结束 带着这四个最朴素的愿景，让我们开始吧。 一贯的，一上来我就将问题抛给了chatGPT 1请用Python3.10 编写一个贪吃蛇的游戏，给我完整的代码。 这里我们需要注意一下，你的描述越清晰，得到的结果越有可能是你想要的。在这里，我告诉了chatGPT，我希望用Python写一个游戏，而且版本号是3.10。并且，我告诉了chatGPT，请给我完整的代码。因为如果我不这么说的话，那很有可能chatGPT会先给我一堆的步骤而不是实际的代码。 在之后，我得到了chatGPT的答复： 完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import pygameimport random# 游戏窗口的大小WINDOW_WIDTH = 800WINDOW_HEIGHT = 600# 蛇身和食物的大小BLOCK_SIZE = 20# 定义颜色WHITE = (255, 255, 255)BLACK = (0, 0, 0)RED = (255, 0, 0)# 初始化 Pygamepygame.init()# 创建游戏窗口window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))pygame.display.set_caption(&quot;贪吃蛇游戏&quot;)clock = pygame.time.Clock()def game_loop(): game_over = False # 蛇的初始位置和速度 snake_x = WINDOW_WIDTH // 2 snake_y = WINDOW_HEIGHT // 2 snake_x_change = 0 snake_y_change = 0 # 食物的初始位置 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE while not game_over: for event in pygame.event.get(): if event.type == pygame.QUIT: game_over = True elif event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT: snake_x_change = -BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_RIGHT: snake_x_change = BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_UP: snake_y_change = -BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_DOWN: snake_y_change = BLOCK_SIZE snake_x_change = 0 # 更新蛇的位置 snake_x += snake_x_change snake_y += snake_y_change # 绘制游戏窗口 window.fill(BLACK) pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.draw.rect(window, WHITE, [snake_x, snake_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.display.update() # 碰撞检测 if snake_x == food_x and snake_y == food_y: food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE clock.tick(20) # 控制游戏的帧率 pygame.quit()# 启动游戏game_loop() 从代码里我们可以看出来，文件引用了库pygame以及random, 其中，pygame需要我们单独安装一下: 1conda install pygame 建立了一个snake_v1.py的文件，并把这段代码粘贴到文件里之后，我开始尝试运行它： 1python ~/xx/game/snake_v1.py 事情果然没有想的那么简单，根本就是毫无动静。我开始询问chatGPT chatGPT给到的答案似乎没有什么意义，这些事情在开始之前我就已经做好了。在短暂的思考之后，我忽然想到，是否因为我是Mac系统，从而导致了窗口不出现，于是我继续问问题： 然后继续问： 这一次，我抓到了重点。不明白为什么刚才我提到我是Mac系统的时候他不告诉我，先不管这些，在其中添加这段代码后，游戏终于可以运行了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import pygameimport random# 游戏窗口的大小WINDOW_WIDTH = 800WINDOW_HEIGHT = 600# 蛇身和食物的大小BLOCK_SIZE = 20# 定义颜色WHITE = (255, 255, 255)BLACK = (0, 0, 0)RED = (255, 0, 0)# 初始化 Pygamepygame.init()# 创建游戏窗口window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))pygame.display.set_caption(&quot;贪吃蛇游戏&quot;)pygame.display.flip()clock = pygame.time.Clock()def game_loop(): game_over = False # 蛇的初始位置和速度 snake_x = WINDOW_WIDTH // 2 snake_y = WINDOW_HEIGHT // 2 snake_x_change = 0 snake_y_change = 0 # 食物的初始位置 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE while not game_over: for event in pygame.event.get(): if event.type == pygame.QUIT: game_over = True elif event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT: snake_x_change = -BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_RIGHT: snake_x_change = BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_UP: snake_y_change = -BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_DOWN: snake_y_change = BLOCK_SIZE snake_x_change = 0 # 更新蛇的位置 snake_x += snake_x_change snake_y += snake_y_change # 绘制游戏窗口 window.fill(BLACK) pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.draw.rect(window, WHITE, [snake_x, snake_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.display.update() # 碰撞检测 if snake_x == food_x and snake_y == food_y: food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE clock.tick(20) # 控制游戏的帧率 pygame.quit()# 启动游戏game_loop() 不知道各位看出什么问题了没有，问题如下： 动作太快了，完全是地狱级别的难度； 蛇似乎不会增长，吃了物品没有变长； 撞到墙壁后游戏虽然停止了，但是没任何提示，蛇就这么消失了； 于是，我只能让chatGPT再帮我优化代码： 当然，又有了其他问题， 蛇在撞到自己之后没有办法结束游戏 游戏无法退出 游戏无法重新开始 Game Over之后显示的是乱码 当然，有编程经验的人大概能明白，乱码的话一定是因为中文显示不正确，而目前，我也没打算支持中文。干脆用英文好了。 于是，我又开始提需求，在连续三四轮这种询问之后，终于来到了最后一次： 然后我得到了如下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137import pygameimport random# 游戏窗口的大小WINDOW_WIDTH = 800WINDOW_HEIGHT = 600# 蛇身和食物的大小BLOCK_SIZE = 20# 定义颜色WHITE = (255, 255, 255)BLACK = (0, 0, 0)RED = (255, 0, 0)# 初始化 Pygamepygame.init()# 创建游戏窗口window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))pygame.display.set_caption(&quot;贪吃蛇游戏&quot;)pygame.display.flip()clock = pygame.time.Clock()def game_loop(): game_over = False game_restart = False game_paused = False while not game_restart: if game_over: show_game_over() # 蛇的初始位置和速度 snake_x = WINDOW_WIDTH // 2 snake_y = WINDOW_HEIGHT // 2 snake_x_change = 0 snake_y_change = 0 # 蛇身的初始位置和长度 snake_body = [] snake_length = 1 # 食物的初始位置 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE score = 0 while not game_over: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() quit() elif event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT and snake_x_change != BLOCK_SIZE: snake_x_change = -BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_RIGHT and snake_x_change != -BLOCK_SIZE: snake_x_change = BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_UP and snake_y_change != BLOCK_SIZE: snake_y_change = -BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_DOWN and snake_y_change != -BLOCK_SIZE: snake_y_change = BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_SPACE: game_paused = not game_paused elif event.key == pygame.K_q: pygame.quit() quit() if game_paused: continue # 更新蛇的位置 snake_x += snake_x_change snake_y += snake_y_change # 碰撞检测 if snake_x == food_x and snake_y == food_y: # 蛇吃到了食物，增加长度和得分 snake_length += 1 score += 1 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE # 更新蛇的身体 snake_head = [snake_x, snake_y] snake_body.append(snake_head) if len(snake_body) &gt; snake_length: del snake_body[0] # 检测碰撞 if snake_head in snake_body[:-1] or snake_x &lt; 0 or snake_x &gt;= WINDOW_WIDTH or snake_y &lt; 0 or snake_y &gt;= WINDOW_HEIGHT: game_over = True # 绘制游戏窗口 window.fill(BLACK) pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE]) for body_part in snake_body: pygame.draw.rect(window, WHITE, [body_part[0], body_part[1], BLOCK_SIZE, BLOCK_SIZE]) # 显示实时分数 font = pygame.font.Font(None, 28) text = font.render(&quot;Score: &quot; + str(score), True, WHITE) window.blit(text, (10, 10)) pygame.display.update() clock.tick(10) # 控制游戏的帧率 # 重新开始游戏 game_loop()def show_game_over(): font = pygame.font.Font(None, 36) text = font.render(&quot;Game Over. Press Space to restart&quot;, True, WHITE) text_rect = text.get_rect(center=(WINDOW_WIDTH // 2, WINDOW_HEIGHT // 2)) while True: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() quit() elif event.type == pygame.KEYDOWN and event.key == pygame.K_SPACE: game_loop() window.fill(BLACK) window.blit(text, text_rect) pygame.display.update() clock.tick(10)# 启动游戏game_loop() 大家可以看看效果： 我们可以看到，这个游戏相对来说是比较完整了，速度正常了，有计分系统，蛇能正常增长，碰到自己游戏会结束，碰到墙壁后也会Game Over，并且可以通过SPACE来重新开始。并且，我还加入了按Q键的时候退出的功能。 当然，还可以继续完善，比如随着时间的流逝，速度上可以渐渐加快等等。就看你怎么想，然后其他的交给chatGPT。 通过这次的示例演示，其中重点不是教大家如何做一个贪吃蛇游戏，而是教大家如何利用chatGPT来解决你需要解决的问题。当然，我需要收回我开头说的话，chatGPT并不能帮你解决你不熟悉的问题。就比如，如果我完全不懂这其中内容的话，可能我窗口都打不开，我完全都不知道我什么时候才能解决Mac系统中不一样的部分，而也正是因为有一些简单的经验，才让我考虑的那个层面，从而针对性提问解决了问题。 所以要记住，AI并不能帮你解决你完全不懂的问题，起码，你要知道你想问什么，也要知道问题大概卡在哪里了，针对性继续提问。 最后，友情提示一下，不要用API来完成这一次次的对话，经验之谈，去买个Plus，比API交互便宜多了。你看那一串串的代码重复的给你写出来，你完全不知道会耗费多少Token。那些宝贵的Token，还是用在聊天窗无法完成的任务上比较合适。","link":"/Use-AI-to-write-a-snake-game/"},{"title":"10 利用AI索引并分析文献和图片","text":"Hi, 我是茶桁。 看到我这篇文章的读者们不知道有多少人是接触过ChatGPT或者其他人工智能产品的。 市面上目前充斥着大量的人工智能产品，从聊天，文案，脚本，音乐，绘画等方方面面都涵盖了。但是不知道有多少人遇到过以下的场景不知道该如何解决： 我需要针对一篇很长的文章（可以是论文，可以是小说）进行总结或者分析的时候，就开始无从下手。因为ChatGPT在接收长度上是有限制的，这个长度我大概测试过，如果你用的是WebGPT，那么应该中文应该是在2500字左右，多一个字都会告诉你长度超出限制。而我们一篇论文，起码来说都是5000字以上的。分两段来喂给ChatGPT当然可以，但是上下文关联有时候会遇到问题，ChatGPT也会给你胡编乱造。 有的时候我从客户那里接收到的是一张图片，也许是截图，也许就是拍的一张照片。那么，怎样利用ChatGPT去分析这张图片上的内容，然后根据我的需求给我相应的答案呢？ 以上这两点，估计是很多人遇到想解决的。而今天这篇文章，就是从这两点入手教你如何解决。 大语言模型的不足 让我们打开ChatGPT来问一些常识性的问题，这个问题对于大部分上过学的中国人来说，都能从课本上了解到： “鲁迅先生在日本学习医学的老师是谁？” 结果如下图，这个“嘉泽源之助”到底是谁呢？我也不知道，得到这个答案的时候，我还特意去Google了一下，根本找不到相关资料。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Use-AI-to-index-and-analyze-documents-and-images/"},{"title":"13 使用多步提示语让AI帮你写测试","text":"Hi，大家好，我是茶桁。 很遗憾在上一讲，也就是第12讲的时候，咱们对于利用AI写一个VBA宏来执行Excel任务的过程并不顺利，仔细想来既然大家都在这里看这个系列文章了，应该也基本都会Python的，所以一个Excel自动化也并无太大影响，毕竟，这种商业软件的集成一定是早晚的事情，咱们也不必在这里死磕这一个问题。 那么本节课程呢，我们会通过chatGPT的不断交互，去完成一个测试任务。 在很多时候，我们探索性开发一些功能可以极大提高我们的效率，但是这个过程并不能做成一个完整的产品。我们理想中的产品应该是“自动化”的，我们只需要用自然语言输入自己的需求，对应的代码就自动写出来了。 那么如果中间出现了问题怎么办？当然是AI可以自己拿到反馈自己更正自己了，完全不需要人工去介入调试。 下面，让我们开始吧。 代码的起源 让AI自己调试自己的需求听起来是不是很不可思议？随着GPT-4的发布，还有就是未来模型能力的进一步增强，这个骑士并不是遥不可及。是的，我又在这里贩卖焦虑了，那些低廉的测试们，想要自己的退路了吗？ 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/"},{"title":"使用Telnet数据流看世界杯","text":"使用Telnet数据流创造了正在进行比赛的ASCII影像, 尽管这难以令人想象. 看看他是怎么做的… 在比赛开始10分钟以前, 简单的打开Windows开始-运行窗口, 输入 telnet ascii-wm.net 2006 , 你将会看到 “现场直播” 视频流. 明显的, 这是互联网应用的又一创新. 来试一下吧! http://ascii-wm.net/ 由于我家里只有电脑没有电视，所以全程我都是这么干的。。。可惜，经常与主机断开连接。。而且，很多时候我看不懂。。。郁闷。只能知道个大概。。有兴趣的可以去看看＠","link":"/Use-the-telnet-data-stream-to-watch-the-World-Cup/"},{"title":"VH 情怀黄铜原子笔","text":"「知乎专栏地址」 收到 @罗文森 赠送的笔已经很多天了，快两个星期了吧。 一直在找时间想写一篇评测出来，终究是没抽出时间，况且，我不是写手！眼看时间一天天过去，心里也越来越愧疚。 其实最主要的也不单单是写不出什么，而是总归要用它画两幅画出来，才会感觉的出来到底合不合心意。 对于文具的偏爱，估计是从学生时代就开始了。记得高中的时候缠着老爸给买了第一个奢侈品：一支派克。虽然很珍惜，却并不好用。自此对文具就更是挑剔。 一支好笔，真的能陪一个人好久好久。 打拆包之前，就一股浓浓的逼格。 不过问题来了，我不知道怎么打开包装！XD - 一直不敢使劲，生怕损坏了什么，折腾了好久，才明白，原来连着盒盖的那张纸，是用来撕的。 打开之后，金色的笔体和其皮外套分开躺在盒内，逼格更甚了。 喜欢盒子内部的那句话：“合适的形态总有他合适的作用，还原物的本质，至真至纯，用一支笔唤起书写的初心。”， 好吧，又是一个走情怀的产品。不过对于这个包装和笔本身的设计来说，这个情怀我还是蛮受用的。 而关于细节上，笔的打磨很是花费了一番功夫，那种黄铜的质感以及似乎岁月沉淀的感觉，让人爱不释手。（这些天只要是要签字的机会，我基本都拿出它来装逼）。 而关于旋转笔头，旋转的过程中特别的舒服。到位后的力回馈也刚刚好，让我转来转去玩了好久。额，给玩的不灵光了。（看来关于使用强度，长家还是需要加强）。 关于那件小皮装，似乎有点小了，装进去是用了一些力气的。额，拿出来就更费劲了，太紧了。不过相信用一段时间，也就松了。对于这样一直精致的情怀笔，这件皮衣还是相当必要的。 笔当然还是要用才能知道是否合心意, VH 这支黄铜笔，拿在手上的感觉，重量十足，根本是那些塑料笔没办法比的。但是这个重量不大不小，真的正合适。想起厂商吹的牛逼：“打造了最佳的配重”。 这牛逼不为过。 至于握感上，这个笔对于我来说似乎纤细了一点，可能也是因为我握惯了粗的笔，包括我最爱的那支53Pencil，也是比较粗的。而在厂家原装的那支笔芯，使用起来也是非常顺畅（就喜欢这样顺畅的笔芯）。不过用完需要更换笔芯的时候，就需要买好一点了，要不可惜了这么好的笔。 最后随笔画了两幅画来试笔。 太久不画画了，大家将就看吧！ 有想买的，链接在这里 : 首页-vh企业店 PS：VH家的那个无线充电器，也是逼格满满。打算入一个。。。","link":"/VH-brass-pen/"},{"title":"VPS 设置 Hexo","text":"首先需要感谢@lucifr，我现在这篇文就是在iPad上登录VPS完成的。最后还是忍不住入手了下边两个APP: 当然我其实到现在并不完美，因为rsync和自动执行generate的代码我没有完成。安装incrond的时候总会出错，于是无法执行集群文件同步.所以现在还是在终端里执行generate和cp -rf /home/xxxx/* /home/xxxx 我这里并不是要教设置步骤，因为其实@lucifr 已经在他的这篇文里写的很清楚了，我就写几点注意事项 搞定VPS操作和基本的Linux命令很重要。 要搞定lnmp，参照这里的lnmp详细介绍 新版本的Hexo有更改，在同一目录里是找不到/cli/generate.js的，更别说console.log语句了 @lucifr所说的新建立一个Dropbox账户，意思是在VPS主机上建立一个账户用来执行Dropbox同步，而不是新建立一个Dropbox账户。 其他… 好吧，写其他是因为iPad上用VI进行编辑实在有点难受，现在先这样了，以后有时间了再写一个更详细的。","link":"/VPS-setting-hexo/"},{"title":"VSCO FILM 00 FREE STARTER PACK","text":"本文知乎专栏 VSCOCam是iOS上一款滤镜相机，其最著名的地方就是他们的胶片滤镜。 而其实VSCO在这款APP之前，就一直在做胶片滤镜，有OS X和WIN两个平台的版本。最主要的是作为LightRoom的插件存在。 而现在提供免费下载的这款滤镜就是针对LightRoom的滤镜插件，有OS X和WIN两个版本提供。 We are thrilled to announce VSCO Film 00, our first-ever FREE starter pack. Perfect for anyone who uses VSCO Cam and is looking to take the next step, VSCO Film 00 brings beautiful presets, custom camera profiles, and the familiar VSCO editing experience to your desktop in Adobe Lightroom. VSCO Film 00 includes two of our most popular film stocks (Kodak Gold 100 from Film 05 and Kodak Tri-X from Film 06) and is available for download now. 有需要的可以点击原文链接进行下载，不要感谢我，我是雷锋。 PS: 关于之前我提到的Enlight导出图片丢失数据的部分，我在此向读者和开发者致歉，原因是导出的如果是PNG图片是不能保存那些信息的，可以将图片的质量从Pro调小到Hight, 保存的就是JPEG图片，那么导出的图片地理位置和相机信息就会一并保存了。","link":"/VSCO-FILM-00-FREE-STARTER-PACK/"},{"title":"关于我","text":"茶桁是我笔名，80后, 老派思想者, 固守着年代感的东西... 和大部分这个年代成长起来的孩子一样, 怀念着龙珠, 灌篮高手, 追着海贼王. Github: @hivandu Twitter: @hivan Design: @hivandu Notes: @DUART 对新奇的事物仍然抱持敏感而探索的心境, 并且喜欢有条不紊. 坚信有生之年能看到人类和AI和平共处的景象, 坚信有生之年可以看到第一批星际移民. 这里, 我们谈谈思维, 哲学, 经济以及信息管理. 还会夹杂着我的一些私货, 那都是一些遥远的技能, 已经离我而去很久了. 说了那么多, 先搬家吧. 另外, 在我另一个试验田里, 有着自己对产品和技术的追求, 也欢迎大家围观; 这篇博客是简历在Hexo + Github Pages + vercel上, 有兴趣的小伙伴可以自行Google一下相关教程, 这里就不引述了.","link":"/about/"},{"title":"数据产品经理的自我修养","text":"","link":"/accomplishment_dpm/"},{"title":"Alfred 2 Plugin -- open in Atom","text":"用于在Alfred 2中用命令快速调用Atom打开所见文档，类似于Open in Sublime， 为了自己方便建立了一个workflow，有需要的下载吧！ Open in Atom.alfredworkflow","link":"/alfred-2-plugin----open-in-atom/"},{"title":"测试allove易博","text":"这次测试是在模拟器上做的，测试下来效果都还不错。就是模拟器本身在速度上有点难以接受。而截图一会会在上传到相册之后再转链过来，和大多数android博客程序一样，功能上都似乎差不多。。。而易博多加了两项功能。。。一个是feed rss订阅，还有一个是涂鸦功能。不过我看来，订阅功能完全可以取消。因为本身功能并不是很强大，并且对程序本身也是个累赘，占用一定的资源。况且现在feed订阅的apk数不胜数，而我本人多习惯打开浏览器通过google reader进行阅读。建议阉割掉订阅而专心做一样比较好。。。。而另外一样涂鸦绝对是亮点，大多数时候写博客人会希望有什么东西能够画一个草略的示意图。。。我多数时候都有这种想法。加上android系列手机的触摸屏，一定能够有完美的手绘体验。。。。 自己先在这里鼠标绘制一个试试看。呵呵。。。。 真累啊，就先这样吧. 涂鸦发表出来才看到,原来不支持横屏...因为我是横向画的,所以.....呃! 建议两点,一点是支持横屏涂鸦上传,还有一点是支持更改画布背景! 还有就是,默认似乎软件在发布文章以后强制性插入自己软件的Google Adsense,并且软件内没有地方可供更改!这点我很不喜欢...流氓性质需要改改!我理解开发者希望有额外收入的心情,但请尊重博客作者和读者,尊重软件使用者和你自己! BTW:当然,我给予的评价和评论都不甚完善...并且也只是我一个人的建议而已!如果读者对这款测试apk有兴趣的话(恩,还没放出)可以联系alloveteam[at]gmail.com索要!但是记得要对此软件有建设性意见.不要尝新后就over了.有好的建议,开发者才能更好的去完善,我们才有更好的软件可以使用!对吧?","link":"/allove/"},{"title":"android 2.1版本无法开机解决","text":"不含刷的变砖的机子！ 这个版本的rom是一定需要SPL的，比起HTC hero的rom来说速度有提升！但是没那么华丽(当然我说的是源生系统，我对theme之类的不太有兴趣！)。。。 但是刷SPL有风险，请慎重行事！ 关于刷机2.1版本后无限火花不开机的情况，主要是Recovery的问题，刷成V2.5应该都可以解决！本人测试OK。 手机端安装可以下载程序：flashrec.apk PC端安装需要USB驱动，然后通过更新工具更新。这里一定要注意区别清楚机型版本，不同机型是不能通用的，再次提示，一定要注意看清楚，千万不要着急。 点击此行下载用于 G1 和 沃达丰版 G2 使用的更新程序 大小: 5369404 字节 MD5: 8CA35537D253EB19CC0D28A45D153FC4 SHA1: 88C3E3ED444A1B96C59594215A81B5C5F589C42E CRC32: 97089632 点击此行下载用于 HTC 版 G2 使用的更新程序 大小: 4848659 字节 MD5: 7217E7DCCFEAB4BFB254B86778FFBD5C SHA1: 8373BDB338D3E6EBA8622CDC19EFBF7AE3484451 CRC32: A9BD3E5E 再次提醒，千万别下错了！ 保证手机与电脑连接无误后，双击更新程序运行即可。","link":"/android-2-1/"},{"title":"Android 2.2 App2sd 问题","text":"其实比起以前版本的app2sd来说,设置是一样的!只是多了一个步骤,就是需要给rom添加一个sdext.然后所有的设置就和以前的版本一模一样了. 首先当然需要有一个已经分好区的sdcard,具体设置可以查看我以前的文,有ubuntu下进行分区的和windows下的! 然后需要最近版本的SPL和Radio,这个本人不提供了,可以自行解决!伸手党可以留下自己的邮箱,我提供下载地址! 准备工作做足后,第一步就是需要在手机上建立一个sdext访问.这里提供一个文本文档:下载： fr-patch134.zip 放在sdcard根目录,然后在连接手机的情况下在终端如下操作: adb shell # sh /sdcard/fr-patch134.txt sdext busybox df -h 如果看到有/sd-ext分区,OK,以下的事情就顺理成章了,参考我以前发布的app2sd步骤操作就好了! 至此所有问题解决! 此处为后续更新,由于之前忽略了点东西,所以这里做一个补充! 由于2.2rom和以前版本的一些差别,在做app2sd之前,需要挂在system,sd-ext和data分区,这是需要注意的一点!挂在命令为:mount 例子: mount system 有什么不明白的再问吧!","link":"/android-2-2-app2sd/"},{"title":"Android 2.2 for G1","text":"親愛的Android,我回歸了...說實話,後兩個月我真的有些壓抑.現在感覺解脫了! Android 2.2 For G1 已經被C神放出,貌似解決了相機問題,而其他問題暫時沒有進行測試,因為本人也正在下載中...萬分期待! 先提供下載地址,等待我適用後再放試用報告.. http://drop.io/ionstorm/asset/defcon-dream-ota-eng-t1-signed-zip PS:如果沒有語言要求可以下載這個版本就OK了,如果強烈需求中文,需要下載中文補丁包 试用报告: 仍然没有中文 系统默认没有中文输入法(这个到不是问题) 没有Google Map,市场无法下到.并且安装了4.2开发版后打开就崩溃(这点对于喜欢Buzz的朋友是个致命伤) 3D图库效果有. 相机不能用,和2.1不同的是,就算拍照有图片,打开来看也是一个android小人.... WIFI正常使用 蓝牙没有测试. 速度真的比2.1快了不少,没有一点卡的感觉. Settings里有CyanogenMod settings,可以直接更新Rom,可是我这里链接失败. Apn需要自行设置,具体的设置方法可在网上Google Vpn使用正常 因为地图没有打开,所以GPS模块没有测试. 其他不进行补充了,由于以上有些原因是我无法接受的.所以打算刷回1.6rom,以前刷2.1是为了绑定自己和公司双帐号,现在不必了!不用忍受2.1的速度了...","link":"/android-2-2-for-g1/"},{"title":"Android 3.0 Preview","text":"整个界面都显得很陌生啊!大家可以从视频中看到. 锁屏界面更改很大,目前不知道是解锁手势还是横向滑动还是画圈. HomeScreen主屏幕，可以看到四角的设计，左上给搜索按钮，支持语音搜索，右上为应用菜单按钮，左下为返回、Home、菜单按钮，右下角则是状态标志，屏幕中间为程序快捷菜单和桌面工具 有一个社交工具的集合，可以查看不同来源的好友更新.不过这个在大陆的情况就...你们懂得! 有个桌面管理器,可以进行桌面的各项设置. 拥有全新的浏览器，可以查看各种完整版网页 拥有全新的Android版本Gmail界面! Gtalk界面也是全新的,并且支持双向视频聊天!不过流量上....不敢想! 全新的Youtube视频墙,不过在国内,我们都是墙内的,它在墙外而已! &lt;li&gt;全新的Google地图界面,支持3D模式导航!&lt;/li&gt; 此外还会有Google图书等更多全新内容!","link":"/android-3-0-preview/"},{"title":"Android 3.0","text":"好吧,7.1号Eldar Murtazin在其博客上已经放出一个可信度比较高的谣言,就是Android3.0将于十月份发布,代号姜饼(Gingerbread) 据传,3.0rom将会针对高端市场,分辨率达到了1280X760,支持此系统的最低配置将为1Ghz处理器,512M内存.可以这么说,在3.0系统发布以后,就可以正式宣布G1被彻底淘汰了. Android 3.0 Gingerbread will be released in mid- October (around 15 -16th), 2010. First handsets shipping in November/December – for the Holiday Season. Minimum hardware requirements for Android 3.0 devices are: 1GHZ CPU, 512MB or RAM, displays from 3.5” and higher. (We all, of course, heard that Android handsets with 2GHz CPU’s are coming) New 1280×760 resolution available for the devices with displays of 4” and higher. (Anyone thinking about Android tablets now? ) Completely revamped user interface. If you want to get a feeling of what Android 3.0 Gingerbread UX is like, check out the Gallery App on Nexus One. The same overall feel, light animated transitions,etc. Natively, through all the UI. Android’s split into 2 branches becomes official. 3.0 for top of the line/high end devices. Cheap, low-end mass market handsets will keep Android 2.1/2.2 详情可以参看:http://www.unwiredview.com/2010/06/30/android-3-0-gingerbread-details-1280x760-resolution-1ghz-minimum-specs-mid-oct-release/","link":"/android-3-0/"},{"title":"Android 4.0 通讯录与Google+的深度整合","text":"在4.0以前,我记得Android里的通讯录名称是“contact”.而在4.0之后,我的手机上是4.0.3,将其更改为“people”了. 而更改的不仅仅是这小小的名称!大家都记得在之前版本的contact里,拨打界面和通讯录界面是整合在一起的.所以那个时候我第一屏单单放一个拨打电话就OK了.需要查找通讯录的话,可以切换tab. 1.理念: 而在4.0里面,你在people里面找不到拨打电话界面,而在拨打电话的界面里也找不到通讯录.Google将这两个单独分立了开来!貌似不方便了,其实不是,反而变得异常容易整理和操作!Google在Android4.0里的理念就是将电话和联系人完全分离开,这本来就不是一回事!智能手机时代,谁说的联系人就一定只能是打电话和发短信的? 2.整合: 说了些小变化,下面切入正题,就是Android 4.0 之后通讯录与Google+的深度整合! 这也是我昨晚没事整理联系人的时候才注意到的,不知道是不是4.0之处就是这样的,还是到了4.0.3的改变! 当我们进入一个单独的联系人界面的时候,发现除了常用的PHONE,EMAIL等选项之后,会有一个CONNECTIONS项目选项,这里会显示手机上安装过的SNS程序里联系人的关联账户! 比如WhatsApp,Twitter或Facebook等! 当然,你也有可能看到一个Add connection选项. 这个选项,其实就是和Google+整合的选单! 可以选择添加联系人到自己的Google+ 圈子里!当然,有可能你的联系人根本没有在Google+注册过! 我猜测,Google以后有可能会将Google+的圈子代替联系人来使用!也就是说,你所有的联系人都是要圈养的.不管他是否注册过Google+,利用圈子的概念管理联系人,其实比较起来而言,比Gmail里的通讯录要高效一点! 接下来,才是深度整合的重点,以我自己为例: 用过4.0的朋友都知道,联系人选单是可以向左拖动的,就是右边还有一块和联系人有关的“update”选项!当然,这一块内容需要对方联系人有Gtalk或者Google+才会显现出来! 平时就会显示Gtalk上的签名状态,而对于有Google+的联系人,则会显示他在Google+中的信息! 我们大家应该都用过MSN或者QQ,应该可以想象一下,MSN当初和Space深度整合,QQ和QQ空间深度整合的情形! 当你查看某个联系人的时候,他在自己空间里发表的文章或相片,都会在其信息里显示出来!当然,QQ有的时候会闹点小情绪,来一两次大姨妈…我们可能没那么及时看到!所以说这个概念并不新奇,只是这次Google借用到了手机上而已! 哈哈,想象一下,够方便吧? 3.想法: 写到这里,我到觉得Google有些小气了. 右边状态栏完全可以让出来给其他的SNS APP来使用而不只是Google+,譬如Twitter和Facebook,或者是QQ空间!既然有了这样一个功能,而又是在消费者手上的,那么决定权就该交给消费者!否则,自己联系人里没多少用Google+的,右边信息栏岂不是浪费? 我想,再以后,这块地方应该会被其他SNS APP占用吧!届时,通过手机通讯录,大家就可以看到某人最近在做什么了!而无需再登录单独的App去查看! 4.后记: 当然,除了Contact以外,Gallery等都和Google的产品有深度整合!而这些,和其他APP整合的可能性是有的!比如Gallery里直接查看Flickr里的相片而不只是Picasa.不过在Google推销自己的Google+这段时间里,我看是不太可能了!","link":"/android-4-contacts-the-depth-integration-with-google-plus/"},{"title":"Android G1 的优化","text":"我是第一批使用android的用户,那个时候没有别的选择,只有G1好选择.所幸买的是英版全白.这个机型据说是很少的.包装里带彩贴的那种! 可是时间长了,G1的诟病也就出现了,系统不断升级,虽然有很多自制包提供下载,解决了官方不在支持G1的问题,可是速度上和原来的设计问题没有办法解决! 不过好在网民的智慧是无穷的,今天带给大家三个特殊的小东西,用来优化你得G1. 10m rom HACK 刷新方法，拷贝boot-cm_2629-dp_mem-xtra.img文件到sd卡，启动到recovery模式下，进入console： mount -a flash_image boot /sdcard/boot-cm_2629-dp_mem-xtra.img JIT enabled Dalvik VM 进入recovery直接升级zip文件就好了 Audio Hack v3.2 apk 这个是mark里2欧元的付费软件.用来加大默认铃音. 下载地址: [download id=\"1\"]","link":"/android-g1-optimization/"},{"title":"Make Android Your Own-Androidify","text":"Androidify,由Google推出的一个app，可以让Android使用者在手机上创造属于自己的形象Android。是一套单纯的纸娃娃系统,可以自定义包括肤色、发色、衣服、裤子、鞋子、饰品等众多套件，还可变更身体的比例，其实还颇好玩的哩。只要直接在Android Market搜寻Androidify就能找到这个可爱的小软件。 跳转有Google针对这个app推出的小短片。不过还不如自己赶快下载回来玩比较实在。","link":"/android-ownandroidify/"},{"title":"","text":"在安卓中叫兽曾经写过两个换肤教程.而教程中写的是替换原文件包...其实这个完全没必要.只要在模拟器的快捷方式中加入一段代码就好了... -avd avdname -skin skinname 而皮肤文件可以分开来放.如图:","link":"/android-skin/"},{"title":"Android 简易访问Twitter,youtube,facebook方法","text":"其实就是修改hosts文件. 而这个hosts文件我是已经修改完毕的...直接cat到手机内覆盖原文件就可以了! 执行之前请将hosts复制到sdcard的根目录,然后cmd,cd如adb目录,然后执行其下代码: adb remount cat /sdcard/hosts &gt; /etc/hosts 一切搞定! 请对于hosts上的IP地址低调传播,谢谢! PS:本hosts修改大法已经基本完全失效,基本所有有效果的IP地址都被屏蔽,如果有条件,自己建立一个VPN吧!不过对于联通的用户我要给你一个大大的警告:联通屏蔽VPN.....","link":"/android-twitteryoutubefacebook/"},{"title":"Auto-save-photo-to-qqmail","text":"前言 为什么是QQ Mail? 因为它大,而且不断自动扩容,你想把它装满暂时是不太现实. 而且来说,QQ邮箱的体验还是非常不错的!过滤规则也很能满足要求,归档搜索查找都不错!一些不涉及隐私而又想保存的文件或者照片或者其他什么东东,存在QQ邮箱里还是不错的!比如:XXX 如果想同步到Google+请看完文章后看最后部分的更新说明! 如何实现 通过众所周知的ifttt 其实,这主要是一个我为了保存自己照片的方式!(爱信不信,不相信拉倒!) 建立一个task, if Instagram 设定条件:New Liked photo, then Gmail 设定条件:Send an email. 好了,填上 To address: xxx@qq.com 就OK了! 简单吧? 如果你熟悉某个联系人,那么建立规则by Username,然后包含此用户名关键词的主题都标上相应的关键词.比如by ladiiprang在邮箱规则里就可以加上\"妹子\"的tags. 注意点 如果你不想后期被众多的新邮件搞得头昏脑胀的话,那么你一开始就要设定好过滤条件! 在ifttt中,Send an email的时候Subject记得填写上一些关键词,比如From Instagram,这样,对于主题内有关键词的邮件就好管理的多了.添加过滤规则就好了!将来自己发送邮箱Gmail的邮件主题包含From和Instagram的都自动移动到一个新建的文件夹内,Ex:Photo DB,完工! 后记 同理,我们也可以建立来自Flickr的发送规则,原理是一样的!注意Gmail邮箱里的过滤条件要建好!否则Gmail爆满是迟早的事情!运用这样的规则,我们还可以发送Dropbox里的文档到QQ邮箱内保存,不过规则限定发送的只能是Public内的文档! 其实一开始我不确定是发送文件还是只有地址!所以我开始的方法很绕,就是将相片想Save到Dropbox,然后再通过Dropbox建立if.then.Gmail.不过试验下来既然能直接发送文件,建立task就简单多了! 还等什么,快去你的Instagram和Flickr上收藏妹子到邮箱内吧! 更新 本来因为标题的原因,这点是不加在这里的!但是想着再写一篇一样意义的文章很没意思,所以就在这里说明一下好了! 在我这篇文章发布之后,G+上看到了电脑玩物的作者+esor huang 的一篇讲解Instagram同步到Dropbox和Google+的说明!以及这篇E文!说起来,这样的同步方式确实很笨拙.你打算电脑24小时开着picasa来为你同步么?那么,我从我这篇文的基础上考虑可行方案!记得之前Picasaweb给每个人都有一个邮箱推送地址!就是类似username.password@picasaweb.com这样的地址!好吧,有邮箱地址就简单了.不过这个地址需要你再登录picasaweb.com去找,在Google+页面上是找不到的!同理,Flickr也有类似的推送地址! 该怎么做我想你已经清楚了吧! 最后,我想到了是否同样可以传送到QQ相册!毕竟和邮箱最大的不同就是相册是用来分享的,而邮箱是用来保存的!可惜,QQ没有针对相册的推送,而推送到QQ空间的XXX@qzone.qq.com这个地址也是必须QQ邮箱内部发才行!是的,和你们一样,我又想到了邮箱转发规则.用QQ邮箱收到邮件后转发到QQ空间邮箱去不就好了!测试后,果然. ... 果然没那么简单,这次我失误了,特么的QQ小气到不允许自己的QQ邮箱转发邮件到QQzone的邮箱来自动推送文章!提示这是一个无效地址!不过也无所谓了,毕竟是推送文章的邮件地址,你也不想自己的QQ空间全是大片的文章,而且每篇文章里只有一张相片吧 ? 这个说明本来是在G+上有提出的,但是有基友测试成功后给的是这里的url,所以我就想,补上这个说明!谁说是一样的到底,但是如果不提Picasaweb有邮件推送地址,估计很多人都已经忘记了!为了找同步到G+上的朋友会看得糊里糊涂!","link":"/auto-save-photo-to-qqmail/"},{"title":"Auto operation Weibo","text":"The code address of this article is: auto operation weibo Chromedrive download: Taobao Mirror , need to be consistent with your Chrome version auto operation weibo 123456789101112131415161718192021222324252627from selenium import webdriverimport timedriver = webdriver.Chrome('/Applications/chromedriver')# login weibodef weibo_login(username, password): # open weibo index driver.get('https://passport.weibo.cn/signin/login') driver.implicitly_wait(5) time.sleep(1) # fill the info: username, password driver.find_element_by_id('loginName').send_keys(username) driver.find_element_by_id('loginPassword').send_keys(password) time.sleep(1) # click login driver.find_element_by_id('loginAction').click() time.sleep(1)# set username, passwordusername = 'ivandoo75@gmail.com'password = 'ooxx'# Mobile phone verification is required here, but still can’t log in fully automaticallyweibo_login(username, password) follow user 12345678910111213141516171819202122def add_follow(uid): driver.get('https://m.weibo.com/u/' + str(uid)) time.sleep(1) # driver.find_element_by_id('follow').click() follow_button = driver.find_element_by_xpath('//div[@class=&quot;btn_bed W_fl&quot;]') follow_button.click() time.sleep(1) # select group group_button = driver.find_element_by_xpath('//div[@class=&quot;list_content W_f14&quot;]/ul[@class=&quot;list_ul&quot;]/li[@class=&quot;item&quot;][2]') group_button.click() time.sleep(1) # cancel the select cancel_button = driver.find_element_by_xpath('//div[@class=&quot;W_layer_btn S_bg1&quot;]/a[@class=&quot;W_btn_b btn_34px&quot;]') cancel_button.click() time.sleep(1)# 每天学点心理学UIDuid = '1890826225'add_follow(uid) create text and publish 1234567891011121314151617181920212223242526272829303132333435def add_comment(weibo_url, content): driver.get(weibo_url) driver.implicitly_wait(5) content_textarea = driver.find_element_by_css_selector('textarea.W.input').clear() content_textarea = driver.find_element_by_css_selector('textarea.W.input').send_keys(content) time.sleep(2) comment_button = driver.find_element_by_css_selector('.W_btn_a').click()# post the textdef post_weibo(content): # go to the user index driver.get('https://weibo.com') driver.implicitly_wait(5) # click publish button # post_button = driver.find_element_by_css_selector('[node-type=&quot;publish&quot;]').click() # input content word to textarea content_textarea = driver.find_element_by_css_selector('textarea.W_input[node-type=&quot;textEl&quot;]').send_keys(content) time.sleep(2) # click publish button post_button = driver.find_element_by_css_selector(&quot;[node-type='submit']&quot;).click() time.sleep(1)# comment the weiboweibo_url = 'https://weibo.com/1890826225/HjjqSahwl'content= 'here is Hivan du, Best wish to u.'# auto send weibocontent = 'Learning is a belief!'post_weibo(content)","link":"/auto_operation_weibo/"},{"title":"针对某一个项目自动切换node版本","text":"nvm作为node的版本管理器，并不具备自动切换版本切换的功能，有的时候我们需要针对某一个项目切换当前的node版本，这个时候就需要用到其他工具了。比如avn 举例项目:project 因为最近Node更新到10之后，我将系统默认版本切换到了10，有不更新不舒服斯基强迫症 而project 编译的版本为8，否则会出现编译出错。 123$ brew install nvm$ nvm i -g avn$ avn steup 之后在project根目录中添加一个文件.node-version 123$ touch .node-version$ echo v8 &gt;&gt; .node-version #node需要切换的版本$ echo `source &quot;$HOME/.avn/bin/avn.sh&quot; # load avn` &gt;&gt; ~/.zshrc 这样就可以了。 不过不排除报错的情况，如果是brew 安装的nvm, 则默认nvm.sh并不在~/.nvm目录内，这个时候可能需要在执行一下某段脚本。一样添加到~/.zshrc内 1$ echo `[[ -s &quot;$(brew --prefix nvm)/nvm.sh&quot; ]] &amp;&amp; source $(brew --prefix nvm)/nvm.sh` &gt;&gt; ~/.zshrc 再切换一下项目目录 12$ cd $project$ avn activated v8.11.2 (avn-nvm v8.11.2) 至此完成了！","link":"/avn-change-node-version-for-a-project/"},{"title":"Boston house analysis","text":"The source code: Boston House 1234567891011121314151617181920212223# Import package# Used to load the Boston housing price data setfrom sklearn.datasets import load_boston# pandas toolkit If you are unfamiliar with pandas, you can refer to the official 10-minute tutorial: https://pandas.pydata.org/pandas-docs/stable/10min.htmlimport pandas as pdimport numpy as np# seaborn for drawingimport seaborn as snsimport matplotlib.pyplot as plt# Show drawing%matplotlib inlinedata = load_boston() # load datasedata.keys() # Fields inside datadf = pd.DataFrame(data['data'])# Looking at the first 5 rows of the dataframe, we can see that the column names are numbersdf.head(5)data['feature_names'] # Feature name The Table params and chinese info 123456789101112131415params chinese infoCRIM 住房所在城镇的人均犯罪率ZN 住房用地超过 25000 平方尺的比例INDUS 住房所在城镇非零售商用土地的比例CHAS 有关查理斯河的虚拟变量（如果住房位于河边则为1,否则为0 ）NOX 一氧化氮浓度RM 每处住房的平均房间数AGE 建于 1940 年之前的业主自住房比例DIS 住房距离波士顿五大中心区域的加权距离RAD 离住房最近的公路入口编号TAX 每 10000 美元的全额财产税金额PTRATIO 住房所在城镇的师生比例B 1000(Bk-0.63)^2,其中 Bk 指代城镇中黑人的比例LSTAT 弱势群体人口所占比例MEDV 业主自住房的中位数房价（以千美元计） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# Replace numeric column names with feature namesdf.columns = data['feature_names']df.head(5)# The target is the house price, which is also our target value. We assign the target value to the dataframedf['price'] = data['target']df.head(5)# View the correlation coefficient between the feature and price, positive correlation and negative correlationsns.heatmap(df.corr(), annot=True, fmt='.1f')plt.scatter(df['RM'], df['price'])plt.figure(figsize=(20, 5))# View the data distribution display of some features and pricefeatures = ['LSTAT', 'RM']target = df['price']for i, col in enumerate(features): plt.subplot(1, len(features), i+1) x = df[col] y = target plt.scatter(x, y, marker = 'o') plt.title('{} price'.format(col)) plt.xlabel(col) plt.ylabel('price')# Simple example: univariate forecast pricex = df['RM']y = df['price']history_notes = {_x: _y for _x, _y in zip(x,y)}history_notes[6.575]# Find the top three prices that are closest to RM:6.57,similary_ys = [y for _, y in sorted(history_notes.items(), key=lambda x_y: (x_y[0] - 6.57) ** 2)[:3]]similary_ys# Calculate the average of threenp.mean(similary_ys) Use historical data to predict data that has never been seen before, the most direct method K-Neighbor-Nearst 12345678def knn(query_x, history, top_n = 3): sorted_notes = sorted(history.items(), key = lambda x_y: (x_y[0] - query_x)**2) similar_notes = sorted_notes[:top_n] similar_ys = [y for _, y in similar_notes] return np.mean(similar_ys)knn(5.4, history_notes) In order to obtain results faster, we hope to obtain predictive power by fitting a function \\[ f(rm) = k * rm + b \\] Random Approach \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} (\\hat{y_i} - y_i) ^ 2 \\] \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] 1234567891011121314151617181920212223def loss(y_hat, y): return np.mean((y_hat - y)**2)import randommin_loss = float('inf')best_k, best_b = None, Nonefor step in range(1000): min_v, max_v = -100, 100 k, b = random.randrange(min_v, max_v), random.randrange(min_v, max_v) y_hats = [k * rm_i + b for rm_i in x] current_loss = loss(y_hats, y) if current_loss &lt; min_loss: min_loss = current_loss best_k, best_b = k, b print(f'{step}, we have func f(rm) = {k} * rm + {b}, lss is :{current_loss}')plt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x]) Monte Carlo simulation(蒙特卡洛模拟) Supervisor \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{k}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i) * rm_i \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{b}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i)\\] 123456789101112131415161718192021222324252627def partial_k(k, b, x, y): return 2 * np.mean((k*x+b-y) * x)def partial_b(k, b, x, y): return 2 * np.mean(k*x+b-y)k, b = random.random(), random.random()min_loss = float('inf')best_k, best_b = None, Nonelearning_rate = 1e-2for step in range(2000): k, b = k + (-1 * partial_k(k, b, x, y) * learning_rate), b + (-1 * partial_b(k, b, x, y) * learning_rate) y_hats = k * x + b current_loss = loss(y_hats, y) if current_loss &lt; min_loss: min_loss = current_loss best_k, best_b = k, b print(f'setp {step}, we have func f(rm) = {k} * rm + {b}, lss is :{current_loss}')best_k, best_bplt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x]) Supervised Learning We turn the forecast of housing prices into a more responsible and sophisticated model. What should we do? \\[ f(x) = k * x + b \\] \\[ f(x) = k2 * \\sigma(k_1 * x + b_1) + b2 \\] \\[ \\sigma(x) = \\frac{1}{1 + e^(-x)} \\] 12345678910111213141516171819def sigmoid(x): return 1 / (1+np.exp(-x))sub_x = np.linspace(-10, 10)plt.plot(sub_x, sigmoid(sub_x))def random_linear(x): k, b = random.random(), random.random() return k * x + bdef complex_function(x): return (random_linear(x))for _ in range(10): index = random.randrange(0, len(sub_x)) sub_x_1, sub_x_2 = sub_x[:index], sub_x[index:] new_y = np.concatenate((complex_function(sub_x_1), complex_function(sub_x_2))) plt.plot(sub_x, new_y) We can implement more complex functions through simple, basic modules and repeated superposition For more and more complex functions? How does the computer seek guidance? What is machine learning? The shortcomings of this method of KNN, what is the background of the proposed linear fitting How to get faster function weight update through supervision method The combination of nonlinear and linear functions can fit very complex functions Deep learning we can fit more complex functions through basic function modules Assigment: \\[ L2-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{(\\hat{y} - y)}^2 \\] \\[ L1-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{|(\\hat{y} - y)|} \\] L2-Loss becomes L1Loss and achieves gradient descent Realize L1Loss gradient descent from 0 1. import package 12import numpy as npimport pandas as pd 2. load data 1234567891011from sklearn.datasets import load_bostondata = load_boston()data.keys()data_train = data.datadata_traget = data.targetdf = pd.DataFrame(data_train, columns = data.feature_names)df.head()df.describe() # Data description, you can view the statistics of each variable 3. Data preprocessing Normalization or standardization can prevent a certain dimension or a few dimensions from affecting the data too much when there are very many dimensions, and secondly, the program can run faster. There are many methods, such as standardization, min-max, z-score, p-norm, etc. How to use it depends on the characteristics of the data set. Further reading-数据标准化的迷思之深度学习领域 12345678910111213from sklearn.preprocessing import StandardScaler# z = (x-u) / s u is the mean, s is the standard deviationss = StandardScaler() data_train = ss.fit_transform(data_train)# For linear models, normalization or standardization is generally required, otherwise gradient explosion will occur, and tree models are generally not requireddata_train = pd.DataFrame(data_train, columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'])data_train.describe() # y=Σwixi+# Because the derivation of b is all 1, add a bias b to the data and set it to 1, as a feature of the data and update the gradient wi*b=widata_train['bias'] = 1data_train Divide the data set, where 20% of the data is used as the test set X_test, y_test, and the other 80% are used as the training set X_train, y_train, where random_state is the random seed 1234567from sklearn.model_selection import train_test_splittrain_x, test_x, train_y, test_y = train_test_split(data_train, data_traget, test_size = 0.2, random_state=42)print('train_x.shape, train_y.shape', train_x.shape, train_y.shape)print('test_x.shape, test_y.shape', test_x.shape, test_y.shape)train_x = np.array(train_x) Model training and gradient update 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def l1_cost(x, y, theta): &quot;&quot;&quot; x: 特征 y: 目标值 thta: 模型参数 &quot;&quot;&quot; k = x.shape[0] total_cost = 0 for i in range(k): total_cost += 1/k * np.abs(y[i] -theta.dot(x[i, :])) return total_costdef l2_cost(x, y, theta): k = x.shape[0] total_cost = 0 for i in range(k): total_cost += 1/k * (y[i] -theta.dot(x[i,:])) ** 2 return total_costnp.zeros(10).shapedef step_l1_gradient(x, y, learning_rate, theta): &quot;&quot;&quot; Function to calculate the gradient of the MAE loss function Return the gradient value 0 for the non-differentiable point at 0 X:特征向量 y：目标值 learing_rate:学习率 theta:参数 &quot;&quot;&quot; n = x.shape[0] # print(n) e = y - x @ theta gradients = - (x.T @ np.sign(e)) / n # sign is a sign function thata = theta - learning_rate * gradients return thetadef step_l2_gradient(x, y, learning_rate, theta): k = x.shape[0] n = x.shape[1] gradients = np.zeros(n) for i in range(k): for j in range(n): gradients[j] += (-2/k) * (y[i] - (theta.dot(x[i, :]))) * x[i, j] theta = theta - learning_rate * gradient return theta# def step_gradient(X, y, learning_rate, theta):# &quot;&quot;&quot;# X:特征向量# y：目标值# learing_rate:学习率# theta:参数# &quot;&quot;&quot;# m_deriv = 0# N = len(X)# for i in range(N):# # 计算偏导# # -x(y - (mx + b)) / |mx + b|# m_deriv += - X[i] * (y[i] - (theta*X[i] + b)) / abs(y[i] - (theta*X[i] + b))# # We subtract because the derivatives point in direction of steepest ascent# theta -= (m_deriv / float(N)) * learning_rate# # theta = theta - learning_rate * gradients# return thetadef gradient_descent(train_x, train_y, learning_rate, iterations): k = train_x.shape[0] n = train_x.shape[1] theta = np.zeros(n) # Initialization parameters loss_values = [] # print(theta.shape) for i in range(iterations): theta = step_l1_gradient(train_x, train_y, learning_rate, theta) loss = l1_cost(train_x, train_y, theta) loss_values.append(loss) print(i, 'cost:', loss) return theta, loss_values# Training parameterslearning_rate = 0.04 # Learning rateiterations = 300 # Number of iterationstheta, loss_values = gradient_descent(train_x, train_y, learning_rate, iterations)","link":"/boston_analysis/"},{"title":"Boston house price CART regression tree","text":"On the code 12345678910111213141516171819202122232425262728293031323334353637383940# CART regression tree predictionfrom sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_bostonfrom sklearn.metrics import r2_score,mean_absolute_error, mean_squared_errorfrom sklearn.tree import DecisionTreeRegressor,export_graphvizimport graphviz# Prepare data setboston = load_boston()# Explore dataprint(boston.feature_names)# Get feature set and pricefeatures = boston.dataprices = boston.target# Randomly extract 33% of the data as the test set, and the rest as the training settrain_features, test_features, train_price, test_price = train_test_split(features,prices,test_size=0.33)# Create CART regression treedtr = DecisionTreeRegressor()# Fitting and constructing CART regression treedtr.fit(train_features, train_price)# Predict housing prices in the test setpredict_price = dtr.predict(test_features)grap_data = export_graphviz(dtr, out_file=None)graph = graphviz.Source(grap_data)# Result evaluation of test setprint(f'Regression tree mean squared deviation:',mean_squared_error(test_price, predict_price))print(f'Regression tree absolute value deviation mean:',mean_absolute_error(test_price, predict_price))# Generate regression tree visualizationgraph.render('Boston') !&gt; Before running this code, please ensure that the relevant dependencies have been installed;","link":"/boston_data_CART/"},{"title":"bye-google-reader","text":"今早一打开网页,满篇都是关于Google将于7月1日正式关闭Reader的消息. 再见吧,Reader. 这八年,基本每天,我都会去看看你... 可是又能如何,到了该走的时候了.即便这个理由让我完全无法接受. Bye, 我亲爱的GReader...","link":"/bye-google-reader/"},{"title":"CM6 test0 32b","text":"好吧,熟悉的人看到标题应该能猜到这就是Android 2.2 的CM版本.没想到这么快会出现,和之前发布的版本不同,这次编译与CM大神的版本!没想到在和儿子过生的当间就发布了... 基于7月5日最新CM6源码编译，新增了电池百分比显示的开关，在Cyanogenmod 设置里进行更改 默认关闭jit，在32a/32b等低端机上开启jit对性能没有改善，反倒更占内存，故在这个版本中关闭jit 修改ADW的默认设置，使其常驻内存，改善从其它程序退回桌面的速度 修改ADW的壁纸图库，用AOSP的图库替换了CM的图库 修改了framework.jar，使用了geesun的代码试其支持中文运营商显示 使用了最新的FRF91的GAPPS CM6自带的contact文件有不少bug，故换成了aosp的contact，虽然相对cm6的功能更少，但非常稳定 重新编译了kernel，个人感觉比默认的kernel更稳定 进一步汉化了framework和superuser等程序 新增32a的支持，32a的用户也可以使用 Known Issues： 第一次启动时可能会意外重启，完成设置后就不会出现这个问题 相机中按0x变焦按键会使相机fc 摄像无法正常使用 由于Gapps都是Nexus专用的，故其素材的尺寸都很大，特别是gmail，显示出来很大，这个暂时无法解决 Dream和Magic因为性能问题，不支持flash，即便刷了2.2也不可能运行flash，所以不要去市场下载flash程序了，不会起作用的，另外也不要再求使用flash的方法了，在地球上不存在解决方法，除非你换手机 App2sd： rom支持app2sd，但默认没有开启。 开启方法： 在超级终端中输入： su pm setInstallLocation 2 即可开启app2sd，不需要有ext分区就可以使用，但官方的这个app2sd还不太稳定，不建议使用。 Download： Dream/Magic 32b:http://thesoloblack.com/rom/cm6-test0-32b-0705-fixed.zip Magic 32a:http://thesoloblack.com/rom/cm6-test0-32a-0705-fixed.zip 本更新信息和下载链接均来源于机锋网!","link":"/cm6-test0-32b/"},{"title":"CnBloggerCon 2012","text":"一年一度的中文网志年会与今晨八点开始，不过没想到的是昨晚就已经完成了开幕式。在Google+上由Isaac Mao主持开幕Hangout视频。可惜我没有看到！ 以上的年会开放营的设计示意图本来我是想稍加修改一下再传到自己的Flickr上的！结果一直到今天开幕都没有太多时间来做这件事情，其实这篇博客都应该是昨天晚上就生产出来的才对。 今年的年会与以前的年会有所不同，没有设置会场，而启用了“云智慧”的概念。而Google+中的会议视频以及才推出没多久的Events为年会此种形式提供了可行性！ 第一天的会议主题为“公民媒体”，可以在如下两个地址内参与讨论： +《云平行会： 麦康瑞在全球之声2012峰会》 +《云访谈：老虎庙和佐拉谈公民记者》 Google+上的Events讨论地址：中文网志年会2012 Events 还请各位中文Blogger们积极参加，因为要翻墙，So,记得戴套，或自备安全工具！你们懂得。","link":"/cnbloggercon-20/"},{"title":"关于设备转向后的自适应","text":"关于移动端的适配，都知道其实rem是比较好的一个适配方案，但是rem是根据根目录的字体大小来调解的，那么，我们在做网页的时候，屏幕旋转后，能否让根目录的字体跟着变化呢？ 先上代码： 1234$(function(){ var size = $(window).width() / 25; $('html').css('font-size': size);}); 这样在css中用rem单位是没什么问题，但是如果屏幕旋转之后，你就会发现，真的不能看了就。原因就是屏幕旋转以后，根上的字体并没有随之变化。 所以我们来加上 12345678910111213// 监视设备方向window.addEventListener(&quot;orientationchange&quot;, function() { media();}, false);function media(argument) { // 因为获取尺寸出错，需要延迟获取 setTimeout(function(){ var size = $(window).width() / 25; console.log('the device size: '+size); $('html').css('font-size', size); }, 200); }","link":"/css-rem-and-javascript/"},{"title":"自定义文件上传框","text":"其实这根本就不值得写出来，只是可能前几步大家都做了，只是最后一步就忽略了。 我们在自定义input:file的时候，一般来说都是外边包一层，里边在写一个&lt;input type=\"file\"&gt;, 然后将其透明值设置成0,然后再定义外层的样式来达到自定义的目的。 HTML： &lt;div class=&quot;upfileOutWrap&quot;&gt; &lt;div class=&quot;upfileWrap&quot;&gt;&lt;input type=&quot;file&quot;&gt;&lt;/div&gt; &lt;div class=&quot;upfileBG&quot;&gt;upload image&lt;/div&gt; &lt;/div&gt; CSS： .upfileOutWrap { cursor: pointer; width: 199px; height: 42px; line-height: 42px; position: relative; } .upfileWrap{ width: 100%; height: 100%; position: absolute; top:-1; left: -1; z-index:2; } .upfileWrap input{ opacity: 0; filter: alpha(opacity=0); cursor: pointer; width: 100%; height: 100%; font-size: 32px; } .upfileBG{ width:100%; height:100%; background: url(./images/upload.png) no-repeat; font-size: 14px; color: white; position: absolute; top:-1; left: -1; padding-left:10px; z-index:1; } 可是这个时候还是有点问题，就是万恶的IE下边。 IE下边的input标签默认都是有光标的，:file也不例外，而且IE下边必须要点击”Browse”或者双击input输入框才会有效果。那么这个时候在IE下就会出现如图的莫名其妙的问题，注意左边的光标，并且还需要双击才会弹出文件选择窗口。 这个时候如果你把input透明度设置成100显示出来，就会发现原来是这样的。 所以这个时候，如果是其他标准浏览器，那么设置好input的高宽就搞定了，而IE下边，还必须考虑如何让”Browse”按钮能铺满我们所自定的div样式。这样我们才能实现IE下不出现光标，而且单击弹出文件选择窗口。 这个时候，看似毫无办法，其实我们可以选择增加字体的大小。当字体变成32px的时候，就是这个样子的。 好了，这样我们就搞定了，将input:file 继续设置为完全透明。那个可恶的光标不见了，我们也可以实现IE下单击。当然，字体到底用多大的，要视你自己定义的视觉效果来看，自己调试吧。 Final CSS: .upfileOutWrap { cursor: pointer; width: 199px; height: 42px; line-height: 42px; position: relative; } .upfileWrap{ width: 100%; height: 100%; position: absolute; top:-1; left: -1; z-index:2; } .upfileWrap input{ opacity: 0; filter: alpha(opacity=0); cursor: pointer; width: 100%; height: 100%; } .upfileBG{ width:100%; height:100%; background: url(./images/upload.png) no-repeat; font-size: 14px; color: white; position: absolute; top:-1; left: -1; padding-left:10px; z-index:1; }","link":"/custom-inputfile/"},{"title":"Debian安装SSH","text":"要求： 安装Debian 8.* 64位操作系统 分区要求: 不要分区 Partitioning mehod: use entire disk Partitioning scheme: All files in one partition 选择源镜像 (mirror country) 请选择china 然后选择ftp.cn.debian.org 程序和服务需求 debian 默认最小安装，安装的时候不用安装桌面环境和标准系统实用程序(以下两个不需要勾选): - Debian destop environment - Standard system utilities 如果有SSH server选项，请务必勾选，会省很多麻烦 安装SSH debian最小安装默认是没有配置apt-get源的，这个时候无法实用apt-get install命令，所以在安装SSH之前，我们需要先配置apt-get: 配置apt-get 终端内操作 1234567# 首先我们需要备份原有配置文件cp /etc/apt/sources.list /etc/apt/sources.listbak# 然后对资源列表文件进行编辑vi /etc/apt/sources.list# 当然也可以实用nano命令nano /etc/apt/source.list PS: 如果对VI操作不熟悉的，可以看这里 vi编辑器常见命令实用 如果安装的时候按照之前我给的步骤来，那么这会的sources.list应该是这样的 对文件进行更改，将以下命令加入文件并保存: 12deb http://ftp.cn.debian.org/debian/ jessie main contrib non-free deb-src http://ftp.cn.debian.org/debian/ jessie main contrib non-free 更改后的文件如图: main, contrib, non-free 分属不同的源，添加后可以从不同的源仓库更新文件索引 至此apt-get源就配置完毕，接下来我们就可以安装SSH了 安装SSH 在终端内输入以下命令: 12345678# 更新apt-get源apt-get update# 更新系统apt-get upgrade# 安装SSHapt-get install ssh 这样就好了，SSH安装完毕 注意 如果一直安装不能成功，请往下看: 首先，请ping ftp.cn.debin.org 和 ping mirrors.163.com 来测试一下能否ping的通域名，如果ping不通，请往下看: 有的时候机房安装debian后会出现域名解析问题,这又是另外一个问题。比如ping 123.111.123.111 是OK的，但是如果ping对应的域名如: ping mirrors.163.com就会出现unknow host的问题。 似乎linux很大一部分都会出现这种问题，能ping的通IP但是ping不通域名。那么请查看以下原因解决: 1. 查看DNS解析是否有问题，确定设置了域名服务器: cat /etc/resolv.conf 123nameserver 114.114.114.114 nameserver 8.8.8.8 nameserver 8.8.4.4 2. 确保网关已设置 grep GATEWAY /etc/sysconfig/network-scripts/ifcfg* 1/etc/sysconfig/network-scripts/ifcfg-eth0:GATEWAY=192.168.40.1 如果未设置，则通过以下方法增加网关 route add default gw 192.168.40.1 或者手工编写/etc/sysconfig/network-scripts/ifcfg* 然后重启network服务: service network restart 3. 确保可用dns解析 grep hosts /etc/nsswitch.conf 文件打开后为: 1hosts: files dns 4. 查看是否防火墙的问题 因为域名解析用到了53端口,需要把下面设置配置到防火墙里: 1234iptables -A INPUT -p udp --sport 53 -j ACCEPT iptables -A OUTPUT -p udp --dport 53 -j ACCEPT iptables -A INPUT -p udp --dport 53 -j ACCEPT iptables -A OUTPUT -p udp --sport 53 -j ACCEPT 如果找不到原因或者不知道怎么设置，那么就用以下最笨的方法: 如果出现这样的问题，更新sources.list后会无法更新也无法安装ssh. 如果出现这样的问题，更新sources.list地址为一下地址: 12345678deb http://123.58.173.186/debian/ jessie main non-free contribdeb http://123.58.173.186/debian/ jessie-updates main non-free contribdeb http://123.58.173.186/debian/ jessie-backports main non-free contribdeb-src http://123.58.173.186/debian/ jessie main non-free contribdeb-src http://123.58.173.186/debian/ jessie-updates main non-free contribdeb-src http://123.58.173.186/debian/ jessie-backports main non-free contribdeb http://123.58.173.186/debian-security/ jessie/updates main non-free contribdeb-src http://123.58.173.186/debian-security/ jessie/updates main non-free contrib 利用IP地址代替域名，但是测试下来只有163的镜像可以这样做。来源为网易镜像的帮助文档: Debian镜像使用帮助","link":"/debian-install-ssh/"},{"title":"Digits recognition","text":"The code address of this article is: digit recognition Convolution operation demo 123456789101112131415import pylabimport numpy as npfrom scipy import signal# set imgimg = np.array([[10, 10, 10, 10, 10],[10, 5, 5, 5, 10], [10, 5, 5, 5, 10], [10, 5, 5, 5, 10], [10, 10, 10, 10, 10]])# set convolutionfil = np.array([[-1, -1, 0], [-1, 0, 1], [0, 1, 1]])# convolution the imgres = signal.convolve2d(img, fil, mode='valid')# output the resultprint(res) output 123[[ 15 10 0] [ 10 0 -10] [ 0 -10 -15]] A image demo 1234567891011121314151617181920212223import matplotlib.pyplot as pltimport pylabimport cv2import numpy as npfrom scipy import signal# read the imageimg = cv2.imread('./data/weixin.jpg', 0) # Any picture# show the imageplt.imshow(img, cmap='gray')pylab.show()# set the convolutionfil = np.array([[-1,-1,0], [-1, 0, 1], [0, 1, 1]])# convolution operationres = signal.convolve2d(img, fil, mode='valid')print(res)# show convolution imageplt.imshow(res, cmap = 'gray')pylab.show() use LeNet model to recognize Mnist handwritten digits 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import kerasfrom keras.datasets import mnistfrom keras.layers import Conv2D, MaxPooling2Dfrom keras.layers import Dense, Flattenfrom keras.models import Sequentialimport warningswarnings.filterwarnings('ignore')# load data(train_x, train_y), (test_x, test_y) = mnist.load_data()train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)train_x = train_x / 255test_x = test_x / 255train_y = keras.utils.to_categorical(train_y, 10)test_y = keras.utils.to_categorical(test_y, 10)# create sequential modelsmodel = Sequential()# The first convolutional layer: 6 convolution kernels, the size is 5*5, relu activation functionmodel.add(Conv2D(6, kernel_size = (5,5), activation='relu', input_shape=(28, 28, 1)))# the second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size = (2, 2)))# the third convolutional layer: 16 convolution kernels, the size is 5*5, relu activation functionmodel.add(Conv2D(16, kernel_size = (5, 5), activation = 'relu'))# the second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size = (2, 2)))# Flatten the parameters, which is called a convolutional layer in leNet5. in fact, this layer is a one-dimensional vector, the same as the fully connected layermodel.add(Flatten())model.add(Dense(120, activation = 'relu'))# Fully connected layer, the number of output nodes is 84model.add(Dense(84, activation = 'relu'))# The output layer uses the softmax activation function to calculate the classification probabilitymodel.add(Dense(10, activation='softmax'))# set the loss function and optimizer configurationmodel.compile(loss = keras.metrics.categorical_crossentropy, optimizer = keras.optimizers.Adam(), metrics = ['accuracy'])# Incoming training data for trainingmodel.fit(train_x, train_y, batch_size = 128, epochs = 2, verbose = 1, validation_data = (test_x, test_y))# Evaluate the resultsscore = model.evaluate(test_x, test_y)print('Error: %.4lf' % score[0])print('Accuracy: ', score[1]) 12345678Train on 60000 samples, validate on 10000 samplesEpoch 1/260000/60000 [==============================] - 37s 616us/step - loss: 0.3091 - accuracy: 0.9102 - val_loss: 0.1010 - val_accuracy: 0.9696Epoch 2/260000/60000 [==============================] - 36s 595us/step - loss: 0.0876 - accuracy: 0.9731 - val_loss: 0.0572 - val_accuracy: 0.981410000/10000 [==============================] - 3s 328us/stepError: 0.0572Accuracy: 0.9814000129699707","link":"/digits_recognition/"},{"title":"Dropbox","text":"这真是一个好服务呢…相对skydrive来说虽然少了3G的空间,但是对于保密和可见的设定很丰富,更重要的是,dropbox的客户端支持mac和windows用户直接拖拽传…像建立文件夹以及上传下载文件,简直就和在自己的电脑里操作一样简单… 唯一不足的由于网络原因,我的所有操作都会有延迟…得等他慢慢去同步文件的操作.看演示视频,外国的网络真是强大啊! 在此感谢lenvow的邀请信. 对应的,我也有了十个邀请的机会,有需要的可以在地下留言申请…. 2008.7.2更新:我发现我是比较懒的…有人对Dropbox的测试给出了更详细的说明…看这里: 而自然,我的十个邀请还一个都没有送出去呢!:)可见我这里确实没有人什么人来往！ 如果需要的,还是趁早吧！","link":"/dropbox/"},{"title":"使用Plotnine制作元素周期表","text":"首先需要了解元素周期表以及元素数据: 维基百科的元素周期表词条 元素数据 元素周期表基本构成如下: 族：表中的每一列就是一族，从左向右依次为 1、2……18 族。 周期：表中的行。 元素：每个方框表示一个元素，其中包括元素符号、名称、原子序数、原子量。 在主表下面还有镧系元素和锕系元素表。 用颜色区分金属、非金属等常见的物质状态。 最终呈现: 其他形状元素周期表 导入和处理数据 1234567# 导入依赖import pandas as pdimport numpy as npfrom plotnine import *# 读取数据elements = pd.read_csv('~/data/cbcpv/elemanets/elements.csv') 研究数据集 1234567891011121314151617181920212223242526272829303132elements.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 118 entries, 0 to 117Data columns (total 21 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 atomic number 118 non-null int64 1 symbol 118 non-null object 2 name 118 non-null object 3 atomic mass 118 non-null object 4 CPK 118 non-null object 5 electronic configuration 118 non-null object 6 electronegativity 97 non-null float64 7 atomic radius 71 non-null float64 8 ion radius 92 non-null object 9 van der Waals radius 38 non-null float64 10 IE-1 102 non-null float64 11 EA 85 non-null float64 12 standard state 99 non-null object 13 bonding type 98 non-null object 14 melting point 101 non-null float64 15 boiling point 94 non-null float64 16 density 96 non-null float64 17 metal 118 non-null object 18 year discovered 118 non-null object 19 group 118 non-null object 20 period 118 non-null int64 dtypes: float64(8), int64(2), object(11)memory usage: 19.5+ KB&quot;&quot;&quot; 特征group就是该元素所在的族，但是，如果用elements['group'] 查看所有内容，会发现有的记录中用 '-' 标记，说明它不属于任何族，说明它们应该是镧系元素或者锕系元素。根据数据分析的通常要求，'-' 符号最好用数字表示，这里用 ﹣1 转化数据集 123456789101112131415161718# 转换族elements['group'] = [-1 if g=='-' else int(g) for g in elements['group']]elements['group']&quot;&quot;&quot;0 11 182 13 24 13 ..113 14114 15115 16116 17117 18Name: group, Length: 118, dtype: int64&quot;&quot;&quot; 特征 bonding type、metal 都是分类数据，因此在类型上进行转化。 123# 转化分类数据elements['bonding type'] = elements['bonding type'].astype('category')elements['metal'] = elements['metal'].astype('category') 将原本的整数型atomic number特征,转化为字符串类型 1elements['atomic_number'] = elements['atomic number'].astype(str) 元素周期表有两个部分,上面一部分每个元素是属于某一个族的,即group特征中的1-18, 而对于值是-1的则表示这些元素应该在下面的镧系或者锕系元素表中。下面分别用 top 变量和 bottom 变量引用这两部分元素集合. 123## 分别用top和bottom变量引用上下部分元素集合top = elements.query('group != -1').copy()bottom = elements.query('group == -1').copy() 元素周期表中横向表示的是族（group），纵向表示的是周期（period），用下面的方式在 top 中创建两个特征，分别为“族”和“周期”的值。 1234567891011121314151617181920212223242526272829303132333435363738## 在top中区分“族”(group)和“周期”(period)的值&quot;&quot;&quot;横向表示族,纵向表示周期&quot;&quot;&quot;top['x'] = top.grouptop['y'] = top.periodtop['x']&quot;&quot;&quot;0 11 182 13 24 13 ..113 14114 15115 16116 17117 18Name: x, Length: 90, dtype: int64&quot;&quot;&quot;top['y']&quot;&quot;&quot;0 11 12 23 24 2 ..113 7114 7115 7116 7117 7Name: y, Length: 90, dtype: int64&quot;&quot;&quot; 除了上面的部分之外，下面的锕系和镧系元素也要做类似的配置。不过，横坐标不能用 group 特征的值，因为前面设置为 ﹣1。 12345678nrows = 2&quot;&quot;&quot;hshift 和 vshift 分别表示横、纵间距，这样就为每个锕系和镧系元素增加了横纵坐标值。&quot;&quot;&quot;hshift = 3.5vshift = 3bottom['x'] = np.tile(np.arange(len(bottom) // nrows), nrows) + hshiftbottom['y'] = bottom.period + vshift 每个元素都占了一个小方块,所以,这个小方块(元素块)的大小要设置一下 123## 设置元素占据的小矩形tile_width = 0.95tile_height = 0.95 开始画图 1234(ggplot(aes('x', 'y')) + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height))) 这里只有美学映射,没有传入数据集.因为在图层对象中,要传入不同的数据集: “top”和“bottom”. top表示主表中的, bottom表示下面的锕、镧系元素 geom_tile绘制安放元素块图层,并使用top数据集,在引入一个图层,绘制bottom对应的图层. 但是我们发现表反了, 所以需要实现在Y轴方向上的坐标轴翻转. 123456(ggplot(aes('x', 'y')) +geom_tile(top, aes(width=tile_width, height=tile_height)) +geom_tile(bottom, aes(width=tile_width, height=tile_height)) # 在Y轴上进行翻转 +scale_y_reverse() # new) 基本样式已经有了。 前面已经把特征“metal”的数据转换为分类数据，下面用这些数据对不同元素的小矩形（以后简称“元素块”）上色。 1234567(ggplot(aes('x', 'y')) # 对数据不同的元素块进行上色 + aes(fill='metal') # new + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + scale_y_reverse() ) 然后,我们要将化学元素的有关信息写到这些元素块上,这里要写到元素块上的包括: 原子序数，对应着数据集中的特征是“atomic number”； 元素符号，对应着数据集中的特征是“symbol”； 元素名称，对应着数据集中的特征是“name”； 原子量，对应着数据集中的特征是“automic mass” 在这里,我们要绘制四个图层,以便安放四个元素信息, 每个图层上面一个特征,并且每个图层的位置、字号大小等都不相同. 为此我们写一个函数方法来实现: 1234567891011121314151617181920&quot;&quot;&quot;nudge_x: 文本在水平方向上的相对位置nudge_y: 文本在竖直方向上的相对位置ha: 可选'left', 'center', 'right', 标示水平方向的对齐方式va: 可选'top', 'center', 'bottom', 表示竖直方向的堆砌方式size: 字号大小fontweight: 字族中的字体粗细&quot;&quot;&quot;def inner_text(data): layers = [geom_text(data, aes(label='atomic_number'), nudge_x=-0.40, nudge_y=-.40, ha='left', va='top', fontweight='normal', size=6), geom_text(data, aes(label='symbol'), nudge_y=.1, size=9), geom_text(data, aes(label='name'), nudge_y=-0.125, fontweight='normal', size=4.5), geom_text(data, aes(label='atomic mass'), nudge_y=-.3, fontweight='normal', size=4.5) ] return layers 然后我们将函数inner_text应用到绘图流程中去 12345678910111213&quot;&quot;&quot;分别调用两次是因为有top和bottom两个数据&quot;&quot;&quot;(ggplot(aes('x', 'y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) # 绘制上部分图层 + inner_text(top) # new # 绘制下部分图层 + inner_text(bottom) # new + scale_y_reverse()) 是不是觉得图很难看,原因在于我们还没对其进行调整,下面我们就要细微的调整图层,包括大小等 12345678910111213(ggplot(aes('x', 'y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) + scale_y_reverse() # coord_equal作用是设置坐标系的横轴和纵轴 # expand=False, 意味着坐标系的大小由制图所用数据决定 + coord_equal(expand=False) # new # 一个新主题,规定了图纸的尺寸 + theme(figure_size=(12,6)) # new) 在默认的主题中，横纵坐标的图上长度相等，也就是图像是呈现在一张正方形的图纸上，coord_equal 的作用就是设置坐标系的横轴和纵轴，它与 coord_fixed 是完全等效的，能够改变图纸的大小和长宽比例。参数 expand 的值是布尔值，如果为 False，则意味着坐标系的大小（即图纸的大小）由制图所用数据决定。 新增的第二个图层对象是一个新的主题，在其中规定了图纸的尺寸。 我们仔细研究元素周期表,发现Lu和Lr两个元素比较特殊,其实它们不是单独的元素,而是对应着下部分两行的,因此要对这两个进行处理,以区分出与其他元素的不同. 我们将其分为两半,使用过PS作图的同学应该能想到两个不同颜色的图层叠加,上面的图层只有下面图层的一半,那么看起来就像是被分成了两半. 123456# split_df 是绘制新元素块所需要的数据集。split_df = pd.DataFrame({ 'x': 3-tile_width/4, 'y': [6,7], 'metal': pd.Categorical(['lanthanoid', 'actinoid'])}) 123456789101112(ggplot(aes('x','y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) # 将新的数据集用于叠加Lu和Lr的图层上进行遮挡 + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) # new + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) + scale_y_reverse() + coord_equal(expand=False) + theme(figure_size=(12, 6))) 基本制作完成了,下面来美化一下: 123456789101112131415161718(ggplot(aes('x', 'y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) + scale_y_reverse() # 对元素块填充色进行转换 + scale_fill_brewer(type='qual', palette=3) + coord_equal(expand=False) # 增加了一个经典的主题图层对象 + theme_void() + theme(figure_size=(12, 6), # 增加一个主题图层,并设置了该图层的尺寸和背景色 plot_background=element_rect(fill='white') )) 到最后了,我们要解决主表中的元素表上族和周期的问题 观察主表中的每一列,注意我们已经把Y轴映射反序了,如果在H元素的元素块上标注族的序号为“1”, 那么这个“1”的Y轴坐标应该是y=1, 同样,Sc元素块上标注族的需要“3”, 那么“3”的Y轴坐标应该是y=4. 这样,我们就可以创建每列及其对应的Y轴坐标了. 1234567## 创建每列(即:族, 编号为1-18)及其对应的Y轴坐标groupdf = pd.DataFrame({ 'group': range(1, 19), 'y': np.repeat([1,2,4,2,1], [1,1,10,5,1])})groupdf group y 0 1 1 1 2 2 2 3 4 3 4 4 4 5 4 5 6 4 6 7 4 7 8 4 8 9 4 9 10 4 10 11 4 11 12 4 12 13 2 13 14 2 14 15 2 15 16 2 16 17 2 17 18 1 让我们来标注族的序号 123456789101112131415161718192021222324252627## 标注族序号(ggplot(aes('x','y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) # 标注每一列族序号的文本图层 # aes('group', 'y', label='group') 重写了X轴和Y轴的映射 # inherit_aes=False, 不继承映射配置 + geom_text(groupdf, aes('group', 'y', label='group'), color='gray', nudge_y=.525, va='bottom', fontweight='normal', size=9, inherit_aes=False ) # 以Y轴调转坐标轴 + scale_y_reverse() # 对元素块填充色进行转换 + scale_fill_brewer(type='qual', palette=3) + coord_equal(expand=False) # 增加了一个经典的主题图层对象 + theme_void() + theme(figure_size=(12, 6), # 增加一个主题图层,并设置了该图层的尺寸和背景色 plot_background=element_rect(fill='white'), )) 最终,我们标注玩周期就完成了. 周期是对每一行的标注,一共7行,因为标注在左侧,可以把它看成是左侧的Y轴标示,可以在图层上通过对Y轴标示的设置完成周期的标注. 1234567891011121314151617181920212223242526272829303132333435363738## 开始标注周期, 最终完成(ggplot(aes('x', 'y')) # 把特征'metal'的数据转换为分类数据,进行元素块上色 + aes(fill='metal') # 创建上部元素块 + geom_tile(top, aes(width=tile_width, height=tile_height)) # 创建Lu和Lr的半个元素块 + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) # 创建下部元素块 + geom_tile(bottom, aes(width=tile_width, height=tile_height)) # 创建文字图层, 把化学元素的有关信息写到元素块中 + inner_text(top) + inner_text(bottom) # 标注每一列族序号的文本图层 # aes('group', 'y', label='group') 重写了X轴和Y轴的映射 # inherit_aes=False, 不继承映射配置 + geom_text(groupdf, aes('group', 'y', label='group'), color='gray', nudge_y=.525, va='bottom', fontweight='normal', size=9, inherit_aes=False ) # 以Y轴调转坐标轴, 增加了纵坐标主刻度标示数字。 + scale_y_reverse(breaks=range(1, 8), limits=(0, 10.5) ) # 对元素块填充色进行转换 + scale_fill_brewer(type='qual', palette=3) + coord_equal(expand=False) # 增加了一个经典的主题图层对象 + theme_void() + theme(figure_size=(12, 6), # 增加一个主题图层,并设置了该图层的尺寸和背景色 plot_background=element_rect(fill='white'), # 增加了参数 axis_text_y，对 Y 轴标示的显示格式进行了设置。 axis_text_y=element_text(margin={'r':5}, color='gray', size=9) )) 完成...","link":"/elements_by_plotnine/"},{"title":"enlight VS snapseed2","text":"本文知乎专栏地址 在Enlight出来之前，Snapseed曾经是我最主要的修图工具。 当然，Enlight确实是大而全的一款App，但是使用中，却让我有了无法忍受的一点。那就是丢失部分图片信息。 导出的时候，Enlight导出所用的格式是PNG，而Snapseed仍然是jpg导出。自然大家都知道PNG图片所用的格式是无损的，这就是说，Snapseed导出的图像相比Enlight要小很多，因为iPhone默认拍照所保存的都是jpg格式，所以即便再导出PNG格式，那所增加的部分应该是软件通过算法添加进去的。 而，Enlight在导出的PNG格式图片里，却丢失了相机信息，地理位置信息和更重要的拍照时间信息。照片的时间被导出的时间所代替。 同样一张照片，我们先使用Enlight进行修图 然后使用Snapseed 2 修图 以下是使用Metapho查看的照片信息。 1. 首先我们来看Enlight导出的照片信息: 可以看到Camera和Location信息都没有了，照片被导出成PNG格式，而且照片大了很多。 2. 我们再来看看Snapseed导出的照片信息 可以看到Camera和Location信息都保存完好。 这也是Enlight最让我无法接受的一点，其实Enlight在设置里是可以选择增加地理信息的，也就是会调用GPS模块，但是当导出的时候这些信息就全都没有了，即便是用Enlight来拍照。不知道下一版本的Enlight是否能够改善这点。如果对这些信息无所谓的同学，Enlight倒是现今为止最强大的修图工具，Snapseed虽然更新到版本2了，但是似乎依然没有加入曲线。Snapseed2的具体更新，可以点击原文链接查看「领客」内的文。 ———","link":"/enlight-VS-snapseed2/"},{"title":"A Preliminary Study of Machine Learning","text":"Gradient 123456789101112131415def loss(k): return 3 * (k ** 2) + 7 * k -10# -b / 2a = -7 / 6def partial(k): return 6 * k + 7k = ramdom.randint(-10, 10)alpha = 1e-3 # 0.001for i in range(1000): k = k + (-1) * partial(k) * alpha print(k, loss(k))","link":"/example_002/"},{"title":"Introduction to Artificial Intelligence","text":"The code address of this article is: Example 01 The source code is in ipynb format, and the output content can be viewed. rule based 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import random from icecream import ic#rules = &quot;&quot;&quot;#复合句子 = 句子 , 连词 句子#连词 = 而且 | 但是 | 不过#句子 = 主语 谓语 宾语#主语 = 你| 我 | 他 #谓语 = 吃| 玩 #宾语 = 桃子| 皮球# #&quot;&quot;&quot;rules = &quot;&quot;&quot;复合句子 = 句子 , 连词 复合句子 | 句子连词 = 而且 | 但是 | 不过句子 = 主语 谓语 宾语主语 = 你| 我 | 他 谓语 = 吃| 玩 宾语 = 桃子| 皮球 &quot;&quot;&quot;def get_grammer_by_description(description): rules_pattern = [r.split('=') for r in description.split('\\n') if r.strip()] target_with_expend = [(t, ex.split('|')) for t, ex in rules_pattern] grammer = {t.strip(): [e.strip() for e in ex] for t, ex in target_with_expend} return grammer#generated = [t for t in random.choice(grammer['句子']).split()]#test_v = [t for t in random.choice(grammer['谓语']).split()]def generate_by_grammer(grammer, target='句子'): if target not in grammer: return target return ''.join([generate_by_grammer(grammer, t) for t in random.choice(grammer[target]).split()])if __name__ == '__main__': grammer = get_grammer_by_description(rules) #ic(generated) #ic(test_v) #ic(generate_by_grammer(grammer)) ic(generate_by_grammer(grammer, target='复合句子')) water pouring 12345678910111213141516171819202122232425262728293031323334353637383940def water_pouring(b1, b2, goal, start=(0, 0)): if goal in start: return [start] explored = set() froniter = [[('init', start)]] while froniter: path = froniter.pop(0) (x, y) = path[-1][-1] for (state, action) in successors(x, y, b1, b2).items(): if state not in explored: explored.add(state) path2 = path + [(action, state)] if goal in state: return path2 else: froniter.append(path2) return []def successors(x, y, X, Y): return { ((0, y+x) if x + y &lt;= Y else (x + y - Y, Y)): 'X -&gt; Y', ((x + y, 0) if x + y &lt;= X else (X, x + y - X)): 'X &lt;- Y', (X, y): '灌满X', (x, Y): '灌满Y', (0, y): '倒空X', (x, 0): '倒空Y', }if __name__ == '__main__': print(water_pouring(4, 9, 5)) print(water_pouring(4, 9, 5, start=(4, 0))) print(water_pouring(4, 9, 6))","link":"/example_01/"},{"title":"Initial exploration of machine learning","text":"The code address of this article is: Example 02 The source code is in ipynb format, and the output content can be viewed. ## Gradient 123456789101112131415161718192021222324import randomdef loss(k): return 3 * (k ** 2) + 7 * k - 10 # -b / 2a = -7 / 6def partial(k): return 6 * k + 7k = random.randint(-10, 10)alpha = 1e-3 # 0.001for i in range(1000): k = k + (-1) * partial(k) *alpha print(k, loss(k)) # out&quot;&quot;&quot;7.959 124.32404299999999-7.918246 122.66813714954799show more (open the raw output data in a text editor) ...-1.1833014444482555 -14.082503185837805&quot;&quot;&quot; Cutting Problem All the dynamic programming: sub-problems Overlapping sub-problems parse solution 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from collections import defaultdictfrom functools import lru_cache# least recent usedprices = [1, 5, 8, 9, 10, 17, 17, 20, 24, 30, 33]complete_price = defaultdict(int)for i, p in enumerate(prices): complete_price[i+1] = p solution = {}cache = {}#&lt;- if when n .... is huge. size(cache)# keep most important information.@lru_cache(maxsize=2**10)def r(n): # a very classical dynamic programming problem # if n in cache: return cache[n] candidates = [(complete_price[n], (n, 0))] + \\ [(r(i) + r(n-i), (i, n - i)) for i in range(1, n)] optimal_price, split = max(candidates) solution[n] = split # cache[n] = optimal_price return optimal_pricedef parse_solution(n, cut_solution): left, right = cut_solution[n] if left == 0 or right == 0: return [left+right, ] else: return parse_solution(left, cut_solution) + parse_solution(right, cut_solution)if __name__ == '__main__': print(r(19)) print(parse_solution(19, solution)) # out&quot;&quot;&quot;55[11, 6, 2]&quot;&quot;&quot; Dynamic 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748from collections import defaultdictfrom functools import wrapsfrom icecream import icoriginal_price = [1,5,8,9,10,17,17,20,24,30,33]price = defaultdict(int)for i, p in enumerate(original_price): price[i+1] = p def memo(func): cache = {} @wraps(func) def _wrap(n): if n in cache: result = cache[n] else: result = func(n) cache[n] = result return result return _wrap @memodef r(n): max_price, split_point = max( [(price[n],0)] + [(r(i) + r(n-i), i) for i in range(1, n)], key=lambda x: x[0] ) solution[n] = (split_point, n-split_point) return max_price def not_cut(split): return split == 0def parse_solution(target_length, revenue_solution): left, right = revenue_solution[target_length] if not_cut(left): return [right] return parse_solution(left, revenue_solution) + parse_solution(right, revenue_solution) solution = {}r(50)ic(parse_solution(20,solution))ic(parse_solution(19,solution))ic(parse_solution(27,solution))# out&quot;&quot;&quot;ic| parse_solution(20,solution): [10, 10]ic| parse_solution(19,solution): [2, 6, 11]ic| parse_solution(27,solution): [6, 10, 11][6, 10, 11]&quot;&quot;&quot; Gradient descent 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npimport matplotlib.pyplot as pltimport randomfrom icecream import icdef func(x): return 10 * x**2 + 32*x + 9def gradient(x): return 20 *x + 32 x = np.linspace(-10, 10)steps = []x_star = random.choice(x)alpha = 1e-3for i in range(100): x_star = x_star + -1*gradient(x_star)*alpha steps.append(x_star) ic(x_star, func(x_star))fig, ax = plt.subplots()ax.plot(x, func(x))&quot;&quot;&quot;ic| x_star: 9.368, func(x_star): 1186.3702400000002ic| x_star: 9.14864, func(x_star): 1138.732618496show more (open the raw output data in a text editor) ...ic| x_star: -0.1157435825983131, func(x_star): 5.430171125980905[&lt;matplotlib.lines.Line2D at 0x7fd6d19545d0&gt;]&quot;&quot;&quot;for i, s in enumerate(steps): ax.annotate(str(i+1), (s, func(s))) plt.show() k-means-finding-centers K-means 123456789101112131415161718192021222324252627282930313233343536373839404142from pylab import mplmpl.rcParams['font.sans-serif'] = ['FangSong'] # Specify the default fontmpl.rcParams['axes.unicode_minus'] = False # Solve the problem that the minus sign'-' is displayed as a square in the saved imagecoordination_source = &quot;&quot;&quot;{name:'兰州', geoCoord:[103.73, 36.03]},{name:'嘉峪关', geoCoord:[98.17, 39.47]},{name:'西宁', geoCoord:[101.74, 36.56]},{name:'成都', geoCoord:[104.06, 30.67]},{name:'石家庄', geoCoord:[114.48, 38.03]},{name:'拉萨', geoCoord:[102.73, 25.04]},{name:'贵阳', geoCoord:[106.71, 26.57]},{name:'武汉', geoCoord:[114.31, 30.52]},{name:'郑州', geoCoord:[113.65, 34.76]},{name:'济南', geoCoord:[117, 36.65]},{name:'南京', geoCoord:[118.78, 32.04]},{name:'合肥', geoCoord:[117.27, 31.86]},{name:'杭州', geoCoord:[120.19, 30.26]},{name:'南昌', geoCoord:[115.89, 28.68]},{name:'福州', geoCoord:[119.3, 26.08]},{name:'广州', geoCoord:[113.23, 23.16]},{name:'长沙', geoCoord:[113, 28.21]},//{name:'海口', geoCoord:[110.35, 20.02]},{name:'沈阳', geoCoord:[123.38, 41.8]},{name:'长春', geoCoord:[125.35, 43.88]},{name:'哈尔滨', geoCoord:[126.63, 45.75]},{name:'太原', geoCoord:[112.53, 37.87]},{name:'西安', geoCoord:[108.95, 34.27]},//{name:'台湾', geoCoord:[121.30, 25.03]},{name:'北京', geoCoord:[116.46, 39.92]},{name:'上海', geoCoord:[121.48, 31.22]},{name:'重庆', geoCoord:[106.54, 29.59]},{name:'天津', geoCoord:[117.2, 39.13]},{name:'呼和浩特', geoCoord:[111.65, 40.82]},{name:'南宁', geoCoord:[108.33, 22.84]},//{name:'西藏', geoCoord:[91.11, 29.97]},{name:'银川', geoCoord:[106.27, 38.47]},{name:'乌鲁木齐', geoCoord:[87.68, 43.77]},{name:'香港', geoCoord:[114.17, 22.28]},{name:'澳门', geoCoord:[113.54, 22.19]}&quot;&quot;&quot; Feacutre Extractor 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768city_location = { '香港': (114.17, 22.28)}test_string = &quot;{name:'兰州', geoCoord:[103.73, 36.03]},&quot;import repattern = re.compile(r&quot;name:'(\\w+)',\\s+geoCoord:\\[(\\d+.\\d+),\\s(\\d+.\\d+)\\]&quot;)for line in coordination_source.split('\\n'): city_info = pattern.findall(line) if not city_info: continue # following: we find the city info city, long, lat = city_info[0] long, lat = float(long), float(lat) city_location[city] = (long, lat)city_location# output&quot;&quot;&quot;{'香港': (114.17, 22.28), '兰州': (103.73, 36.03),show more (open the raw output data in a text editor) ... '澳门': (113.54, 22.19)}&quot;&quot;&quot;import mathdef geo_distance(origin, destination): &quot;&quot;&quot; Calculate the Haversine distance. Parameters ---------- origin : tuple of float (lat, long) destination : tuple of float (lat, long) Returns ------- distance_in_km : float Examples -------- &gt;&gt;&gt; origin = (48.1372, 11.5756) # Munich &gt;&gt;&gt; destination = (52.5186, 13.4083) # Berlin &gt;&gt;&gt; round(distance(origin, destination), 1) 504.2 &quot;&quot;&quot; lon1, lat1 = origin lon2, lat2 = destination radius = 6371 # km dlat = math.radians(lat2 - lat1) dlon = math.radians(lon2 - lon1) a = (math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon / 2) * math.sin(dlon / 2)) c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)) d = radius * c return d Vector Distances 余弦距离 Cosine Distance 欧几里得距离 Euclidean Distance 曼哈顿距离 Manhattan distance or Manhattan length 12345678910111213import matplotlib.pyplot as pltimport networkx as nximport warningswarnings.filterwarnings('ignore')%matplotlib inline# set plt, show chineseplt.rcParams['font.sans-serif'] = ['Arial Unicode MS']plt.rcParams['axes.unicode_minus'] = Falsecity_graph = nx.Graph()city_graph.add_nodes_from(list(city_location.keys()))nx.draw(city_graph, city_location, with_labels=True, node_size=30) K-means: Initial k random centers 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394k = 10import randomall_x = []all_y = []for _, location in city_location.items(): x, y = location all_x.append(x) all_y.append(y)def get_random_center(all_x, all_y): r_x = random.uniform(min(all_x), max(all_x)) r_y = random.uniform(min(all_y), max(all_y)) return r_x, r_yget_random_center(all_x, all_y)# output&quot;&quot;&quot;(93.61182991130997, 37.01816228131414)&quot;&quot;&quot;K = 5centers = {'{}'.format(i+1): get_random_center(all_x, all_y) for i in range(K)}from collections import defaultdictcloset_points = defaultdict(list)for x, y, in zip(all_x, all_y): closet_c, closet_dis = min([(k, geo_distance((x, y), centers[k])) for k in centers], key=lambda t: t[1]) closet_points[closet_c].append([x, y])import numpy as npdef iterate_once(centers, closet_points, threshold=5): have_changed = False for c in closet_points: former_center = centers[c] neighbors = closet_points[c] neighbors_center = np.mean(neighbors, axis=0) if geo_distance(neighbors_center, former_center) &gt; threshold: centers[c] = neighbors_center have_changed = True else: pass ## keep former center return centers, have_changeddef kmeans(Xs, k, threshold=5): all_x = Xs[:, 0] all_y = Xs[:, 1] K = k centers = {'{}'.format(i+1): get_random_center(all_x, all_y) for i in range(K)} changed = True while changed: closet_points = defaultdict(list) for x, y, in zip(all_x, all_y): closet_c, closet_dis = min([(k, geo_distance((x, y), centers[k])) for k in centers], key=lambda t: t[1]) closet_points[closet_c].append([x, y]) centers, changed = iterate_once(centers, closet_points, threshold) print('iteration') return centerskmeans(np.array(list(city_location.values())), k=5, threshold=5)# output&quot;&quot;&quot;iterationiterationiterationiterationiteration{'1': array([99.518, 38.86 ]), '2': array([117.833, 39.861]), '3': array([91.11, 29.97]), '4': array([106.81, 27. ]), '5': array([116.87166667, 27.6275 ])}&quot;&quot;&quot;plt.scatter(all_x, all_y)plt.scatter(*zip(*centers.values())) 12for c, points in closet_points.items(): plt.scatter(*zip(*points)) 1234567891011121314151617181920212223city_location_with_station = { '能源站-{}'.format(i): position for i, position in centers.items()}city_location_with_station# output&quot;&quot;&quot;{'能源站-1': (108.82946246581274, 26.05763939719317), '能源站-2': (97.96769355736322, 22.166113183141032), '能源站-3': (114.05390380408154, 38.7698708467224), '能源站-4': (118.49242085311417, 28.665716162786204), '能源站-5': (125.08287617496866, 25.55784683330647)}&quot;&quot;&quot;def draw_cities(citise, color=None): city_graph = nx.Graph() city_graph.add_nodes_from(list(citise.keys())) nx.draw(city_graph, citise, node_color=color, with_labels=True, node_size=30)%matplotlib inlineplt.figure(1,figsize=(12,12)) draw_cities(city_location_with_station, color='green')draw_cities(city_location, color='red') About the dataset This contains data of news headlines published over a period of 15 years. From the reputable Australian news source ABC (Australian Broadcasting Corp.) Site: http://www.abc.net.au/ Prepared by Rohit Kulkarni 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as np import pandas as pd import matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.feature_extraction import textfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.cluster import KMeansfrom nltk.tokenize import RegexpTokenizerfrom nltk.stem.snowball import SnowballStemmerimport warningswarnings.filterwarnings('ignore')%matplotlib inlinedata = pd.read_csv(&quot;./data/abcnews-date-text.csv&quot;,error_bad_lines=False,usecols =[&quot;headline_text&quot;])data.head()# output&quot;&quot;&quot;headline_text0 aba decides against community broadcasting lic...1 act fire witnesses must be aware of defamation2 a g calls for infrastructure protection summit3 air nz staff in aust strike for pay rise4 air nz strike to affect australian travellers&quot;&quot;&quot;data.to_csv('abcnews.csv', index=False, encoding='utf8')data.info()# output&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 1103665 entries, 0 to 1103664Data columns (total 1 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 headline_text 1103665 non-null objectdtypes: object(1)memory usage: 8.4+ MB&quot;&quot;&quot; Deleting dupliate headlines(if any) 12data[data['headline_text'].duplicated(keep=False)].sort_values('headline_text').head(8)data = data.drop_duplicates('headline_text') NLP Preparing data for vectorizaion However, when doing natural language processing, words must be converted into vectors that machine learning algorithms can make use of. If your goal is to do machine learning on text data, like movie reviews or tweets or anything else, you need to convert the text data into numbers. This process is sometimes referred to as “embedding” or “vectorization”. In terms of vectorization, it is important to remember that it isn’t merely turning a single word into a single number. While words can be transformed into numbers, an entire document can be translated into a vector. Not only can a vector have more than one dimension, but with text data vectors are usually high-dimensional. This is because each dimension of your feature data will correspond to a word, and the language in the documents you are examining will have thousands of words. TF-IDF In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf. Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification. One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model. 123456789101112131415punc = ['.', ',', '&quot;', &quot;'&quot;, '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',&quot;%&quot;]stop_words = text.ENGLISH_STOP_WORDS.union(punc)desc = data['headline_text'].valuesvectorizer = TfidfVectorizer(stop_words = stop_words)X = vectorizer.fit_transform(desc)word_features = vectorizer.get_feature_names()print(len(word_features))print(word_features[5000:5100])# output&quot;&quot;&quot;96397['abyss', 'ac', 'aca', 'acacia', 'acacias', 'acadamy', 'academia', 'academic', 'academics', 'academies', 'academy', 'academys', 'acai', 'acapulco', 'acars', 'acason', 'acasuso', 'acb', 'acbf', 'acc', 'acca', 'accan', 'accc', 'acccc', 'acccs', 'acccused', 'acce', 'accedes', 'accelerant', 'accelerants', 'accelerate', 'accelerated', 'accelerates', 'accelerating', 'acceleration', 'accelerator', 'accen', 'accent', 'accents', 'accentuate', 'accentuates', 'accentuating', 'accenture', 'accept', 'acceptability', 'acceptable', 'acceptably', 'acceptance', 'acceptances', 'accepted', 'accepting', 'acceptor', 'acceptors', 'accepts', 'accerate', 'acces', 'access', 'accessary', 'accessed', 'accesses', 'accessibility', 'accessible', 'accessing', 'accessories', 'accessory', 'accesss', 'acci', 'accid', 'accide', 'acciden', 'accidenatlly', 'accidenbt', 'accident', 'accidental', 'accidentally', 'accidently', 'accidents', 'acciona', 'accis', 'acclaim', 'acclaimed', 'acclamation', 'acclimatise', 'acco', 'accolade', 'accolades', 'accom', 'accomm', 'accommoda', 'accommodate', 'accommodated', 'accommodates', 'accommodating', 'accommodation', 'accomo', 'accomodation', 'accomommodation', 'accompanied', 'accompanies', 'accompaniment']&quot;&quot;&quot; Stemming Stemming is the process of reducing a word into its stem, i.e. its root form. The root form is not necessarily a word by itself, but it can be used to generate words by concatenating the right suffix. For example, the words fish, fishes and fishing all stem into fish, which is a correct word. On the other side, the words study, studies and studying stems into studi, which is not an English word. Tokenizing Tokenization is breaking the sentence into words and punctuation, 12345stemmer = SnowballStemmer('english')tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')def tokenize(text): return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())] Vectorization with stop words(words irrelevant to the model), stemming and tokenizing 123456789101112131415vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)X2 = vectorizer2.fit_transform(desc)word_features2 = vectorizer2.get_feature_names()print(len(word_features2))print(word_features2[:50]) # output&quot;&quot;&quot;65232[&quot;'a&quot;, &quot;'i&quot;, &quot;'s&quot;, &quot;'t&quot;, 'aa', 'aaa', 'aaahhh', 'aac', 'aacc', 'aaco', 'aacta', 'aad', 'aadmi', 'aag', 'aagaard', 'aagard', 'aah', 'aalto', 'aam', 'aamer', 'aami', 'aamodt', 'aandahl', 'aant', 'aap', 'aapa', 'aapt', 'aar', 'aaradhna', 'aardman', 'aardvark', 'aargau', 'aaron', 'aaronpaul', 'aarwun', 'aat', 'ab', 'aba', 'abaaoud', 'ababa', 'aback', 'abadi', 'abadon', 'abal', 'abalon', 'abalonv', 'abama', 'abandon', 'abandond', 'abandong']&quot;&quot;&quot;vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)X3 = vectorizer3.fit_transform(desc)words = vectorizer3.get_feature_names() For this, we will use k-means clustering algorithm. ### K-means clustering (Source Wikipedia) Elbow method to select number of clusters This method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified. Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance. Basically, number of clusters = the x-axis value of the point that is the corner of the \"elbow\"(the plot looks often looks like an elbow) 123456789101112from sklearn.cluster import KMeanswcss = []for i in range(1,11): kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0) kmeans.fit(X3) wcss.append(kmeans.inertia_)plt.plot(range(1,11),wcss)plt.title('The Elbow Method')plt.xlabel('Number of clusters')plt.ylabel('WCSS')plt.savefig('elbow.png')plt.show() As more than one elbows have been generated, I will have to select right amount of clusters by trial and error. So, I will showcase the results of different amount of clusters to find out the right amount of clusters. 123456print(words[250:300])# output&quot;&quot;&quot;['decis', 'declar', 'defenc', 'defend', 'delay', 'deliv', 'demand', 'deni', 'despit', 'destroy', 'detent', 'develop', 'die', 'director', 'disabl', 'disast', 'discuss', 'diseas', 'dismiss', 'disput', 'doctor', 'dog', 'dollar', 'domest', 'donald', 'donat', 'doubl', 'doubt', 'draw', 'dri', 'drink', 'drive', 'driver', 'drop', 'drought', 'drown', 'drug', 'drum', 'dump', 'dure', 'e', 'eagl', 'earli', 'eas', 'east', 'econom', 'economi', 'edg', 'educ', 'effort']&quot;&quot;&quot; 3 Clusters 12345678910111213kmeans = KMeans(n_clusters = 3, n_init = 20, n_jobs = 1) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)kmeans.fit(X3)# We look at 3 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid)) # output&quot;&quot;&quot;0 : new, say, plan, win, council, govt, australia, report, kill, fund, urg, court, warn, water, australian, nsw, open, chang, year, qld, interview, wa, death, face, crash1 : polic, investig, probe, man, search, offic, hunt, miss, arrest, death, car, shoot, drug, seek, attack, assault, say, murder, crash, charg, driver, suspect, fatal, raid, station2 : man, charg, murder, court, face, jail, assault, stab, die, death, drug, guilti, child, sex, accus, attack, woman, crash, arrest, car, kill, miss, sydney, alleg, plead&quot;&quot;&quot; 5 Clusters 123456789101112131415kmeans = KMeans(n_clusters = 5, n_init = 20, n_jobs = 1)kmeans.fit(X3)# We look at 5 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))# output&quot;&quot;&quot;0 : man, plan, charg, court, govt, australia, face, murder, accus, jail, assault, stab, urg, drug, death, attack, child, sex, die, woman, guilti, say, alleg, told, car1 : new, zealand, law, year, plan, open, polic, home, hospit, centr, deal, set, hope, australia, look, appoint, announc, chief, say, south, minist, govt, rule, servic, welcom2 : say, win, kill, report, australian, warn, interview, open, water, fund, nsw, crash, death, urg, year, chang, wa, sydney, claim, qld, hit, attack, world, set, health3 : council, plan, consid, fund, rate, urg, seek, new, merger, water, land, develop, reject, say, mayor, vote, chang, elect, rise, meet, park, push, want, govt, approv4 : polic, investig, man, probe, search, offic, hunt, miss, arrest, death, car, charg, shoot, drug, seek, attack, assault, murder, crash, say, driver, fatal, suspect, raid, woman&quot;&quot;&quot; 6 Clusters 12345678910111213141516kmeans = KMeans(n_clusters = 6, n_init = 20, n_jobs = 1)kmeans.fit(X3)# We look at 6 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid)) # output&quot;&quot;&quot;0 : council, govt, australia, report, warn, urg, fund, australian, water, nsw, chang, qld, wa, health, elect, rural, countri, hour, sa, boost, climat, govern, servic, south, consid1 : man, charg, murder, court, face, jail, assault, stab, die, death, drug, guilti, child, sex, accus, attack, woman, crash, arrest, car, kill, miss, sydney, plead, alleg2 : polic, investig, probe, man, search, offic, hunt, miss, arrest, death, car, shoot, drug, seek, attack, crash, assault, murder, charg, driver, say, fatal, suspect, raid, warn3 : win, kill, court, interview, crash, open, death, sydney, face, year, claim, hit, attack, world, set, final, day, hous, die, home, jail, talk, return, cup, hospit4 : new, zealand, law, year, plan, open, council, polic, home, hospit, centr, deal, set, hope, australia, appoint, look, announc, chief, say, govt, south, minist, mayor, welcom5 : say, plan, council, govt, water, need, group, chang, labor, minist, govern, opposit, public, mp, health, union, green, hous, develop, resid, report, expert, cut, australia, mayor&quot;&quot;&quot; 8 Clusters 123456789101112131415161718kmeans = KMeans(n_clusters = 8, n_init = 20, n_jobs = 1)kmeans.fit(X3)# Finally, we look at 8 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid)) # output&quot;&quot;&quot;0 : polic, say, man, miss, arrest, jail, investig, car, search, murder, attack, crash, kill, probe, die, hunt, shoot, assault, offic, drug, stab, accus, fatal, guilti, bodi1 : death, hous, polic, toll, investig, man, probe, inquest, rise, woman, coron, blaze, price, public, white, babi, sentenc, famili, road, spark, jail, prompt, blame, custodi, report2 : plan, council, govt, water, new, say, develop, hous, group, chang, unveil, reject, park, urg, centr, public, expans, green, resid, health, reveal, labor, govern, opposit, power3 : court, face, man, accus, told, hear, murder, high, case, appear, rule, charg, alleg, appeal, drug, jail, woman, death, assault, order, sex, stab, challeng, teen, polic4 : australia, govt, kill, report, warn, australian, urg, fund, nsw, interview, water, open, crash, qld, chang, wa, year, day, claim, hit, attack, sydney, set, health, world5 : new, council, zealand, law, fund, year, consid, water, urg, open, say, seek, rate, centr, mayor, govt, elect, look, develop, land, deal, hope, set, push, home6 : win, award, cup, titl, open, gold, stage, world, final, tour, elect, australia, lead, seri, aussi, claim, second, australian, big, england, grand, m, battl, race, record7 : charg, man, murder, face, assault, drug, polic, child, sex, woman, teen, death, stab, drop, alleg, attack, rape, men, guilti, shoot, bail, sydney, fatal, driver, yo&quot;&quot;&quot; Because even I didn't know what kind of clusters would be generated, I will describe them in comments. Other discussions 1234567891011121314151617181920212223242526272829303132333435363738394041import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom nltk.corpus import stopwordsfrom sklearn.feature_extraction.text import CountVectorizerimport gensimfrom collections import Counterimport stringfrom nltk.stem import WordNetLemmatizer, PorterStemmerfrom nltk.tokenize import word_tokenizeimport pyLDAvis.gensim_modelsfrom wordcloud import WordCloud, STOPWORDSfrom textblob import TextBlobfrom spacy import displacyimport nltkimport warningswarnings.filterwarnings('ignore')# set pltplt.rcParams['font.sans-serif'] = ['Arial Unicode MS']plt.rcParams.update({'font.size': 12})plt.rcParams.update({'figure.figsize': [16, 12]})# plt.figure(figsize = [20, 20])plt.style.use('seaborn-whitegrid')df = pd.read_csv('../data/abcnews-date-text.csv', nrows = 10000)df.head()# output&quot;&quot;&quot;publish_date headline_text0 20030219 aba decides against community broadcasting lic...1 20030219 act fire witnesses must be aware of defamation2 20030219 a g calls for infrastructure protection summit3 20030219 air nz staff in aust strike for pay rise4 20030219 air nz strike to affect australian travellers&quot;&quot;&quot; The data set contains only two columns, the release date and the news title. For simplicity, I will explore the first 10,000 rows in this dataset. Since the titles are sorted by publish_date, they are actually two months from February 19, 2003 to April 7, 2003. Number of characters present in each sentence Visualization of text statistics is a simple but insightful technique. They include: Word frequency analysis, sentence length analysis, average word length analysis, etc. These really help to explore the basic characteristics of text data. For this, we will mainly use histograms (continuous data) and bar graphs (categorical data). First, let me look at the number of characters in each sentence. This can give us a rough idea of the length of news headlines. 1df['headline_text'].str.len().hist() number of words appearing in each news headline The histogram shows that the range of news headlines is 10 to 70 characters, usually between 25 and 55 characters. Now, we will continue to explore the data verbatim. Let's plot the number of words that appear in each news headline. 1df['headline_text'].str.split().map(lambda x: len(x)).hist() Analysing word length Obviously, the number of words in news headlines is in the range of 2 to 12, and most of them are between 5 and 7. Next, let's check the average word length in each sentence. 1df['headline_text'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x)).hist() The average word length is between 3 and 9, and the most common length is 5. Does this mean that people use very short words in news headlines? Let us find out. One reason that may not be the case is stop words. Stop words are the most commonly used words in any language (such as \"the\", \"a\", \"an\", etc.). Since the length of these words may be small, these words may cause the above graphics to be skewed to the left. Analyzing the number and types of stop words can give us some in-depth understanding of the data. To get a corpus containing stop words, you can use the nltk library. Nltk contains stop words from multiple languages. Since we only deal with English news, I will filter English stop words from the corpus. Analysing stopwords 1234567891011121314151617181920212223242526272829# Fetch stopwordsimport nltknltk.download('stopwords')stop=set(stopwords.words('english'))# output&quot;&quot;&quot;[nltk_data] Downloading package stopwords to /Users/xx/nltk_data...[nltk_data] Package stopwords is already up-to-date!&quot;&quot;&quot;# Create corpuscorpus=[]new= df['headline_text'].str.split()new=new.values.tolist()corpus=[word for i in new for word in i]from collections import defaultdictdic=defaultdict(int)for word in corpus: if word in stop: dic[word]+=1 # Plot top stopwordstop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] x,y=zip(*top)plt.bar(x,y) Draw popular stop words Most common words We can clearly see that in the news headlines, stop words such as \"to\", \"in\" and \"for\" dominate. So now that we know which stop words appear frequently in our text, let's check which words other than these stop words appear frequently. We will use the counter function in the collection library to count the occurrence of each word and store it in a list of tuples. This is a very useful feature when we are dealing with word-level analysis in natural language processing. 12345678910counter=Counter(corpus)most=counter.most_common()x, y=[], []for word,count in most[:40]: if (word not in stop): x.append(word) y.append(count) sns.barplot(x=y,y=x) Wow! In the past 15 years, \"America\", \"Iraq\" and \"War\" have dominated the headlines. \"We\" here may mean the United States or us (you and me). We are not a stop word, but when we look at the other words in the picture, they are all related to the United States-the Iraq War and \"we\" here may mean the United States. Ngram analysis Ngram is a continuous sequence of n words. For example, \"Riverbank\", \"Three Musketeers\" and so on. If the number of words is two, it is called a double word. For 3 characters, it is called a trigram, and so on. Viewing the most common n-grams can give you a better understanding of the context in which the word is used. Bigram analysis To build our vocabulary, we will use Countvectorizer. Countvectorizer is a simple method for labeling, vectorizing and representing corpora in an appropriate form. Can be found in sklearn.feature_engineering.text Therefore, we will analyze the top news in all news headlines. 123456789101112def get_top_ngram(corpus, n=None): vec = CountVectorizer(ngram_range=(n, n)).fit(corpus) bag_of_words = vec.transform(corpus) sum_words = bag_of_words.sum(axis=0) words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) return words_freq[:10]top_n_bigrams=get_top_ngram(df['headline_text'],2)[:10]x,y=map(list,zip(*top_n_bigrams))sns.barplot(x=y,y=x) Trigram analysis We can observe that dualisms such as \"anti-war\" and \"killed\" related to war dominate the headlines. How about triples? 123top_tri_grams=get_top_ngram(df['headline_text'],n=3)x,y=map(list,zip(*top_tri_grams))sns.barplot(x=y,y=x) We can see that many of these hexagrams are a combination of \"face the court\" and \"anti-war protest.\" This means that we should spend some effort on data cleaning to see if we can combine these synonyms into a clean token. Topic modelling Use pyLDAvis for topic modeling exploration Topic modeling is the process of using unsupervised learning techniques to extract the main topics that appear in the document set. Latent Dirichlet Allocation (LDA) is an easy-to-use and efficient topic modeling model. Each document is represented by a topic distribution, and each topic is represented by a word distribution. Once the documents are classified into topics, you can delve into the data for each topic or topic group. But before entering topic modeling, we must do some preprocessing of the data. we will: Tokenization: The process of converting sentences into tokens or word lists. remove stopwordslemmatize: Reduce the deformed form of each word to a common base or root. Convert to word bag: word bag is a dictionary where the key is the word (or ngram/tokens) and the value is the number of times each word appears in the corpus. With NLTK, you can easily tokenize and formalize: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import nltknltk.download('punkt')nltk.download('wordnet')# output&quot;&quot;&quot;[nltk_data] Downloading package punkt to /Users/xx/nltk_data...[nltk_data] Package punkt is already up-to-date![nltk_data] Downloading package wordnet to /Users/xx/nltk_data...[nltk_data] Unzipping corpora/wordnet.zip.True&quot;&quot;&quot;def preprocess_news(df): corpus=[] stem=PorterStemmer() lem=WordNetLemmatizer() for news in df['headline_text']: words=[w for w in word_tokenize(news) if (w not in stop)] words=[lem.lemmatize(w) for w in words if len(w)&gt;2] corpus.append(words) return corpus corpus = preprocess_news(df)# Now, let's use gensim to create a bag of words modeldic=gensim.corpora.Dictionary(corpus)bow_corpus = [dic.doc2bow(doc) for doc in corpus]# We can finally create the LDA model:lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 4, id2word = dic, passes = 10, workers = 2)lda_model.show_topics()# output&quot;&quot;&quot;[(0, '0.010*&quot;say&quot; + 0.007*&quot;cup&quot; + 0.006*&quot;war&quot; + 0.005*&quot;world&quot; + 0.005*&quot;back&quot; + 0.005*&quot;plan&quot; + 0.005*&quot;green&quot; + 0.004*&quot;win&quot; + 0.004*&quot;woman&quot; + 0.004*&quot;new&quot;'), (1, '0.010*&quot;govt&quot; + 0.009*&quot;war&quot; + 0.009*&quot;new&quot; + 0.007*&quot;may&quot; + 0.005*&quot;sars&quot; + 0.005*&quot;call&quot; + 0.005*&quot;protest&quot; + 0.005*&quot;boost&quot; + 0.005*&quot;group&quot; + 0.004*&quot;hospital&quot;'), (2, '0.018*&quot;police&quot; + 0.015*&quot;baghdad&quot; + 0.014*&quot;man&quot; + 0.005*&quot;missing&quot; + 0.005*&quot;claim&quot; + 0.005*&quot;court&quot; + 0.005*&quot;australia&quot; + 0.004*&quot;move&quot; + 0.004*&quot;murder&quot; + 0.004*&quot;charged&quot;'), (3, '0.030*&quot;iraq&quot; + 0.015*&quot;war&quot; + 0.007*&quot;iraqi&quot; + 0.007*&quot;council&quot; + 0.006*&quot;troop&quot; + 0.005*&quot;killed&quot; + 0.004*&quot;crash&quot; + 0.004*&quot;soldier&quot; + 0.004*&quot;open&quot; + 0.004*&quot;say&quot;')]&quot;&quot;&quot; Theme 0 represents things related to the Iraq war and the police. Theme 3 shows Australia's involvement in the Iraq War. You can print all the topics and try to understand them, but there are tools that can help you run this data exploration more effectively. pyLDAvis is such a tool, it can interactively visualize the results of LDA. Visualize the topics 123pyLDAvis.enable_notebook()vis = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dic)vis On the left, the area of each circle represents the importance of the topic relative to the corpus. Because there are four themes, we have four circles. The distance between the center of the circle indicates the similarity between themes. Here you can see that Topic 3 and Topic 4 overlap, which indicates that the themes are more similar. On the right, the histogram of each topic shows the top 30 related words. For example, in topic 1, the most relevant words are \"police\", \"new\", \"may\", \"war\", etc. Therefore, in our case, we can see many war-related words and topics in the news headlines. Wordclouds Wordcloud is a great way to represent text data. The size and color of each word appearing in the word cloud indicate its frequency or importance. It is easy to create a wordcloud using python, but we need to provide data in the form of a corpus. 123456789101112131415161718192021stopwords = set(STOPWORDS)def show_wordcloud(data, title = None): wordcloud = WordCloud( background_color='white', stopwords=stopwords, max_words=100, max_font_size=30, scale=3, random_state=1 ) wordcloud=wordcloud.generate(str(data)) fig = plt.figure(1, figsize=(12, 12)) plt.axis('off') plt.imshow(wordcloud) plt.show() show_wordcloud(corpus) Similarly, you can see that terms related to war are highlighted, indicating that these words often appear in news headlines. There are many parameters that can be adjusted. Some of the most famous are: stopwords: stop a group of words appearing in the image. max_words: Indicates the maximum number of words to be displayed. max_font_size: Maximum font size. There are many other options to create beautiful word clouds. For more detailed information, you can refer to here. Text sentiment Sentiment analysis is a very common natural language processing task in which we determine whether the text is positive, negative or neutral. This is very useful for finding sentiments related to comments and comments, allowing us to gain some valuable insights from text data. There are many projects that can help you use python for sentiment analysis. I personally like TextBlob and Vader Sentiment. 1234567from textblob import TextBlobTextBlob('100 people killed in Iraq').sentiment# output&quot;&quot;&quot;Sentiment(polarity=-0.2, subjectivity=0.0)&quot;&quot;&quot; Textblob Textblob is a python library built on top of nltk. It has been around for a while and is very easy to use. The sentiment function of TextBlob returns two attributes: Polarity: It is a floating-point number in the range of [-1,1], where 1 means a positive statement and -1 means a negative statement. Subjectivity: refers to how personal opinions and feelings affect someone’s judgment. The subjectivity is expressed as a floating point value with a range of [0,1]. I will run this feature on news headlines. TextBlob claims that the text \"100 people killed in Iraq\" is negative, not a view or feeling, but a statement of fact. I think we can agree to TextBlob here. Now that we know how to calculate these sentiment scores, we can use histograms to visualize them and explore the data further. 123456def polarity(text): return TextBlob(text).sentiment.polaritydf['polarity_score']=df['headline_text'].\\ apply(lambda x : polarity(x))df['polarity_score'].hist() You will see that the polarity is mainly between 0.00 and 0.20. This shows that most news headlines are neutral. Let's categorize news as negative, positive, and neutral based on the scores for a more in-depth study. Postive , Negative or Neutral ? 1234567891011121314def sentiment(x): if x&lt;0: return 'neg' elif x==0: return 'neu' else: return 'pos' df['polarity']=df['polarity_score'].\\ map(lambda x: sentiment(x)) plt.bar(df.polarity.value_counts().index, df.polarity.value_counts()) Yes, 70% of news is neutral, only 18% of positive news and 11% of negative news. Let's look at the positive and negative headlines. 1234567891011df[df['polarity']=='neg']['headline_text'].head(5)# output&quot;&quot;&quot;7 aussie qualifier stosur wastes four memphis match23 carews freak goal leaves roma in ruins28 council chief executive fails to secure position34 dargo fire threat expected to rise40 direct anger at govt not soldiers crean urgesName: headline_text, dtype: object&quot;&quot;&quot; Vader The next library we are going to discuss is VADER. Vader is better at detecting negative emotions. It is very useful in the context of social media text sentiment analysis. The VADER or Valence Aware dictionary and sentiment reasoner is an open source sentiment analyzer pre-built library based on rules/dictionaries and is protected by the MIT license. The VADER sentiment analysis class returns a dictionary that contains the possibility that the text appears positive, negative, and neutral. Then, we can filter and select the emotion with the highest probability. We will use VADER to perform the same analysis and check if the difference is large. 1234567891011121314151617181920212223from nltk.sentiment.vader import SentimentIntensityAnalyzernltk.download('vader_lexicon')sid = SentimentIntensityAnalyzer()def get_vader_score(sent): # Polarity score returns dictionary ss = sid.polarity_scores(sent) #return ss return np.argmax(list(ss.values())[:-1]) &quot;&quot;&quot;[nltk_data] Downloading package vader_lexicon to[nltk_data] /Users/xx/nltk_data...&quot;&quot;&quot;df['polarity']=df['headline_text'].\\ map(lambda x: get_vader_score(x))polarity=df['polarity'].replace({0:'neg',1:'neu',2:'pos'})plt.bar(polarity.value_counts().index, polarity.value_counts()) Yes, the distribution is slightly different. There are even more headlines classified as neutral 85%, and the number of negative news headlines has increased (to 13%). Named Entity Recognition Named entity recognition is an information extraction method in which entities existing in the text are classified into predefined entity types, such as \"person\", \"location\", \"organization\" and so on. By using NER, we can gain insight into the entities that exist in a given text data set of entity types. Let us consider an example of a news article. In the above news, the named entity recognition model should be able to recognize Entities, such as RBI as an organization, Mumbai and India as Places, etc. There are three standard libraries for named entity recognition: Stanford Nell space NLTK I will use spaCy, which is an open source library for advanced natural language processing tasks. It is written in Cython and is known for its industrial applications. In addition to NER, spaCy also provides many other functions, such as pos mark, word to vector conversion, etc. SpaCy’s Named Entity Recognition has been published in OntoNotes 5 has been trained on the corpus and supports the following entity types There are three kinds of pre-trained models for English in SpaCy. I will use en_core_web_sm to complete our task, but you can try other models. To use it, we must first download it: 12345678910111213141516171819202122232425# !python -m spacy download en_core_web_sm# Now we can initialize the language model:import spacyfrom spacy import displacyimport en_core_web_smnlp = en_core_web_sm.load()# nlp = spacy.load(&quot;en_core_web_sm&quot;)# One of the advantages of Spacy is that we only need to apply the nlp function once, and the entire background pipeline will return the objects we needdoc=nlp('India and Iran have agreed to boost the economic \\viability of the strategic Chabahar port through various measures, \\including larger subsidies to merchant shipping firms using the facility, \\people familiar with the development said on Thursday.')[(x.text,x.label_) for x in doc.ents] &quot;&quot;&quot;[('India', 'GPE'), ('Iran', 'GPE'), ('Chabahar', 'GPE'), ('Thursday', 'DATE')]&quot;&quot;&quot; We can see that India and Iran are confirmed as geographic locations (GPE), Chabahar is confirmed as a person, and Thursday is confirmed as a date. We can also use the display module in spaCy to visualize the output. 123from spacy import displacydisplacy.render(doc, style='ent') This can make sentences with recognized entities look very neat, and each entity type is marked with a different color. Now that we know how to perform NER, we can further explore the data by performing various visualizations on the named entities extracted from the data set. First, we will run named entity recognition on news headlines and store entity types. NER Analysis 123456789101112def ner(text): doc=nlp(text) return [X.label_ for X in doc.ents] ent=df['headline_text'].apply(lambda x : ner(x))ent=[x for sub in ent for x in sub]counter=Counter(ent)count=counter.most_common()# Now, we can visualize the entity frequency:x,y=map(list,zip(*count))sns.barplot(x=y,y=x) Now we can see that GPE and ORG dominate the headlines, followed by the PERSON entity. We can also visualize the most common tokens for each entity. Let's check which places appear the most in news headlines. Most common GPE 12345678910def ner(text,ent=&quot;GPE&quot;): doc=nlp(text) return [X.text for X in doc.ents if X.label_ == ent] gpe=df['headline_text'].apply(lambda x: ner(x,&quot;GPE&quot;))gpe=[i for x in gpe for i in x]counter=Counter(gpe)x,y=map(list,zip(*counter.most_common(10)))sns.barplot(y,x) I think we can confirm the fact that \"America\" means America in news headlines. Let's also find the most common names that appear on news headlines. Most common person 123456per=df['headline_text'].apply(lambda x: ner(x,&quot;PERSON&quot;))per=[i for x in per for i in x]counter=Counter(per)x,y=map(list,zip(*counter.most_common(10)))sns.barplot(y,x) Saddam Hussein and George Bush served as presidents of Iraq and the United States during the war. In addition, we can see that the model is far from perfect to classify \"vic govt\" or \"nsw govt\" as individuals rather than government agencies. Pos tagging Use nltk for all parts of speech markup, but there are other libraries that can do the job well (spaacy, textblob). 123456789101112import nltknltk.download('averaged_perceptron_tagger')sentence=&quot;The greatest comeback stories in 2019&quot;tokens=word_tokenize(sentence)nltk.pos_tag(tokens)# Notice:# You can also use the spacy.displacy module to visualize the sentence part of the speech and its dependency graph.doc = nlp('The greatest comeback stories in 2019')displacy.render(doc, style='dep', jupyter=True, options={'distance': 90}) We can observe various dependency labels here. For example, the DET tag indicates the relationship between the word \"the\" and the noun \"stories\". You can check the list of dependency labels and their meanings here. Okay, now that we know what a POS tag is, let's use it to explore the title data set. Analysing pos tags 1234567891011def pos(text): pos=nltk.pos_tag(word_tokenize(text)) pos=list(map(list,zip(*pos)))[1] return pos tags=df['headline_text'].apply(lambda x : pos(x))tags=[x for l in tags for x in l]counter=Counter(tags)x,y=list(map(list,zip(*counter.most_common(7))))sns.barplot(x=y,y=x) We can clearly see that nouns (NN) dominate in news headlines, followed by adjectives (JJ). This is typical for news reports, and for art forms, higher adjective (ADJ) frequencies may happen a lot. You can investigate this in more depth by investigating the most common singular nouns in news headlines. Let us find out. Nouns such as \"war\", \"Iraq\", and \"person\" dominate the news headlines. You can use the above functions to visualize and check other parts of the voice. Most common Nouns 123456789101112131415def get_adjs(text): adj=[] pos=nltk.pos_tag(word_tokenize(text)) for word,tag in pos: if tag=='NN': adj.append(word) return adjwords=df['headline_text'].apply(lambda x : get_adjs(x))words=[x for l in words for x in l]counter=Counter(words)x,y=list(map(list,zip(*counter.most_common(7))))sns.barplot(x=y,y=x) Dependency graph 12doc = nlp('She sells seashells by the seashore')displacy.render(doc, style='dep', jupyter=True, options={'distance': 90}) Text readability Textstat 12from textstat import flesch_reading_easedf['headline_text'].apply(lambda x : flesch_reading_ease(x)).hist() complex headlines? Almost all readability scores exceed 60. This means that an average of 11-year-old students can read and understand news headlines. Let's check all news headlines with a readability score below 5. 123456789101112131415x=[i for i in range(len(reading)) if reading[i]&lt;5] &quot;&quot;&quot;rror loading preloads:Failed to fetch dynamically imported module: https://file+.vscode-resource.vscode-webview.net/Users/xx/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/datascience-ui/errorRenderer/errorRenderer.js&quot;&quot;&quot;news.iloc[x]['headline_text'].head() &quot;&quot;&quot;Error loading preloads:Failed to fetch dynamically imported module: https://file+.vscode-resource.vscode-webview.net/Users/xx/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/datascience-ui/errorRenderer/errorRenderer.js&quot;&quot;&quot; Final thoughts In this article, we discussed and implemented various exploratory data analysis methods for text data. Some are common and little known, but all of them can be an excellent addition to your data exploration toolkit. Hope you will find some of them useful for your current and future projects. To make data exploration easier, I created a \"exploratory data analysis of natural language processing templates\", which you can use for your work. In addition, you may have seen that for each chart in this article, there is a code snippet to create it. Just click the button below the chart. Happy exploring! From: https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr","link":"/example_02/"},{"title":"Machine Learning Part-01","text":"Linear Regression Example Implement Linear Regression for Boston House Price Problem 123456789import randomimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsimport numpy as npfrom sklearn.datasets import load_bostonfrom matplotlib.animation import FuncAnimationimport re Part-01: Linear Regression 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200housing_price = load_boston()dataframe = pd.DataFrame(housing_price['data'])dataframe.columns = housing_price['feature_names']dataframe['price'] = housing_price['target']# sns.heatmap(dataframe.corr(), annot=True, fmt='.1f')# plt.show()print(dataframe.columns) &quot;&quot;&quot;Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'price'], dtype='object')&quot;&quot;&quot;rm = dataframe['RM']lst = dataframe['LSTAT']target = dataframe['price']def model(x, w, b): return np.dot(x, w.T) + bdef loss(yhat, y): return np.mean( (yhat - y) ** 2)def partial_w(x1, x2, y, yhat): return np.array([2 *np.mean((yhat - y) * x1), 2 * np.mean((yhat - y) * x2)])def partial_b(x1, x2, y, yhat): return 2 * np.mean((yhat - y))w = np.random.random_sample((1, 2))print(w)b = 0alpha = 1e-5epoch = 200history = []history_k_b_loss = [] &quot;&quot;&quot;[[0.76646144 0.3095512 ]]&quot;&quot;&quot;for e in range(epoch): losses = [] for batch in range(len(rm)): random_index = random.choice(range(len(rm))) x1, x2 = rm[random_index], lst[random_index] y = target[random_index] yhat = model(np.array([x1, x2]), w, b) loss_v = loss(yhat, y) w = w - partial_w(x1, x2, y, yhat) * alpha b = b - partial_b(x1, x2, y, yhat) * alpha losses.append(loss_v) history_k_b_loss.append((w, b, loss_v)) if batch % 100 == 0: print('Epoch: {}, Batch: {}, loss: {}'.format(e, batch, np.mean(losses))) history.append(np.mean(losses)) &quot;&quot;&quot;Epoch: 0, Batch: 0, loss: 151.86271856102778Epoch: 0, Batch: 100, loss: 263.5872813250959show more (open the raw output data in a text editor) ...Epoch: 199, Batch: 500, loss: 28.308274447364248&quot;&quot;&quot;````## Logstic Regression```pythonhousing_price = load_boston()dataframe = pd.DataFrame(housing_price['data'])dataframe.columns = housing_price['feature_names']dataframe['price'] = housing_price['target']rm = dataframe['RM']lst = dataframe['LSTAT']price = dataframe['price']print(np.percentile(price, 66)) &quot;&quot;&quot;23.53&quot;&quot;&quot;# plt.hist(target)# plt.show()dataframe['expensive'] = dataframe['price'].apply(lambda p: int(p &gt; np.percentile(price, 66)))expensive = dataframe['expensive']# print(dataframe.head())print(dataframe['expensive']) &quot;&quot;&quot;0 11 0 ..505 0Name: expensive, Length: 506, dtype: int64&quot;&quot;&quot;def logistic(x): return 1 / (1 + np.exp(-x))def model(x, w, b): return logistic(np.dot(x, w.T) + b)def loss(yhat, y): return -1 * np.sum(y*np.log(yhat) + (1 - y) * np.log(1 - yhat))def partial_w(x1, x2, y, yhat): return np.array([np.sum((yhat - y) * x1), np.sum((yhat - y) * x2)])def partial_b(x1, x2, y, yhat): return np.sum(yhat - y) w = np.random.random_sample((1, 2))print(w) &quot;&quot;&quot;[[0.69565948 0.90768813]]&quot;&quot;&quot;b = 0alpha = 1e-5epoch = 200history = []history_k_b_loss = []for e in range(epoch): losses = [] for batch in range(len(rm)): random_index = random.choice(range(len(rm))) x1, x2 = rm[random_index], lst[random_index] y = expensive[random_index] yhat = model(np.array([x1, x2]), w, b) loss_v = loss(yhat, y) w = w - partial_w(x1, x2, y, yhat) * alpha b = b - partial_b(x1, x2, y, yhat) * alpha losses.append(loss_v) history_k_b_loss.append((w, b, loss_v)) if batch % 100 == 0: print('Epoch: {}, Batch: {}, loss: {}'.format(e, batch, np.mean(losses))) history.append(np.mean(losses)) &quot;&quot;&quot;Epoch: 0, Batch: 0, loss: 3.14765267665445e-06Epoch: 0, Batch: 100, loss: 13.555508645878497show more (open the raw output data in a text editor) ...Epoch: 199, Batch: 500, loss: 0.31372698791846687&quot;&quot;&quot;predicated = [model(np.array([x1, x2]), w, b) for x1, x2 in zip(rm, lst)]true = expensivedef accuracy(y, yhat): return sum(1 if i == j else 0 for i, j in zip(y, yhat)) / len(y) print(accuracy(true, predicated)) &quot;&quot;&quot;0.0&quot;&quot;&quot; decision boundary Linear Regression: Regression is implemented, including the definition of linear functions, why use linear functions, the meaning of loss, the meaning of gradient descent, stochastic gradient descent Use Boston house price dataset. The data set of Beijing housing prices in 2020, why didn’t I use the data set of Beijing housing prices? Boston: room size, subway, highway, crime rate have a more obvious relationship, so it is easier to observe the relationship Beijing's housing prices:! Far and near! Room Condition ==》 School District! ! ! ! =&gt; Very expensive Haidian District 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import randomimport numpy as npimport pandas as pdfrom sklearn.datasets import load_bostondataset = load_boston()data = dataset['data']target = dataset['target']columns = dataset['feature_names']dataframe = pd.DataFrame(data)dataframe.columns = columnsdataframe['price'] = target# print(dataframe.corr()) # show the correlation of dataframe variables# correlation =&gt; If one value increases, it will cause another value to increase, and the correlation coefficient is closer to 1 if it increases in a certain proportion.# correlation =&gt; 0 means there is no relationship between the two# correlation =&gt; -1 One value increases, the other value must decrease, and the decrease is in equal proportion# sns.heatmap(dataframe.corr())# plt.show()# RM: The average number of bedrooms in the community# LSTAT: Percentage of low-income people aroundrm = dataframe['RM']lstat = dataframe['LSTAT']def linear(x, w, b): # vectorized model return np.dot(x, w.T) + bdef loss(yhat, y): # numpy broadcast numpy广播方法 return np.mean( (yhat - y) ** 2)def partial_w(x, y, yhat): return np.array([2 * np.mean((yhat - y) * x[0]), 2 * np.mean((yhat - y) * x[1])])def partial_b(x, y, yhat): return 2 * np.mean((yhat - y))def optimize(w, b, x, y, yhat, pw, pb, learning_rate): w = w + -1 * pw(x, y, yhat) * learning_rate b = b + -1 * pb(x, y, yhat) * learning_rate return w, b def train(model_to_be_train, target, loss, pw, pb): w = np.random.random_sample((1, 2)) # w normal b = np.random.random() # 0 深度学习的时候会和大家详细解释 learning_rate = 1e-5 epoch = 200 losses = [] for i in range(epoch): batch_loss = [] for batch in range(len(rm)): # batch training index = random.choice(range(len(rm))) rm_x, lstat_x = rm[index], lstat[index] x = np.array([rm_x, lstat_x]) y = target[index] yhat = model_to_be_train(x, w, b) loss_v = loss(yhat, y) batch_loss.append(loss_v) w, b = optimize(w, b, x, y, yhat, pw, pb, learning_rate) if batch % 100 == 0: print('Epoch: {} Batch: {}, loss: {}'.format(i, batch, loss_v)) losses.append(np.mean(batch_loss)) return model_to_be_train, w, b, losses if __name__ == &quot;__main__&quot;: import matplotlib.pyplot as plt target = dataframe['price'] model, w, b, losses = train(linear, target, loss, partial_w, partial_b) plt.plot(losses) predicate = model(np.array([19, 7]), w, b) print(predicate) plt.show() &quot;&quot;&quot;Epoch: 0 Batch: 0, loss: 165.0318036522631Epoch: 0 Batch: 100, loss: 1936.2111196826459show more (open the raw output data in a text editor) ...Epoch: 199 Batch: 500, loss: 0.024829543832110872[88.74340551]&quot;&quot;&quot; Logstic Regression Linear Regression: Regression is implemented, including the definition of linear functions, why use linear functions, the meaning of loss, the meaning of gradient descent, stochastic gradient descent Use Boston house price dataset. The data set of Beijing housing prices in 2020, why didn’t I use the data set of Beijing housing prices? Boston: room size, subway, highway, crime rate have a more obvious relationship, so it is easier to observe the relationship Beijing's housing prices:! Far and near! Room Condition ==》 School District! ! ! ! =&gt; Very expensive Haidian District Harder than deep learning: 1. compiler 2. programming language &amp; automata 3. computer graphic 4. complexity system 5. computing complexity 6. operating system 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130from sklearn.datasets import load_bostonimport pandas as pdimport numpy as npdataset = load_boston()data = dataset['data']target = dataset['target']columns = dataset['feature_names']dataframe = pd.DataFrame(data)dataframe.columns = columnsdataframe['price'] = target# print(dataframe.corr()) # show the correlation of dataframe variables# correlation =&gt; If one value increases, it will cause another value to increase, and the correlation coefficient is closer to 1 if it increases in a certain proportion.# correlation =&gt; 0 means there is no relationship between the two# correlation =&gt; -1 One value increases, the other value must decrease, and the decrease is in equal proportion# sns.heatmap(dataframe.corr())# plt.show()# RM: The average number of bedrooms in the community# LSTAT: Percentage of low-income people aroundrm = dataframe['RM']lstat = dataframe['LSTAT']price = dataframe['price']greater_then_most = np.percentile(price, 66)dataframe['expensive'] = dataframe['price'].apply(lambda p: int(p&gt; greater_then_most))target = dataframe['expensive']print(dataframe[:20]) &quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX \\0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 5 0.02985 0.0 2.18 0.0 0.458 6.430 58.7 6.0622 3.0 222.0 6 0.08829 12.5 7.87 0.0 0.524 6.012 66.6 5.5605 5.0 311.0 7 0.14455 12.5 7.87 0.0 0.524 6.172 96.1 5.9505 5.0 311.0 8 0.21124 12.5 7.87 0.0 0.524 5.631 100.0 6.0821 5.0 311.0 9 0.17004 12.5 7.87 0.0 0.524 6.004 85.9 6.5921 5.0 311.0 10 0.22489 12.5 7.87 0.0 0.524 6.377 94.3 6.3467 5.0 311.0 11 0.11747 12.5 7.87 0.0 0.524 6.009 82.9 6.2267 5.0 311.0 12 0.09378 12.5 7.87 0.0 0.524 5.889 39.0 5.4509 5.0 311.0 13 0.62976 0.0 8.14 0.0 0.538 5.949 61.8 4.7075 4.0 307.0 14 0.63796 0.0 8.14 0.0 0.538 6.096 84.5 4.4619 4.0 307.0 15 0.62739 0.0 8.14 0.0 0.538 5.834 56.5 4.4986 4.0 307.0 16 1.05393 0.0 8.14 0.0 0.538 5.935 29.3 4.4986 4.0 307.0 17 0.78420 0.0 8.14 0.0 0.538 5.990 81.7 4.2579 4.0 307.0 18 0.80271 0.0 8.14 0.0 0.538 5.456 36.6 3.7965 4.0 307.0 19 0.72580 0.0 8.14 0.0 0.538 5.727 69.5 3.7965 4.0 307.0 PTRATIO B LSTAT price expensive 0 15.3 396.90 4.98 24.0 1 1 17.8 396.90 9.14 21.6 0 2 17.8 392.83 4.03 34.7 1 3 18.7 394.63 2.94 33.4 1 4 18.7 396.90 5.33 36.2 1 5 18.7 394.12 5.21 28.7 1 6 15.2 395.60 12.43 22.9 0 7 15.2 396.90 19.15 27.1 1 8 15.2 386.63 29.93 16.5 0 9 15.2 386.71 17.10 18.9 0 10 15.2 392.52 20.45 15.0 0 11 15.2 396.90 13.27 18.9 0 12 15.2 390.50 15.71 21.7 0 13 21.0 396.90 8.26 20.4 0 14 21.0 380.02 10.26 18.2 0 15 21.0 395.62 8.47 19.9 0 16 21.0 386.85 6.58 23.1 0 17 21.0 386.75 14.67 17.5 0 18 21.0 288.99 11.69 20.2 0 19 21.0 390.95 11.28 18.2 0 &quot;&quot;&quot;def sigmoid(x): return 1 / (1 + np.exp(-x))def model(x, w, b): return sigmoid(np.dot(x, w.T) + b)def loss(yhat, y): return -np.sum(y*np.log(yhat) + (1 - y)*np.log(1 - yhat))def partial_w(x, y, yhat): return np.array([np.sum((yhat - y) * x[0]), np.sum((yhat - y) * x[1])])def partial_b(x, y, yhat): return np.sum((yhat - y)) model, w, b, losses = train(model, target,loss, partial_w, partial_b)random_test_indices = np.random.choice(range(len(rm)), size=100)decision_boundary = 0.5 &quot;&quot;&quot;Epoch: 0 Batch: 0, loss: 5.380792320433632Epoch: 0 Batch: 100, loss: 4.821708458450062show more (open the raw output data in a text editor) ...Epoch: 199 Batch: 500, loss: 0.052809537616594626&quot;&quot;&quot;for i in random_test_indices: x1, x2, y = rm[i], lstat[i], target[i] predicate = model(np.array([x1, x2]), w, b) predicate_label = int(predicate &gt; decision_boundary) print('RM: {}, LSTAT: {}, EXPENSIVE: {}, Predicated: {}'.format(x1, x2, y, predicate_label)) &quot;&quot;&quot;RM: 5.701, LSTAT: 18.35, EXPENSIVE: 0, Predicated: 0RM: 4.973, LSTAT: 12.64, EXPENSIVE: 0, Predicated: 0show more (open the raw output data in a text editor) ...RM: 6.678, LSTAT: 6.27, EXPENSIVE: 1, Predicated: 1&quot;&quot;&quot; One thing left is to check the accuracy of our model! ! How to measure the quality of the model: 1. accuracy precision recall f1, f2 score AUC-ROC curve Introduce a very very important concept: -&gt; over-fitting and under-fitting (over-fitting and under-fitting) The entire machine learning process is constantly adjusting over-fitting and under-fitting!","link":"/example_03/"},{"title":"Machine Learning Part-02","text":"Data Pre-processing Feature-Extractor Split Training, Test, Validation Build Model Gradient Descent Evaluation Predicat Analysis House Price Regression 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129## load datafrom sklearn.datasets import load_boston## ususlly will load in csvdata = load_boston()print(data['DESCR'])&quot;&quot;&quot;_boston_dataset:Boston house prices dataset---------------------------**Data Set Characteristics:** :Number of Instances: 506 show more (open the raw output data in a text editor) ...Morgan Kaufmann.&quot;&quot;&quot;import pandas as pdimport numpy as npdf = pd.DataFrame(data['data'])df.columns = data['feature_names']df[df['CHAS'] == 1]&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT142 3.32105 0.0 19.58 1.0 0.8710 5.403 100.0 1.3216 5.0 403.0 14.7 396.90 26.82... 1.1296 24.0 666.0 20.2 347.88 8.88&quot;&quot;&quot;## Pre-processingdf.std()&quot;&quot;&quot;CRIM 8.601545ZN 23.322453INDUS 6.860353CHAS 0.253994NOX 0.115878RM 0.702617AGE 28.148861DIS 2.105710RAD 8.707259TAX 168.537116PTRATIO 2.164946B 91.294864LSTAT 7.141062dtype: float64&quot;&quot;&quot;df['CHAS'] = df['CHAS'].astype('int')df['CHAS'] = df['CHAS'].astype('category')df['RAD'] = df['RAD'].astype('int')df['RAD'] = df['RAD'].astype('category')df&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98...505 0.04741 0.0 11.93 0 0.573 6.030 80.8 2.5050 1 273.0 21.0 396.90 7.88506 rows × 13 columns&quot;&quot;&quot;df['RAD']&quot;&quot;&quot;0 11 22 2...505 1Name: RAD, Length: 506, dtype: categoryCategories (9, int64): [1, 2, 3, 4, ..., 6, 7, 8, 24]&quot;&quot;&quot;from sklearn.preprocessing import OneHotEncoderonehoter = OneHotEncoder()chas_and_rad_vec = onehoter.fit_transform(df[['CHAS', 'RAD']])## Standarlizefrom sklearn.preprocessing import StandardScalerss = StandardScaler()df.shape&quot;&quot;&quot;(506, 13)&quot;&quot;&quot;real_vec = ss.fit_transform(df.drop(columns = ['CHAS', 'RAD']))chas_and_rad_vec[0].toarray()&quot;&quot;&quot;array([[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])&quot;&quot;&quot;import numpy as npnp.mean(real_vec, axis = 0)&quot;&quot;&quot;array([-1.12338772e-16, 7.89881994e-17, 2.10635198e-16, -1.96592852e-16, -1.08828186e-16, -1.47444639e-16, -8.42540793e-17, 0.00000000e+00, -4.21270397e-16, -7.44244367e-16, -3.08931624e-16])&quot;&quot;&quot;np.std(real_vec, axis = 0)&quot;&quot;&quot;array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])&quot;&quot;&quot;real_vec.shape&quot;&quot;&quot;(506, 11)&quot;&quot;&quot;chas_and_rad_vec.shape&quot;&quot;&quot;(506, 11)&quot;&quot;&quot;## Feature-ExtractorX = np.concatenate((real_vec, chas_and_rad_vec.toarray()), axis = 1)y = data['target']## Split Training, Test, Validationdef split_train_val_test(X, y, test_ratio = 0.2, val_ratio = 0.2): indices = np.random.choice(range(len(X)), size = len(X), replace=False) train_indices = indices[:int(len(X) * (1-test_ratio) * (1 - val_ratio))] val_indices = indices[int(len(X)*(1-test_ratio) * (1-val_ratio)): int(len(X) * (1-test_ratio))] test_indices = indices[int(len(X) * (1-test_ratio)):] return (X[train_indices], y[train_indices]), (X[val_indices], y[val_indices]), (X[test_indices], y[test_indices])(X_train, y_train), (X_val, y_val), (X_test, y_test) = split_train_val_test(X, y) sklearn.model_selection.train_test_split also could be used Build-Model 1234567from sklearn.linear_model import LinearRegressionregression = LinearRegression()regression.fit(X_train, y_train)&quot;&quot;&quot;LinearRegression()&quot;&quot;&quot; Question: If overfittiing or underfitting? Explain: Why validation set is more useful in deep learning Gradient Descent Evaluation 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152regression.score(X_train, y_train)&quot;&quot;&quot;0.7477980609064946&quot;&quot;&quot;regression.score(X_val, y_val)&quot;&quot;&quot;0.7611715890963341&quot;&quot;&quot;regression.score(X_test, y_test)&quot;&quot;&quot;0.711869928554872&quot;&quot;&quot;## Interpreterregression.coef_&quot;&quot;&quot;array([-1.04208922, 1.30263494, 0.29143618, -2.31827512, 2.40383155, 0.25013857, -3.55953868, -1.68823412, -2.37743843, 0.74411049, -3.79489254, -0.79143926, 0.79143926, -2.51995654, -2.20671004, 0.65594998, -0.31683083, -0.07929752, -2.15244627, -0.06686364, 1.93167854, 4.75447632])&quot;&quot;&quot;regression.intercept_&quot;&quot;&quot;22.070279554739386&quot;&quot;&quot;### PredictX_test[0]&quot;&quot;&quot;array([ 1.68404594, -0.48772236, 1.01599907, 1.07378711, 0.21279502, 1.11749449, -0.93188642, 1.53092646, 0.80657583, -3.61192313, 2.29842066, 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. ])&quot;&quot;&quot;regression.predict([X_test[0]])&quot;&quot;&quot;array([9.64589284])&quot;&quot;&quot;import matplotlib.pyplot as pltfor i in range(5): plt.scatter(X[:, 5], y) plt.scatter(X[:, 5], regression.predict(X))plt.show() 1234567891011121314151617import matplotlibmatplotlib.colors%matplotlib inlinedef show_predication_result(x, target): width = 3 fig,ax = plt.subplots(x.shape[1]//width + 1, width, figsize = (40,40)) for i in range(x.shape[1]): ix = np.unravel_index(i, ax.shape) plt.sca(ax[ix]) ax[ix].title.set_text('Feature-{}'.format(i)) plt.scatter(x[:, i], target) plt.scatter(x[:, i], regression.predict(x)) show_predication_result(X_train, y_train) 1show_predication_result(X_val, y_val) 1show_predication_result(X_test, y_test) Outliers Part-02 Logstic Regression Data Pre-processing Feature-Extractor Split Training, Test, Validation Build Model Gradient Descent Evaluation Predicat Analysis Pre-processing 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom struct import unpackdef loadmnist(imagefile, labelfile): # Open the images with gzip in read binary mode images = open(imagefile, 'rb') labels = open(labelfile, 'rb') # Get metadata for images images.read(4) # skip the magic_number number_of_images = images.read(4) number_of_images = unpack('&gt;I', number_of_images)[0] rows = images.read(4) rows = unpack('&gt;I', rows)[0] cols = images.read(4) cols = unpack('&gt;I', cols)[0] # Get metadata for labels labels.read(4) N = labels.read(4) N = unpack('&gt;I', N)[0] # Get data x = np.zeros((N, rows*cols), dtype = np.uint8) #Initialize numpy array y = np.zeros(N, dtype = np.uint8) # Initialize numpy array for i in range(N): for j in range(rows*cols): tmp_pixel = images.read(1) # Just a single byte tmp_pixel = unpack('&gt;B', tmp_pixel)[0] x[i][j] = tmp_pixel tmp_label = labels.read(1) y[i] = unpack('&gt;B', tmp_label)[0] images.close() labels.close() return (x, y) X_train, y_train = loadmnist('~/data/course_data/t10k-images-idx3-ubyte','~/data/course_data/t10k-labels-idx1-ubyte') X_test, y_test = loadmnist('~/data/course_data/train-images-idx3-ubyte','~/data/course_data/train-labels-idx1-ubyte') X_train.shape &quot;&quot;&quot; (10000, 784) &quot;&quot;&quot; X_test &quot;&quot;&quot; array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]], dtype=uint8) &quot;&quot;&quot; y_test &quot;&quot;&quot; array([5, 0, 4, ..., 5, 6, 8], dtype=uint8) &quot;&quot;&quot; plt.figure(figsize = (20, 4))for index, (image, label) in enumerate(zip(X_train[0:5], y_train[0:5])): plt.subplot(1, 5, index+1) plt.imshow(np.reshape(image, (28, 28))) plt.title('Traininng: %i\\n' % label, fontsize = 20) We only choose label with 0 and 6 1234567891011121314151617181920212223242526272829303132333435zero_train_indices = np.where(y_train == 0)one_train_indices = np.where(y_train == 6)train_indices = np.concatenate((zero_train_indices[0], one_train_indices[0]))zero_test_indices = np.where(y_test == 0)one_test_indices = np.where(y_test == 6)test_indices = np.concatenate((zero_test_indices[0], one_test_indices[0]))train_indices = np.random.choice(train_indices, size = len(train_indices), replace=False)test_indices = np.random.choice(test_indices, size = len(test_indices), replace=False)val_ratio= 0.2train_indices = train_indices[: int(len(train_indices) * (1 - val_ratio))]val_indices = train_indices[int(len(train_indices) * (1 - val_ratio)):]binary_x_train = X_train[train_indices]binary_x_test = X_test[test_indices]binary_x_val = X_train[val_indices]binary_y_train = y_train[train_indices]binary_y_test = y_test[test_indices]binary_y_val = y_train[val_indices]import randombinary_y_train&quot;&quot;&quot;array([6, 0, 0, ..., 6, 0, 0], dtype=uint8)&quot;&quot;&quot;plt.imshow(np.reshape(binary_x_train[1], (28,28)))plt.title('Training: %i\\n' % binary_y_train[1], fontsize =20)&quot;&quot;&quot;Text(0.5, 1.0, 'Training: 0\\n')&quot;&quot;&quot; 123456789101112131415from collections import CounterCounter(binary_y_train)&quot;&quot;&quot;Counter({6: 768, 0: 782})&quot;&quot;&quot;Counter(binary_y_test)&quot;&quot;&quot;Counter({6: 5918, 0: 5923})&quot;&quot;&quot;Counter(binary_y_val)&quot;&quot;&quot;Counter({0: 148, 6: 162})&quot;&quot;&quot; Build model 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from sklearn.linear_model import LogisticRegressionclf = LogisticRegression(random_state = 0, solver = 'lbfgs')# L-BFGS-B - Software for Large-scale Bound-constrained Optimizationimport warningswarnings.filterwarnings('ignore')clf.fit(binary_x_train, binary_y_train)&quot;&quot;&quot;LogisticRegression(random_state=0)&quot;&quot;&quot;clf.coef_&quot;&quot;&quot;array([[ 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,show more (open the raw output data in a text editor) ... 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])&quot;&quot;&quot;clf.intercept_&quot;&quot;&quot;array([0.00016519])&quot;&quot;&quot;clf.score&quot;&quot;&quot;&lt;bound method ClassifierMixin.score of LogisticRegression(random_state=0)&gt;&quot;&quot;&quot;clf.score(binary_x_train, binary_y_train)&quot;&quot;&quot;1.0&quot;&quot;&quot;clf.score(binary_x_val, binary_y_val)&quot;&quot;&quot;1.0&quot;&quot;&quot;binary_x_test.shape&quot;&quot;&quot;(11841, 784)&quot;&quot;&quot;binary_y_test.shape&quot;&quot;&quot;0.9865720800608057&quot;&quot;&quot;predicated_result = clf.predict(binary_x_test)np.where(binary_y_test != predicated_result)&quot;&quot;&quot;(array([ 17, 45, 66, 137, 260, 279, 323, 453, 529, 739, 753, 947, 1034, 1248, 1290, 1422, 1434, 1444, ... 10677, 10739, 10750, 10979, 11010, 11058, 11104, 11113, 11366, 11389, 11421, 11458, 11528, 11659, 11760]),)&quot;&quot;&quot;lookup_index = 1184plt.imshow(np.reshape(binary_x_test[lookup_index], (28, 28)))plt.title('Actual Value: {} ; Predict Value: {} \\n'.format(binary_y_test[lookup_index], predicated_result[lookup_index]), fontsize = 20)&quot;&quot;&quot;Text(0.5, 1.0, 'Actual Value: 6 ; Predict Value: 6 \\n')&quot;&quot;&quot; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from sklearn import metricsbinary_y_test[0]&quot;&quot;&quot;6&quot;&quot;&quot;predicated_result[0]&quot;&quot;&quot;6&quot;&quot;&quot;metrics.precision_score(binary_y_test, predicated_result, average = 'macro')&quot;&quot;&quot;0.9865879016517065&quot;&quot;&quot;metrics.precision_score(binary_y_test, predicated_result, pos_label = 6)&quot;&quot;&quot;0.9837056946077608&quot;&quot;&quot;metrics.recall_score(binary_y_test, predicated_result, pos_label = 6)&quot;&quot;&quot;0.9895234876647516&quot;&quot;&quot;fpr, tpr, threshold = metrics.roc_curve(binary_y_test, predicated_result, pos_label = 6)metrics.auc(fpr, tpr)&quot;&quot;&quot;0.9865733258009728&quot;&quot;&quot;cm = metrics.confusion_matrix(binary_y_test, predicated_result)import seaborn as snsfrom sklearn.metrics import confusion_matrixdata = confusion_matrix(binary_y_test, predicated_result)data&quot;&quot;&quot;array([[5826, 97], [ 62, 5856]])&quot;&quot;&quot;df_cm = pd.DataFrame(data, columns = np.unique(binary_y_test), index = np.unique(binary_y_test))plt.figure(figsize = (10, 7))sns.set(font_scale=1.4) # for label sizesns.heatmap(df_cm, cmap='Blues', annot=True, annot_kws = {'size': 16}) # font size&quot;&quot;&quot;&lt;AxesSubplot:&gt;&quot;&quot;&quot; 1234567df_cm.index.name = 'Actual'df_cm.columns.name = 'Predicted'plt.figure(figsize = (10, 10))sns.heatmap(df_cm, cmap='Blues', annot=True, annot_kws={'size': 16})&quot;&quot;&quot;&lt;AxesSubplot:xlabel='Predicted', ylabel='Actual'&gt;&quot;&quot;&quot; Boston code reproduction and reference answers 123456789101112131415161718192021222324252627282930313233# Import package# Used to load the Boston housing price data setfrom sklearn.datasets import load_boston# pandas toolkit For students who are new to pandas, please refer to the official 10-minute tutorial: https://pandas.pydata.org/pandas-docs/stable/10min.htmlimport pandas as pd# seaborn for drawingimport seaborn as snsimport numpy as np # numpy# Show drawing%matplotlib inlinedata = load_boston()data.keys()&quot;&quot;&quot;dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])&quot;&quot;&quot;df = pd.DataFrame(data['data'])df.head()&quot;&quot;&quot; 0 1 2 3 4 5 6 7 8 9 10 11 120 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.981 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.142 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.033 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.944 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33&quot;&quot;&quot;data['feature_names']&quot;&quot;&quot;array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='&lt;U7')&quot;&quot;&quot; Field meaning 名称 中文描述 CRIM 住房所在城镇的人均犯罪率 ZN 住房用地超过 25000 平方尺的比例 INDUS 住房所在城镇非零售商用土地的比例 CHAS 有关查理斯河的虚拟变量（如果住房位于河边则为1,否则为0 ） NOX 一氧化氮浓度 RM 每处住房的平均房间数 AGE 建于 1940 年之前的业主自住房比例 DIS 住房距离波士顿五大中心区域的加权距离 RAD 离住房最近的公路入口编号 TAX 每 10000 美元的全额财产税金额 PTRATIO 住房所在城镇的师生比例 B 1000(Bk-0.63)^2,其中 Bk 指代城镇中黑人的比例 LSTAT 弱势群体人口所占比例 MEDV 业主自住房的中位数房价（以千美元计） 1234567891011121314151617181920df.columns = data['feature_names']df.head()&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.981 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.142 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.033 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.944 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33&quot;&quot;&quot;df['price'] = data['target']df.head(2)&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT price0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.9 4.98 24.01 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.9 9.14 21.6&quot;&quot;&quot;sns.heatmap(df.corr(), annot=True, fmt='.1f') 12345import matplotlib.pyplot as pltplt.scatter(df['RM'], df['price'])&quot;&quot;&quot;&lt;matplotlib.collections.PathCollection at 0x7fe0f984f810&gt;&quot;&quot;&quot; 12345678910111213plt.figure(figsize = (20, 5))features = ['LSTAT', 'RM']target = df['price']for i, col in enumerate(features): plt.subplot(1, len(features), i+1) x = df[col] y = target plt.scatter(x, y, marker = 'o') plt.title('{} vs price'.format(col)) plt.xlabel(col) plt.ylabel('price') 1234567891011121314151617181920x = df['RM']y = df['price']history_notes = {_x: _y for _x, _y in zip(x, y)}history_notes[6.575]&quot;&quot;&quot;24.0&quot;&quot;&quot;# Find the top three prices closest to RM:6.57,similary_ys = [y for _, y in sorted(history_notes.items(), key=lambda x_y: (x_y[0]-6.57) ** 2)[:3]]similary_ys&quot;&quot;&quot;[23.8, 24.0, 24.8]&quot;&quot;&quot;np.mean(similary_ys) # Calculate the average of three&quot;&quot;&quot;24.2&quot;&quot;&quot; Using historical data to predict data that has never been seen before, the most direct method K-Neighbor-Nearst 1234567891011def knn(query_x, history, top_n=3): sorted_notes = sorted(history.items(), key=lambda x_y: (x_y[0] - query_x) ** 2) similar_notes = sorted_notes[:top_n] similar_ys = [y for _, y in similar_notes] return np.mean(similar_ys)knn(5.4, history_notes)&quot;&quot;&quot;15.700000000000001&quot;&quot;&quot; In order to obtain results faster, we hope to obtain predictive power by fitting a function \\[ f(rm) = k * rm + b \\] Random Approach \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} (\\hat{y_i} - y_i) ^ 2 \\] \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] 12345678910111213141516171819202122232425262728293031323334def loss(yhat, y): return np.mean((yhat - y) **2)import randommin_loss = float('inf')best_k, bes_b = None, Noneprint(min_loss)min_loss = float('inf')best_k, bes_b = None, Nonefor step in range(1000): min_v, max_v = -100, 100 k, b = random.randrange(min_v, max_v), random.randrange(min_v, max_v) y_hats = [k * rm_i + b for rm_i in x] current_loss = loss(y_hats, y) if current_loss &lt;min_loss: min_loss = current_loss best_k, best_b = k, b print('In step {}, we have obtained the function f(rm) = {} * rm + {}, at this time loss is: {}'.format(step, k, b, current_loss))&quot;&quot;&quot;In step 0, we have obtained the function f(rm) = 14 * rm + -78, at this time loss is: 212.87040239525695In step 70, we have obtained the function f(rm) = 10 * rm + -47, at this time loss is: 88.70654683794466In step 256, we have obtained the function f(rm) = 13 * rm + -55, at this time loss is: 68.45390542094862In step 526, we have obtained the function f(rm) = 10 * rm + -37, at this time loss is: 54.977297826086954&quot;&quot;&quot;plt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x])&quot;&quot;&quot;&lt;matplotlib.collections.PathCollection at 0x7fe0980f37d0&gt;&quot;&quot;&quot; Monte Carlo simulation Supervisor \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{k}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i) * rm_i \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{b}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i)\\] 12345678910111213141516171819202122232425262728293031323334def partial_k(k, b, x, y): return 2 * np.mean((k*x+b-y) *x)def partial_b(k, b, x, y): return 2*np.mean(k*x+b-y) k, b = random.random(), random.random()min_loss = float('inf')best_k, best_b = None, Nonelearning_rate = 1e-2for step in range(2000): k,b = k+(-1*partial_k(k,b,x,y) * learning_rate), b+(-1*partial_b(k,b,x,y) * learning_rate) y_hats = k * x +b current_loss = loss(y_hats, y) if current_loss &lt; min_loss: min_loss = current_loss best_k, best_b = k, b print('On the {} step, we have func f(rm) = {} * rm + {}, loss is {} now'.format(step, k, b, current_loss))&quot;&quot;&quot;On the 0 step, we have func f(rm) = 6.968714597804018 * rm + -21.099847342593957, loss is 45.86961514375004 nowOn the 1 step, we have func f(rm) = 6.9692276199804555 * rm + -21.103110737199852, loss is 45.86852398135223 nowshow more (open the raw output data in a text editor) ...On the 1999 step, we have func f(rm) = 7.783005326604901 * rm + -26.279646762684518, loss is 44.468037178267025 now&quot;&quot;&quot;best_k, best_b&quot;&quot;&quot;(10, -37)&quot;&quot;&quot;plt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x]) Supervised Learning We turn the forecast of housing prices into a more responsible and sophisticated model. What should we do? \\[ f(x) = k * x + b \\] \\[ f(x) = k2 * \\sigma(k_1 * x + b_1) + b2 \\] \\[ \\sigma(x) = \\frac{1}{1 + e^(-x)} \\] 1234def sigmoid(x): return 1 / (1 + np.exp(-x))sub_x = np.linspace(-10, 10)plt.plot(sub_x, sigmoid(sub_x)) 12345678910def random_linear(x): k, b = random.random(), random.random() return k * x + bdef complex_function(x): return (random_linear(x))for _ in range(10): index = random.randrange(0, len(sub_x)) sub_x_1, sub_x_2 = sub_x[:index], sub_x[index:] new_y = np.concatenate((complex_function(sub_x_1), complex_function(sub_x_2))) plt.plot(sub_x, new_y) We can implement more complex functions through simple, basic modules and repeated superposition For more and more complex functions? How does the computer seek guidance? What is machine learning? The shortcomings of the KNN method, what is the background of the proposed linear fitting How to obtain faster function weight update through supervision method The combination of nonlinear and linear functions can fit very complex functions Deep learning we can fit more complex functions through basic function modules Assigment \\[ L2-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{(\\hat{y} - y)}^2 \\] \\[ L1-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{|(\\hat{y} - y)|} \\] Change L2-Loss in the code to L1Loss and implement gradient descent Realize L1Loss gradient descent from 0 1 Import package 12import numpy as npimport pandas as pd 2 Load data set 123456789101112131415161718192021222324252627282930313233from sklearn.datasets import load_bostonboston = load_boston()boston.keys()&quot;&quot;&quot;dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])&quot;&quot;&quot;X = boston.datay = boston.targetdf = pd.DataFrame(boston.data, columns = boston.feature_names)df.head()&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.981 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.142 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.033 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.944 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33&quot;&quot;&quot;df.describe() # Data description, you can view the statistics of each variable&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTATcount 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000mean 3.613524 11.363636 11.136779 0.069170 0.554695 6.284634 68.574901 3.795043 9.549407 408.237154 18.455534 356.674032 12.653063std 8.601545 23.322453 6.860353 0.253994 0.115878 0.702617 28.148861 2.105710 8.707259 168.537116 2.164946 91.294864 7.141062min 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 187.000000 12.600000 0.320000 1.73000025% 0.082045 0.000000 5.190000 0.000000 0.449000 5.885500 45.025000 2.100175 4.000000 279.000000 17.400000 375.377500 6.95000050% 0.256510 0.000000 9.690000 0.000000 0.538000 6.208500 77.500000 3.207450 5.000000 330.000000 19.050000 391.440000 11.36000075% 3.677083 12.500000 18.100000 0.000000 0.624000 6.623500 94.075000 5.188425 24.000000 666.000000 20.200000 396.225000 16.955000max 88.976200 100.000000 27.740000 1.000000 0.871000 8.780000 100.000000 12.126500 24.000000 711.000000 22.000000 396.900000 37.970000&quot;&quot;&quot; 3 Data preprocessing Normalization or standardization can prevent a certain dimension or a few dimensions from affecting the data too much when there are very many dimensions, and secondly, the program can run faster. There are many methods, such as standardization, min-max, z-score, p-norm, etc. How to use it depends on the characteristics of the data set. Extended reading-the deep learning field of the myth of data standardization 12345678910111213141516171819from sklearn.preprocessing import StandardScalerss = StandardScaler() # z = (x-u) / s u is the mean, s is the standard deviationX = ss.fit_transform(df) # For linear models, normalization or standardization is generally required, otherwise there will be a gradient explosion, which is generally not required for tree modelsdf = pd.DataFrame(X, columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX' ,'PTRATIO','B','LSTAT'])df.describe()&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTATcount 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02mean 2.808469e-17 6.599903e-16 -4.633974e-16 -4.353127e-16 1.404235e-16 -1.755293e-17 2.176564e-16 -1.685082e-16 -5.055245e-16 8.987102e-16 -1.067218e-15 4.493551e-16 -2.246775e-16std 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00min -4.197819e-01 -4.877224e-01 -1.557842e+00 -2.725986e-01 -1.465882e+00 -3.880249e+00 -2.335437e+00 -1.267069e+00 -9.828429e-01 -1.313990e+00 -2.707379e+00 -3.907193e+00 -1.531127e+0025% -4.109696e-01 -4.877224e-01 -8.676906e-01 -2.725986e-01 -9.130288e-01 -5.686303e-01 -8.374480e-01 -8.056878e-01 -6.379618e-01 -7.675760e-01 -4.880391e-01 2.050715e-01 -7.994200e-0150% -3.906665e-01 -4.877224e-01 -2.110985e-01 -2.725986e-01 -1.442174e-01 -1.084655e-01 3.173816e-01 -2.793234e-01 -5.230014e-01 -4.646726e-01 2.748590e-01 3.811865e-01 -1.812536e-0175% 7.396560e-03 4.877224e-02 1.015999e+00 -2.725986e-01 5.986790e-01 4.827678e-01 9.067981e-01 6.623709e-01 1.661245e+00 1.530926e+00 8.065758e-01 4.336510e-01 6.030188e-01max 9.933931e+00 3.804234e+00 2.422565e+00 3.668398e+00 2.732346e+00 3.555044e+00 1.117494e+00 3.960518e+00 1.661245e+00 1.798194e+00 1.638828e+00 4.410519e-01 3.548771e+00&quot;&quot;&quot; \\[ y=Σwixi+b \\] Because the derivation of b is all 1, add a bias b to the data and set it to 1, as a feature of the data and update the gradient wi*b=wi 1234567891011121314151617df['bias'] = 1df&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT bias0 -0.419782 0.284830 -1.287909 -0.272599 -0.144217 0.413672 -0.120013 0.140214 -0.982843 -0.666608 -1.459000 0.441052 -1.075562 11 -0.417339 -0.487722 -0.593381 -0.272599 -0.740262 0.194274 0.367166 0.557160 -0.867883 -0.987329 -0.303094 0.441052 -0.492439 12 -0.417342 -0.487722 -0.593381 -0.272599 -0.740262 1.282714 -0.265812 0.557160 -0.867883 -0.987329 -0.303094 0.396427 -1.208727 13 -0.416750 -0.487722 -1.306878 -0.272599 -0.835284 1.016303 -0.809889 1.077737 -0.752922 -1.106115 0.113032 0.416163 -1.361517 14 -0.412482 -0.487722 -1.306878 -0.272599 -0.835284 1.228577 -0.511180 1.077737 -0.752922 -1.106115 0.113032 0.441052 -1.026501 1... ... ... ... ... ... ... ... ... ... ... ... ... ... ...501 -0.413229 -0.487722 0.115738 -0.272599 0.158124 0.439316 0.018673 -0.625796 -0.982843 -0.803212 1.176466 0.387217 -0.418147 1502 -0.415249 -0.487722 0.115738 -0.272599 0.158124 -0.234548 0.288933 -0.716639 -0.982843 -0.803212 1.176466 0.441052 -0.500850 1503 -0.413447 -0.487722 0.115738 -0.272599 0.158124 0.984960 0.797449 -0.773684 -0.982843 -0.803212 1.176466 0.441052 -0.983048 1504 -0.407764 -0.487722 0.115738 -0.272599 0.158124 0.725672 0.736996 -0.668437 -0.982843 -0.803212 1.176466 0.403225 -0.865302 1505 -0.415000 -0.487722 0.115738 -0.272599 0.158124 -0.362767 0.434732 -0.613246 -0.982843 -0.803212 1.176466 0.441052 -0.669058 1506 rows × 14 columns&quot;&quot;&quot; Divide the data set, where 20% of the data is used as the test set X_test, y_test, and the other 80% are used as the training set X_train, y_train, where random_state is the random seed 1234567891011from sklearn.model_selection import train_test_splitX_train, X_test, y_train,y_test = train_test_split(df, y, test_size = 0.2, random_state = 42)print('X_train.shape, y_train.shape:', X_train.shape, y_train.shape)print('X_test.shape, y_test.shape', X_test.shape, y_test.shape)&quot;&quot;&quot;X_train.shape, y_train.shape: (404, 14) (404,)X_test.shape, y_test.shape (102, 14) (102,)&quot;&quot;&quot;X_train = np.array(X_train) Model training and gradient update 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697def l1_cost(X, y, theta): &quot;&quot;&quot; X: 特征 y: 目标值 theta: 模型参数 &quot;&quot;&quot; k = X.shape[0] total_cost = 0 for i in range(k): total_cost =+ 1/k * np.abs(y[i] - theta.dot(X[i, :])) return total_cost def l2_cost(X, y, theta): k = X.shape[0] total_cost = 0 for i in range(k): total_cost += 1/k * (y[i] - theta.dot(X[i, :])) ** 2 return total_cost np.zeros(10).shape&quot;&quot;&quot;(10,)&quot;&quot;&quot;def step_l1_gradient(X, y, learning_rate, theta): &quot;&quot;&quot; Function to calculate the gradient of the MAE loss function Return the gradient value 0 for the non-differentiable point at 0 X: feature vector y: target value learing_rate: learning rate theta: parameter &quot;&quot;&quot; n = X.shape[0] print(n) e = y-X @ theta gradients = -(X.T @ np.sign(e)) / n theta = theta-learning_rate * gradients return theta def step_l2_gradient(X, y, learning_rate, theta): k = X.shape[0] x = X.shape[1] gradients = np.zeros(n) for i in range(k): for j in range(n): gradients[j] += (-2/k) * (y[i] - (theta.dot(X[i, :]))) * X[i, j] theta = theta - learning_rate * gradients return theta def step_gradient(X, y, learning_rate, theta): &quot;&quot;&quot; X: feature vector y: target value learing_rate: learning rate theta: parameter &quot;&quot;&quot; m_deriv = 0 N = len(X) for i in range(N): # Calculate the partial derivative # -x(y-(mx + b)) / |mx + b| m_deriv +=-X[i] * (y[i]-(theta*X[i] + b)) / abs(y[i]-(theta*X[i] + b)) # We subtract because the derivatives point in direction of steepest ascent theta -= (m_deriv / float(N)) * learning_rate# theta = theta-learning_rate * gradients return thetadef gradient_descent(X_train, y_train, learning_rate, iterations): k = X_train.shape[0] n = X_train.shape[1] theta = np.zeros(n) loss_values = [] print(theta.shape) for i in range(iterations): theta = step_l1_gradient(X_train, y_train, learning_rate, theta) loss = l1_cost(X_train, y_train, theta) loss_values.append(loss) print(i, 'cost:', loss) return theta, loss_values # Training parameterslearning_rate = 0.04 # Learning rateiterations = 300 # number of iterationstheta ,loss_values = gradient_descent(X_train, y_train, learning_rate, iterations)&quot;&quot;&quot;(14,)4040 cost: 0.045943991727139124041 cost: 0.045848379493882215404show more (open the raw output data in a text editor) ...299 cost: 0.017838215258874083&quot;&quot;&quot; Heart Practise 1234import pandas as pdpath = '~/data/'dataPath = path + 'heart.csv'train_data = pd.read_csv(dataPath) Field meaning 字段名 含义 age 年龄 sex 性别(1 = 男性, 0 = 女性) cp 胸部疼痛类型(值1：典型心绞痛，值2：非典型性心绞痛，值3：非心绞痛，值4：无症状） trestbps 血压 chol 胆固醇 fbs 空腹血糖（&gt; 120 mg/dl，1=真；0=假） restecg 心电图结果（0=正常，1=患有ST-T波异常，2=根据Estes的标准显示可能或确定的左心室肥大） thalach 最大心跳数 exang 运动时是否心绞痛（1=有过；0=没有） oldpeak 运动相对于休息的ST slop 心电图ST segment的倾斜度(值1:上坡，值2:平坦，值3:下坡） ca 透视检查看到的血管数 thal 缺陷种类（3=正常；6=固定缺陷；7=可逆缺陷） target 是否患病（0=否，1=是） Print a brief summary of the data set 12345678910111213141516171819202122232425262728293031train_data.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 303 entries, 0 to 302Data columns (total 14 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 303 non-null int64 1 sex 303 non-null int64 2 cp 303 non-null int64 3 trestbps 303 non-null int64 4 chol 303 non-null int64 5 fbs 303 non-null int64 6 restecg 303 non-null int64 7 thalach 303 non-null int64 8 exang 303 non-null int64 9 oldpeak 303 non-null float64 10 slope 303 non-null int64 11 ca 303 non-null int64 12 thal 303 non-null int64 13 target 303 non-null int64 dtypes: float64(1), int64(13)memory usage: 33.3 KB&quot;&quot;&quot;train_data.target.value_counts()&quot;&quot;&quot;1 1650 138Name: target, dtype: int64&quot;&quot;&quot; Change the \"sex\" column to two columns \"sex_0\" and \"sex_1\". 1sex = pd.get_dummies(train_data['sex'], prefix = &quot;sex&quot;) Add \"sex_0\" and \"sex_1\" to the data set. 1train_data = pd.concat([train_data,sex], axis = 1) And delete the sex column 1train_data = train_data.drop(columns = ['sex']) Print out the first five lines. Check whether sex_0, sex_1 are added successfully, and whether sex is deleted successfully. 123456789train_data.head()&quot;&quot;&quot; age cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target sex_0 sex_10 63 3 145 233 1 0 150 0 2.3 0 0 1 1 0 11 37 2 130 250 0 1 187 0 3.5 0 0 2 1 0 12 41 1 130 204 0 0 172 0 1.4 2 0 2 1 1 03 56 1 120 236 0 1 178 0 0.8 2 0 2 1 0 14 57 0 120 354 0 1 163 1 0.6 2 0 2 1 1 0&quot;&quot;&quot; Get sample label 12345y_data = train_data.target.valuestrain_data.shape&quot;&quot;&quot;(303, 15)&quot;&quot;&quot; Get sample feature set 12345x_data = train_data.drop(['target'],axis=1)x_data.shape&quot;&quot;&quot;(303, 14)&quot;&quot;&quot; Divide the data set Parameters: test_size=0.3, random_state=33 12from sklearn.model_selection import train_test_splitX_train,X_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=33) Normalization Import the StandardScaler package and initialize 12from sklearn.preprocessing import StandardScalerstandardScaler = StandardScaler() fit function/module is used to train model parameters 1standardScaler.fit(X_train) Standardize the training set and test set 12X_train = standardScaler.transform(X_train)X_test = standardScaler.transform(X_test) Define logistic regression model 123from sklearn.linear_model import LogisticRegression log_reg = LogisticRegression()log_reg.fit(X_train,y_train) Calculate the training set score 1234log_reg.score(X_train,y_train)&quot;&quot;&quot;0.8537735849056604&quot;&quot;&quot; Calculate the test set score 1234log_reg.score(X_test,y_test)&quot;&quot;&quot;0.8461538461538461&quot;&quot;&quot; Use the classification_report function to display a text report of the main classification indicators 12345678910111213from sklearn.metrics import classification_reporty_predict_log = log_reg.predict(X_test)print(classification_report(y_test,y_predict_log))&quot;&quot;&quot; precision recall f1-score support 0 0.93 0.78 0.85 50 1 0.78 0.93 0.84 41 accuracy 0.85 91 macro avg 0.85 0.85 0.85 91weighted avg 0.86 0.85 0.85 91&quot;&quot;&quot;","link":"/example_04/"},{"title":"Machine Learning Part-03","text":"Decision trees Machine learning basics - use decision trees to make predictions about coupons In order to get close to real life and applications, the processing of actual data sets is the main focus. From January 1, 2016 to June 30, 2016, real online and offline consumption behaviors are predicted to be used by users within 15 days after receiving coupons in July 2016. Note: In order to protect the privacy of users and businesses, all data is anonymized, and biased sampling and necessary filtering are used. Data set ccf_offline_stage1_train.csv (training data) Field Description User_id 用户ID Merchant_id 商户ID Coupon_id 优惠券ID：null表示无优惠券消费，此时Discount_rate和Date_received字段无意义 Discount_rate 优惠率：x 代表折扣率；x:y表示满x减y。单位是元 Distance user经常活动的地点离该merchant的最近门店距离是x*500米（如果是连锁店，则取最近的一家门店），x\\(\\in[0,10]\\)；null表示无此信息，0表示低于500米，10表示大于5公里； Date_received 领取优惠券日期 Date 消费日期：如果Date=null &amp; Coupon_id != null，该记录表示领取优惠券但没有使用，即负样本；如果Date!=null &amp; Coupon_id = null，则表示普通消费日期；如果Date!=null &amp; Coupon_id != null，则表示用优惠券消费日期，即正样本； 123456789101112131415161718192021222324252627282930313233343536373839404142# load pluginimport pandas as pdimport numpy as np# load datatrain_data = pd.read_csv('~/data/ccf_offline_stage1_train.csv')train_data.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 1754884 entries, 0 to 1754883Data columns (total 7 columns): # Column Dtype --- ------ ----- 0 User_id int64 1 Merchant_id int64 2 Coupon_id float64 3 Discount_rate object 4 Distance float64 5 Date_received float64 6 Date float64dtypes: float64(4), int64(2), object(1)memory usage: 93.7+ MB&quot;&quot;&quot;train_data.head()&quot;&quot;&quot; User_id Merchant_id Coupon_id Discount_rate Distance Date_received Date0 1439408 2632 NaN NaN 0.0 NaN 20160217.01 1439408 4663 11002.0 150:20 1.0 20160528.0 NaN2 1439408 2632 8591.0 20:1 0.0 20160217.0 NaN3 1439408 2632 1078.0 20:1 0.0 20160319.0 NaN4 1439408 2632 8591.0 20:1 0.0 20160613.0 NaN&quot;&quot;&quot;print(train_data.shape)data = train_data.dropna(how = 'any')print(train_data.shape)&quot;&quot;&quot;(1754884, 7)(1754884, 7)&quot;&quot;&quot; Discount_rate是object类型的，object在pandas中代表字符串，字符串类型不能输入模型中，所以需要改为数值类型 123456789101112print('Discount_rate 类型: \\n', data['Discount_rate'].unique())# [0,1] 表示折扣率# x:y 表示满 x 减 y&quot;&quot;&quot;Discount_rate 类型: ['20:1' '20:5' '30:5' '50:10' '10:5' '50:20' '100:10' '30:10' '50:5' '30:1' '100:30' '0.8' '200:30' '100:20' '10:1' '200:20' '0.95' '5:1' '100:5' '100:50' '50:1' '20:10' '150:10' '0.9' '200:50' '150:20' '150:50' '200:5' '300:30' '100:1' '200:10' '150:30' '0.85' '0.6' '0.5' '300:20' '200:100' '300:50' '150:5' '300:10' '0.75' '0.7' '30:20' '50:30']&quot;&quot;&quot; Convert Discount_rate into numerical features Discount type x:y 表示满 x 减 y 将 x:y 类型的字符串设为1 [0,1] 表示折扣率 将 [0,1] 类型的字符串设为 0 12345678910111213141516171819202122232425262728293031323334353637def getDiscountType(row): if ':' in row: return 1 else: return 0data['Discount_rate'] = data['Discount_rate'].apply(getDiscountType)&quot;&quot;&quot;See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy import sys&quot;&quot;&quot;data.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;Int64Index: 67165 entries, 6 to 1754880Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 User_id 67165 non-null int64 1 Merchant_id 67165 non-null int64 2 Coupon_id 67165 non-null float64 3 Discount_rate 67165 non-null int64 4 Distance 67165 non-null float64 5 Date_received 67165 non-null float64 6 Date 67165 non-null float64dtypes: float64(4), int64(3)memory usage: 4.1 MB&quot;&quot;&quot;# load plugin# Import DecisionTreeClassifier modelfrom sklearn.tree import DecisionTreeClassifier# Import train_test_split, used to divide the data set and test setfrom sklearn.model_selection import train_test_split# Import accuracy_score accuracy indexfrom sklearn.metrics import accuracy_score add label row to the dataset Labeling Label Label which samples are positive samples y=1 and which are negative samples y = -1 Forecast goal: the user's consumption within 15 days after receiving the coupon (Date-Date_received &lt;= 15) means to receive the coupon and use it within 15 days, that is, a positive sample, y = 1 (Date-Date_received&gt; 15) means that the coupon has not been used within 15 days, that is, a negative sample, y = 0 pandas tutorial on time https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html 1234567891011def label(row): if row['Date'] != 'null': td = pd.to_datetime(row['Date'], format = '%Y%m%d') - pd.to_datetime(row['Date_received'], format = '%Y%m%d') if td &lt;= pd.Timedelta(15, 'D'): return 1 return 0data['label'] = data.apply(label, axis = 1)&quot;&quot;&quot;See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy&quot;&quot;&quot; Statistics positive and negative samples 123456print(data['label'].value_counts())&quot;&quot;&quot;1 570600 10105Name: label, dtype: int64&quot;&quot;&quot; Divide the data set 80% training set 20% test set 80% train 20% test 1X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.2, random_state=3) Check the number and category distribution of training samples 12345678y_train.value_counts()&quot;&quot;&quot;2751537 966641735 86 ..4461556 1Name: User_id, Length: 34984, dtype: int64&quot;&quot;&quot; Check the number and type distribution of test samples 12345678y_test.value_counts()&quot;&quot;&quot;6641735 272751537 22 ..89464 1Name: User_id, Length: 11405, dtype: int64&quot;&quot;&quot; Initialize the classification decision tree model, the depth is 5 layers 1model = DecisionTreeClassifier(max_depth=6, random_state = 1) Model training 1model.fit(X_train, y_train) Model prediction 1y_pred = model.predict(X_test) Model evaluation 1234accuracy_score(y_test, y_pred)&quot;&quot;&quot;0.011315417256011316&quot;&quot;&quot; Change the standard of the model selection feature to entropy 1model = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=2) Model training 1model.fit(X_train, y_train) predict 1y_pred = model.predict(X_test) Evaluate 1accuracy_score(y_test, y_pred) In addition to the above key steps, you can explore the data by yourself, as well as any other forms of feature preprocessing methods and feature engineering processing. I hope to focus on understanding the development process of machine learning tasks. For the skills and methods of data processing, it is encouraged to invest more time to explore. iris 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifier, export_graphviz iris = load_iris()X = iris.data y = iris.targettree_clf = DecisionTreeClassifier()tree_clf.fit(X, y)export_graphviz( tree_clf, out_file=&quot;~/data/course_data/iris_tree.dot&quot;, feature_names=iris.feature_names, class_names=iris.target_names, rounded=True, filled=True)for line in open('~/data/course_data/iris_tree.dot'): print(line)&quot;&quot;&quot;digraph Tree {node [shape=box, style=&quot;filled, rounded&quot;, color=&quot;black&quot;, fontname=helvetica] ;edge [fontname=helvetica] ;0 [label=&quot;petal length (cm) &lt;= 2.45\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]\\nclass = setosa&quot;, fillcolor=&quot;#ffffff&quot;] ;1 [label=&quot;gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]\\nclass = setosa&quot;, fillcolor=&quot;#e58139&quot;] ;0 -&gt; 1 [labeldistance=2.5, labelangle=45, headlabel=&quot;True&quot;] ;2 [label=&quot;petal width (cm) &lt;= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]\\nclass = versicolor&quot;, fillcolor=&quot;#ffffff&quot;] ;0 -&gt; 2 [labeldistance=2.5, labelangle=-45, headlabel=&quot;False&quot;] ;3 [label=&quot;petal length (cm) &lt;= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]\\nclass = versicolor&quot;, fillcolor=&quot;#4de88e&quot;] ;2 -&gt; 3 ;4 [label=&quot;petal width (cm) &lt;= 1.65\\ngini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]\\nclass = versicolor&quot;, fillcolor=&quot;#3de684&quot;] ;3 -&gt; 4 ;5 [label=&quot;gini = 0.0\\nsamples = 47\\nvalue = [0, 47, 0]\\nclass = versicolor&quot;, fillcolor=&quot;#39e581&quot;] ;show more (open the raw output data in a text editor) ...16 [label=&quot;gini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]\\nclass = virginica&quot;, fillcolor=&quot;#8139e5&quot;] ;12 -&gt; 16 ;}&quot;&quot;&quot; Salient Features 1tree_clf.feature_importances_ Build Decision Tree: CART 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import pandas as pdmock_data = { 'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'], 'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'], 'family_number': [1, 1, 2, 1, 1, 1, 2], 'bought': [1, 1, 1, 0, 0, 0, 1],}dataset = pd.DataFrame.from_dict(mock_data)import numpy as npfrom collections import Counterdef entropy(elements): counter = Counter(elements) probabilities = [counter[e] / len(elements) for e in elements] return -sum(p * np.log10(p) for p in probabilities)def find_the_min_spilter(training_data: pd.DataFrame, target: str) -&gt; str: x_fields = set(training_data.columns.tolist()) - {target} spliter = None min_entropy = float('inf') for f in x_fields: elements = set(training_data[f]) for e in elements: sub_spliter_1 = training_data[dataset[f] == e][target].tolist() entropy_1 = entropy(sub_spliter_1) sub_spliter_2 = training_data[dataset[f] != e][target].tolist() entropy_2 = entropy(sub_spliter_2) entropy_v = entropy_1 + entropy_2 if entropy_v &lt; min_entropy: min_entropy = entropy_v spliter = (f, e) print('spliter is: {}'.format(spliter)) print('the min entropy is: {}'.format(min_entropy)) return spliterfind_the_min_spilter(dataset, 'bought')&quot;&quot;&quot;spliter is: ('income', '+10')the min entropy is: 0.7176797562470717('income', '+10')&quot;&quot;&quot;dataset[dataset['income'] == '-10']&quot;&quot;&quot; gender income family_number bought1 F -10 1 16 M -10 2 1&quot;&quot;&quot;dataset[dataset['income'] != '-10']&quot;&quot;&quot; gender income family_number bought0 F +10 1 12 F +10 2 13 F +10 1 04 M +10 1 05 M +10 1 0&quot;&quot;&quot;","link":"/example_05/"},{"title":"Machine Learning Part-04","text":"SVM 12345678910111213import numpy as nplabel_a = np.random.normal(6, 2, size = (50,2))label_b = np.random.normal(-6, 2, size = (50,2))import matplotlib.pyplot as plta = [1, 2, 3]b = [-1,-2, -3]plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))plt.show() 12345678910111213141516171819202122232425262728293031323334353637label_a_x = label_a[:, 0]label_b_x = label_b[:, 0]def f(x, k, b): return k*x -b k_and_b = []for i in range(100): k, b = (np.random.random(size = (1,2)) * 10 - 5)[0] if np.max(f(label_a_x, k, b)) &lt;= -1 and np.min(f(label_b_x, k, b)) &gt;= 1: print(k, b) k_and_b.append((k, b))&quot;&quot;&quot;-3.4732670434285517 -2.3248316389039325-3.654276254462583 0.01110189858052646-2.4609031871010014 -0.3932180655739925-2.9206497777762843 0.2595456609552631-4.07589152330003 -0.6463313059119606-3.1950366475236835 -1.8558958669742989-4.316670785852706 -3.1033030808371653-4.124339773909792 -1.5741734685470687-4.20817621470405 0.4368323022696625-3.7098120657624003 -0.38196175566618784-3.2053446683533315 0.12822700803583054-4.534694169094692 1.143734501297419-4.8124714209376425 0.8707258703100704&quot;&quot;&quot;plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))for k, b in k_and_b: x = np.concatenate((label_a_x, label_b_x)) plt.plot(x, f(x, k, b))plt.show() 1234567plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))k,b = sorted(k_and_b, key = lambda t: abs(t[0]))[0]x = np.concatenate((label_a_x, label_b_x))plt.plot(x, f(x, k, b))plt.show() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from sklearn.datasets import load_bostondatasets = load_boston()data, target = datasets['data'], datasets['target']import pandas as pddf = pd.DataFrame(data)df.columns = datasets['feature_names']import randomdef random_select(df, drop_num = 4): columns = random.sample(list(df.columns), k = len(df.columns) - drop_num) return df[columns] from sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeRegressorsample_x = random_select(df)regressioner = DecisionTreeRegressor()(X_train, X_test, y_train, y_test) = train_test_split(sample_x, target, test_size = 0.3)regressioner.fit(X_train, y_train)regressioner.score(X_train, y_train)&quot;&quot;&quot;1.0&quot;&quot;&quot;regressioner.score(X_test, y_test)&quot;&quot;&quot;0.8110635350395325&quot;&quot;&quot;def random_tree(train_X, train_y, test_X, test_y, drop_n = 4): train_sample = random_select(train_X, drop_num = drop_n) regressioner = DecisionTreeRegressor() regressioner.fit(train_sample, train_y) train_score = regressioner.score(train_sample, train_y) test_score = regressioner.score(test_X[train_sample.columns], test_y) print('train score = {}; test score = {}'.format(train_score, test_score)) y_predicat = regressioner.predict(test_X[train_sample.columns]) return y_predicat def random_forest(train_X, train_y, test_X, test_y, tree_n = 4): predicat = np.array([random_tree(train_X, train_y, test_X, test_y) for _ in range(tree_n)]) return np.mean(predicat, axis = 0) (X_train, X_test, y_train, y_test) = train_test_split(df, target, test_size = 0.3)forest_predict = random_forest(X_train, y_train, X_test, y_test)&quot;&quot;&quot;train score = 1.0; test score = 0.5367061884031776train score = 1.0; test score = 0.4983695562874999train score = 1.0; test score = 0.6715869370883646train score = 1.0; test score = 0.6210922529610217&quot;&quot;&quot;forest_predict&quot;&quot;&quot;array([10.925, 21.1 , 30.625, 28.025, 22.525, 17.65 , 20.6 , 17.325, 29.175, 14.95 , 40.775, 19.55 , 12.175, 23.675, 10.775, 22.1 , ... 15.575, 20.5 , 22.775, 30.725, 18.975, 16.45 , 22.05 , 18.925])&quot;&quot;&quot;from sklearn.metrics import r2_scorer2_score(y_test, forest_predict)&quot;&quot;&quot;0.7840500839091215&quot;&quot;&quot; Entropy: 熵 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npfrom collections import Counterfrom icecream import icfrom functools import lru_cachedef pr(es): counter = Counter(es) def _wrap(e): return counter[e] / len(es) return _wrapdef entropy(elements): # Information Entropy p = pr(elements) return -np.sum(p(e) * np.log(p(e)) for e in set(elements))def gini(elements): p = pr(elements) return 1-np.sum(p(e) ** 2 for e in set(elements)) pure_func = giniic(pure_func([1, 1, 1, 1, 1, 0]))ic(pure_func([1, 1, 1, 1, 1, 1]))ic(pure_func([1, 2, 3, 4, 5, 8]))ic(pure_func([1, 2, 3, 4, 5, 9]))ic(pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'c']))ic(pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'd']))&quot;&quot;&quot;ic| pure_func([1, 1, 1, 1, 1, 0]): 0.2777777777777777ic| pure_func([1, 1, 1, 1, 1, 1]): 0.0ic| pure_func([1, 2, 3, 4, 5, 8]): 0.8333333333333333ic| pure_func([1, 2, 3, 4, 5, 9]): 0.8333333333333333ic| pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'c']): 0.44897959183673464ic| pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'd']): 0.61224489795918370.6122448979591837&quot;&quot;&quot; Random forest 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879from sklearn.datasets import load_bostonfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitimport numpy as npimport pandas as pdfrom sklearn.metrics import r2_scorehouse = load_boston()X = house.datay = house.targetx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)tree_reg = DecisionTreeRegressor()tree_reg.fit(x_train, y_train)print('whole dataset train acc: {}'.format(tree_reg.score(x_train, y_train)))print('whole dataset test acc: {}'.format(tree_reg.score(x_test, y_test)))&quot;&quot;&quot;whole dataset train acc: 1.0whole dataset test acc: 0.6776520888466615&quot;&quot;&quot;def random_forest(train_x, train_y, test_x, test_y, drop_n=4): random_features = np.random.choice(list(train_x.columns), size=len(train_x.columns)-drop_n) sample_x = train_x[random_features] sample_y = train_y reg = DecisionTreeRegressor() reg.fit(sample_x, sample_y) train_score = reg.score(sample_x, sample_y) test_score = reg.score(test_x[random_features], test_y) print('sub sample :: train score: {}, test score: {}'.format(train_score, test_score)) y_predicated = reg.predict(test_x[random_features]) return y_predicated, test_score with_feature_names = pd.DataFrame(X)with_feature_names.columns = house['feature_names']x_train, x_test, y_train, y_test = train_test_split(with_feature_names, y, test_size=0.3, random_state=0)tree_num = 4predicates = []for _ in range(tree_num): predicated, score = random_forest(x_train, y_train, x_test, y_test) predicates.append((predicated, score))&quot;&quot;&quot;sub sample :: train score: 1.0, test score: 0.5640870175410873sub sample :: train score: 1.0, test score: 0.29024437819534354sub sample :: train score: 1.0, test score: 0.37812117132843814sub sample :: train score: 1.0, test score: 0.5650888856735524&quot;&quot;&quot;predicates_value = [v for v, s in predicates]forest_scores = [s for v, s in predicates]print('the score of forest is : {}'.format(r2_score(y_test, np.mean(predicates_value, axis=0))))&quot;&quot;&quot;the score of forest is : 0.680193104551715&quot;&quot;&quot;weights = np.array(forest_scores) / np.sum(forest_scores)weights_score = np.zeros_like(np.mean(predicates_value, axis=0))for i, v in enumerate(predicates_value): weights_score += v * weights[i] print('the score of weighted forest is : {}'.format(r2_score(y_test, weights_score)))&quot;&quot;&quot;the score of weighted forest is : 0.6956613076019385&quot;&quot;&quot; Show SVM 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npimport matplotlib.pyplot as pltlabel_a = np.random.normal(6, 2, size=(50, 2))label_b = np.random.normal(-6, 2, size=(50, 2))plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))label_a_x = label_a[:, 0]label_b_x = label_b[:, 0]def f(x, w, b): return w * x + b k_and_b = []for i in range(100): k, b = (np.random.random(size=(1, 2)) * 10 - 5)[0] if np.max(f(label_a_x, k, b)) &gt;= -1 and np.min(f(label_b_x, k, b)) &gt;= 1: print(k, b) k_and_b.append((k, b))&quot;&quot;&quot;0.17732109082579406 3.9508645615428843-0.8649868307954458 1.7349996177756957...-2.2969567032985783 2.171321001904926&quot;&quot;&quot;for k, b in k_and_b: x = np.concatenate((label_a_x, label_b_x)) plt.plot(x, f(x, k, b)) print(k_and_b)&quot;&quot;&quot;[(0.17732109082579406, 3.9508645615428843), (-0.8649868307954458, 1.7349996177756957), (-0.818317924604357, 0.352843348193578), (-0.19730603224472976, 4.002168852007262), ...(-2.2969567032985783, 2.171321001904926)]&quot;&quot;&quot;w, b = min(k_and_b, key=lambda k_b: k_b[0])all_x = np.concatenate((label_a_x, label_b_x))plt.plot(all_x, f(all_x, w, b), 'r-o')plt.show() Integrated learning Ensemble learning is a machine learning paradigm that solves the same problem by training multiple models. In contrast to ordinary machine learning methods that try to learn a hypothesis from training data, ensemble methods try to construct a set of hypotheses and use them in combination. Next, we will use the decision tree and its integrated version to model the classic data set Mnist and observe the differences in different integration methods. 123456789!ls!unzip mnist_test.csv.zip &amp;&amp; unzip mnist_train.csv.zipimport numpy as npimport pandas as pdfrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier Build a data set The Mnist data set used this time is not in the original format. In order to more easily adapt to this training, the 28 * 28 pictures in the original data set are flatten operation, it becomes 784 features, the columns in the DataFrame below: 1x1, 1x2, ..., 28x28, representing the i row and j column in the picture The pixel value of is a grayscale image, so the pixel value is only 0 and 1 1234567891011train_df = df = pd.read_csv('~/data/mnist_train.csv')train_df.head()&quot;&quot;&quot; label 1x1 1x2 1x3 1x4 1x5 1x6 1x7 1x8 1x9 ... 28x19 28x20 28x21 28x22 28x23 28x24 28x25 28x26 28x27 28x280 5 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 01 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 02 4 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 03 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 04 9 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 05 rows × 785 columns&quot;&quot;&quot; View training data information:, whether there is NaN, how many pieces of data are there... 1234567891011121314151617181920212223242526272829train_df.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 60000 entries, 0 to 59999Columns: 785 entries, label to 28x28dtypes: int64(785)memory usage: 359.3 MB&quot;&quot;&quot;test_df = df = pd.read_csv('~/data/mnist_test.csv')test_df.head()&quot;&quot;&quot; label 1x1 1x2 1x3 1x4 1x5 1x6 1x7 1x8 1x9 ... 28x19 28x20 28x21 28x22 28x23 28x24 28x25 28x26 28x27 28x280 7 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 01 2 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 02 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 03 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 04 4 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 05 rows × 785 columns&quot;&quot;&quot;test_df.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 10000 entries, 0 to 9999Columns: 785 entries, label to 28x28dtypes: int64(785)memory usage: 59.9 MB&quot;&quot;&quot; Build training and test data 12345678910X_train = train_df.iloc[:, 1:]y_train = train_df.iloc[:, 0]X_test = test_df.iloc[:, 1:]y_test = test_df.iloc[:, 0](X_train.shape, y_train.shape), (X_test.shape, y_test.shape)&quot;&quot;&quot;(((60000, 784), (60000,)), ((10000, 784), (10000,)))&quot;&quot;&quot; Decision Tree First train a simple decision tree to see how it performs 1234567891011121314151617181920dtc = DecisionTreeClassifier()dtc.fit(X_train, y_train)dtc.score(X_train, y_train)&quot;&quot;&quot;1.0&quot;&quot;&quot;dtc.score(X_test, y_test)&quot;&quot;&quot;0.8753&quot;&quot;&quot;dtc = DecisionTreeClassifier(min_samples_leaf=8)dtc.fit(X_train, y_train)dtc.score(X_train, y_train), dtc.score(X_test, y_test)&quot;&quot;&quot;(0.9311666666666667, 0.8795)&quot;&quot;&quot; From the above results, we can see that by adjusting the parameter min_samples_leaf, the overfitting situation has been alleviated. What does this parameter mean? Why increasing it can alleviate the overfitting problem? The meaning of min_samples_leaf is the minimum number of samples contained in the leaf nodes of the decision tree. By increasing this parameter, the decision tree can not capture any of the subtle features of the training data during training, resulting in excessive training data. Fitting: The large number of samples of leaf nodes can also play a role in voting and enhance the generalization performance of the model. You can try to continue to increase the value of this parameter and try to find the best parameter. In addition to this parameter, you can also try to adjust the parameters such as min_samples_split and max_features. For the specific meaning, please refer to sklearn documentation Second question: Try to adjust other parameters to see the performance of the decision tree on the test set Random Forest Take a look at the bagging version of the decision tree and how the random forest performs! 1234567rfc = RandomForestClassifier(n_estimators = 10)rfc.fit(X_train, y_train)rfc.score(X_train, y_train), rfc.score(X_test, y_test)&quot;&quot;&quot;(0.99905, 0.9513)&quot;&quot;&quot; It is worthy of the integrated version. It basically achieves better performance under the default parameters. The accuracy of the test set is about 7% higher than that of the ordinary decision tree. However, comparing the training and test results, it can be found that there is still a certain degree of overfitting. , Try to adjust some parameters below 1234567rfc = RandomForestClassifier(n_estimators = 20)rfc.fit(X_train, y_train)rfc.score(X_train, y_train), rfc.score(X_test, y_test)&quot;&quot;&quot;(0.9999, 0.96)&quot;&quot;&quot; After increasing the parameter n_estimators, the accuracy of the test set has increased by about 1%. The meaning of this parameter is to train 20 decision trees at the same time, and finally integrate the results. The increase of this parameter can be simply regarded as voting The number of people increases, so the final result will inevitably be more robust. You can try to continue to increase this parameter, or adjust other parameters such as max_samples, appropriately less than the total amount of training data, which can increase the difference between different sub-models and further improve the generalization performance. It can also adjust the parameters of the base learner (decision tree). For the meaning of the parameters, see sklearn documentation GBDT Let's compare the performance of the boosting version of the decision tree GBDT! 1234567gbc = GradientBoostingClassifier(n_estimators=10)gbc.fit(X_train, y_train)gbc.score(X_train, y_train), gbc.score(X_test, y_test)&quot;&quot;&quot;(0.8423, 0.846)&quot;&quot;&quot; As expected, the performance has been greatly improved, and the indicators of the training set are basically the same as those of the test set, and there is no overfitting, so it should be possible to continue to try to improve this parameter. Generally, in the absence of overfitting, we only need to consider continuing to increase the complexity of the model. This is the fastest way to improve performance. When the complexity of the model increases to the point of over-fitting, we then consider using some methods to reduce over-fitting. Bagging The aforementioned random forest and GBDT are ensemble learning algorithms based on decision trees, but it should be noted that ensemble learning is not exclusive to decision trees. Any other learner can be used as a base learner for ensemble learning, such as Logistic regression, support vector machine. Bagging is short for \"bootstrap aggregating\". This is a meta-algorithm, which takes M sub-samples (with replacement) from the initial data set, and trains the prediction model on these sub-samples. The final model is obtained by averaging all sub-models, which usually produces better results. The main advantage of this technique is that it combines regularization, all you need to do is choose good parameters for the base learner. The following uses the general api provided by sklearn to construct an integrated learning algorithm 12345678# Still use decision tree as base learnerbgc = BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=1.0, n_estimators=20)bgc.fit(X_train, y_train)bgc.score(X_train, y_train), bgc.score(X_test, y_test)&quot;&quot;&quot;(0.9935166666666667, 0.9506)&quot;&quot;&quot; Third question Logistic regression as a base learner 1234567bgc = BaggingClassifier(LogisticRegression(max_iter = 500), max_samples=0.5, max_features=1.0, n_estimators=20)bgc.fit(X_train, y_train)bgc.score(X_train, y_train), bgc.score(X_test, y_test)&quot;&quot;&quot;(0.9421166666666667, 0.9228)&quot;&quot;&quot; Above we have successfully used logistic regression as the base learner to complete integrated learning. You can try to use only logistic regression for training, and compare the performance of the single model with the bagging version of logistic regression. Boosting Boosting refers to a series of algorithms that can transform a weak learner into a strong learner. The main principle of boosting is to combine a series of weak learners (only better than random guessing). For those samples that were misclassified in the early stages of training, the boosting algorithm will give more attention. Then combine the predictions by weighted majority voting (classification) or weighted sum (regression) to produce the final prediction. 123456abc = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=10, learning_rate=0.01)abc.fit(X_train, y_train)abc.score(X_train, y_train), abc.score(X_test, y_test)&quot;&quot;&quot;(1.0, 0.875)&quot;&quot;&quot; Comparing the boosting integrated version of decision tree and logistic regression, we can find that logistic regression has better generalization ability, and decision tree is easier to overfit 123456abc = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=8), n_estimators=10, learning_rate=0.01)abc.fit(X_train, y_train)abc.score(X_train, y_train), abc.score(X_test, y_test)&quot;&quot;&quot;(0.9981833333333333, 0.9532)&quot;&quot;&quot; In fact, over-fitting is not a bad thing. If your model cannot be over-fitted, it means that it cannot fit the training data well. Therefore, the decision tree is very over-fitted at the beginning, which also shows its potential. , You can see that after the above parameters are adjusted, the boosting version of the decision tree easily exceeds the boosting version of the logistic regression","link":"/example_06/"},{"title":"RNN","text":"Simple RNN Define function Import the required libraries 12345678import ioimport osimport unicodedataimport stringimport globimport torchimport random 123# alphabet small + capital letters + &quot;.,;'&quot;ALL_LETTERS = string.ascii_letters + &quot;.,;'&quot;N_LETTERS = len(ALL_LETTERS) Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427 123456def unicode_to_ascii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in ALL_LETTERS ) 123456789101112131415161718192021def load_data(): # Build the category_lines dictionary, a list of names per language category_lines = {} all_categories = [] def find_files(path): return glob.glob(path) # Read a file and split into lines def read_lines(filename): lines = io.open(filename, encoding = 'utf-8').read().strip().split('\\n') return [unicode_to_ascii(line) for line in lines] for filename in find_files('~/data/course_data/names/*.txt'): category = os.path.splitext(os.path.basename(filename))[0] all_categories.append(category) lines = read_lines(filename) category_lines[category] = lines return category_lines, all_categories To represent a single letter, we use a “one-hot vector” of size &lt;1 x n_letters&gt;. A one-hot vector is filled with 0s except for a 1 at index of the current letter, e.g. \"b\" = &lt;0 1 0 0 0 ...&gt;. To make a word we join a bunch of those into a 2D matrix &lt;line_length x 1 x n_letters&gt;. That extra 1 dimension is because PyTorch assumes everything is in batches - we’re just using a batch size of 1 here. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# Find letter index from all_letters, e.g. &quot;a&quot; = 0def letter_to_index(letter): return ALL_LETTERS.find(letter) # Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensordef letter_to_tensor(letter): tensor = torch.zeros(1, N_LETTERS) tensor[0][letter_to_index(letter)] = 1 return tensor # Turn a line into a &lt;line_length x 1 x n_letters&gt;,# or an array of one-hot letter vectorsdef line_to_tensor(line): tensor = torch.zeros(len(line), 1, N_LETTERS) for i, letter in enumerate(line): tensor[i][0][letter_to_index(letter)] = 1 return tensor def random_training_example(category_lines, all_categories): def random_choice(a): random_idx = random.randint(0, len(a) - 1) return a[random_idx] category = random_choice(all_categories) line = random_choice(category_lines[category]) category_tensor = torch.tensor([all_categories.index(category)], dtype = torch.long) line_tensor = line_to_tensor(line) return category, line, category_tensor, line_tensor if __name__ == '__main__': print(ALL_LETTERS) print(unicode_to_ascii('Ślusàrski')) category_lines, all_categories = load_data() print(category_lines['Italian'][:5]) print(letter_to_tensor('J')) # [1, 57] print(line_to_tensor('Jones').size()) # [5, 1, 57] &quot;&quot;&quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,;'Slusarski['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])torch.Size([5, 1, 56])&quot;&quot;&quot; Second Example 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# Import the required librariesimport torchimport torch.nn as nnimport matplotlib.pyplot as pltclass RNN(nn.Module): # implement RNN from scratch rather than using nn.RNN def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(input_size + hidden_size, hidden_size) self.i2o = nn.Linear(input_size + hidden_size, output_size) self.softmax = nn.LogSoftmax(dim = 1) def forward(self, input_tensor, hidden_tensor): combined = torch.cat((input_tensor, hidden_tensor), 1) hidden = self.i2h(combined) output = self.i2o(combined) output = self.softmax(output) return output, hidden def init_hidden(self): return torch.zeros(1, self.hidden_size)category_lines, all_categories = load_data()n_categories = len(all_categories)n_hidden = 128rnn = RNN(N_LETTERS, n_hidden, n_categories)# one stepinput_tensor = letter_to_tensor('A')hidden_tensor = rnn.init_hidden()output, next_hidden = rnn(input_tensor, hidden_tensor)print(output.size())print(next_hidden.size())&quot;&quot;&quot;torch.Size([1, 18])torch.Size([1, 128])&quot;&quot;&quot;# whole sequence/nameinput_tensor = line_to_tensor('Albert')hidden_tensor = rnn.init_hidden()output, next_hidden = rnn(input_tensor[0], hidden_tensor)print(output.size())print(next_hidden.size())&quot;&quot;&quot;torch.Size([1, 18])torch.Size([1, 128])&quot;&quot;&quot;def category_from_output(output): category_idx = torch.argmax(output).item() return all_categories[category_idx] print(category_from_output(output))&quot;&quot;&quot;German&quot;&quot;&quot;criterion = nn.NLLLoss()learning_rate = 0.005optimizer = torch.optim.SGD(rnn.parameters(), lr = learning_rate)def train(line_to_tensor, category_tensor): hidden = rnn.init_hidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_to_tensor[i], hidden) loss = criterion(output, category_tensor) optimizer.zero_grad() loss.backward() optimizer.step() return output, loss.item() current_loss = 0all_losses = []plot_steps, print_steps = 1000, 5000n_iters = 100000for i in range(n_iters): category, line, category_tensor, line_tensor = random_training_example(category_lines, all_categories) output, loss = train(line_tensor, category_tensor) current_loss += loss if (i + 1) % plot_steps == 0: all_losses.append(current_loss / plot_steps) current_loss = 0 if (i + 1) % print_steps == 0: guess = category_from_output(output) corrent = 'CORRECT' if guess == category else f'WRONG ({category})' print(f'{i+1} {(i+1) / n_iters *100} {loss:.4f} {line} / {guess} {corrent}') &quot;&quot;&quot;5000 5.0 2.5063 Bureau / Scottish WRONG (French)10000 10.0 1.4726 Bitar / Arabic CORRECT15000 15.0 1.9405 Bazilevitch / Russian CORRECT20000 20.0 1.5565 Dupont / French CORRECT25000 25.0 0.1202 Majewski / Polish CORRECT30000 30.0 1.1579 Kucharova / Czech CORRECT35000 35.0 1.0075 Sheng / Chinese CORRECT40000 40.0 0.8343 Masih / Arabic CORRECT45000 45.0 0.5371 Fan / Chinese CORRECT50000 50.0 0.3260 Vinh / Vietnamese CORRECT55000 55.00000000000001 2.5464 Pahlke / Polish WRONG (German)60000 60.0 1.5921 Clark / Scottish CORRECT65000 65.0 4.3648 Paulis / Greek WRONG (Dutch)70000 70.0 1.3289 Thian / Vietnamese WRONG (Chinese)75000 75.0 2.2715 Kelly / English WRONG (Irish)80000 80.0 1.0069 Siu / Korean WRONG (Chinese)85000 85.0 0.8168 Kan / Chinese CORRECT90000 90.0 0.2283 Dinh / Vietnamese CORRECT95000 95.0 2.0048 Abbascia / Japanese WRONG (Italian)100000 100.0 0.6310 O'Shea / Irish CORRECT&quot;&quot;&quot;plt.figure()plt.plot(all_losses)plt.show() 12345678910111213141516171819202122232425262728293031def predict(input_line): print(f'\\n &gt; {input_line}') with torch.no_grad(): line_tensor = line_to_tensor(input_line) hidden = rnn.init_hidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) guess = category_from_output(output) print(guess) while True: sentence = input('Input: ') if sentence == 'quit': break predict(sentence)&quot;&quot;&quot; &gt; ChineseIrish &gt; EnglishEnglish &gt; JapaneseFrench &gt; FrenchGerman&quot;&quot;&quot; LSTM Modeling trigonometric functions Use LSTM to fit sine and cosine functions Use numpy to build time series data based on sine function Use keras to build a simple regression network, mainly using the LSTM network structure to fit the periodicity of the sine function, and visualize the fitted sine function image and the real function image Related knowledge points Time series data construction and forecasting Time series model building, training, evaluation and visualization based on keras LSTM 123456789101112131415161718# Import necessary libraries# Build dataimport numpy as np# Build a modelfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Inputfrom tensorflow.keras.layers import LSTMfrom tensorflow.keras.layers import Dense# Printing progress barfrom tqdm import tqdm# Visualizationimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline 1. Construct a data set This module will use numpy to construct time series data. There are two main steps: Define the sine function (cosine function) Select historical data window size to construct time series data 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def ground_func(x): &quot;&quot;&quot; sine / cosine function Args: x: numpy.ndarray return: sin(x) or cos(x) &quot;&quot;&quot; y = np.sin(x) return ydef build_data(sequence_data, n_steps): &quot;&quot;&quot; Use sine function data to build X, y Args: sine_data: numpy.ndarray n_steps: history data window size return: X: numpy.ndarray, y: numpy.ndarray &quot;&quot;&quot; # init X, y = [], [] seq_len = len(sequence_data) for start_idx in tqdm(range(seq_len), total=seq_len): end_idx = start_idx + n_steps if end_idx &gt;= seq_len: break cur_x = sequence_data[start_idx: end_idx] cur_y = sequence_data[end_idx] X.append(cur_x) y.append(cur_y) X = np.array(X) y = np.array(y) X = X.reshape(*X.shape, 1) return X, y # Construct the original sine/cosine function sequencexaxis = np.arange(-50 * np.pi, 50 * np.pi, 0.1)sequence_data = ground_func(xaxis)len(sequence_data)# Take 1000 data for visualizationplt.figure(figsize = (20, 8))plt.plot(xaxis[:1000], sequence_data[:1000]) 1234567n_steps = 20X, y = build_data(sequence_data, n_steps)X.shape, y.shape&quot;&quot;&quot; 99%|█████████▉| 3122/3142 [00:00&lt;00:00, 1557955.63it/s]((3122, 20, 1), (3122,))&quot;&quot;&quot; 2. Build the model This module builds a timing model based on the LSTM and Dense layer in keras. The following points need to be noted: 1. Choose the right hidden size 2. Choose a suitable activation function, such as relu, tanh 3. The optimizer chooses sgd, adam, etc. 3. The loss function chooses cross entropy loss function (cross_entropy) or mean square error (mse), etc. 123456789101112131415161718192021222324252627282930313233343536def create_model(): &quot;&quot;&quot; Build a LSTM model fit sine/cosine function. hints: 1. a LSTM fit time pattern (ref: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) 2. a Dense for regression (ref: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) &quot;&quot;&quot; model = Sequential() model.add(Input(shape = (20, 1))) model.add(LSTM(32, activation='tanh')) model.add(Dense(1, activation='tanh')) model.compile(optimizer = 'adam', loss = 'mse') return model# Initialize the model and print related informationmodel = create_model()model.summary()&quot;&quot;&quot;Instructions for updating:Call initializer instance with the dtype argument instead of passing it to the constructorModel: &quot;sequential&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================lstm (LSTM) (None, 32) 4352 _________________________________________________________________dense (Dense) (None, 1) 33 =================================================================Total params: 4,385Trainable params: 4,385Non-trainable params: 0_________________________________________________________________&quot;&quot;&quot; 3. Model training 12345678910111213141516# Try to change epochs and add callbacks, such as EarlyStopping (https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)history = model.fit(X, y, batch_size = 32, epochs = 25, verbose = 1)plt.plot(history.history['loss'], label='loss')plt.legend(loc ='upper right') # draw the loss image&quot;&quot;&quot;Instructions for updating:Use tf.where in 2.0, which has the same broadcast rule as np.whereEpoch 1/253122/3122 [==============================] - 4s 1ms/sample - loss: 0.1433Epoch 2/253122/3122 [==============================] - 3s 879us/sample - loss: 0.0072show more (open the raw output data in a text editor) ...Epoch 25/253122/3122 [==============================] - 3s 858us/sample - loss: 2.2191e-05&quot;&quot;&quot; 4. Forecast This module uses a function different from the training data to construct test data to verify the generalization performance of the model. The main steps are as follows: 1. Define a new function (sine/cosine) 2. Use the trained model to make predictions 3. Visually compare model prediction results with real values 123456789101112131415161718192021222324252627282930313233343536def test_func(x): &quot;&quot;&quot; sine/cosine function, different from ground_func above. Args: x: numpy.ndarray return: sin(x) or cos(x) &quot;&quot;&quot; y = np.cos(x) return y test_xaxis = np.arange(0, 10 * np.pi, 0.1)test_sequence_data = test_func(test_xaxis)# Use the initial n_steps of historical data to start forecasting, and the subsequent data will use the predicted data as historical data for further forecastingy_preds = test_sequence_data[:n_steps]# Step by step forecastfor i in tqdm(range(len(test_xaxis)-n_steps)): model_input = y_preds[i: i+n_steps] model_input = model_input.reshape((1, n_steps, 1)) y_pred = model.predict(model_input, verbose = 0) y_pred = np.append(y_preds, y_pred)plt.figure(figsize = (10,8))plt.plot(test_xaxis[n_steps:], y_preds[n_steps:], label ='predictions')plt.plot(test_xaxis, test_sequence_data, label ='ground truth')plt.plot(test_xaxis[:n_steps], y_preds[:n_steps], label ='initial sequence', color ='red')plt.legend(loc ='upper left')plt.ylim(-2,2)plt.show()&quot;&quot;&quot;100%|██████████| 295/295 [00:01&lt;00:00, 183.91it/s]&quot;&quot;&quot; Recurrent Neural Networks source 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import pandas as pd# load datatimeserise_revenue = pd.read_csv('~/data/course_data/time_serise_revenue.csv')sales_data = pd.read_csv('~/data/course_data/time_serise_sale.csv')timeserise_revenue.head()&quot;&quot;&quot; Unnamed: 0 day_1 day_2 day_3 day_4 day_5 day_6 day_7 day_8 day_9 ... day_51 day_52 day_53 day_54 day_55 day_56 day_57 day_58 day_59 day_600 0 2.622866 2.657832 2.771121 2.815845 2.876267 2.859229 2.844758 2.793797 2.736443 ... 1.228701 1.290414 1.474886 1.563295 1.736197 1.797285 1.978940 2.198979 2.277908 2.403300...4 4 1.702631 1.825995 2.038047 2.194083 2.313903 2.417883 2.567613 2.650782 2.729691 ... 1.258760 1.137150 1.109007 1.104999 1.150137 1.204513 1.221350 1.327023 1.387304 1.5573635 rows × 61 columns&quot;&quot;&quot;def sample_from_table(sample_size, dataframe): sample_row = dataframe.sample().values[0] begin_column = random.randint(0, len(sample_row) - sample_size - 1) return (sample_row[begin_column: begin_column + sample_size], sample_row[begin_column + 1: begin_column + sample_size + 1]) import torchimport torch.nn as nnfrom torch.nn import functional as Ffrom torch.autograd import Variablefrom torch import optimimport numpy as npimport math, randomimport matplotlib.pyplot as pltimport seaborn as sns# Generating a noisy multi-sin waveclass FullyConnected(nn.Module): def __init__(self, x_size, hidden_size, output_size): super(FullyConnected, self).__init__() self.hidden_size = hidden_size self.linear_with_tanh = nn.Sequential( nn.Linear(10, self.hidden_size), nn.Tanh(), nn.Linear(self.hidden_size, self.hidden_size), nn.Tanh(), nn.Linear(self.hidden_size, output_size) ) def forward(self, x): yhat = self.linear_with_tanh(x) return yhat class SimpleRNN(nn.Module): def __init__(self, x_size, hidden_size, n_layers, batch_size, output_size): super(SimpleRNN, self).__init__() self.hidden_size = hidden_size self.n_layers = n_layers self.batch_size = batch_size # self.inp = nn.Linear(1, hidden_size) self.rnn = nn.RNN(x_size, hidden_size, n_layers, batch_first=True) self.out = nn.Linear(hidden_size, output_size) # 10 in and 10 out def forward(self, inputs, hidden=None): hidden = self.__init__hidden() # print('Forward hidden {}'.format(hidden.shape)) # print('Forward inps {}'.format(inputs.shape)) output, hidden = self.rnn(inputs.float(), hidden.float()) # print('Out1 {}'.format(output.shape)) output = self.out(output.float()) # print('Forward outputs {}'.format(output.shape)) return output, hidden def __init__hidden(self): hidden = torch.zeros(self.n_layers, self.batch_size, self.hidden_size, dtype = torch.float64) return hidden # Set datasetsource_data = sales_data# Fully Connected Modeln_epochs = 100n_iters= 50hidden_size = 2 # try to change this parametersn_layers = 2batch_size = 5seq_length = 10n_sample_size = 50x_size = 1fc_model = FullyConnected(x_size, hidden_size, output_size = seq_length)fc_model = fc_model.double()criterion = nn.MSELoss()optimizer = optim.SGD(fc_model.parameters(), lr = 0.01)losses = np.zeros(n_epochs)plt.imshow(fc_model.state_dict()['linear_with_tanh.0.weight'])plt.show() 1234567891011121314151617181920212223242526272829303132333435for epoch in range(n_epochs): for iter_ in range(n_iters): _inputs, _targets = sample_from_table(n_sample_size, source_data) inputs = Variable(torch.from_numpy(np.array([_inputs[0:10], _inputs[10:20], _inputs[20:30], _inputs[30:40], _inputs[40:50]], dtype = np.double))) targets = Variable(torch.from_numpy(np.array([_targets[0:10], _targets[10:20], _targets[20:30], _targets[30:40], _targets[40:50]], dtype = np.double))) outputs = fc_model(inputs.double()) optimizer.zero_grad() loss = criterion(outputs, targets) loss.backward() optimizer.step() losses[epoch] += loss if iter_ % 10 == 0: plt.clf() plt.ion() plt.title('Epoch {}, iter {}'.format(epoch, iter_)) plt.plot(torch.flatten(outputs.detach()), 'r-', linewidth = 1, label = 'Output') plt.plot(torch.flatten(targets), 'c-', linewidth = 1, label = 'Label') plt.plot(torch.flatten(inputs), 'g-', linewidth = 1, label = 'Input') plt.draw() plt.pause(0.05) A total of 5 * 99 pictures were rendered in the middle, so I won’t show them one by one. RNN Model 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263n_epochs = 100n_iters = 50hidden_size = 2 # try to change this parametersn_layers = 2batch_size = 5seq_length = 10n_sample_size = 50x_size = 1output_size = 1rnn_model = SimpleRNN(x_size, hidden_size, n_layers, int(n_sample_size / seq_length), output_size)criterion = nn.MSELoss()optimizer = optim.SGD(rnn_model.parameters(), lr = 0.01)losses = np.zeros(n_epochs)for epoch in range(n_epochs): for iter in range(n_iters): _inputs, _targets = sample_from_table(n_sample_size, source_data) inputs = Variable(torch.from_numpy(np.array([_inputs[0:10], _inputs[10:20], _inputs[20:30], _inputs[30:40], _inputs[40:50]], dtype = np.double)).unsqueeze(2)) targets = Variable(torch.from_numpy(np.array([_targets[0:10], _targets[10:20], _targets[20:30], _targets[30:40], _targets[40:50]], dtype = np.double)).unsqueeze(2).float()) # [49] # print('Inputs {}, targets {}'.format(inputs.shape, targets.shape)) # Use teacher forcing 50% of the time # force = random.random() &lt; 0.5 outputs, hidden = rnn_model(inputs.double(), None) optimizer.zero_grad() loss = criterion(outputs, targets) loss.backward() optimizer.step() losses[epoch] += loss if iter % 10 ==0: plt.clf() plt.ion() plt.title('Epoch {}, iter {}'.format(epoch, iter)) plt.plot(torch.flatten(outputs.detach()), 'r-', linewidth = 1, label = 'Output') plt.plot(torch.flatten(targets), 'c-', linewidth = 1, label = 'Label') plt.plot(torch.flatten(inputs), 'g-', linewidth = 1, label = 'Input') plt.draw() plt.pause(0.05)# if epoch &gt; 0:# print(epoch, loss) A total of 5 * 99 pictures were rendered in the middle, so I won’t show them one by one. 12plt.plot(losses[20:])plt.show()","link":"/example_08/"},{"title":"Advanced Deep Learning","text":"Different optimer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npimport torchx = np.random.random(size=(100, 8))linear = torch.nn.Linear(in_features=8, out_features=1)sigmoid = torch.nn.Sigmoid()linear2 = torch.nn.Linear(in_features=1, out_features=1)model = torch.nn.Sequential(linear, sigmoid, linear2).double()train_x = torch.from_numpy(x)print(model(train_x).shape)yture = torch.from_numpy(np.random.uniform(0, 5, size=(100, 1)))# print(x)print(yture.shape)&quot;&quot;&quot;torch.Size([100, 1])torch.Size([100, 1])&quot;&quot;&quot;loss_fn = torch.nn.MSELoss()optimer = torch.optim.SGD(model.parameters(), lr=1e-5)for e in range(100): for b in range(100 // 1): # stochastic gradient descent # for b in range(100 // 10): # mini-batch gradient descent # for b in range(100 // 100): # batch gradient descent batch_index = np.random.choice(range(len(train_x)), size=20) yhat = model(train_x[batch_index]) loss = loss_fn(yhat, yture[batch_index]) loss.backward() print(loss) optimer.step()&quot;&quot;&quot;tensor(5.0873, dtype=torch.float64, grad_fn=&lt;MseLossBackward&gt;)tensor(3.4337, dtype=torch.float64, grad_fn=&lt;MseLossBackward&gt;)show more (open the raw output data in a text editor) ...tensor(2.1481, dtype=torch.float64, grad_fn=&lt;MseLossBackward&gt;)&quot;&quot;&quot; Matrix dimension 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from torch import nnimport torchimport numpy as npx = torch.from_numpy(np.random.random(size=(4, 10)))print(x.shape)&quot;&quot;&quot;torch.Size([4, 10])&quot;&quot;&quot;model = nn.Sequential( nn.Linear(in_features=10, out_features=5).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Softmax())ytrue = torch.randint(8, (4, ))print(ytrue)&quot;&quot;&quot;tensor([4, 0, 7, 7])&quot;&quot;&quot;loss_fn = nn.CrossEntropyLoss()print(model(x).shape)print(ytrue.shape)loss = loss_fn(model(x), ytrue)print(torch.randint(5, (3, )))loss.backward()for p in model.parameters(): print(p, p.grad) Advanced deep learning 123456789101112131415# Basic computing libraryimport numpy as np# Deep learning libraryimport torchimport torch.nn as nnimport torch.optim as optimimport torchvisionimport torch.nn.functional as Fimport torchvision.transforms as transforms# Auxiliary drawing galleryimport matplotlib.pyplot as plt# Time operation libraryimport time# Progress bar control libraryfrom tqdm import tqdm Project 1: Forward propagation of simple neural network Question 1: Define the initial parameters and activation function You need to use numpy to implement the forward propagation process of the neural network and calculate the final output result of the output layer. In order to complete the above tasks, we need to make the following assumptions: 1. The value entered is [3,5] 1. The two weights of the hidden layer h1 are [2,4], [4,-5] 1. The two weights of the hidden layer h2 are [-1,1], [2,2] 1. The weight of the output layer is [-3,7] 1. All layers do not use bias 1. All hidden layers need to add tanh activation function 12345678910111213141516# TODO: Define a numpy array with the input data of the neural network:input_data = np.array([3, 5])# TODO: Define a numpy array with the content of the hidden layer and output layer weights of the neural network:# Tips: The weight dictionary has been built, you only need to fill in the corresponding value according to the hidden layer nameweights = {'h11': np.array([2, 4]), 'h12': np.array([4, -5]), 'h21': np.array([-1, 1]), 'h22': np.array([2, 2]), 'out': np.array([-3, 7])}# TODO: Improve the following tanh activation function:def tanh(x): return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) Question 2: Calculate the neural network output layer by layer In the calculation of the neural network, it is necessary to first multiply the weight of the layer to be calculated with its input data, and then sum, and then through the operation of the activation function, it can be output to the next layer. Below we will use the layer as the unit to perform calculations: The first is the first hidden layer. You need to multiply, sum, and input the data of the input layer and the weight of the hidden layer into the activation function. 123456789101112131415161718print(input_data * weights['h11'])a = tanh(input_data * weights['h11']).sum()b = tanh((input_data * weights['h11']).sum())print(a,b)&quot;&quot;&quot;[ 6 20]1.9999877116507956 1.0&quot;&quot;&quot;# TODO: multiply, sum, and input the data of the input layer and the weight of the first hidden layer into the activation function.hidden_11_value = tanh(input_data * weights['h11']).sum()hidden_12_value = tanh(input_data * weights['h12']).sum()hidden_1_output = np.array([hidden_11_value, hidden_12_value])&quot;&quot;&quot;1.9999877116507956-7.550282621338056e-11[ 1.99998771e+00 -7.55028262e-11]&quot;&quot;&quot; Next is the second hidden layer, the operation of this layer is exactly the same as the previous layer. 12345# TODO: multiply, sum, and input the data output by the upper layer and the weight of the second hidden layer into the activation function.hidden_21_value = tanh(hidden_1_output * weights['h21']).sum()hidden_22_value = tanh(hidden_1_output * weights['h22']).sum()hidden_2_output = np.array([hidden_21_value, hidden_22_value]) Finally, there is the output layer. At this time, there is only one node that needs to be calculated, and there is no need to add an activation function. 12# TODO: multiply and sum the data output by the upper layer and the weight of the output layeroutput = (hidden_2_output * weights['out']).sum() At this point, you have completed all the calculations. Now let's print out the output of these layers and have a look. 1234print(output)&quot;&quot;&quot;9.887385002294863&quot;&quot;&quot; Project 2: CIFAR-10 Image Classification Preparation The data set used in this project can be directly exported from the torchvision library. Here are some basic data operations (data download may take a few minutes, please be patient). 1234567891011121314151617181920212223242526272829##Define various transformation operations on the image, including converting the array to tensor, and regularizing the image#transforms.Compose is mainly used for some common graphics transformations, such as cropping and rotation#Traverse the list array and perform each transforms operation on the img in turntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.48216, 0.44653), (0.24703, 0.24349, 0.26159))))#Export the CIFAR10 data set in torchvision. The root is the directory where the data is stored after downloading. The train controls whether it is in the training phase, the download controls whether it needs to be downloaded, and the transform passes in a series of image transformations.trainset = torchvision.datasets.CIFAR10(root='~/data/course_data/', train=True, download=True, transform=transform)testset = torchvision.datasets.CIFAR10(root='~/data/course_data/', train=False, download=True, transform=transform)#Used to divide the training data into multiple groups, this function throws a group of data each time.trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)#Used to divide the test data into multiple groups, this function throws a group of data each time.testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False)&quot;&quot;&quot;Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ~/data/course_data/cifar-10-python.tar.gz170499072it [02:24, 1181561.38it/s] Extracting ~/data/course_data/cifar-10-python.tar.gz to ~/data/course_data/Files already downloaded and verified&quot;&quot;&quot; After the data download is complete, we can simply check the data label to see if it is correct with the data set in the exercise description. 12345678910111213trainset.classes&quot;&quot;&quot;['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']&quot;&quot;&quot; Let's check the data image again. 123456789101112131415161718192021222324252627282930#Display the pictures visually#Define drawing functiondef imshow(inp, title = None): &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot; # Define the canvas for drawing fig = plt.figure(figsize = (30, 30)) # Convert the dimensions of the picture inp = inp.numpy().transpose((1,2,0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) # Standardize the picture inp = std * inp + mean # The value of the entire image array is limited to the specified value a_min, and a_max inp = np.clip(inp, 0, 1) # Visual display of pictures plt.imshow(inp,)# Get a batch of datainputs, classes = next(iter(trainloader))# Display in grid format, the function is to combine several images into one imageout = torchvision.utils.make_grid(inputs)# plt.imshow() can display the picture and also display its formatimshow(out, title = [trainset.classes[x] for x in classes]) Question 1: Build a simple neural network After the data is ready, you need to build a simple neural network. 12345678910111213141516# TODO: define a layer 3 fully connected neural network, the input dimension is 32*32*3, the output dimension of the first layer is 1000, the output dimension of the second layer is 500, and the output dimension of the third layer is 10class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(32*32*3, 1000) self.fc2 = nn.Linear(1000, 500) self.fc3 = nn.Linear(500, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) return self.fc3(x)# Instantiate the neural network classnet = Net() After the model structure is defined, the loss function and optimizer need to be determined. 12345# Define loss function-cross entropycriterion = nn.CrossEntropyLoss()# Define the optimizer, pass the parameters of the neural network to the optimizer, and define the learning rateoptimizer = optim.Adam(net.parameters(), lr = 3e-4) Question 2: Neural Network Training The main content of the model has been completed, and the training can be carried out below. In the process of model training, the following steps are generally followed: Big for loop-epochs, used to manage a set of data loop training several times Small for loop-step, used to retrieve data from dataloader in batchsize unit Clear the gradient of the optimizer Read in data and label, and perform shape transformation (can be done or not) Run the forward propagation process of the model Generate the final result based on the model output Calculate the loss Calculate the gradient based on the loss Update parameters based on gradient 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# TODO: training modelnum_epochs = 10since = time.time()net.train()for epoch in range(num_epochs): print(f'Epoch {epoch + 1} / {num_epochs}') running_loss = 0.0 running_corrects = 0 # Take out each batch of data in a loop from the trainloader for data in tqdm(trainloader): # TODO: Completion code inputs, labels = data inputs = inputs.view(-1, 32 * 32 * 3) optimizer.zero_grad() outputs = net(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) loss.backward() optimizer.step() # Calculation of the loss function of a batch of data running_loss += loss.item() * inputs.size(0) # Calculation of the accuracy of a batch of data running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / trainloader.dataset.data.shape[0] epoch_acc = running_corrects.double() / trainloader.dataset.data.shape[0] print('train loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc)) print('-' * 20)time_elapsed = time.time()-sinceprint('Trainning complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed% 60))&quot;&quot;&quot;Epoch 1 / 10100%|██████████| 3125/3125 [01:04&lt;00:00, 48.74it/s]train loss: 1.6377 Acc: 0.4185--------------------Epoch 2 / 10100%|██████████| 3125/3125 [01:04&lt;00:00, 48.15it/s]train loss: 1.4254 Acc: 0.4962--------------------Epoch 3 / 10100%|██████████| 3125/3125 [01:06&lt;00:00, 47.29it/s]train loss: 1.3065 Acc: 0.5372--------------------Epoch 4 / 10100%|██████████| 3125/3125 [01:04&lt;00:00, 48.76it/s]train loss: 1.2026 Acc: 0.5729--------------------Epoch 5 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 49.98it/s]train loss: 1.1129 Acc: 0.6033--------------------Epoch 6 / 10100%|██████████| 3125/3125 [01:01&lt;00:00, 51.17it/s]train loss: 1.0252 Acc: 0.6343--------------------Epoch 7 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 49.67it/s]train loss: 0.9373 Acc: 0.6668--------------------Epoch 8 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 49.63it/s]train loss: 0.8545 Acc: 0.6936--------------------Epoch 9 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 50.02it/s]train loss: 0.7770 Acc: 0.7242--------------------Epoch 10 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 50.16it/s]train loss: 0.7020 Acc: 0.7492--------------------Trainning complete in 10m 33s&quot;&quot;&quot; Question 3: Model evaluation After completing the model training, the model needs to be evaluated to verify the accuracy of the model on the test set. Tips: In the model training log, the accuracy acc is also printed, but this is the accuracy of the model on the training set, not the accuracy on the test set. You can observe the accuracy of the training set and the accuracy of the test set to see if there is any difference. 12345678910111213141516# TODO: Complete model evaluationcorrect, total = 0, 0net.eval()for data in tqdm(testloader): inputs, labels = data inputs = inputs.view(-1, 32 * 32 * 3) outputs = net(inputs) _, predicted = torch.max(outputs, 1) total += labels.size(0) correct += (predicted == labels).sum().item()print('The testing set accuracy of the network is: %d %%'% (100 * correct / total))&quot;&quot;&quot;100%|██████████| 625/625 [00:03&lt;00:00, 157.71it/s]The testing set accuracy of the network is: 53 %&quot;&quot;&quot;","link":"/example_07/"},{"title":"CNN","text":"The source code: example_09: CNN CNN Principle 123456789101112import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom struct import unpackfrom torchvision.datasets import MNISTfrom sklearn.linear_model import LogisticRegressionimport torchfrom PIL import Imagefrom torch import nnmnist_dataset_train = MNIST(root = '~/data/course_data', train=True, download = True)mnist_dataset_test = MNIST(root = '~/data/course_data', train=False, download = True) The first machine vision problem: Let the computer automatically distinguish between 0 and 6 123456789101112X_train = mnist_dataset_train.data.numpy()y_train = mnist_dataset_train.targets.numpy()X_test = mnist_dataset_test.data.numpy()y_test = mnist_dataset_test.targets.numpy()&quot;&quot;&quot;Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gzDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ~/data/course_data/MNIST/raw/train-images-idx3-ubyte.gz9913344it [00:02, 4759648.85it/s] ...5120it [00:00, 12492633.21it/s]Extracting ~/data/course_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ~/data/course_data/MNIST/raw&quot;&quot;&quot; Explain CNN principles 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140def conv(image, filter_): # Convolution operation print(image.shape) print(filter_.shape) assert image.shape[-1] == filter_.shape[-1] test_image = image height, width = filter_.shape[0], filter_.shape[1] filter_result = np.zeros(( test_image.shape[0]-height + 1, test_image.shape[1]-width + 1 )) for h in range(test_image.shape[0]-height + 1): for w in range(test_image.shape[1]-width + 1): sub_windows = test_image[h: h + height, w: w + width, :] op = np.sum(np.multiply(sub_windows, filter_)) filter_result[h][w] = op return filter_result# Part 2: Strides&quot;&quot;&quot;Try to modify stride in Conv Function&quot;&quot;&quot;# Part3: Pooling&quot;&quot;&quot;Create a pooling cell for conv&quot;&quot;&quot;# Part4: Volume&quot;&quot;&quot;Create 3-d volume filter&quot;&quot;&quot;# Part5: Fully Connected Layers&quot;&quot;&quot;Create Fully Connected Layer, to flatten&quot;&quot;&quot;# Part6: Cross-Entropy&quot;&quot;&quot;Create Cross-Entropy cell to get loss value&quot;&quot;&quot;# Part7: ResNet&quot;&quot;&quot;Why we need resNet, and its functions&quot;&quot;&quot;class ResBlock(nn.Module): &quot;&quot;&quot; A very basic ResNet unit The unit passed: batch normal The output value retains the original input value, so that our result does not dissipate &quot;&quot;&quot; def __init__(self, n_channel): super(ResBlock, self).__init__() self.conv = nn.Conv2d(n_channel, n_channel, kernel_size = 3, padding=1, bias = False) self.bath_norm = nn.BatchNorm2d(num_features = n_channel) torch.nn.init.constant_(self.bath_norw.weight, 0.5) torch.nn.init.zeros_(self.bath_norm.bias) torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity ='relu') # sum(windows * filter) ==&gt; The larger the windows, the larger the added value, the smaller the windows, the smaller the value def forward(self, x): out = self.conv(x) out = self.conv(out) out = self.bath_norm(out) out = torch.relu(out) return out + x if __name__ == '__main__': image = Image.open('~/data/course_data/doo.jpeg') image_array = np.array(image) plt.imshow(image_array) # Robert 算子 rebert_1_kernel = np.array([ [1, 0], [0, -1] ]) robert_2_kernel = np.array([ [0, 1], [-1, 0] ]) #Sobel 算子 sobel_x_kernel = np.array([ [-1, 0, 1], [-2, 0, 2], [-1, 0, 1] ]) sobel_y_kernel = np.array([ [-1, -2, -1], [0, 0, 0], [1, 2, 1] ]) # Laplacian 算子 laplacian_kernel = np.array([ [0, 1, 0], [1, -4, 1], [0, 1, 0] ]) filters = [ np.array([sobel_x_kernel] * 3), np.array([sobel_y_kernel] * 3), np.array([laplacian_kernel] * 3) ] for i, f in enumerate(filters): print('applying filter: {}'.format(i)) plt.subplot(3, 3, i * 3 + 1) plt.imshow(image_array) filter_result = conv(image_array, f) plt.subplot(3, 3, i * 3 + 2) plt.imshow(filter_result) plt.subplot(3, 3, i * 3 + 3) plt.imshow(filter_result, cmap = 'gray')plt.show()#ResNet&quot;&quot;&quot;applying filter: 0(1931, 1931, 3)(3, 3, 3)applying filter: 1(1931, 1931, 3)(3, 3, 3)applying filter: 2(1931, 1931, 3)(3, 3, 3)&quot;&quot;&quot; Identification codes Train a model to classify and recognize the characters in the verification code, and finally complete the verification code recognition The data set used contains a total of 36 characters from 0-9 and AZ. There are 50 pictures for each character in the training set, and 10 pictures for each character in the verification set. The verification code data set is composed of 4 character pictures taken out randomly. become. Related knowledge points Data Reading Use torch to build, train, and verify models Model prediction and image segmentation analyze Question 1-Establish a character comparison table We can reverse each pair of keys and values by traversing the dictionary and store them in a new dictionary. The sample code is as follows: 1new_dict = {v: k for k, v in old_dict.items()} #### Question 2-Define datasets and dataloader In opencv-python, you can use image = cv2.medianBlur(image, kernel_size) for median filtering. #### Question 3-Define the network structure In torch, the convolution and fully connected layers are defined as follows: 12conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)fc = nn.Linear(in_features, out_features, bias) #### Question 4-Define the model training function The model training process of the torch framework includes operations such as clearing the gradient, forward propagation, calculating the loss, calculating the gradient, and updating the weight, among which: 1. Clear the gradient: the purpose is to eliminate the interference between step and step, that is, use only one batch of data loss to calculate the gradient and update the weight each time. Generally can be placed first or last; 1. Forward propagation: use a batch of data to run the process of forward propagation to generate model output results; 1. Calculate the loss: use the defined loss function, model output results and label to calculate the loss value of a single batch; 1. Calculate the gradient: According to the loss value, calculate the gradient value required in this optimization in the ownership of the model; 1. Update weight: Use the calculated gradient value to update the value of all weights. The sample code of a single process is as follows: 12345&gt;&gt;&gt; optimizer.zero_grad() # Clear the gradient (can also be placed in the last line)&gt;&gt;&gt; output = model(data) # forward propagation&gt;&gt;&gt; loss = loss_fn(output, target) # Calculate loss&gt;&gt;&gt; loss.backward() # Calculate the gradient&gt;&gt;&gt; optimizer.step() # update weight Programming Import the library to be used in this project 123456789101112131415import osimport cv2import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.utils.data import Dataset, DataLoaderimport torchvisionimport torchvision.transforms as transformsimport numpy as npimport pickleimport PILimport matplotlib.pyplot as pltfrom PIL import Imageos.environ['KMP_DUPLICATE_LIB_OK'] = 'True' Understanding the data set Define the data path 123train_data_dir = '~/data/course_data/train_data.bin'val_data_dir = '~/data/course_data/val_data.bin'verification_code_dir = '~/data/course_data/verification_code_data.bin' The data set used is stored in a binary file, and we need to define a function to read the picture in the binary file. 1234def load_file(file_name): with open(file_name, mode ='rb') as f: result = pickle.load(f) return result See what the data set looks like: 123456789101112train_data = load_file(train_data_dir)img_test = list()for i in range(1, 1800, 50): img_test.append(train_data[i][1])plt.figure()for i in range(1, 37): plt.subplot(6, 6, i) plt.imshow(img_test[i-1]) plt.xticks([]) plt.yticks([])plt.show() View single big picture 12345# plt.subplot(6, 6, i)plt.imshow(train_data[500][1])plt.xticks([])plt.yticks([])plt.show() It can be seen that there is a lot of noise in the character picture, and the noise will have an adverse effect on the model prediction result, so we can use a specific filter to eliminate the picture noise during data preprocessing. Question 1-Establish a character comparison table A simple observation shows that there are no duplicates in the key and value in the character dictionary just defined. Therefore, the key and value in the dictionary can be reversed so that we can use the value to find the key (convert the model prediction result into a readable character) Now you need to complete the following code to reverse the keys and values in the dictionary (for example: dict={'A':10,'B':11} and get new_dict={10:'A ',11:'B'} 12345char_dict = {'0':0,'1':1,'2':2,'3':3,'4':4,'5':5,'6':6,'7':7,'8':8,'9':9,\\ 'A':10,'B':11,'C':12,'D':13,'E':14,'F':15,'G':16,'H':17,'I':18,'J':19,'K':20,'L':21,'M':22,\\ 'N':23,'O':24,'P':25,'Q':26,'R':27,'S':28,'T':29,'U':30,'V':31,'W':32,'X':33,'Y':34,'Z':35 }new_char_dict = {v : k for k, v in char_dict.items()} Question 2-Define datasets and dataloader We need to use torch.utils.data.Dataset as the parent class to define our own datasets in order to standardize our own datasets. 123456789101112131415class iDataset(Dataset): def __init__(self, file_name, transforms): self.file_name = file_name # file name self.image_label_arr = load_file(self.file_name) # read binary file self.transforms = transforms # Image converter def __getitem__(self, index): label, img = self.image_label_arr[index] img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) # Convert the picture to grayscale img = cv2.medianBlur(img, 5) # Use median blur to remove image noise img = self.transforms(img) # Transform the image return img, char_dict[label[0]] def __len__(self): return len(self.image_label_arr) Now we can define transform and dataloader. 12345678910transform = transforms.Compose([transforms.ToPILImage(), transforms.Resize([28, 28]), # Adjust the image size to 28*28 transforms.ToTensor(), # Convert the picture to tensor transforms.Normalize(mean = [0.5], std = [0.5])]) # Perform normalization processingtrain_datasets = iDataset(train_data_dir, transform)train_loader = DataLoader(dataset=train_datasets, batch_size=32, shuffle = True)val_datasets = iDataset(val_data_dir, transform)val_loader = DataLoader(dataset=val_datasets, batch_size = 32, shuffle = True) Question 3-Define the network structure After the data is ready, we need to define a simple convolutional neural network. The input of the neural network is [batchsize,chanel(1),w(28),h(28)], and the output is 36 categories. Our neural network will use 2 convolutional layers with 2 fully connected layers. The parameter settings of these four layers are shown in the following table (the default parameters can be used directly if they are not marked): 1. conv1: in_chanel=1, out_chanel=10, kernel_size=5 1. conv2: in_chanel=10, out_chanel=20, kernel_size=3 1. fc1: in_feature=2000, out_feature=500 4. fc2: in_feature=500, out_feature=36 1234567891011121314151617181920212223class ConvNet(nn.Module): def __init__(self): super().__init__() # TODO: self.conv1 = nn.Conv2d(1, 10, 5) self.conv2 = nn.Conv2d(10, 20, 3) self.fc1 = nn.Linear(20 * 10 * 10, 500) self.fc2 = nn.Linear(500, 36) def forward(self, x): # inputsize: [b, 1, 28, 28] in_size = x.size(0) # b out = self.conv1(x) out = F.relu(out) out = F.max_pool2d(out, 2, 2) out = self.conv2(out) out = F.relu(out) out = out.view(in_size, -1) out = self.fc1(out) out = F.relu(out) out = self.fc2(out) out = F.log_softmax(out, dim = 1) return out Question 4-Define the model training function Next, we need to complete the model training function to achieve the following operations: 1. Clear the gradient 1. Forward propagation 1. Calculate the gradient 1. Update weights 12345678910def train(model, train_loader, optimizer, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if(batch_idx + 1) % 10 == 0: print('Train Epoch: {} [{} / {} ({:.0f} %)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) Define model test function 123456789101112def test(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: output = model(data) test_loss += F.nll_loss(output, target, reduction = 'sum') pred = output.max(1, keepdim = True)[1] correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%) \\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) Define model and optimizer We define the model structure we just built as model and choose to use the Adam optimizer. 12model = ConvNet()optimizer = optim.Adam(model.parameters()) Model training and testing We can first set the number of epochs to 3 and perform model training to see how accurate the model is and whether it meets the requirements of verification code recognition. If the model accuracy is not enough, you can also try to adjust the number of epochs and retrain. 123456789101112131415161718192021222324252627282930EPOCHS = 3for epoch in range(1, EPOCHS + 1): train(model, train_loader, optimizer, epoch) test(model, val_loader) &quot;&quot;&quot;Train Epoch: 1 [288 / 1800 (16 %)] Loss: 3.340514Train Epoch: 1 [608 / 1800 (33 %)] Loss: 2.872326Train Epoch: 1 [928 / 1800 (51 %)] Loss: 1.977929Train Epoch: 1 [1248 / 1800 (68 %)] Loss: 1.098688Train Epoch: 1 [1568 / 1800 (86 %)] Loss: 0.535660Test set: Average loss: 0.2888, Accuracy: 328/360 (91%) Train Epoch: 2 [288 / 1800 (16 %)] Loss: 0.072813Train Epoch: 2 [608 / 1800 (33 %)] Loss: 0.139866Train Epoch: 2 [928 / 1800 (51 %)] Loss: 0.109487Train Epoch: 2 [1248 / 1800 (68 %)] Loss: 0.058259Train Epoch: 2 [1568 / 1800 (86 %)] Loss: 0.013144Test set: Average loss: 0.0099, Accuracy: 360/360 (100%) Train Epoch: 3 [288 / 1800 (16 %)] Loss: 0.010245Train Epoch: 3 [608 / 1800 (33 %)] Loss: 0.004797Train Epoch: 3 [928 / 1800 (51 %)] Loss: 0.002203Train Epoch: 3 [1248 / 1800 (68 %)] Loss: 0.006250Train Epoch: 3 [1568 / 1800 (86 %)] Loss: 0.005230Test set: Average loss: 0.0028, Accuracy: 360/360 (100%)&quot;&quot;&quot; Define model test function 123456789101112def test(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: output = model(data) test_loss =+ F.nll_loss(output, target, reduction = 'sum') pred = output.max(1, keepdim = True)[1] correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print('\\nTest set: Average loss: {:.4f}, Accuracy : {}/{} ({:.0f}%) \\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) Define model and optimizer We define the model structure we just built as model and choose to use the Adam optimizer. 12model = ConvNet()optimizer = optim.Adam(model.parameters()) Model training and testing We can first set the number of epochs to 3 and perform model training to see how accurate the model is and whether it meets the requirements of verification code recognition. If the model accuracy is not enough, you can also try to adjust the number of epochs and retrain. 1234567891011121314151617181920212223242526272829EPOCHS = 3for epoch in range(1, EPOCHS + 1): train(model, train_loader, optimizer, epoch) test(model, val_loader)&quot;&quot;&quot;Train Epoch: 1 [288 / 1800 (16 %)] Loss: 3.508450Train Epoch: 1 [608 / 1800 (33 %)] Loss: 3.288610Train Epoch: 1 [928 / 1800 (51 %)] Loss: 2.584805Train Epoch: 1 [1248 / 1800 (68 %)] Loss: 1.180833Train Epoch: 1 [1568 / 1800 (86 %)] Loss: 0.564084Test set: Average loss: 0.0088, Accuracy : 316/360 (88%) Train Epoch: 2 [288 / 1800 (16 %)] Loss: 0.173177Train Epoch: 2 [608 / 1800 (33 %)] Loss: 0.043262Train Epoch: 2 [928 / 1800 (51 %)] Loss: 0.054462Train Epoch: 2 [1248 / 1800 (68 %)] Loss: 0.052596Train Epoch: 2 [1568 / 1800 (86 %)] Loss: 0.013714Test set: Average loss: 0.0006, Accuracy : 360/360 (100%) Train Epoch: 3 [288 / 1800 (16 %)] Loss: 0.004590Train Epoch: 3 [608 / 1800 (33 %)] Loss: 0.007654Train Epoch: 3 [928 / 1800 (51 %)] Loss: 0.004135Train Epoch: 3 [1248 / 1800 (68 %)] Loss: 0.003140Train Epoch: 3 [1568 / 1800 (86 %)] Loss: 0.003019Test set: Average loss: 0.0001, Accuracy : 360/360 (100%) &quot;&quot;&quot; The model has been trained! Does the test set accuracy of the last epoch exceed 99%? Identification codes After successfully implementing the digital recognition, we can start the verification code recognition! First, import the verification code data set: 1verification_code_data = load_file(verification_code_dir) Let's choose a picture at random (Figure 6) to see what the verification code looks like. 1234image = verification_code_data[6]IMG = Image.fromarray(cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB))plt.imshow(IMG)plt.show() Let's take a look at what effect the median filter can have on the captcha image. 123img = cv2.medianBlur(image.copy(), 5)plt.imshow(img)plt.show() Finally, let us look at the actual results of verification code recognition: 1234567891011121314151617181920212223242526272829IMAGES = list()NUMS = list()for img in verification_code_data: IMAGES.append(img) img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) image_1 = img[:, :80] image_2 = img[:, 80:160] image_3 = img[:, 160:240] image_4 = img[:, 240:320] img_list = [image_1, image_2, image_3, image_4] nums = [] for one_img in img_list: one_img = transform(one_img) one_img = one_img.unsqueeze(0) output = model(one_img) nums.append(new_char_dict[torch.argmax(output).item()]) NUMS.append('Verification_code: '+ ''.join(nums))plt.figure(figsize = (20, 20))plt.subplots_adjust(wspace = 0.2, hspace=0.5)for i in range(1, 11): plt.subplot(5, 2, i) plt.title(NUMS[i-1], fontsize = 25, color = 'red') plt.imshow(IMAGES[i - 1]) plt.xticks([]) plt.yticks([])plt.show()","link":"/example_09/"},{"title":"Business Intelligence(BI)","text":"Use LeNet model to recognize Mnist handwritten digits 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import tensorflow as tf#print(tf.__version__)from tensorflow.keras import layersfrom tensorflow.keras.datasets import mnistfrom tensorflow.keras.layers import Conv2D, MaxPooling2Dfrom tensorflow.keras.layers import Dense, Flattenfrom tensorflow.keras.models import Sequentialimport numpy as npimport warningswarnings.filterwarnings('ignore')# Data loading#(train_x, train_y), (test_x, test_y) = mnist.load_data() #Download the data set from the Internetdata = np.load('~/data/course_data/mnist.npz') #Read data set from local#print(data.files)train_x, train_y, test_x, test_y = data['x_train'], data['y_train'], data['x_test'], data['y_test']warnings.filterwarnings('ignore')# Input data is mnist data settrain_x = train_x.reshape(train_x.shape[0], 28, 28, 1)test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)train_x = train_x / 255test_x = test_x / 255train_y = tf.keras.utils.to_categorical(train_y, 10)test_y = tf.keras.utils.to_categorical(test_y, 10)# Create sequential modelmodel = Sequential()# The first layer of convolutional layer: 6 convolution kernels, the size is 5*5, relu activation functionmodel.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))# The second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size=(2, 2)))# The third layer of convolutional layer: 16 convolution kernels, size 5*5, relu activation functionmodel.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))# The second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size=(2, 2)))# Flatten the parameters, which is called a convolutional layer in LeNet5. In fact, this layer is a one-dimensional vector, the same as the fully connected layermodel.add(Flatten())model.add(Dense(120, activation='relu'))# Fully connected layer, the number of output nodes is 84model.add(Dense(84, activation='relu'))# The output layer uses the softmax activation function to calculate the classification probabilitymodel.add(Dense(10, activation='softmax'))# Set the loss function and optimizer configurationmodel.compile(loss=tf.keras.metrics.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])# Pass in training data for trainingmodel.fit(train_x, train_y, batch_size=128, epochs=2, verbose=1, validation_data=(test_x, test_y))# Evaluate the resultsscore = model.evaluate(test_x, test_y)print('error:%0.4lf' %score[0])print('Accuracy:', score[1])&quot;&quot;&quot;Train on 60000 samples, validate on 10000 samplesEpoch 1/260000/60000 [==============================] - 39s 643us/sample - loss: 0.3172 - acc: 0.9096 - val_loss: 0.1105 - val_acc: 0.9626Epoch 2/260000/60000 [==============================] - 39s 652us/sample - loss: 0.0892 - acc: 0.9725 - val_loss: 0.0664 - val_acc: 0.979010000/10000 [==============================] - 4s 358us/sample - loss: 0.0664 - acc: 0.9790error:0.0664Accuracy: 0.979&quot;&quot;&quot; Use LR to classify MNIST handwritten digits 12345678910111213141516171819202122232425262728293031323334from sklearn.model_selection import train_test_splitfrom sklearn import preprocessingfrom sklearn.metrics import accuracy_scorefrom sklearn.datasets import load_digitsfrom sklearn.linear_model import LogisticRegressionimport matplotlib.pyplot as plt# Download Datadigits = load_digits()data = digits.data# Data Explorationprint(data.shape)# View the first imageprint(digits.images[0])# The meaning of the numbers represented by the first imageprint(digits.target[0])# Display the first imageplt.gray()plt.title('Handwritten Digits')plt.imshow(digits.images[0])plt.show()&quot;&quot;&quot;(1797, 64)[[ 0. 0. 5. 13. 9. 1. 0. 0.] [ 0. 0. 13. 15. 10. 15. 5. 0.] [ 0. 3. 15. 2. 0. 11. 8. 0.] [ 0. 4. 12. 0. 0. 8. 8. 0.] [ 0. 5. 8. 0. 0. 9. 8. 0.] [ 0. 4. 11. 0. 1. 12. 7. 0.] [ 0. 2. 14. 5. 10. 12. 0. 0.] [ 0. 0. 6. 13. 10. 0. 0. 0.]]0&quot;&quot;&quot; 12345678910111213141516# Split the data, use 25% of the data as the test set, and the rest as the training settrain_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)# Adopt Z-Score standardizationss = preprocessing.StandardScaler()train_ss_x = ss.fit_transform(train_x)test_ss_x = ss.transform(test_x)# Create LR classifierlr = LogisticRegression()lr.fit(train_ss_x, train_y)predict_y=lr.predict(test_ss_x)print('LR accuracy rate: %0.4lf'% accuracy_score(predict_y, test_y))&quot;&quot;&quot;LR accuracy rate: 0.9644&quot;&quot;&quot;","link":"/example_12/"},{"title":"FREEDOM","text":"许很多的影片我都应该从新温习一遍。不只是因为我学的就是这个，更重要的是每次看一遍都能理解一些新的东西。 勇敢的心，1995年电影界最成功影片。先不去评论其他技术上的细节。我只是为一个名族英雄折服。freedom。 中国古话里就说过：生命诚可贵，爱情价更高，若为自由故，两者皆可拋。 不知道从什么时候开始，这句话被滥用了。人们总是以此来津津乐道自由的重要。歪曲的道理不能称之为道理。这里的自由，我根本不原意理解成为个人的自由。至少国有国法，家有家规这话我还不会去颠覆它。 任何环境总是有规矩才会成方圆，才会有乐趣。 自由，更深层的含义是民族上的。。。为本民族的自由而战，豪情万丈。所以梅尔.吉普森最后的”freedom”如此震撼人心。。 至此，我仍然相信，民族利益高于一切。一切政治上的形式主义都可以扔到一边。for my people.I will…","link":"/freedom/"},{"title":"Natural Language Processing NLP","text":"Resnet Visualize 12345678910111213141516171819202122232425262728293031323334353637383940414243import torchvisionimport torch.nn.functional as Ffrom torchvision.transforms import transformsfrom torch import nnimport torchimport matplotlib.pyplot as pltfrom icecream import icfrom PIL import Imageimport numpy as npdef visualize_model(model, input_, output): width = 8 fig, ax = plt.subplots(output[0].shape[0] // width, width, figsize=(20, 20)) for i in range(output[0].shape[0]): ix = np.unravel_index(i, ax.shape) plt.sca(ax[ix]) ax[ix].title.set_text('filter-{}'.format(i)) plt.imshow(output[0][i].detach()) plt.show()preprocess = transforms.Compose([ transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),])resnet = torchvision.models.resnet18(pretrained=True) # transfer step 1: load pretrained modelconv_model = [m for _, m in resnet.named_modules() if isinstance(m, torch.nn.Conv2d)]&quot;&quot;&quot;Downloading: &quot;https://download.pytorch.org/models/resnet18-f37072fd.pth&quot; to /Users/lilithgames/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth100%|██████████| 44.7M/44.7M [00:34&lt;00:00, 1.36MB/s]&quot;&quot;&quot;for m in conv_model: m.register_forward_hook(visualize_model)myself = preprocess(Image.open('~/data/course_data/doo.jpeg'))with torch.no_grad(): resnet(myself.unsqueeze(0)) # un-squeeze for convert myself to [ [myself] ] Only some pictures are posted here Transfer Example 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import torchvisionimport torch.nn.functional as Ffrom torchvision.transforms import transformsfrom torch import nnimport torchimport matplotlib.pyplot as pltfrom icecream import icpreprocess = transforms.Compose([ transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),])cifar_10 = torchvision.datasets.CIFAR10('~/data/course_data/', download=False, transform=preprocess)train_loader = torch.utils.data.DataLoader(cifar_10, batch_size=128, shuffle=True)resnet = torchvision.models.resnet18(pretrained=True) # transfer step 1: load pretrained modelfor param in resnet.parameters(): param.requires_grad = False # frozen weights feature_num = resnet.fc.in_featuresresnet.fc = nn.Linear(feature_num, 10) # rewrite fc classifieric(resnet(cifar_10[0][0].unsqueeze(0)))criterion = nn.CrossEntropyLoss()optimizer = torch.optim.SGD(resnet.parameters(), lr=1e-3, momentum=0.9)epochs = 2losses = []&quot;&quot;&quot;return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)ic| resnet(cifar_10[0][0].unsqueeze(0)): tensor([[-0.0763, -0.4537, 0.8168, 0.2136, -0.0465, 0.4844, -0.4026, 0.8763, -0.7048, -0.7375]], grad_fn=&lt;AddmmBackward&gt;)&quot;&quot;&quot;for epoch in range(epochs): epoch_loss = 0 for i, (images, labels) in enumerate(train_loader): ic(epoch, i) output = resnet(images) loss = criterion(output, labels) optimizer.zero_grad() loss.backward() optimizer.step() epoch_loss += loss.item() if i &gt; 0: print('Epoch: {} batch:{}, loss ==&gt; {}'.format(epoch, i, epoch_loss / i)) losses.append(epoch_loss / i)&quot;&quot;&quot;ic| epoch: 0, i: 0ic| epoch: 0, i: 1ic| epoch: 0, i: 2Epoch: 0 batch:1, loss ==&gt; 5.118020296096802ic| epoch: 0, i: 3Epoch: 0 batch:2, loss ==&gt; 3.8235710859298706ic| epoch: 0, i: 4...ic| epoch: 0, i: 203Epoch: 0 batch:202, loss ==&gt; 1.4433288293899875...&quot;&quot;&quot;plt.plot(losses)plt.show()&quot;&quot;&quot;Because the last time is too long to run, the losses are not assigned&quot;&quot;&quot; Resnet Transfer Learning 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import torchvisionimport torch.nn.functional as Fcifar_10 = torchvision.datasets.CIFAR10('~/data/course_data', download=False, transform=preprocess)train_loader = torch.utils.data.DataLoader(cifar_10, batch_size=512, shuffle=True)plt.imshow(cifar_10[10][0].permute(1, 2, 0))for param in res_net.parameters(): param.requires_grad = False# Parameters of newly constructed modules have requires_grad=True by defaultnum_ftrs = res_net.fc.in_featuresres_net.fc = nn.Linear(num_ftrs, 10) # only update this part parameters criterion = nn.CrossEntropyLoss()# Observe that only parameters of final layer are being optimized as# opposed to before.optimizer_conv = optim.SGD(res_net.fc.parameters(), lr=0.001, momentum=0.9)# Decay LR by a factor of 0.1 every 7 epochslosses = []epochs = 10for epoch in range(epochs): loss_train = 0 for i, (imgs, labels) in enumerate(train_loader): print(i) outputs = res_net(imgs) loss = criterion(outputs, labels) optimizer_conv.zero_grad() loss.backward() optimizer_conv.step() loss_train += loss.item() if i &gt; 0 and i % 10 == 0: print('Epoch: {}, batch: {}'.format(epoch, i)) print('-- loss: {}'.format(loss_train / i)) losses.append(loss_train / len(train_loader)) Show Resnet 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import cv2import numpy as npimport torchfrom torchvision import transformsfrom torchvision.models import resnet18from torchsummary import summaryimport matplotlib.pyplot as pltdef show_one_model(model, input_, output): width = 8 fig, ax = plt.subplots(output[0].shape[0] // width, width, figsize=(20, 20)) for i in range(output[0].shape[0]): ix = np.unravel_index(i, ax.shape) plt.sca(ax[ix]) ax[ix].title.set_text('Filter-{}'.format(i)) plt.imshow(output[0][i].detach()) # plt.pause(0.05) input('this is conv: {}, received a {} tensor, press any key to continue: '.format(model, input_[0].shape)) plt.show() def main(img): &quot;&quot;&quot; Forward propagation, print feature maps during the transfer process &quot;&quot;&quot; # Define device, transforms transform = transforms.Compose([transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.ToTensor(), ]) # Process pictures, define models img = transform(img).unsqueeze(0) model = resnet18(pretrained=True) # Print model summary, which can be used for convolutional layer comparison summary(model, (3, 224, 224)) for p in model.parameters(): print(p) conv_models = [m for _, m in model.named_modules() if isinstance(m, torch.nn.Conv2d)] for conv in conv_models: conv.register_forward_hook(show_one_model) with torch.no_grad(): model(img) # conv_models = [m for _, m in model.named_modules() if isinstance(m, torch.nn.Conv2d)] # # first_conv = conv_models[0] # # show_one_model(first_conv, img, output=first_conv(img)) if __name__ == '__main__': img = cv2.imread('~/data/course_data/doo.png') main(img) &quot;&quot;&quot;---------------------------------------------------------------- Layer (type) Output Shape Param #================================================================ Conv2d-1 [-1, 64, 112, 112] 9,408 BatchNorm2d-2 [-1, 64, 112, 112] 128show more (open the raw output data in a text editor) ... -2.5093e-02, 6.7847e-03, -1.7868e-02, -7.8250e-04, -6.3448e-03], requires_grad=True)&quot;&quot;&quot;","link":"/example_10/"},{"title":"G1 app2sd 完全教程","text":"声明1:你所需要的软件在这里可以下载的到! 声明2.app2sd虽然可以省却手机内存,但是也有许多不便的地方!操作后SDcard就是机子的一部分,不能随便摘取.我用的4G的卡,在机子挂在后存储有问题!不知道其他卡如何.所以在存储文件和音乐的时候还是需要用到读卡器,而这个时候我必须选择关机!直接卸载SDcard会造成机子程序出错!而不得不从新执行一遍app2sd的过程!并且执行过后也会存在一些不可知的问题!如果对稳定性比较看重的人这里可以飘过了! 声明3.我的sdcard已经在手机内通过!懒得再刷,所以没有用我的card抓图!本教程图片多为网上现成图片来完成!而图片不是一个地方抓取的!所以图片上的容量会有差距.但是刷机过程没有错 从新格盘,正好用自己的图!顺便说一下,ubuntu下的默认抓图真恶心!每抓一张都要从新启动一次程序! 所需要的准备的工作: 1.SDcard(必须) 2.分区软件(必须,windows下可以使用Acronis Disk Director Suite,支持vista.linux下可以直接利用终端分区!) 3.Android SDK(非必须,可以再网上下载Terminal Emulator.apk,安装后在手机上输入adb下的指令完成操作!) 首先我们要将SDcard分区,分成fat32和ext2,至于ext3是否可行我没有测试过,有兴趣的可以试试并且留言告诉我测试报告! 我选择的是在ubuntu的终端执行,这样操作比较靠谱.而在windows下的分区软件不是很稳定!会造成诸多不可见的错误! windows下的分区软件有Acronis Disk Director Suite以及PartitionManager,至于分区魔术师可以略过,因为它不支持分区SDcard.Acronis Disk Director Suite软件分区可以移步到此查看! 以USB内存卡方式插上电脑，或者用读卡器插上电脑 像我的ubuntu，它会自动挂载你的卡。 把东西备份好，然后卸载。一定要卸载，不然无法分区 启动ubuntu或者您的linux系统,在终端内输入如下代码: dmesg //查看所连接的设备! 可以看到sdb或者sdc之类的设备名称!假设我以下操作都为sdc设备! sudo fdisk /dev/sdc //这里需要说明,如果linux下非root,必须要输入sudo来取得root权限进行操作.以下类同! p是显示当前分区 n是创建 d是删除 w是应用你的操作 doo@ubuntu:~# sudo fdisk /dev/sdc Command (m for help): d &lt; ==删除当前分区 Command (m for help): p &lt;==显示一下，确定已删除 Disk /dev/sdc: 3965 MB, 3965714432 bytes 122 heads, 62 sectors/track, 1024 cylinders Units = cylinders of 7564 * 512 = 3872768 bytes Disk identifier: 0x9dfd42a5 Device Boot Start End Blocks Id System Command (m for help): Command (m for help): m &lt; ==查看帮助 Command action a toggle a bootable flag b edit bsd disklabel c toggle the dos compatibility flag d delete a partition l list known partition types m print this menu n add a new partition o create a new empty DOS partition table p print the partition table q quit without saving changes s create a new empty Sun disklabel t change a partition's system id u change display/entry units v verify the partition table w write table to disk and exit x extra functionality (experts only) Command (m for help): n &lt;==新建分区，选择主分区 Command action e extended p primary partition (1-4) p Partition number (1-4): 1 &lt;==指定该主分区为1号 First cylinder (1-1024, default 1): &lt;==敲回车，直接使用SD卡的最开头 Using default value 1 Last cylinder or +cylinders or +sizeK(K,M,G) (1-1024, default 1024): +3300M &lt;==填入分区的大小 Command (m for help): n &lt;==新建分区，选择扩展分区(所有逻辑分区加起来就是扩展分区) Command action e extended p primary partition (1-4) p Partition number (1-4): 2 &lt;==扩展分区的序号是2 First cylinder (895-1024, default 895): &lt;==敲回车，直接接着剩余空间的最开头 Using default value 895 Last cylinder or +cylinders or +sizeK(K,M,G) (895-1024, default 1024): &lt;==敲回车，用默认的，使用全部剩余空间 Using default value 1024 Command (m for help): Command (m for help):p Disk /dev/sdc: 3965 MB, 3965714432 bytes 122 heads, 62 sectors/track, 1024 cylinders Units = cylinders of 7564 * 512 = 3872768 bytes Device Boot Start End Blocks Id System /dev/sdc1 1 894 733792+ 83 Linux /dev/sdc2 729 1024 272128+ 83 Linux 创建好两个分区后, 我们还需要用命令t修改分区卷标, 选择分区1改卷标为c 命令为 Command (m for help):t t &lt; ==修改卷标 partition number (1-4): 1 &lt;==输入1来制定第一个分区. Hex code (type L to List codes): c &lt;==输入C来制定卷标 Changed system type of partition 1 to c (W95 FAT32 (LBA)) Command (m for help): w &lt;==将缓冲写入SD卡,应用你的操作 The partition table has been altered! Calling ioctl() to re-read partition table. WARNING: If you have created or modified any DOS 6.X partitions, please see the fdisk manual page for additional information. Syncing disks. doo@ubuntu:~# doo@ubuntu:~# sudo ls /dev/sdc* &lt; ==查看分区情况 /dev/sdc /dev/sdc1 /dev/sdc2 doo@ubuntu:~#sudo mkfs.vfat /dev/sdc1 &lt;==格式化第一个主分区。 mkfs.vfat 3.0.1 (23 Nov 2008) doo@ubuntu:~# sudo mkfs.ext2 /dev/sdc2 &lt;==格式化第二个分区 mke2fs 1.41.4 (27-Jan-2009) warning: 139 blocks unused Filesystem laber= OS type: Linux Block size=1024 (log=0) Fragment size=1024 (log=0) 123360 inodes, 491521 blocks 24583 blocks (5.00%) reserved for the super user First data block=1 Maximum filesyetem blocks=67633152 68 block groups 8192 blocks per group, 8192 fragments per group 2856 inodes per group Superblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409 Writing inode tables: done Writing superblocks and filesystem accounting information: done This filesystem will be automatically checked every 38 mounts or 180 days.whichever comes first. Use tune2fs -c or -i to override. doo@ubuntu:-$ 分区完毕后ubuntu会自动挂在两个盘符.表示成功! 然后需要手机必须为app2sd版本的rom,在windows 命令提示符下输入命令查看: 以下步骤必须安装android sdk.(其实一下步骤不一定需要在windows cmd下进行,在网上下载一个android的终端Terminal Emulator.apk,然后启动此程序在手机内输入以下指令是一样的!只是在sdcard的系统盘下建立app文件夹并挂载到android rom上! ) C:and Settings&gt;cd c:&lt; ==cd到sdk adb.exe C:&gt;adb devices &lt; ==查看连接的硬件和设备 List of devices attached 000000000000 device &lt;==分区过硬盘以后连接会显示000000000000 的硬件号 C:&gt;adb shell # su &lt; ==如果你还没有取得root权限,那么这一步通不过. su # ls /system &lt;==查看一下system目录下的文件夹 ls /system lib framework media fonts etc customize build.prop usr bin xbin app sd lost+found busybox df -h &lt; ==查看系统盘情况!如果分区成功,那么会在android的系统下显示分区.如下我的385.8M的分区在android的系统内!再往下是sdcard的系统!如果没有那表示分区失败.当然还有一种可能就是你的手机不是app2sd rom busybox df -h Filesystem Size Used Available Use% Mounted on tmpfs 48.3M 0 48.3M 0% /dev tmpfs 4.0M 12.0k 4.0M 0% /sqlite_stmt_journals /dev/block/mtdblock3 67.5M 67.5M 0 100% /system /dev/block/mtdblock5 74.8M 30.4M 44.3M 41% /data /dev/block/mtdblock4 67.5M 1.2M 66.3M 2% /cache /dev/block/mmcblk0p2 385.8M 2.0k 366.5M 0% /system/sd &lt; ==由于在ubuntu下分区后手机内读取sdcard出错,所以后便又分了一次!但是没有抓图,所以容量上和上图有差距.再者本身linux和windows读取SDcard的容量上就有不同! /dev/block//vold/179:1 3.3G 4.0k 3.3G 0% /sdcard # mkdir /system/sd/app &lt;==建立sdcard分区上的app文件夹!如果以前sdcard曾做过app2sd,那么这个文件夹是存在的!会有命令符提示文件夹存在! mkdir /system/sd/app # cd /data cd /data # cp -a app /system/sd/app cp -a app /system/sd/app # rm -r app rm -r app # ln -s /system/sd/app /data/app ln -s /system/sd/app /data/app # reboot reboot 手机自动重启后就OK了.放心安装你所想要的apk程序吧! 顺便说一句:ubuntu的9.04快要放出正式版了!欢迎大家下载试用.","link":"/g1-app2sd/"},{"title":"G1上打造Hero!(更新tips&amp;app)","text":"这本来是安卓上发布的一片帖子,我写在这里主要是为了为自己增加点浏览量!顺便解解眼馋.但是我并不打算在这里提供教程和下载! (由于flickr最近不稳定,所以图片显现不出来!)是由于博客上的flickr插件的原因.. 有兴趣的看这里吧.... 教程中需要的是C6的卡,但是我的是金士顿东京原厂8G卡.试试用一下吧!稍后发试用报告!","link":"/g1-hero-tipsapp/"},{"title":"G1 提权刷机顺序","text":"本人主要是写给在我博客中留言的@leo 以及和他一样刷机无从入手的朋友!技术是一直在更新的,老的技术文档虽然不太适用了,但是参考价值是一定有的!所以也可以翻看我之前的文档,对比本文参看! 长久不用就是会忘记哈...唉..已经很久没有从源头开始刷起了! 拿起公司的黑色美版G1,按一直以来的刷机程序来刷,傻眼了....recovery的版本不对,是最早的版本,机子虽然是1.6版本,但是没有提权,无法获得root权限.也就根本无从刷机. 所以需要从新捡起以前的知识来从源头开始刷起..这就需要一个逻辑性的顺序问题.也就是我们需要将G1降级到RC29或者RC7版本,RC29对应的是美版第一个文件,而我的白色英版对应的则是RC7. 顺序上应该是: 将手机降级并获取root权限 push recovery.img,获取testkey 恢复出厂设置 升级所需要版本. 升级RC版本 升级G2版本 升级APP2SD版本 在第一步就卡着了,源头版本文件没有保存.各大论坛也都没有存档了!找的比较费劲,所以有些文件还是需要保存的.... 具体可以参看博客内的相关链接! 另外,关于技术文档上比较重要的一点就是,在获取testkey以后,最好更换recovery的版本,因为以前的版本选项比较少,只能将刷机包更名为update.zip来进行刷机,而在后期的版本中加入了list,可以选择根目录内的zip包来进行刷机而无需命名为update,并且可以wipe system以及sdcard等多个内容.这就是所说的full wipe,现在很多rom都需要full wipe才可以刷机,避免出错! 并且更重要的是,在后期版本中的recovery中加入了ext3 to ext4的更换分区功能.玩过linux的应该对这个分区很熟悉!而在黑色美版中我刷入了1.61版的recovery,依旧刷的1.6版本rom.","link":"/g1-mentioned-order-of-the-right-brush/"},{"title":"G1 蓝牙传输更新 Bluetooth File Transfer 1.4","text":"不知道有没有人和我遇到了一样的问题!在G1rom更新到固件1.6以后,app2sd的Bluex1.12版本无法安装,而无app2sd版本的在安装后只能发送文件而无法打开蓝牙端口接收! 有兴趣的朋友可以到这里下载以后试试. 而这次寻到一个新的软件,版本为1.4版本,相信不是以前的bluex的版本更新,其实我自己也不确定是否为更新版本,因为图标已经换的很彻底了,并且内部结构也与以前大不一样!最重要的一点,原来支持文档类别存放,而现在却统一放在一个文件夹下!确实有些很不方便... 不过这个apk只需要在sdcard内直接安装就可以使用,需要提供root权限...而传输和接受都OK,并且比较顺畅! 另外,此软件我没有在hero的固件上测试,不知道是否支持hero,有兴趣的自己试试吧!而hero上的蓝牙传输其实有解决办法了...如下: More progress 11:04pm CST 8/26/09 Tracked down what calls the BTIP service, it's /system/lib/libandroid_runtime.so . Tried replacing it with a cupcake build, rebooted and ran into the issue where /system/framework/framework.jar is still referencing calls that were in the Hero libandroid_runtime.so . So replaced framework.jar and framework.odex from cupcake build and got the following error. D/AndroidRuntime( 1517): &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; AndroidRuntime START &lt; &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; D/AndroidRuntime( 1517): CheckJNI is OFF I/dalvikvm( 1517): DexOpt: mismatch dep signature for '/system/framework/core.odex' E/dalvikvm( 1517): /system/framework/framework.jar odex has stale dependencies I/dalvikvm( 1517): Zip is good, but no classes.dex inside, and no valid .odex file in the same directory D/libc-abort( 1517): abort() called in pid 1517 Any \"educated\" ideas? Questions &amp; Progress 01:09pm CST 8/17/09 So lately what I've been trying to do is find where a reference is made to actually call the BTIPS service. I've been lookiing in /system/framework and /data/app_s/Settings.apk but haven't found it yet. What I'm hoping to do is modify the file and have it call BT the same way cupcake did. Has anyone else found where a reference to \"btips\" is at? Settings.apk, which is what pops up when on home screen and you hit menu-&gt;settings, only makes a call to \"android:targetClass=\"com.android.settings.bluetoo th.BluetoothSettings\" Anywho, if you find it in any system libraries or framework files let me know. Please no PM's or posts about where you \"THINK\" it may be at. I've already tried the random guessing stuff, now I'm going through libraries one by one trying to find it. Some more notes 12:30pm CST 7/24/09 Here are some notes of interest. There are two versions of the /system/bin/bts daemon that are floating around on the Hero builds md5sum bts 29ffa46f12c01e3690690752b4e2d58d bts md5sum bts 5aeaca42d67d3b3c64ceda9ee4bfec1a bts There are also two versions of the TIInit_5.3.53.bts firmware files. One is actually just the brf6300.bin file renamed to match what Hero is looking for in /etc/firmware md5sum TIInit_5.3.53.bts d7a214bdb9b4fbc2b4e2dd7e3ab95df0 TIInit_5.3.53.bts md5sum TIInit_5.3.53.bts cb3d2ecbfc97c026a0dcceb8c959b7db TIInit_5.3.53.bts If you run \"strings\" on /system/bin/bts and grep for \"TII\" you'll be able to tell which firmware files that version supports TIInit_3.4.27.bts TIInit_4.2.38.bts TIInit_5.2.34.bts TIInit_5.3.53.bts TIInit_6.2.31.bts Nice picture illustrating BT architecture in Android 7:04pm CST 7/17/09 A note for ROM devs 02:27pm CST 7/17/09 Something to note, Hero does not use any of the following legacy services and therefore they can be removed from init.rc &amp; init.trout.rc . This is mainly something the ROM cookers should pay attention to. The btips service actually handles all of this now. REMOVE THE FOLLOWING: service hcid /system/bin/hcid -s -n -f /etc/bluez/hcid.conf socket bluetooth stream 660 bluetooth bluetooth socket dbus_bluetooth stream 660 bluetooth bluetooth # init.rc does not yet support applying capabilities, so run as root and # let hcid drop uid to bluetooth with the right linux capabilities group bluetooth net_bt_admin misc disabled service hciattach /system/bin/hciattach -n -s 115200 /dev/ttyHS0 texas 4000000 flow user bluetooth group bluetooth net_bt_admin disabled service hfag /system/bin/sdptool add --channel=10 HFAG user bluetooth group bluetooth net_bt_admin disabled oneshot service hsag /system/bin/sdptool add --channel=11 HSAG user bluetooth group bluetooth net_bt_admin disabled oneshot Found something 01:48pm CST 7/17/09 I was looking through init.trout.rc and noticed the following lines chown bluetooth bluetooth /sys/devices/platform/msm_serial_hs.0/serial_lock_cpu chmod 0660 /sys/devices/platform/msm_serial_hs.0/serial_lock_cpu This may not seem like much but this node does not actually exist in our builds. It's possible, and probably likely, that HTC modified their kernel to support the changes that were made in the bts (btips) daemon. We all are pretty much not using the HTC kernel, we're using custom compiled kernels from JAC or Cyanogen. I tried using the RUU kernel but couldn't boot at all. Is anyone able to get their phone booting off the RUU kernel and NOT one of the custom kernels that are floating around in these ROMs? If so, can you check if this device node exists? I believe booting off that kernel could be the answer to the UART clock issues I'm getting and missing devices in /sys . NEXT I have been toying around with the following value in init.rc that seems to affect whether or not I get an error. /proc/sys/net/unix/max_dgram_qlen The default is 10, the RUU release of Hero sets it to 999. If I change that to 10000 then it pauses the BT services and just sits there. If I revert to default I get the same error that I see when its set to 999. Wondering if there's a happy medium in queue length (qlen). Just me thinking out loud. Latest progress 11:43pm CST 7/15/09 I wanted to post some newer results I've been having with BT debugging on Hero. I found out how to circumvent the UART disable error. This is done by having the service btips statement in init.rc to look as follows service btips /system/bin/bts socket bluetooth stream 666 bluetooth bluetooth socket dbus_bluetooth stream 666 bluetooth bluetooth group bluetooth net_bt_admin root misc disabled oneshot The most important part is \"oneshot\" which tells Android to NOT restart the btips service after it dies. If you leave this off then it will relaunch btips service and tie up the I2C bus. The newest error I'm getting is the inability to launch HCI. This is hopefully the LAST error before I can get BT functional! Anyways, just wanted to update everyone that I have not stopped working on bluetooth. 1247718990.888806 BTSTACK(778) INFO | UATRAN: HCI Command was not acknowledged with an event [ vendor/ti/btips-linux/B_TIPS/btstack/hcitrans/uart/uarttran.c:298 ] 1247718990.889935 BTSTACK(778) INFO | HCI: HCI_Process detected transport failure [ vendor/ti/btips-linux/B_TIPS/btstack/stack/hci/hci_proc.c:1596 ] 1247718990.890179 BTSTACK(778) INFO | RADIOMGR: RmgrHciCallback: 0x6 [ vendor/ti/btips-linux/B_TIPS/btstack/stack/radiomgr.c:364 ] 1247718990.890362 BTSTACK(778) INFO | RADIOMGR: HCI init failed (retrying) [ vendor/ti/btips-linux/B_TIPS/btstack/stack/radiomgr.c:386 ] 1247718990.890484 BTSTACK(778) INFO | RADIOMGR: HCI init error [ vendor/ti/btips-linux/B_TIPS/btstack/stack/radiomgr.c:335 ] 1247718990.890637 BTSTACK(778) INFO | ME: HCI Init complete status: 22 [ vendor/ti/btips-linux/B_TIPS/btstack/stack/me/me.c:1220 ] 1247718990.890789 BTSTACK(778) INFO | CMGR: Received event HCI_INIT_ERROR [ vendor/ti/btips-linux/B_TIPS/btstack/profiles/common/conmgr.c:591 ] 1247718990.890942 BTSTACK(778) INFO | Dbus | inside _BTBUS_COMMON_BTL_callback with event: 6 0[ vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_common.c:62 ] 1247718990.893536 BTSTACK(778) INFO | sending dbus message from BTBUS_COMMON_BTL_callback in {vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_common.c:84}[ vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_utils.c:189 ] 1247718990.898022 BTSTACK(778) INFO | Dbus | _BTBUS_COMMON_BTL_callback signal sent: 6 0[ vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_common.c:87 ] 1247718990.898358 BTSTACK(778) FATAL | HCI Init Status Received while neither FM nor BT On in progress[ vendor/ti/btips-linux/EBTIPS/btl/ti_chip_mngr/ti_chip_mngr.c:1232 ] 1247718990.898541 BTSTACK(778) Assert | 0[ vendor/ti/btips-linux/EBTIPS/btl/ti_chip_mngr/ti_chip_mngr.c:1232 ] 1247718990.899121 BTSTACK(778) FATAL | signal 11 sent to our program from address 0xdeadbaad and code 1[ vendor/ti/btips-linux/EBTIPS/apps/btt_task.c:102 ] I'll update this main post as I, or others, come up with progress or advancements. The directories for this are already created in the latest Hero init.rc . Just need to create the ddb file touch /data/btips/TI/BtDeviceDb.ddb chmod 666 /data/btips/TI/BtDeviceDb.ddb The results of making these changes is you are able to get ALL bluetooth services and sockets created. Bluetooth is working from the commandline, just not on the frontend where we need it. PS:xda那边似乎有人已经放出hero shippment rom, 蓝牙问题应该已经解决了....静待佳音吧! :[download id=\"6\"] | skydrive 下载","link":"/g1-bluetooth-file-transfer-1-4/"},{"title":"Google  Android 4.0","text":"10,19日上午十点左右,Google,三星在香港联合举办了发布会,推出了下一代Android系统(Ice Cream Sandwich)Android4.0,以及下一代Nexus智能手机,三星Galaxy Nexus Prime. 这次的软件开发包也在这次大会宣布可以下载使用,在developer,android.com上. 这次更新重新设计了浏览器的界面,改进了Gmail功能,增强了照相和摄影.可以进入编辑模式,使用各种系统自带的滤镜效果和其他编辑操作! 当然,我最欣赏的改进就是这个全新分享功能 一种全新的分享方法，Andriod Bean，使得我们分享变得非常简单。比如说现在他有一个Andriod的手机，把手机对过去想和我分享，就是通过Andriod Bean来分享，把两个手机背靠背的放在一起内容就过来了，这个文章就已经过来了，就是这样轻轻一碰。我想看一下Google Map，现在我看到的是一个地图，在香港弥敦道上面的，现在要把这个地图直接发到我的手机上，同样是把两个手机背靠背放在一起，我手机上也出现了同样的地图。再和大家分享最后一个例子，他在玩一个很酷的游戏，我手机上没有，但是我也很想玩这个游戏，虽然我也可以到Andriod市场上浏览、下载，但是我选择用更酷的方式来做。也是把手机背靠背的放在一起，然后就直接把我带到Andriod市场上的这个游戏页面上，我就不用浏览了，直接下载就可以了。Andriod Bean可以让我们加入某一个社交群体、分享图片、内容，极大的释放了我们的想象力。 关于更新问题,怀疑这次N1这个亲儿子估计也应该可以赶上末班车,因为Google官方不是声明,但凡运行Android2.3的机子,都可以更新并运行4.0么!现在能做的就是坐等CM大神了!","link":"/google-android-40/"},{"title":"Gmail location in signature","text":"labs的新功能,在你的邮件当中显示你所在的地理坐标! 我开始还有些许疑问,到底是内嵌地图还是别的显示方式...那如果收取方不是Gmail的邮箱是否就不能现实等等! 启用后发现原来是在签名栏上多加了一条信息:Sent from: Cangzhou Hebei China. 其实这个功能不能说是无用,但是我觉得类似鸡肋! 怎么说呢...至少在国内这个功能比较无用.这仅仅是我的感觉!","link":"/gmail-location-in-signature/"},{"title":"Google+ 研究","text":"已經忘記今天是第幾天試用Google+了，這兩天圈內當然免不了的大部分都在討論Google+，而什麼所謂的G+應用技巧，什麼25條關於G+的，還有什麼50條，100條類似的！反正不止是G+內部，包括twitter，facebook，微博以及博客圈內，大家都在樂此不彼的討論Google+！ 可是我這裡不想討論一些技術上的東西，也不想討論它是否真的能夠打敗Facebook或者twitter！我只想從真正的社交上去分析一下我自己的對G+內部細微的探討！ 找不到好圖，又懶得自己做。所以借用一下+lucifr Liu的圖！反正都做了，表浪費！原圖有 PSD文檔可以下載！我按照自己的標準稍稍修改了一下！ 讓我們先來探討一下G+中的circles，雖然所有的社交網絡中都有這樣一種概念，但是沒有Google那樣去深挖它，延伸它！在多部分的社交網絡中，或許都是在模仿Facebook，或許就是在模仿Twitter！而這兩者的基本機制就是相互添加對方為好友，才算開始真正的社交！當然，Twitter的機制要鬆散一點，即便雙方沒有相互fo對方或者只有一方fo了，一樣可以互相聊天！而Facebook對於社交圈子的概念就相對封閉了一點，如果只有一方fo了對方，那麼一樣不構成相互溝通的條件！ 而比較尷尬的是，Facebook作為一個最大的SNS網站，基本上老老少少都在上邊進行基礎社交，這也構成了現在年輕人逐步放棄Facebook的主因！因為這個如果都相互fo了對方，那麼這個circles的範圍就太大了。只要我發佈了一條消息，我的老師，我的父母，我的同學以及我的基友，大家都可以看到這條消息！而在真實的社交範圍中，這是不可想像的！比如，包二奶這裡就是個十分不適合的場所！當然，我們可以私信！！！！！！好吧，我舉的例子有些屎，我承認！那麼讓我們想像一下，總有些事情是不希望父母或者另外的人知道的，而只想在小範圍內的圈子內傳播的！那麼，Google+給了你這個實現的方式，就是circles概念的區分和交集！這個還需要你對聯繫人進行具體的整理才可以！有些人，即是朋友又是同事，這就是兩個circles的交集，那麼我一條信息不管發佈在我的朋友圈還是同事圈，這個人都可以收到我的信息！加入說，同事圈裏又有些話題需要討論而不想讓老闆看到該如何，讓我們建立一個將老闆排除在外的同事圈！ 大概瞭解了Circles的概念以後，我想討論的就是Google+的一些細節的地方！我們都知道，無論是微博還是SNS社交網站比如開心，人人網都會有Share分享功能！這個功能的作用在於，看到好的東西以後，可以直接分享給自己的社交圈裏的所有人觀看！這也是網絡病毒營銷的根源！而在Google+中，我們注意到一個細節，並且進行了一番討論。Share這個功能被分割了。我們以大家熟悉的微博為例，當我Share某話題以後，如果原帖被修改或者刪除，那麼我Share過去的內容也會跟著變動！而且原帖會被注明轉帖和評論了多少次！在G+中就完全不是這樣，當我Share某個帖子後，這個帖子就變成我複製過去的了！只會聲明原帖出自什麼人。並且評論不會跟著原帖一起變動，而我在新Share的帖子下邊回覆，也不被記錄在原帖的評論裏！這就是我們所討論的被分割！可是我們思考一下真實社交圈的情況，我們從某個人那裡聽到了一個消息，然後四處散播！當那個人再對同樣的圈子內修正了這個消息或者否定了這個消息以後，我所散播出去的圈子并不能收到這樣一個反饋，那麼，只有我自己去再去修正一下自己所說過的話！那麼也就是，每個人都必須對自己所說的話和所散播的言論負責！而當你所散播了消息以後，即便和你之前聽到的是一模一樣的，那麼這個討論也只限制在你自己的圈子內而和原來的圈子無關！這個說明，Google+在細節上真的是在充分考慮真實社交的環境而在努力模擬再現！ 大家也都發現，G+可以被當作郵箱來用！當你發表一篇消息給某個特定的人或者一個圈子的人。那麼對方會收到一個消息。這個消息最終如何接受有三種情況！第一種情況，對方沒有G+帳號，那麼這條消息肯定是直接發佈到對方郵箱裏！第二種情況，對方有G+帳號而你所寫接收方是聯繫人或者圈子，並且對方有關閉郵件提醒，那麼對方會在G+上接收到這條信息。第三種情況就是對方有G+帳號並且沒有關閉郵件提醒，那麼就是G+和郵箱都會收到這條消息！ 不過，對於給沒有G+帳戶的人用G+發佈邀請還可以，如果要當作邮箱来用，還是奉勸大家不要這樣做！因為G+發佈郵件的郵箱並不是你的郵箱，是隨機生成的一個郵箱地址，並不是固定的！也就是說對方即便收到也無法回覆！而我做了一個實驗，給我的163，QQ，以及yahoo和gmail郵箱都各發了一個郵件！有的郵件無影無蹤了，而即便收到的郵件，也是等了很久以後才收到的！所以，如果要發郵件的話，請點擊黑又長上邊+You後邊的Gmail選項，老老實實的用Gmail發郵件吧！ PS：此處之後還會不斷更新，分享我對Google+的一些想法！當然，是有了感悟後才會來更新！以後關於Google+的研究如無特殊情況也都在本帖內討論！想到哪寫到哪，不分那麼多條條框框了！ 其實，關於Google+，還有很多值得研究的細節，Google也在不斷完善！可以看得出這真的是一個很有誠意的互聯網產品！相應號召，大家都搬家吧！","link":"/google-research/"},{"title":"Gmail 的转发上限","text":"在Gmail中创建转发过滤器是有上限的,这个估计没有多少人知道!我想也是因为没有多少人有实际的需求… 今天我在Twitter上抱怨Gmail里的邮件过多,占用了很大一部分空间,以至于我的Gmail空间已经临近上限… 也终于迫使我开始清理邮件! ###问题产生:### 在清理的时候发现,除了一些比较大附件的邮件以外,大部分占用空间的邮件都是一些广告邮件! 我很少设置过滤器删除广告邮件,特别是一些推送服务的广告,不过大多都是直接略过收件箱存档而已,有空了还可以去看看…这也使得我这几年来邮箱里布满了此类邮件! 当然,清理的过程还是比较顺利的,毕竟这些邮件的时效性的原因,这些邮件删除起来一点也不心疼! 那么,对于新邮件该怎么处理呢,总不能继续占用存储空间吧.也不好直接删除,毕竟有的时候我还是要看看的. 那么,过滤器这个时候就起作用了! 我的想法很简单,既然QQ邮箱空间是无上限的,而广告邮件又无关乎隐私问题,所以可以放心的转发到QQ空间进行保存!所以对一些广告邮件设置了forward filter…然后问题来了,当我建立到一定的数量的时候,Gmail开始提醒我“转发邮件地址过多,无法创建过滤器 这可怎么办,总该是有解决办法的!于是查阅Gmail的帮助手册发信,Gmail是支持布尔运算符来查找邮件的!而这些布尔运算符一样可以创建到filter里. OK,问题到这里就变得简单了… ###解决方案:### 比如说,我要将 renren.com kaixin001.com 高朋 卓越 美团 拉手等邮件过滤直接forward到QQ邮箱里,那么其实创建一条filter就够了!按如下格式renren.com OR kaixin001.com OR 高朋 OR 卓越 OR 美团 OR 拉手,其中“OR”是Gmail中所支持的布尔运算符,而且必须大写. 延伸阅读: 希望这篇文可以帮助更多的人更好的利用Gmail.更详尽的运算符可以查阅Gmail的帮助手册","link":"/gmail-forward-upper-limit/"},{"title":"Google Translate","text":"一些比较有意思的发音,自己点击传送门去听! 一段DJ 不雅段子1 春哥纯爷们儿，铁血真汉子，人民好兄弟，父亲好儿子。 莫装b，装b遭雷劈 呃,这个是....","link":"/google-translate/"},{"title":"Gphone SD卡分区(更新)","text":"以前写过一个Gphone app2sd的完全教程,在那个教程当中有完整的分区步骤,可是那个分区模式是需要linux系统才可以执行,而我当时用的是ubuntu. 现在可行的分区模式也就是linux下,然后windows下的分区软件.譬如PartitionManager,AcronisDiskDirector,不要去想PQ魔术师,那个不支持sd卡分区!就这么多了么?...其实,如果你有Gphone手机的话,完全可以用windows dos来分区! 先说条件: 1.Gphone手机 2.SDcard 3.android sdk 4.一条usb连接线 步骤: 首先开机进入recovery模式，按ALT+X进入“console” 打开cmd,输入: adb shell parted /dev/block/mmcblk0 print 可以看到分区的情况,一般来说都是一个分区,如果以前做过app2sd那么就是两个...删除这两个分区.如下图为三个分区: 然后输入代码删除分区: rm 1 rm 2 rm 3 (如果只有两个分区或者一个分区的,执行一步操作就可以了,也就是rm&lt; 数字&gt;) 在完成后就是一个未分区的SDcard 然后对SDcard重新分区,注意需要根据你的卡大小来分配各分区的大小,一般linux-swap最大32M.ext分区500M足够了,最大不要超过1GB.不过似乎有将linux-swap分成96Mb的...输入以下代码分区: mkpartfs primary fat32 0 7445 mkpartfs primary ext2 7445 7873 300-500都可 mkpartfs primary linux-swap 7873 7969 务必是96M 不然你有C6卡也不能全速体验HERO了... 至此,分区工作就完成了.可以输入print来检查一下. 以下为可执行可不执行步骤...就是将ext2转换为ext3/ext4 转换为ext3输入以下命令: upgrade_fs 转换ext4输入以下命令: tune2fs -O extents,uninit_bg,dir_index /dev/block/mmcblk0p2 e2fsck -fpDC0 /dev/block/mmcblk0p2 upgrade_fs 结束以后,输入 parted /dev/block/mmcblk0 print 验证是否升级到ext3/ext4 然后quit退出,重启手机. 本教程参考了安卓网上安装hero的步骤","link":"/gphone-sdcard/"},{"title":"habari近期进度","text":"其实在装上habari,解决了导入wordpress和乱码问题以后,就一直没有什么进度了! 这两天一直在寻找能有效将wordpress功能平滑过度到habari的有效方法,只可惜我的水平确实有限.对代码一点都不了解.没办法,只能寻找一些其他途径希望能绕过这部分.可惜的是habari的程序似乎并没有那么简单. 在研究themes的时候发现,默认模板总有两个是可以实现截断的.那么在截断这个问题上就是themes在起作用.可是在查看了默认模板以后真的是很失望,在代码上我根本无计可施,没有任何头绪可以让我找到具体是哪里实现了截断的功能! 而在换用模板以后,发现天佑提供的archives插件已经实现了在wordpress上实现的archives页面.这点似乎还是蛮简单的.并不需要在后台专门建立一个页面模板,而只用在添加页面的时候增加一段天佑提供的代码就可以实现. 那么对于tag页面以及其他功能性的页面是否一样如此简单...不得而知,因为我具体没有做其他测试! 在css上,我有意使用现在所用的模板,而期望通过修改css来更改页面展示的宽度.我想,在css上大致应该是相通的.那么,这就将是我的下一步工作! 其实到现在为止,剩下的工作就全部都是themes和plugins了.修改代码,移植而已!...如果能弄通了,那么在不远的将来就可以像驾驭wordpress一样驾驭habari了.期待ing....","link":"/habari-progress/"},{"title":"繼續habari","text":"habari確實有很多地方是我喜歡的,當然,要說完全轉到這個平臺上還不成熟!因為我很多東西根本無力一個人解決...慢慢來吧,暫時還是更新我的wordpress,到時候再全部轉過去! 我的habari測試平臺 後臺非常簡單 在plugins中選取flickr plugin安裝后編寫文章是這個效果: 雖然後臺是完全html編寫,但是對於插入圖片不需要自己寫代碼.直接點擊display就可以了!會自動加入顯示代碼,當然,如果你不喜歡flickr或沒有帳戶,也可以上傳到自己的服務器上.而也有habari自己的插入插件.效果是一樣的! 現在要轉過去的話面臨幾個問題 涉及到themes,雖然k2已經很好用了.但是more代碼并不能截斷,這看來和模板有關.估計天佑是修改了自己的模板的! 看著天佑的模板很成熟了,如果能拿過來直接用當然最好!呵呵. 時區問題,默認時區不知道去哪裡修改!找了一圈都沒有在後臺中發現... 其他的都暫時是小問題!以後慢慢解決... 另外就是前文中提到的亂碼問題.說出來大家不要遺憾...我沒有特意去解決,就是從新安裝了一邊,而安裝的時候輸入了中文名作為博客名,就沒有亂碼了!而導入的時候也一并沒有亂碼了! 想想,中間有一個小小的操作...就是dupola曾經讓我在後臺中執行了一次整理,編碼為general_ci","link":"/habari-go-on/"},{"title":"Logistic regression to diagnose heart disease","text":"The preject source code url : Heart load data 1234567891011121314import pandas as pdfrom sklearn.linear_model import LogisticRegressionfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_splitdata = pd.read_csv('./data/heart.csv')# the csv url: https://github.com/hivandu/colab/blob/master/AI_Data/data/heart.csv# Print a brief summary of the data setdata.info()data.shapedata.target.value_counts() The params meaning 123456789101112131415Params Meaning age 年龄 sex 性别(1 = 男性, 0 = 女性)cp 胸部疼痛类型(值1：典型心绞痛，值2：非典型性心绞痛，值3：非心绞痛，值4：无症状）trestbps 血压 chol 胆固醇 fbs 空腹血糖（&gt; 120 mg/dl，1=真；0=假） restecg 心电图结果（0=正常，1=患有ST-T波异常，2=根据Estes的标准显示可能或确定的左心室肥大） thalach 最大心跳数 exang 运动时是否心绞痛（1=有过；0=没有）oldpeak 运动相对于休息的STslop 心电图ST segment的倾斜度(值1:上坡，值2:平坦，值3:下坡） ca 透视检查看到的血管数 thal 缺陷种类（3=正常；6=固定缺陷；7=可逆缺陷）target 是否患病（0=否，1=是） Perform analysis 12345678910111213141516171819202122232425# Change the &quot;sex&quot; column into two columns &quot;sex_0&quot; and &quot;sex_1&quot;sex = pd.get_dummies(data['sex'], prefix = 'sex') # Add &quot;sex_0&quot; and &quot;sex_1&quot; to the data set. data = pd.concat([data, sex], axis = 1)# And delete the sex column.data = data.drop(columns = ['sex'])# Print out the first five lines. Check whether sex_0, sex_1 are added successfully, and whether sex is deleted successfully.data.head()# Get sample labeldata_y = data.target.valuesdata_y.shape# Get sample feature setdata_x = data.drop(['target'], axis = 1)data_x.shape# Divide the data settrain_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size = 0.3, random_state=33) Normalization 1234567891011121314151617181920212223# initializess = StandardScaler()# The fit function/module is used to train model parametersss.fit(train_x)# Standardize the training set and test settrain_x = ss.transform(train_x)test_x = ss.transform(test_x)# Define a logistic regression modellr = LogisticRegression()lr.fit(train_x, train_y)# Calculate the training set scorelr.score(train_x, train_y)# Calculate test set scorelr.score(test_x, test_y)# Use the classification_report function to display a text report of the main classification indicatorspredict = lr.predict(test_x)print(classification_report(test_y, predict))","link":"/heart/"},{"title":"Hello 2012","text":"以2012起头,其实主要是想要写2011 今天是2011年的最后一天了,明天正式步入毁灭年!谁知道去,也许是真的呢...这样的社会,早点灭了的好! 总体来说,今年是非常不顺的一年.不管是对世界,对中国,还是对我来说... 由于微博的兴旺,各种事情仿佛一瞬间被放大,被拉近...而我们忽然发现,世界原来可以如此精彩...不过这精彩的代价,确实让人有些心痛!这一年里,最欣喜的事情就是在Google Plus里看@变态辣椒的漫画.真是一个好地方啊... 不过这个月初开始不太愿意在那里发言,主要原因还是发现其实放在什么地方,都会有脑残的出现! 而最让人头疼的是,脑残们都不会觉得自己是脑残,根本说不通道理...而他们那些偏激的想法,让我感到恐惧! 是的,如果人类经历一次灭顶之灾,而忽然发现自己周围仍然在酝酿这种灭顶之灾的苗头,你会感到恐惧! 所幸,临到结尾,@韩寒 抛出了三篇惹人口风的文章...看来丫是想给大家留个值得咀嚼的话题在年内!而在问内提到里\"清算\"这样的字眼... 唉,说多了,其实也是废话! 况且说, 我这里真不适合谈论这些.","link":"/hello-2012/"},{"title":"Bluehost上安裝Habari","text":"Habari我還在試用階段...不得承認wordpress確實是一個好的博客程序,但是對於他的日進臃腫我有點微詞... 可憐我不是代碼編寫出身,所以很多問題不得不求助於人,還好身邊有一個好老師\"天佑\"給我提供了很多幫助! 以下就寫寫近期的一些內容. 因為bluehost原本就只願PHP5,而其他的一些標準我不太懂,但是據我安裝下來基本都已經全部滿足,唯一的就是需要自己開啟pdo for mysql. 在這之前你最好是下載一個requirements.php來測試一下你的主機是否已經為你的habari做好了準備. 將requirements.php上傳到自己需要安裝的文件夾下,輸入http://youblogurl/requirements.php 如果切OK,那么就會如下顯示: 如若不然,就會有如下顯示: 我這裡提示的是需要安裝或者打開pdo,如果你也是bluehost,那么基本就是這個提示了! 其實到這步是不需要聯繫客服幫你打開的,默認bluehost就已經安裝了pdo,主要是需要打開而已! 在自己的服務器上建立php.ini文檔,在CP上點擊PHP Config,然後選擇install php.ini master file 不出意外在根服務器上就已經建立php.ini了,加入如下語句: extension_dir = \"/usr/lib/php/extensions/no-debug-zts-20060613\" extension=pdo.so extension=pdo_mysql.so 到這裡當然還沒有結束,需要將php.ini複製到你所要安裝的habari文件夾... 這步一定要做.我不知道原理,所以不要問我,我只是如此操作了,成功了.反而刪除后就會出現需要激活pdo的提示. 然後自然就是下載habari,上傳到安裝目錄,輸入路徑...然後就是和wordpress的安裝順序一樣了! 這裡要提示一點:在網上有說安裝habari要將其目錄設定為777,我的經驗是不要這樣設置,這樣會造成訪問此目錄的時候出現505錯誤頁面...\"天佑\"幫我測試的時候就是如此!...後來我更改回來后就正常訪問了... 當然,很多人都是wordpress的用戶,所以如果你需要導入wordpress的原始數據的話需要做如下工作: 安裝一個插件,用以取消自動保存. 禁用後臺的多版本保存. 刪除數據庫中多餘的文章版本. 完成一上步驟后在habari導入的時候才不會有重複文章導入! 當然,在我的habari測試中還出現了亂碼問題,不僅僅是導入的時候出現亂碼,在輸入博客標題,寫新文章都會出現中文亂碼問題,在發此文的時候這個問題還沒有得到解決,只能讓各位待續了! 再次感謝\"天佑\"的幫助!","link":"/install-habari-at-bluehost/"},{"title":"Google正式推出代码搜索 Code Search","text":"来自小熊在线 Google新一轮的发布热潮在黄金周涌现。这就是Google刚推出的代码搜索，即Google Code Search。根据Garett Rogers的介绍，Google代码搜索结果来自Google的索引。也就是说理论上Google能找到的代码，你都可以利用Google代码搜索找到。 你可以利用Google代码搜索来搜索各种函数的定义以及相关的示例代码，还可以直接使用正则表达式搜索以获得更精确的结果。另外，你还可以限定搜索某种语言、许可或文件名。 对于程序员而言，这个代码搜索工具应用挺实用的。Google Code Search允许编程人员搜索代码用法范例，以更好地理解代码功能。该服务中索引了数十亿行代码，来源是Web上保存的文档以及SourceForge、Google Code等开源软件项目库。 Google还提供了一个API允许第三方开发人员将代码搜索框整合到他们的开发工具中。 点击进入Google Code Search","link":"/google-code-search/"},{"title":"habari的時區和more","text":"在這之前還一直在煩惱habari的時區問題和more代碼截斷. fireyy給了我一些幫助. habari最新的版本已經加上了時區的調整.而這個版本是需要svn方式獲取才可以.基於bluehost默認沒有開通ssh方式.那么我就只有選擇等待habari下一版本的正式發布,又或者聯繫bluehost增加ssh訪問,并花一定時間來學習和熟悉svn獲取的方式! 另外一個就是代碼截斷.在habari總一樣可以試用more代碼來對文章進行截斷,在所用模板總加入: // Only uses the tag, with the 'more' as the link to full post Format::apply_with_hook_params( 'more', 'post_content_out', 'more' ); 這樣在編寫entry的時候就與wordpress的效果是相同的了. 而整個habari的模板代碼和wordpress是比較類似的!頭疼的是plugin和wordpress有很多不一樣的地方!所以對於不懂代碼的我來說,就無從著手了... 奇怪的一點是我現在說使用的habari模板激活后有一個setting的選項,點擊以後并無反應! 我想本身是可以修改的... 比較丟人的地方在於,前邊的文章總提到了歸檔頁面,其實并不是真的在頁面中直接添加代碼就可以了!而是我所用的模板中已經加入了兼容代碼...對於如何建立頁面來實現仍然不清楚! 2009-2-21 01:53 BTW:好吧,我承認我自己又傻了一回...并不是需要在服務器上安裝SVN才可以的..也可以將svn服務器上的程序下載到本地然後再上傳,效果是一樣的!而我現在說安裝的habari就更新到了0.6beta.這樣就完全解決了時區問題!並且增加了zh_tw的文件包!雖然漢化并不完全,但是已經很好了!","link":"/habaris-time-zone-and-more/"},{"title":"Debian上安装Node服务器","text":"本教程只针对我自己，记录用，而且并不是完整版，不对其他人负责。请尽量不要参照本文 最近一直在学习NodeJS，本地上玩的差不多了，总要去架设个服务器跑一下，选择了digitalocean加的$5/mo 服务，安装了个Debian，至于为什么是Debian，好吧，因为。。。 言归正传，一遍操作一遍记录一下，好记性不如烂笔头嘛。 至于怎么注册DigitalOcean这里就不在详述了，从SSH登录开始吧。 SSH登录 未免麻烦，最好是选择SSH登录，官方有详细的介绍： How To Use SSH Keys with DigitalOcean Droplets 这里要说一下，DigitalOcean每次登录的时候都会告诉我密码过期，害得我重置了无数次密码。如果你也遇到这种问题，那么就先选择Conole Access, 然后在弹出的窗口控制台进行操作，修改root密码后再在本地操作。 12345678910111213141516171819202122// 创建SSH keyssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (~/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in ~/.ssh/id_rsa.Your public key has been saved in ~/.ssh/id_rsa.pub.The key fingerprint is:4a:dd:0a:c6:35:4e:3f:ed:27:38:8c:74:44:4d:93:67 demo@aThe key's randomart image is:+--[ RSA 2048]----+| .oo. || . o.E || + . o || . = = . || = S = . || o + = + || . o + o . || . o || |+-----------------+ 1cat ~/.ssh/id_rsa.pub 然后添加到digitalocean的SSH Keys里，Name随便起 之后我们就可以链接服务器了 1cat ~/.ssh/id_rsa.pub | ssh root@[your.ip.address.here] &quot;cat &gt;&gt; ~/.ssh/authorized_keys&quot; 然后，就可以直接登录了: 1ssh root@[your.ip.address.here] 安装Node 我选择的方式是源码安装 1234567891011121314151617// update system$ sudo apt-get update$ sudo apt-get install git-core curl build-essential openssl libssl-dev// Clone node$ cd /usr/local/src$ git clone https://github.com/nodejs/node$ cd node// select checkout$ git tag$ git checkout v4.4.7// install$ ./configure$ make$ sudo make install 漫长的等待，然后就可以查询了$ node -v， 这会就会出现安装的node version 安装NPM 12345$ wget https://npmjs.org/install.sh --no-check-certificate$ chmod 777 install.sh$ ./install.sh$ npm -v3.10.5 安装zsh(不是必要) 123456789sudo apt-get zshgit clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh// copy defult zshrccp ~/.zshrc ~/.zshrc.bak// set oh-my-zsh to usecp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrccash -s /bin/zshsudo shutdown -r now 安装Ruby 安装rbenv 1234567891011git clone git://github.com/sstephenson/rbenv.git ~/.rbenv# 用来编译安装 rubygit clone git://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build# 用来管理 gemset, 可选, 因为有 bundler 也没什么必要git clone git://github.com/jamis/rbenv-gemset.git ~/.rbenv/plugins/rbenv-gemset# 通过 gem 命令安装完 gem 后无需手动输入 rbenv rehash 命令, 推荐git clone git://github.com/sstephenson/rbenv-gem-rehash.git ~/.rbenv/plugins/rbenv-gem-rehash# 通过 rbenv update 命令来更新 rbenv 以及所有插件, 推荐git clone git://github.com/rkh/rbenv-update.git ~/.rbenv/plugins/rbenv-update# 使用 Ruby China 的镜像安装 Ruby, 国内用户推荐git clone git://github.com/AndorChen/rbenv-china-mirror.git ~/.rbenv/plugins/rbenv-china-mirror 12345echo 'export PATH=&quot;$HOME/.rbenv/bin:$PATH&quot;' &gt;&gt; ~/.bashrcecho 'eval &quot;$(rbenv init -)&quot;' &gt;&gt; ~/.bashrc#rbenv install 2.3.1 特么什么都能遇到，远端locale和本地不符，提示无法安装 1sudo locale-gen en_US.UTF-8 // or 1sudo dpkg-reconfigure locales 1vim /etc/ssh/ssh_config 注释或删除AcceptEnv LANG LC_* (服务器SSH配置) 然后断开SSH重新登录，不行重启一下服务器，就好了。 1sudo shutdown -r now 继续: 1rebnv install 2.3.1 部署Nginx 1$ sudo apt-get install nginx 其实Nginx也不是必要装的，Node自己可以跑服务！ 1nohup node app.js 但是如果要多域名的话，需要用到Nginx反代，额，这部分还不懂。再去研究下！ 顺便，加一个删除Nginx的步骤: 1234567sudo apt-get --purge remove nginxsudo apt-get autoremovedpkg --get-selections|grep nginx// 罗列出与nginx相关的软件， nginx-common deinstall 然后sudo apt-get --purge remove nginx-common","link":"/install-node-to-Debian/"},{"title":"hexo-generator-feed","text":"前段时间想通过QQ邮箱将自己的写的博客里的文章转发到QQ空间去。众所周知，这种转发似乎只有通过QQ邮箱发邮件转发一条路。要不就只有Ctrl+C，Ctrl+V。我是很厌恶这种方式的。 可是在QQ邮箱里，并没有收到自己从使用hexo以后的任何更新。好吧，我知道问题了，我失去了feed订阅路径。而Google的结果是，hexo自身并不带feed订阅，如果要支持订阅需要安装插件，也就是“hexo-generator-feed\" 这货。 后来，良久之后@lucifr也是这么告诉我的。 前段时间家里有事，离开上海，回来以后又因为工作需要恶补，一直没时间弄！这刚刚才弄好。。。。 说一下这货，其实是蛮简单的，我也就不再细述了，官方文档也将如何安装写的很清晰。自己查阅一下就好。","link":"/hexo-generator-feed/"},{"title":"King of pop 2014","text":"I MISS U.","link":"/king-of-pop-2014/"},{"title":"iReader","text":"这是来自花儿开了的Ajax 习作。iReader 是一个在线的 RSS 阅读器。借助于 Ajax 技术，使页面在无刷新的情况下便可以实时读取 RSS Feed 并将其格式化好并呈现给用户。需要占用一部分网络空间进行上传才可以。代替了我以前应用的第三方服务。。还是放在自己服务器上的东西最实在。不会害怕丢失或者服务器损坏不能应用服务。 这是效果，具体还是去花儿开了的网站去看看吧。","link":"/ireader/"},{"title":"Mac MAX_open","text":"在使用Hexo的过程里，经常会卡在deploy指令上，错误原因之一可能是因为Mac的MAX_open数小的原因，Linux默认为1024，而Mac上只有256，所以只要修改MAX_open数就可以了。指令如下： 1234567891011$ sudo sysctl -w kern.maxfiles=20480kern.maxfiles: 12288 -&gt; 20480$ sudo sysctl -w kern.maxfilesperproc=18000kern.maxfilesperproc: 10240 -&gt; 18000$ ulimit -S -n 2048bubbyroom.com$ ulimit -n2048 其中，$ ulimit -n是用于查看Mac的MAX_open数的指令。只执行修改之前可以先执行此指令查看一下。 后记：在Terminal中修改了MAX_open仅适用于当前窗口，新建Tab，窗口后在新的Tab和窗口里都会失效。","link":"/mac-max_open/"},{"title":"Mac隐藏&#x2F;显示桌面图标","text":"Shell: 1defaults write com.apple.finder CreateDesktop -bool FALSE;killall Finder &amp; 1defaults delete com.apple.finder CreateDesktop;killall Finder AppleScript: 1234567display dialog &quot;桌面图标设置为可见或隐藏?&quot; buttons {&quot;可见&quot;, &quot;隐藏&quot;} with icon 2 with title &quot;Switch to presentation mode&quot; default button 1set switch to button returned of resultif switch is &quot;隐藏&quot; then do shell script &quot;defaults write com.apple.finder CreateDesktop -bool FALSE;killall Finder&quot;else do shell script &quot;defaults delete com.apple.finder CreateDesktop;killall Finder&quot;end if","link":"/mac-False-Desktop-icon/"},{"title":"mac apache","text":"经常忘记命令，还是自己稍微记录一下好，免得每次都去Google。 打开 apache sudo apachectl start 關閉 Apache： sudo apachectl stop 重開 Apache： sudo apachectl restart","link":"/mac-apache/"},{"title":"mac:port occupied","text":"查询: 1$ lsof -i:3000 显示: 12COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEnode 2243 du 12u IPv6 0xc9b8c91a94a8da89 0t0 TCP *:hbci (LISTEN) 结束: 1$ kill -9 2243","link":"/mac-port-occupied/"},{"title":"Mac显示隐藏文件","text":"转载自Mac疯，记录而已，不长用，老是记不住，每次都要Google，郁闷！ 显示文件： defaults write com.apple.finder AppleShowAllFiles -bool true 隐藏文件 defaults write com.apple.finder AppleShowAllFiles -bool false 记得KillAll Finder来重启Finder","link":"/macxian-shi-yin-cang-wen-jian/"},{"title":"「马拉松跑步数据","text":"先引入数据,准备进行分析 1234# 引入数据import pandas as pddata = pd.read_csv('~/data/cbcpv/marathon/marathon.csv')data.sample(5) OUT: age gender split final 19841 34 M 01:55:25 04:50:03 11002 28 W 01:55:00 04:11:00 11619 26 M 01:40:28 04:13:52 4068 34 M 01:38:30 03:30:21 6922 35 M 01:37:44 03:48:37 这个数据集有以下几个特征： age，运动员的年龄 gender，运动员的性别 split，半程所用时间 final，全程所用时间，即最终成绩 自然,要先了解下数据的具体情况 1234567891011121314data.info()# 输出结果&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 37250 entries, 0 to 37249Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 37250 non-null int64 1 gender 37250 non-null object 2 split 37250 non-null object 3 final 37250 non-null objectdtypes: int64(1), object(3)memory usage: 1.1+ MB 可以看到并没缺失值, 不过split和final特征中的数据不实数字类型, 用字符串表示了所用时间长度, 所以我们需要进行转化: 1234import datetimedef convert_time(s): h,m,s=map(int, s.split(':')) return datetime.timedelta(hours=h, minutes=m, seconds=s) 使用完成的方法进行数据转换,我们从新读取一下数据: 123456789101112131415df=pd.read_csv( '~/data/cbcpv/marathon/marathon.csv', converters={ 'split': convert_time, 'final': convert_time })df.dtypes# 输出结果age int64gender objectsplit timedelta64[ns]final timedelta64[ns]dtype: object 这次数据已经转换为timedelta64类型, 下面我们就要转化时间为整数, 一般的做法都是秒或者毫秒数: 1234567d = datetime.timedelta(hours=1, minutes=0, seconds=0)df2 = pd.DataFrame({'time':[d]})df2.astype(int)# 输出结果 time0 3600000000000 我们看到的输出结果,是“纳秒“(ns)单位: \\[1s=10^9ns\\] 我们还需要转化为秒: 1234567d=datetime.timedelta(hours=1, minutes=0, seconds=0)df2=pd.DataFrame({'time':[d]})df2.astype(int) * 1e-9# out time0 3600.0 然后我们要讲split和final两个特征的数据进行转化 123df['split_sec']=df['split'].astype(int) * 1e-9df['final_sec']=df['final'].astype(int) * 1e-9df.sample(5) OUT: age gender split final split_sec final_sec 11725 35 M 0 days 01:53:53 0 days 04:14:19 6833.0 15259.0 19815 24 M 0 days 01:58:45 0 days 04:49:57 7125.0 17397.0 5754 49 M 0 days 01:42:39 0 days 03:41:05 6159.0 13265.0 33166 46 M 0 days 02:31:37 0 days 06:06:17 9097.0 21977.0 9226 36 W 0 days 01:49:06 0 days 04:01:55 6546.0 14515.0 现在多了两个特征split_sec和final_sec, 都是以秒为单位的浮点数. 描述统计 先了解数据: 1df.describe() OUT: age split final split_sec final_sec count 37250.000000 37250 37250 37250.000000 37250.000000 mean 40.697369 0 days 02:03:54.425664429 0 days 04:48:09.303597315 7434.425664 17289.303597 std 10.220043 0 days 00:22:55.093889674 0 days 01:03:32.145345151 1375.093890 3812.145345 min 17.000000 0 days 01:05:21 0 days 02:08:51 3921.000000 7731.000000 25% 33.000000 0 days 01:48:25 0 days 04:02:24 6505.000000 14544.000000 50% 40.000000 0 days 02:01:13 0 days 04:44:25 7273.000000 17065.000000 75% 48.000000 0 days 02:16:11 0 days 05:27:36 8171.000000 19656.000000 max 86.000000 0 days 04:59:49 0 days 10:01:08 17989.000000 36068.000000 居然年龄上最大的数据是86, 让我们看看特征的数据分布: 123%matplotlib inlineimport seaborn as snsax=sns.boxplot(x=df['age']) 这个箱线图反应了, 数据里确实有一些“离群值”. 数据分布 研究下数据分布, 看看split_sec和final_sec 1sns.displot(df['split_sec']) 1sns.displot(df['final_sec']) 整体看来,两个特征下的数据都符合正态分布, 但是final_sec的分布图比较胖. 这次我们把gender这个分类特征添加进来: 1sns.violinplot(x='gender', y='final_sec', data=df) 这些看到, 男性运动员在总体上还是比女性运动员要快一些. 寻找优秀的原因 跑马拉松或者了解这项运动的人都清楚, 运动员很关注整个赛程中前后半程的时间比较,好的选手是后半程用时和前半程近似. 因此, 我们来研究下, 这些运动员前后半程用时情况. 12345g=sns.jointplot('split_sec', 'final_sec', data=df, kind='hex')# 绘制一条直线, 作为参考import numpy as npg.ax_joint.plot(np.linspace(4000, 16000), np.linspace(8000, 32000), ':k') 横坐标是splict_sec特征, 即半程用时. 纵轴表示final_sec特征, 全程用时. 途中可以看出, 的确是越优秀的运动员,前半程用时越接近全程用时的一半, 甚至还有少数后半程跑的更快的. 我们做个计算来深入研究下: 12df['split_frac']=1-2*df['split_sec']/df['final_sec']df.sample(5) OUT: age gender split final split_sec final_sec split_frac 2065 35 W 0 days 01:31:41 0 days 03:14:40 5501.0 11680.0 0.058048 9001 43 W 0 days 01:58:19 0 days 04:00:44 7099.0 14444.0 0.017031 30039 34 M 0 days 02:25:17 0 days 05:39:21 8717.0 20361.0 0.143755 27456 62 W 0 days 02:13:28 0 days 05:25:01 8008.0 19501.0 0.178709 13335 41 M 0 days 01:45:36 0 days 04:21:00 6336.0 15660.0 0.190805 用直方图再增加一个参考线来看看split_frac特征中的数据分布: 1234import matplotlib.pyplot as pltsns.displot(df['split_frac'], kde=False)# 垂直于 x 轴的直线，0 表示 x 轴位置plt.axvline(0, color='k', linestyle='--') 从这张图中, 更清晰的看到全体参赛者的运动安排. 再来探究下不同特征之间的关系: 12345sns.pairplot( data=df, vars=['age','split_sec','final_sec','split_frac'], hue='gender') 让我们来看下80岁选手的数量: 1234(df.age&gt;=80).sum()# OUT15 下面, 我们划分下年龄段,看看各年龄段的成绩分布: 12345678910df['age_dec']=df['age'].map(lambda age: 10*(age//10))sns.violinplot( x='age_dec', y='split_frac', hue='gender', data=df, split=True, inner='quartile', palette=['lightblue', 'lightpink']) 看这张图, 我们发现,不同性别的运动员的split_frac特征数据分布中, 年龄越大,前后端的时间分布比相对集中. 再看看全程用时分布比较: 123456789sns.violinplot( x='age_dec', y='final_sec', hue='gender', data=df, split=True, inner='quartile', palette=['lightblue', 'lightpink']) 从30岁往后, 明显年纪越大,用时越长.","link":"/marathon/"},{"title":"NFC和O2O","text":"总体来说,这是我最看好的移动应用之一! 注意,是之一. 首先,我要说的是SNS网站真的没有再大的作为了!人们已经开始从那些疯狂中开始慢慢冷却了下来,而真正实际用到SNS网站的人也是小部分而已.造势,炒作,宣传,营销...当然,我不能否认这些平台在作为传统媒体的延伸甚至是替代品上的功效. 可是又真有多少人会真的坚持活在这种亢奋的状态下.除非有大批的粉丝不停的为自己打鸡血吧? 所以我总认为,移动社交产品可以打住了.至于查找附近聊友的功能,诸如微信以及添加新功能的米聊以及后来居上的一些产品,不要看宣传的标语很美,实际上还不是沦为宅男腐女查找就近炮友的必备品而已! 好吧,对SNS产品的吐槽结束了,我真正要说的是NFC以及O2O这两个东西! 首先我们需要扫盲一下,NFC就是Near Field Communication,翻译过来就是近距离无线通信，是一种短距离的高频无线通信技术，允许电子设备之间进行非接触式点对点数据传输，在十厘米（3.9英吋）内，交换数据。这个技术由免接触式射频识别（RFID）演变而来.而我们平时所用的交通卡等一类射频卡种,都是RFID技术. &lt;而O2O(Online to Offline)就不是什么技术了,而是一种互联网商业模式,就是把线上的消费者带到现实中的商店中去在线支付购买线下商品和服务,再到线下去享受服务. 而实际上,团购,就是一种O2O模式.可是我心里的O2O模式,完全不是像团购那样的垃圾.这种模式在中国,已经早早的走入了死胡同,一条死路!我所要说的O2O,是一种集合所有商家优惠折扣的消费资源,并一卡通吃的卡片发行商. 而也终于让我遇到一个,这就是么卡.有兴趣的可以去看看,我可没有要推销么卡的意思,也无意为他写什么行销软文.只是在这里纯探讨而已! 好吧,下面让我们实际展望一下,现在很多手机已经开始内置NFC技术.比如iPhone 4S,三星盖世兔等等,那么,你能理解我说什么了,我们以后也就完全可以不必身上大卡小卡的带在身上,一个手机就OK了.交通卡,银行卡,还有各大商场卖场的贵宾卡折扣卡,都可以完完全全的装载我们的手机里! 这其实已经应该不是什么新技术,手机代替交通卡和信用卡,日本早就开始做先行者了!而实际上,O2O模式在中国也才起头而已,而重点,并不是技术怎样怎样,而是线下的资源掌握的如何.要不团购团队中实际上最大的是营销团队呢. 而像么卡这样将所有会员卡全部装到手机里的模式,一样取决于,谁掌握了最大的市场!当你的卡通吃了全世界大大小小的商家卖场的时候,相信所有人都愿意只带着一部手机走遍天下的感觉! 好了,展望结束...大家想去吧,哈哈!","link":"/nfc-and-o2o/"},{"title":"不要新版Google Reader","text":"什么都不想多说,只想请求Google能恢复旧版的Google Reader以及分享方式! 我承认自己是一个Google的重度用户,但是并不代表没有其替代!现在看来除了Gmail以外,没有什么Google产品是不能被其他产品代替的! 我相信以前Google所谓的不作恶名言确实是出自对于自己充分的自信,而现在的作恶是为了什么,难道是对自己公司开始不自信了? 这样的做法只能让忠实的用户离开Google,而不能为了GPlus增加用户,纯粹的短视! 所以,在此请求Google能恢复旧版的Reader界面以及分享方式! From --- 一个重度Google用户","link":"/no-new-google-reader/"},{"title":"nitrous.io","text":"世界，真心是美好的啊！ 好吧，再使用nitrous之前，我一直都不知道，原来世界可以如此美好。 不需要Dropbox君，不需要git的master了，而且可以随时随地hexo。 妈妈再也不用担心我的md文档丢失了。。。 这货不仅仅是node.js,还有Rails，python以及Go环境哦。。。 亲们，还在等什么。如果windows下的Ruby环境配置让你想砸机子的冲动，那么赶快来nitrous.io吧！","link":"/nitrousio/"},{"title":"Note about my ubuntu","text":"这是我为了便于自己以后方便而记录的一些关于我ubuntu上设置所需要的内容！当然其中内容都是通用的，只是如果你们要拿去用的话记得将路径以及一些变量变成你自己的！ about system $ sudo gedit /etc/apt/sources.list 12345678910deb http://mirrors.163.com/ubuntu/ karmic main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-bac. kports main restricted universe multiverse $ sudo apt-get update and $ sudo apt-get upgrade and hosts $ gedit /etc/hosts # My Dropbox have new hosts file about ATI Donload form This link 12cd **sudo sh **.run That's ok about Keepass 2 so...apt-get 123$ sudo apt-add-repository ppa:jtaylor/keepass$ sudo apt-get update$ sudo apt-get install keepass2 10.04 所需软件包 1 install mono #[link](http://mono-project.com/DistroPackages/Ubuntu) 1234567Click on &quot;System&quot;, &quot;Administration&quot;, &quot;Software Sources&quot;.Click on the &quot;Other Software&quot; tab.Click on &quot;Add...&quot;, and enter the line:**deb http://badgerports.org lucid main**Click on &quot;Add Source&quot;Click on &quot;Authentication&quot;, then on &quot;Import Key File&quot;Download this [GPG key file](http://badgerports.org/directhex.ppa.asc), ID 0E1FAD0C, and select it in the &quot;Import Key File&quot; windowClick on &quot;Close&quot;, then &quot;Reload&quot; when the pop-up appears. You're all set! 2 $ sudo apt-add-repository ppa:jtaylor/keepass 3 $ sudo apt-get update 4 $ sudo apt-get install keepass2 about Java Download from This link and 1234$ cd **$ tar **.tar.gz(64bit)$ sudo update-alternatives --install &quot;/usr/bin/java&quot; &quot;java&quot; &quot;/home/hivan/software/jdk1.7.0_04/bin/java&quot; 1$ sudo update-alternatives --config java 其实就是配置一下默认路径就OK了！ 或者以下方法： $ gedit ~/.bashrc 末尾添加变量 1234JAVA_HOME=/home/hivan/software/jdk1.7.0_04JRE_HOME=${JAVA_HOME}/jreCLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libPATH=${JAVA_HOME}/bin:$PATH and 123456789$ source ~/.bashrc$ sudo update-alternatives --install /home/hivan/software/jdk1.7.0_04/bin/java 300 $ sudo update-alternatives --install /home/hivan/software/jdk1.7.0_04/bin/javac 300 $ sudo update-alternatives --install /home/hivan/software/jdk1.7.0_04/bin/jar 300 $ sudo update-alternatives --config java #用于替换Java，可能会跳出选择框$ java -version #测试java version &quot;1.7.0_04-ea&quot;Java(TM) SE Runtime Environment (build 1.7.0_04-ea-b19)Java HotSpot(TM) 64-Bit Server VM (build 23.0-b20, mixed mode) about goagent 1234567$ cd$ mkdir bin$ cd bin建立一个proxy.sh file$ gedit proxy.sh输入： python /***/goagent/local/proxy.py save and quit$ chmod 700 proxy.sh about ruby 12345678$ sudo apt-get install apache2 curl git libmysqlclient-dev mysql-server nodejs$ bash -s stable &lt; &lt;(curl -s https://raw.github.com/wayneeseguin/rvm/master/binscripts/rvm-installer)$ echo '[[ -s &quot;$HOME/.rvm/scripts/rvm&quot; ]] &amp;&amp; . &quot;$HOME/.rvm/scripts/rvm&quot; # Load RVM function' &gt;&gt; ~/.bashrc$ source .bashrc$ rvm requirements$ sudo apt-get install build-essential openssl libreadline6 libreadline6-dev curl git-core zlib1g zlib1g-dev libssl-dev libyaml-dev libsqlite3-0 libsqlite3-dev sqlite3 libxml2-dev libxslt-dev autoconf libc6-dev ncurses-dev automake libtool bison subversion$ rvm install 1.9.3$ rvm 1.9.3 –-default #有可能出错**RVM is not a function, selecting rubies with 'rvm use ...' will not work.** 则执行：`$ rvm alias create default 1.9.3 about Rails 12$ gem install bundler rails rdoc #rdoc 为Octopress所需组建$ rails -v #如果“**程序“rvm”尚未安装。**”则检查一下.bashrc的路径配置 about python 123456789$ python -VPython 2.6.6$ curl -kL http://github.com/utahta/pythonbrew/raw/master/pythonbrew-install | bash$ . $HOME/.pythonbrew/etc/bashrc$ pythonbrew install 2.7.1$ pythonbrew switch 2.7.1Switched to Python-2.7.1$ python -VPython 2.7.1 ubuntu 12中默认是2.7，如果要安装3.2和上述步骤一样！改一下版本号 about Sublime Text 2 12345678910#!/usr/bin/env xdg-open[Desktop Entry]Name=Sublime Text 2Comment=Sublime Text 2Exec=/home/hivan/software/&quot;Sublime Text 2&quot;/sublime_textIcon=/home/hivan/software/Sublime Text 2/Icon/128X128/sublime_text.pngTerminal=falseType=ApplicationCategories=Application;Development;StartupNotify=true 设置权限：可执行文件！ and 12$ cd /home/hivan/software/&quot;Sublime Text 2&quot;/$ sudo cp &quot;Sublime Text 2.desktop&quot; /usr/share/applications Ctrl+` 1import urllib2,os;pf='Package Control.sublime-package';ipp=sublime.installed_packages_path();os.makedirs(ipp) if not os.path.exists(ipp) else None;open(os.path.join(ipp,pf),'wb').write(urllib2.urlopen('http://sublime.wbond.net/'+pf.replace(' ','%20')).read()) Ctrl+Shift+P 1 ZenCoding 2 Alignment 3 Markdown 4 setting user { \"ignored_packages\": [] } download setting from this link Finally, input right, you can only choose scim,so... 12$ sudo apt-get install scim$ sudo apt-get install scim-pinyin ... 12scim设置－&gt;全局设置－&gt;将预编辑字符串嵌入到客户端中勾去掉scim设置-&gt;gtk-&gt;嵌入式候选词标 勾去掉 about Retext 123sudo add-apt-repository ppa:mitya57sudo apt-get updatesudo apt-get install retext or This link about Octopress Go to This Link add and remove ppa 12$ sudo add-apt-repository ppa:name/name$ sudo add-apt-repository -r ppa:name/name Temporary end, to be continued...","link":"/note-about-my-ubuntu/"},{"title":"拿福能千人挑战活动","text":"如何加入: 在之前@mg12 那里有看到一个博客广告.右下角的地方有一个拿福能的广告. 就我所见,吴钊童鞋很少会在页面上添加广告的,除非他认为可以做的.这和我的页面上那么多广告不同.所以他的页面一直很清爽!这次少见的添加了拿福能,我想一定有些搞头.所以就去注册了,并且,我从@Denis那里也看到了相应的广告,还看到了一篇拿福能博主北京聚会的Post,而且对其大加赞赏!So,我想,这个广告提供商有搞头. 拿福能是做什么的 总的说来,拿福能是一个广告提供商,可以称作网络营销公司.说起来Feedsky以前做过付费软文大家应该都不陌生了.不过我知道众多的博主都被Feedsky的付费评论给搞伤了,很难相信同类的公司能做的起来.自然,我也有些将信将疑.不过对于博主们来说,有多一份的收入总比没有的强.在国内,写独立博客确实是一件非常让人心酸的事情,在国外独立博主获得丰厚的回报的时候,我们从几年前就开始在探讨国内博客的生存之道,到今天为止仍然还是没有找到合适的出路.那么这次拿福能既然为博主们提供了如此好的一个机遇,大家不妨做做看! 并且,拿福能有一些和其他不同的是,他们有一些线下的付费活动.就如@mg12 介绍的,他们会提供一些电影或者之类的观赏活动,然后在博主回来之后写影评或者剧透.当然,这基本属于一种电影宣传活动!更重要的是,有无数的独立博主可以相互交流.不管你是哪一类的博客,总有一个主题是适合你的! 活动内容 据说,拿福能在国外已经做了好几年了.并且有一些非常成功的案例,而此次开发国内市场,当然是期望在国内也有所作为.所以搞了这么一个千人挑战活动,分发50,000元奖金来招募一千个博主.届时,人满之后这一千个博主将会平分这50,000元奖金. 有兴趣的,可以去拿福能的千人挑战活动主页参看!","link":"/nuffnang-challenge-1000/"},{"title":"Pageflakes","text":"Pageflakes 是一个出色的 Web 2.0 Personal Portal。类似与 Google IG 和 Live.com 的东西，但我个人觉得风格简洁的很得体，介于 Google IG 的光秃秃和 Live.com 过于花哨之间，不仅给访客带来不错的视觉体验,而且使页面的载入速度也有不小提升。 Pageflakes 预设了 150+ 个 Flakes，这个 Flake 就相当于 Google IG 和 Live.com 的 Gadget，很丰富也很实用，特别是那个 TO-DO-LIST Flakes，让人爱不释手（有没有类似的桌面工具，麻烦推荐一下？）。Pageflakes 支持自定义 RSS，同时个性页面同 Live.com 一样支持多页，这样我们可以将 Flakes 和 Feeds 分好类，一页一页的看，很方便。需要说明一点的是，这个东西仅仅支持在IE浏览器下，我使用firefox的内核开始根本不能访问，可以自动利用ietab跳到了IE内核上。。。。。而且sign up和login形同虚设。。。点击sign up后出现的提示,这个东西似乎只只是同一电脑下的为默认同一用户。不知道在重新安装系统以后会如何。。。。。。。有内行的指教一下。","link":"/pageflakes/"},{"title":"pokemon","text":"引入依赖和数据 123456789import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as pltimport ospath = os.path.expanduser('~/data/cbcpv/pokemon/')df = pd.read_csv(path + 'pokemon.csv', index_col=0, encoding='cp1252') 探索数据 1df.sample(5) OUT: Name Type 1 Type 2 Total HP Attack Defense Sp. Atk Sp. Def Speed Stage Legendary # 75 Graveler Rock Ground 390 55 95 115 45 45 35 2 False 82 Magneton Electric Steel 465 50 60 95 120 70 70 2 False 79 Slowpoke Water Psychic 315 90 65 65 40 40 15 1 False 123 Scyther Bug Flying 500 70 110 80 55 80 105 1 False 9 Blastoise Water NaN 530 79 83 100 85 105 78 3 False 对比并了解下数据集的各个特征类型: 12345678910111213141516171819202122df.info()# OUT&lt;class 'pandas.core.frame.DataFrame'&gt;Int64Index: 151 entries, 1 to 151Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 151 non-null object 1 Type 1 151 non-null object 2 Type 2 67 non-null object 3 Total 151 non-null int64 4 HP 151 non-null int64 5 Attack 151 non-null int64 6 Defense 151 non-null int64 7 Sp. Atk 151 non-null int64 8 Sp. Def 151 non-null int64 9 Speed 151 non-null int64 10 Stage 151 non-null int64 11 Legendary 151 non-null bool dtypes: bool(1), int64(8), object(3)memory usage: 14.3+ KB 可以看到Type 2这个特征有缺失值, 其他的没有, 而且显示的为正数型, 很符合数据分析的要求. 接下来用散点图研究特征Attack和 Defense的关系 1234567sns.lmplot( x='Attack', y='Defense', data=df, fit_reg=False, hude='Stage') 我们这里参数使用了fit_reg=False, 隐藏了回归线. 在Seaborn中是没有单独绘制散点图的方法的,但是通过参数设置,实现了散点图的绘制.如果此参数设置为True 接下来用箱线图看下各特征数据分布: 1sns.boxplot(data=df) 这个结果显示出, Total, Stage以及Legendary特征的数据是不适合在这里绘制散点图的, 需要对特征进行适当选择 12stats_df=df.drop(['Total', 'Stage', 'Legendary'], axis=1)sns.boxplot(data=stats_df) 这样,比较清晰的看出几个特征的数据分布情况了, 非数字的特征自动摒弃. 在研究Seaborn, 我们知道还有用i中研究数据分布的函数sns.violinplot, 我们尝试用它绘制特征Attack相对于特征Type 1的数据(这是一个分类行特征)的分布. 123456df['Type 1'].unique()# OUTarray(['Grass', 'Fire', 'Water', 'Bug', 'Normal', 'Poison', 'Electric', 'Ground', 'Fairy', 'Fighting', 'Psychic', 'Rock', 'Ghost', 'Ice', 'Dragon'], dtype=object) 上面显示了特征Type 1中唯一数据, 即数据的值. 123456789101112131415161718192021222324252627282930313233343536373839sns.set( style='whitegrid', rc={ 'rigure.figsize':(11.7, 8.27) # 设置了画布的尺寸 })pkmn_type_colors=[ '#78C850', # Grass '#F08030', # Fire '#6890F0', # Water '#A8B820', # Bug '#A8A878', # Normal '#A040A0', # Poison '#F8D030', # Electric '#E0C068', # Ground '#EE99AC', # Fairy '#C03028', # Fighting '#F85888', # Psychic '#B8A038', # Rock '#705898', # Ghost '#98D8D8', # Ice '#7038F8', # Dragon]sns.violinplot( x='Type 1', y='Attack', data=df, inner=None, # 去掉提琴图中的竖线 palette=pkmn_type_colors)sns.swarmplot( x='Type 1', y='Attack', color='k', # 数据的点的颜色 alpha=0.7 )plt.title('Attack by Type') pkmn_type_colors是一个列表, 列出的颜色对应着特征Type 1中的唯一值. 因为去掉了提琴图内部的竖线,所以整个图没有太乱, 想知道有竖线的是什么样子, 可以注释掉inner=None这个参数. 之前我们删除了三个特征得到了一个变量stats_df引用的数据集: 1stats_df.sample() OUT: Name Type 1 Type 2 HP Attack Defense Sp. Atk Sp. Def Speed # 128 Tauros Normal NaN 75 100 95 40 70 110 数据结果中看出来, 特征HP Attack Defense Sp.Atk Sp.Def Speed都是整数, 在df.info()中也能看出来.现在有需求, 如果把这些特征分布进行可视化, 而且要放到一个坐标系中进行比较? 参考: 先使用pd.melt函数, 将所指定的特征进行归并 123456melted_Df=pd.melt( stats_df, id_vars=['Name', 'Type 1', 'Type 2'], # 保留的特征 var_name='Stat' # 其余特征规定到这一列内)melted_df.sample(10) OUT: Name Type 1 Type 2 Stat value 291 Kabutops Rock Water Attack 115 406 Marowak Ground NaN Defense 110 821 Machoke Fighting NaN Speed 45 129 Gyarados Water Flying HP 95 281 Lapras Water Ice Attack 85 586 Vaporeon Water NaN Sp. Atk 110 483 Nidoqueen Poison Ground Sp. Atk 75 93 Gengar Ghost Poison HP 60 791 Vulpix Fire NaN Speed 65 481 Nidoran‰ªÛ Poison NaN Sp. Atk 40 这样,在melted_df数据集中的Stat特征中的数据就是分类数据, 值是stats_df中被归并的特征名称. 12345melted_df['Stat'].unique()# OUTarray(['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed'], dtype=object) 在此基础上, 我们绘制反应分类特征数据分布的图示. 12345sns,swarmplot( x='Stat', y='value', data=melted_df) 还可以在此基础上,再叠加一层分类: 1234567sns.swarmplot( x='Stat', y='value', data=melted_df, hue='Type 1' # 叠加一层分类)plt.legend(bbox_to_anchor=(1, 1), loc=2)","link":"/pokemon/"},{"title":"picasaweb","text":"这一下就经过了很多日子，本来前些天是想写一些教程的。 现在脑子里的想法大多都被时间冲淡了。只是随便写写吧。 由于我的机子重新装了系统的原因。迅雷里的原始连接我也找不到了，有兴趣的朋友可以自己去找找看 picasa2和picasaweb有很多的不同，最大的不一样大概就是现在没有推出中文版本。一切都是E文。自信能看得懂的朋友可以下载来试试。 picasaweb是和自己的google账号绑定的。google的picasaweb服务在网上有250M的存储空间。可以利用picasaweb直接上传到自己的账号上。我去试验了一下，服务很不错，而且有很多新的功能，可惜的是不支持外连，估计快可以了，因为每一张图片都有自己的地址。 Picasaweb和原版本还有很多新加的功能。其中我比较感兴趣的就是色相搜索。输入color:xxx(比如red)就会有相应色相的相片被搜索出来。效果大家可以自己下载以后自己看看@。在我使用过程中，虽然E文给我找了不少麻烦，但是总体上感觉这次的升级还是有很多实用的功能在里边。 好了，软件谈完了，说说最近看的电影。。我这个人不喜欢在网上下载，质量太侮辱我眼睛。所以我一般都是等到碟版出来以后买回来看。。。。等到现在才看了达芬奇密码和碟三，但是我并不是要向大家说这两部片子，而是我看的另外一部:《东京审判》这是一部设计历史题材和政治题材的片子。1945年日本签署投降书的以后，联盟国法官团赶赴东京对日本的20多位战犯进行审判。长达817次审判和最后一次宣判共818次的审判历程。 印象最深的是曾志伟饰演的日本人。从他嘴里说出来的:狗日的日本鬼子。 不知道这句话是因为深受其害还是因为别的原因。再怎么样，一个日本人不会如此辱骂自己的国家吧。就象中国人自己，如果说了一句中国猪，反映会是怎样的？ so。。。最后还是不得不感叹战争带来的危害。所有的一切，也都仅仅是战争引起来的而已 不过再如何，日本到现在为止的历史观确实让我恶心。我到现在才知道靖国神社原来一共供奉着2000多位战犯，而其中有7位是甲级战犯，而最大的也是最可恶的战端挑起者东条英机，也被供奉在里边。这，是对人类文明的一种侮辱。 OK。。。写到这里了。。该去看书了","link":"/picasaweb/"},{"title":"解决Mac M1 原生Photoshop找不到CEP扩展面板","text":"研究这个问题, 也属于是撞上了! 我现在使用的PS版本为: 在Photoshop内, 我画画时一直使用的是第三方的色轮插件Coolorus, 长这样: 好用与否, 可以说, 谁用谁知道. 前些日子发现Photoshop2022有M1原生版本, 不用再使用转译版本, 我心想, 应该速度上会快很多吧! 兴奋的更新完后才发现, Coolorus面板无论如何找不到了, 原本应该在窗口下的“扩展(旧版)”菜单也找不到. 无论怎么折腾都不行, 而且设置面板里的增效工具也是灰色无法设置: 正当我心灰意冷准备返回2021时, 忽然想到, Adobe早就在PS中启用UXP插件了, 而CEP插件因为历史原因一直也无法完全取消, 那么既然是早就做的事情, 为什么2022版本里全给抹杀了呢, 重点是, 面板里设置项虽然不可点击, 但是还在? 问题应该不是出在版本上, 而是出在M1原生的问题上, 我试着去设置了转译, 再次重新打开PS, 果然不出所料, 扩展(旧版)菜单又回来了. 好吧, 这回知道是怎么回事了, 也就好解决了. 方法一: 返回PS 22.21版本, 简单粗暴 方法二: 讲PS2022设置为Rosetta转译打开方式, 同样简单粗暴! 回答一下可能大家问到的问题: 速度上, 感觉不出有什么太大的变化 是的, Coolous照样无法使用, 我快崩溃了, 正在纠结到底是放弃Coolous使用Photoshop原始色轮, 还是回到2021 反正问题是这么个问题, 解决方案也有了!大家自行抉择吧!","link":"/ps-find-cep-for-m1/"},{"title":"重裝「Yosemite」","text":"重裝系統之後，很多東西需要重裝，特別是開發環境。而開發環境的先後順序和設置，一直是我頭疼的事情。這次就着從新安裝了一遍，把很多東西都記錄下來。之前那個環境被我搞的亂七八糟，並且恢復不回來了。 多大多數次序參照 @mrzhang 系統偏好設置 更改電腦名稱 共享 允許安裝任何來源APP 安全性與隱私 --》通用 設置快捷鍵 鍵盤 --》 快捷鍵 配置VPN以及SSH 很重要，因爲很多源都在牆外了 安裝輸入法 下載並安裝\"Squirrel\" 下載並安裝\"SCU\" 安裝Sublime Text 3 設置package Ctrl+ , and setting: 12345678910111213141516171819202122232425262728293031{ &quot;caret\\_style&quot;: &quot;phase&quot;, &quot;color\\_scheme&quot;: &quot;Packages/Color Scheme - Default/Solarized (Light).tmTheme&quot;, &quot;font\\_face&quot;: &quot;Monaco&quot;, &quot;font\\_size&quot;: 13.0, &quot;hightlight\\_line&quot;: true, &quot;hightlight\\_modified\\_tabs&quot;: true, &quot;ignored\\_packages&quot;: [ &quot;Vintage&quot; ], &quot;indent\\_to\\_bracket&quot;: true, &quot;draw\\_centered&quot;: false, //居中显示 &quot;line\\_numbers&quot;: true, //显示行号 &quot;gutter&quot;: true, //显示行号边栏 &quot;fold\\_buttons&quot;: true, //显示折叠按钮 &quot;fade\\_fold\\_buttons&quot;: true, //始终显示折叠按钮 &quot;rulers&quot;: [], //列显示垂直标尺，在中括号里填写数字，宽度按字符计算 &quot;spell\\_check&quot;: false, //拼写检查 &quot;hot\\_exit&quot;: true, //保留未保存内容 &quot;line\\_padding\\_bottom&quot;: 1, &quot;line\\_padding\\_top&quot;: 1, &quot;scroll\\_past\\_end&quot;: true, //文本最下方缓冲区 &quot;tab\\_size&quot;: 2, // Tab制表宽度 &quot;translate\\_tabs\\_to\\_spaces&quot;: true, //缩进和遇到Tab键用空格替代 &quot;wide\\_caret&quot;: true, &quot;word\\_wrap&quot;: true, &quot;match\\_tags&quot;: true, //HTML下突出显示光标所在标签的两端。 &quot;match\\_selection&quot;: true, //全文高亮当前选中字符 &quot;wrap\\_width&quot;: 80} 編輯設置 /etc/paths 123456/usr/local/bin/usr/local/sbin/usr/bin/usr/sbin/bin/sbin 安裝Xcode 1xcode-select --install 安裝Homebrew 1ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/homebrew/go/install)&quot; PS: 這裏可能會很長時間的等待 設置Sublime終端鏈接 1ln -s /Applications/Sublime\\ Text.app/Contents/SharedSupport/bin/subl /usr/local/bin/sm Git, autojump 1brew install git autojump Oh My Zsh 1curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | sh 設置 ~/.zshrc: 1234export NVM\\_NODEJS\\_ORG\\_MIRROR=&quot;http://npm.taobao.org/dist&quot;[[ -s &quot;$HOME/.nvm/nvm.sh&quot; ]] &amp;&amp; . &quot;$HOME/.nvm/nvm.sh&quot;export NODE\\_PATH=$NVM\\_DIR/$(nvm\\_ls current)/lib/node\\_modules 安裝 NodeJS 12nvm install 0.11.15nvm alias default 0.11.15 安裝rbenv 123456789git clone git://github.com/sstephenson/rbenv.git \\~/.rbenv# 用来编译安装 rubygit clone git://github.com/sstephenson/ruby-build.git \\~/.rbenv/plugins/ruby-build# 用来管理 gemset, 可选, 因为有 bundler 也没什么必要git clone git://github.com/jamis/rbenv-gemset.git \\~/.rbenv/plugins/rbenv-gemset# 通过 gem 命令安装完 gem 后无需手动输入 rbenv rehash 命令, 推荐git clone git://github.com/sstephenson/rbenv-gem-rehash.git \\~/.rbenv/plugins/rbenv-gem-rehash# 通过 rbenv update 命令来更新 rbenv 以及所有插件, 推荐git clone https://github.com/rkh/rbenv-update.git \\~/.rbenv/plugins/rbenv-update 設置~/.zshrc 12export PATH=&quot;$HOME/.rbenv/bin:$PATH&quot;eval &quot;$(rbenv init -)&quot; 其他 安裝Ruby 1234rbenv install -l # list all available versionsrbenv install 2.1.5 # install a Ruby versionrbenv global 2.1.5 # set the global versionrbenv versions # list all installed Ruby versions 配置gem源 1234gem sources -a http://ruby.taobao.org/ -r https://rubygems.org/echo 'gem: --no-document' \\&gt;\\&gt; \\~/.gemrcgem updategem update --system 安装 MongoDB, MySQL 1brew install mongodb mysql 設置開機自啓動「可選」 123mkdir -p \\~/Library/LaunchAgentsln -sfv /usr/local/opt/mongodb/\\*.plist \\~/Library/LaunchAgentsln -sfv /usr/local/opt/mysql/\\*.plist \\~/Library/LaunchAgents 安装 Pow 12curl get.pow.cx | shgem install powder Powder 是一套管理工具 SSH-KeyGen 12ssh-keygen -t rsacat \\~/.ssh/id\\_rsa.pub 安裝Rails, sass, compass 以及 hexo 12gem install rails sass compassnpm install -g hexo 安裝必要工具 1234gem install mysql2gem install capistranogem install capistrano-ext Snippets - Download 安裝其他APP","link":"/reinstall-mac-osx-Yosemite/"},{"title":"QQ微博邀请自助贴","text":"请将使用过的地址留言贴出来,方便别人啊!谢谢... -----------------------无耻分割线 http://t.qq.com/invite/b11fba23eb7660c2c261 http://t.qq.com/invite/bcd728928b5aec7aa341 -------20100502---------- http://t.qq.com/invite/b39659769ad1bf81f6df http://t.qq.com/invite/7bcb9d4450de2a05e0fb http://t.qq.com/invite/ae7cb0aae3a651365a71 -------------20100524------------------ http://t.qq.com/invite/7bcb9d4450de2a05e0fb http://t.qq.com/invite/bfc518a208c9f3dc0a03 http://t.qq.com/invite/d8c50c6c10beb588554d http://t.qq.com/invite/3a37bacfc981cfde2d59 http://t.qq.com/invite/5bfd6e3d075ac7f9f0cb http://t.qq.com/invite/19baed1f205b3f3e5173","link":"/qq-microblogging-invite-self-stick/"},{"title":"How to set up networkx in Chinese","text":"Problem description Hi, everynone, when we use networkx to display Chinese, we will find that Chinese cannot be displayed. Solution Download the font in the attachment; https://github.com/hivandu/practise/blob/master/resource/SimHei.ttf Execute in jupyter notebook 12import matplotlibprint(matplotlib.__path__) Find the path to matplotlib, and then cd to this path, after cd to this path, continue cd, cd to the path map-data/fonts/ttf. Then replace the file DejaVuSans,ttf with the file we just. 1$ mv SimHei.ttf nx.draw(city_graph, city_location, with_labels = True, node_size = 10).ttf Among them, the ttf font used. I have uploaded it to everyone.","link":"/set_chinese_for_networkx/"},{"title":"在Yosemite中设置Pow","text":"在Yosemite中，Pow安装和启动是有问题的。这是因为ipfw被移除了，所以如果要在Yosemite中跑Pow，需要做些设置才可以。 1, 添加文件com.pow到/etc/pf.anchors/目录内 sudo vim /etc/pf.anchors/com.pow 2, 在文件内添加代码: 1234rdr pass on lo0 inet proto tcp from any to any port 80 -&gt; 127.0.0.1 port 20559rdr pass on en0 inet proto tcp from any to any port 80 -&gt; 127.0.0.1 port 20559rdr pass on en9 inet proto tcp from any to any port 80 -&gt; 127.0.0.1 port 20559 NOTE: 代码后一行必须要有换行符，否则会出现语法错误 3, 打开文件/etc/pf.conf 4, 添加代码: rdr-anchor \"pow\"，需要添加到rdr-anchor \"com.apple/*\"下一行 5, 打开文件/etc/pf.anchors/com.apple, 并添加代码: 12load anchor &quot;pow&quot; from &quot;/etc/pf.anchors/com.pow&quot; NOTE: 一样必须有换行符 6, 终端执行: sudo pfctl -f /etc/pf.conf 7, 好了，现在可以打开pf了: sudo pfctl -e","link":"/setting-pow-at-Yosemite/"},{"title":"模拟器的app2sd","text":"一个读者@XXX(因为个人意愿隐掉名字) 发mail询问我关于windows下模拟器app2sd的问题,先不说有没有必要,说一下我测试的结果! 本来我给他回邮件是让他试一下apptosd.apk的,后来越想越不对劲,所以自己做了下测试!结果如下: 然后通过adb shell也无法操作. # mkdir /system/sd/app mkdir /system/sd/app mkdir failed for /system/sd/app, No such file or directory 不过仔细想想,app2sd必须满足的条件我们在windows上根本就不存在,首先我们必须要一个app2sd的支援固件!然后我们需要sdcard分出一个ext2的分区... 而这两个条件全部都不满足!那基本上可以说没有办法! 看以后有没有高手可以实现模拟器上安装修改固件,那么可以安装一个app2sd的固件,而另外一个必须满足的条件就是必须将建立的虚拟sdcard分出一个ext2分区来! 满足了这两个条件,那么所有的都会水到渠成! BTW:下午这位读者给我的回复: 非常感谢！ 我测试的结果跟您是一样的，不过后面那个建目录的不一样。下面是我对您博文上的一点分析。 然后通过adb shell也无法操作. 【XXX】google好象改过linux内核，adb shell登录以后几种命令都有权限限制。在虚拟sd卡上建立文件夹有所有权限，然而用adb push传上去的就没有可执行的权限，使用chmod命令修改权限也不成功。 # mkdir /system/sd/app 12mkdir /system/sd/appmkdir failed for /system/sd/app, No such file or directory 【XXX】在system目录下adb shell 命令是没有写权限的。你这个尝试如果是mkdir /system/sd就会报“mkdir failed for sd, Read-only file system”的错误，但是在data目录下就能够创建目录。 不过仔细想想,app2sd必须满足的条件我们在windows上根本就不存在,首先我们必须要一个app2sd的支援固件!然后我们需要sdcard分出一个ext2的分区… 而这两个条件全部都不满足!那基本上可以说没有办法! 看以后有没有高手可以实现模拟器上安装修改固件,那么可以安装一个app2sd的固件,而另外一个必须满足的条件就是必须将建立的虚拟sdcard分出一个ext2分区来! 【XXX】appsd的固件这个是什么概念？用mksdcard创建的虚拟sdcard不就是对应的手机上的sdcard么？虚拟sdcard为什么要分出一个ext2分区呢？ext2分区一个什么概念，sdcard要分出ext2分区的原理是什么？ 能否简单介绍一下？或者介绍一下相关的资料？谢谢。 另：对您给我传的那个apk我不是很了解，这个文件从哪里来的，它都做了些什么事？ 下面是我adb shell后ls –l查看到的各文件夹权限，是有加载虚拟sdcard的。 123456789101112131415drwxrwxrwt root root 2009-08-10 04:45 sqlite_stmt_journalsdrwxrwx--- system cache 2009-07-21 09:01 cached---rwxrwx system system 2009-08-10 04:52 sdcardlrwxrwxrwx root root 2009-08-10 04:45 etc -&gt; /system/etcdrwxr-xr-x root root 2009-05-15 00:53 systemdrwxr-xr-x root root 1970-01-01 00:00 sysdrwxr-x--- root root 1970-01-01 00:00 sbindr-xr-xr-x root root 1970-01-01 00:00 proc-rwxr-x--- root root 9075 1970-01-01 00:00 init.rc-rwxr-x--- root root 1677 1970-01-01 00:00 init.goldfish.rc-rwxr-x--- root root 106568 1970-01-01 00:00 init-rw-r--r-- root root 118 1970-01-01 00:00 default.propdrwxrwx--x system system 2009-05-15 00:58 datadrwx------ root root 1970-01-01 00:00 rootdrwxr-xr-x root root 2009-08-10 04:46 dev 这里请注意system，sdcard和data它们的权限以及各自的意义。 123d---rwxrwx system system 2009-08-10 04:52 sdcarddrwxr-xr-x root root 2009-05-15 00:53 systemdrwxrwx--x system system 2009-05-15 00:58 data 下面说下我对各个信息的理解。首先第一列这是表示的各用户的权限，d代表这是文件夹，rwx分别代表读、写、执行权限。 d后面第一组三个字符表示当前用户的读写执行权限，第二组代表group用户的权限，第三组表示other用户的权限。 然后是第二列，表示当前用户对该文件夹的权限级别，第三列代表该文件夹的当前用户。 如果我对这组信息的含义理解方式正确的话，那么这里我就有疑问了： 1. linux下面有system这个权限级别吗？我有个同事说只有root、group和other，所以我很奇怪这里的system这个权限级别是怎么回事，它有什么样的权限，能做到怎么样。 2. sdcard这个目录非常奇怪，自己的用户权限都没有，group和other用户却有所有权限，在sdcard目录里面建立的目录权限跟sdcard的权限一样。 3. 我们自己写的应用程序，不知道是属于什么样的权限级别，是作为什么样的用户来访问各目录包括sd卡的，手机sd卡和虚拟sd卡。","link":"/simulator-app2sd/"},{"title":"Run xgboost on Mac and Regression data","text":"The source code: xgboost_regression update at 2021-09-07: Install xgboost on Apple M1 1234567git clone --recursive https://github.com/dmlc/xgboostmkdir xgboost/my_buildcd xgboost/my_buildCC=gcc-11 CXX=g++-11 cmake ..make -j4cd ../python_package/Users/xx/miniforge3/envs/tf/bin/python setup.py install u must install miniforge for M1, conda create -n tf python=3.9.5 Run xgboost In the process of using xgboost, I encountered a small obstacle, that is, xgboost cannot be run normally on the M1 of the Mac. It needs to be tossed. The following is the installation process: 1. Homebrrew is required first 2. Install gcc and cmake 123brew install gccbrew install cmakebrew install libomp 3. Download xgboost package Yes, you cannot use the network package to install, you need to download, compile and install by yourself. Fortunately, the process is not troublesome: Source: http://mirrors.aliyun.com/pypi/simple/xgboost/ I downloaded xgboost-1.4.2.tar.gz 4. Installation Enter cd ~/download/ run 1pip install xgboost-1.4.2.tar.gz Okay, you can introduce it to try 12from xgboost import XGBClassifierxb = XGBClassifier() xgboost Regression load data 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import pandas as pdimport warnings%pylab inlinewarnings.filterwarnings('ignore')# load data from urldf = pd.read_csv('./data/Titanic.txt', sep=',', quotechar='&quot;', encoding='ISO 8859-15')df.info()df.head()# Filter some featuresfeatures = df[['pclass', 'age', 'sex']]# Labellabel = df['survived']features.info()# Missing values ​​are filled with meanfeatures['age'].fillna(df['age'].mean(), inplace=True)features.info()# Divide the datasetfrom sklearn.model_selection import train_test_splittrain_x, test_x, train_y, test_y = train_test_split(features, label, test_size = 0.25, random_state=33)# Feature vectorizationfrom sklearn.feature_extraction import DictVectorizervec = DictVectorizer(sparse = False)train_x = vec.fit_transform(train_x.to_dict(orient='record'))test_x = vec.transform(test_x.to_dict(orient='record'))# Random forest training and predictionfrom sklearn.ensemble import RandomForestClassifierrfc = RandomForestClassifier()rfc.fit(train_x, train_y)print('The accuracy of random Forest Classifier on testing set:', rfc.score(test_x, test_y))&quot;&quot;&quot;The accuracy of random Forest Classifier on testing set: 0.7781155015197568&quot;&quot;&quot;# xgboost training and predictionfrom xgboost import XGBClassifierxb = XGBClassifier()xb.fit(train_x, train_y)print(f'The accuracy:', xb.score(test_x, test_y))&quot;&quot;&quot;The accuracy: 0.7750759878419453&quot;&quot;&quot;","link":"/run_xgboost_on_M1_and_regression/"},{"title":"sketch中打开高版本文件","text":"sketch也不知道什么时候开始年费化了，也不能打开高版本文件了。（妈蛋） 据说是为了促进销量和保护版本。 打开包文件，然后打开包内的meta.json 替换头部: 1{&quot;commit&quot;:&quot;335a30073fcb2dc64a0abd6148ae147d694c887d&quot;,&quot;appVersion&quot;:&quot;43.1&quot;,&quot;build&quot;:39012 替换尾部 1&quot;commit&quot;:&quot;335a30073fcb2dc64a0abd6148ae147d694c887d&quot;,&quot;build&quot;:39012,&quot;appVersion&quot;:&quot;43.1&quot;,&quot;variant&quot;:&quot;NONAPPSTORE&quot;,&quot;version&quot;:88},&quot;version&quot;:88,&quot;saveHistory&quot;:[&quot;NONAPPSTORE.39012&quot;],&quot;autosaved&quot;:0,&quot;variant&quot;:&quot;NONAPPSTORE&quot;} 这里实际有几个key:commit, appVersion, build, version,NONAPPSTORE value替换成相应的值就OK了。","link":"/sketch-open-hight-version-file/"},{"title":"socialnomics VS Sina","text":"注： 这篇文是转载自Google+上 @Isaac Mao。地址 在此之前，还真不知道socialnomics这样一个概念。我在自己的博客上很少转发别人的文，转发此文，原意完全是为了能更广的传播，因为Google+相当一部分人打不开，而文又比较长不适合发微博，即便发了，相信Sina上也很快就回删除。让Sina自己和自己玩去吧。 转文： #SINA #Alert 自6月17日新浪微博刪除我賬戶以來，以每天平均10條微博，10美元/條的低價索賠，並發分析新浪審查自毀的信件給全球基金經理建議“削” $SINA 股票 http://stocktwits.com/symbol/SINA?q=… 是用來劫殺新浪股票 $SINA 的一個社會媒體網站，讓草根投資者社群也了解新浪的吞尾風險也是必要的，給基金經理的信函是呼應。新浪一天不對封殺帳號進行補償，就要承擔更大代價 一邊剝開新浪股票的風險，還可以一邊 Short $SINA 然後 來呼應，從頹勢股票中賺錢才是自由市場的真諦 （大家都可以做） 一家基金就能把 $SINA 壓下去，然後其他聰明一點的基金也都會問怎麼回事，不過我更相信草根的翅膀 http://stocktwits.com/symbol/SINA?q=… #Alarm 自從6/17日新浪刪除本人等帳號以來，其股票 $SINA 下跌$4.24美元(8.02%) ，市值蒸發2億美金，但還遠遠不夠補償，大家受壓後不在新浪轉世就是砝碼 2006年抵制MSN Space聲援@mranti等網誌作者，這個服務在2009年自廢；2007年公開信警告谷歌不要自我審查，否則會自推到角落，谷歌2010年退出中國市場。新浪自以為玩轉本土，其實是更深的自掘 社會性媒體其實與經濟的對應關係正在勾連起來，這個研究領域叫做 #socialnomics ，也就是社會力量對經濟指標的影響，品牌正邪與價值的對應關係會越來越明顯。新浪自以為刪除某個帳號無所謂，殊不知這種信號就是走衰的開始。 備考：7月18日，北京時間0:35，Stocktwits 也停了我的帳號， http://stocktwits.com/symbol/SINA 還派“羊水軍“來洗白，可見新浪 $Sina 正在多麼努力挽救自己。推友們很容易把你們給淹死誒","link":"/socialnomics-vs-sina/"},{"title":"SOLOVE Air-M20000","text":"第一次为产品写评测吧我这是，也不太记得了。不过以下这个电源，觉得值得写上一篇。 「SOLOVE移动电源 Air-M20000」 照例先来几张开箱图 本来应该再早一个月拿到这款电源的，因为产能的原因，跳票了。好吧，后来又因为快递单的遗漏，跳票了更久。不过好东西都是值得等待的。 第一次打开包装盒的时候，拿在手上满满的质感，身躯娇小，容量却很大。不过说起来，因为控制了高宽的原因（高宽仅与一张信用卡大小相仿），厚度不太理想。达到了一枚1元硬币的厚度，虽然握在手上的手感十分舒服，但是如果对于想揣在衣服口袋里的人来说，可能这个厚度稍稍有点不甚让人满意，说起来，10000mAh的容量，能做到如此地步已经很不容易了。 而对于SOLOVE Air-M20000的设计，相信也是能俘获很多人的心。电源灯的效果真的是很漂亮。 也许是因为控制尺寸的原因吧，SOLOVE Air-M20000并没有像其他大容量移动电池一样配备两个输出插孔，只有一个。输入Micro-USB, 输出为USB-A. 不过联想多数时候，我的小米电源另一个输出口都空的情况，其实一个已经足够了，重点是轻便易携带。 上周五这款电源已经到货了，没有第一时间拿来写当然是为了接受下周末的检验。 实际使用中，周五的晚上电源满电，周六出门12点半到下午6点，iPhone 6从3点开始电量百分之15，接入SOLOVE，1小时后达到92%，期间不停的在发微信。然后取下电源，周六没有为SOLOVE充电，周日12点多左右出门，到下午6点多回来，iPhone 6经历了两次空电的情况。而两次SOLOVE都将电量充满。 实际使用情况下，SOLOVE的容量还是不错的，iPhone 6来回充满三到四次应该是不成问题，更重要的是，充电速度很快。在不断使用过程中，一个小时基本就可以完全充满。当然，每个不同型号的手机可能都有差别。 所以SOLOVE的Air-M20000, 无论是从设计，做工还是续航情况，都还是蛮值得入手的。唯一遗憾的点，为什么给了我一个红色的，而不是黑色或白色的。。。。","link":"/solove/"},{"title":"玩转Stable Diffusion WebUI 各类模型","text":"Stable Diffusion WebUI 最有意思的地方不是在安装好之后生成图像，而是各种各样的模型。 提前警告：如果你的硬盘空间不够大的话，还是不要随便玩模型了，随随便便就是好几 G，又得甚至于 10 多个 G。 目前我仅留了最常用的 SD V1.5 和 SD V2.1两个模型，大小为 13G。 另外还需要说明一点，就是我曾经测试过用 NAS 来存储模型使用，完全不能用，暂时没有时间具体去研究到底什么原因。只有老老实实的继续在本地硬盘上跑。所以 NAS 上存了大量模型，真需要用到的时候再复制过来。 写这篇文章也是因为近期玩模型过程中打算整理一波，一是方便自己，二么也算是对其他小伙伴做些贡献。 Stable Diffusion 各种模型层出不穷，要说完估计需要费一番功夫，所以我摒弃其他小模型，只整理收集大模型，就是 ckpt 和 safetensors。如果你也打算跟着我一起玩模型但是还未安装，可以先参看我之前的文章： 在 Apple Silicon M1/M2 Mac 上安装和运行Stable Diffusion 说实话，我找了好多关于如何在 M1/M2 上安装和运行 Stable Diffusion 的教程和帖子，发现相互之间借鉴的不少，但是能用的确实没几个。 寻找一番后，发现其实没那么复杂。也不知道为什么网上的那么多教程搞得那么复杂，又是这个又是那个的一大堆，简单实现的方式有好几种： https://www.hivan.me/How%20to%20install%20and%20run%20Stable%20Diffusion%20on%20Apple%20Silicon 还是先从最基础的模型开始： Stable Diffusion 其他多数模型基本上都是从这个基础模型上再次训练得到的。 Stable Diffusion v2.1 SDv2.1提升了人物生成能力，因为SDv2.0大量增加了风景、建筑物和动物的数据集，减少了人物的学习量。 SDv2.1提高了NSFW过滤器准确度，因为SDv2.0的成人过滤器过滤的太狠，错误判定很多 即使是极端长宽比的图像也能顺利生成。 解剖学的身体和手（特别是手掌）的描写精度提高。 512 X 512 model : stabilityai/stable-diffusion-2-1-base · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-1-base 768 X 768 model: stabilityai/stable-diffusion-2-1 · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-1 img2img model stabilityai/stable-diffusion-2-depth · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-depth 重绘model stabilityai/stable-diffusion-2-inpainting · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-inpainting 超分 model stabilityai/stable-diffusion-x4-upscaler · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler Stable Diffusion V 1.5 runwayml/stable-diffusion-v1-5 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main Stable Diffusion V 1.4 CompVis/stable-diffusion-v-1-4-original · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/CompVis/stable-diffusion-v-1-4-original NovelAI 大名鼎鼎的 NovelAI，属于商业泄露模型。经过人在回路精细微调，可以生成高质量的二次元图像。但是千万时刻记得这个可是商用泄露模型，要注意避免法律风险： pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/animefull-latest.tar Waifu Diffusion 基于 Stable Diffusion 模型训练得到，增加了动漫及人物训练得到的模型，基本平时各种公开场合看到 WD 就是他。 WD 和 NovelAI 模型有些同质化，但是 NovelAI 实际是商用模型泄露，在某些使用情况下是有风险的。而 WD 不是，不过也不是说他绝对安全，毕竟 WD 也使用 Danbooru 进行学习，所以如果你关心这个需要注意一点。 Waifu Diffusion V1.5 这个模型使用是需要一个 yaml 文件的，究其原因是这个模型是基于 SD V2 得出的，需要把和 Model 同名的 yaml 文件放在模型所在的文件夹下，目前 1.5 模型是 beta2 版本，持续迭代 ing… Waifu Diffusion v1.5 beta waifu-diffusion/wd-1-5-beta · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/waifu-diffusion/wd-1-5-beta VAE(1.4 VAE 通用) vae/kl-f8-anime2.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt YAML waifu-diffusion/wd-1-5-beta2 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/waifu-diffusion/wd-1-5-beta2/tree/main/checkpoints Waifu Diffusion V1.4 和 1.5 版本一样，基于 SD V2得到的，依然需要下载 yaml 文件放在 model 同文件夹下。 Waifu Diffusion V 1.4 wd-1-4-anime_e1.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e1.ckpt wd-1-4-anime_e2.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e2.ckpt VAE(1.5 通用） vae/kl-f8-anime2.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt YAML e2 和 e1 是通用的，但是需要改名 hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/tree/main *Elysium Anime* 生成偏真实风格的动漫图片，风格比较偏向西式，光影还不错。 模型推荐写下面这些负面提示，可有效提升质量。 1lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry Elysium_V1 偏真实风的模型，手画的还不错，模型底稿基本是以西方人为主，所以生成的脸也偏西方人。 Elysium_V1.ckpt · hesw23168/SD-Elysium-Model at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hesw23168/SD-Elysium-Model/blob/main/Elysium_V1.ckpt *SD_Elysium_Kuro_Model* 与Anything 4.0、WD 1.4等合并后经过微调的二次元用模型。已经包含 WD 的“kl-f8-anime2”VAE 文件，因此无需使用额外的 VAE 文件 Elysium_Kuro_Anime_V1.safetensors · hesw23168/SD_Elysium_Kuro_Model at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hesw23168/SD_Elysium_Kuro_Model/blob/main/Elysium_Kuro_Anime_V1.safetensors *Elysium_Anime_V3* 动漫的附加学习模型，NSFW化相当严重，有更清晰的轮廓和轻微的三维效果。基于Elysium_V1 Elysium_Anime_V3.safetensors · hesw23168/SD-Elysium-Model at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hesw23168/SD-Elysium-Model/blob/main/Elysium_Anime_V3.safetensors *Anything系列* Anything是个神奇的二次元模型，据说是基于几十种模型融合+未知图片训练而来，随便写几个提示，就能到的不错的结果。不过这个模型整个就是一团混沌，实际训练模型，过程，方法，作者全部都是未知的。模型容易过拟合，非专业人士，请不要在此基础上训练模型。 Anything v3.0 “应该”是基于NAI模型+WD+SD等几十种模型+手部图片强化训练得出的。实际训练模型，过程，方法，作者全部都是未知的。如果没有.vae.pt，图片整体颜色浓度（饱和度）会更很浅。PS：Anything v3.0 的 .vae.pt 文件可以用于 NAI。 Anything V3.0 fp16: magnet:?xt=urn:btih/:45cd353ac4fa87098db5e3a6a349539710a3a1f5&amp;dn=Anything-V3.0-fp16.zip Anything v3.0 fp32: magnet:?xt=urn:btih/:d9db662ab5ace77004b3348c23c9381380c27156&amp;dn=Anything-V3.0-fp32.zip Anything v3.0 full-ema: magnet:?xt=urn:btih/:80460036625fb61dce4bc6e7dab744744309a2a0&amp;dn=Anything-V3.0-fullema.zip huggingface.co https://huggingface.co/Linaqruf/anything-v3-better-vae/tree/main Anything v4 自称是Anything最新版本的模型，实际一切都是未知的。仅需几个提示即可生成详细的 2D 插图的能力以及使用 danbooru 标签的能力。整体比过拟合的v3更自然，人物姿势等更容易操作。 anything-v4.0-pruned.safetensors · andite/anything-v4.0 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/andite/anything-v4.0/blob/main/anything-v4.0-pruned.safetensors Anything v4.5 貌似是Anything v4的进化，但实际一切都是未知的。比v4画风更柔和一点。 anything-v4.5-pruned.safetensors · andite/anything-v4.0 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/andite/anything-v4.0/blob/main/anything-v4.5-pruned.safetensors Zeipher 生成更符合真人解剖结构的真人模型，训练集以女性图像为主官方网站是 https://ai.zeipher.com，已经关闭。请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 F222 f222.safetensors · acheong08/f222 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/acheong08/f222/blob/main/f222.safetensors F111 f111.ckpt · Reachout/F111 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/Reachout/F111/blob/main/f111.ckpt 3DKX 因为Zeipher官方已经GG，这是热心网友创建的衍生3DKX模型如果你想让你的 3D 角色有一张更“二次元”的脸，提示词最开始写 “3d cartoon of”，或者如果你想要经典的 3D 渲染外观，写“a 3d render of”高分辨率模型，推荐分辨率为 1152 x 768 或更高 3DKX_1.0b f111.ckpt · Reachout/F111 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/Reachout/F111/blob/main/f111.ckpt R34 从网站“rule34.xxx”的 150,000 张图像中进行训练。rule34.xxx几乎全是NSFW图片，所以你懂的 r34_e4 1.99 GB file on MEGA https://mega.nz/file/yJgDUCzA#zOD2yeE6QLBqPEjEpIi2b4FWOlb64yVUveOd_eW6teI 磁力链接：magnet:?xt=urn:btih/:ed9f0e3f849d7119107ef4e072c6abeb129e1a51&amp;dn=r34_e4.ckpt EVT pixiv排行榜模型 基于pixiv排行图片训练，夹杂有部分R18排行图片 Evt_V4_e10_ema Evt_V4_e10_ema.safetensors · haor/Evt_V4-preview at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/haor/Evt_V4-preview/blob/main/Evt_V4_e10_ema.safetensors EVT_V3 haor/Evt_V3 · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/haor/Evt_V3 EVT_V2 haor/Evt_V2 · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/haor/Evt_V2 Basil_mix 逼真的真人模型，基于亚洲风格训练，支持Danbur标签提示词需要加载VAE，不然画面色彩浓度和边缘会很淡提示词应尽可能简单不要堆砌大量质量标签和负面提示，不然会适得其反。请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 basil_mix basil mix.ckpt · nuigurumi/basil_mix at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/nuigurumi/basil_mix/blob/main/basil%20mix.ckpt VAE vae-ft-mse-840000-ema-pruned.ckpt · stabilityai/sd-vae-ft-mse-original at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.ckpt Chillout Mix 逼真的真人模型，基于亚洲风格训练，支持Danbur标签提示词请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 chillout mix _ NiPruned Fp32 Fix chilloutmix_NiPrunedFp32Fix.safetensors · Inzamam567/useless_Chillout_mix at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/Inzamam567/useless_Chillout_mix/blob/main/chilloutmix_NiPrunedFp32Fix.safetensors Uber Realistic Porn Merge 如名字所说，逼真的真人Porn模型，简称 URPM 模型请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 Uber Realistic Porn Merge Uber Realistic Porn Merge (URPM) | Stable Diffusion Checkpoint | Civitai For early access builds and to support daily work on URPM, please check out my patreon! https://www.patreon.com/uber_realistic_porn_merge , or disc... https://civitai.com/models/2661/uber-realistic-porn-merge-urpm","link":"/stable-diffusion-webUI-models/"},{"title":"vue 2.0 自定义filter并挂载到全局使用","text":"其实早几天前就进入了第三部分,而因为第二部分面向对象程序设计糊里糊涂,搞得再接下来的学习里有很多实例根本看不懂,或者很难回忆起所学的知识点.不得不回头又一个字一个字的老老实实的看了一遍,并且将每个字都敲出来更新到了Sites上!至此算是比较牢固了..而更新的部分,相信对于想进入Java世界的新手们也算是一个好的参考. 其中前两部分分为八章,第一部分为基础程序设计,第二部分为面向对象程序设计 ,第三部分则是实战的Java应用程序设计.我询问过一个朋友,说是对于Android开发来说,Java的前两部分属于基础,非常重要.而第三部分也就无所谓了.不过想来,还是将所有的东西全部抓牢以后再开始.毕竟Java已经学到这一步,规规矩矩做一个Java开发者也未尝不可.下面给出Java前两部分的链接,算是再次推广我的Learn Wiki.而第三部分将不会在此更新了.有兴趣的自己跟着我在Sites上的更新翻看吧,更新速度要视我自己的学习速度而定!现在手上的教程为:《Java 开发与实战经典》 第一部分:Java基础程序设计 1.Java概述及开发环境搭建 2.简单的Java程序 3.Java基础程序设计 4.数组与方法 第二部分:Java面向对象程序设计 5.面向对象(基础篇) 6.面向对象(高级篇) 7.异常的捕获与处理 8.包及访问控制权限 第三部分将不会继续在这里给出链接.请自行查看 http://learn.hivan.me/index/home/java-learn/Java-DAP","link":"/the-third-part-of-the-java-learning/"},{"title":"vue 2.0 自定义filter并挂载到全局使用","text":"vue 2.0 开始，取消默认filter, 需要自定义。 而自定义之后每次在需要使用的组件内引用也确实蛮麻烦的。 所以我们就来将定义的filter挂载到全局使用。 vue2.0 filter相关文档 定义 引用 挂载 使用 /src/filters/ - format.js 123export default function(val){ ...} index.js 12345import format from &quot;./format&quot;;export default{ format: format,} /src/ - main.js 123456789...import commonFiltes from './filters/index'Object.keys(commonFiltes).forEach(function (key, index, arr) { Vue.filter(key, commonFiltes[key]);})... /src/components/ - xxx.vue 1234567&lt;template&gt;...&lt;div&gt;{{ data | format }}&lt;/div&gt;&lt;/template&gt;&lt;script&gt;...&lt;/script&gt;","link":"/vue2.0-custom-filter-to-global/"},{"title":"使用WAMP5搭建Apache+MySQL+PHP环境","text":"目前有不少AMP（Apache）的集成软件，可以让我们一次安装并设置好。这对于不熟悉AMP的用户来说，好处多多。 一、使用AMP集成软件的优点： 1、可避免由于缺乏AMP的知识，而无法正确设置环境； 2、可快速安装并设置好AMP环境，让我们直接开始真正感兴趣的软件，如xoops； 3、可方便的搭建测试环境，对于测试“是AMP环境问题，还是XOOPS造成的问题”很有帮助，采用排除法即可。 二、常用的AMP集成软件： 1、AppServ：http://www.appservnetwork.com/ 这个软件在台湾很流行。看到不少书籍也极力推荐，估计都是受台湾用户的影响。 2、XAMPP：http://www.apachefriends.org/en 这个软件支持多个平台,WinOS X，目前也有不少人使用。 3、WAMP5：http://www.wampserver.com/en/ 这是我今天极力推荐的，绝对五星级。注意它的名字是带个5的哦，意思就是WAMP5使用最新的PHP5版本，正如官方网站上的口号：Discover PHP5 with WAMP5 ! 三、根据我的经验，WAMP5有如下优点： 1、XOOPS在WAMP5中使用，没有任何问题。 2、WAMP5专注于Windows平台，安装设置及其简单。 3、PHP默认的是5.x版本，如果需要php4.x，只要安装php4.x插件，就可以在两者之间自由的切换，非常方便。 4、MySQL默认的是5.x版本，但可以通过选择老版本的WAMP5，从而使用4.x的MySQL。 5、可视化的菜单管理，极其方便。如，打开关闭php extention、Apache module等，直接通过菜单选择就可以。 6、还有各种插件，如ZEND OPTIMIZER ADD-ON等。 7、如果有疑问，官方还有论坛可以求助。 ……更多优点：谁用谁知道，早用早知道^_^ 四、使用WAMP5的经验、技巧 1、安装时的设置：可以自定义WWW根目录的存放位置哦，强烈建议放到D盘等安全的分区中，以避免万一系统崩溃，造成数据丢失。 2、对于中文用户来说，安装结束后，首先要设置的是，把MySQL的数据库默认编码改为UTF-8，这样可以排除很多中文乱码问题：在WAMP5菜单中选择打开my(wamp).ini，设置其中的default-character-set=utf8, 然后重启WAMP5。 3、数据库默认的密码是空的，可以在phpMyAdmin中设置root帐号的密码为123456；当然修改之后，就要跟着修改phpMyAdmin的配置文件config.inc.php，否则phpMyAdmin就进不了数据库啦： &gt;\\(cfg[’Servers’][\\)i][’user’] = ‘root’; &gt;\\(cfg[’Servers’][\\)i][’password’] = ‘123456′; 4、如果需要mysql4.x + php4.x，可选用WAMP5 1.44版本以及插件PHP4.3.11； 5、由于MySQL4.1之后版本对密码验证的方法发生了改动，如果在WAMP5中使用php4.x，那么就需要启用MySQL的old password功能，否则无法登陆phpMyAdmin。在WAMP5菜单中选择MySQL/Mysql console，然后输入下列命令： &gt;mysql&gt; SET PASSWORD FOR &gt;-&gt; ‘root’@'localhost’ = OLD_PASSWORD(’123456′); 6、XOOPS用户关心的时区问题：WAMP5默认的时区是格林威治标准时间(GMT)，这就意味着WAMP5默认的服务器时区是GMT，但我们可以更改服务器默认时区，以对应北京时间。打开WAMP5菜单中的php.ini，在文档最后添加如下代码即可： &gt;[Date] &gt;; Defines the default timezone used by the date functions &gt;date.timezone = “Asia/Shanghai” 如果你不在北京，那么就改动上述的设置即可，具体设置值可参考：http://us2.php.net/manual/en/timezones.php 通过上述WAMP5的设置之后，我们在XOOPS中的时区设置就可以这样：服务器时区、默认时区、个人帐号的时区这三者都设置为上述的时区就可以了，如：北京时间（GMT+8） OK，这就是所有的AMP设置秘籍！！","link":"/wamp5-make-apachemysqlphp/"},{"title":"vux更改Tabbar选中状态","text":"在vux的文档和示例中，都没有明确的说明tabbar上v-model的使用 文档中将v-model说明放在了TabbarItem示例下，但是其实这个应该是放在Tabbar上 1234567891011121314151617181920212223&lt;template&gt; &lt;router-view class=&quot;view&quot; v-on:changeTab=&quot;changeTab&quot;&gt;&lt;/router-view&gt; &lt;tabbar v-model=&quot;index&quot;&gt; &lt;tabbar-item&gt;&lt;/tabbar-item&gt; ... &lt;tabbar-item&gt;&lt;/tabbar-item&gt; &lt;/tabbar&gt;&lt;/template&gt;&lt;script&gt;data(){ return{ index:0, ... }}methods:{ changeTab(num){ ... this.index = num; ... }}&lt;/scirpt&gt; 然后子组件中调用 123mounted(){ this.$emit('changeTab', 2)} 这样就便于在不同的组件内都可以更改Tabbar选中状态","link":"/vux-tabbar-selected/"},{"title":"什么事ifttt,ifttt怎么玩?","text":"原帖出自師北寰的網絡日誌，當中很多條目內容也都是來自於Twitter！由於原帖地處偏僻，需要翻山越嶺，所以為了照顧國內用戶，特轉此貼！這也是我為數不多的轉帖之一。當然，我知道，這個服務可能過段時間完全開放後又會和國內無緣！就像G+才出世兩天就被牆了一樣！不過這也充分證明了G+的優秀！而現階段，最好玩的網絡應用就是G+和ifttt。所以一般都是if oo then xx! 什么是ifttt？ ifttt是一坨网站：ifttt.com 即If This Then That，你可以在ifttt上设定一个条件，当达到你设定的一个条件时，便触发一个（你指定）动作。这里的「条件」和「动作」是指开放的互联网服务，比 如flickr，twitter，facebook，youtube等。别问我有没有新浪微博、人人网，优秀且可靠的互联网服务都在国外。 国内没法抄ifttt。ifttt最重要是服务稳定，就国内普遍鸡贼的情况来看，if端服务时不时封闭一下是常事。创业企业抄好了，大公司眼红把自己网站接口关闭再抄一个出来，创业公司就得玩完。大公司抄的话，竞争对手们也会关闭自己的服务，这玩意儿在国内没法玩。 ifttt的稳定性是关键，可以定制一连串的if…then任务出来，但如果中间某一个服务出问题，后面的任务就全失效了（当然，这么定制也挺笨的）。 ifttt非常重要的一个优点是，将常用服务（twitter，加星或分享的Google reader条目，加过标签的instagram和flickr照片）中的重要资料，全部发送到一个存储服务（Dropbox，evernote，Gmail），需要用的时候检索起来将非常方便。 ifttt可以怎么玩？ 好玩的可以有：if某女谈论「失恋」、「男友+讨厌」、「伤心」、「难过」，then 发送一条短信。ifttt泡妞必备… 非常实用的应用可以有：New fav tweets to evernote ifttt还解决了我以前在北京十分急需的一个功能：if 北京美国大使馆空气监测站的空气质量指数超过250，then 发送一条短信…当然，其它方法也能实现，但ifttt方便太多了。 未来的应用有：ifttt的出现真可以实现未来你挂了也能一个人办丧事：if三十天未发推，then启动一系列任务：1.发邮件告诉殡仪馆来收尸（亲，你可以看我的google location）；2.自动转账；3.发表遗书告诉亲友可于30天后到某处悼念；4.分享生前录好的视频，最后再操一遍GFW。 在未来，随着越来越多社交服务的出现，以及多条件任务功能的推出，玩ifttt的花样将呈爆发式增长，乐趣无穷。 ifttt还可以有什么玩法？ @mranti: ifttt应用举例：if 某男A和某女B同时check in同一个地方，then 短信我的手机：“A和B有奸情，而且正在进行”。八卦利器啊！ @hecaitou: 理想状态下的ifttt应用场景：一旦老婆的推上出现“加班”字样，立即激活一条手机短信通知。同时，自动检测谷歌日历，找出几个今晚没有事情的老友。随后，在FB上新建一个活动“今晚喝大酒”，一旦超过3人同意，触发一条订餐消息给餐厅。餐厅查询Evernote，找到这群人最喜欢的菜和酒。on Twitter: http://twitter.com/hecaitou/status/85927850749857792 @mranti: ifttt应用举例：if 明天下雨，发推DM给自己的心仪女友：“亲爱的，明天出门带伞，我是你的阳光”。 on Twitter: http://twitter.com/mranti/status/85927810924937219 最后这两条和菜头和安替的推是我在twitter fav之后，在写这篇文章过程中自动保存到evernote的，服务十分流畅，文章写起来太方便了有没有？ —-以下内容为下午四点十五分更新—- 出去溜达了一圈回来，脑子里一直在想ifttt，ifttt简直是个太科幻的产品了，第一次觉得人工智能——不对，是机器智能——离自己这么近。ifttt比Google、Facebook都要伟大得太多。 ifttt就是一个反射，它把你想象得到的任何一个动作反射为另外一个你能想象得到的动作，并且它不像生物体一样会被躯干束缚。ifttt上将出现拉马克进化？ @hecaitou: ifttt里面，如果在Channel之上，提供一个Task的自由市场。让各种Geek做出各种奇奇怪怪的Task来，用户添加Task而不是点选Channel，那就连盈利的问题都解决了。 on Twitter: http://twitter.com/wuyagege/status/85959272638324736 @mranti: 在ifttt的世界里面，各位姑娘小心了，什么恋爱短信啊、花啊、DM关怀啊、贴心礼品啊，都可能是程序的Task算出来的。而且ifttt的世界中，一个人死了，他对一个女生的关心也可以一直持续下去，仿佛天天都在。 on Twitter: http://twitter.com/mranti/status/85974845216665600 @boatman: ifttt神就神在即使被墙，只要设置好this和that的关联性，墙并无法阻止this触发that，除非GFW把所有的channel全部封锁才有可能抑制ifttt，但当ifttt支持自定义channel时，就是神也难救方滨兴。 on Twitter: http://twitter.com/Ryan_XxOo/status/85975132866220032 @Doriscafe: 我死后，请你替我照顾她。每天给她发短信叫早，订花，在推特上mention她，赞她，天气好提醒加衣，天气不好提醒带伞，请你替我照顾她，只要服务器不倒下，就直到永远。#ifttt on Twitter: http://twitter.com/Doriscafe/status/85975909429018624 @juicy_luna: 我个人觉得吧，#ifttt 就是把生物里的神经反射运用进了网路里，甚至还会扩展到物质生活。也就是说，它担负起神经链的作用，将能把一切行动串联起来，形成纵横的网络。。奇妙的世界。。 on Twitter: http://twitter.com/juicy_luna/status/85976819626549248 @duck_1984: 超级多米诺啊 蝴蝶效应啊 ifttt毁灭世界啊 自寻死路啊愚蠢的人类 还有更多⋯⋯你来补充。","link":"/what-is-ifttt-and-how-to-play/"},{"title":"Yosemite访问用户级服务器目录","text":"升级到OSX 10.10(Yosemite)以后，localhost是可以正常访问的，只是localhost/~user无法打开了，提示403错误。 网上查找资料，说是随着系统的更新，Apache本本更新到2.4.9，PHP也更新到了5.5.14，所以Apache的配置就需要做相应的修改。 首先，我们需要确定打开了Apache 1sudo apachectl start 然后设置允许访问用户目录 修改httpd.conf配置 1sudo subl /etc/apache2/httpd.conf command + f 查找代码，并去掉注释符 # 123456LoadModule authz_core_module libexec/apache2/mod_authz_core.soLoadModule authz_host_module libexec/apache2/mod_authz_host.soLoadModule userdir_module libexec/apache2/mod_userdir.soLoadModule php5_module libexec/apache2/libphp5.soInclude /private/etc/apache2/extra/httpd-vhosts.confInclude /private/etc/apache2/extra/httpd-userdir.conf 修改httpd-userdir.conf配置 1sudo subl /etc/apache2/extra/httpd-userdir.conf command + f 查找以下代码，去掉注释符# 1Include /private/etc/apache2/users/*.conf 修改yourUserName.conf配置 1sudo subl /etc/apache2/users/username.conf PS: username为你的用户名称，如果没有该文件则新建一个，然后将内容修改为: 123Options Indexes MultiViewsAllowOverride NoneRequire all granted 然后设置文件权限为755 1sudo chmod 755 /etc/apache2/users/haibor.conf 最后我们需要重启Apache 1sudo apachectl restart","link":"/yosemite-open-usersite/"},{"title":"Windows phone 7 破解工具开放下载","text":"微软Windows Phone 7应用程序商店早前被发现存在严重漏洞，Tobias对微软和各位开发者做以警告，希望微软能竟快的将漏洞补上，但是微软方面没有实质动静。现在终于有一款XAP下载工具露面，WP7 MktPlace 0.36d于今日被正式公开提供下载！ 通过这款强大的XAP下载软件，普通用户只需轻点download就能批量下载Marketplace市场上所有的XAP安装包。然后通过另一个Xap_Dll_Signer签名工具来将版权认证相关内容删除（已通过智机网技术组测试可行），这样一来，所有已经对WP7手机解锁的用户便都可以轻易的在自己的手机上部署Marketplace上的收费软件。暂时不说对开发者是否存在一定的担忧,但是对于国内喜欢免费大餐的用户来说,这一定是个好消息! [download id=\"4\"] 往期版本下载及更多详情请参看官网","link":"/windows-phone-7-carck-download/"},{"title":"从美国三大协会说供应链管理的演变","text":"前面说到，供应链管理从采购、运营和物流管理发展而来，它是对从供应商的供应商到客户的客户的产品流、信息流和资金流的集成管理，以实现对客户价值的最大化，以及供应链成本的最小化。企业之间的竞争不再是企业与企业之间的竞争，而是供应链与供应链之间的竞争。过去二三十年来，美国的汽车业在日本汽车大厂的强大攻势下一败涂地，就是一个供应链战胜另一个供应链的例子。 在美国，这种集成的供应链管理概念不是一蹴而就的，而是经过几十年的演进发展而来的。这里我们从采购、运营和物流职业协会的发展历史出发，阐述供应链管理在美国的发展——要知道，判断一个职能的发展，最简单的方式就是看相应行业、职业协会的发展。 简言之，供应链管理不是一个领域，而是三个。过去不是，现在不是，在可以预见的将来仍会保持在多个领域齐头并进。这从目前美国跟供应链管理相关的行业协会可见一斑。 供应管理协会（ISM） 供应管理协会是世界上规模最大、影响最大的供应管理组织，拥有四万多会员。它发布的采购经理人指数（PMI）跟踪生产、库存、订单量等变化，是美国经济的风向标，被新闻媒体、学术研究、华尔街和政府部门广泛引用。它的前身是美国采购经理联合会（NAPM）。在100多年的发展过程中，伴随着供应管理在美国公司的重要性不断提升，该协会的侧重点从采购发展到供应管理，再到供应链管理。 图1-3简单地表述了在美国，采购从采购代理发展到采购管理，再发展到供应管理的过程。最早的采购是采购代理，即内部客户确定了需求，找好了供应商，价格也往往都谈好了，采购负责签合同、下订单，把东西买回来。当时的专业协会就叫“采购代理人协会”（1915年成立）。后来，采购说，我不但可以帮你下订单，而且可以帮你找供应商、管供应商、谈价钱、谈合同，于是就变成采购管理，协会也改名“采购经理人协会”（1968年）。再到后来，采购说，我不但可以帮你找供应商、管供应商，而且可以处理运输、物流、进出口，把一切打点好，直到产品进了我们的仓库，于是就演变成了供应管理，专业组织也改名“供应管理协会”（2002年）。 但是，供应管理协会的核心仍然是采购与供应管理，而不是广义上的供应链管理。2002年前后，采购经理协会改名为供应管理协会时，有人问，为什么不改为“供应链管理协会”？ISM的答复是供应链管理太广泛，还不够定型。是的，在供应链领域，不管你问美国还是中国的职业经理人，你们的头衔是什么，答复大多是采购、运营或物流，而不是供应链管理。即使是供应链经理，他们的职责也往往侧重某个领域，并不是我们真正意义上的大供应链。但毫无疑问，从采购管理到供应管理，是向供应链管理迈进了一大步。 图1-3 采购管理发展到供应链管理 这也体现在职业认证上。供应管理协会原来的认证是注册采购经理（C.P.M.），在30多年的历史中，全球认证人数超过4万，认证内容覆盖价格、质量、交付、合同管理、供应商选择、供应商谈判、国际贸易、公司管理及人力资源管理等。为适应采购向供应管理的发展，供应管理协会在2008年推出“供应管理专业人士认证”（CPSM），以取代C.P.M.，这标志着向供应链管理更进一步。相对而言，CPSM的要求比C.P.M.更高，也反映了供应（链）管理比采购管理要求更高。[1] [1] 对于CPSM认证的细节，可阅读《供应链管理的职业认证》一文，收在我的《供应链管理：实践者的专家之路》一书中，机械工业出版社于2017年出版；或在我的“供应链管理专栏”（www.scm-blog.com）上查询同名文章。 运营管理协会（APICS） APICS是美国生产与库存管理协会的缩写。与供应管理协会侧重采购相对应，APICS历来侧重于生产与库存管理。为适应向供应链管理发展的趋势，该组织在2004年更名为APICS—运营管理协会（具体如图1-4所示）。考虑到APICS在美国乃至世界的影响，运营管理协会仍旧保留APICS字眼，也显示不放弃在生产与库存控制方面的传统优势。 图1-4 APICS更名，进入供应链管理时代 在美国宏观经济中，APICS的地位没有供应管理协会高，比如没有像供应管理协会的PMI那样有影响力的宏观经济指数。但在生产与库存管理领域，APICS享有崇高的声望。它的生产与库存管理认证（CPIM）在生产企业受到普遍重视。其认证内容侧重于生产的计划、控制和实施，即如何把销售计划转变为需求计划、生产主计划，然后细化到生产计划、物料供应计划，再到生产线的排程和控制，内容包罗万象。自1973年首次推出以来，全球总共有10.7万人得到此项认证。[1] 伴随着更名，APICS在2005年推出“供应链专业人士认证”（CSCP）。从字面上看，该认证针对供应链管理；从内容上看，该认证试图覆盖供应管理的三大范畴（采购与供应管理、生产运营管理、物流管理）；从级别上看，该认证比CPIM高，它要求一定年限的相关工作经验，而CPIM则没有。在过去10多年来，该认证与CPIM并存，可视作CPIM向供应链管理的延伸，以实现APICS在供应链管理领域与供应管理协会（ISM）两分天下的目的。 2014年4月30日，APICS宣布与Supply Chain Concil（SCC）合并。这种合并，一方面是为了抱团取暖—2008年金融危机以来，经济低迷，美国专业协会的经费大减；另一方面是为了挽救运营管理在美国日益衰落的局面——外包盛行，供应链全球化下，很多公司的生产制造被外包给低成本国家的供应商，原来由运营部门负责的任务转移到采购部门，原来的生产、库存、计划等专业人士也纷纷转业，而以这些人为基础的运营管理协会也就每况愈下。 这从他们的董事会成员可以看出：APICS的董事会成员中（2018年），12个董事会成员，只有两位是《财富》500强的副总，其余大多是些总监、经理、顾问[2]——生产外包到别的国家，有些生产运营的专家就只能转入咨询业。作为对比，看看其竞争对手供应管理协会的董事会，13个董事会成员中，只有一两个不是《财富》500强的副总裁、首席采购官级别（2018年）[3]。我们说这些，并不是某相声中说的，两个人拿名片打牌，看谁的名片上的头衔大；而是说，这些职业协会的董事会大部分是志愿者，大企业的高管们是否愿意把自己的时间贡献出来，从侧面反映了一个协会在行业的影响力。 好消息是，运营管理协会APICS这些年来继续与别的职业协会合并。2015年7月，它与原来的美国运输和物流协会合并，算是正式跨入物流领域，也给它的认证库里增加了一个新成员：运输与物流认证（CTL）。通过这一系列的兼并，运营管理协会可以说成为美国三大供应链职业协会中涉猎范围最广的一个。当然，兼收并蓄的风险呢，也是可能变成大而杂，丧失聚焦点，变成三不像。 [1] CPIM Transformed for Today’s Busy Professionals. APICS网站，www.apics.org. [2] 2018 APICS Board of Directors. APICS网站，www.apics.org. [3] ISM Board of Directors. ISM网站，www.instituteforsupplymanagement.org. 供应链管理专业人士协会（CSCMP） 这是美国第三个与供应链管理相关，也有相当影响力的协会。它的前身是物流管理协会（CLM）。顺应物流管理向供应链管理的过渡，供应链管理专业人士协会（CSCMP）试图从物流管理的角度出发，来“蚕食”供应链管理这一热门行业。 物流管理协会的影响更多是在物流教育领域，这从他们的主席人选可见一斑：物流专业的一些知名教授曾担任过该组织的主席，例如国内熟悉的鲍尔索克斯（密歇根州立大学教授，1964~1965年任主席）、门泽尔（田纳西大学教授，2000~2001年任主席）等。 相信在短时间内，供应管理专业人士协会很难大幅增加在工业界的影响，也很难成为一个纯粹意义上的供应链管理协会。这从该协会的董事会可见一斑：15位董事会成员中，6位是教授或来自大学。作为对比，运营管理协会APICS的董事会只有1位教授，而供应管理协会ISM则是清一色的大企业高管。 在发展历史上，供应链管理专业人士协会很好地诠释了从小到大、从部分到全部的发展历程。如图1-5所示，1963年，美国实物配送协会成立，表明最早的物流以运输为主，简单地说就是管着一帮卡车司机，做着把东西从A点搬送到B点的“实物配送”。[1]到了20世纪80年代，这些人说，我们不但可以把东西从A点搬到B点，而且可以对付整个过程的仓储、配送、海关等多道手续，以及伴随而来的信息流，这就变成了物流管理。于是在1985年，实物配送协会改为物流管理协会，覆盖运输以外的更多业务。再后来，物流管理说，我们也可以对付采购、运营的事啊。得，这就变成了供应链管理—2004年，物流协会改名供应链管理专业协会，正式从物流跨入供应链领域。不过对我们供应链领域的人而言，一看你是供应链管理职业人协会的，就知道你的前世今生是物流。 图1-5 从实物配送到物流管理到供应链管理 从认证角度而言，长期以来，这个机构一直没有能与供应管理协会和运营管理协会相匹敌的认证。最近注意到，供应链管理专业人士协会推出了SCPro的认证。整个认证分为三级：第一级是供应链管理核心知识，覆盖供应链管理领域的八个方面；第二级是供应链挑战的分析与实施，基于案例来测试学员对供应链管理知识的应用能力；第三级是供应链转型，需要在学术机构导师的指导下，分析具体企业的真实状况，规划一个供应链改进项目，来取得真实的业务成果，比如提高投资回报率、缩短周转周期等。[2]这个认证目前在国内还没有看到，在美国的影响也尚需建立。[3] 从上述美国三大职业协会的发展可以看出，在可预见的未来，供应链管理仍将以一个综合领域的面目存在，在采购、运营和物流的基础上继续发展。但集成的趋势很明显，不但在行业协会，而且在工业界、教育界。 很多公司在集成采购、运营和物流管理三个部门，设立全球供应链部。ERP软件提供者如SAP、Oracle促进了这一趋势：它们的软件使跨职能协作更加容易。学术界也有越来越多的系、专业改名为供应链管理。美国MBA排名中也增设了供应链管理/物流管理专业，与传统的会计、金融、营销、国际管理等分庭抗礼，说明供应链管理作为一个专业已经形成。 资源 CPSM认证由美国供应管理协会提供，详情见www.ism.ws CPIM和CSCP认证由美国运营管理协会提供，详情见www.apics.org 这些认证在国内都可以参加，其中CPSM已经汉化。 延伸阅读 《供应链管理在国内的发展》，节选自我的另一本书《供应链管理：实践者的专家之路》。十几年前，我在申请北美商学院时，第一次听说供应链管理；七八年前，国内的一些大型企业启动供应链转型；最近几年，越来越多的中小企业着眼供应链，解决日益严峻的成本和库存问题。扫描二维码，阅读全文。 [1] 有个学术刊物，名字叫International Journal of Physical Distribution &amp; Logistics Management，翻译过来就是《实物配送和物流管理国际学刊》。从1970年创刊至今，都快半个世纪了，还看不到一点要寿终正寝的样子。 [2] 见供应链管理专业人士协会的网站：http://cscmp.org [3] 判断一个认证的价值，最简单的就是看招聘网站上，有多少职位要求或者建议应聘者有这个认证。我到Monster.com（这是美国的一个主要招聘网站）上，搜索SCPro认证，只发现两个职位；搜索ISM的CPSM认证，发现487个岗位；搜索APICS的CPIM认证，出来700个岗位；搜索CSCP，有358个岗位（2018年10月10日，搜索时不限职位所在的地域）。再搜索“注册供应链管理师”CSCM认证，发现17个岗位—这个认证最近突然在国内冒起来，到处都有人在宣传，说是国家人力资源和社会保障部认可的，引得很多读者三天两头到我这里求证，问这个认证是不是主办者宣称的那样，是个美国主流认证，这里算是一并答复。","link":"/%E4%BB%8E%E7%BE%8E%E5%9B%BD%E4%B8%89%E5%A4%A7%E5%8D%8F%E4%BC%9A%E8%AF%B4%E4%BE%9B%E5%BA%94%E9%93%BE%E7%AE%A1%E7%90%86%E7%9A%84%E6%BC%94%E5%8F%98/"},{"title":"15. 使用LLMChain连接Google和计算器","text":"大家好，我是茶桁. 在上一节课中，我们学习了如何使用LangChain这个Python包链式调用OpenAI的API。通过链式调用，我们可以将需要多轮询问AI才能解决的问题封装起来，将需要多轮自然语言调用才能解决的问题变成一个函数调用。 然而，LangChain对我们的帮助远不止于此。最近，ChatGPT发布了Plugins插件机制。通过Plugins，ChatGPT可以浏览整个互联网，还可以接入诸如Wolfram这样的科学计算工具，能够解决许多大语言模型难以解决的问题。不过，这是需要Plus用户才可享用的，并且每一个小时内的对话Token都是有限制的。 但是，这并不重要，我们通过LangChain也能实现类似的功能。在今天的课程中，我们将继续深入挖掘Langchain，看看它如何解决这些问题。 解决 AI 数理能力的难题 虽然许多人发现 ChatGPT 在回答各种问题时表现得很好，但是当涉及到计算三位数乘法时，它就显得有些力不从心了。它似乎只是快速估算一个数字，而不是真正准确计算。为了解决这个问题，我们需要进一步研究 AI 数学能力的提升。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E4%BD%BF%E7%94%A8LLMChain%E8%BF%9E%E6%8E%A5Google%E5%92%8C%E8%AE%A1%E7%AE%97%E5%99%A8/"},{"title":"使用Python库unstructured揭秘文本数据","text":"在数据的世界里，文本数据是特别复杂的。它不像数字数据那样被分成整齐的行和列。作为一个副业，我正在开发自己的个人人工智能助手。其目的是利用我的笔记和文件中的数据来回答我的问题。重要的好处是，所有的数据处理都将在我的电脑上进行，确保没有文件被上传到云端，而且我的文件将保持隐私。 为了处理这种非结构化的数据，我发现unstructured的Python库非常有用。它是一个灵活的工具，可以处理各种文档格式，包括Markdown、、XML和HTML文档。 从unstructured的开始 你可以通过以下方式轻松安装该库： 1pip install unstructured 装载和分割文件 你想对你的文件做的第一件事是把它分割成更小的部分或章节。这个过程被称为分区，使其更容易分类和提取文本。 以下是你如何做的： 123from unstructured.partition.auto import partitionelements = partition(filename=&quot;example-docs/note.md&quot;) example-docs/note.md： 123## My test titleAnd here is a sample text. 当我们分割一个文档时，输出是一个文档元素对象的列表。这些元素对象代表了源文档的不同组成部分。unstructured库支持各种元素类型，包括Title, NarrativeText, 和ListItem。要访问元素类型，你可以使用category方法： 1234for element in elements: print(f&quot;{element.category}:&quot;) print(element) print(&quot;\\n&quot;) Output: 123456TitleMy test titleNarrativeTextAnd here is a sample text. 文档元素的列表可以用convert_to_dict函数转换为字典的列表： 123from unstructured.staging.base import convert_to_dictdict_data = convert_to_dict(elements) Output: 1234567891011121314151617181920[{'type': 'Title', 'coordinates': None, 'coordinate_system': None, 'layout_width': None, 'layout_height': None, 'element_id': 'a3114599252de55bea36c288aa9aa199', 'metadata': {'filename': 'sample-doc.md', 'filetype': 'text/markdown', 'page_number': 1}, 'text': 'My test title'}, {'type': 'NarrativeText', 'coordinates': None, 'coordinate_system': None, 'layout_width': None, 'layout_height': None, 'element_id': '6e78562ede477550604528df644630e8', 'metadata': {'filename': 'sample-doc.md', 'filetype': 'text/markdown', 'page_number': 1}, 'text': 'And here is a sample text.'}] 但由于我想把这些文本块存储在数据库中，并对数据进行一些探索性分析，所以我用convert_to_dataframe函数把文本元素转换成pandas数据框架： 123from unstructured.staging.base import convert_to_dataframedf = convert_to_dataframe(elements) 获取元数据 unstructured库的一个整洁的特点是它如何跟踪它从文档中提取的元素的各种元数据。例如，你可能想知道哪些元素来自哪个页码。你可以像这样提取某个文档元素的元数据： 12doc_metadata = elements[0].metadata.to_dict()print(doc_metadata) Output: 1{'filename': 'note.md', 'filetype': 'text/markdown', 'page_number': 1} 当源文件中的信息可用时，所有文件类型都会返回以下元数据字段：filename、file_directory、date、filetype和page_number。 筹备Transformers 当你准备将你的文本送入转化器模型进行进一步处理时，你可以使用stage_for_transformers函数。这个函数通过将你的文本元素分割成适合模型注意力窗口的大块来准备。 在下面的例子中，我使用了一个叫做SentenceTransformers的库： 12345from sentence_transformers import SentenceTransformerfrom unstructured.staging.huggingface import stage_for_transformersmodel = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)chunked_elements = stage_for_transformers(elements, model.tokenizer) And now I can load all the notes in a specific directory, so I can convert them to embedding vectors later: 1234567all_elements = []root_dir = '/corpus'for directory, subdirectories, files in os.walk(root_dir): for file in files: full_path = os.path.join(directory, file) all_elements += partition(filename=full_path) unstructured 的局限性 这个库也有一些问题和限制。 当加载和解析docx文件时，它不能正确地将子弹头识别为ListItem，大多数情况下将它们标记为NarrativeText或Title。这使得标题识别也不可靠，因为当你查看输出时，你无法确定每个标题实际上是一个标题还是一个被错误地标记为标题的列表项。(issue on github) 当处理大型文档时，没有办法知道每个段落或标题的父类是什么。这可能是一个非常有用的功能，特别是在将数据反馈给LLM的时候。 (issue on github) 替代品 在玩了unstructured之后，我试图看看是否有更好的替代品可以用python来阅读文档。虽然我需要加载各种格式的文件，但我缩小了搜索范围，首先找到阅读docx文件的替代品（因为这是你从Google Drive下载一大文件夹的文件时得到的格式）。以下是我找到的东西： python-docx 它看起来很强大，但操作起来很复杂。 我试着加载和解析了几个docx文件。我遇到的最大问题是加载任何包含超链接的文本。由于某种未知的原因，超链接的文本在最后的输出中被返回为空。这使得它不能用于我的目的，因为链接文本提供了文本中的宝贵信息。 优点：它能够为标题提供标题级别的信息（如Heading 1、Heading 2等）。 docx2txt 它在hood下使用 python-docx。 只返回加载的文档的一个巨大的全文字符串。这就要求我把我的文档分割成有意义的小块，这可不是一件容易的事。 优点：它对超链接没有任何问题，而且输出的文本是可读的、有用的。 优点：它也非常容易使用。 simplify_docx 它在 python-docx 的基础上工作。 这个库基本上将python-docx的复杂输出转换为更容易使用的json输出。 它对超链接也有同样的问题，当段落中有一个链接时，会返回空文本。 所以我现在会继续使用unstructured。值得一提的是，使用LangChain或其他类似的工具可以更容易地完成这一点。然而，我建立这个个人AI助手的部分动机是学习之旅。通过使用unstructured加载文档和其他类似工具进行嵌入等，我对底层流程有了更深的了解，而不是使用LangChain这样的一站式解决方案。 我将在未来的文章中分享更多关于我在构建个人人工智能助手方面取得的进展，敬请关注「坍缩的奇点」， 或到外网关注「茶桁- MAMT」。","link":"/%E4%BD%BF%E7%94%A8Python%E5%BA%93unstructured%E6%8F%AD%E7%A7%98%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE/"},{"title":"使用 Transformers 进行语音转文本的完整入门指南","text":"我与音频数据打交道的次数比我意识到的要多得多。 世界上充满了音频数据和亟待解决的相关问题。我们可以使用机器学习来解决其中的许多问题。您可能对用于训练机器学习模型的图像、文本和表格数据以及用于解决这些领域问题的机器学习并不陌生。随着Transformer架构的出现，解决音频相关问题的准确性大大高于之前已知的方法。我们将学习音频ML的基础知识，使用变压器将语音转换为文本，并学习使用Huggingface库通过机器学习解决音频相关问题。 了解音频机器学习的基础知识并获得相关背景知识。 了解如何为机器学习收集、存储和处理音频数据。 了解一项常见且有价值的任务：使用机器学习将语音转换为文本。 了解如何使用Huggingface工具和库来完成音频任务--从寻找数据集到训练模型，并使用它们利用Huggingface Python库通过机器学习解决音频问题。 本文作为AI系列文章的附加部分，但是并不放入系列之内，以保证其整体性。 自 2010 年代初期深度学习革命发生以来，AlexNet 在识别物体方面超越了人类的专业知识，Transformer 架构可能是自那时以来最大的突破。Transformers 使以前无法解决的任务成为可能，并简化了许多问题的解决方案。虽然它最初的目的是为了在自然语言翻译中获得更好的结果，但很快它不仅被应用于自然语言处理中的其他任务，而且还被跨领域应用——ViT或视觉变压器用于解决与图像相关的任务，决策变压器用于决策强化学习代理中的制作，最近一篇名为 MagViT 的论文演示了 Transformer 在各种视频相关任务中的使用。 这一切都始于现在著名的论文《Attention is All You Need》，该论文介绍了导致Transformers 诞生的注意力机制。本文并不假设您已经了解 Transformers 架构的内部工作原理。 尽管在公共领域和普通开发人员领域，ChatGPT 和 GitHub Copilot 是非常著名的名字，但深度学习已经在许多领域的许多实际用例中使用——视觉、强化学习、自然语言处理等。 近年来，我们了解了许多其他用例，例如药物发现和蛋白质折叠。音频是深度学习尚未完全解决的迷人领域之一；从某种意义上说，Imagenet 数据集中的图像分类是通过卷积神经网络解决的。 我假设您有使用 Python 的经验。基本的Python知识是必要的。您应该了解库及其常见用法。 我还假设您了解机器学习和深度学习的基础知识。 不需要具备Transformers 知识，但会有所帮助。 关于音频数据的注意事项：该平台不支持插入音频，因此我创建了一个包含所有代码和音频数据的 Colab 笔记本。你可以在这里找到它。在Google Colaboratory中启动它，您可以从笔记本上播放浏览器中的所有音频。 您可能已经见过音频 ML 的实际应用。说“Hi, Siri”或“Okay, Google”就会启动各自平台的助手——这就是与音频相关的机器学习的实际应用。这种特殊的应用被称为“关键字检测”。 但在这个领域中，使用 Transformer 很有可能解决许多问题。但是，在开始使用 Transformer 之前，让我快速告诉您在 Transformer 之前如何解决与音频相关的任务。 在《Transformers 》出现之前，音频数据通常被转换为梅尔谱图——描述手头音频剪辑的图像，并将其视为一幅图像并输入卷积神经网络进行训练。在推理过程中，音频样本首先被转换为梅尔谱图表示，CNN 架构将基于此进行推理。 现在我将快速向您介绍“librosa”Python 包。这是一个处理音频数据非常有用的包。我将生成一个梅尔光谱图，让您了解它们的外观。您可以在网上找到librosa 文档。 首先，通过从终端运行以下命令来安装 librosa 库： 1pip install librosa 然后，在您的笔记本中，您必须像这样简单地导入它： 1import librosa 我们将使用与库捆绑在一起的一些数据来探索该库的一些基本功能。 1array, sampling_rate = librosa.load(librosa.ex(&quot;trumpet&quot;)) 我们可以看到librosa.load()方法返回一个音频数组以及喇叭声音的采样率。 12345import matplotlib.pyplot as pltimport librosa.displayplt.figure().set_figwidth(12)librosa.display.waveshow(array, sr=sampling_rate) 这会将音频数据值绘制成如下图： 在 X 轴上，我们看到时间，在 Y 轴上，我们看到剪辑的幅度。通过以下方式收听： 123from IPython.display import Audio as audaud(array, rate=16_000) 您可以在我为此博文创建的Colab 笔记本中聆听声音。 使用 librosa 直接绘制梅尔谱图。 1234567891011121314151617import numpy as npS = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=128, fmax=8_000)S_dB = librosa.power_to_db(S, ref=np.max)plt.figure().set_figwidth(12)librosa.display.specshow(S_dB, x_axis=&quot;time&quot;, y_axis=&quot;mel&quot;, sr=sampling_rate, fmax=8000)plt.colorbar() 我们使用梅尔谱图而不是其他表示形式，因为它比其他表示形式包含更多的信息——一条曲线中的频率和幅度。您可以访问有关 Analytics Vidhya 的这篇精彩文章，了解有关频谱图的更多信息。 这正是 Transformer 之前的音频 ML 中的大量输入数据的样子，用于训练卷积神经网络。 正如《Attention is All You Need》论文中介绍的那样，注意力机制成功地解决了与语言相关的任务，因为从高层次来看，注意力头在预测下一个序列时决定序列的哪一部分比其他部分更值得关注令牌。 现在，音频是序列数据的一个非常合适的例子。音频自然是由自然界或我们的语音器官（例如人类语音或动物声音）的振动产生的连续信号。但计算机既不能处理也不能存储连续数据。所有数据都是离散存储的。 音频的情况也是如此。仅存储特定时间间隔的值；这些功能足以听歌、看电影以及通过电话或互联网与我们自己交流。 变压器也处理这些数据。 就像NLP（自然语言处理）一样，我们可以根据不同的需求使用不同架构的Transformer。我们将使用编码器-解码器架构来完成我们的任务。 如前所述，我们将在每个流程步骤中使用 Huggingface 库。您可以导航到 Huggingface 数据集中心来查看音频数据集。我们将在这里计算的数据集是 MINDS 数据集。它是来自不同语言的说话者的语音数据的数据集。数据集中的所有示例都带有完整注释。 让我们加载数据集并对其进行一些探索。 首先，安装 Huggingface 数据集库。 1pip install datasets pip install 确保我们下载的数据集库增加了对音频相关功能的支持。 然后我们探索 MINDS 数据集。我强烈建议您浏览数据集的Huggingface 页面并阅读数据集卡。 在 Huggingface 数据集页面上，您可以看到数据集具有非常相关的信息，例如任务、可用语言和使用数据集的许可证。 现在我们将加载数据并了解更多信息。 123456from datasets import load_dataset, Audiominds = load_dataset(&quot;PolyAI/minds14&quot;, name=&quot;en-AU&quot;, split=&quot;train&quot;)minds = minds.cast_column(&quot;audio&quot;, Audio(sampling_rate=16_000)) 请注意数据集的加载方式。名字在前，我们只对澳大利亚口音英语感兴趣，我们只对训练分组感兴趣。 在输入训练或推理任务之前，我们希望所有音频数据具有相同的采样率。这是通过代码中的“Audio”方法完成的。 我们可以研究个别例子，如下所示： 12example = minds[0]example {‘path’: ‘/root/.cache/huggingface/datasets/downloads/extracted/a19fbc5032eacf25eab0097832db7b7f022b42104fbad6bd5765527704a428b9/en-AU~PAY_BILL/response_4.wav’,‘audio’: {‘path’: ‘/root/.cache/huggingface/datasets/downloads/extracted/a19fbc5032eacf25eab0097832db7b7f022b42104fbad6bd5765527704a428b9/en-AU~PAY_BILL/response_4.wav’,‘array’: array([2.36119668e-05, 1.92324660e-04, 2.19284790e-04, …,9.40907281e-04, 1.16613181e-03, 7.20883254e-04]),‘sampling_rate’: 16000},‘transcription’: ‘I would like to pay my electricity bill using my card can you please assist’,‘english_transcription’: ‘I would like to pay my electricity bill using my card can you please assist’,‘intent_class’: 13, ‘lang_id’: 2} 这很容易理解。它是一个带有级别的 Python 字典。我们已经存储了路径和采样率。查看字典中的转录键。当我们对自动语音识别感兴趣时，它包含标签。[“audio”][“aray”]包含我们将用于训练或推断的音频数据。 我们可以轻松收听任何我们想要的音频示例。 123from IPython.display import Audio as audaud(example[&quot;audio&quot;][&quot;array&quot;], rate=16_000) 您可以在Colab Notebook中收听音频。 现在，我们清楚地了解数据的外观及其结构。我们现在可以继续从自动语音识别的预训练模型中进行推断。 Huggingface hub 有许多模型，可用于各种任务，如文本生成、摘要、情感分析、图像分类等。我们可以根据我们想要的任务对中心中的模型进行排序。我们的用例是语音到文本，我们将探索专门为此任务设计的模型。 为此，您应该导航到https://huggingface.co/models，然后在左侧边栏上单击您想要的任务。在这里，您可以找到可以立即使用的模型，或者找到一个很好的候选模型来微调您的特定任务。 在上图中，我已经选择了自动语音识别作为任务，并且我得到了右侧列出的所有相关模型。 注意不同的预训练模型。像 wav2vec2 这样的一种架构可以有许多针对特定数据集进行微调的模型。 您需要进行一些搜索并记住可用于使用该模型或微调的资源。 我认为Facebook 的wav2vec2-base-960h将适合我们的任务。我再次鼓励您访问模型页面并阅读模型卡。 Huggingface 有一个非常友好的 API，可以帮助完成各种与 Transformer 相关的任务。 之前，我们找到了任务所需的模型，现在我们将其与上一节中看到的 Pipeline 方法一起使用。 首先，安装 Huggingface 变压器库。 1pip install transformers 然后，导入 Pipeline 类并选择任务和模型。 1234567from transformers import pipelineasr = pipeline(&quot;automatic-speech-recognition&quot;, model=&quot;facebook/wav2vec2-base-960h&quot;)print(asr(example[&quot;audio&quot;][&quot;example&quot;])) # example is one example from the dataset 输出是： 1{'text': 'I WOULD LIKE TO PAY MY ELECTRICITY BILL USING MY CAD CAN YOU PLEASE ASSIST'} 您可以看到这与我们上面看到的注释非常匹配。 这样，您就可以从任何其他示例中得到推论。 在本指南中，我介绍了音频数据处理和探索的基础知识以及音频机器学习的基础知识。在简要讨论用于音频机器学习的 Transformer 架构之后，我向您展示了如何在 Huggingface 中心使用音频数据集以及如何通过 Huggingface 模型中心使用预训练模型。 您可以使用此工作流程解决许多与音频相关的问题，并通过利用变压器架构来解决这些问题。 音频机器学习涉及通过机器学习技术解决音频领域现实世界中出现的与音频相关的问题。 由于音频数据存储为数字序列，因此可以将其视为与序列相关的问题，并使用我们已有的用于解决其他序列相关问题的工具来解决。 由于 Transformer 成功解决了与序列相关的问题，我们可以使用 Transformer 架构来解决音频问题。 由于语音数据和音频数据通常由于年龄、口音、说话习惯等因素而存在很大差异，因此针对特定数据集使用微调的解决方案总是更好。 Huggingface 拥有许多与音频相关的解决方案，涉及数据集、训练模型以及使用和调整训练和微调的简单方法。 Huggingface Audio ML 课程，了解有关音频机器学习的更多信息 Allen Downey 的《Think DSP》深入研究数字信号处理 Q1. 什么是音频机器学习？ 答：音频机器学习是使用机器学习技术解决与音频数据相关的问题的领域。示例包括：通过关键字检测打开和关闭智能家居中的灯，通过语音转文本向语音助手询问当天的天气等。 Q2。如何收集机器学习的音频数据？ 答：机器学习通常需要大量数据。要收集音频机器学习的数据，必须首先决定要解决什么问题。并收集相关资料。例如，如果您正在创建一个名为“Jarvis”的语音助手，并希望用“Good day, Jarvis”这句话来激活它，那么您需要收集来自不同地区、不同年龄、属于不同国家的人说出的这句话。多种性别 - 并使用适当的标签存储数据。在每个音频任务中，标记数据非常重要。 Q3。什么是机器学习中的音频分类？ 答：音频分类是一项机器学习任务，旨在将音频样本分类为一定数量的预定类别。例如，如果在银行部署音频模型，则可以使用音频分类根据客户的意图对来电进行分类，以将呼叫转发到适当的部门（贷款、储蓄账户、支票和汇票、共同基金） ， ETC。","link":"/%E4%BD%BF%E7%94%A8Transformers%E8%BF%9B%E8%A1%8C%E8%AF%AD%E9%9F%B3%E8%BD%AC%E6%96%87%E6%9C%AC/"},{"title":"14. 使用链式调用简化多步提示语","text":"Hi, 大家好，我是茶桁。 OpenAI 的大语言模型提供了 Completion 和 Embedding 两个核心接口。 我们可以通过增加提示语（Prompt）历史记录来提高模型的回答准确性和自然性。还可以将 Embedding提前索引好存起来，以此做到让AI根据外部知识来回答问题， 在我们多次与AI对话的过程中，讲AI返回的答案放在新的问题里，那么我们就可以让AI帮主我们给自己的代码撰写单元测试了。 以上这些方法是自然语言类应用中常见的模式。为了方便应用开发者使用这些模式，开源社区开发了名为 Langchain 的开源库，使用 Langchain，我们可以更加快速地实现之前利用大语言模型实现过的功能，并且可以更好地将模型集成到我们的业务系统中，实现更加复杂、有价值的功能。 何谓链式调用 在第 11 讲中，我们学习了 llama-index 的使用，并在此过程中已经安装了 Langchain。虽然 Langchain 也有类似 llama-index 的功能，但这不是 Langchain 的主要卖点。Langchain 带来的第一个主要优势就在于它的名字，也就是链式调用。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E4%BD%BF%E7%94%A8%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%E7%AE%80%E5%8C%96%E5%A4%9A%E6%AD%A5%E6%8F%90%E7%A4%BA%E8%AF%AD/"},{"title":"供应链的根本是协作，那为什么不协作","text":"我们知道，供应链是采购把东西买进来，生产来加工增值，物流负责配送给客户。自从有现代企业以来，就有人做采购，有人做生产运营，有人做物流配送。也就是说，采购、运营和物流管理由来已久，至少有上百年的历史，那为什么供应链管理是个新概念，直到20世纪80年代才出现？根本原因：单一指标驱动下，职能之间山头林立，协作度低，形不成供应链。让我细细道来。 职能之间山头林立，协作度低，形不成供应链 传统模式下，企业是职能导向，驱动员工行为的是职能目标，即自上而下的目标。比如对采购来说，就是采购价格最低，因为老板对采购的最大要求就是省钱；对生产来说，就是产能利用率最高，这也是老板最为关注的。你知道，价格没有最低，只有更低，如果牺牲交付、质量和服务的话。生产也类似：为了提高产能利用率，降低单位生产成本，那就减少换线，以延长交付周期，牺牲交付绩效为代价。 也就是说，在传统模式下，驱动员工行为的是竖向的效率型指标（比如成本更低、产能利用率更高），缺乏横向的服务型指标（比如交付更快、质量更好）。在绩效考核的驱动下，每个职能关注的重点是顶头上司，而不是兄弟职能的诉求，这样，职能与职能之间就串不起来，形不成供应链。这也是职能之间协作度低的根本原因。 这也是为什么兄弟职能之间经常互相挖坑，不管是自觉还是不自觉地；而上下级之间就很少互坑，协作也容易多了。这里的关键是强相关的指标：上级的目标百分之百传递给下级，下级的绩效也百分之百汇总给上级，大家是同一条绳子上的蚂蚱。但跨职能之间则不是，职能之间的横向联系不够强。也就是说，纵向指标之下，你关注的是来自上司的需求；横向指标缺失，你就不会那么关注兄弟职能，也就是内部客户的诉求。结果就是山头林立，局部优化盛行。 当然，有人会说，我也有横向指标啊，比如交付和质量。你当然有，但问题是，把你敲得满头是包的，是没完成你老板的事，还是兄弟职能不满意？或者说，让你晚上睡不着觉的，是你老板的事，还是兄弟职能的？如果是前者的话，说明横向的质量、交付等指标还是不够强大；支配我们行为的，还是自上而下的指标。 而解决方案呢，就是建立强相关的横向指标，让职能之间有类似于上下级之间的强联系。对于每个具体的职能、具体的员工来说，他们不但要有纵向的效率指标（比如成本、产能利用率、库存周转率），还要有横向的服务指标（比如按时交货率、质量合格率），这是一对表面矛盾，但实际相辅相成的指标[1]，是打破职能壁垒，促进跨职能协作，形成供应链的关键。 这道理不难，你天生就懂，因为兄弟职能不配合，你的第一大招就是想方设法让对方“背指标”，背的就是横向的服务型指标。那为什么总是给“背”不上呢？或者说名义上“背”上了，实际上却没有呢？ 这里的根本原因有二：其一，横向指标在绩效考核中权重太低，没法起到“强相关”的作用，引导员工的行为改变。其二，企业的管理精细度不够，没法有效客观量化横向指标——没法客观量化，就不知道；不知道，就没法管理，还是没法“强相关”。 第一个原因无须多言。对于第二个原因，让我们拿供应商的按时交货率为例来说明。之所以用按时交货率，是因为这是所有横向指标中最为直观，也最为简单明了的：要么按时，要么不按时，有什么可争辩的？且慢，这问题远没有那么简单。 先说什么是按时。计划说，按时就是能够满足客户需求，也就是说，以客户的需求日期为基准。采购马上就有异议：供应商的正常交期是30天，客户的需求日期只给3天，这怎么能做到呢？不公平。那什么叫公平呢？采购就说按照供应商的正常交期，要么是合同约定，要么是报价时约定。这时候销售、计划、生产马上“跳”起来了：这世界不是个完美的世界，如果客户每次都给我们足够的响应周期，那还要采购干什么？ 就这样，计划基于需求日期，3月1日就要货；采购基于标准交期，3月31日才交付。两个极端，对另一方都不公平。不公平就没有约束力——连法律都有规定，强迫签订的不公平合同不具法律效力。那什么叫公平？供应商说，我理解你们3月1日要货，但物理定律没法违背：车工需要x天，铣工需要y天，最后的精加工需要z天，也就是说，最快也是3月15日，否则要货没有，要命一条。就这样，计划、采购和供应商三方达成一致，3月15日就成为按时不按时的标准，也就是说，基于供应商的承诺日期。 这道理很简单，是不是？没错，对于一个具体的订单来说，这是不难。但想想看，一个公司，每天动辄有几十几百个订单，每个订单都这么来回拉锯，达成三方一致，可不是件容易的事。好不容易达成一致，第二天需求变了，得，又得重新来一次。这工作量有多大，离开电子商务的支持，简直不可想象。 在电子商务发达的企业，采购订单由ERP自动生成，发给供应商；供应商确认交期、数量、单价，通过电子商务传递给采购方；如果供应与需求匹配，这就作为供应商的承诺写入ERP，成为后续判断是否按时的标准，不需要任何人工介入；如果不匹配，系统会自动提醒供应商做出更好的承诺；还不够好的话，采购员、物控员、催货员就人工介入，打电话，发邮件，找老板，督促供应商改进交付，直到供应商做出三方能够达成一致的承诺。第二天需求变了，这样的流程就重来一次。 看得出，有电子商务支持的话，百分之八九十的事儿由信息系统做了，员工只是负责那5%、10%的例外；没有电子商务的话，员工就不得不把所有的情况都当例外，我还没见过一个公司，能人工确认每个订单的供应商承诺日期，并随着需求日期的更新而更新。遗憾的是，大多数企业都没有这样的电子商务，所以就没有能力做精、做细，在订单和料号层面客观统计按时交货率。 没有三方认可的承诺日期，计划就基于需求日期统计，按时交货率自然很差；采购为了自保，就基于正常交期统计，按时交货率自然很好。告到老板那里，老板一看，双方都有道理啊，只好批评教育，再宣教一番“以客户为导向”，这事儿就不了了之了。结果呢，计划只能以内部客户的身份，从道义上给采购压力；而采购呢，虽然“背”着供应商的按时交付指标，但实际上形同虚设，起不到“强相关”的作用，驱动他们的仍然是单一的价格指标。 最简单的按时交付都这么难以客观统计，质量、服务等指标就更难客观量化。就拿供应商的质量问题来说，每一个质量问题，都意味着生产线、质检、供应商以及设计之间无穷尽的扯皮，大量Email乱飞，大多企业根本没有资源来梳理清楚。同理，没法客观统计的就没法管理，这样，供应商质量指标就没法落实到采购头上，驱动采购的呢，依然是自上而下的成本指标。 做不精细，企业没法有效量化横向指标，就不得不借助企业文化来推动跨职能协作，让大家“学雷锋”，但没法从根本上解决问题。 既然职能之间的横向指标难以客观建立，有些企业就采取组织措施，成立集成供应链部门，让采购、运营、物流、计划、客服等职能统一汇报到同一个总监，通过组织措施打通这些职能之间的壁垒。 图1-6所描述的，就是一个本土名企的“集成供应链”。该公司设立首席供应官，与负责营销、产品的两位老总平行，一起汇报给CEO。在首席供应官下，有负责供应商选择的寻源、负责工厂的生产，以及端对端的供应链。 有人或许会问，既然寻源与生产都是供应链的一部分，为什么没有包括在集成供应链里？这里主要有两个原因：其一，生产管理成百成千的员工，有很多琐碎杂务；寻源要跟设计、供应商打交道，有很多商务关系要维护，供应链总监的精力有限，没法对付那么多的事情。其二，成本压力大的时候，企业就倾向于集中采购，把寻源单列出来，在更高层面整合需求，增加规模优势，获取更好的采购价格。 图1-6 集成供应链是通过组织措施打通部门墙 有趣的是，也正是在这个名企，寻源有时候归供应链，有时候又独立出来。其后的驱动因素呢，就是企业的业务需求：当速度不够快的问题更大时，寻源划归供应链，从寻源到订单处理都在同一个职能，以快速响应市场需求；当成本不够低的问题突出时，寻源就单列出来，以获取更大的规模优势。很多企业的采购时而集中，时而分散，后面的驱动因素也是一样。 集成供应链让客服、计划、执行采购、仓储、配送等职能处于同一部门，其好处是，即便职能之间没有客观的横向指标，也可以通过组织措施，促进这些子职能之间的协作，以控制局部优化，推动全局优化。 比如有个公司，原来采购、物流分别汇报给不同的总监，再到不同的副总，最后到同一个高级副总裁。两个职能，两条迥异的汇报线，从员工到经理到总监到副总，都是单一指标驱动：采购希望供应商发货越快越好，物流希望运输成本越低越好，就经常出现互坑的情况，比如采购员动不动就让加急运输，而加急运输费用呢，则由物流部门来买单。 当这个公司成立集成供应链后，采购经理和物流经理都汇报给同一个供应链总监，总监层面既对采购的按时交付负责，也对物流的运输成本负责。在一对相互矛盾的指标驱动下，总监一看到加急运费那么高，就马上找手下采购经理的麻烦；采购经理就找采购员的麻烦，于是采购员也就“理性”多了，再也不敢动不动就24小时加急，超额的物流费用也就得以控制。 集成供应链的另一个好处是“冤有头，债有主”，给销售等内部客户一个解决方案。当没有集成供应链时，销售问责计划，为什么交付不按时；计划一转身，就把问题推给了生产，说生产不及时；生产自然有采购垫背，说供应商没有按时交付；采购就把设计拉出来，说设计变更；而设计，则把球踢给了销售，说都怪客户的需求变更。最后，没有一个职能真正对销售负责。成立集成供应链部门后，责任到此为止：从接到客户订单开始，计划、采购、生产、包装、配送，都归供应链总监负责，供应链总监对这些职能“要打要骂”随便，但最终的交付呢，找供应链总监就行了。 业界人士说 以前PMC、制造、品保各自独立的时候，产线和供应商一旦有问题，马上就暴露出来；现在把这些职能集成到生产事业部（跟集成供应链类似——作者注），供应端的问题反倒不容易暴露，而一旦发现，就是大问题。 刘宝红答 在供应链上，没问题是最大的问题。这就如小孩子们在一起，总是会打打闹闹的；一旦没声音了，有经验的妈妈都知道，完了，肯定是在什么地方干坏事呢。 没有一种组织结构是完美的。一种组织结构解决了一些问题，必然会产生另一些问题。关键是要看解决的问题多，还是制造的多。但如果这种集成能够更好地解决更多问题，集中、集成还是值得的。 组织越是集成、集中，组织内的问题就越不容易暴露。比如当生产和计划分离时，你会经常听到生产抱怨计划；而当生产部门自己做计划、自己做执行的时候，抱怨的声音就小了很多——谁会自己抱怨自己呢？但你知道，那并不是因为生产自己做计划做得更好。 有些企业习惯于独立各职能，目的之一就是暴露问题。我的经验是，越是管理粗放的企业，比如大型央企、国企和内地的一些大型民企，职能之间的集成度越低，职能与职能之间的监督就越强，防止贪腐等行为。当然，这种多权分立会造成别的问题，比如唯一责任人缺失，在后文的“多权分立，供应商成了公共草地”部分还会详细讲到。 实践者问 供应链管理部门得如何设置，才能让供应链更加有效？ 刘宝红答 不知道，因为组织结构一定要跟业务需求联系起来，才能讨论有效无效。比如当成本压力大时，集中采购是很好的组织结构；但是，当速度不够快成为大问题时，集中采购就不是有效的组织形式。 这里我要补充的是，你不一定得有供应链管理部门，才能管理供应链。供应链管理更多是流程型管理，而不是组织型管理。流程稳健，完全可以不要“供应链管理”部门。比如苹果就没有“供应链管理”部。我的老东家也是。流程稳健的标志是职能之间的横向指标完善。比如在一些企业，计划、采购、生产、物流等职能之间设立了强相关的指标，在计划的驱动下，各部门各司其职，也能取得良好的供应链绩效。 [1] 比如对采购来说，既要价格低，又要质量好；对计划来说，既要交付好，又要库存低；对设计来说，既要产品性能好，又要满足目标成本。这些都是表面上相互矛盾，实际上却一致的指标。工作做到位，两者都能达到。比如计划做好了，知道计划客户要的，交付就好；也知道不计划客户不要的，库存就低。这点在《供应链的三道防线：需求预测、库存计划、供应链执行》一书中有详细阐述，刘宝红、赵玲著，机械工业出版社于2018年出版。 小贴士 供应链管理的“儒家”与“法家” 传统的日本供应链是长期关系，或者说，更像“儒家”的做法。[1]在长期关系下，绩效考核相对很次要。这就如一家人，相互之间很少会设定指标。而约束双方行为的呢，也正是长期关系，是未来，因为在长期关系下，双方都有很多可失去的，所以就更加理性。 比如在论资排辈的终身雇用制下（当然，现在的日本也早已不尽如此），员工表现不好，就没有好的晋升机会；跳槽后，又得从头开始，从最基本的做起，损失反倒更大，这促使员工在现有工作上尽职尽责（当然也承受很多委屈和压力，你到日本的地铁上，一眼望去，职业人的满头灰白就是证明）。同理，在长期合作下，供应商知道未来一部分业务是它的，如果不把现在的事做好，风险就是失去未来生意，这也驱使供应商更好地干活。 但是，对于北美和中国企业来说，这就很难适用。美国和中国其实惊人地相似，放在企业行为上，那就是短期关系。如果非要说有什么不同的话，那就是中国比美国更短期罢了。不管是企业之间，还是企业与员工之间，短期关系意味着没有未来；没有未来就意味着没有可失去的；没有可失去的，你自然就没法拿未来来约束对方。那怎么办？就只能推行“法家”的做法，基于契约来管理。 契约有两种：其一，竖向的契约，这是上下级之间的契约，也是最基本的契约，驱动职能内部上下级之间的协作；其二，横向的契约，这是兄弟职能、公司与公司之间的契约，驱动跨部门、跨公司协作。前者体现为纵向指标，后者体现为横向指标。纵向指标大家都熟悉，横向指标的好处呢，就是别光顾着把所有的水都放到自己田里：种好自己的一亩三分地要紧，分点水到邻居的地里也要紧。分多少呢，不是靠发扬风格，而是约定好的，即横向指标。 过去三四十年来，本土企业从没有契约的“大锅饭”，过渡到有竖向契约的市场经济，现在正在建立横向契约的路上。但是，对本土企业来说，契约化还远未完成。一方面是文化原因：传统的文化是基于关系的，要变成冷冰冰的契约关系，会有各种挑战；另一方面是能力原因：企业的管理精细度还不够，没法有效客观量化绩效，建立强相关的横向指标。 于是，很多企业就“儒”“法”并举，一方面大张旗鼓宣扬企业文化，这是儒家的做法；另一方面推行绩效管理，典型的法家做法。而做得好的企业，这两方面都做得不错。 就拿华为和海尔来说，这是一南一北两个非常有代表性的本土名企，在外人看来都是企业文化非常强的企业，似乎靠的就是任正非和张瑞敏的一张嘴。但是，华为和海尔不是靠企业文化吃饭的，如果把它们理解为儒家信徒就大错特错了；企业文化背后，它们靠的是异常严酷的绩效考核，典型的“法家”做法，而这正是局外人不知道，或者不愿意知道的。 比如海尔的“日清日毕”，字面上文质彬彬，翻译成白话可就不了：今天的事儿没做完，晚上你就不要回家。他们甚至为每一个员工独立核算，你对内部客户做了多少事，那是你的营收；内部供应商为你做了多少事，那是你的成本，力求每个人、每件事的账都算得清清楚楚。 再比如华为的能上能下，几年前我跟他们的两位销售高管会面，其中一位总监指着另一位副总说，以前他们两个的职位正好相反，后来因为绩效原因，两个人就倒过来了。能者上，不能者下，这话说起来多么容易，但在注重关系文化的氛围里，有几个企业能真正做到？华为可以说把法家精神发挥到了极点。 这里要补充的是，传统的日本企业虽然是儒家做法，绩效考核不是很严格，但不要忽视它们的绩效统计能力。在我所熟悉的全球企业中，日本企业可以说是数据最齐全，分析最到位的。夸张点说，它们的每一件事都有数据支持，员工的Excel用得烂熟，决策更多的是基于数据，而不是判断。 你可以不算账，但不能没有账。放在绩效管理上，就是你不一定要考核，但不能不统计。要知道，企业大了，几亿元、几十亿元的规模，离开绩效统计，就没人知道真相——不统计就不知道，而不知道就无法管理。管理能力的一大标志就是数据的充分与否。放在古代帝国的文明程度上，就是能否造册征税：中原文明有能力做全国普查，能够按丁、按亩征税；而草原上的野蛮人呢，就像匈奴，称雄北方几个世纪，往往连自己有多少人马都弄不清，最后连片瓦片都没留下，就消失在历史的长河中。 [1] 这里说的“传统”，主要指20世纪日本崛起的那段时间，大致在八九十年代前后。当时美国系统学习日本的做法，现在能看到的关于日本管理方式的文献，大都是那个时段产生的。当然，过去二三十年里，日本经历了显著的变化，管理方式也在变化，在有些做法上与欧美更加趋同，因文献不足，这里就不予探讨了。 案例 找替换供应商时，技术与质量不积极 有位职业人新近晋升供应链经理，全面负责公司的采购、质量和物流管理。摆在他面前的第一要事就是供应商质量问题：有些关键的供应商，质量问题一直没法解决。这位经理的解决方案呢，就是启动供应商淘汰机制，另行选择更好的供应商。但是，技术和质量人员都不够积极。他问我该如何设置绩效考核，提高这些职能与采购协作的意愿，尽快找到替换供应商。 案例企业年度营收十亿元左右，不大也不小，职能之间的部门导向有，但壁垒远远没有大公司的那么高。在开发替换供应商上，技术、质量人员积极性不高，看上去是个“不愿意”的问题，其实是因为供应商没选好、没管好，根本上是个“不能够”的问题。而解决方案呢，要从选好、管好供应商，争取首发命准上找，而不是第一个供应商没选好、没管好，就再找一个，让各部门重复投入资源。 遗憾的是，很多人分不清“不愿意”和“不能够”，误把后者当前者，一味地在绩效考核上做文章，自然没法解决问题。 就拿案例公司来说，供应商选好后，后续管理跟不上，有选择、没管理，好供应商也会变坏（这点后文还会详细谈到）。就质量问题来说，他们先是单纯依靠质量部门来应对，比如驻场管理，自然没法解决；最终变成了整体供应商问题，采购就拿淘汰代替管理，让技术、质量开发替代供应商，意味着更多的技术验证、质量评估工作。质量、技术虽然不知道根本的解决方案，却明白找替代供应商并不能真正解决问题，因为新供应商一进来，也会有老供应商的问题——这两个职能早已吃过很多亏，受过很多苦，对这点有很多切身体会。 所以，在找新供应商上，技术与质量消极反抗，出工不出力，也就不足为奇了。而解决方案呢，不是给这两个职能定指标，让他们更加愿意开发替代供应商；而是要改进供应商的选择与管理，争取第一次就把供应商选到位、管到位。 类似的情况很多。 比如有个企业的供应链老总说，他们的销售预测保守，不愿意建库存，导致供应链赶工加急。怎么才能促使销售多建库存？这不是个绩效考核问题，这是个能力建设问题：该公司采取“销售提需求”的做法，让一线销售人员做需求预测；那么多的一线销售，每人预测自己的客户需求，预测的颗粒度那么小，预测的准确度注定很低；预测准确度低，库存的风险大，销售人员建库存就自然保守。所以解决方案不是给销售绩效考核，而是改善需求预测流程，比如在公司层面集中预测，兼顾关键销售人员的职业判断，提高预测的准确度[1]，让销售能够更有效地管控库存风险，从而更愿意建库存。 延伸阅读 对于跨职能协作，我的基本观点是要通过绩效考核，设立强相关的横向指标来解决不愿协作的问题。当然，过犹不及，绩效考核也有副作用。比如《孔雀效应》一文中讲到的单向选择，还有我们都熟悉的“手术很成功，病人却死了”，都是常见的“绩效考核病”。在我的“供应链管理专栏”（www.scm-blog.com） 上搜索题目，或者扫描二维码，即可阅读《孔雀效应》全文。 [1] 对于一线销售为什么做不好需求预测，详细内容可参见我和赵玲的《供应链的三道防线：需求预测、库存计划、供应链执行》一书，106~117页。","link":"/%E4%BE%9B%E5%BA%94%E9%93%BE%E7%9A%84%E6%A0%B9%E6%9C%AC%E6%98%AF%E5%8D%8F%E4%BD%9C%EF%BC%8C%E9%82%A3%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%8D%8F%E4%BD%9C/"},{"title":"17. 利用LangChain让AI做决策","text":"Hi，大家好。我是茶桁。 在第 11 讲中，我向您介绍了如何将各种资料内容向量化，借助Llama-index建立索引，对我们自己的文本资料进行问答。在过去的3讲中，我们深入了解了如何使用Langchain。该工具可帮助我们整合AI对语言的理解和组织能力、外部各种资料或者SaaS的API，以及您自己编写的代码。通过整合这些功能，我们可以使用自然语言完成更复杂的任务，而不仅仅是闲聊。 但到目前为止，我们所有基于ChatGPT的应用基本上都是“单项技能”，例如前面关于“藤野先生”的问题或上一讲中查询最新天气或通过Python进行算术运算。这本质上是限制AI只针对我们预先索引或实时搜索的数据进行回答。 给AI加上多项选择能力 要做一个能跑在生产环境上的 AI 聊天机器人，需要的不止一个技能。在电商领域，最起码需要以下三个技能： 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E5%88%A9%E7%94%A8LangChain%E8%AE%A9AI%E5%81%9A%E5%86%B3%E7%AD%96/"},{"title":"20. 尝试让机器拥有声音","text":"大家好，我是Hivan。 好久不见了，今天我们来讨论下如何让机器拥有声音。 回顾一下我们上一讲的内容，我们已经成功使用Whisper模型使得AI能够理解我们说的话。这为我们带来了很多应用，例如让AI代替我们收听播客并总结内容。然而，这只是单向的交流模式。现在，让我们探索更深入的可能性，让AI不仅仅能够“听懂”我们的话，而且通过ChatGPT回答我们的问题，并将所有内容合成语音，用声音与我们进行双向交互。 这就是我们本次探索的主题：让AI说话。我们将学习如何使用云端API进行语音合成（Text-To-Speech），同时也会介绍开源模型，使您能够在本地CPU上实现这一功能，让数据安全问题不再是困扰。 让我们一起，给机器赋予声音吧！ 使用 Azure 云进行语音合成 语音合成技术早已迈入成熟阶段，你所听到的许多短视频配音都借助此技术实现。无论是科大讯飞、阿里云、百度、AWS Polly还是Google Cloud，国内外的大公司纷纷提供了类似的云服务。然而，今天我们将带您领略微软Azure云的语音合成API，主要是因为以下两个原因： 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E5%B0%9D%E8%AF%95%E8%AE%A9%E6%9C%BA%E5%99%A8%E6%8B%A5%E6%9C%89%E5%A3%B0%E9%9F%B3/"},{"title":"19. 快速倾听和总结音频内容","text":"Hi，大家好，我是茶桁。 其实到第18章的时候，我们处理文本的内容就全部都结束了，从本节课开始，我们要开始学习如何处理音频和图像。 我不知道有没有人和我一样的习性，就是比起视频和音频文件来说，还是跟喜欢看文本文件。这其中最主要的一个原因就是因为文本内容我们可以准确定位，而对于文本内容的接收速度还取决于我们输入设备（眼睛和处理信息的脑部）速度。而音频或者视频则不然，我们必须听完讲述者所说的话，即便你开到2倍速，速度依然受限，而且无法准确定位。那有没有什么办法能快速完成对音频文件内信息的获取呢，自然就是将语音内容转换成文本的能力。 其实到这一步，类似于Premiere或者剪映等剪辑软件都可以完成，不仅如此，在AI大行其道的今天，市面上应该也有不少Audio2Text的服务或者应用。接下来，我们要讲的就是一个杀手级服务了。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E5%BF%AB%E9%80%9F%E5%80%BE%E5%90%AC%E5%92%8C%E6%80%BB%E7%BB%93%E9%9F%B3%E9%A2%91%E5%86%85%E5%AE%B9/"},{"title":"观点：我们无法通过改造自己摆脱气候危机","text":"让我们面对现实吧——气候变化是人类最大的失误。我们已经知道它近一个世纪了。科学是清楚的。然而，我们什么也没做。真是太尴尬了。 现在，全球领导人终于开始忙着收拾残局。但是，尽管我们需要的大多数气候解决方案已经存在，但我们似乎无法按照所需的速度和规模部署它们。 简而言之，世界正在变暖，而我们却无法让它降温。去年，人类向大气中排放的\\(CO_2\\)比以往任何时候都多（呃……WTF？）。 可以理解的是，领导者们都害怕极了。这促使他们探索一些非常愚蠢且完全危险的想法。他们最糟糕的脑波之一是地球工程——也就是用地球的气候扮演上帝的角色。（这里使用的“地球工程”并不是指碳去除技术，据我们所知，碳去除技术是相当合法的。） 其中一些建议包括增亮云层、改变海洋的化学成分，或者向大气中发射粒子来使太阳的光线变暗——会出现什么问题呢？ 虽然这些提议听起来像是反乌托邦科幻电影中的内容，但改变地球气候实际上非常容易且成本低廉。 太阳能地球工程是这些“解决方案”中最具争议性的一种。其最受欢迎的衍生产品是平流层气溶胶喷射，涉及将灰尘喷射到大气中，以减少照射到地球表面的阳光量。这项技术的灵感来自于火山云，众所周知，火山云在一次大喷发后可以使整个地球冷却多年。 云增加了地球表面的反射率。平流层气溶胶喷射旨在通过将灰尘喷洒到高层大气中来复制这种效果，以期冷却气候。 虽然平流层气溶胶注入对于阻止全球变暖可能非常有效，但它可能会打开潘多拉魔盒的问题。根据联合国最近的一份报告，干扰全球自然气候可能会破坏臭氧层，改变全球降雨模式，并导致严重的地缘政治紧张局势。 尽管支持者称太阳能地球工程将是对抗变暖的短期措施，但《科学美国人》最近发表的一项研究表明，如果政客们确实决定向大气中发射尘埃，他们可能会在“几个世纪或更长时间”内危险地依赖它。 为了向大气中排放足够的灰尘来抑制变暖，每年可能需要数万次高空飞行。这一过程的突然停止可能会导致温度飙升，其速度可能快于生命的适应速度，这一概念被称为“终止休克”。 还有一个道德问题，即技术修复可以减轻政客和企业尽快脱碳的压力。 简而言之，太阳能地球工程相当于气候变化创可贴。 尽管存在风险，美国政府去年还是启动了一项为期五年的研究计划，探索将更多阳光反射回太空的方法，为进一步资助这项新兴技术奠定了基础。 比尔·盖茨、乔治·索罗斯和 Facebook 联合创始人达斯汀·莫斯科维茨等亿万富翁都表达了兴趣，而 60 名著名科学家则希望进行小规模太阳能地球工程现场实验。 甚至还有一家名为 Make Sunsets 的初创公司，基于其向大气中释放二氧化硫以遏制变暖的承诺而预售碳信用额。 美国初创公司 Make Sunsets 因未经批准进行平流层气溶胶注入测试而被逐出墨西哥。《麻省理工科技评论》的几位研究人员谴责了“Make Sunsets”，称其努力“为时过早”。 预防原则——或者对普通人来说“如果有疑问，就不要考虑”——是健全环境决策的基本前提之一，也是我们今后应该注意的原则。 欧盟本周宣布，呼吁就气候地球工程可能使用带来的风险进行“最高国际级别”会谈，这可能是积极的一步。 欧盟官员在周三的联合通讯中表示：“这些技术给人类和生态系统带来了新的风险，同时也可能加剧国家之间的权力失衡，引发冲突并引发无数道德、法律、治理和政治问题。” “我们不能用造成问题的思维方式来解决问题。 尽管欧盟正在采取预防措施，但它并不完全反对这些技术，而是寻求制定管理这些技术的“规则”。 其他人则采取更强硬的立场。 乌得勒支大学哥白尼可持续发展研究所的弗兰克·比尔曼在去年发表的一份声明中警告说，“人们对太阳能地球工程的风险知之甚少，而且永远无法完全了解。” 比尔曼是一群著名气候科学家的领导者，呼吁就太阳能地球工程达成不使用协议。换句话说，全球范围内禁止其开发。 “太阳能地球工程的研究并不是像其倡导者所说的那样，为预防气候灾难而准备 B 计划。相反，它只会推迟和破坏当前的全球气候政策，”他说。 “此外，现有的国际机构体系无法有效监管这项技术在全球范围内的部署。太阳能地球工程不是解决方案。” 我完全同意，教授。通过扮演上帝的角色来操纵气候，我们不仅面临着使我们的困境恶化的风险，而且还发出了一个危险的信息——人类可以简单地通过设计方法来解决问题，而不是从根本上解决问题（想想广泛的文化、社会和政治）变换）。 正如爱因斯坦的一句名言：“我们不能用创造问题的思维方式来解决问题。”","link":"/%E6%88%91%E4%BB%AC%E6%97%A0%E6%B3%95%E9%80%9A%E8%BF%87%E6%94%B9%E9%80%A0%E8%87%AA%E5%B7%B1%E6%91%86%E8%84%B1%E6%B0%94%E5%80%99%E5%8D%B1%E6%9C%BA/"},{"title":"18. 根据垂直需求微调模型","text":"大家好，我是茶桁。 最近事情太多，这一节课更新的有些晚了。 首先我们先了解一下我们本节课讲要讲一些什么，我们之前介绍过 llama-index 和 LangChain，学习了将大语言模型和自己的知识库组合来解决问题的方法。这个方法中，我们不需要调整我们使用的模型，而是使用嵌入向量索引我们的数据，并在需要时查询索引来解决问题。 然而，我们也可以完全利用自己的数据，创建一个新的模型来解决问题。这种方法是OpenAI提供的微调模型功能。这也是我们要探讨的大语言模型的最后一个主题。 如何微调模型 我们都知道，AI其实是建立在大语言模型之上的，而模型再如何补全，也没有办法全知全能。在很多时候，AI所回答的内容常常错漏百出，甚至于一些垂直领域可能完全词不达意。这些其实都是因为缺少了特定领域的训练数据，而我们要做的，就是要补全这一部分数据进行训练，为我们自己的需求微调出一个擅长本领域的模型。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E6%A0%B9%E6%8D%AE%E5%9E%82%E7%9B%B4%E9%9C%80%E6%B1%82%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B/"},{"title":"不要只看供应链管理的一个方面","text":"供应链管理不是软件？ 其实产生疑惑不无道理。供应链管理的范畴非常广泛，对它的认识就像盲人摸象：很多人是IT背景，他们想到的就是供应链管理软件，例如Oracle、SAP；对很多采购人员来说，供应链管理就是采购和供应商管理，即确保供应商保质、保量、按时提供价格合理的产品或服务；对于生产管理出身的人讲，供应链不过是生产管理的延伸罢了；对于物流行业的人来说，供应链管理则往往被等同于物流运输、车辆调度、仓储管理等。 在国内，很多人分不清供应链管理与物流管理。2010年，我拜访一位本土顶尖企业的首席执行官。该首席执行官几次提到物流管理，说ISM可以在物流管理上做出更大贡献。我想他指的应该是供应链管理，因为ISM侧重于供应链管理中的采购与供应管理，跟物流管理还离得比较远。当然，因为远离操作层，在一个上千亿元人民币规模的大公司CEO看来，这些区别或许不怎么重要。 大致在20世纪90年代，物流和供应链管理两个概念先后传入国内。这并不是说以前就没有物流和供应链——有人类的时候就有，只是不一定这么叫罢了。当时的大背景是物流成本太高，约束着本土供应链的效率。物流成本包括仓储、分销、运输、库存、物料搬运、第三方物流等费用，跟一个国家的基础设施息息相关。当时美国的物流成本是GDP的10%左右，得益于发达的高速公路网和信息基础设施；中国的物流成本是GDP的20%左右，跟当时落后的基础设施不无关系。 在国内，物流是供应链的瓶颈，因而就成为供应链管理的重点，乃至一叶障目，不见森林，让很多人误把物流管理当作供应链管理。当然，过去一二十年来，国内大规模投资高铁、高速公路等基础建设，这一情况得到显著改善，物流成本占GDP的百分比逐年下降，比如到2018年有望降到14%左右。作为对比，美国这些年来也是一路下降，达到7.2%，全球平均水平为11.2%。我想，这跟这些年来信息技术的发展不无关系：信息流驱动产品流，信息流的效率高了，物流的效率也会改善。 到底什么是供应链管理呢？ 供应链是从客户的客户到供应商的供应商，供应链管理是对贯穿其中的产品流、信息流和资金流的集成管理，以最大化给客户的价值、最小化供应链的成本。它是一个综合管理、全局优化的思想，以摆脱单个公司、单个职能层面的局部优化，实现供应链条的全局优化为目标。 在实践操作中，供应链管理由三大块构成： 供应管理（寻源） 运营管理（加工） 物流管理（交付） 跨越企业管理中的供、产、销三大块（如图：） 资料来源：Supply Chain Council. 简单地说，就是采购把东西买进来，生产来加工增值，物流来配送给客户。这三大块是执行职能，由计划驱动——也可以说计划是供应链的第四大领域。计划是供应链的引擎。很多执行层面的问题，看上去是没做到，其实往往是没想到——计划不到位造成的。这也是为什么在供应链运营模型（SCOR）中，计划处于采购、运营和物流之上。 从三大职能上讲，供应管理侧重于采购和供应商管理，使供应商成为公司的有机延伸；生产运营管理力求以最有效的方式完成产品、服务的增值过程；而物流管理则力求以最经济、迅捷的方式把货物从A点流动到B点。 从三条流上讲，产品流从供应商向客户流动，是供应链的实物流（如果是从客户向供应商方向的话，则称为逆向物流）；资金流是从客户流向供应商，是供应链的血液；而信息流则是双向流通，构成供应链的神经系统。 在竖向集成盛行的年代，供、产、销大都处于同一个公司内部。例如20世纪早期的福特汽车，从炼铁厂到零部件再到整车组装，都曾试图集中在自己旗下，尽管从来没有真正集成到这一步。最近二三十年以来，竖向集成解体，外包盛行，这三大功能越来越依赖供应商，例如零部件来自供应商，生产靠外包制造商，物流靠第三方物流公司。作为采购方，对这三部分的集成管理是供应链管理的重点。同样的道理，竞争也不再局限于公司与公司之间，竞争变成供应链与供应链之间的竞争。 值得注意的是，任何一个新的领域，都是在已有领域的基础上发展而来的。供应链管理也不例外，它从供应管理、运营管理、物流管理等分别向相邻的领域扩展而成。反映在学术机构，在北美，虽然专业都叫供应链管理，但不同大学的供应链管理专业侧重点不同。比如在亚利桑那州立大学，供应链专业历来以采购见长，而田纳西大学则侧重物流，麻省理工侧重运输。根本原因呢，就是这些学校的供应链管理是从这些具体领域发展而来的。 除了采购、生产和物流外，工业工程也是供应链管理的“近亲”。在亚利桑那州立大学工业工程的很多教授都在供应链管理系任教，后来甚至有一位成为供应链管理系的系主任。一位在密歇根大学工业工程系就读的博士，研究的却是供应链管理，后来到俄勒冈大学的商学院任教，教授的也是供应链管理。 在美国，很多大学的供应链管理专业设在商学院。比如MBA排名中，一个分支就是供应链管理。除此之外，也有很多工学院设立供应链管理的研究生专业。这几年，有好几个大学设立一年制的供应链管理硕士课程，比如马里兰大学、俄亥俄州立大学、南加州大学等，吸引了大批的中国留学生，造成同质化严重，也是个问题。得州大学达拉斯分校也有供应链专业。 在研究领域，有很多杰出的研究者都是从别的领域来的，对供应链管理专业的建立贡献巨大。比如斯坦福大学的李效良（Hau Lee）教授，他关于“牛鞭效应”的研究可以说奠定了供应链的理论基础。但这些研究大都是他在工学院时做的，师承工业工程、管理科学和运筹学，上溯到MIT的系统动力学（System Dynamics）。李效良在担任《管理科学》（Management Science）杂志主编期间（1997~2002年），在这个管理学领域最权威的学术期刊之一上，刊登了大量的供应链管理文章，可以说让供应链管理正式成为一个专业领域。 在工业界，鲜有能够跨越供应链的三个领域的实践者。尽管很多公司试图把采购、运营和物流等职能集中到一起，组成全球供应链或全球运营部，但下面的分支部门仍旧围绕三个职能划分。道理很简单：没有人能够掌握所有的采购、运营、物流，外加计划等众多领域的专业技能。对供应链管理的认识仍会处于“盲人摸象”状态：采购背景的人说是采购的延伸，物流的人说是物流的延伸，而生产部门则认为是运营管理的延伸。一些流程分析、软件背景的人，则更多地从端对端的流程角度出发，理顺供应链的产品流、信息流和资金流，提供了一个全新的供应链管理视角。条条大路通罗马，这些都可成为公司搭建卓越供应链的起点。 就本土企业来说，20世纪90年代后期，华为导入IBM的集成供应链的概念，旨在打通职能部门之间的横向联系，提高供应链的效率，可以说是开了国内集成供应链的先河。华为所在的电信设备行业批量小，品种多，复杂度高，集成供应链确实是关键的解决方案之一。2005年，联想并购IBM的PC业务，全盘接受了IBM的供应链管理体系。 这几年，供应链管理的概念更加深入各行各业，不光是大企业，还有中小企业；不光是制造业，还有建筑业、电商业、餐饮业等。除了制造业外，还有建筑、零售、服装、餐饮、电商等行业。根本原因呢，是这些企业认识到供应链的价值，认识到打通部门之间的壁垒、通过全局优化来提高公司绩效的重要性。 20多年来，供应链管理在中国遍地开花 有个朋友，曾经担任西贝餐饮的副总裁。他说，餐饮看上去是开餐馆，其实比拼的还有供应链实力。比如原材料的获取、储存、加工等——很多原材料有很强的季节性，比如西贝用的羊肉来自内蒙古草原，内蒙古羊肉最好的在秋季，羊一定要在那两三个月内从内蒙古的牧场收齐、宰杀、冰冻，供后面的一整年用，这些都需要一流的供应链计划和执行来支持。因为餐馆所处位置一般为市内黄金地段，租金很贵，所以店面都较小，大多菜的加工其实是在中央厨房完成的，及时运送到店面后，做些简单的最后加工就上菜了。中央厨房就跟制造业的工厂差不多。从这个意义上讲，西贝这样的餐饮业跟生产、零售业没有本质区别。 2000年国内很少听到供应链管理的概念，更不用说有这专业了；十多年后，国内已经有很多大学设立了供应链管理专业，各种各样的供应链公司如雨后春笋，供应链管理的概念也更加深入。尤其是经历二三十年的高速发展后，企业普遍面临“增长陷阱”[1]，越来越多的人意识到，企业要生存，不但需要开发好的产品（主要是设计的责任）、卖个好价钱（销售的任务），而且要以适当的成本、速度生产出来（供应链的责任）。随着整体经济的进一步成熟、放缓，降本增效的压力必将加剧，而作为降本增效的主要源泉，供应链管理任重道远。 在之后篇幅里，会从采购、运营和物流的角度学习供应链管理，希望能有个全面的认识。 !&gt; 这里是脚注。（由于docsify缺乏脚注功能，所以只能直接写了，没有链接回跳） 「1」：增长陷阱”指企业发展到一定阶段，营收增速放缓，不再增长甚至下跌，而成本由于惯性还会继续上升（比如不管使用与否，设备折旧会照旧；不管生意好坏，员工每年的工资总得加几个点），导致利润率越来越低，甚至亏本。详细内容可参考我的另一本书《供应链管理：高成本、高库存、重资产的解决方案》，机械工业出版社于2016年出版。 采购和供应链管理 在供应链管理的三大职能中，供应管理与供应链管理只是一字之差，可以说是供应链管理的“近亲”。但是，供应管理的重点是供应商这一外在战略资源，与运营管理侧重公司内部生产运营、物流管理侧重产品和信息的流通形成对比。 供应管理起源于采购管理。从严格意义上讲，供应管理的范畴远大于采购管理。但为了行文方便，采购管理和供应管理在本书中通用（如果没有特别注明的话）。 在美国，传统上采购的地位不是很高，因为传统上美国公司的竖向集成度挺高，对外来资源依赖度低。作为管理外来资源的采购部门，其主要任务是围绕订单处理日常交易。简单地说，内部客户（如工程师）说，我要买这个，采购的任务就是下订单，确认价格、交期，把货按时拿到。 采购部门的吸引力有限，就成了那些百无一用的人的最后落脚点。就如我在亚利桑那州立大学读书时，一位叫皮尔森的教授曾经说，如果一个人干不了销售、设计、生产等，那只能去做采购了；如果连钱也不会花，那就只能卷起铺盖另谋高就，去祸害我们的竞争对手吧。 美国如此，中国也是：传统的计划经济下，外在资源主要依靠国家统一调配，公司A的产品给B做原材料，价格都由政府规定了，采购自然也就可有可无了。在那些比较封闭的行业，比如军工、航空业，还能看到传统经济的影子：长期以来，中国的航空业是半竖向集成的，整个行业其实就是中航工业和它的子公司们，行政命令历来扮演重要角色。现在为了开发商用大飞机，得跟那么多的全球供应商打交道，用工业界通用的方式做生意，采购面临的挑战可想而知。 在采购管理上，经常听人说，如果你连花钱都不会（做采购），那可真是百无一用了。就如李鸿章对儿子说，如果你连做官都不会，你可就一无是处了（大意）。其实我们都知道，做官的学问可大了，离开了那些官僚，一个国家的运作就会大受影响。采购也是：采购是一个大职业，尤其是在有些行业，产品成本的百分之七八十都来自供应商的情况下，采购已经远远超越持币购物，而是在管理公司百分之七八十的增值活动——供应商表面上在赚我们百分之七八十的钱，实际上在帮我们做百分之七八十的事，而采购呢，则对选择和管理供应商负责，对这些增值活动负责。 更进一步，常言说得好，卖得好不如买得好：采购每节省一块钱的开支，利润就增加一块；销售增加一块钱的销售，利润大致增加一毛。不管是零售业，还是制造业、服务业，采购的重要性毋庸置疑。在有些行业，比如电商和贸易行业，由于没有生产，供应链的所有增值环节都在供应商处，采购的价值就更大了。 随着很多行业转向外包战略，外购额逐渐增长，成为公司开支中的最大一块，公司对供应商的依赖度越来越高。而作为管理供应商的对口职能，采购的重要性也在日益上升。在美国，设置首席运营官的公司越来越少，设置首席采购官的则越来越多，根本原因就在于增值活动以前主要发生在公司内部，由首席运营官负责，现在则越来越多地外包给供应商，由首席采购官负责。 采购的地位提升，其重心也从订单处理转为对供应商的战略管理，过渡到供应管理。20世纪80年代，麦肯锡的一位顾问在《哈佛商业评论》上发表文章，题为《采购必须成为供应管理》[1]，吹响了这一战略转移的号角。但是，整个过程花了二三十年。2002年，美国采购经理人联合会（NAPM）更名为供应管理协会（ISM），是这一过程的里程碑事件，标志着供应管理正式成为主流。 与采购管理的围绕订单处理相对应，供应管理更侧重供应商的战略管理，通过分析开支、确认需求、评估供应商、选择供应商、签订协议、管理供应商绩效来确保以合适的成本保质保量地获取资源。从时间跨度上讲，供应管理向前延伸到设计和新产品开发，向后延伸到产品的生命周期结束；从影响的对象上讲，供应管理延伸到对公司的资产、现金流等的管理，直接影响公司的盈利。 在北美，有些公司已经开始统计供应管理的贡献，例如净利润率是10%，其中0.5%是供应管理通过降低采购成本等来实现的。首席采购官这一头衔能够与首席财务官、首席运营官等相提并论，也反映了采购与供应管理战略地位的提高。 从供应链的角度来看，采购处于公司的内外结合点，是管理供应链的理想选择。作为采购部门，突破对传统职能的认识，在管理供应链上发挥更大作用，也是提升采购在公司地位的一个有效办法。采购对内管理需求（比如设计的新产品寻源、生产部门的量产需求）、对外管理供应商（比如供应商选择和绩效管理），通过理顺需求来理顺供应，其实就是在管理供应链，或者说管理供应链的一大块。 在一些大型国企、央企，以及管理粗放的民营企业，采购并没有意识到这些。他们对自己的定位主要是招投标，以及供应商出了问题后的应急反应。[2]没有了需求管理，很多需求一落地就是紧急需求，给后续的供应链执行带来很大挑战；没有系统的供应商管理，供应商层面的问题没有解决，导致订单层面的问题不断，供应绩效长期在低水平徘徊。这些都是采购面临的大问题，也是供应链管理的大挑战，我们在后面还会详细讲到。 资源 美国高级采购研究中心（CAPS Research，www.capsresearch.org）。该中心是美国供应管理协会与亚利桑那州立大学合作成立的，有一系列专题研究，侧重采购与供应管理，是全球该领域的顶尖研究机构。 资源 微信公众号“宫迅伟采购频道”，有一系列的采购与供应管理方面的原创文章。 延伸阅读 有一个门类专门讲采购管理（http://scm-blog.com/cat-23），可以阅读更多采购管理方面的文章。 [1] 文章英文名为Purchasing Must become Supply Management，发表于1983年9月，作者为Peter Kraljic（中文译名“卡拉杰克”）。卡拉杰克是麦肯锡的咨询顾问，在德国汉诺威工业大学获博士学位，对采购界的影响深远。有名的“卡拉杰克矩阵”就是由他提出的：他参照投资模型，按照收益影响和供应风险两个维度，把采购项分为四类，区别对待。这是采购管理中一个最为根本的模型，有很大的指导意义。更多细节可参考百度百科“卡拉杰克模型”词条。 [2] 可以说，招投标是采购工作的一部分；但如果是采购的主要任务，这个企业的采购注定是“小采购”。采购的主要任务是选择、管理供应商，招投标是供应商选择的一种方法，而且是很不完美的方法。在招投标盛行的企业、机构，伴随着招投标的往往是供应商的有选择、没管理，供应商绩效一塌糊涂。我们在后文还会详细阐述解决方案。 物流管理：从A点到B点 简单讲完了采购和供应管理，我们来看一下物流管理。 原美国物流管理协会、现供应链管理专业人士协会对物流管理的定义如下： 物流管理是供应链管理的一部分，即为满足客户需求，通过计划、实施和控制，促成产品、服务和信息从发源地到消费点的有效流动及储藏。 这定义有点长，拗口，但说明了几点： 第一，物流管理是供应链管理的一部分。作为美国物流管理方面的权威组织，供应链专业人士协会的定义有相当的权威性，确定了物流与供应链的关系。在2004年，该协会名称从物流管理改为供应链管理，也反映了物流管理向供应链管理的延伸。与此类似，运营管理、采购与供应管理也在向供应链管理延伸、靠拢，从它们的更名上可见一斑：运营管理协会以前叫美国库存与生产控制学会（2004年改名），供应管理协会以前叫美国采购经理联合会（2002年改名）。 第二 ，物流管理的对象是产品、服务、信息的流动与储存。简而言之，就是把产品从A点搬到B点，并处理过程中的服务、信息。值得注意的是，它不负责采购（那是供应管理的任务），也不负责生产（那是生产和运营管理的事）。这个界限表明了物流管理想与运营管理、供应管理三分供应链管理的天下。 第三，物流管理不但管理产品、服务、信息的正向流动（从供应商到客户），而且管理其反向流动（从客户到供应商，即逆向物流）。逆向物流日趋重要，是退货、保修、返修等售后服务的重要一环，也更难管理。国内可能还体会不到，如果你在美国，到沃尔玛这样的大超市去看看，节假日后，退货的队跟买货的差不多长，你就知道逆向物流面临的挑战了。 按照上述定义，物流管理的对象包括运输、车队、仓储、物料处理、订单履行、物流网络设计、库存管理，以及对第三方物流服务商的管理等。当然，有时候物流管理也会涉及采购、生产、包装和客户服务等。它不但要优化物流的各环节，而且要考虑与其他职能的集成。 在国内，很多人片面地把物流等同于运输，就是把产品从A点搬到B点，看上去很简单，其实不然。光从它占美国、日本GDP的7%左右，占中国GDP的15%左右来说就不简单。单拿它的分支行业来说，运输业、仓储业等本身就大得不得了，整个物流行业，你能想象有多大吗？ 有趣的是，查一下几十年前的定义，物流管理还包括采购，采购被视作入厂物流[1]的一部分。这也与当时竖向集成为主、采购的地位低下不无关系。在有些公司，比如欧洲的一些公司，负责订单处理的采购员汇报给物流，而不是采购部门。上汽大众也是类似的设置，估计是受德国大众的影响。中国有物流与采购联合会、物流与采购网，都是物流在先，采购在后，一定程度上也反映了采购与物流的关系。 在美国，采购管理领域的研究者，有很多原先也是毕业于物流管理系。例如，美国经典的采购教科书的作者David Burt教授，原来就是毕业于斯坦福大学的物流管理专业。我在亚利桑那州立大学的教授Lisa Ellram呢，虽然研究方向主要是采购，但博士学位却来自俄亥俄州立大学的物流管理。 在国内，物流（logistics）早些年被译作后勤学，又称军事物流学。这跟物流与军事联系由来已久不无关系。诸葛亮六出岐山，据说是一人打仗，需要五人做后勤支持，后勤是最大的挑战，而输也是输在后勤上。左宗棠在西北平叛，“惟秦陇之事，筹饷难于筹兵，筹粮难于筹饷，而筹转运尤难于筹粮，窘迫情形，为各省所未见。”[2]——在陕甘一带，筹钱比招兵难，筹粮比筹钱难，而粮草的运转比筹粮更难，说的也是物流后勤之难。 第二次世界大战后期的诺曼底登陆，表面上是一场战役的成功，不为人所知的是后勤物流的杰作。以前说美军能够在24小时内开赴全球的任何地方；现在呢，美军第82空降师可以在18个小时内到达世界的任何地方，拼的还是物流的实力。2017年，中印的6·18洞朗对峙事件，之所以能够和平解决，与中国的物流运输能力分不开——得益于这些年在西藏的铁路、公路建设，我们能够迅速地在西藏投放大批重型武装，对印度形成有效吓阻。 资源 微信公众号“物流沙龙” www.logclub.com 是物流管理领域的一个交流平台。这个沙龙已有十余年的历史，一直坚持在物流管理领域。 [1] 入厂物流是inbound logistics的翻译，简单地说，就是把原材料、半成品等运入厂区，比如从供应商到工厂。 [2] 胜利在望却甘愿求和，左宗棠西北平乱为何要选马家军做朝廷代理人.百家号“史料不辑”. 运营管理：千遍万遍不走样 运营管理是供应链管理的三大组成之一，当然也可以说供应链管理是运营管理的延伸。那究竟什么是运营管理呢？ 微软的英卡特百科全书对运营管理定义如下：“运营管理是对主要商业活动的管理，即组织和控制最基本的商业活动，为客户提供产品或服务。”这与美国运营管理经典教科书[1]的定义挺接近：“运营管理是对公司相关体系的设计、运作和改进，以制造产品和提供服务。”它是把原材料、人力、技术、资金、设备等转化为产品、服务的增值过程，是每一个管理人员都没法回避的。 运营管理协会，即原来的美国生产与库存控制学会，对运营管理的定义有明显的生产和库存管理的痕迹，但贴切地反映了运营管理的兼容并蓄：“运营管理是对研发、工业工程、管理信息系统、质量管理、生产管理、库存管理、会计等职能的集成，以有效地规划、利用和控制生产或服务机构。” 运营管理不是制造业专有，从“制造与服务业运营管理学会”的名字就可见一斑。在美国，国内生产总值GDP的79.7%来自服务业（2017年）[2]，运营管理的研究重心也在从制造业向服务业转移。很多起源于制造业的概念，也被移植到服务业。麦当劳把流水生产线用到快餐服务，就是一个例子——流水线最早由福特汽车导入，是个制造行业的实践。 一位在戴尔担任过运营经理的朋友说，运营管理都是些琐碎繁杂的事。没错，不过运营管理的这些柴米油盐事，却关系到公司的基本运作，如质量、交货、服务等，任何一件小事都可能让你的生产线停顿下来，所以非常重要。一位纳斯达克100的大公司的首席运营官说，他的全球运营部门是“啥事都牵扯”（in the middle of everything），也是同样的道理。 琐碎繁杂，微不足道，干一遍没什么难，难就难在千遍万遍不走样。这就如麦当劳的炸薯条本身没什么了不起，真正了不起的是，不管在世界什么地方，由什么肤色的人炸，是早晨还是晚上，这薯条都炸得一个样。就如海尔集团首席执行官张瑞敏所说，“不简单，就是将简单的事做千遍万遍做好；不容易，就是将容易的事做千遍万遍做对。”背后没有成套的系统、流程是不可能的。运营管理的价值就体现在对这些系统、流程的设计、运营和改进上。而且只有从日常运营的繁杂琐事中上升到流程、系统的实质问题并改进，运营管理者才能脱颖而出。这点同样适用于供应链管理。 在北美的大公司，运营管理和供应链管理相互搭接。例如，在IBM这类推行集成供应链管理的公司，运营管理是供应链管理的一部分；而在另一些公司，采购、物流等是全球运营部门的一部分，汇报给全球运营部。究竟是运营汇报给供应链，还是供应链汇报给运营，这并不重要：水无定型、法无定法，关键是组织结构要能够满足公司的业务要求，并随着业务的发展而调整。 业界人士说 在一些国企，运营管理部是虚岗，跳出实际的业务流程去进行所谓的运营优化管理。在我看来，这种做法非常低效：一方面，该岗位平时可有可无，即使该部门全部放假也完全不影响业务进程，对从业人没有任何直接的业务压力，没有压力就没有动力，很难出成绩和效果；另一方面，会产生外行指挥内行的现象，反而干扰正常业务流程。如果用跨部门的专项项目，或者类似精益生产的改善小组，效果应该更好。常设专职的工作组、委员会一般都不是解决问题的好组织形式。——米良疯，微信公众号“供应链管理专栏”读者 刘宝红答 其实何止国企，大企业都有这问题。专业分工下，内行埋头干活儿，没时间抬头看路；外行在教人干活儿，但不知道活儿是怎么干的。干活儿的跟教人干活儿的是两层皮，注定效果会打折扣。最早的日本企业的“质量圈”（quality circle），就是干活儿的人自己在改进，两层皮的问题就比较小。 [1] 该书英文名为Operations Management for Competitive Advantage。作者为Richard Chase，F. Robert Jacobs和Nicholas Aquilano，2005年由McGraw-Hill/Irwin出版社出版。该书的更新版本有中文版，名为《运营管理》，由任建标翻译，机械工业出版社2015年出版。 [2] List of Countries by GDP Sector Composition.维基百科，www.wikipedia.org. 供应链管理的几个“小亲戚” 除了采购、运营和物流管理外，供应链管理还有好几个“小亲戚”，比如运筹学、系统动力学、工业工程、信息技术等。 运筹学为供应链的优化提供了工具，比如线性规划、数理统计等。如果你看20世纪60年代以来的文献，库存计划、生产排程、配送网络优化等领域到处都是运筹学的影子。而供应链的真正优化，也离不开这些数理统计模型。国内高校中，有些供应链教授就是运筹学背景。比如上海交大安泰学院的陈晓荣博士，就是个运筹学专家，现负责全球运营领袖MBA课程（交大和MIT合办，在我看来是国内最好的供应链管理MBA）。 运筹学有很多模型和算法，相对北美而言，也是国内教授比较擅长之处。在美国，供应链管理的顶级研究，比如发表在《管理科学》（Management Science）等上面的论文，大多也离不开数理模型，那些杰出的研究者呢，也是以华人和印度裔为主。 供应链管理的另一个“亲戚”，甚至可以说是“近亲”，是系统动力学。该学科源自麻省理工的Jay Forrester教授，着眼供应链条上各个环节之间的互动，力图全局优化，可以说是供应链管理的鼻祖。作为供应链管理的经典游戏，“啤酒游戏”就是由Forrester教授在20世纪60年代开发的，后来演化成多种版本，用来展示供应链上没法回避的“牛鞭效应”[1]，也能在系统动力学上找到起源。 供应链管理最早在制造业发展起来，而制造业离不开IE和IT—供应链管理的另两个“亲戚”。前者是工业工程，可以说是现代管理之母，生产线、仓储配送设施等的优化，都离不开工业工程；后者是信息技术，比如以ERP为核心的信息系统，撑起了企业和供应链的框架。供应链的流程，特别是订单层面的基本流程，其实是固化在信息系统里。 我在硅谷工作的那些年，经常跟负责ERP的分析员们开玩笑，说公司把我们这些负责供应链业务的人都开掉也没关系，只要保留他们那些维护ERP的人员就行了——企业的基本流程，比如订单处理，是固化在ERP中的，而这些分析员最熟悉ERP和业务流程，招些新人，由他们培训就可以了。 也是因为这个原因，有些公司的供应链改进由CIO牵头。比如时不时有公司联系我，希望我来帮助他们改进供应链管理，联系人的头衔中屡屡就有CIO的字眼。有个计算机巨头邀请我去培训他们的IT人员（主要是分析员），主要原因就是他们最熟悉业务流程，需要承担供应链绩效改进的责任。电商、贸易行业，我就见过好几个CIO在负责制定需求预测、库存计划逻辑。 这有很多问题。最主要的是CIO虽说熟悉基本的业务流程，但并不一定熟悉业务本身。打个比方：CIO很熟悉在ERP里，库存如何从一个库存点转移到另一个库存点，但这跟库存控制没有关系——库存控制取决于合理的需求预测、合理的库存计划，光熟悉那些ERP里的指令是远远不够的。所以，CIO在供应链的组织设计、绩效考核、主干流程方面往往经验不足，因而不是主导供应链绩效改进的合适人选。[2] [1] 简单地说，“牛鞭效应”就是由于信息不对称，需求变动沿着供应链传递时会逐级放大，越是远离需求源，放大的幅度越大。后文会详细探讨。 [2] 这方面有个案例，在我和赵玲合著的《供应链的三道防线：需求预测、库存计划、供应链执行》一书中（145~151页）。该案例讲的是在一个本土企业，供应链改进原来由IT驱动，最后转向由集成供应链来负责，因为后者更加熟悉业务机制。","link":"/%E7%9B%B2%E4%BA%BA%E6%91%B8%E8%B1%A1/"},{"title":"6. Python的高阶函数","text":"Hi，大家好。 我是茶桁。 本节课，我们来学习一下Python中的「高阶函数」。 递归函数 让我们先来了解一下，什么是递归函数。 递归函数就是定义一个函数，然后在此函数内，自己调用自己。 既然是自己调用自己，那这个函数必须要有一个结束才行，否则会一直重复的调用下去，直到调用层数越来越多，最终会导致栈溢出。 让我们先写一个雏形： 1234567891011# 初步认识一下递归函数def recursion(num): print(num) recursion(num - 1)recursion(3)# 执行结果3 2 1 0 -1 -2 -3 -4 -5 -6 -7 ....RecursionError: maximum recursion depth exceeded while calling a Python object 最后，导致栈溢出，程序报错。 那么这个程序到底做了什么？ 首先，我们定义了一个函数，然后执行，执行的时候给了一个参数3。 进入程序之后，先将3打印了一遍，然后在函数内部，又调用了一遍自己，参数为3-1，也就是传了一个参数2，在进入函数之后，打了了2， 继续自己调用自己，传参2-1，1-1, 0-1, ...就这样一直循环下去。 那么我们怎么样让这个程序停下来？就是在函数自己调用自己之前，加上一个限制条件： 1234567891011121314# 初步认识一下递归函数 3 2 1 0def recursion(num): print(num) # 检测当前值是否到0 if num &gt; 0: # 调用函数本身 recursion(num - 1)recursion(3)# 执行结果3210 我们给调用之前加了一个条件，如果num &gt; 0才允许继续执行，这样，当程序传递了1-1之后，执行了最后一次打印，然后就不向下执行了。 不过不要以为程序到这里就结束了，我们多加一行代码试试看： 123456789101112# 初步认识一下递归函数 3 2 1 0def recursion(num): print(num, end=&quot; &quot;) # 检测当前值是否到0 if num &gt; 0: # 调用函数本身 recursion(num - 1) print(num, end=&quot; &quot;) # 又加了一个print函数recursion(3)# 执行结果3 2 1 0 0 1 2 3 如果你不知道程序做了什么，我们稍微分析一下： 1234567891011解析当前递归函数的执行过程：```recursion(3) ==&gt; 3 recursion(3-1) ==&gt; 2 recursion(2-1) ==&gt; 1 recursion(1-1) ==&gt; 0 recursion(0) ==&gt; 0 recursion(1) ==&gt; 1 recursion(2) ==&gt;2recursion(3) ==&gt; 3``` 也就是，在递归函数中，程序是一层一层的进入，然后再一层一层的返回。 这就好像是， 我们在上学的时候，你坐在最后一排，但是你有个心仪的女孩坐在最前面。你想要对方电话，这个时候你传递一个纸条给前面的同学，前面的同学再往前传，一直往前传到女孩手里。女孩看完之后，写完回复再一次次的传回来。最后你满怀期待的打开一看：“滚。” 当然，我们的递归函数和这个不同的地方是最后不会多加那个“滚”字。 回调函数 什么是回调函数呢？ 我们首先来思考一个问题： 1234def func(a): print(a)func(a) 在这个简单的函数中，我们已经学会了传值a给到func()，那么参数到底可以传一些什么进去？a可以是什么？能不能是一个函数呢？ 这就引出了我们现在的内容： 1234567891011121314# 带有回调函数参数的函数def func(obj): print(obj, type(obj)) # 并且在函数中调用传递进来的形参函数 obj()def _self(): print(&quot;i am _self&quot;)func(_self)# 执行结果&lt;function _self at 0x111ed4280&gt; &lt;class 'function'&gt;i am _self 可以看到，我们选择执行的是func函数，但是最后打印出了_self函数中语句。原因就是我们在执行func函数的时候，将_self函数作为参数传递给了func的形参obj， 我们在其中打印了obj以及obj的类型，并且最后执行了一下obj， 实际上也就是执行了一遍_self函数。 如果在一个函数中要求传递的参数是一个函数作为参数，并且在函数中使用了传递进来的函数，那么这个函数我们就可以称为是一个回调函数。 我们拿系统内部的一个现成的函数来重新封装一个新的函数来试试： 123456789101112131415# 做一个数学计算的函数def func(x, y, obj): &quot;&quot;&quot; 此函数用来整合其他的数学运算 在当前函数中，需要接收三个参数，前两个为数值，最后一个为函数 x, y: int f: function :return: &quot;&quot;&quot; print(obj(x, y))func(2, 3, pow)# 执行结果8 在日后使用这个函数的时候，就可以传入数值和要做什么计算的方法，就可以了。 当然，这个函数写的并不完善，比如，我们在执行func(2, 3, sum)的时候就会报错，原因是因为sum()函数内部是要进行迭代的的，然而int类型中没有魔法方法__iter__, 所以无法迭代。所以，要想这个函数具有通用性，还需要在内部完成很多工作。 闭包函数 之前我们在回调函数中将函数作为参数进行了传递，那么问题来了，既然函数能作为参数进行传递，那能不能作为参数被return呢？ 12345678910def person(): money = 0 def work(): print(money) return workperson()# 执行结果&lt;function __main__.person.&lt;locals&gt;.work()&gt; 我们可以看到，work函数被成功返回出来了。但是并未继续执行， 因为其内部的print()没起作用。 我们用一个变量来接收这个返回的函数： 1234567891011def person(): money = 0 def work(): print(money) return workres = person()res()# 执行结果0 说明res接收到返回的work()函数，并且最后执行成功了。 好了，让我们继续为这个函数做一点什么，看看有什么变化。 12345678910111213def person(): money = 0 def work(): nonlocal money money += 100 print(money) return workres = person()res()# 执行结果100 这个结合前几节所讲的内容就很好理解了对吧？ nonlocal关键字拿到上一层函数定义的变量，然后在内层函数中进行使用，最后打印出来。 那我们继续执行会如何？让我们多执行几次： 12345678910111213141516171819# 定义一个函数def person(): money = 0 # 函数中定义了一个局部变量 # 定义内函数 def work(): nonlocal money # 在内函数中使用了外函数的临时变量 money += 100 print(money) # 在外函数中返回了内函数，这个内函数就是闭包函数 return workres = person() # return work res = workres() # res() == work()res()res()res()res()# 此时 就不能够在全局中对money这个局部变量进行任何操作了，# 闭包的作用：保护了函数中的变量不受外部的影响，但是又能够不影响使用 你会不会认为会一直打印100? 让我们看看执行结果到底是怎样的： 12345100200300400500 怎么样，是不是完全没想到？这个就是闭包函数的特点。 在一个函数内返回了一个内函数，并且这个返回的内函数还使用了外函数中局部变量，这个就是闭包函数。其特点为： 在外函数中定义了局部变量，并且在内部函数中使用了这个局部变量 在外函数中返回了内函数，返回的内函数就是闭包函数 ⚠️ 主要在于保护了外函数中的局部变量，既可以被使用，又不会被破坏。 检测一个函数是否为闭包函数，可以使用func.__closure__，如果是闭包函数返回cell 匿名函数 -- lambda表达式 首先，我们先弄清楚什么是匿名函数： 匿名函数的意思就是说可以不使用def来定义，并且这个函数也没有名字。 在Python中，我们可以使用lambda表达式来定义匿名函数。我们需要注意，lambda仅仅是一个表达式，并不是一个代码块，所以lambda又称为一行代码的函数。 在lambda表达式中，也有形参，并且不能够访问除了自己的形参之外的任何数据，包括全局变量。其语法如下： 12# 语法：lambda [参数列表]:返回值 让我们来尝试写写看，我们先来定义一个普通的加法运算的函数： 123456# 封装一个函数做加法运算# 普通函数def sum(x, y): return x+yprint(sum(2, 3)) 毫无疑问，执行结果为5。那么接下来，用lambda该怎么写呢？ 123456# 改成lambda表达式来封装res = lambda x, y:x+yprint(res(4, 4))# 执行结果8 结合闭包函数的讲解，这里就应该很容易看懂了吧？一样的地方就是，使用了一个变量res来接收这个返回的函数，然后执行res函数。 让我们再来一段： 12345res = lambda sex:&quot;很man&quot; if sex=='male' else '很nice'print(res('female'))# 执行结果很nice 只看结果的话，我们很清楚这段函数最后执行到了else语句内。但是是如何进入的呢？让我们将这段代码用普通函数的写法展开来看看： 1234567def func(sex): if sex == 'male': return '很man' else: return '很nice'func('female') 这样是不是很清晰了？回过头来让我们看刚才那段lambda表达式，我们可以这样去看： 12345# lambda 参数列表: 真区间 if 表达式判断 else 假区间lambda sex: '很man' if sex=='male' else '很nice'# 然后用一个变量接收函数res = lambda sex: '很man' if sex=='male' else '很nice' 所以可以看出来，其实lambda十分的方便，并且并不难理解，当你习惯了lambda之后，会非常便捷。 迭代器 迭代器是一个很有意思的功能，可以说是Python中最具特色的功能之一，它是访问集合元素的一种方式。 迭代器是一个可以记住访问遍历的位置的对象。从集合的第一个元素开始访问，直到集合中的所有元素被访问完毕。 迭代器只能是从前往后一个一个的遍历，不能后退。 我们把之前一直使用的range()拿过来看： 123456# range(10, 3, -1) 返回一个可迭代的对象for i in range(10, 3, -1): print(i, end=&quot; &quot;) # 执行结果10 9 8 7 6 5 4 表面上来看，似乎range()本身就是一个迭代器，可是我们来尝试做个实验： 12345x = range(5)print(next(x))# 执行结果TypeError: 'range' object is not an iterator 当我们尝试调用next()函数的时候报错了，被告知range不是一个迭代器。 那么，range不是迭代器，究竟是什么呢？这里我们就要先深入研究下迭代器的特性： 严格来说，迭代器是指实现了迭代协议的对象，迭代协议是指实现了iter方法并返回一个实现了next方法的迭代器对象，并通过StopIterator一场标识迭代完成。 iter() iter()能把迭代的对象，转为一个迭代器对象，其参数为可迭代的对象(str, list, tuple, dict)， 返回值为迭代器对象。其中需要注意的一点是：迭代器一定是一个可以迭代的对象，但是可迭代的对象并不一定是迭代器。 我们在迭代器上使用iter会得到相同的对象： 123456789i = iter([1, 2, 3])print(iter(i) is i)print(id(iter(i)))print(id(i))# 执行结果True44255020964425502096 基于此，我们可以这样实现： 12345i = iter([1, 2, 3, 4])list(zip(i, i))# 执行结果[(1, 2), (3, 4)] next() next()函数可以去调用迭代器，并返回迭代器中的下一个数据。 我们使用iter函数可以从任何可迭代对象中获取一个迭代器： 123456789a = iter([1, 2, 3])print(a)print(next(a))print(1 in a)# 执行结果&lt;list_iterator object at 0x124111a50&gt;1False 可以看到，我们使用iter函数可以从任何可迭代对象中获取一个迭代器。而且迭代器有个特点，即每次用完一个元素即消耗掉该元素，不会保留在迭代器中，也就是说，是一次性的。 1234567891011res = iter([1, 2, 3, 4])print(next(res), end=&quot; &quot;)print(next(res), end=&quot; &quot;)print(next(res), end=&quot; &quot;)print(next(res), end=&quot; &quot;)# print(next(res))r = list(res)print(r)# 执行结果1 2 3 4 [] 可以看到，list里面已经空了。 我们用for来取值： 123456789res = iter([1, 2, 3, 4])for i in res: print(i, end=&quot; &quot;)r = list(res)print(r)# 执行结果1 2 3 4 [] for直接将迭代内的元素全部取完了，所以最后打印下一个值的时候也显示空了。所以我们可以得到迭代器的取值方案： 迭代器的取值方案 next()：调用一次获取一次，直到数据被取完。 list()：使用list函数直接取出迭代器中的所有数据。 for：使用for循环遍历迭代器的数据 总结一下： 迭代器是一个可以记住遍历的位置的对象。 迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。 迭代器有两个基本的方法：iter() 和 next()。 字符串，列表或元组对象都可用于创建迭代器： 那么，再回过头来看看range range可以像任何其他可迭代对象一样循环使用，但是它并不具备迭代器中的一些特性，比如，我们之前实验过，range并不能使用next方法，而我们可以从range中得到一个迭代器： 1234iter(range(3))# 执行结果&lt;range_iterator at 0x124184570&gt; 我们在迭代器中使用元素就会消耗掉该元素，但是我们遍历一个range对象并不消耗它， 比如： 很明显我们可以重复使用。 来一个更直接的，我们之前用for获取了迭代里的值，我们对range()也来使用一下看看会不会有不同的结果： 123456789res = range(1,5)for i in res: print(i, end=&quot; &quot;)r = list(res)print(r)# 执行结果1 2 3 4 [1, 2, 3, 4] 一样的代码，对象不同。我们可以明显看到区别，range拿到最后里面的元素并没有减少。这也说明了，range并不是迭代器。 实际上，range的迭代是通过iter协议来实现的，只是一种类似迭代器的鸭子类型，并非真正的迭代器。 其实，有一种可以直接检测迭代器和可迭代对象的方法： 12345678910111213141516171819202122232425# 检测迭代器和可迭代对象from collections.abc import Iterator, Iterable varstr = '123456'res = iter(varstr)r = range(1, 5)# isinstance() 检测一个数据是不是一个指定的类型# Iterable: 迭代对象，Iterator: 迭代器r1 = isinstance(varstr, Iterable) r2 = isinstance(varstr, Iterator)r3 = isinstance(res, Iterable)r4 = isinstance(res, Iterator) r5 = isinstance(r, Iterable) r6 = isinstance(r, Iterator)print(f'varstr 是迭代对象：{r1}, \\t varstr 是迭代器: {r2}')print(f'res 是迭代对象：{r3}, \\t res 是迭代器: {r4}')print(f'r 是迭代对象：{r5}, \\t r 是迭代器: {r6}')# 执行结果varstr 是迭代对象：True, varstr 是迭代器: Falseres 是迭代对象：True, res 是迭代器: Truer 是迭代对象：True, r 是迭代器: False 今天的知识点讲到这就结束了，接下来，让我们来做两个小练习。 练习题 递归查询斐波那契数列位数 还记得我们之前讲过的斐波那契数列吗？不记得没关系，我们来复习一下： 1# 斐波那契数列: 0, 1, 1, 2, 3, 5, 8, 13... 我们这次来实现一个函数，用于查询斐波那契数列中当前位置的数值是多少。 1234567891011121314# 递归实现斐波那契数列def fibonacci(n): if n == 1: return 0 elif n == 2 or n == 3: return 1 else: return fibonacci(n-1) + fibonacci(n-2) res = fibonacci(6)print(res)# 执行结果5 我为大家画了张图，来看看程序内部到底做了些什么： 从这张图中，我们可以看到递归的步骤和返回的结果。 递归实现阶乘 什么是阶乘？比如我们实现7的阶乘，那么就是 \\[ 1\\times2\\times3\\times4\\times5\\times6\\times7 \\] 让我们来试着实现一下: 123456789101112# 实现阶乘def factorial(n): if n == 1: return 1 else: return n*factorial(n-1) res = factorial(7)print(res)# 执行结果5040 验证一下看看： 1234print(1*2*3*4*5*6*7)# 执行结果5040 看来结果没问题，那让我们来看看程序内发生了什么： 12345678910111213141516'''factorial(7) =&gt; 7 * factorial(6) =&gt; 6 * factorial(5) =&gt; 5 * factorial(4) =&gt; 4 * factorial(3) =&gt; 3 * factorial(2) =&gt; 2 * factorial(1) =&gt; factorial(1) = 1 2 * 1 = 2 3 * 2 = 6 4 * 6 = 24 5 * 24 = 120 6 * 120 = 7207 * 720 = 5040''' 虽然实现了，最后还是不得不说几点注意事项： 递归函数的效率并不高，所以尽量能不用就不要用。 一个函数如果调用后没有结束，那么栈空间中就一直存在，直到这个函数运算结束才会销毁。 好了，今天的课程到此结束。大家课后记得多练习。下课！","link":"/Higher-order-functions/"},{"title":"7. Python的内置函数","text":"Hi，大家好。我是茶桁。 讲完了基础函数和高阶函数之后，我们这一节来研究下Python的内置函数，看看Python在安装完毕之后的解释器里，到底都预先给我们提供好了哪些可用的函数。 本节内容着重介绍一些常用函数，并且会做一些应用上的示例。当然，对于Python的内置函数，我们还可以查询官方文档，我这节参照的为3.10版本文档 range()函数 这几节课中，我们频繁使用并且着重介绍过这个函数，那我们就从它开始介绍吧。 一般我们需要遍历一个数值序列的时候，range()函数就会派上用场，它生成算数级数。 12345678910'''range() 函数功能： 能够生成一个置顶的数值序列参数： start: 开始的值，默认为0 stop: 结束的值 [, step]: 可选，步进值， 默认为1返回值： 可迭代的对象，数字序列'''range(start, stop, [, step]) 让我们来看一下： 12345res = range(10)print(res, type(res))-----------------------------range(0, 10) &lt;class 'range'&gt; 可以看到这其实就是一个range的类，其实在我们Python中，任何数据都是一个对象而已。 来看案例： 123456# range函数的使用方式res = range(11)print(list(res))-----------------------------[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 当我们的range内只写一个参数时，这个参数就是stop值，也就是从start的默认值0开始到输入的参数值（stop）之前为止，比如这段代码中，stop会结束到11之前，也就是10。 我们在这段代码中，将range的内容转化成一个list并打印了出来。当然，我们也可以使用循环，依次去除range内的内容： 12345for i in res: print(i, end=&quot; &quot;) -----------------------------0 1 2 3 4 5 6 7 8 9 10 记得上节课我们提到过，range()是不支持next()函数的，不过如果我们将其转成迭代器，就可以使用next()函数调用： 123456789res = iter(range(11))print(next(res))print(next(res))print(list(res))-----------------------------01[2, 3, 4, 5, 6, 7, 8, 9, 10] 可以看到，使用iter转成迭代器之后，可以正常使用next()函数，并且我们再次查看res的内容，0,1已经被拿走，只将剩余内容转化为list打印了出来。 当我们在range中添加两个参数的时候，start就是第一个参数，第二个参数就是stop值。 123456# 添加两个参数for i in range(5, 10): print(i, end=&quot; &quot;)-----------------------------5 6 7 8 9 当我们输入三个参数的时候，第一个参数为start, 第二个参数为stop, 第三个参数就是[, step]， 比如： 123456# 添加三个参数for i in range(1, 10, 3): print(i, end=&quot; &quot;) -----------------------------1 4 7 这段代码的含义就是从1开始, 以3为步进来提取数字，并打印出来，一直到10之前的数字为止。 如果不太理解步进值的可以执行数一遍就理解了，比如我们从1开始顺序往后数3个数，那就是2、3、4，数到了4, 再继续往后数3个数，就是5、6、7，数到了7。再继续往后就是8、9、10。但是，我们代码中的stop值为10，所以到9就结束了，也就是说，我们这段代码就只取出了1, 4, 7三个值。 三种参数值的情况我们都了解之后我们可以思考下，难道我们只能选择顺序取值吗？其实不然，我们还可以倒叙取值，聪明的小伙伴可能想到了，调换一下start和stop值不就可以了嘛？我们从10开始取值，取到0为止： 12for i in range(10, 0): print(i, end=&quot; &quot;) 执行一下，哎，似乎什么都没打印出来。这又是为什么呢？是不是出BUG了？ 其实，什么都没打印出来才是正确的，这是因为，虽然我们给了开始和结束值，但是我们遗忘了一个重要的参数，那就是步进值step，这个值默认可是1，从10开始+1来计数，无论如何也算不到0。所以，我们将步进值改成负数，也就是倒着数了: 12345for i in range(10, 0, -1): print(i, end=&quot; &quot;) -----------------------------10 9 8 7 6 5 4 3 2 1 至此，我们可以得到结论，是否倒叙取值除了开始和结束值，更重要的是看step是正数还是负数。 123456res = range(-10,-20,-1) # [-10, -11, -12, -13, -14, -15, -16, -17, -18, -19]res = range(-20,-10) # [-20, -19, -18, -17, -16, -15, -14, -13, -12, -11]res = range(-10,10)# [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9] zip()函数 zip() 函数可以接受多个可迭代的对象，然后把每个可迭代对象中的第i隔元素组合在一起，形成一个新的迭代器。 12345'''参数： *iterables, 任意个的可迭代对象返回值： 返回一个元组的迭代器'''zip(*iterables) 让我们来直接看示例： 123456789n1 = '1234'n2 = ['a', 'b', 'c']n3 = ['A', 'B', 'C', 'D']# 调用zip函数，合成新的元组迭代器res = zip(n1, n2, n3)print(list(res))-----------------------------[('1', 'a', 'A'), ('2', 'b', 'B'), ('3', 'c', 'C')] 我知道你们看到这个执行结果会有很多疑问，先别着急，我们先看一下它是否包含迭代器的特性： 12for i in res: print(i) 当你执行这段代码的时候就会发现，似乎什么都没发生。 那到底是怎么回事？我们不用for，让我们再转换一次list之后看看里边有什么： 1234print(list(res))-----------------------------[] 列表居然是空的。是不是瞬间想到了什么？ 没错，这个似乎就是迭代器的特性之一，当其中元素被使用之后，会删掉使用过的元素。而我们之前在执行print(list(res))的时候，已经将内部元素都转成list并展现过，所以现在res内的元素都被删掉了。 没事，让我们再重新来定义一次，也就是重新给res内填满元素然后直接for循环一次看看： 12345678910111213n1 = '1234'n2 = ['a', 'b', 'c']n3 = ['A', 'B', 'C', 'D']# 调用zip函数，合成新的元组迭代器res = zip(n1, n2, n3)for i in res: print(i) -----------------------------：('1', 'a', 'A')('2', 'b', 'B')('3', 'c', 'C') 我们可以看到，每次打印i的时候都打印了一个元组，而这个元组就是一个新元素，比如第一行('1', 'a', 'A'), 这整个元组就是一个新元素。 让我们再用next试试（当然我又重新填满了res）： 123456print(next(res))print(next(res))-----------------------------('1', 'a', 'A')('2', 'b', 'B') next函数也能正常执行，那可以说明，zip确实组合成了一个新的迭代器。 现在我们返回来再看一遍代码中的n1,n2,n3, 分别是1234, [‘a’, 'b', 'c'], ['A', 'B', 'C', 'D']。最后组成的迭代器对象为：[('1', 'a', 'A'), ('2', 'b', 'B'), ('3', 'c', 'C')]。 通过分析可以看出来，zip的工作原理是先分别取可迭代对象的第一个元素组合成一个元组，然后再分别取第二个元素组合成一个元组，依次往后取... 可是n1, n3分别都是四个元素，为什么我们最后只组合成了三个元组？那是因为n2中只包含了三个元素，当在其中找不到第四个元素的时候，就会放弃组合。 来，让我们在看一个示例： 1234567n1 = [1, 2, 3, 4]n2 = [22, 33, 44, 55]res = zip(n1, n2)print(list(res))-----------------------------[(1, 22), (2, 33), (3, 44), (4, 55)] 大家看到最后的执行结果有没有觉得很眼熟？可能很多小伙伴一时间想不到，我们来调整一下： 123456[ (1, 22), (2, 33), (3, 44), (4, 55)] 记住这个数据结构，我们在后期做数据分析的时候， 当我们做矩阵运算的时候用的非常多。 不知道大家是否都学过高等数学里的线性代数、微积分，包括概率统计。这些在我们之后做数据分析，数据挖掘，包括机器学习、人工智能这些科学运算里面，非常重要的一些数学功底。 不太记得了也没关系，这些我后面将会专门拿几节出来给大家补一下这方面。 让我们继续，zip还有一种应用方式，当其与*运算符结合使用的时候，可以用来拆解列表： 12345678910# zip 与 * 运算符相结合使用x = [1, 2, 3]y = [4, 5, 6]print(zip(x, y))print(*zip(x, y))-----------------------------&lt;zip object at 0x107b8d200&gt;(1, 4) (2, 5) (3, 6) 可以看到，zip是一个迭代器，*zip这生成了组合好的多个元组数据。 比如： 12345678x1 = [1, 2, 3]y1 = [4, 5, 6]x2, y2 = zip(*zip(x, y))print(x2, y2)-----------------------------(1, 2, 3) (4, 5, 6) 这样，我们就将两个列表转换成了两个元组。当然，其实我们这样操作还不如直接使用tuple函数来的方便快捷一点。 那下面，我们就看看都有哪些数据类型转换相关的内置函数。 数据类型转换相关的内置函数 这些函数的功能非常简单和单一，属于拿来就用的函数，我们就仅列出来，不多做介绍了。 int() 将其它类型数据转为整型 float()转为浮点类型 bool()转为布尔类型 complex()转为复数 str()转为字符串类型 list 转为列表类型 tuple转为元组类型 dict 转为字典类型 set 转为集合类型 变量相关函数 id() 获取当前数据的ID标识 type() 获取当前数据的类型字符串 print()数据的打印 input()获取输入的数据 isinstance()检测是否为指定的数据类型 数学相关函数 abs()获取一个数的绝对值 1234print(abs(-99.99))-----------------------------99.99 sum()求和 从 start 开始自左向右对 iterable 中的项求和并返回总计值 1234print(sum([1,2,3]))-----------------------------6 max() 获取最大值 123456print(max([1,2,3]))print(max(99,12,45))-----------------------------399 min() 获取最小值 123456print(min([2,1,6,-9]))print(min(6,7,1,0,-2))------------------------------9-2 pow(x, y)幂运算 返回 x 的 y 次幂 1234print(pow(2,3)) -----------------------------8 round(x, n) 对x四舍五入，小数点保留n位 123456print(round(3.1415926))print(round(3.1415926,2))-----------------------------33.14 round这个函数不是绝对意义上的四舍五入，在取整这个问题是是奇进偶退： 123456print(round(3.5))print(round(4.5))-----------------------------：44 进制函数及字符集 bin() 将数值类型转为二进制 1234print(bin(123)) -----------------------------0b1111011 int() 将二进制转化为整型 1234print(int(0b1111011))-----------------------------123 oct() 转为八进制数 1234print(oct(123))-----------------------------0o173 hex() 转为十六进制数 1234print(hex(123))-----------------------------0x7b ASCII及字符集 ASCII，全称为美国信息互换标准代码。是一套基于拉丁字母的字符编码，共收录了 128 个字符，用一个字节就可以存储，它等同于国际标准 ISO/IEC 646。它一共有128个支付，最后更新是1986年。 我们要知道的是，ASCII 编码是美国人给自己设计的，他们并没有考虑欧洲那些扩展的拉丁字母，也没有考虑韩语和日语，我大中华几万个汉字更是不可能被重视。计算机也是美国人发明的，起初使用的就是 ASCII 码，只能显示英文字符。各个国家为了让本国公民也能正常使用计算机，开始效仿 ASCII 开发自己的字符编码，例如 ISO/IEC 8859（欧洲字符集）、shift_Jis（日语字符集）、GBK（中文字符集）等。 从65开始到90为止，是大写字母（A ~ Z), 97到122是小写字母(a ~ z)，48到57是0 ~ 9。 而我们经常使用的是GB2312-80, GBK和GBK18030以及Unicode字符集。 GB2312-80 是 1980 年制定的中国汉字编码国家标准。共收录 7445 个字符，其中汉字 6763 个。 GBK 于1995年制定 收录了 21003 个汉字。GBK向下与 GB 2312 编码兼容， GBK18030 2001年的1月正式强制执行，是我国制订的以汉字为主并包含多种我国少数民族文字（如藏、蒙古、傣、彝、朝鲜、维吾尔文等）的超大型中文编码字符集强制性标准，其中收入汉字70000余。 Unicode（统一码、万国码、单一码）是计算机科学领域里的一项业界标准，包括字符集、编码方案等。 它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。1990年开始研发，1994年正式公布。 UTF-8 以字节为单位对Unicode进行编码。 我们现在写代码的时候基本遵循UTF-8编码为主。 有的时候，我们是需要将字符转为ASCII， 也有对应的方法： 1234print(ord('a'))-----------------------------a 将ASCII转为字符也一样： 1234print(chr(65))-----------------------------A 高阶函数 和上一节课不同，我们现在要讲的高阶函数，是Python解释器里的内置高阶函数。 sorted() 很多时候，我们在处理数据的时候都需要对数据进行排序。不管是以序号，名称还是日期的方式。sorted()就是我们最常用的排序函数： 12345678910sorted(iterable, [reverse, key])‘’‘运行原理：把可迭代数据里面的元素，一个一个的取出来，放到key这个函数中进行处理，并按照函数中return的结果进行排序，返回一个新的列表功能：排序参数： iterable:可迭代的数据 （容器类型数据，range数据序列，迭代器） reverse:可选，是否反转，默认为False，不反转， True反转 key:可选， 函数，可以是自定义函数，也可以是内置函数返回值：排序后的结果’‘’ 我们来看几个示例，首先我们先来看看默认的排序方式：从小到大： 123456arr = [3,7,1,-9,20,10]res = sorted(arr) print(res)-----------------------------[-9, 1, 3, 7, 10, 20] 当然，既然我们能从小到大来进行排序，那就可以用从大到小的方式： 12345arr = [3,7,1,-9,20,10]print(sorted(arr,reverse=True))-----------------------------[20, 10, 7, 3, 1, -9] 现在我们得到了从小到大排序，也得到了从大到小排序。然后我们再来作妖：能不能按照所有数字的绝对值大虾哦进行排序呢？哎，还记得我们刚讲过的数学相关的函数里有一个求绝对值的函数嘛？既然sorted()这个函数里的参数key可以接收函数，那让我们结合在一起试试看： 123456arr = [3,7,1,-9,20,10]res = sorted(arr,key=abs)print(res)-----------------------------[1, 3, 7, -9, 10, 20] 果然，我们得到了想要的结果。来分析下内部到底做了什么： 1234[3,7,1,-9,20,10] # 原始列表3 7 1 9 20 10 # 求绝对值1 3 7 9 10 20 # 给绝对值进行排序1 3 7 -9 10 20 # 转换成原本的值 那现在，我再多尝试一下，我试试看自己定义一个函数： 12def func(num): return num % 2 函数定义好了，让我们尝试使用自定义函数对数据进行排序： 12345arr = [3,2,4,6,5,7,9]print(sorted(arr, key = func))-----------------------------[2, 4, 6, 3, 5, 7, 9] 看似起结果了。那到底函数内干了些什么呢？让我们在其中多打印一点东西出来，看个明白： 1234567891011121314151617def func(num): print(num, num % 2, end=&quot; &quot;) print() return num % 2 arr = [3,2,4,6,5,7,9]print(sorted(arr, key = func))-----------------------------3 1 2 0 4 0 6 0 5 1 7 1 9 1 [2, 4, 6, 3, 5, 7, 9] 这样我们就很清晰的看到了对原数字和取余结果，在对取余进行排序之后，再在取余的结果上进行默认的从小到大进行排序，就得到了最后的结果[2, 4, 6, 3, 5, 7, 9] 不过，这种功能大多数时候我们基本是临时用一下，特意写一个方法似乎有点多余。还记得咱们之前讲的匿名函数吧？让我们用匿名函数优化一下： 1234567# 用匿名函数优化arr = [3,2,4,6,5,7,9]res = sorted(arr, key=lambda num:num%2)print(res)-----------------------------[2, 4, 6, 3, 5, 7, 9] 正是我们所要的结果。 从这就能看出来，高阶函数sorted()的key因为能接收自定义函数，所以给了我们很大的可玩空间。小伙伴们还能想到哪些方法，快去做做实验。 map() 这个函数在对传入的可迭代数据中的每一个元素进行处理，然后返回一个新的迭代器： 12345678map(func, *iterables)'''功能： 对传入的可迭代数据中的每个元素放入到函数中进行处理，返回一个新的迭代器参数： func 函数 自定义函数|内置函数 iterables：可迭代的数据返回值：迭代器''' 让我们先来看一个普通的函数： 12345678910# 把一个字符串数字的列表转为整型列表items = ['1', '2', '3', '4']new_list = []for i in items: new_list.append(int(i))print(new_list)-----------------------------[1, 2, 3, 4] 我们将一个内部元素均为字符串的列表，转成了一个整型列表。 不过这个函数看起来似乎还是有些麻烦，让我们再用map试试看： 123456items = ['1', '2', '3', '4']res = map(int, items)print(list(res))-----------------------------[1, 2, 3, 4] 看，是不是简便多啦？当然，最后map最后生成的是迭代器而并不是列表，我们还是需要转化一下数据类型。 这段代码中map的处理方式其实非常简单，它将items里的每一个元素用int方法转换成整型，转换完之后形成一个新的迭代器，然后返回。 再让我们看一个示例感受一下, 这次我们将两段对比写在一起： 123456789101112131415161718# 普通方法进行实现items = [1, 2, 3, 4]res = []for i in items: x = i ** 2 res.append(x)print(res)# 使用map函数处理items = [1, 2, 3, 4]def func(x): return x ** 2res = map(func, items)print(res, list(res))-----------------------------[1, 4, 9, 16]&lt;map object at 0x107ea5030&gt; [1, 4, 9, 16] 我们看到执行结果完全一样，不过使用map()的方式更简洁，逻辑也更清晰。我们要知道，Python本身是自带幂次方方法的。即便是我们自己来实现，其实我们还可以把代码写的更简洁： 123456items = [1, 2, 3, 4]res = map(lambda x:x**2, items)print(res,list(res))-----------------------------&lt;map object at 0x107c98610&gt; [1, 4, 9, 16] 基于map的应用，我们来个小作业吧： 我们现在有这样一个列表：['a','b','c','d'] 要求将其转换成：[65,66,67,68] 最后，再给大家留个课后作业， 我给大家两个函数及其功能介绍，大家自己去尝试看看，然后自己琢磨下其用法。 reduce(func, iterable) 功能：每一次从 iterable 拿出两个元素，放入到func函数中进行处理，得出一个计算结果，然后把这个计算结果和iterable中的第三个元素，放入到func函数中继续运算，得出的结果和之后的第四个元素，加入到func函数中进行处理，以此类推，直到最后的元素都参与了运算 filter(func, iterable) 功能：过滤数据，把 iterable 中的每个元素拿到 func 函数中进行处理，如果函数返回True则保留这个数据，返回False则丢弃这个数据。 这两个函数在处理数据上作用都非常大。 好了，下课。大家有问题记得留言。","link":"/python-Built-in-functions/"},{"title":"8. 数据类型 - 字符串详解","text":"Hi, 大家好。我是茶桁。 前几节课中我们学习了函数，那么这节课开始，我们花几节课返过头来详细的学习一下Python内的数据类型。第一节课，让我们先从字符串开始： 回顾字符串的定义方式 了解转义字符 字符串格式化的方法 字符串相关函数 字符串的定义方式 单引号定义字符串 ‘ ’ 双引号定义字符串“ ” 三引号定义字符串‘’‘内容’‘’或者“”“内容”“” 字符串定义时，引号可以互相嵌套 转义字符 一个普通的字符出现在转义符\\的后面时，实现了另外一种意义。 \\ 转义符，续行符。 作为转义符时，在\\后面出现的字符可能会实现另外一种意义。 作为续行符时，在行尾使用了\\后，可以换行继续书写内容。 123456str = '123'\\ '456'print(str)---123456 打印结果看，并未换行，说明续行符起作用了。 \\n 代表一个换行符 123456str = &quot;岁月是一把杀猪刀， \\n但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪刀， 但是它拿长得丑的人一点办法都没有。 \\r代表光标位置（从\\r出现的位置开始作为光标的起点） 12345str = &quot;岁月是一把杀猪刀， \\r但是它拿长得丑的人一点办法都没有。&quot;print(str)---但是它拿长得丑的人一点办法都没有。 \\t代表一个水平制表符（table 缩进） 12345str = &quot;岁月是一把杀猪刀\\t但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪刀 但是它拿长得丑的人一点办法都没有。 \\b 代表一个退格符 12345str = &quot;岁月是一把杀猪刀\\b但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪但是它拿长得丑的人一点办法都没有。 注意看，并不是毫无改变的打印出来了，整句话中\\b前面的刀这个字被退格了。 \\\\ 反转义\\，输出了\\，取消\\的转义效果 12345str = &quot;岁月是一把杀猪刀\\\\n但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪刀\\n但是它拿长得丑的人一点办法都没有。 第二个\\被前面的\\转义了，所以n就不会再被转义，也就没有换行。 r， 如果我们想把转义字符也作为普通字符输出，那我们可以在字符串的最前面加上r 12345str = r&quot;岁月是一把杀猪刀\\n但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪刀\\n但是它拿长得丑的人一点办法都没有。 字符串内的转移字符\\n被打印了出来。 字符串相关的操作 字符串+操作, 将参与运算的字符串相加后组成一个新的字符串。 123456789str=&quot;君不见，黄河之水天上来，奔流到海不复回。&quot;str2 = &quot;君不见，高堂明镜悲白发，朝如青丝暮成雪。&quot;res = '将进酒\\n'+ str + '\\n' + str2print(res)---将进酒君不见，黄河之水天上来，奔流到海不复回。君不见，高堂明镜悲白发，朝如青丝暮成雪。 字符串*操作，str*n就是 将当前字符串重复n遍 1234567str = '重要的话说三遍\\n' * 3print(str)---重要的话说三遍重要的话说三遍重要的话说三遍 字符串[]切片操作 字符串的索引操作，字符串中只能使用[]下标访问，不能修改。 123456str[start:stop:step]功能，获取str的特定下标或者对str进行切片操作参数： start: 可选，开始值，默认为0 stop: 可选，结束值，默认为len(str) step：可选，步进值，默认为1 因为所有参数都是可选项，所以其实我们可以什么参数都不给，直接使用默认值： 12345678str = '凡诗之所谓风者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。'print(str)# 等同于print(str[::])---凡诗之所谓风者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。凡诗之所谓风者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。 当我们写一个值，那就是获取指定下标的元素： 1234print(str[6])---者 但是当我们只写一个值，并且后面跟上符号::, 那含义就是从start开始，向后取完： 1234print(str[6::])---者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。 从这我们可以看出来，当我们只写一个单独的值而没有加::的时候，含义就是从start开始，但是并不向后继续取值，而有了::就是继续向后取值。其实，只写一个:也是一样的，因为只要知道向后取值，step默认值就是为1: 1234print(str[6:])---者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。 那如果我们在这个基础上加上一个值，那就是从start开始直到stop之前。和range()一样，取不到stop。 1234print(str[2:6])---之所谓风 然后再多加一个值，和range()一样，就是往后数step个数再取值： 1234print(str[2:15:2])---之谓者多于巷谣 其实，这里比较饶的并不是如何取值，二是::这两个符号。当我们将上面讲的这些内容了解通透后，就可以玩转字符串的切片了。 那对应的，如果我们想将字符串完全取值，但是是隔一个取一个，那我们就可以使用start和stop的默认值，而只定义step 1234print(str[::2])---凡之谓者多于巷谣作所男相咏，言情也 那如果我们想让整个字符串倒过来呢？ 1234print(str[::-1])---。也者情其言各，歌咏与相女男谓所，作之谣歌巷里于出多，者风谓所之诗凡 字符串的格式化方法 常用的字符串的格式化方法就是format() 先让我们看看最普通的方式： 12s = '茶桁'str = '乘舟将欲行，忽闻岸上踏歌声。' 我定义了这两个字符串，现在我想将两段字符串合在一起变成一句“茶桁乘舟将欲行，忽闻岸上踏歌声。”（嗯，权吾乃青蓮居士。） 很多小伙伴是不是觉得太简单了，我们之前学了+号，直接拼接不就好了。自然也是可以的，只是我们现在要用更普遍和便捷的方式来完成： 123456s = '茶桁'str = '{}乘舟将欲行，忽闻岸上踏歌声。'.format(s)print(str)---茶桁乘舟将欲行，忽闻岸上踏歌声。 假如说，我们现在只有诗词的大半句，其中少了踏歌行这三个字，那我们又该如何？那我们就往format中传入两个参数，后面那个参数自定义出这三个字符就可以了： 123456s = '茶桁'str = '{}乘舟将欲行，忽闻岸上{}。'.format(s, &quot;踏歌行&quot;)print(str)---茶桁乘舟将欲行，忽闻岸上踏歌行。 看到这里，我们是不是认为字符串使用format就只能顺序传值？第一个答案填入第一个空，第二个答案填入第二个空... 其实不只是如此，字符串后使用format，其中的{}还可以接受索引传参： 123456s = '茶桁'str = '{1}乘舟将欲行，忽闻岸上{0}。'.format(&quot;踏歌行&quot;, s)print(str)---茶桁乘舟将欲行，忽闻岸上踏歌行。 通过索引传参的适用范围毕竟还是有限，我们很容易一不小心就会把参数顺序搞乱。那还有没有其他办法呢？ 我们还可以通过关键字传参： 12345str = '{s2}乘舟将欲行，忽闻岸上{s1}。'.format(s1 = &quot;踏歌行&quot;, s2 = &quot;茶桁&quot;)print(str)---茶桁乘舟将欲行，忽闻岸上踏歌行。 那假如说我们得到的是一个列表数据，是否需要先转换数据？其实也没必要，format支持对容器型数据的传参数： 12345str = '豪放派：{0[0]}，婉约派：{0[1]}，流氓派:{0[3]},蛋黄派：{0[2]}'.format(['李白','辛弃疾','达利园','茶桁'])print(str)---豪放派：李白，婉约派：辛弃疾，流氓派:茶桁,蛋黄派：达利园 那么如果是字典类型的呢？那就更简单了，我们之前提到的关键字传参，不就正好对应字典吗？ 123456dict = {'a':'茶桁', 'b':'蛋黄派'}str = '{a}乘舟将欲行，忽闻岸上{b}'.format(**dict)print(str)---茶桁乘舟将欲行，忽闻岸上蛋黄派 嗯，不错。似乎我们创建了一句新的诗句。 其实，format还有其他的用法，就是直接用关键字f， 比如： 12345str = f'{dict[&quot;a&quot;]}乘舟将欲行，忽闻岸上{dict[&quot;b&quot;]}'print(str)---茶桁乘舟将欲行，忽闻岸上蛋黄派 f是在3.7版本中新增的格式化方法，在使用的过程中，要注意字符串符号“”和‘’的嵌套关系。 在基本使用之外，我们还有一些风骚的特殊用法，比如，我们可以用format直接限定小数的位数： 12345str = '圆周率是多少：{:.5f}'.format(3.1415926)print(str)---圆周率是多少：3.14159 字符串相关函数 在Python中，字符串应该是最常见的数据类型，对应字符串的函数也有不少。大家可以去看看官方的文档 英文字符与字符检测相关函数 我们可以返回字符串的副本，并且将首字母大写，其余小写： 12345str = 'I am a data product manager'str.capitalize()---'I am a data product manager' 因为我在使用Jupyter Notebook，所以即便我么有使用print，依然可以打印出执行结果。只是仅可以打印最后一个执行的函数。 可以把字符串中的一个单词的首字母大写： 1234str.title()---'I Am A Data Product Manager' 可以全部改为大写： 1234str.upper()---'I AM A DATA PRODUCT MANAGER' 把字符串全部改为小写 1234str.lower()---'i am a data product manager' 字符串中的大小写字符转换，大写转小写，小写转大写: 1234str.swapcase()---'i AM A DATA PRODUCT MANAGER' 检测字符是否包含在字符串内： 1234print('o' in 'love')---True 检测字符串是否为全部大写字母组成 1234str.isupper()---False 检测字符串是否为全部小写字母组成 1234str.islower()---False 检测字符串是否符合标题title的要求 1234str.istitle()---False 检测字符串是否由数字和字母组成，如果字符串中包含来非数字字母的其它字符，则返回False 1234str.isalnum()---False 检测字符串是否全部由字符(包含英文字符和中文)组成 1234str.isalpha()---False 检测字符串是否由纯数字字符组成 1234'123'.isdigit()---True 检测当前字符串是否为 空格 字符组成 ' ’ 1234' '.isspace()---True 检测字符串是否以指定的字符开始的，也可以指定开始和结束的位置 1234str.startswith('I')---True 1234str.startswith('a', 5)---True 检测字符串是否以 指定的字符 结束的，也可以指定开始和结束的位置 12345678print(str.endswith('a'))print(str.endswith('a', 5, 11))print(str.endswith('a', 1, 6))---FalseTrueTrue 字符串的查找和操作相关函数（✨ 重点） 前面铺垫了那么多之后，接下来这部分，才是这一节的重点。 让我们先从查找来看： str.find(sub[, start[, end]]) find会返回一个子字符串，找到字符中符合条件的第一个字符出现的索引位置，未找到则返回-1 12345str = &quot;I am a data product manager.&quot;print(str.find('am'))---2 让我们用切片的方式反过来找一下看看： 12345res = str.find('am')str[res:res+2]---'am' 我们从之前可以知道res取值为2，现在等于是str[2:4]， 正好是am所在的位置。 find中有start和end，是支持切片查找的： 123456print(str.find('am', 0, 4))print(str.find('am', 4, 10))---2-1 可以看到，在从4开始找到10的时候找不到am, find有一个功能相同，但是方向不同的方法rfind(), 和find的不同点只是，rfind是从后往前找的。 str.index(sub[, start[, end]]) 类似于find()， 但在找不到子字符串的时候会引发ValueError 1234str.index('python')---ValueError: substring not found str.count(sub[, start[, end]]) 这个函数会在字符串中去查找sub在其中[start, end]范围内非重叠出现的次数。 123456print(str.count('a'))print(str.count('a', 5, 12))---63 接下来让我们看看字符串操作相关的函数： str.split(sep=None, maxsplit=-1) 这个方法可以按照指定的分隔符(sep)，把字符串分隔成列表。 12345str = 'user_admin_id_123'str.split('_')---['user', 'admin', 'id', '123'] 整个方法里的maxsplit是进行多少次拆分，比如1为一次拆分，也就是会返回2个元素。默认值为-1，意思是不限制拆分次数。 1234str.split('_', 1)---['user', 'admin_id_123'] str.rsplit(sep=None, maxsplit=-1) 和split方法相似，只是方向不同。这个是从后向前获取。 1234str.rsplit('_')---['user', 'admin', 'id', '123'] 这段代码可以看到功能上是完全一样的，如果我们把maxsplit加进去，就能看到方向上的不同： 1234str.rsplit('_', 1)---['user_admin_id', '123'] 这样就能清晰看到，rsplit是从后面开始拆分的。 str.join(iterable) join的功能和split可以看成是相反的，是使用指定的字符串，把一个容器中的元素连接成一整个字符串 12345str = ['user', 'admin', 'id', '123']'_'.join(str)---'user_admin_id_123' str.strip([chars]) 去除字符串左右两侧的指定字符, chars参数为置顶要溢出字符的字符串，默认移除空白符。 12345str = ' chaheng 'str.strip(' ')---'chaheng' 这个函数有两个伴生函数，一个是rstrip， 从方法名应该能猜的出来，这是去掉字符串右侧的指定字符，另一个是lstrip， 这是去除左侧的指定字符。 123456str.rstrip(' ')str.lstrip(' ')---' chaheng''chaheng ' len()函数可以获取当前字符串的长度 1234len(str)---9 str.replace(old, new[, count]) 可以替换对应的字符串，将old都替称为new。count则是替换次数。比如一个字符串内出现了十次old， 我`count给的5, 则只替换前5次出现的old字符串。 1234567str = 'abcabcabcabcabcabc'str.replace('a', 'e')str.replace('a', 'e', 2)---'ebcebcebcebcebcebc''ebcebcabcabcabcabc' 可以注意一下两次打印的区别。 这次就不留练习题了，字符串的查询和操作函数属于重中之重，大家最好是多去练习几遍，将其中的方法记会杯熟。 好，今天就到这里。咱们下节课再见。","link":"/Detailed-of-string/"},{"title":"9. 数据类型 - 列表详解","text":"Hi，大家好。我是茶桁。 最近几节课，我们都是在详细讲解Python内的数据类型，上一节课我们详细了解了字符串，这节课，让我们来详解一下列表。 首先，我们先有一个大的概念，列表，其实就是一组有序的数据组合；另外，列表中的数据是可以被修改的。也就是说，列表是一个可变序列类型。 列表定义 如何在Python的定义列表，记住以下几点就可以了： 可以使用中括号进行定义[] 可以使用list()函数定义 还可以使用列表推导式定义: [x for x in iterable] 在定义列表中的元素时，需要在每个元素之间使用逗号（英文逗号），进行分隔。[1, 2, 3] 列表中的元素可以是任意类型，通常用于存放同类项目的集合。 列表的基本操作 让我们先来定义一个列表: 1234567items = [1, 2, 3, 4]items2 = list('1234')print(items,'\\n', items2)---[1, 2, 3, 4] ['1', '2', '4', '5'] 我们使用了最基本的两个方式来定义列表。至于列表推导式， 先不用着急，我们后面会单独讲它。 我们可以看到，刚才我刻意将item和items两个列表定义了不同种类的元素，那他们到底能否拼接在一起？我们尝试一下列表的相加： 1234print(items + items2)---[1, 2, 3, 4, '1', '2', '3', '4'] 没问题，两种不同类型的元素拼接到了一起，组成了一个新的列表。 让我们将这段代码搞的复杂一点，新的列表对于我要的模拟数据来说太少了，我想再增加5倍的长度： 1234print((items + items2) * 5)---[1, 2, 3, 4, '1', '2', '3', '4', 1, 2, 3, 4, '1', '2', '3', '4', 1, 2, 3, 4, '1', '2', '3', '4', 1, 2, 3, 4, '1', '2', '3', '4', 1, 2, 3, 4, '1', '2', '3', '4'] 没毛病，也就是说，我将小学学到的基本数学运算用到这里完全适用。 那如果用到减法呢，虽然难以想象最后的结果，试试中可以： 1234print(items - items2)---TypeError: unsupported operand type(s) for -: 'list' and 'list' 果然是我想多了，完全不支持操作数类型。 那是不是关于列表的操作也就到此为止了？并不是，列表除了利用加和乘进行拼接和循环的操作之外，还有很多其他的基本操作，比如： 12345items[2] = 9print(items, &quot;\\t&quot;,items[3])---[1, 2, 9, 4] 4 这里，我们利用了列表的下标操作修改了列表内的下标[2]的元素（第三个），并且将修改后的列表和列表内下标[3]的元素打印了出来。 有这样一种情况大家想过没有，这个列表呢，我并不知道有多长，但是我知道最后一个数字，现在我就想把最后一个数字取出来该怎么办？用len()获取长度之后再-1? 是不是太麻烦了？ 还记得之前咱们讲过，下标是可以从后往前数的吗？ 1234items[-1]---4 嗯，我想再这个列表添加几个数字： 1234items[4] = 10---IndexError: list assignment index out of range 哎，我似乎想的并不对。本以为原列表下标[3]是最后一个元素，那我多加一个下标就会再多加一个元素，可是似乎并不行。那么我们该怎么在列表内最佳元素呢？ 可以尝试一下专门添加元素的append()函数： 123456items = [1, 2, 3, 4]items.append(2)print(items)---[1, 2, 3, 4, 2] 加是加了，可是我们之前是想加10的，现在不小心加成2了，不行，我要删了它。该怎么办？随便吧，我就记得windows的CMD命令中的删除文件似乎是del，试试看： 12345del items[-1]print(items)---[1, 2, 3, 4] 居然成了... 这就神奇了。看起来，Python并不是很难。不过我们这里不得不说，在Python中还有一个针对列表删除元素的方法：pop() 12345items = [1, 2, 3, 4]items.pop()---[1, 2, 3] pop([index=-1])函数专门用于移除列表中的一个元素，其中参数index为索引值，默认为-1，也就是说默认是从列表移除最后一个值。 1234567# 将索引值改为从前数第一个items = [1, 2, 3, 4]items.pop(0)print(items)---[2, 3, 4] 列表中的切片 在学习了列表的基本操作之后，我们来看看列表中的切片。提前说一声，在数据分析的应用中，对数据整理的过程绝大多数时候都需要用到列表的切片操作， 所以大家这部分要好好理解。 列表的切片操作基本语法其实很简单 list[开始值:结束值:步进值] 看起来很熟悉对吧？在我们之前介绍字符串相关的操作的时候，就是这种方式。其用法和字符串中也是如出一辙： list[开始值:] 从开始值索引到列表的最后 list[:结束值]从列表最前面索引到结束值之前 list[开始值:结束值]按照给到的开始值开始索引，到结束值之前为止。 当然，除了这三个基本的操作之外还有list[::], list[::步进值], list[开始值::步进值], list[:结束值:步进值],list[开始值:结束值:步进值]，我们下面一一的来看看，在字符串相关操作中没有特别理解的没关系，这里再来加深下印象： 12# 先来定义一个列表方便后续操作items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP'] 从开始值索引到最后： 1234print(items[2:])---['Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP'] 从下标[2]开始，也就是从第三个Ruby开始向后索引。 从最前面索引到结束值之前： 1234print(items[:2])---['Python', 'Java'] 现在我们让这两个语言单独露脸，算是对它们进行补偿了。 从开始值索引到结束值之前： 1234print(items[2:3])---['Ruby'] 哎，为什么只索引出来一个值？因为结束值为3，它之前不就是2吗。开始值也是2，那可不就只有一个值而已。 这回，我们把步进值加上： 12345# 加上步进值print(items[0:-1:2])---['Python', 'Ruby', 'C++', 'JavaScript'] 从最前面索引到最后，步进值为2，所以是隔一个索引一个。那为什么PHP没索引到？估计你又忘了，是索引到结束值之前，不包含结束值，自然PHP就没被索引到。 只有步进值会是什么情况？ 12345# 只有步进值print(items[::-2])---['PHP', 'JavaScript', 'C++', 'Ruby', 'Python'] 步进值为负数，那显然是从后向前索引了。隔一个索引一个，等等，为啥第一个Python被索引到了？ 那是因为，当我们开始值和结束值都没取值的情况下，默认是从头到尾索引，现在嘛，应该是从尾到头索引。也就是包含了头尾，不存在最后一个值之前，所以列表内的所有值都索引了一个遍，只是因为有步进值的关系，所以变成隔一个索引一个。 再让我们将所有值都去掉，只留下[::]试试看： 1234567# 删掉所有值试试print(items[::])print(items[:])---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP']['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP'] 从结果上看，中括号内一个冒号和两个冒号出来的结果是一样的。 在索引查找之后，我们来看看，利用切片的方式是否可以对列表内的元素进行更新和删除？ 从指定下标开始，到指定下标前结束，并替换为对应的数据(容器类型数据，会拆分成每个元素进行赋值) 123456items = [1, 2, 3, 4, 5]items[2:4] = [7, 8]print(items)---[1, 2, 7, 8, 5] 指定的切片内的元素被替换掉了。 刚才我们使用切片替换元素的时候元素是一一对应的，那如果我们没有对应的话会发生什么？ 1234567# 切片范围大于添加元素的个数items = [1, 2, 3, 4, 5]items[2:6] = [7]print(items)---[1, 2, 7] 结果并没有报错，而是将切片范围内的元素都移除之后添加了一个元素7。我们再试试其他的： 1234567# # 切片范围小于添加元素的个数items = [1, 2, 3, 4, 5]items[2:3] = [7, 8, 9, 0]print(items)---[1, 2, 7, 8, 9, 0, 4, 5] 可以看到，比起原本的列表，我们的值增加了。原本下标[2]的元素被移除之后，在这个位置插入了[7,8,9,0]四个元素。 以此，我们可以总结切片更新列表，实际上就是删除掉切片范围内的元素，再在原来的位置上插入新加的元素，并且将之后的元素向后移动。 那既然这样的话，我们是不是可以利用这种特性对列表内的元素进行删除？ 123456items = [1, 2, 3, 4, 5]items[2:4] = []print(items)---[1, 2, 5] 没毛病，确实可以这么用。 当然，除了这种插入空列表的方式之外，还有其他方式可以删除列表内的指定元素, 还记得前面我们介绍的del方法吗？ 123456items = [1, 2, 3, 4, 5]del items[2:4]print(items)---[1, 2, 5] 那既然我们可以用添加空列表的方式来删除列表内的元素，del是不是就没用武之地了？实际上，并非如此。del有一个特殊的用法，就是在利用步进值来跳着删除元素： 123456items = [1, 2, 3, 4, 5]del items[0:6:2]print(items)---[2, 4] 那聪明的小伙伴肯定想，添加空列表的方式也加上步进值就不行吗？我们来试试： 123456items = [1, 2, 3, 4, 5]items[0:5:2] = []print(items)---ValueError: attempt to assign sequence of size 0 to extended slice of size 3 报错提示我们，序列分配不正确。说明我们不能这样使用。如果要这样使用的话，替换的元素个数必须对应才行： 123456items = [1, 2, 3, 4, 5]items[0:4:2] = [9, 10]print(items)---[9, 2, 10, 4, 5] 列表相关函数(✨ 重点) 除了以上介绍的关于列表的一些方法之外，Python还为我们提供了一些列表常用的相关函数： len() 这个函数可以检测当前列表的长度，列表中元素的个数： 12345items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']len(items)---9 count() 这个函数可以检测当前列表中指定元素出现的次数： 1234items.count('Python')---1 append() 这个函数前面我们已经介绍过了，就是向列表尾部追加新的元素，返回值为None。 12345items.append('SQL')print(items)---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', 'SQL'] insert() 这个函数可以向列表中指定的索引位置添加新的元素。 12345items.insert(4, 'Go')print(items)---['Python', 'Java', 'Ruby', 'Rust', 'Go', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', 'SQL'] pop() 还记得我们之前删除列表中元素的时候介绍pop()函数吗？其实，pop()函数是对指定索引位置上的元素做出栈操作，然后返回出栈的元素 123456789items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']print(items.pop())print(items.pop(2))print(items)---PHPRuby['Python', 'Java', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node'] 默认的情况下，pop()是把列表的最后一个元素出栈，当给值之后，是将指定索引的元素进行出栈。 remove() 这个函数是专门删除特定元素用的，可以指定列表中的元素进行删除，只删除第一个，如果没有找到，则会报错。 12345678items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.remove('PHP')print(items)items.remove('Go')---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node']ValueError: list.remove(x): x not in list 可以看到，第一个remove成功删除了PHP，但是第二个remove并未在列表中找到Go，所以报错。 index() 这个函数可以查找指定元素在列表中第一次出现的索引位置 12345items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.index('PHP')---8 除此之外，index()还能接收索引值，当输入索引值的时候，index()会在指定范围内查找元素的索引位置： 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']print(items.index('Ruby', 0, 5))items.index('PHP', 0, 5)---2ValueError: 'PHP' is not in list 可以看到，指定范围内没有说要查找的元素的时候就会报错，告知元素不在列表内。 extend() 这个函数接收一个容器类型的数据，把容器的元素追加到原列表中 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']a = ['Go', 'MATLAB']items.extend(a)print(items)---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', 'Go', 'MATLAB'] 看到这，是不是感觉很像两个列表相加？那既然我们可以将两个列表相加了，这个方法似乎有些多余了。 这么想的小伙伴们，我们再来看两段示例： 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']a = (1, 2, 3)items.extend(a)print(items)---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', 1, 2, 3] 另外一段： 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']a = (1, 2, 3)items = items + aprint(items)---TypeError: can only concatenate list (not &quot;tuple&quot;) to list 可以看到，第二段代码直接报错了。那说明，相加这个操作必须两个都是列表才可以，不支持列表和元组相加。可是extend()方法是支持将任意一个容器类型的数据中的元素追加到原列表中的。 我们再来多看一段： 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']a = '1234'items.extend(a)print(items)---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', '1', '2', '3', '4'] 将a定义为一段字符串，一样可以使用extend()来接收并追加到原列表内。 clear() 这个函数比较简单，就是清空列表内容 123456items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.clear()print(items)---[] reverse() 这个函数可以对列表进行翻转 123456items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.reverse()print(items)---['PHP', 'Node', 'JavaScript', 'Swift', 'C++', 'Rust', 'Ruby', 'Java', 'Python'] sort() 该函数将对列表进行排序, 在默认的情况下，会对元素进行从小到大的排序 123456items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.sort()print(items)---['C++', 'Java', 'JavaScript', 'Node', 'PHP', 'Python', 'Ruby', 'Rust', 'Swift'] 额，这样似乎并不明显，我们重新换个案例。不过大家也可以想想现在这段代码中，为什么会有这样的结果。 123456items = [9, 3, 5, 2, 1, 7, 8, 0, 6]items.sort()print(items)---[0, 1, 2, 3, 5, 6, 7, 8, 9] 嗯，这回明显了。 除了从小到大排序外，我们还可以将其从大到小排序，利用关键参数reverse来开启： 123456items = [9, 3, 5, 2, 1, 7, 8, 0, 6]items.sort(reverse=True)print(items)---[9, 8, 7, 6, 5, 3, 2, 1, 0] OK，现在让我们来回过头来解释一下第一段代码中的结果：['C++', 'Java', 'JavaScript', 'Node', 'PHP', 'Python', 'Ruby', 'Rust', 'Swift']， 之所以会产生这样的结果，不是因为它按英文字母来排序，当然这么想也对但是不全对，而是因为它的排序依据是ASCII码，之前我们学习过，ASC II码只包含了128个字符，仅仅是美国的标准，128个字符里面都是西文码，那么如果中间包含了中文会怎样呢？ 不如我们直接来看看： 123456items = [9, 3, 5, 2, 1, 7, 8, 0, '茶', '桁']items.sort(reverse=True)print(items)---TypeError: '&lt;' not supported between instances of 'int' and 'str' 完了，直接报错。不过这个似乎和编码无关，而是数据类型的问题，告诉我们字符和整型之间不能排序。别问我怎么看懂的，我也是查字典。 知道是数据类型的问题就好办了，我们将数据类型变成一致的再试试： 123456items = ['9', '3', '5', '2', '1', '7', '8', '0', '茶', '桁']items.sort(reverse=True)print(items)---['茶', '桁', '9', '8', '7', '5', '3', '2', '1', '0'] 居然成功了，那既然是ASCII码，那为什么还会支持中文排序呢？还记得我们除了介绍ASCII码之外，还介绍过一个Unicode编码。那即是说，Python中的sort()排序的依据是Unicode编码。 当然，除了默认规则之外，我们还可以自己对排序进行干预，加上你想要的规则。sort(key)内的key参数可以接收一个函数，按照函数的处理结果进行排序： 123456items = [-5, -3, 5, 2, 0, -9, 12, 14, -1, -6]items.sort(key=abs)print(items)---[0, -1, 2, -3, -5, 5, -6, -9, 12, 14] 这一段是不是让小伙伴们想到之前我们在Python的内置函数中介绍高阶函数的内容？没错，就是一样的。所以，我们这次就不对函数内部排序过程进行分析了，有兴趣的小伙伴可以回去看看第七节的内容。 深拷贝与浅拷贝 接着，让我们来看看关于拷贝的问题，先说浅拷贝。 说到浅拷贝，实际上是仅拷贝了列表中的一维元素，如果列表中存在二维元素或容器，则为引用而不是拷贝。使用copy函数或者copy模块中的copy函数拷贝的都是浅拷贝。 123456items = [1, 2, 3]res = items.copy()print(items, '\\t', res)---[1, 2, 3] [1, 2, 3] copy()之后的新列表和原列表内容上是一样的。 接着让我们来操作一下copy之后的res 123456789items = [1, 2, 3]res = items.copy()del res[2]print(items)print(res)---[1, 2, 3][1, 2] 可以看到，对res进行操作完全不影响原列表的内容。这就说明，copy产生的新列表和原列表并不是一个列表，我们可以验证一下看看： 123456print(id(items))print(id(res))---46363598724636086464 当我们用id()函数的时候，可以看到他们是两个完全不同的id 刚才我们定义的items是一个一维列表，接着让我们再来定义一个多维列表来尝试一下: 123456789items = [1, 2, 3, 4, ['a', 'b', 'c']]res = items.copy()del res[3]print(items)print(res)---[1, 2, 3, 4, ['a', 'b', 'c']][1, 2, 3, ['a', 'b', 'c']] 我们可以看到，做删除操作之后，res内容变了，但是原列表items没变化。似乎和之前的并没有什么不同，让我们再继续试试: 1234567del res[3][1]print(res)print(items)---[1, 2, 3, ['a', 'c']][1, 2, 3, 4, ['a', 'c']] 发生了什么？我们明明是操作的res而不是原列表items， 为什么items也发生了变化？难道是id是相同的吗？来，试试就知道了。 123456print(id(items))print(id(res))---46364272644636085824 似乎并不相同。那既然不是同一个元素，为什么我们操作res的时候，items也会跟着一起变化？ 别着急，让我们接着看下面的操作: 123456print(id(items[4])) # items这个位置是列表['a', 'c']print(id(res[3])) # res这个位置是列表['a', 'c']---46352459524635245952 如何，一模一样对吧？这就说明，在items以及它的copy列表res中，这个嵌套的列表是同一份。这也就能解释为什么我们对res内的嵌套列表进行操作的时候, items也发生了变化。 这个就是我们在一开始说到的，copy仅仅是拷贝了列表中的一维元素，对二维元素和容器仅仅是引用，这个应用对象当然还是原来那个对象。所以，两者的id才是是同一个。 浅拷贝我们理解完之后，才看看什么是深拷贝。 深拷贝和浅拷贝比起来就有深度的多，嗯，这么讲是因为深拷贝不仅仅是拷贝了当前的列表，同时还把列表中的多维元素或容易也拷贝了一份，而不是像浅拷贝一样仅仅是引用。完成深拷贝的函数是copy模块中的deepcopy函数。 12345items = [1, 2, 3, ['a', 'b', 'c']]res = items.deepcopy()---AttributeError: 'list' object has no attribute 'deepcopy' 额，尴尬。居然报错了... 似乎deepcopy并不是和copy函数一样的用法。 细心的小伙伴应该之前就注意到了，在介绍copy函数和deepcopy函数的时候，我都在强调是copy模块中的这句话，确实，我们在使用deepcopy的时候，是需要先引用模块再使用的，并且，使用方式也有一些不同: 1234567import copyitems = [1, 2, 3, ['a', 'b', 'c']]res = copy.deepcopy(items)print(res)---[1, 2, 3, ['a', 'b', 'c']] 没错，我们这就对items完成了深拷贝，生成了新的列表res。 那到底是否是真的深拷贝呢？让我们试一试： 1234567891011print(id(items))print(id(res))print(id(items[3]))print(id(res[3]))---4636282048463479987246362851204637491072 没问题，我们打印出来的id各不一样，包括items内的二维列表以及res内的二维列表，id也都不同，说明确实是深拷贝。 不放心的小伙伴，我们再来更改列表元素测试一下: 12345678del res[3][0]print(res[3])print(items[3])---['b', 'c']['a', 'b', 'c'] 可以看到，当我们更改res内的二维列表时，items并未发生改变。说明二维列表我们也一样完成了拷贝，而不是像浅拷贝一样仅是引用了。 列表推导式 在本文最开始，我们介绍列表的时候提过三种列表生成方式，包括直接定义列表, 用list函数，最后一个就是列表推导式。那我们接下来，就要详细讲讲列表推导式。 列表推导式提供了一个更简单的创建列表的方法，常见的用法是把某种操作应用于序列或可迭代的每个元素上，然后使用其结果来创建列表，或者通过满足某些特定条件元素来创建子序列。 采用一种表达式的当时，对数据进行过滤或处理，并且把结果组成一个新的列表。 哎，最怕就是定义和文字过多，让我们直接上示例吧。 基本的列表推导式使用方式 结果变量 = [变量或变量的处理结果 for 变量 in 容器类型数据] 现在，假设我们想要创建一个平方列表： 123456789# 使用普通方法完成items = []for i in range(10): items.append(i**2)print(items)---[0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 这是用我们所学过的内容来进行创建，当然，我们还学过另外一种方式: 123456# 使用 map函数和list完成items = list(map(lambda x: x**2, range(10)))print(items)---[0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 这里似乎有一点复杂，咱们还是来分析一下吧。 首先，我们创建了一个匿名函数lambda x:x**2, 再创建了一个可迭代对象range(10)。 接着，我们给map函数传入了这两个参数，分别传给了func和*iterables, 关于map函数，我们在第七节：内置函数中有讲解过，完了的小伙伴可以翻看前面复习一下。 map函数在对传入的可迭代数据中的每一个元素进行处理，然后返回一个新的迭代器, 最后用list函数将这个新的迭代器转换成了一个列表。 然后，我们将传入的func函数用一个匿名函数 没错，我们使用map函数和list也可以进行实现。 那么最后，让我们来看看列表推导式如何完成这个需求： 123456# 列表推导式items = [i**2 for i in range(10)]print(items)---[0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 简简单单一句话，比用map函数更简单的逻辑，就完成了我们的需求。这一句话的代码其实逻辑桁清晰，也很好理解： 首先，我们用for来进行循环传值给i， 接着，我们用i**2来得到我们期望的值，最后生成列表。本质上，其实和我们用的第一种普通方法是一样的。 接着我们再来看一个， 我们现在有一个字符串'1234'， 想要得到这样一个列表[2, 4, 6, 8]。照例，从普通方法开始： 12345678910# 普通方法str = '1234'items = []for i in str: items.append(int(i)*2)print(items)---[2, 4, 6, 8] OK，没问题。我们继续： 123456789items.clear()print(items)items = list(map(lambda x:int(x)*2, str))print(items)---[][2, 4, 6, 8] 可以看到，我们先将items清空之后再继续操作的，这次我们用了list+map的方式，一样得到了我们想要的结果。 最后，当然是用列表推导式的方式： 123456789items.clear()print(items)items = [int(i)*2 for i in str]print(items)---[][2, 4, 6, 8] 同样，我们得到了想要的结果。 讲到这里了，我给大家秀一个小技巧，俗称骚操作，就是我们其实可以运用位运算操作符： 123456789items.clear()print(items)items = [int(i) &lt;&lt; 1 for i in str]print(items)---[][2, 4, 6, 8] 具体代码执行的时候发生了什么，就算是给大家留个小思考题。提示：可以回头翻看下我们之前讲到的位运算符。 带有判断条件的列表推导式 除了基本的列表推导式，我们还有一种带有判断条件的列表推导式。 结果变量 = [变量或变量的处理结果 for 变量 in 容器类型数据 条件表达式] 相比起基本的列表推导式，我们现在多了一个条件表达式，那么我们该怎么利用呢？来个需求：从0 ~ 9，求所有的偶数并且形成一个新的列表。这回，我们就只完成常规方法和列表推导式，对比着来观察一下： 12345678910# 常规方式items = []for i in range(10): if i % 2 == 0: items.append(i)print(items)---[0, 2, 4, 6, 8] 很好，我们完成了需求。接下来，大家试试不看我下面写的代码，自己从常规方式思考下该怎么写，然后自己运行一下试试写对了没，最后，再和我写的对比一下看看咱们写的有没有区别。 12345items = [i for i in range(10) if i % 2 == 0]print(items)---[0, 2, 4, 6, 8] 没错，就是这么简单，你做对了吗？ 带有条件判断的多循环推导式 现在有这样一个需求，我们拿到两个列表[1,2,3], [3,1,4], 要将这两个列表中的元素两两组合，要求组合的元素不能重复： 12345678910# 常规方法items = []for x in [1, 2, 3]: for y in [3, 1, 4]: if x != y: items.append((x,y))print(items)---[(1, 3), (1, 4), (2, 3), (2, 1), (2, 4), (3, 1), (3, 4)] 这样，我们就完成了刚才的需求。那用推导式该如何实现呢？ 12345items = [(x, y) for x in [1, 2, 3] for y in [3, 1, 4] if x != y]print(items)---[(1, 3), (1, 4), (2, 3), (2, 1), (2, 4), (3, 1), (3, 4)] 没毛病，我们完全实现了刚才的需求。这个很好理解对吧？ 让我们接着来看最后一个推导式的形式。 对于嵌套循环的列表推导式 这次我们直接写需求，然后上示例。 需求为，现在我们有一个3x4的矩阵，由3个长度为4的列表组成，我们现在要交换其行和列。 注意哦，这个行转列需求在处理数据的时候经常会用到。 1234567891011121314151617# 需求样例'''[ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]==&gt;[ [1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]''' 来，让我们尝试着实现一下： 1234567891011121314151617181920212223242526# 首先，定义初始数据，大家可以直接copy我给到的矩阵数据# 先定义数据arr = [ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]# 使用常规方法items = []for i in range(4): res = [] for row in arr: res.append(row[i]) items.append(res)print(items)# 使用列表推导式, 我们从内向外来写items = [[row[i] for row in arr] for i in range(4)]print(items)---[[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]][[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]] 这样，我们就完成了刚才的需求。我们拆解呢，还是从外层开始讲起： 首先，因为我们发现数据是4列，所以我们设定了一个range(4)来进行4次迭代，将0,1,2,3这四个下标分别传到内层循环。 然后我们开始在arr内循环找到当前的row, 循环会依次去寻找[1,2,3,4]，[5,6,7,8],[9,10,11,12]。然后将每一个row中的寻找当前的row[i]，并且填入一个新列表内。那么这三组列表中的row[i]就会是这样的： row[1]分别为1, 5, 9, row[2]分别为2, 6, 10.... 依次类推。当外层循环完成之后，就正好是组成了新的四个新的列表，最后再将新列表依次传到items这个空列表内，就完成了。 那同样都是两次for循环嵌套，为什么上面那个案例就是顺序写的，内层for循环写在了后面，而下面这个案例的内层for循环就写到了前面呢？ 好的，让我们来看看，如果将下面这个案例的内存for循环写在后面会是怎样的: 12345items = [row[i] for i in range(4) for row in arr]items---[1, 5, 9, 2, 6, 10, 3, 7, 11, 4, 8, 12] 看到了吗？顺序还是对的，只是依次传入了数据，并未形成矩阵。那有小伙伴就说了，那是不是因为没在row[i]上加[]从而形成列表呢？ 好的，让我们再来做一个实验： 12345items = [[row[i]] for i in range(4) for row in arr]items---[[1], [5], [9], [2], [6], [10], [3], [7], [11], [4], [8], [12]] 可以看到，列表是形成了，但是却是一个元素占一个列表，并没形成我们想要的矩阵。 估计小伙伴们看出来了，在推导式中，因为变量或变量的处理结果必须放在前面，所以我们为了要形成矩阵内层新的row，所以才必须将处理结果和内层循环方法放在一起，并加上[]来确保这组结果能形成一个列表, 也就是我们现在这样： 12345items = [[row[i] for row in arr] for i in range(4)]items---[[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]] 看一次没看懂的小伙伴，可以多看看，尝试着自己去分解，将其理解透彻。因为这个方法在我们后续的数据清洗中使用非常频繁。 小练习 为了能达到练习的目的，从这一节开始，所有练习可以不在课程中展示了。大家先做一下，然后可以在我下一节课中的源码中去找答案，然后来看看和自己做的是否一样。 以下所有练习必须使用列表推导式来实现 有些练习不止一个方法，大家尝试用多种方法来实现一下。 做完的小伙伴可以在课程后面留言讨论。 12345678910111213141516171819202122232425# 1. 让我们尝试将字典中的健值对转成`key = value`的数据格式{'user':'admin', 'age':'20', 'phone':'133'} 转为 ['user=admin','age=20','phone=133']# 2. 把列表中的所有字符全部转为小写['A', 'CCCC', 'SHIss', 'Sipoa','Chaheng', 'Python','dsAhio']# 3. x是0～5之间的偶数，y是0~5之间的奇数，把x，y组成一个元组，放到列表中# 4. 使用列表推导式完成九九乘法表# 5. 求M, N中矩阵和元素的乘积'''M = [ [1, 2, 3], [4, 5, 6], [7, 8, 9]]N = [ [2, 2, 2], [3, 3, 3], [4, 4, 4]]''' 最后，大家记得做练习并且留言，下课。","link":"/Detailed-of-list/"},{"title":"10. 数据类型 - 元组详解","text":"Hi，大家好。我是茶桁。 之前两节分别介绍了字符串和列表，今天，我们来讲讲另外一个常用到的数据类型：元组。 元组和列表很像，两者都是一组有序的数据的组合。但是也有很多不同点，比如元组内的元素一旦定义了就不可以再修改，因此元组称为不可变数据类型。 元组定义 元组的定义方式包括以下要点： 定义元组变量 = (), 或者变量 = tuple() 可以使用变量 = (*iterable)定义含有数据的元组 ⚠️ 需要注意：如果元组中只有一个元素时，这唯一的元素后面也必须加逗号，这是为了区分其他元素标识这是一个元组: (1,) 特例： 变量 = 1,2,3， 这种方式也可以定义为一个元组。 元组的相关操作 由于元组是一个不可变的数据类型，因此其在创建之后只能使用索引进行访问，无法进行其他操作。访问方式其实和列表一样，同样可以使用切片方式获取元素。 元组可以进行切片操作，在访问数据这件事情上和列表几乎一样，没有什么区别，所以完全可以借鉴上一节我讲的内容来看，这里就不详细介绍了，仅仅给大家写出一些案例： 1234567891011121314151617181920# 常见的元组切片索引查询操作tup = 1, 2, 3, 4, 5, 5, 4, 3, 2, 1print('[:]:\\t',tup[:]) # 获取全部print('[::]:\\t', tup[::]) # 获取全部print('[1:]:\\t', tup[1:]) # 从索引1开始获取到最后print('[1:3]:\\t', tup[1:3]) # 从索引1开始索引到3之前print('[:3]:\\t', tup[:3]) # 从0开始索引到3之前print('[1:5:2]:\\t', tup[1:5:2]) # 从1开始索引到5之前，步进值为2print('[::2]:\\t', tup[::2]) #从0开始索引到最后，步进值为2print('[5:1:-1]:\\t', tup[5:1:-1]) # 从5开始往前索引到1， 步进值为-1。---[:]: (1, 2, 3, 4, 5, 5, 4, 3, 2, 1)[::]: (1, 2, 3, 4, 5, 5, 4, 3, 2, 1)[1:]: (2, 3, 4, 5, 5, 4, 3, 2, 1)[1:3]: (2, 3)[:3]: (1, 2, 3)[1:5:2]: (2, 4)[::2]: (1, 3, 5, 4, 2)[5:1:-1]: (5, 5, 4, 3) 除了常用的切片操作之外，和列表一样，元组也能使用一些基本函数来完成查询操作 12345678910111213# 获取元组的长度print(len(tup))# 统计一个元素在元组中出现的次数print(tup.count(5))# 获取一个元素在元组内的下标（索引值）print(tup.index(5, 1, 9))---1024 除此之外，元组还可以引用基础的数学运算符+和*来进行加和乘的运算。 1234567# 加和乘操作print((1, 2, 3) + ('a', 'b'))print((1, 2, 3) * 5)---(1, 2, 3, 'a', 'b')(1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3) 有一个同学曾经问过我：既然元组是不可修改的，那为什么还能用加和乘的运算呢？ 不知道在座的小伙伴有没有这种想法？ 这样吧，我重新写一段代码，小伙伴们应该就明白了: 123456789101112# 元组是这样的tup = (1, 2, 3, 4)print(id(tup))tup2 = (3, 4, 5, 6)tup = tup + tup2print(id(tup))---4446580864445062912044506479844459297216 不知道大家看明白没有。解释一下，其实就是说，在进行加法和乘法运算的时候，即便我们的变量名是一样的，实际上也是生成了一个新的元组，而不是之前那一个了。所以这个并非是修改和更新，而是创建。 为了对比，我再写一段更新的代码给大家看： 12345678# 尝试更新元组tup = (1, 2, 3, 4, 5, 6)print(tup[2])del tup[2]print(tup)---TypeError: 'tuple' object doesn't support item deletion 可以看到，报错提示了，tuple对象不支持删除项目 作为对比，我们看看列表的： 123456789# 看看列表更新（只看id）items = [1, 2, 3, 4, 5, 6]print(id(items))del items[2]print(id(items), '\\t',items)---44504159364450415936 [1, 2, 4, 5, 6] 可以看到，不仅是内部元素被删除了，并且id完全没有变化。也就是说，我们是在这个列表本身做了删除动作，并未生成新的列表。关于这部分，我们上一节中的深拷贝和浅拷贝讲的很清楚，大家可以回去好好看看理解一下。 元组推导式 生成器 在起初，我们先来看看元组是否和列表一样支持使用推导式。 12345tup = (i for i in range(10))print(tup)---&lt;generator object &lt;genexpr&gt; at 0x109cfa570&gt; 这段并非是报错，而是打印出了tup的类型：生成器对象。 我们之前学过，使用列表推导式生成的结果是一个列表，但是元组似乎和列表并不一样，生成的结果是一个生成器对象。 12列表推导式 ==&gt; [变量运算 for i in 容器] ==&gt; 结果 是一个 列表元组推导式 ==&gt; (变量运算 for i in 容器) ==&gt; 结果 是一个 生成器 那这里就有个疑问了，什么是生成器？ 生成器是一个特殊的迭代器，生成器可以自定义，也可以使用元组推导式去定义。 生成器是按照某种算法去推算下一个数据或结果，只需要往内存中存储一个生成器，节约内存消耗，提升性能。 语法 里面是推导式，外面是一个()的结果就是一个生成器 自定义生成器，含有yield关键字的函数就是生成器 那么，我们到底应该怎样操作生成器呢？ 既然生成器是迭代器的一种，那我们是否可以使用迭代器的操作方法来操作生成器呢？ 说干就干，让我们直接操作做实验： 1234567891011tup = (i for i in range(10))print(next(tup))print(next(tup))print(next(tup))print(list(tup))---012[3, 4, 5, 6, 7, 8, 9] 没毛病，确实支持next()函数，并且内部元素在使用后也被移除了。 12345# 让我们将其转为元组print(tuple(tup))---() 哎，为什么里面是空的？那是因为，我们上一段代码中的最后一句，已经讲所有迭代器内的元素转为了列表，素衣目前迭代器tup内是没有任何元素了，所以我们转过来必须是空的。 再来生成一个，我们来试试用for对它进行循环： 123456tup = (i for i in range(10))for i in tup: print(i, end=&quot; &quot;) ---0 1 2 3 4 5 6 7 8 9 可以看到结果没有问题。可以推断出，生成器和迭代器没有任何区别，我们在平时使用的时候，就直接将它作为迭代器使用就可以了。 yield关键字 在之前，我们提到了，含有yield关键字的函数就是生成器。 它返回的结果是一个迭代器。我们可以理解为，生成器函数就是一个返回迭代器的函数。 那么yield有哪些需要注意的点呢？我们先在下面列一下，之后再带着大家一起过： yield和函数中的return有点像 共同点： 执行到这个关键字后会把结果返回来 不同点： return会把结果返回，并结束当前函数的调用 yield会返回结果，并记住当前代码执行的位置，下一次调用时会从上一次离开的位置继续向下执行。 上实验： 123456789101112# 定义一个普通函数def func(): print('Hello yield') return 'yield' print('Hello again')func()func()---Hello yieldHello yield 在这个自定义函数内，return执行的时候，就会结束当前函数的调用，而在之前，第一个print()函数正确执行了，但是第二个print()函数因为在return之后，所以并未运行。即便我们一共执行了两次函数，可是也仅仅是讲第一个print()函数执行了两次。 123456789101112131415# 尝试使用yield定义一个生成器函数def func(): print('Hello yield') yield 'yield' print('Hello again') yield 'again'# 调用生成器函数， 返回一个迭代器res = func()next(res)next(res)---Hello yieldHello again 可以看到，当我们使用next()函数的时候，迭代器起作用了，每执行一次，分别调用第一个yield之前和之后的print()，也就是说继续执行了。 那如何验证yield的返回呢？我们将这段代码改造一下： 12345678910111213141516171819# 尝试使用yield定义一个生成器函数def func(): print('Hello yield') yield 'return yield' print('Hello again') yield 'return again'# 调用生成器函数， 返回一个迭代器res = func()str = next(res)print(str)str = next(res)print(str)---Hello yieldreturn yieldHello againreturn again 没问题，依次打印出了返回值return yield和return again。 还记得我们之前教过，使用list函数去调用，可以讲迭代器的返回结果，作为容器的元素，让我们再来改造一下这段代码： 1234567891011121314151617# 尝试使用yield定义一个生成器函数def func(): print('Hello yield') yield 'return yield' print('Hello again') yield 'return again'# 调用生成器函数， 返回一个迭代器res = func()items = list(res)print(items)---Hello yieldHello again['return yield', 'return again'] 我们看见，确实，返回结果被依次放入了一个list容器中。 当然，除了list函数之外，还可以使用for来获取迭代器内容： 1234567891011121314151617181920# 尝试使用yield定义一个生成器函数def func(): print('Hello yield') yield 'return yield' print('Hello again') yield 'return again'# 调用生成器函数， 返回一个迭代器res = func()items = []for i in res: items.append(i)print(items)---Hello yieldHello again['return yield', 'return again'] 我们来分析一下在以上这几段代码中，生成器函数调用时到底是什么过程。 首先，调用生成器函数，返回一个迭代器： 第一次去调用迭代器，走到当前的生成器函数中，遇到第一个yield, 把return yield返回，并且记住当前的执行状态（位置），暂停了执行，等待下一次的调用 第二次去调用迭代器，从上一次遇到的yield位置开始执行，遇到了第二个yield，把return again返回，并重新记录状态，暂停执行，等待下一次调用。 如果最后又调用了迭代器，那么会从上一次的yield位置开始，可是后面没有了，就会超出范围，抛出异常：StopIteration:。 那么这种一次一次调用执行的方式什么时候适用呢？比如说，我们在处理一个非常大的数据，电脑可能吃不住，这个时候我们就可以拆开来一次一次的执行获取结果。 小练习 为了能达到练习的目的，从这一节开始，所有练习可以不在课程中展示了。大家先做一下，然后可以在我下一节课中的源码中去找答案，然后来看看和自己做的是否一样。 以下所有练习必须使用列表推导式来实现 有些练习不止一个方法，大家尝试用多种方法来实现一下。 做完的小伙伴可以在课程后面留言讨论。 上一节的练习已经放到本次教程的源码内，可以在此获取：https://github.com/hivandu/AI_Cheats/tree/main/Python 1今天就一个练习：使用生成器改写斐波那契数列函数","link":"/Detailed-of-tuple/"}],"tags":[{"name":"产品经理","slug":"产品经理","link":"/tags/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"CV","slug":"CV","link":"/tags/CV/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"BI","slug":"BI","link":"/tags/BI/"},{"name":"LLM","slug":"LLM","link":"/tags/LLM/"},{"name":"Math","slug":"Math","link":"/tags/Math/"},{"name":"Neural Network","slug":"Neural-Network","link":"/tags/Neural-Network/"},{"name":"Chrome","slug":"Chrome","link":"/tags/Chrome/"},{"name":"ChatGPT","slug":"ChatGPT","link":"/tags/ChatGPT/"},{"name":"wordpress","slug":"wordpress","link":"/tags/wordpress/"},{"name":"Mac","slug":"Mac","link":"/tags/Mac/"},{"name":"Stable Diffusion","slug":"Stable-Diffusion","link":"/tags/Stable-Diffusion/"},{"name":"App","slug":"App","link":"/tags/App/"},{"name":"Google","slug":"Google","link":"/tags/Google/"},{"name":"javascript","slug":"javascript","link":"/tags/javascript/"},{"name":"Gmail","slug":"Gmail","link":"/tags/Gmail/"},{"name":"Photoshop","slug":"Photoshop","link":"/tags/Photoshop/"},{"name":"Model","slug":"Model","link":"/tags/Model/"},{"name":"python","slug":"python","link":"/tags/python/"}],"categories":[{"name":"从零开始接触人工智能大模型","slug":"从零开始接触人工智能大模型","link":"/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"AI秘籍","slug":"AI秘籍","link":"/categories/AI%E7%A7%98%E7%B1%8D/"},{"name":"Python","slug":"AI秘籍/Python","link":"/categories/AI%E7%A7%98%E7%B1%8D/Python/"}],"pages":[]}