{"posts":[{"title":"2023年薪酬最高的科技工作中产品经理赫然在列","text":"在最近一段时间内，互联网以及科技公司面临了很大的挑战，工作岗位迅速减少。这让我很大程度上看衰了互联网行业，并且认为目前只是开始，更大的裁员潮还没延伸到中小企业。让我们来看看数据： 就数据（美国的数据）来看，现实非常严峻，2023年迄今为止已有809家科技公司裁员211,400名员工（这个数据一直在变化，最新数据可以看这里），但是各组织仍然在快速招聘人才，以填补云技术、区块链和网络安全等新兴领域的滋味。 看出来了么？并不是互联网和技术行业不香了，而是技术正在进行一轮行业替换，岗位需求发生了大变化。虽然早几年前大家都知道这些岗位是日后的方向，可是这一下子发生了一个断层的变化，导致很大一部分人没有完成转变，科技公司也没进行缓慢的过渡，再加上整个大环境的经济压力，才导致了如今全世界范围内（中国并未逃脱）的大裁员。如果最近一连串的科技裁员让您感到惊慌，请放心，科技作为一个行业仍然健康发展。 而事实上，83% 的美国人力资源专业人士表示，在过去 12 个月里招聘候选人一直很困难，这导致了未来缺乏必要的技术专业人员的更大问题。 麦肯锡最近的一份报告发现，43%的组织目前面临技能缺口，而来自Korn Ferry的单独数据表明，到2030年可能会出现8500 万人短缺，导致同年潜在年收入总计损失 8.5 万亿美元。 好消息是，对于那些寻找新工作的人来说，机会很多。但是坏消息是，对于35岁以上的求职者，这依然是一个难以跨过去的坎，我们不得不承认一点，对于在国内的求职者们，35岁这个节点比在国外更加明显。（为我自己默哀。） 好了，说了这么多，还是要有点干货的。接下来咱们根据国外的相关数据，详细介绍五个薪酬最高的技术职位，数据来源于Payscale，可以在VentureBeat上找到数千个职位，就算国内的求职者们碰不到这些职业，我们依然可以从这些岗位的数据来分析一下目前最吃香的相关职位，为自己的转型做个有力的参考（以下内容都是基于美国当前数据所做的分析）。 1. 云计算解决方案架构师 预计2023年云计算应用将超过6000 亿美元，并将推动人工智能和Web3等新兴技术。 平均工资： 132,700 美元 如果您有云计算方面的经验，德勤正在招聘一名云解决方案架构师，负责核心业务运营（CBO）组合的工作，以帮助C-suite和项目负责人通过新兴和颠覆性技术改造他们的组织并加速任务执行。 此外，SAIC 正在聘请一名专门从事系统工程的云解决方案架构师，以协助确定技术解决方案，解决技术差距，如在其国家情报社区（NIC）业务部门、美国政府任务和信息技术部门内的蜂窝和云服务。 2. 产品经理（软件） 产品经理负责根据数据制定策略，其角色不断发展，因此是任何销售产品或服务的组织不可或缺的一部分。 由于我本身就是一名数据产品经理，这里我不得不发表一些感慨。不过我们还是得认清一个事实，产品经理的门槛在不断变高，不要认为PRD和原型就可以胜任了，我们从数据中可以看到，数据产品、策略产品以及安全相关的产品经理更容易赢得心仪的工作。 基于此，我觉得我还是的多写点数据产品经理的相关文章了。 平均工资： $102,866 如果您正在该领域寻找职位，西门子正在寻找一名高级产品经理来领导网络安全产品的产品策略的开发和执行，与保护和自动化产品/解决方案的产品管理人员密切合作，以确保无缝集成网络安全功能。 与此同时，苹果公司正在招聘一名新产品技术项目经理。在此职位中，您将需要建立矩阵管理并监督材料预测、规划、分析和报告、物流准备、预算、采购和配置管理活动。 3. 网络安全工程师 由于数据泄露和网络威胁仍然是一个大问题——网络犯罪预计每年增长 15%，到2025 年将达到每年 10.5 万亿美元——网络安全领域迫切需要拥有保护企业及其资产的技能和经验的专业人员来自恶意软件攻击。 平均工资： $99,887 国土安全部特别投资于网络安全，因此政府和军事承包商 Booz Allen Hamilton 正在美国各地招聘各种网络安全工程师职位，包括华盛顿、圣安东尼奥和埃尔塞贡多。 在这些职位上，您将需要提供国家和国际层面的网络安全解决方案。 4. 软件工程师 美国劳工统计局预测，从 2021 年到 2031 年，软件开发人员、质量保证分析师和测试人员的就业人数预计将增长 25%，新增 411,400 个就业岗位。 平均工资： $90,777 对于那些拥有丰富经验的人，诺斯罗普·格鲁曼公司正在招聘一名软件工程师/首席软件工程师，作为其企业范围数字化转型的一部分。在此职位上，您将支持工程应用和产品的生成，例如实验室电子战 (EW) 以及靶场训练和模拟系统。 经验丰富的 Aces Incorporated 也正在招聘一名软件工程师来应对美国政府最困难的挑战。 对于金融服务行业的职位，摩根大通银行正在招聘一名全栈首席软件工程师，以安全、稳定和可扩展的方式增强、构建和交付值得信赖的市场领先技术产品。 5. 区块链工程师 虽然大多数人认为区块链的唯一功能是加密货币，但该技术目前已应用于医疗保健、房地产、抵押贷款处理和游戏等各个领域，并且是一个正在增长的领域。 平均工资： 90,000美元 cyberThink Inc 正在寻找一名区块链工程师，带领技术开发人员和云工程师团队建立 AWS 区块链集成环境并管理数据接口和链码开发。 在西海岸，Third Republic 正在与一个开发团队合作，该团队为财富 500 强公司提供创新的软件开发解决方案，以聘请区块链开发人员。","link":"/2023%E5%B9%B4%E8%96%AA%E9%85%AC%E6%9C%80%E9%AB%98%E7%9A%84%E7%A7%91%E6%8A%80%E5%B7%A5%E4%BD%9C%E4%B8%AD%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E8%B5%AB%E7%84%B6%E5%9C%A8%E5%88%97/"},{"title":"纪念","text":"纪念 1234567891011121314&lt;math xmlns='http://www.w3.org/1998/Math/MathML'&gt; &lt;mn&gt;25&lt;/mn&gt; &lt;mo&gt; &amp;#x00D7;&lt;!--multiplication sign --&gt;&lt;/mo&gt; &lt;msup&gt; &lt;mrow&gt; &lt;mn&gt;2&lt;/mn&gt; &lt;/mrow&gt; &lt;mrow&gt; &lt;mn&gt;6&lt;/mn&gt; &lt;/mrow&gt; &lt;/msup&gt; &lt;mo&gt;=&lt;/mo&gt; &lt;mo&gt;?&lt;/mo&gt;&lt;/math&gt; 以上，为了纪念！","link":"/25X2de6cifang/"},{"title":"《AI秘籍》预告","text":"Hi, 大家好，我是茶桁，这里为自己做个广告，目前打算开始写一整个系列《AI秘籍》。 这一段时间内我写过一个系列《零基础学习大语言模型》（目前还没写完）。 说实话，这个系列其实原出处并不是我，严谨的说来，有涉嫌擦边“洗稿”的嫌疑，所以最后放弃了收费的想法，仅仅对一些模型，资源以及计算结果进行了补偿性的收费。不过在写这个系列的同时，我开始有了自己的一些想法，打算真正写一个属于自己的系列文章。 因为我的个人博客并没有付费阅读的功能，所以还在看平台。第一选择自然是我的微信订阅号，有想过发到少数派里，但是并不清楚少数派对我文章的审核会是什么结果，能成为专栏发出来不太有信心。 说说这个专栏本身，参照我几个自媒体平台的数据来看，Python的基础知识还是更受欢迎一点，我想大概也是更多基础不太好的小伙伴希望能入行吧。所以这次我准备从基础开始写起，总的来说分成以下几个大的篇章： 第一卷：Python 第二卷：核心知识 第三卷：核心能力培养 第四卷：NLP 第五卷：BI 第六卷：CV 第七卷：扩展 - 数学 第八卷：扩展 - 英语 大致的算了一下，可能这个系列会耗费比较长的时间和精力，也希望小伙伴们能多多支持。 在这里，放上我的公众号订阅方式： 最后，找到合适的订阅平台之后，本篇内容应该会有更新。","link":"/AI%20Cheats%20Trailer/"},{"title":"卡尔曼滤波器的非数学介绍","text":"如果你想查看的话，本文的代码可以在我的Github上查看。 卡尔曼滤波器非常巧妙。如果你从未听说过卡尔曼滤波器，那么一种非常直观（也可以说是还原）的思考方式就是将其视为一个漏斗，在这里你可以从多个嘈杂的信息源中获取信息，并将其浓缩为一个更精确的统计数据。 如果这一切听起来含糊不清，请不要担心。稍后，我们将把这句话剥离成一个更容易理解的例子，希望能进一步加深我们的直觉。要研究和推理卡尔曼滤波器，没有比数学更好的工具了。但同样，卡尔曼滤波器的基础数学具有挑战性，包含线性代数、概率论和微积分等内容。因此，并非所有人都能轻松掌握。这篇文章的目的就是希望为你提供一个易于理解的直观印象，或许能促使你自己深入研究这个问题。现在，让我们开始吧，同时牢记这一点：\"以下内容仅提供直觉，可能并不完整\"。 让我们先问一句：\"为什么卡尔曼滤波器是必要的？对于这个问题，一个简单而又故意模糊的答案是：现实生活并不完美。请看这个激励性的例子：想象一艘船在一个维度上行驶，从港口出发（x=0）并行驶一段距离。这艘船的发动机被设定为为船提供一个恒定的速度，例如 10 米/秒。 我们首先要问的问题是，在离开港口 2 秒钟后，船到底在哪里？很自然，你会说船离港口的距离是 210=20m，因为毕竟距离 = 速度 时间。在理想世界里，这的确是正确的，根本不需要卡尔曼滤波器。但在现实世界中，情况绝非如此简单。首先，可能没有足够的发动机能够产生足够的力，使每个时间点的速度始终保持在 10 米/秒。当然，你可能会在某些时候获得 10.00001 的速度，或在其他时候获得 9.99999 m/s 或介于两者之间的某个数字，但正如所说，99.99% 的完美终究还是不完美。其次，即使你说你确实拥有这样一个完美的发动机，但当你施加一个精确测量的力时，你也不可能获得预期的完美速度。波浪运动可能会让你的船稍微慢一点，或者风可能会让它加速，或者谁也不知道什么东西会以什么方式对它产生影响。 因此，仅仅通过测量你想要的位置，你永远无法确定你的船在哪里。 那么，我们是否注定永远无法真正知道自己的位置呢？不尽然！这就是传感器的用武之地。想象一下，你，水手，随身携带一个全球定位系统。这样，GPS 就能精确地告诉您在任何给定时刻的位置！事实上，你现在甚至不需要船的速度，因为无论船如何行驶，你的全球定位系统都能准确地告诉你所在的位置。问题解决了吗？就像我说的，不完全是。在现实中，传感器经常会出现错误，而且不可靠。也就是说，它们确实能告诉你你在哪里，但测量结果可能并不精确。因此，您的 GPS 可能会告诉您，3 秒钟后您距离港口 29.998 米或 30.002 米，甚至是距离港口 100 米，但这种可能性极低。此外，您也无法确保传感器永远不会出现故障。以 GPS 传感器为例。一旦你发现自己身处没有卫星覆盖的地区，它就会失灵。事实上，如果有一个传感器能保证永远不会离线，并能以任意的精确度测量出你想知道的信息，那就根本不需要卡尔曼滤波器了。 有了这些，我们现在就可以回答为什么需要卡尔曼滤波器了。而答案与我们之前已经确定的并没有什么不同。卡尔曼滤波器是一个漏斗，它能接收两个或更多不完美、不可靠的信息源，并对你想知道的信息做出更准确的估计。在这个例子中，卡尔曼滤波器会把你在任何时间的速度估计值和 GPS 估计值（如果有的话）作为输入，然后给出比这两个信息加起来更准确的估计值！事实上，如果你有更多的信息来源，比如雷达或声纳，甚至是你目前在水中看到的鱼的种类，理论上你可以将这些测量结果结合起来，从而对你的位置做出更准确的估计。 因此，现在的问题是，如果不使用这样的数学知识（摘自维基百科），我们如何理解卡尔曼滤波器的作用和原理？ 首先，我们假设船上没有一名乘客，而是有一千名乘客，每个人都有自己的 GPS 设备。现在，每位乘客都可以通过以下方式进行基于速度的估算，从而估算出自己的位置（进而估算出船的位置）： 123456from random import gaussdef new_position(last): velocity = 10 wind = gauss(0, 2) wave = gauss(0, 0.1) return last + velocity + wind + wave 注：有关高斯函数的可选但更完整的解释，请参阅下面的附录。目前，只需说明它会产生一个随机数（正数/负数），其顺序由第二个参数指示即可。 从本质上讲，这 1000 名乘客中的每一个人都是这样做的：取上一次已知的位置（在现在之前的时间），加上速度，并且知道风和水波会轻微地改变航向，再加上一些随机的估计波动。现在，如果这些乘客真的有估算风速和水速的好方法，他们就会使用它。但因为他们没有，所以只能用随机数来估计影响。实际上，现实生活中也是如此。我们不可能测量所有的东西，所以我们只能用一些简单的方法来估计它们，就像我们上面用平均值（0）和偏差参数（0.1 和 2）所做的那样。 现在我们进入卡尔曼滤波法的第二阶段，即测量。在这一阶段，所有乘客都知道，由于风噪和水噪的影响，他们对自己的状态（所处位置）只有不完全的了解，因此，他们会利用自己的传感器来改善自己的状态： 1234567891011def sensor(t): if t == 3: # oops, passing through a thunderstorm. GPS fluctuating! sensor_noise = gauss(5, 10) elif t == 6: # uh-oh, satellite unavailable! sensor_noise = gauss(-5, 10) else: sensor_noise = gauss(0, 1) return true_position[t] + sensor_noise 请记住，传感器是一种不精确的设备，也就是说，它们返回的统计数据大多是正确的，在本例中就是变量 true_position，但它们本身也有噪声，我们再次使用高斯函数随机生成的数字来模拟这种噪声。此外，我们在这里还模拟了传感器的不稳定性，即在某些情况下（t=3 和 t=6），由于某些因素传感器基本上是不可用的，而这些因素并非完全不可想象。因此，每位乘客在使用传感器时，实际上都会得到不同的测量结果。 想象一下，这艘船现在离开港口，每秒行驶这些距离： 12true_position = [0, 9, 19.2, 28, 38.1, 48.5, 57.2, 66.2, 77.5, 85, 95.2] 也就是说，船从港口出发（x=0），第一秒行驶 9 米，第二秒行驶 10.2 米，最后到达 19.2 米，以此类推。现在，乘客们的任务是利用他们所掌握的嘈杂且不可靠的测量数据，尽可能准确地预测出每一秒的不同位置。 因此，在时间 t = 1 时，乘客可以通过上述函数得到这些读数： 1# 如果 t=0 时的新位置为 0，则 t=1 时的新位置为 0new_position(0) =&gt; 9.37 (error -0.37)# t = 1s 时的传感器读数sensor(1) =&gt; 8.98 (error +0.02) 所有乘客都是如此。现在的问题是，真相到底是什么？是我们的牛顿物理知识更可靠，还是 GPS 传感器更可靠？在这种特殊情况下，由于我们已经知道船的真实位置距离 true_position 变量 9 米，答案可能是显而易见的，但情况并非总是如此。在这种情况下，为了将这两个独立的统计数据结合起来，我们实际上采用了一种非常简单的方法：取两者的平均值！在上面的例子中，我们可以得出以下结果 1combined =&gt; (9.37+8.98)/2 =&gt; 9.17 (error -0.17) 请注意，在这个例子中，综合统计量的误差比单独的速度估计值要小，但比传感器估计值要差。但问题是，我们实际上可以做得比取平均值更好。考虑一下这样的情况：你知道你的传感器实际上是最先进的，而且非常可靠。这实际上意味着你应该更倾向于传感器的数据，而不是速度更新的数据。实际上，您可以通过使用加权平均值来做到这一点。请看这段代码 123def combine(A, B, trustA, trustB): total_trust = trustA + trustB return (A * trustA + B * trustB) / total_trust 这就综合了 A 和 B 来源的两个数字，但也考虑到了您对这些来源的信任程度。因此，如果您将其称为 12combine(9.37, 8.98, 10, 1) =&gt; 9.33 (error -0.33)combine(9.37, 8.98, 1, 10) =&gt; 9.01 (error -0.01) 在第一次调用中，您对源 A（速度）的信任度远高于源 B（传感器），即 10 比 1，因此得到的答案更倾向于源 A，即更接近 9.37。这种基于信任的加权平均法是卡尔曼滤波器的核心，也是它的数据组合能力所在。 但现在，我们遇到了一个新问题。哪个来源更可信，或者如何计算可信度？是应该优先考虑速度呢？还是应该优先考虑 GPS 测量结果？决定这一点的是偏差或方差指标。想想看，什么更值得信赖？是波动剧烈的信息源还是没有波动的信息源？试想一下，你收听 10 个气象广播电台，其中 4 个告诉你会下雨，6 个告诉你会是晴天。现在想象一下，你登录 10 个天气网站，其中 9 个告诉你会下雨，1 个告诉你会是晴天。哪个消息来源更可靠？你倾向于相信大多数气象广播电台告诉你的（晴天）？还是你倾向于相信天气网站告诉你的（下雨）？理性的做法是更倾向于网站的结论，因为许多网站的结论都是一致的，即它们的方差较小，而气象广播电台，至少在这个例子中，它们的结论似乎波动很大，所以也许不应该太相信。 这样，完整的更新步骤就变成了这样： 1234567891011121314151617181920212223242526272829303132from statistics import variance# Find updated positions per passenger at t secondsdef update(t, last): velocity_updates = [] sensor_updates = [] for p in range(1000): # for each passenger # new velocity update based on last known position # for the passenger velocity_updates.append(new_position(last[p])) sensor_updates.append(sensor(t)) # Calculate trust metrics for velocity and sensor measurements # Remember that as fluctuation increases, trust decreases # And vice-versa fluctuation_velocity = variance(velocity_updates) fluctuation_sensor = variance(sensor_updates) # calculate trust trust_velocity = 1/fluctuation_velocity trust_sensor = 1/fluctuation_sensor # combine these together for each passenger combined = [] for p in range(1000): combined.append(combine(A = velocity_updates[p], B = sensor_updates[p], trustA = trust_velocity, trustB = trust_sensor)) # Sensor updates &amp; velocity updates returned for plotting purposes return sensor_updates, velocity_updates, combined 注：有关方差函数的更多信息，请参阅附录。现在，只需将其视为数字列表波动的度量。 这段代码相对简单。对于每位乘客，它都会进行基于速度的噪声测量和基于传感器的噪声测量。根据所有乘客的这些测量结果，计算出每个测量结果的信任度指标，作为方差的倒数（因为方差增加，信任度降低），然后调用包含相关信任度参数的组合方法。值得注意的是，这里的每位乘客都在为自己进行位置更新。在这些单个更新结束后，可以根据所有乘客位置的平均值推断出船只本身的实际位置。 我们使用以下代码来连接上述整个代码。 123456789101112131415161718192021# We'll do a final plot using this listplot_data = []def update_plot(t, sensor, velocity, combined_position): # add true position at this time plot_data.append({'passenger': 'true', 'type': 'true', 'time': t, 'position': true_position[t]}) # for each passengers for p in range(1000): plot_data.append({'passenger': p, 'type': 'sensor', 'time': t, 'position': sensor[p]}) plot_data.append({'passenger': p, 'type': 'velocity', 'time': t, 'position': velocity[p]}) plot_data.append({'passenger': p, 'type': 'combined', 'time': t, 'position': combined_position[p]})update_plot(0, [0]*1000, [0]*1000, [0]*1000)estimated_positions = [0]*1000 # all estimates start from 0for t in range(1, 10): # ten seconds _sensor, _velocity, estimated_positions = update(t, estimated_positions) update_plot(t, _sensor, _velocity, estimated_positions) update_plot 函数只是做一些基本的簿记工作，以存储用于绘图的瞬时统计数据。这里的主要迭代只是最底层的 for 循环，它使用乘客当前的最佳估计值，在任何给定时间持续更新位置估计值。除此以外，代码基本上不言自明。 使用 seaborn 库绘制的结果如下： 由于目前的比例尺，这有点难以解析。让我们放大这两个区域，特别是 t=0.75 至 t=1，即传感器正常工作时，以及 t=2 至 t=4 出现故障时。 注：包络线指的是不确定性。线中的包络线越宽，我们对数字的不确定性就越大。 在第一种情况下，正如您所看到的，所有 1000 名乘客的综合位置估计值比单独的速度估计值要好（绿色），虽然在第一种情况下，我们的估计值确实比我们的传感器读数要差，但在第二种情况下，我们的估计值实际上比单独的故障传感器读数要好得多！这是因为卡尔曼滤波器会自动调整不可预见的波动造成的剧烈变化，并始终为我们提供合理可靠的指标。如下图所示，一旦我们的传感器恢复正常（t=4 到 t=5），卡尔曼滤波器就会再次偏向于传感器（由于传感器读数和真实值重叠太多，所以有点难看）。 我相信你至少对卡尔曼滤波器的工作原理有了一些直观的了解。卡尔曼滤波器的实际理论基础同样引人入胜，如果你的工作需要，我鼓励你继续深入研究。与此同时，我希望这篇文章能证明，代码作为一种形式语言，能在多大程度上帮助人们对那些乍看之下令人生畏的概念产生直觉。我也希望能够通过简单的代码，向大家传授一些我认为很有吸引力的话题的更多见解。 高斯函数 这里唯一需要知道的特殊函数是正态分布函数，即 gauss(0, 0.1) 和 gauss(0,2)。简单地说，它给你一个随机数，这个数通常在 0 附近（技术上正确的说法是以 0 为中心），而得到离 0 更远的数的几率由第二个参数控制，即 2 和 0.1。 因此，如果调用 gauss(0,0.1)，得到 0.06、-0.07、-0.06、0.02、-0.23、-0.06、0.09 等数字的可能性较大，顺序不分先后。 而如果调用 gauss(0,2)，则更有可能得到 1.05、1.03、-1.06、0.32、1.29、-0.40、-1.72 等数字，同样不分先后。 直观地说，第二个参数也叫标准偏差，控制着测量值的波动程度。在上面的代码中，这意味着您通常会认为风的偏差过大（大风天？请注意下面直方图中偏差=2 和偏差=0.1 所产生的数字的频率（特别注意 x 轴）。虽然数字的范围有很大不同，但这两个直方图的形状看起来差不多。这种钟形分布被称为高斯分布、正态分布或钟形曲线分布，在自然界中经常出现。 方差 方差是衡量一致性的标准。也就是说，如果一致性好，方差就小，反之亦然。在上图中，由于 x 轴实际上是自动调整的，所以你无法完全看到方差。如果我们在相同的坐标轴限制内绘制上图中的直方图，就会得到如下结果： 注意到第一张图片有多宽了吗？这是因为其中的数字变化很大。也就是说，你会发现其中有很多 -2、2、0 和一些 4、-4。但在第二幅图中，你会发现很多 0、0.1、-0.1 等，但你会发现-2、2 等的数量会少很多。正确地说，第一个分布的方差（准确地说是 4）大于第二个分布的方差（0.01）。有关方差的更多信息，请上网查阅。","link":"/2023_8_3_Kalman/"},{"title":"新专辑《AI秘籍》，你所感兴趣的一切","text":"Hi，大家好。我时茶桁。 最近，我花了几天时间仔细思考了一下即将要开始写的专栏《AI秘籍》，再根据自己的能力大概规划了一下。目前大致已经理出了一些相关信息可以分享给大家。 专栏形式 本次专栏应该会以文章的形式先和大家见面，后续还会根据能力以原本的文章为准录制视频版本。 专栏平台 就如前一篇文章公布的内容一致，会优先发表在我的公众号上，当然目前我还在努力寻找其他的专栏平台。 我的预想是尽量能够让大家一篇一篇的购买，不需要必须购买全部专辑，这样朋友们可以根据自己具体需求来进行购买。 而目前我的百家号收费专栏也在申请之中，不知道会不会顺利申请下来。有新的平台入驻之后，我会进行通知的。 专栏内容 在规划专栏的时候，大部分时间基本都放在了规划内容上。包括目录的编排，内容取舍等等。 目前规划中的专栏打算从基础开始，到Python开发，再到一些应用基础，比如AI数学，AI英语等。而由于这些内容都会是针对AI学习的，所以并不会是那种很全面的学习资料。 比如说Python，我们不会讲的很系统，重点会放在数据结构以及数据分析和开发方面。数学等基础当然也会是一致的。 当基础篇学完之后，接下来就是重点了，会根据三个不同的AI方向来进行讲解，分别包括：BI、NLP以及CV。 基本目录如下： 第一篇： Python基础（AI方向） 第二篇：核心基础能力 第三篇：核心知识增强 第四篇：BI 基础 第五篇：CV 基础 第六篇：NLP 基础 第七篇：BI 进阶 第八篇：CV 进阶 第九篇：NLP 进阶 拓展篇1: 数学 拓展篇2: 英语 详细目录如下（进阶部分目录未完全展开）： 一些说明和后续 本专栏暂时价格上还未进行调研，反正第一篇Python部分应该会是全免费发放。毕竟Python课程网上太多了，而且同质化严重，收费感觉没太大必要。所以，咱们写的时候再慢慢想。小伙伴们也可以留言来说说大家期望是一个什么价格，我根据大家留言再结合自己的实际情况最后定价。 另外，除了Python部分之外，数学和英语部分也是免费的。说实话，我数学和英语并不是很好，这两部分我仅仅给大家一个总结和方向，反正也是独立内容，均可以去网上找相关替代的。 专栏在完成之后，会更新一些关于算法和数据库的内容，然后会考虑整篇投放到其他平台去进行完整售卖。 结尾 好了，结束语也无需说太多。让我们一起期待吧，希望在我的课程完成的那一天，各位小伙伴们能完全入门并掌握人工智能。 本次课程的所有代码都会上传到Github上，地址：","link":"/AI-cheats-information/"},{"title":"12 AI帮你写个小插件，轻松处理Excel文件","text":"开头我就要跟各位先说对不起，本来我是很想为大家把这部分实现并完成的。但是很抱歉，因为我用的Mac，而这部分代码实现起来的时候一直会如下报错： 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/AI-create-a-excel-plugin/"},{"title":"Analysis data and research report collection","text":"The purpose is to facilitate finding specific locations when doing data analysis by yourself 1. 国内咨询机构网站数据报告列表 艾瑞研究-艾瑞网 互联网行业报告 艾瑞APP指数 移动App TOP 1000 月度活跃和日活跃 艾瑞PC指数 PC TOP 1000 月度活跃和日活跃 199IT互联网数据中心 中国互联网络信息中心 数据新知 - 易观 【友盟+】数据报告 http://www.dcci.com.cn/report/index.html 北京赛诺市场研究有限责任公司 赛诺数据，智能机出货量的专业统计 数据报告-移动观象台-TalkingData Talkingdata报告 艾媒网-全球移动互联网行业数据发布平台/iiMedia Research出品 DataEye大数据移动营销综合服务商-数据报告 手游方向 ASO100 - 专业App Store数据平台丨ASO优化专家平台丨iOS榜单排行榜查询工具 电影电视行业免费报告列表页 研究娱乐行业 旅游数据报告-旅游圈旅游行业报告 小程序报告-阿拉丁统计 爱应用：一个应用所有历史版本的产品分析截图记录 Appannie，国外下载应用 卡思数据-短视频网红分析数据分析 国金证券研究所 国家宏观经济研究数据和报告 中金研报 抖音快手的热门视频和kol的 各大媒体的每天的热门排行榜 短视频行业的数据 招商证券的电商类报告电商类的行业观察，企业研究，品牌深度报告 短视频和图文内容类的行业报告-新榜 http://www.100ec.cn/zt/wmds/ 涵盖跨境电商所有报告，行业数据和研报 镝数聚-权威数据 海量聚合 提取了报告中的数据，颗粒度比较细 东方财富研报首页 东方财富研报 2. 国家机构公开数据 中国信通院-研究成果-权威发布-权威数据 中国城市轨道交通协会城市地铁线路的流量数据 国家的便民服务查询（包括5A景区list，小微企业名录，法人信用查询，出租车信息查询） 国家宏观经济数据（GDP,CPI，总人口，社会消费品零售总额，粮食产品，PPI，各地区行政规划，各地财政收支等等，分月度季度和年度）部分数据如下 国家统计局（数据多到瞠目结舌，包括年度，季度，国家，国际，年鉴，介乎涵盖所有数据指标和历史）部分数据举例 世界银行的公开数据库（有健康，农业，公共部部门，人口分布，外债，性别，教育，环境，气候变化，能源，贫困等各种公开数据） 世界数据图册（世界和地区统计资料，各国数据，地图，排名）包含的全球的国家公开的数据 国家机关部委的公开数据（包括国家发展和改革委员会，教育部，民政部，司法部，财政部，工业和信息化不，交通运输部，文化和旅游部等） 各城市开放数据（包括浙江数据开放网，青岛数据开放网，贵阳数据开放平台，成都数据公开平台，合肥数据开放平台，河南开放数据平台等） 宏观经济查询数据（包括高校财经信息库，人民网经济数据库，香港统计处，联合国统计司，世界经合组织，欧盟统计局，国际货币基金组织等） 房价数据（包括中国房价指数，房价走势，台湾房价行情，北京房价查询，深圳楼盘成交查询等，上海地铁房价，贝壳指数等） 汽车数据（包括中国汽车工业协会数据，百度汽车网，易车汽车指数，汽车渠道直通车，中国汽车流通协会数据中心，德国汽车工业协会等） 权威发布 | 中华全国商业信息中心 3. 国内互联网公司数据报告网站列表 讲座PPT-腾讯大讲堂 Tencent 腾讯-业绩报告 腾讯大数据-腾讯云数据分析出来的行业报告 百度开放服务平台-百度云数据分析出来的行业报告 百度数据研究中心 提供行业研究报告、行业分析报告-百度数据中心报告 首页-阿里研究院-阿里行业研究报告 企鹅智酷_腾讯网-腾讯出品行业报告 腾讯CDC -腾讯交互设计报告 百度用户体验中心-百度UED用户研究报告 网易用户体验设计中心-网易UED用户研究报告 网络视频数据报告-优酷指数行业报告 PP指数_PPTV聚力-PPTV指数行业报告 360研究报告_360安全中心-360应用商店等产品出品报告 4. 国外咨询机构网站数据报告列表 国外咨询机构较多，数据详实，无论是海外出海产品，海外报告中多有亚洲和中国的重点研究，相关报告和趋势分析都可以选看 Flurry-国外app行业报告 App Annie Blog-app指数报告 https://www.appannie.com/insights/ (Appnnie的行业包括，包括app 分发行业的分发量和收入） BI Intelligence-business insider的报告 Today's Articles on Digital Marketing and Media-emarker的报告 http://www.newzoo.com/category/press-releases/-newzoo侧重于手游行业报告 Gartner Press Release Archives-gartner侧重于硬件的出货量，包括智能机和PC等 IDC - Search Results-IDC的硬件出货量全球报告 Yozzo Telecom News J.P. Morgan Home-摩根投行报告 德勤中国 | 审计, 企业管理咨询, 财务咨询, 风险管理, 税务服务及行业洞察 Precisely Everywhere-comscore的互联网行业报告 Ericsson - A world of communication（Global移动行业报告） GamesIndustry.biz（Global游戏行业报告） http://adfonic.com/（Global广告行业报告） Canalys | Insight. Innovation. Impact.（Global智能机报告） Mobile, Online &amp; Digital Market Research, Data &amp; Consultancy（通信无线报告） Home | GfK Global（终端比较专业的报告） Kantar Worldpanel（主要统计Android和ios的市场份额） PwC publications（皮尤的所有用户，市场研究报告） Fiksu | Data-fueled mobile marketing（统计app用户获取成本和应用商店下载频次的监测） https://www.weforum.org/reports（世界经济论坛的报告，揭示国内外发展的大趋势） Insights - Jampp （Jampp是国外的app 的粘性和转化漏洞的网站，在insights里还有行业的app的retention等benchmark的数据，有些类似flurry的行业数据） 罗兰贝格行业评论 战略和行业评论和报告 普华永道:blog 各个行业的主要发现和行业报告 Website Traffic &amp; Mobile App Analytics （similar web 以色列的网站分析工具，可以分析任何网站，包括用户，来源，终端，分布等等，数据非常棒） CADAS（全球航空公司研究报告）：非常支持和专业 印度互联网年报 - 竺帆 | 助力中企扬帆天竺 （印度出海报告，非常详细） GSMA： 全球移动互联网经济分析报告，全部免费下载报告和数据，从2015年到现在 商业价值研究院 -IBM（行业观点报告比较多） Home - McKinsey Greater China麦肯锡 **BCG - 波士顿咨询公司波士顿 企业管理咨询公司罗兰贝格 Accenture - China埃森哲** 5. 各大公司不定期发布的报告，比如（细分方向的时候用）** 高德地图：2015年度中国主要城市交通分析报告 微信城市服务发布《2015微信政务民生白皮书》 【报告】淘宝发布 2015 中国消费趋势数据，2015 年我们为什么买单？ 互联网增长的第一本数据分析手册-Growing IO的公开手册 移动游戏运营数据分析指标白皮书（一）-Talkingdata 运营指标分析白皮书 多多大学 （多多大学也分享了很多的拼多多运营数据还提供课程，可以看） 6. 找行业内的人事打听内部一手资料 关注一些专门打听行业内部人事的信息来源 这里先推荐一家公共号：晚点LatePost（微信搜索公共号可以 关注） 这家主要是会 看一些行业内部和重要的消息 在行上约人。在行 App 如果想知道一些企业的信息，可以在在行上找到一些行家，曾经一手经营或者运营过祥光额项目和参与过竞品和行业公司的操盘，可以约出来，从信息和方法论角度获得资讯 7. 企业信息报告** 新三板在线 - 中国最大的新三板生态平台（各行各业的新三板上市公司财务数据，高管数据等） 企查查|企业查询（查询企业的产品，品牌和法人信息） 企业注册信息查询（天眼查，同企查查） SEC.gov | Home（美国上市公司年度财务报告） 巨潮资讯网—（中国上市公司季度年度财务报告） Baidu | Investors（各大上市公司季度财报，IR.XX公司.com，比如百度这个） 天眼查（可以查到各个企业的详细信息，还可以查到员工个数） 8. 爬虫网站或者APP的数据 最近研究发现，还有一个好的行业信息获取来源，就是通过站内或者App内的爬虫抓取，这个渠道获取的数据，通常可以帮助你了解行业和竞品的站内使用情况，用户喜欢的内容，用户的分布，用户的行为和喜好等等。 爬虫，简单来说是通过程序来获取网页的信息，整理成数据库，从而进行数据挖掘的得到分析结论的过程。比如你可以爬虫购物的页面，知道哪个商品的销量好，比如你可以爬虫小红书的页面，知道哪些kol收到欢迎，你还可以爬取他们的分类，知道美妆和购物的kol表现好，并且有多少个这样的kol。如果你没有对方的数据库权限（当然你肯定没有），那么从外部爬虫是最好的了解他们业务数据的方式。 通常的搜索方式 是：你要了解的网站/App+爬虫，在搜索平台比如百度搜索 这里举例一些程序员垂直的网站， CSDN网站：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索 简书 - 创作你的创作：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索， V2EX：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索 掘金：在这个网站内搜索：网站/app +爬虫 这个关键词，在站内搜索 9 . 业内微信群 现在发现很多好的内部报告和难以获得报告，是通过加入一些干货群，内部群来获得的。 比如做直播电商的人自己比较关注一些直播和电商带货的详细的数据和报告趋势，大家会自己组建一些干货群，只要是市面上有的报告，自己内部发现的都会往里面扔。 这个是淘宝直播的负责人赵圆圆离开淘宝后创业，同时聚集的几个群，里面关于直播的干货非常多。 其他的关于投资的，趋势，创业的类似群也很多，获取报告也很一手，大家也可以自己开发下这样的群组织。 10. 搜索引擎 搜索引擎还是可以搜到很多你个性化想要找的报告和趋势。以前我没觉得搜索引擎很很难，后来发现也需要学习和熟练使用，才能让其为自己所用。 如何使用搜索引擎 11. 各大公司的财报 通常对于上市公司来说，财报信息包含的内容是最全面的，关于用户，商业，渠道，增长，业务策略等等。所以如果想了解一个公司，如果是上市公司最好第一手先看财报后者SEC（上市报告）。 很多同学问我财报哪里找，不知道怎么看。其实每个公司都有自己的IR（投资者页面），在上面有财报的完整的pdf下载。另外，也推荐大家听听每期的企业conference call（回答财报问题），可以听下CEO对财报的解读。 这里我列举几个大公司财报的网站 wind：金融数据库，包含财报和行业信息（wind的账号可以到闲鱼租） 百度财报 PDF：Baidu | Investors | Home 百度财报解读podcast：音频可以在线听 阿里财报pdf：阿里巴巴集团 腾讯财报pdf：Tencent 腾讯 - 投资者关系 搜狐财报pdf：http://investors.sohu.com/ 拼多多pdf：Investor Relations | Pinduoduo Inc. 拼多多财报解读：音频可以在线听。 如果大家有自己想要了解的公司，在百度or google搜索：公司名字+IR ，可以 定位到他们公司的财报网站页面。在页面上找到conference call或者webcast，可以 找到他们的财报解读音频。 12. 投资机构的统计网站（创业方向选择，投融资选择的时候用） IT桔子 | IT互联网公司产品数据库及商业信息服务（IT桔子，中国创业公司投融资数据和报告） 研究院_ChinaVenture投资中国网-（投中的每个季度的行业融资报告，不定期有专项分析报告） CB Insights - Blog （CBI insights的一系列产品，包括公司的估值，独角兽公司列表等） The Downround Tracker（公司估值下降的趋势） The Complete List of Unicorn Companies（独角兽公司列表） IPO Center: IPO Market, IPO News, IPO Calendars, IPO Pricings, IPO Voting（IPO相关新闻和趋势报告） PrivCo | Private Company Financial Intelligence（美国金融数据公司，主要关注未上市公司的所有投融资资料，目前涵盖的公司包括全世界，当然也包括中国公司） 券商行业研究报告 （国内券商的行业报告，策略报告，可以筛选行业，筛选报告类型） https://pitchbook.com/news/reports（PitchBook的PE,VC，M&amp;A行业报告） 研究院_ChinaVenture投资中国网 （IPO 投融资行业报告） Dow Jones VentureSource 2Q’16 U.S. Venture Capital Report（道琼斯旗下机构Dow Jones LP Source行业投资报告） NVCA Venture Investment（美国国家风险投资协会，每个季度和年度都会出投融资行业报告） PWC-MoneyTree Home（PWC的money tree report是每个季度美国的风险投资行业报告） https://home.kpmg.com/xx/en/home/insights.html （KPMG毕马威的insights报告，一般是每个季度的创投趋势，比较细致的分析） Mattermark - Discover, Enrich, &amp; Analyze Companies（创业公司投资并购信息一站式搜索） M&amp;A, Private Equity &amp; Venture Capital Database（创业公司投资并购信息一站式搜索） DataFox | Prospect Sales Leads with Company Signals（创业公司投资并购信息一站式搜索） CrunchBase accelerates innovation by bringing together data on companies and the people behind them.（创业公司数据库） Venture Intelligence PE/VC database Stock Market Insights | Seeking Alpha （二级市场金融分析网站） Tencent Holdings Ltd -- Trefis（各个公司的revenue model的预测和key driver的趋势，这个网站简直不能再棒） 13. 本地数据库 这个世界有很多有用的信息，搜索引擎只解决了其中20%，其他80%的信息再各个角落，包括微信群，包括口口，甚至包括直播里都有，但是都不在搜索引擎。 就搜索引擎而言，现在很多人只是使用了其中的5%还不到。搜索引擎的技巧可以提升，但是其他80%的信息获取渠道更为隐蔽和无法公开获得的。 我加了很多 群，里面都是这些报告和信息和各行各业的各种信息，这些是搜索引擎提供不了的 这些冰山下的信息才决定了信息的获取的不同和优质与否。 除了上述渠道外，能找到靠谱渠道，找到合适的报告随时存储起来，等用的时候随手打开用是最好的。分享一个我最近看的收藏的精品的报告收藏夹，也希望对你们有用（随时更新） 共享下我看过的精品报告的收藏夹list 14. 怎么提炼自己获取信息的层次和获取信息的价值 找到行业信息报告知识获取信息只是其中一个层次 ，获取信息是否更有价值更直接可用，在于基本功行业信息报告的甄别和获取，积累和提炼，这是非常重要的。 但是 越往上走，越是接近信息更有价值，更新鲜，更真实有效，更直接，有大量的渠道 可以 获得更多 的信息，这些不仅是通过 行业报告获取的，还有包括自己可以控制的方法，包括爬虫，数据挖掘，信息技术 等，还包括人脉，圈子，内幕的等渠道。大家感兴趣可以到这个答案看下详情，我对每个层级的方法的解读。 哪些渠道可以获取一般人不知道的知识和信息 15. 其他（不定期更新） IBM商业价值研究院","link":"/Analysis_data_and_research_report_collection/"},{"title":"Apple M1的AI环境搭建","text":"本文环境搭建的基础是Python3.9， 因为M1为ARM架构，所以放弃了Anaconda，使用Miniforge3。包括Tensorflow, xgboost, Lightgbm, Numpy, Pandas, Matplotlib, NGBoost等。当然，因为是Python3.9， 所以有些库实在是无法使用。 Homebrew 作为Mac的包管理神器，首先当然要先从Homebrew开始。Homebrew已经支持了ARM架构，可以直接进行安装，当然，如果你电脑里以前存在X86的brew支持，请先卸载干净。 Homebrew 卸载 1/bin/bash -c &quot;$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/uninstall.sh)&quot; Install ARM Homebrew 1/bin/bash -c &quot;$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/install.sh)&quot; 执行完毕后，Homebrew安装在/opt/homebrew路径下；在安装完毕后，命令行后会提示执行命令设置环境变量，当然，以防万一，这里也提供一下： 12echo 'eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;' &gt;&gt; ~/.zprofileeval &quot;$(/opt/homebrew/bin/brew shellenv)&quot; 如果是bash shell， 则： 12echo 'eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;' &gt;&gt; ~/.bash_profileeval &quot;$(/opt/homebrew/bin/brew shellenv)&quot; 记得source ~/.zprofile Install X86 Homebrew 1arch -x86_64 /bin/bash -c &quot;$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/install.sh)&quot; X86版本的安装执行完成后命令行未提示添加环境变量。 alias 支持多版本 在终端执行： 12alias brew='arch -arm64 /opt/homebrew/bin/brew'alias ibrew='arch -x86_64 /usr/local/bin/brew' 这里可以看出两者路径区别 设置镜像 中科大源 1234567891011# brewgit -C &quot;$(brew --repo)&quot; remote set-url origin https://mirrors.ustc.edu.cn/brew.git# coregit -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git# caskgit -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.gitbrew update 清华大学源 1234567891011# brewgit -C &quot;$(brew --repo)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git# coregit -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git# caskgit -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-cask.gitbrew update 恢复默认源 1234567891011# brewgit -C &quot;$(brew --repo)&quot; remote set-url origin https://github.com/Homebrew/brew.git# coregit -C &quot;$(brew --repo homebrew/core)&quot; remote set-url origin https://github.com/Homebrew/homebrew-core.git# caskgit -C &quot;$(brew --repo homebrew/cask)&quot; remote set-url origin https://github.com/Homebrew/homebrew-cask.gitbrew update 更多源 Homebrew 其他相关 设置bottles镜像 1234567# bottles for zshecho 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles/bottles' &gt;&gt; ~/.zprofilesource ~/.zprofile# bottles bashecho 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles/bottles' &gt;&gt; ~/.bash_profilesource ~/.bash_profile cask 目前cask是从GitHub上读取软件源，而GitHub Api对访问有限制，如果使用比较频繁的话，可以申请Api Token，然后在环境变量中配置到HOMEBREW_GITHUB_API_TOKEN。 12echo 'export HOMEBREW_GITHUB_API_TOKEN=yourtoken' &gt;&gt; ~/.zprofilesource ~/.zprofile Install Miniforge3 首先需要下载安装包： Download 请下载arm64(Apple Silicon)版本： 下载完成后进入到文件目录，比如我是在~/Download/内，执行： 1bash Miniforge3-MacOSX-arm64.sh 整个执行过程会有大概三次填写yes并回车确定，最后一次会询问你是否执行conda init， 会自动在~/.zshrc内添加环境变量，如果未执行的，可以将下面语句加入文件末尾： 12345678910111213141516# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;# !! Contents within this block are managed by 'conda init' !!__conda_setup=&quot;$('/Users/xx/miniforge3/bin/conda' 'shell.zsh' 'hook' 2&gt; /dev/null)&quot;if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot;else if [ -f &quot;/Users/xx/miniforge3/etc/profile.d/conda.sh&quot; ]; then . &quot;/Users/xx/miniforge3/etc/profile.d/conda.sh&quot; else export PATH=&quot;/Users/xx/miniforge3/bin:$PATH&quot; fifiunset __conda_setupconda activate tf# &lt;&lt;&lt; conda initialize &lt;&lt;&lt; 记得自行更改/Users/xx/内的用户名 等待Miniforge3安装完成，然后设置一个专供学习Tensorflow的虚拟环境 12conda create -n tf python=3.9.5conda activate tf # 将这句添加到~/.zshrc内，每次打开shell都会自动执行 关于conda切换环境的命令，建议自行Google学习一下，很有用。 Install Tensorflow 目前网上流传的Tensorflow安装基本是两个版本，一个是安装一大堆的支持和依赖，一个是使用yml文件提前准备好环境库一键完成环境创建，比如environment.yml： 1conda env create --file=environment.yml --name=tf 其实这一步也很简单，Apple为了大力推广自家的ARM，已经为大家做好了这部分准备，我们只需要安装就行了。 假设目前在tf环境内 123conda install -c apple tensorflow-depspython -m pip install tensorflow-macospython -m pip install tensorflow-metal 好了，结束！ 可以自行利用下面一段代码测试下： 123456789101112from tensorflow.keras import layersfrom tensorflow.keras import modelsmodel = models.Sequential()model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.Flatten())model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(10, activation='softmax'))model.summary() 123456789101112131415from tensorflow.keras.datasets import mnistfrom tensorflow.keras.utils import to_categorical(train_images, train_labels), (test_images, test_labels) = mnist.load_data()train_images = train_images.reshape((60000, 28, 28, 1))train_images = train_images.astype('float32') / 255test_images = test_images.reshape((10000, 28, 28, 1))test_images = test_images.astype('float32') / 255train_labels = to_categorical(train_labels)test_labels = to_categorical(test_labels)model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])model.fit(train_images, train_labels, epochs=5, batch_size=64)test_loss, test_acc = model.evaluate(test_images, test_labels)test_acc 执行过程中可以在资源管理器中看到GPU的占用： 其他 Lightgbm 1conda install Loghtgbm 一句代码解决，完全靠谱。 xgboost xgboost稍微有点麻烦，我测试了最稳妥的安装方式，还是自行编译，那这个时候我们就需要用到brew安装并设置编译环境了： 注意，我用的都是brew而非ibrew, 目前都是在ARM环境下完成操作。 123brew install gccbrew install cmakebrew install libomp 然后下载源码并执行 1234567git clone git@github.com:dmlc/xgboost.gitcd xgboostmkdir buildcd buildCC=gcc-11 CXX=g++-11 cmake ..cd ../python-package/Users/xx/miniforge3/envs/tf/bin/python setup.py install 然后就OK了。 至于其他的，Numpy在安装Tensorflow的时候就自动作为依赖安装了，Pandas, Matplotlib, NGBoost等，执行下方： 123conda install -c conda-forge pandasconda install -c conda-forge matplotlibconda install -c conda-forge ngboost 如果conda内实在没有的，再试试pip安装，再不行，就只能自行下载源码编译了。 目前在当前环境下解决不了的几个库： CatBoost Cairo -&gt; Pycairo GraphEmbedding CV2 igraph 在整个过程中，可能会遇到各种各样的问题，大家要习惯于使用Google和查阅官方文档； 参考 Tensoflow-macos Run xgboost on Mac and Regression data Accelerating TensorFlow Performance on Mac The new Apple M1 chips have accelerated TensorFlow support M1 Mac Mini Scores Higher Than My RTX 2080Ti in TensorFlow Speed Test. GPU acceleration for Apple's M1 chip? M1芯片Mac上Homebrew安装教程 Mac mini M1使用简单体验(编程、游戏、深度学习) Installing TensorFlow 2.4 on MacOS 11.0 without CUDA for both Intel and M1 based Macs 在 M1 芯片 Mac 上使用 Homebrew Apple M1终于让MacBook变的可以炼丹了 Install XGBoost and LightGBM on Apple M1 Macs Installing TensorFlow on the M1 Mac Getting Started with tensorflow-metal PluggableDevice M1芯片mac安装xgboost和lightgbm AI - Apple Silicon Mac M1 机器学习环境 (TensorFlow, JupyterLab, VSCode) M1芯片安装tensorflow 使用MacBook pro M1搭建基于ML Compute加速的TensorFlow深度学习环境 你的Mac有了专用版TensorFlow，GPU可用于训练，速度最高提升7倍 在M1的Mac上安装Tensorflow（避坑版） 在M1芯片Mac上搭建原生适配Python环境 Conda-forge Miniforge M1 mac安装PyTorch的完整步骤指南 macOS M1(AppleSilicon) 安装TensorFlow环境 傻瓜版M1配置Tensorflow-超简单近乎一键完成 environment.yml opencv-python MAC安装Opencv以及Dlib碰到的一些问题 Jupiter Widgets 启动SparkContext报错 MacBook Pro 2020 M1芯片安装xgboost xgboost Homebrew / Linuxbrew 镜像使用帮助 镜像助手 Apple Silicon Mac 安装xgboost M1芯片mac安装xgboost和lightgbm mac安装lightgbm踩坑心得，亲测有效！ MAC 上 使用lightgbm遇到image not found 解决办法总结 杂记-Macbook Pro M1芯片能玩深度学习吗？","link":"/Apple_M1_AI_environment_construction/"},{"title":"07 AI帮你做总结","text":"Hi， 我是茶桁。 在上一节中，我们介绍了如何使用最新的ChatGPT API，注册HuggingFace账户，并将我们的聊天机器人部署出去。在这个过程中，我们学习了实际的应用开发过程，使你对聊天机器人的开发有了充足的体验。在这一讲中，我们将探讨OpenAI的各种接口提供的能力，以更深入地了解这些接口。我们将分别介绍如何利用嵌入（Embedding）进行文本聚类，并使用提示语（Prompt）对文本进行总结。此外，我们还将介绍其他的接口能力，如语言模型和自然语言生成，以帮助您更好地理解和利用OpenAI的各种功能。 基于 Embedding 向量进行文本聚类文本聚类简介 文本聚类是一种自动将未标注的文本根据相似度分成几类的方法。使用 GPT 系列模型进行文本聚类非常简单，我们可以将文本转换为向量，然后使用一些简单的聚类算法，比如最简单的 K-Means 算法。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/AI-can-help-you-summarize-your-content/"},{"title":"人工神经网络","text":"神经元、如何构建网络、高级神经网络 人工神经网络是人工智能（AI）中重要而有趣的一部分。 什么是神经网络？ 神经网络是对大脑神经过程的复制。 它是在计算机上构建的大脑模拟。 神经网络，无论是生物的还是人工的，都由大量的简单单元和神经元组成，它们相互接收和传输信号。 它由细胞体和连接神经元的导线组成。 用生物学语言来说 ： 为神经元提供输入的电线称为树突。 在某些情况下，神经元会向另一个神经元发送信号，这些向外发送信号的导线被称为轴突。 轴突可能与一个或多个树突相连，这种交叉点称为突触。 这个过程会随着我们的成长而不断调整，这种 \"调整 \"被称为记忆或学习。 什么是深度学习？ 深度学习是一种机器学习技术，由相互连接的多层简单处理单元组成。 它的灵感来源于大脑处理视觉信息的方式。 为什么要开发人工神经网络？ 开发人工神经网络（ANN）的原因之一是为了帮助神经科学（研究大脑和神经系统）。 人们相信，通过绘制人脑图谱，我们可以了解意识和智力背后的秘密。 我们已经能够识别异常功能，并帮助大脑避免异常功能。 例如--解决老年痴呆症、因受伤造成的损伤和发育障碍。 开发人工神经网络（ANN）的另一个原因是为了建立更好的人工智能和机器学习技术。 因为，大脑是一个极其复杂的信息处理系统。 人工神经网络的特点 ： ANN 由许多神经元组成，可以同时处理信息。这意味着，我们可以同时处理大量数据，从而提高了效率。 神经元可以同时存储（就像内存一样）和处理信息，因此从存储器中检索数据不会有任何延迟，因而速度很快。 是的，ANN 可以快速处理信息，但很难应用于 \"传统计算机\"（单机处理），因为它一次只能完成一项任务。这就是 GPU 的用武之地。 听说过 GPU 吗？ GPU 是图形处理单元（Graphical Processing Unit）的缩写，它可以进行并行处理，而不是像传统计算机那样进行单一处理。因此，神经网络可以快速完成工作或处理信息。 构建神经网络 ： 权重在神经网络中扮演着重要角色，它通过控制每个输入，让网络从这些数据中学习，从而做出准确的预测。 但是，什么是权重？ 权重就像可调节的旋钮，决定着每个输入对最终输出的影响程度。 例如，为了找到适当的平衡（数据），我们要给输入值加上适当的权重。 因此，通过将每个输入值（神经元）与权重相乘并相加，我们就能实现 \"线性组合\"。 线性组合公式 ： 考虑到我们有 4 个输入，因此我们也需要 4 个权重来平衡它，而且还会有一个额外的固定值，称为截距（偏差）。 截距值是一个偏置值，用作基准值，这样即使输入值为零，网络也能做出预测。 计算公式： 线性组合 = [截距 + Weight 1 × Input 1 + Weight 2 × Input 2 + Weight 3 × Input 3 + Weight 4 × Input 4］ 问：请考虑以下表达式 10.0 + 5.4 × 8 + (-10.2) × 5 + (-0.1) × 22 + 101.4 × (-5) + 0.0 × 2 + 12.0 × (-3) = -543.0 (i) 表达式中的截距（偏差）项是什么？ \"10.0 \"是截距（偏差）数，因为它没有乘以任何变量。 (ii) 这里的输入是什么？ \"8,5,22,-5,2,-3 \"是输入值，因为它是乘法中的第二个数字。 实现线性组合后，再将其传递给 \"激活函数\"。 激活功能： 激活函数就像一个开关，它决定信号是否应该通过，使神经网络能够有效地学习和解决不同的问题。 为图像识别、自然语言处理等进行预测。 激活函数示例 ： 激活函数的一些示例如下 识别函数：什么也不做，只输出线性组合（与线性回归相同，不提供任何新信息，因此很少使用） 步进功能：如果线性组合值大于 0，则通过信号，否则什么也不做 Sigmoid 函数：阶跃函数的 \"软 \"版本 通过线性组合激活函数实现的神经元输出用于预测或决策。 \"感知器--人工神经网络（ANN）之母\"： Perceptron 是一种使用阶跃激活函数的简单神经元模型。 它被用作二元分类任务中的简单分类器。 由于它是第一个正式的神经网络模型，因此被称为 \"ANN 之母\"。 \"现在，让我们回到神经网络的构建上来。 网络架构由层级组成，例如 ： 输入层：由作为输入数据的神经元组成。例如，用于图像识别的图像像素值。 隐藏层：接收输入层的输出，并将自己的输出传递给下一层。 输出层：产生网络的最终结果。例如，用于人脸识别的人的概率值。 为了在这些层中进行线性组合，我们应该能够找到合适的权重。 反向传播 - 找到合适的权重 ： 在过去（20 世纪 80 年代之前），人们曾使用过感知器算法，但寻找权重需要花费大量时间。 因此，人们引入了反向传播算法。它通过层层递进和递退来确定合适的权重，从而做出准确的预测。 现在，让我们举个例子，来识别图像。 建立分类器，对图像显示的是 \"X \"还是 \"O \"进行分类 这里是一个 5 × 5 的网格，因此每幅图像由 25 个像素组成。阴影像素为 1，其他空白像素为 0。 现在，我们应该应用权重，其中在中心位置，权重假设为-1，而在近中心像素位置，权重假设为 1： 因此，在这里，如果线性组合为负数，即激活度为零，则为 \"X\"；如果为正数，则为 \"O\"。 对第一幅图像进行线性组合 ： (忽略 0 值权重，得到） -&gt; 1 × -1 = -1 因此，我们得到 \"X” 对第二幅图像进行线性组合： (忽略 0 值权重，我们得到） -&gt; 1 × 1 + 1 × 1 + 1 × 1 + 1 × 1 = 4 因此我们得到 \"O\" 到目前为止，我们已经了解了 -&gt; 多层网络（超过一层的神经网络）、非线性函数（阶跃激活函数和 Sigmoid 激活函数）、学习规则（如反向传播）。 让我们进入高级神经网络。 卷积神经网络 ： 使用感知器或线性回归可以进行图像处理，但由于需要大量权值，而且无法有效检测图像特征，因此效果和效率都不高。 因此，为了解决这些局限性，人们引入了卷积神经网络。 CNN 或卷积神经网络由卷积层组成，可以自动学习和提取图像特征，如颜色、图案、边缘等。 例如，CNN 可用于动物检测、标志检测等。 如果我们想使用传统方法检测图像或识别图像，它将使用图像中的像素位置来检测物体。因此，我们必须有一张相似的图像才能做到这一点，但对于卷积神经网络来说，这并非必要。 例如，我们有一张位于图像中心的停车标志的训练图像，然后我们会得到一张测试图像，该图像的右上角有一个停车标志。由于训练图像和测试图像的像素值和位置不同，因此无法使用感知器进行检测。不过，通过使用卷积神经网络，它可以成功检测出图像中任何位置的停车标志。 生成式人工智能（Generative AI）： 生成式人工智能是人工智能的一种，可以生成文本、图像、音频和合成数据等各种类型的内容。 它可以是.....： 监督学习法 无监督学习法 半监督学习法 判别模型用于通过标注数据的训练进行分类或预测。 生成模型用于生成新数据，如预测序列中的下一个单词。 生成对抗网络（GAN）： 其原理是让两个神经网络相互竞争。 一个网络将生成与训练数据类似的图像。 另一个网络将对生成的图像和训练图像进行分类。 这样做是为了生成逼真的图像。 上述图像由英伟达公司开发的 GAN 生成。 将人工智能应用于现实问题比解决谜题和游戏更具挑战性。在现实世界的场景中，可能出现的状态数量之多令人目不暇接，使得穷举式搜索或巧妙的启发式方法无法奏效。此外，由于我们无法控制的因素，行动的结果并不总是可以预测的，这就引入了随机性。为了解决这些复杂问题，我们需要将不确定性和概率的概念纳入算法，同时利用先进的神经网络，使我们能够有效地解决现实世界中的人工智能问题。 康康康康恐龙康。最后，你已经掌握了基本的神经网络和高级神经网络的基本知识。","link":"/Artificial-Neural-Network/"},{"title":"Finish the search problem","text":"The code address of this article is: example_01_Assignment Please read the answer below after thinking for yourself Please using the search policy to implement an agent. This agent receives two input, one is @param start station and the other is @param destination. Your agent should give the optimal route based on Beijing Subway system. Dataflow: 1. Get data from web page. Get web page source from: https://baike.baidu.com/item/%E5%8C%97%E4%BA%AC%E5%9C%B0%E9%93%81/408485 You may need @package requests https://2.python-requests.org/en/master/ page to get the response via url You may need save the page source to file system. The target of this step is to get station information of all the subway lines; You may need install @package beautiful soup https://www.crummy.com/software/BeautifulSoup/bs4/doc/ to get the url information, or just use &gt; Regular Expression to get the url. Our recommendation is that using the Regular Expression and BeautiflSoup both. You may need BFS to get all the related page url from one url. Question: Why do we use BFS to traverse web page (or someone said, build a web spider)? Can DFS do this job? which is better? 2. Preprocessing data from page source. Based on the page source gotten from url. You may need some more preprocessing of the page. the Regular Expression you may need to process the text information. You may need @package networkx, @package matplotlib to visualize data. You should build a dictionary or graph which could represent the connection information of Beijing subway routes. You may need the defaultdict, set data structures to implement this procedure. 3. Build the search agent Build the search agent based on the graph we build. for example, when you run: 1&gt;&gt;&gt; search('奥体中心', '天安门') you need get the result: 奥体中心-&gt; A -&gt; B -&gt; C -&gt; ... -&gt; 天安门 HTTP协议 超文本传输协议（HTTP，HyperText Transfer Protocol）是互联网上应用最为广泛的一种网络协议。所有的www文件都必须遵守这个标准。 HTTP用于客户端和服务器之间的通信。协议中规定了客户端应该按照什么格式给服务器发送请求，同时也约定了服务端返回的响应结果应该是什么格式。 请求访问文本或图像等信息的一端称为客户端，而提供信息响应的一端称为服务器端。 客户端告诉服务器请求访问信息的方法： - Get 获得内容 - Post 提交表单来爬取需要登录才能获得数据的网站 - put 传输文件 更多参考： HTTP请求状态 了解200 404 503 - 200 OK //客户端请求成功 - 404 Not Found //请求资源不存在，eg：输入了错误的URL - 503 Server Unavailable //服务器当前不能处理客户端的请求，一段时间后可能恢复正常。 #### Requests 纯粹HTML格式的网页通常被称为静态网页，静态网页的数据比较容易获取。 在静态网页抓取中，有一个强大的Requests库能够让你轻易地发送HTTP请求。 在终端上安装 Requests pip install requents 123456789101112# 获取响应内容import requests# get（输入你想要抓去的网页地址）r = requests.get('https://www.baidu.com/')print('文本编码：（服务器使用的文本编码）', r.encoding)print('响应状态码：（200表示成功）', r.status_code)print('字符串方式的响应体：（服务器响应的内容）', r.text) 拓展知识： Unicode和UTF-8有什么区别?(盛世唐朝回答) 正则表达式 正则表达式的思想是你在人群中寻找你的男朋友/女朋友，他/她在你心里非常有特点。 同样，从一堆文本中找到需要的内容，我们可以采用正则表达式。 正经点说，是以一定的模式来进行字符串的匹配。 掌握正则表达式需要非常多的时间，我们可以先入门，在以后的工作中遇到，可更加深入研究。 使用正则表达式有如下步骤： 寻找【待找到的信息】特点 使用符号找到特点 获得信息 12345678910111213141516171819202122232425262728293031323334353637383940# 请先运行一下、看一下有什么参数？# 请思考，找到会返回什么？没找到会返回什么？import rehelp(re.match)# 请运行之后、思考 match 与 search 的区别?m = re.search('foo', 'seafood')print(m)print(m.group())print('-------------------------')m = re.match('foo', 'seafood')print(m)#### `search`是搜索字符串中首次出现的位置# 匹配多个字符串 |m = re.match('bat|bet|bit', 'bat')print(m.group()) if m is not None else print('None')# 匹配任意单个字符 .m = re.match('.end', 'kend')print(m.group()) if m is not None else print('None')m = re.match('.end', 'end')print(m.group()) if m is not None else print('None')# 字符串集合 []m = re.match('[cr][23][dp][o2]', 'c3p2')print(m.group()) if m is not None else print('None')# [] 与 |是不同的m = re.match('c3po|r2d2', 'c3p2')print(m.group()) if m is not None else print('None') 给大家提供一个字典，供大家查询～ 字符 描述 &lt;/th&gt; 将下一个字符标记为一个特殊字符、或一个原义字符、或一个向后引用、或一个八进制转义符。例如，“n”匹配字符“n”。“”匹配一个换行符。串行“\\”匹配“&lt;/code&gt;”而“(”则匹配“(”。 &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;^&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配“&lt;code&gt;\\n&lt;/code&gt;”或“&lt;code&gt;\\r&lt;/code&gt;”之后的位置。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt; \\* &lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配前面的子表达式零次或多次。例如，zo\\*能匹配“&lt;code&gt;z&lt;/code&gt;”以及“&lt;code&gt;zoo&lt;/code&gt;”。\\* 等价于{0,}。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;+&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配前面的子表达式一次或多次。例如，“&lt;code&gt;zo+&lt;/code&gt;”能匹配“&lt;code&gt;zo&lt;/code&gt;”以及“&lt;code&gt;zoo&lt;/code&gt;”，但不能匹配“&lt;code&gt;z&lt;/code&gt;”。+等价于{1,}。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;?&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配前面的子表达式零次或一次。例如，“&lt;code&gt;do(es)?&lt;/code&gt;”可以匹配“&lt;code&gt;does&lt;/code&gt;”或“&lt;code&gt;does&lt;/code&gt;”中的“&lt;code&gt;do&lt;/code&gt;”。?等价于{0,1}。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;}&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;是一个非负整数。匹配确定的&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;次。例如，“&lt;code&gt;o{2}&lt;/code&gt;”不能匹配“&lt;code&gt;Bob&lt;/code&gt;”中的“&lt;code&gt;o&lt;/code&gt;”，但是能匹配“&lt;code&gt;food&lt;/code&gt;”中的两个o。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,}&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;是一个非负整数。至少匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;次。例如，“&lt;code&gt;o{2,}&lt;/code&gt;”不能匹配“&lt;code&gt;Bob&lt;/code&gt;”中的“&lt;code&gt;o&lt;/code&gt;”，但能匹配“&lt;code&gt;foooood&lt;/code&gt;”中的所有o。“&lt;code&gt;o{1,}&lt;/code&gt;”等价于“&lt;code&gt;o+&lt;/code&gt;”。“&lt;code&gt;o{0,}&lt;/code&gt;”则等价于“&lt;code&gt;o*&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;}&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;和&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;均为非负整数，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&amp;lt;=&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;。最少匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;次且最多匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;次。例如，“&lt;code&gt;o{1,3}&lt;/code&gt;”将匹配“&lt;code&gt;fooooood&lt;/code&gt;”中的前三个o。“&lt;code&gt;o{0,1}&lt;/code&gt;”等价于“&lt;code&gt;o?&lt;/code&gt;”。请注意在逗号和两个数之间不能有空格。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;?&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;当该字符紧跟在任何一个其他限制符（*,+,?，{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;}，{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,}，{&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;,&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“&lt;code&gt;oooo&lt;/code&gt;”，“&lt;code&gt;o+?&lt;/code&gt;”将匹配单个“&lt;code&gt;o&lt;/code&gt;”，而“&lt;code&gt;o+&lt;/code&gt;”将匹配所有“&lt;code&gt;o&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;.&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配除“&lt;code&gt;\\&lt;/code&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;&lt;code&gt;n&lt;/code&gt;&lt;/span&gt;”之外的任何单个字符。要匹配包括“&lt;code&gt;\\&lt;/code&gt;&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;&lt;code&gt;n&lt;/code&gt;&lt;/span&gt;”在内的任何字符，请使用像“&lt;code&gt;(.|\\n)&lt;/code&gt;”的模式。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配pattern并获取这一匹配。所获取的匹配可以从产生的Matches集合得到，在VBScript中使用SubMatches集合，在JScript中则使用$0…$9属性。要匹配圆括号字符，请使用“&lt;code&gt;\\(&lt;/code&gt;”或“&lt;code&gt;\\)&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?:pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配pattern但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用或字符“&lt;code&gt;(|)&lt;/code&gt;”来组合一个模式的各个部分是很有用。例如“&lt;code&gt;industr(?:y|ies)&lt;/code&gt;”就是一个比“&lt;code&gt;industry|industries&lt;/code&gt;”更简略的表达式。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?=pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，“&lt;code&gt;Windows(?=95|98|NT|2000)&lt;/code&gt;”能匹配“&lt;code&gt;Windows2000&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;Windows3.1&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?!pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如“&lt;code&gt;Windows(?!95|98|NT|2000)&lt;/code&gt;”能匹配“&lt;code&gt;Windows3.1&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;Windows2000&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?&amp;lt;=pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;反向肯定预查，与正向肯定预查类拟，只是方向相反。例如，“&lt;code&gt;(?&amp;lt;=95|98|NT|2000)Windows&lt;/code&gt;”能匹配“&lt;code&gt;2000Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;3.1Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;(?&amp;lt;!pattern)&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;反向否定预查，与正向否定预查类拟，只是方向相反。例如“&lt;code&gt;(?&amp;lt;!95|98|NT|2000)Windows&lt;/code&gt;”能匹配“&lt;code&gt;3.1Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”，但不能匹配“&lt;code&gt;2000Windows&lt;/code&gt;”中的“&lt;code&gt;Windows&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;x|y&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配x或y。例如，“&lt;code&gt;z|food&lt;/code&gt;”能匹配“&lt;code&gt;z&lt;/code&gt;”或“&lt;code&gt;food&lt;/code&gt;”。“&lt;code&gt;(z|f)ood&lt;/code&gt;”则匹配“&lt;code&gt;zood&lt;/code&gt;”或“&lt;code&gt;food&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[xyz]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;字符集合。匹配所包含的任意一个字符。例如，“&lt;code&gt;[abc]&lt;/code&gt;”可以匹配“&lt;code&gt;plain&lt;/code&gt;”中的“&lt;code&gt;a&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[^xyz]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;负值字符集合。匹配未包含的任意字符。例如，“&lt;code&gt;[^abc]&lt;/code&gt;”可以匹配“&lt;code&gt;plain&lt;/code&gt;”中的“&lt;code&gt;p&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[a-z]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;字符范围。匹配指定范围内的任意字符。例如，“&lt;code&gt;[a-z]&lt;/code&gt;”可以匹配“&lt;code&gt;a&lt;/code&gt;”到“&lt;code&gt;z&lt;/code&gt;”范围内的任意小写字母字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;[^a-z]&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;负值字符范围。匹配任何不在指定范围内的任意字符。例如，“&lt;code&gt;[^a-z]&lt;/code&gt;”可以匹配任何不在“&lt;code&gt;a&lt;/code&gt;”到“&lt;code&gt;z&lt;/code&gt;”范围内的任意字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\b&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个单词边界，也就是指单词和空格间的位置。例如，“&lt;code&gt;er\\b&lt;/code&gt;”可以匹配“&lt;code&gt;never&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”，但不能匹配“&lt;code&gt;verb&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\B&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配非单词边界。“&lt;code&gt;er\\B&lt;/code&gt;”能匹配“&lt;code&gt;verb&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”，但不能匹配“&lt;code&gt;never&lt;/code&gt;”中的“&lt;code&gt;er&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\cx&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配由x指明的控制字符。例如，\\cM匹配一个Control-M或回车符。x的值必须为A-Z或a-z之一。否则，将c视为一个原义的“&lt;code&gt;c&lt;/code&gt;”字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\d&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个数字字符。等价于[0-9]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\D&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个非数字字符。等价于[^0-9]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\f&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个换页符。等价于\\x0c和\\cL。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\n&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个换行符。等价于\\x0a和\\cJ。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\r&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个回车符。等价于\\x0d和\\cM。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\s&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配任何空白字符，包括空格、制表符、换页符等等。等价于[ \\f\\n\\r\\t\\v]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\S&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配任何非空白字符。等价于[^ \\f\\n\\r\\t\\v]。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\t&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个制表符。等价于\\x09和\\cI。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\v&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配一个垂直制表符。等价于\\x0b和\\cK。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\w&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配包括下划线的任何单词字符。等价于“&lt;code&gt;[A-Za-z0-9_]&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\W&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配任何非单词字符。等价于“&lt;code&gt;[^A-Za-z0-9_]&lt;/code&gt;”。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\x&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，“&lt;code&gt;\\x41&lt;/code&gt;”匹配“&lt;code&gt;A&lt;/code&gt;”。“&lt;code&gt;\\x041&lt;/code&gt;”则等价于“&lt;code&gt;\\x04&amp;amp;1&lt;/code&gt;”。正则表达式中可以使用ASCII编码。.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;num&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;num&lt;/span&gt;，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;num&lt;/span&gt;是一个正整数。对所获取的匹配的引用。例如，“&lt;code&gt;(.)\\1&lt;/code&gt;”匹配两个连续的相同字符。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;标识一个八进制转义值或一个向后引用。如果\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;之前至少&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;个获取的子表达式，则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为向后引用。否则，如果&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为八进制数字（0-7），则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为一个八进制转义值。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;标识一个八进制转义值或一个向后引用。如果\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;之前至少有&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;个获得子表达式，则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;为向后引用。如果\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;之前至少有&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;个获取，则&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为一个后跟文字&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;的向后引用。如果前面的条件都不满足，若&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;和&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m&lt;/span&gt;均为八进制数字（0-7），则\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;将匹配八进制转义值&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nml&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;如果&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;为八进制数字（0-3），且&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;m和l&lt;/span&gt;均为八进制数字（0-7），则匹配八进制转义值&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;nm&lt;/span&gt;l。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th style=&quot;text-align:center;&quot;&gt;\\u&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;&lt;/th&gt; &lt;td style=&quot;text-align:left;&quot;&gt;匹配&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;，其中&lt;span style=&quot;font-family:Times New Roman; font-style:italic;&quot;&gt;n&lt;/span&gt;是一个用四个十六进制数字表示的Unicode字符。例如，\\u00A9匹配版权符号（©）。&lt;/td&gt; &lt;/tr&gt; 12345678910111213# 匹配电子邮件地址patt = '\\w+@(\\w+\\.)?\\w+\\.com'm = re.match(patt, 'nobody@xxx.com')print(m.group()) if m is not None else print('None')# 匹配QQm = re.search('[1-9][0-9]{4,}', '这是我的QQ号781504542,第二个qq号：10054422288')print(m.group()) if m is not None else print('None')# findall() 是search的升级版，可以找到所有匹配的字符串m = re.findall('[1-9][0-9]{4,}', '这是我的QQ号781504542,第二个qq号：10054422288')print(m) if m is not None else print('None') 了解了怎么使用，下面进入实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# get the data (subway for beijing ,from amap)# 你需要用到以下的包import requestsimport reimport numpy as npr = requests.get('http://map.amap.com/service/subway?_1469083453978&amp;srhdata=1100_drw_beijing.json')r.textdef get_lines_stations_info(text): # Please write your code here pass # Traverse the text format data to form the location data structure # Dict of all line information: key: line name; value: list of site names lines_info = {} # A dict of all site information: key: site name; value: site coordinates (x, y) stations_info = {} for i in range(len(lines_list)): # Several questions you may need to think about, get &quot;Metro line name, station information list, station name, coordinates (x, y), add data to the information dict of the station, add data to the subway line dict&quot; passlines_info, stations_info = get_lines_stations_info(r.text)# According to the route information, establish the site adjacency table dictdef get_neighbor_info(lines_info): pass # Add str2 to the adjacency list of site str1 def add_neighbor_dict(info, str1, str2): # Please write code here pass return neighbor_info neighbor_info = get_neighbor_info(lines_info)neighbor_info# Draw subway mapimport networkx as nximport matplotlibimport matplotlib.pyplot as plt# If Chinese characters cannot be displayed, please refer tomatplotlib.rcParams['font.sans-serif'] = ['SimHei']# matplotlib.rcParams['font.family']='sans-serif'# You can use recursion to find all pathsdef get_path_DFS_ALL(lines_info, neighbor_info, from_station, to_station): # Recursive algorithm, essentially depth first # Traverse all paths # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Check input site name passdef get_next_station_DFS_ALL(node, neighbor_info, to_station): pass# You can also use the second algorithm: simple breadth first without heuristic functiondef get_path_BFS(lines_info, neighbor_info, from_station, to_station): # Search strategy: take the number of stations as the cost (because the ticket price is calculated by station) # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Since the cost of each layer is increased by 1, the cost of each layer is the same, and it does not matter whether it is calculated or not, so it is omitted # Check input site name pass# You can also use the third algorithm: heuristic search with path distance as the costimport pandas as pddef get_path_Astar(lines_info, neighbor_info, stations_info, from_station, to_station): # Search strategy: the straight-line distance between the stations of the route is accumulated as the cost, and the straight-line distance from the current station to the target is used as the heuristic function # Check input site name pass As much as you can to use the already implemented search agent. You just need to define the is_goal(), get_successor(), strategy() three functions. Define different policies for transfer system. Such as Shortest Path Priority（路程最短优先）, Minimum Transfer Priority(最少换乘优先), Comprehensive Priority(综合优先) Implement Continuous transfer. Based on the Agent you implemented, please add this feature: Besides the @param start and @param destination two stations, add some more stations, we called @param by_way, it means, our path should from the start and end, but also include the @param by_way stations. e.g 123451. Input: start=A, destination=B, by_way=[C] Output: [A, … .., C, …. B]2. Input: start=A, destination=B, by_way=[C, D, E] Output: [A … C … E … D … B] # based on your policy, the E station could be reached firstly. The Answer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305# get the data (subway for beijing ,from amap)import requestsimport reimport numpy as npr = requests.request('GET', url = 'http://map.amap.com/service/subway?_1469083453978&amp;srhdata=1100_drw_beijing.json')def get_lines_stations_info(text): lines_info = {} stations_info = {} pattern = re.compile('&quot;st&quot;.*?&quot;kn&quot;') lines_list = pattern.findall(text) for i in range(len(lines_list)): pattern = re.compile('&quot;ln&quot;:&quot;.*?&quot;') line_name = pattern.findall(lines_list[i])[0][6:-1] pattern = re.compile('&quot;rs&quot;.*?&quot;sp&quot;') temp_list = pattern.findall(lines_list[i]) station_name_list = [] for j in range(len(temp_list)): pattern = re.compile('&quot;n&quot;:&quot;.*?&quot;') station_name = pattern.findall(temp_list[j])[0][5:-1] station_name_list.append(station_name) pattern = re.compile('&quot;sl&quot;:&quot;.*?&quot;') position = tuple(map(float, pattern.findall(temp_list[j])[0][6:-1].split(','))) stations_info[station_name] = position lines_info[line_name] = station_name_list return lines_info, stations_infolines_info, stations_info = get_lines_stations_info(r.text)# print(stations_info)# print(lines_info)len(lines_info)def get_neighbor_info(lines_info): def add_neighbor_dict(info, str1, str2): list1 = info.get(str1) if not list1: list1 = [] list1.append(str2) info[str1] = list1 return info neighbor_info = {} for line_name, station_list in lines_info.items(): for i in range(len(station_list) -1): sta1 = station_list[i] sta2 = station_list[i+1] neighbor_info = add_neighbor_dict(neighbor_info, sta1, sta2) neighbor_info = add_neighbor_dict(neighbor_info, sta2, sta1) return neighbor_infoneighbor_info = get_neighbor_info(lines_info)print(neighbor_info)import networkx as nximport matplotlibimport matplotlib.pyplot as pltmatplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS']matplotlib.rcParams['font.size'] = 2plt.figure(figsize = (20, 20))stations_graph = nx.Graph()stations_graph.add_nodes_from(list(stations_info.keys()))nx.draw(stations_graph, stations_info, with_labels = True, node_size = 5)stations_connection_graph = nx.Graph(neighbor_info)plt.figure(figsize = (30, 30))nx.draw(stations_connection_graph, stations_info, with_labels = True, node_size = 5)# The first algorithm: recursively find all pathsdef get_path_DFS_ALL(lines_info, neighbor_info, from_station, to_station): # # Recursive algorithm, essentially depth first # Traverse all paths # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Check input site name if not neighbor_info.get(from_station): print('起始站点“%s”不存在。请正确输入！'%from_station) return None if not neighbor_info.get(to_station): print('目的站点“%s”不存在。请正确输入！'%to_station) return None path = [] this_station = from_station path.append(this_station) neighbors = neighbor_info.get(this_station) node = {'pre_station':'', 'this_station':this_station, 'neighbors':neighbors, 'path':path} return get_next_station_DFS_ALL(node, neighbor_info, to_station)def get_next_station_DFS_ALL(node, neighbor_info, to_station): neighbors = node.get('neighbors') pre_station = node.get('this_station') path = node.get('path') paths = [] for i in range(len(neighbors)): this_station = neighbors[i] if (this_station in path): # If this station is already in the path, it means a loop, and this road is unreachable return None if neighbors[i] == to_station: # Find the end, return to the path path.append(to_station) paths.append(path) return paths else: neighbors_ = neighbor_info.get(this_station).copy() neighbors_.remove(pre_station) path_ = path.copy() path_.append(this_station) new_node = {'pre_station':pre_station, 'this_station':this_station, 'neighbors':neighbors_, 'path':path_} paths_ = get_next_station_DFS_ALL(new_node, neighbor_info, to_station) if paths_: paths.extend(paths_) return pathspaths = get_path_DFS_ALL(lines_info, neighbor_info, '回龙观', '西二旗')print('共有%d种路径。'%len(paths))for item in paths: print(&quot;此路径总计%d站:&quot;%(len(item)-1)) print('-'.join(item))# The second algorithm: simple breadth first without heuristic functiondef get_path_BFS(lines_info, neighbor_info, from_station, to_station): # Search strategy: take the number of stations as the cost (because the ticket price is calculated by station) # In this case, the coordinate distance between the sites is difficult to transform into a reliable heuristic function, so only a simple BFS algorithm is used # Since the cost of each layer is increased by 1, the cost of each layer is the same, and it does not matter whether it is calculated or not, so it is omitted # Check input site name if not neighbor_info.get(from_station): print('起始站点“%s”不存在。请正确输入！'%from_station) return None if not neighbor_info.get(to_station): print('目的站点“%s”不存在。请正确输入！'%to_station) return None # The search node is a dict, key=site name, value is a list of sites that contain passing nodes = {} nodes[from_station] = [from_station] while True: new_nodes = {} for (k,v) in nodes.items(): neighbor = neighbor_info.get(k).copy() if (len(v) &gt;= 2): # Do not go to the previous stop pre_station = v[-2] neighbor.remove(pre_station) for station in neighbor: # Traverse neighbors if station in nodes: # Skip the nodes that have been searched continue path = v.copy() path.append(station) new_nodes[station] = path if station == to_station: # Find the path, end return path nodes = new_nodes print('未能找到路径') return Nonepaths = get_path_BFS(lines_info, neighbor_info, '回龙观', '西二旗')print(&quot;路径总计%d站。&quot;%(len(paths)-1))print(&quot;-&quot;.join(paths))# Gaode Navigation is 31 stations, only 1 transfer# The result of the code is 28 stations, but there are 5 transfers# Guess Gaode's path cost is mainly time# The third algorithm: heuristic search with path distance as the costimport pandas as pddef get_path_Astar(lines_info, neighbor_info, stations_info, from_station, to_station): # Search strategy: the straight-line distance between the stations of the route is accumulated as the cost, and the straight-line distance from the current station to the target is used as the heuristic function # Check input site name if not neighbor_info.get(from_station): print('起始站点“%s”不存在。请正确输入！'%from_station) return None if not neighbor_info.get(to_station): print('目的站点“%s”不存在。请正确输入！'%to_station) return None # Calculate the straight-line distance from all nodes to the target node, spare distances = {} x,y = stations_info.get(to_station) for (k,v) in stations_info.items(): x0,y0 = stations_info.get(k) l = ((x-x0)**2 + (y-y0)**2)**0.5 distances[k] = l # Nodes that have been searched, dict # key=site name, value is the minimum cost from a known starting point to this site # 已搜索过的节点，dict searched = {} searched[from_station] = 0 # The data structure is pandas dataframe # index is the site name # g is the path taken, h is the heuristic function value (the current straight-line distance to the target) nodes = pd.DataFrame([[[from_station], 0, 0, distances.get(from_station)]], index=[from_station], columns=['path', 'cost', 'g', 'h']) count = 0 while True: if count &gt; 1000: break nodes.sort_values('cost', inplace=True) for index, node in nodes.iterrows(): count += 1 # Search for the site that is the shortest from the destination among the neighbors neighbors = neighbor_info.get(index).copy() if len(node['path']) &gt;= 2: # Do not search in the reverse direction of this path neighbors.remove(node['path'][-2]) for i in range(len(neighbors)): count += 1 neighbor = neighbors[i] g = node['g'] + get_distance(stations_info, index, neighbor) h = distances[neighbor] cost = g + h path = node['path'].copy() path.append(neighbor) if neighbor == to_station: # Find the goal, end print('共检索%d次。'%count) return path if neighbor in searched: if g &gt;= searched[neighbor]: # Explain that the search path is not optimal, ignore it continue else: searched[neighbor] = g # Modify the node information corresponding to this site# nodes.loc[neighbor, 'path'] = path # 这行总是报错# nodes.loc[neighbor, 'cost'] = cost# nodes.loc[neighbor, 'g'] = g# nodes.loc[neighbor, 'h'] = h # I don’t know how to modify the list element in df, I can only delete and add new rows nodes.drop(neighbor, axis=0, inplace=True) row = pd.DataFrame([[path, cost, g, h]], index=[neighbor], columns=['path', 'cost', 'g', 'h']) nodes = nodes.append(row) else: searched[neighbor] = g row = pd.DataFrame([[path, cost, g, h]], index=[neighbor], columns=['path', 'cost', 'g', 'h']) nodes = nodes.append(row) # All neighbors of this site have been searched, delete this node nodes.drop(index, axis=0, inplace=True) # The outer for loop only runs the first row of data, and then re-sort and then calculate continue print('未能找到路径') return Nonedef get_distance(stations_info, str1, str2): x1,y1 = stations_info.get(str1) x2,y2 = stations_info.get(str2) return ((x1-x2)**2 + (y1-y2)**2)** 0.5paths = get_path_Astar(lines_info, neighbor_info, stations_info, '回龙观', '西二旗')if paths: print(&quot;路径总计%d站。&quot;%(len(paths)-1)) print(&quot;-&quot;.join(paths))# Gaode Navigation is 31 stations, only 1 transfer# The code result is 28 stations, which is the same as the result with the number of subway stations as the cost, but the path is different (from the first traversal algorithm, you can see that there are 3 paths for 28 stations to reach the destination)# Guess Gaode's path cost is mainly time","link":"/Assignment/"},{"title":"将 Bard API 与 ChatGPT 集成：实时数据访问","text":"在人工智能领域，很少有创新能像 OpenAI 的 ChatGPT 一样激发世界的想象力。这种非凡的对话式人工智能改变了我们看待人机交互的方式，展现出一定程度的复杂性、情境意识和创造力，而这些曾经被认为是人类智能的专属领域。 ChatGPT 基于强大的 GPT-3 模型构建，能够进行引人入胜、有意义且令人印象深刻的类人对话。它可以写诗、回答复杂的问题、辅导各种科目、翻译语言，甚至模仿著名作家的写作风格。从本质上讲，它重新定义了我们认为人工智能可能实现的界限。 然而，ChatGPT 的主要缺点是它缺乏实时互联网数据访问。这意味着，虽然 ChatGPT 可以生成高度智能且上下文准确的响应，但其知识基本上被及时冻结，截止日期为 2021 年 9 月。 那么，当出现需要通过将 Google 的 Bard API 与 ChatGPT 集成来获取超出此限制的信息的问题时，会发生什么情况呢？ 以下是使用Python将 Bard API 连接到 ChatGPT 以检索实时数据的分步指南： 第 1 步：安装非官方 Bard Python 库并检索 Cookie 值（API 密钥） 我正在使用Daniel Park使用逆向工程开发的非官方 Bard 库。这个库是一个非常用户友好的Python包。其主要目的是通过 API 从 Google Bard 获取响应。使用 Bard-API，用户可以方便地将 Bard 的自然语言响应集成到他们的 Python 项目和各种应用程序中。 1pip install bardapi 您还可以直接从 Github安装最新版本： 1pip install git+https://github.com/dsdanielpark/Bard-API.git 12345from bardapi import Bardtoken = 'xxxxxxx'bard = Bard(token=token)bard.get_answer(&lt;your query&gt;)['content'] 设置您的 API 密钥 安装 Bard-API 后，使用 Bard cookie 中的 Secure-1PSID 进行身份验证。尽管非正式地称为 API KEY（Cookie 值），但请记住对其保密以确保安全访问。 访问https://bard.google.com/ 按 F12 或右键单击并“检查” 转到应用程序 → Cookie，并将您的 __Secure-1PSID Cookie 值复制到安全位置。 步骤 2：从openai.com获取 OpenAI 密钥并安装 OpenAI 库 访问 OpenAI 网站并获取您的 OpenAI API 密钥。现在安装 OpenAI 库并导入它。 1pip install openai 12import openaiopenai.api_key = &lt;Your_API_Key&gt; 步骤3：将bard请求结果连接到gpt-3.5-turbo模型并设计提示 这里的关键部分是设计将 Bard 结果集成到 ChatGPT API 函数中所需的提示。因此，我为此制定了一个方法： 123456789101112query = input(&quot;Your query&quot;)bard_result = bard.get_answer(query)['content']completion = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Act as an AI chatbot with access to the internet.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Provide a well structured and easily readable text by analyzing this: The first content below is the user's query and the second content below is the result obtained by accessing the internet with the help of google's search alogoritm. Provide the well structured and good mannered answer by processing the user's query and the result from Google search algorithm. /n&quot;+query+' /n '+ bard_result} ])final_response = completion[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]print(final_response) 将所有代码封装在一起，得出结果： 1234567891011121314151617from bardapi import Bardimport openaiopenai.api_key = &lt;Your Key&gt;token = &lt;Your Key&gt;bard = Bard(token=token)query = input(&quot;Your query: &quot;)bard_result = bard.get_answer(query)['content']completion = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Act as an AI chatbot with access to the internet.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Provide a well structured and easily readable text by analyzing this: The first content below is the user's query and the second content below is the result obtained by accessing the internet with the help of google's search alogoritm. Provide the well structured and good mannered answer by processing the user's query and the result from Google search algorithm. /n&quot;+query+' /n '+ bard_result} ])final_response = completion[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]print(final_response) 与其使用 gpt-3.5-turbo 型号，不如试试 gpt-3.5-turbo-16k 和 gpt-4-0314，效果会更好。 通过整合像谷歌的 Bard 这样的应用程序接口，ChatGPT 可以超越目前的局限，为用户提供实时、准确的上下文信息。这将大大增强其协助、教育和与用户互动的能力，为人与人工智能的互动增添一个全新的维度。此外，这还将极大地扩展 ChatGPT 的应用范围，为企业、教育工作者、研究人员和个人带来新的机遇。 我认为这是将互联网接入集成到 ChatGPT 并从 ChatGPT 获得实时见解的最简单方法。","link":"/BardAPI-ChatGPT/"},{"title":"最佳 ChatGPT Chrome 扩展程序","text":"想要轻松访问ChatGPT吗？其中一个最佳方式是通过其一系列Chrome扩展程序。这些扩展程序还为您提供更好的使用ChatGPT的方法，包括帮助您编写更好的提示以获得更好的响应，或为ChatGPT授予搜索互联网的能力，从而提供对更多最新信息的访问。 这是您现在可以使用的最佳 ChatGPT Chrome 扩展程序。 Google 聊天 GPT 嫉妒 Bing Chat 及其在您搜索时与您聊天或使用最新的 GPT-4 语言模型的能力吗？不需要。只需从网上商店获取适用于 Google 的 ChatGPT，您就可以将 ChatGPT 与 Google 搜索一起使用。事实上，只需进行一次普通的 Google 搜索，在结果旁边，您也会收到来自 ChatGPT 的回复，这有时比 Google 结果本身更有用。 Merlin 想要 ChatGPT 在您上网的任何地方响应任何内容？Merlin 将 ChatGPT 带到任何网站，因此您可以突出显示任何文本或网页，并要求 ChatGPT 对其做出响应。您可以让它为您总结一个网页，或为您提供 YouTube 视频的纲要，这样您就不需要全部观看了。 TalkBerry 为什么要在 ChatGPT 上打字，而不是直接与 ChatGPT 对话呢？使用 TalkBerry，您可以简单地与 ChatGPT 通话。只需安装扩展程序并确保您的麦克风或耳机已插入，即可开始使用。使用 TalkBerry，您可以节省大量在输入上的时间，或者将 ChatGPT 用作语言导师，让它聆听并帮助您提高发音和语言理解能力。 TweetGPT 使用 TweetGPT 可以让您的社交媒体游戏更上一层楼。TweetGPT 是 ChatGPT 的插件，利用 AI 聊天机器人工具的强大功能，制作更有趣、更尖刻、更具吸引力或更友好的推文和回复。您可以选择要发布的主题、您的情绪基调和语言，ChatGPT 将完成剩下的工作。如果您对某些措辞不满意，您甚至可以在之后编辑该消息。 GPT-EZ 如果您不喜欢 ChatGPT 界面并想将其更改为您自己的喜好，请尝试 GPT-EZ。它允许您自定义 ChatGPT 网站的 UI，包括配色方案、字体样式和其他选项。此外，它还可以让您更轻松地复制和继续与 ChatGPT 的对话，并让您更轻松地下载对话日志。 SnackPrompt 通过使用一些评价最高的提示来充分利用 AI 聊天机器人。SnackPrompt 列出并排名全球其他聊天机器人用户的最佳提示，让您可以访问一些最新和最强大的 AI 功能。 WebChatGPT ChatGPT 的最大限制之一是它无法访问最近的信息。即使您使用的是最新的 GPT-4 语言模型，它仍然只能访问 2021 年之前的信息。使用 WebChatGPT，您可以让 ChatGPT 能够在网络上搜索更多最新的信息来源。从 Chrome 网上应用店获取扩展程序，在使用 ChatGPT 时只需将其打开即可享受这一方便的功能。 YouTube Summary 喜欢 YouTube 教程，但不想看完序言？让此 ChatGPT 扩展为您总结说明。只需从 YouTube 视频页面获取转录内容，然后将其输入到插件中，您就会立即获得摘要。它还适用于文章、电子邮件或科学论文。","link":"/Best-ChatGPT-Chrome-Extension/"},{"title":"21. 尝试制作你自己的数字人进行播报","text":"Hi， 大家好。我是茶桁。 在之前的课程中，我们接触了AI进行文字回复，语音合成。 那么将这两个组合在一起，我们基本就可以制作一个智能的语音聊天机器人了。看过电影《Her》的同学都应该清楚，AI因为用了女神斯嘉丽.约翰逊的配音，吸引到了不少的观众。 不过， 我们怎么能就满足于此呢，从文字到音频，我们似乎还缺少了一点什么。是啊，谁不希望拥有一个特定的虚拟人来发出自己特定的语音。看着自己在镜头面前侃侃而谈的样子，是不是想想就兴奋？ 把这些需求都结合在一起，那就是“数字人”了，我相信各位小伙伴或多或少都已经接触过，至少在抖音上看到过其他主播的“数字人”了。但是我们不得不说，那些都是一些商业公司的成熟方案，而咱们要实现的内容肯定比不了人家，但是作为概念演示，那是完全够用了。 好了，让我开始吧。首先呢，让我们先制作一个语音聊天机器人 语音聊天机器人的制作 第一步：文本聊天 第一步是什么？当然是需要先做一个「文本聊天机器人」，还记得咱们第六讲的内容吗？接下来，咱们就需要用到第六讲中的代码逻辑，整个UI界面也还是使用Gradio来创建。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Digital-human-broadcasting/"},{"title":"ChatGPT代码解释器：如何为我节省数小时的工作","text":"创建一个交互式世界地图，显示国家人口数量，配以简短的句子描述。 2023年7月6日，OpenAI宣布Code Interpreter将在接下来的一周内向ChatGPT Plus用户开放。它可能是增强ChatGPT的能力和功能的最佳插件之一。 Code Interpreter可以运行代码，允许上传数据，这样您就可以用它来进行数据清理、分析、可视化等许多其他任务。它就像是您指尖上的数据分析师。 听起来很棒吧？接下来我们来看看。 我在一项任务中使用了它，这项任务可能需要我花费几个小时才能完成。根据您的任务和对Python库的熟悉程度，这项任务甚至可能需要更长时间。 在使用ChatGPT时，我只需要写几句话并上传两个CSV文件。 我将逐步介绍整个过程，以及Code Interpreter在理解数据、清理和预处理数据以及创建数据可视化方面的印象深刻之处。 您需要先启用它 如果您想在聊天中使用代码解释器，需要先通过设置启用它。 如何启用代码解释器 开启之后，你在接下来开启新的聊天的时候就可以使用它了。 来个具体任务 我打算创建一个世界地图可视化，其中点的大小与国家的人口成比例。 我有两个数据集，一个是国家人口，另一个是以纬度和经度表示的位置。 您可以轻松地在网上找到这些数据集。如果没有，文末我将提供数据集（以及源代码）。 让我们继续吧 由于这是一个简单的任务，我没有花费太多时间在提示工程上。这是我向ChatGPT请求的第一句话： 你可以在提示窗口左侧使用加号上传数据集。 使用+号上传数据集 首先要做的是读取数据集，这是显而易见的事情。但这还不够令人印象深刻！ 12345678# Code Interpreter 生成的代码import pandas as pd# 将CSV数据加载到pandas DataFrame中population_data = pd.read_csv('/mnt/data/country_population_historic.csv')# 显示 DataFrame 的前几行population_data.head() 然后，ChatGPT要求提供国家位置： 由于我已经拥有相关数据了，所以我要做的就只是上传它： 123456# Code Interpreter 生成的代码# 加载国家坐标数据country_coordinates = pd.read_csv('/mnt/data/country_coordinates.csv')# 显示前几行country_coordinates.head() 数据清洗和预处理 读取数据后的下一步是清洗和预处理数据。我相信你已经听过很多次，这一步通常需要最多的时间。 在创建世界地图可视化之前，有一些问题需要处理。ChatGPT检测到它们并在不到一分钟的时间内提供了解决方案。相当令人印象深刻！ 上面代码段中的代码: 12345678910# 由代码解释器生成的代码# 检查数据框中的国家名称是否一致population_countries = set(population_data['Country Name'])coordinates_countries = set(country_coordinates['name'])# 人口数据中的国家但不在坐标数据中missing_in_coordinates = population_countries - coordinates_countries# 坐标数据中的国家但人口数据中没有missing_in_population = coordinates_countries - population_countries 在坐标数据集和人口数据集中都有一些国家缺失。 ChatGPT在上面的代码片段中做了什么： 创建了Python集合，包含两个数据集中的国家名称。 分别比较这些集合，找到它们之间的差异。 ChatGPT在不到一分钟的时间内解决了这个问题： 以上片段中的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# Code Interpreter 生成的代码# 将人口数据集中的国家名称映射到坐标数据集中country_name_mapping = { 'Bahamas, The': 'Bahamas', 'Brunei Darussalam': 'Brunei', 'Cabo Verde': 'Cape Verde', 'Congo, Dem. Rep.': 'Congo [DRC]', 'Congo, Rep.': 'Congo [Republic]', &quot;Cote d'Ivoire&quot;: &quot;Côte d'Ivoire&quot;, 'Czechia': 'Czech Republic', 'Egypt, Arab Rep.': 'Egypt', 'Gambia, The': 'Gambia', 'Hong Kong SAR, China': 'Hong Kong', 'Iran, Islamic Rep.': 'Iran', &quot;Korea, Dem. People's Rep.&quot;: 'North Korea', 'Korea, Rep.': 'South Korea', 'Kyrgyz Republic': 'Kyrgyzstan', 'Lao PDR': 'Laos', 'Macao SAR, China': 'Macau', 'Micronesia, Fed. Sts.': 'Micronesia', 'Myanmar': 'Myanmar [Burma]', 'North Macedonia': 'Macedonia [FYROM]', 'Russian Federation': 'Russia', 'Sao Tome and Principe': 'São Tomé and Príncipe', 'Slovak Republic': 'Slovakia', 'Syrian Arab Republic': 'Syria', 'Turkiye': 'Turkey', 'Venezuela, RB': 'Venezuela', 'Yemen, Rep.': 'Yemen', 'Eswatini': 'Swaziland'}# 清理人口数据集population_data_clean = population_data.replace({&quot;Country Name&quot;: country_name_mapping})# 删除在坐标数据集中不存在的行population_data_clean = population_data_clean[population_data_clean['Country Name'].isin(coordinates_countries)]# 将人口数据与国家坐标数据合并merged_data = pd.merge(population_data_clean, country_coordinates, left_on='Country Name', right_on='name')# 只保留必要的列merged_data = merged_data[['Country Name', '2022', 'latitude', 'longitude']]# 重命名列以增加清晰度merged_data.columns = ['Country', 'Population', 'Latitude', 'Longitude']merged_data.head() 清理和预处理的数据 现在我们已经有了整洁和清晰的数据格式。这个步骤通常需要很多时间和人工。ChatGPT在大约一分钟内完成了这一步。 创建可视化 ChatGPT首先尝试使用Matplotlib库进行创建。然而，由于没有世界地图图像而失败： 然后，我只是要求它使用 Plotly，这就是我一开始想到的。 以上代码段中的代码： 12345678910111213141516171819202122232425262728293031# 代码由代码解释器生成import plotly.graph_objects as go# 创建散点地理图fig = go.Figure(data=go.Scattergeo( lon = merged_data['Longitude'], lat = merged_data['Latitude'], text = merged_data['Country'] + ': ' + merged_data['Population'].astype(str), mode = 'markers', marker = dict( size = (merged_data['Population'] / 1000000) ** 0.5, # Take square root to better visualize differences sizemode = 'diameter', sizeref = 1, sizemin = 4, color = merged_data['Population'], colorscale = 'Viridis', colorbar_title = &quot;Millions&quot;, )))# 更新布局fig.update_layout( title_text = '2022 World Population', geo = dict( showframe = False, showcoastlines = True, projection_type = 'equirectangular' ))fig.show() 它不能在浏览器中显示交互式世界地图，但成功编写了生成它的代码。我所要做的就是将代码复制粘贴到Jupyter笔记本中。 这是最终结果： 互动世界地图，显示国家人口 最后的话 我们所做的事情： 读取数据集 清洗、预处理和合并它们 创建互动数据可视化 我们所要做的只是写两个句子（并告诉 ChatGPT 使用 Plotly）。我认为这非常令人印象深刻！ 数据 本文已经结束。 文章最后，我将提供数据以及一个jupyter notebook内容，和往常一样，数据将付费查看，以获取一些成本。有想要的朋友可以去我公众号内搜索本文购买：","link":"/ChatGPT%E4%BB%A3%E7%A0%81%E8%A7%A3%E9%87%8A%E5%99%A8/"},{"title":"01 进入AI大门，学会与其交谈","text":"不用问我都知道，你们一定是被ChatGPT的火热出圈导致的开始关注人工智能，也是由于此才看到我这篇文章。 放心，大家想要的我一定会给予，既然大家都想先认识ChatGPT，那么我们就从这个主题开始。 接下来，我们学学如何利用openAI的API来和其沟通。在整个使用过程中，我们都使用的是GPT-3.5的大预言模型。 在本课程中，我们将回答许多问题，例如，OpenAI 的 API 能够实现哪些神奇的事情？OpenAI 的产品被称为已经离通用人工智能（AGI）不远了，它们长什么样子？GPT-3 这样的模型与之前基于深度学习的自然语言处理解决方案有什么不同？我们将通过逐步解释这些问题，使您深入了解这个令人兴奋的领域。 无论您是否是一名程序员，您都可以从本课程中学习如何使用 AI 技术，尤其是大型语言模型，为您的项目和业务提供价值。 基础工作 创建帐号和API Key 了开始学习本课程，您需要先注册一个可以使用 OpenAI 的 API 的账号。您可以通过注册入口进行注册。目前，OpenAI 尚未向中国大陆和香港地区开放，因此您需要自己寻找适当的解决方案进行注册。如果您有更好的解决方案，也欢迎在评论区分享。 注册账号后，您需要点击右上角的账号，然后进入 \"View API Keys\" 页面管理 API Keys。 您可以点击下方的 \"+Create new secret key\" 来创建一个新的 API Key。 您需要将此 API Key 存储在一个安全的位置，因为在后续的 OpenAI 接口调用中，需要使用此 API Key。 储存API Key留用这方便，我使用的是1Password，开了家庭版，很好用。 目前，OpenAI 为所有免费注册的用户提供了 5 美元（原来是 18 美元）的免费 API 额度，足够您体验 API 的功能并完成本课程的学习。如果您想在实际产品中进一步使用此 API，则需要考虑升级到付费账户。 注：在本文完成之时，我发现免费账户已经无法使用免费的API配额了，不仅如此，因为API配额和ChatGPT Plus是两个支付系统，所以Plus并不等同于API 配额，你需要绑定一张信用卡用于支付你的使用量。 未绑卡 已绑卡 关于绑卡这个事，可以自己在网上搜索看，办法总比问题多。不要找我，虽然我有渠道，但是我的渠道很贵，到时候说我骗人钱我可说不清楚。 搭建环境 既然是开发API应用，那必然是需要开发环境的。如果你自己会，那就最好不过了，如果不是太熟悉，可以参考一下我这篇文章： 这篇文章详细的介绍了在Mac内如何搭建AI环境，包括Tensorflow的安装等。 基本上，我们现在需要的是3.10 的Python环境，还有Conda（我习惯用这个），然后在本地安装好Jupyter lab, 如下： 123conda create --name gpt python=3.10conda activate gptconda install -c conda-forge jupyterlab ipywidgets openai 这一段命令的意思是创建一个名为 gpt的python 3.10的开发环境，然后切换到这个环境里，再安装必要的包。 在后面的使用过程中，当然你可以选择jupyter notebook, 也可以和我一样，使用VSCode。 当然，你也可以选择Colab，其实这也是一个Jupyterlab，如果你不想本地搭建环境，那就直接使用Colab吧，不过注意一点，需要科学上网。就算你本地有环境，我还是建议你有些事后使用Colab，能用到一些免费的GPU资源，我的M1没有好的显卡支持，很多时候还是需要上Colab。 使用时候，记得要安装openAI的库，并且设置自己的API Key： 12!pip install openai%env OPENAI_API_KEY=&quot;这里输入你的API Key&quot; 测试一下 让我们现在开始依次写完这段代码，虽然截图内已经有了，但是还是让我们一步步来执行起来，这一段代码，并不是出自我之手，而且直接借鉴的徐文浩的代码： 123456789101112131415161718192021222324252627282930313233import openaiimport json# 设定API Key和模型openai.api_key = &quot;输入你自己的代码&quot;COMPLETION_MODEL = &quot;text-davinci-003&quot;# 设定关键词和描述prompt = &quot;&quot;&quot;Consideration proudct : 工厂现货PVC充气青蛙夜市地摊热卖充气玩具发光蛙儿童水上玩具1. Compose human readale product title used on Amazon in english within 20 words.2. Write 5 selling points for the products in Amazon.3. Evaluate a price range for this product in U.S.Output the result in json format with three properties called title, selling_points and price_range&quot;&quot;&quot;# 写一个调用方法def get_response(prompt): completions = openai.Completion.create ( engine=COMPLETION_MODEL, prompt=prompt, max_tokens=512, n=1, stop=None, temperature=0.0, ) message = completions.choices[0].text return message# 调用方法并打印最终结果print(get_response(prompt)) 然后我们就可以看到返回了： 1234567891011{ &quot;title&quot;: &quot;Glow-in-the-Dark Inflatable PVC Frog Night Market Hot Selling Water Toy for Kids&quot;, &quot;selling_points&quot;: [ &quot;Made of durable PVC material&quot;, &quot;Glow-in-the-dark design for night play&quot;, &quot;Inflatable design for easy storage and transport&quot;, &quot;Perfect for water play and outdoor activities&quot;, &quot;Great gift for kids&quot; ], &quot;price_range&quot;: &quot;$10 - $20&quot;} 这段代码里面，我们调用了 OpenAI 的 Completion 接口，然后向它提了一个需求，也就是为一个我在 1688 上找到的中文商品名称做三件事情。 为这个商品写一个适合在亚马逊上使用的英文标题。 给这个商品写 5 个卖点。 估计一下，这个商品在美国卖多少钱比较合适。 同时，我们告诉 OpenAI，我们希望返回的结果是 JSON 格式的，并且上面的三个事情用 title、selling_points 和 price_range 三个字段返回。 神奇的是，OpenAI 真的理解了我们的需求，返回了一个符合我们要求的 JSON 字符串给我们。在这个过程中，它完成了好几件不同的事情。 第一个是翻译，我们给的商品名称是中文的，返回的内容是英文的。 第二个是理解你的语义去生成文本，我们这里希望它写一个在亚马逊电商平台上适合人读的标题，所以在返回的英文结果里面，AI 没有保留原文里有的“工厂现货”的含义，因为那个明显不适合在亚马逊这样的平台上作为标题。下面 5 条描述也没有包含“工厂现货”这样的信息。而且，其中的第三条卖点 “Inflatable design for easy storage and transport”，也就是作为一个充气的产品易于存放和运输，这一点其实是从“充气”这个信息 AI 推理出来的，原来的中文标题里并没有这样的信息。 第三个是利用 AI 自己有的知识给商品定价，这里它为这个商品定的价格是在 10～20 美元之间。而我用 “Glow-in-the-Dark frog” 在亚马逊里搜索，搜索结果的第一行里，就有一个 16 美元发光的青蛙。 最后是根据我们的要求把我们想要的结果，通过一个 JSON 结构化地返回给我们。而且，尽管我们没有提出要求，但是 AI 还是很贴心地把 5 个卖点放在了一个数组里，方便你后续只选取其中的几个来用。返回的结果是 JSON，这样方便了我们进一步利用返回结果。比如，我们就可以把这个结果解析之后存储到数据库里，然后展现给商品运营人员。 接下来，我们再看一个其他的例子： 1234567prompt = &quot;&quot;&quot;Man Utd must win trophies, says Ten Hag ahead of League Cup final请将上面这句话的人名提取出来，并用json的方式展示出来&quot;&quot;&quot;print(get_response(prompt)) 得到输出结果： 123{ &quot;names&quot;: [&quot;Ten Hag&quot;]} 看出AI干了什么吗？其实从中文中你能知道我需要AI做什么，而他完完全全输出了我想要的。 我们这里的两个例子，其实对应着很多不同的问题，其中就包括机器翻译、文本生成、知识推理、命名实体识别等等。在传统的机器学习领域，对于其中任何一个问题，都可能需要一个独立的机器学习模型。就算把这些模型都免费提供给你，把这些独立的机器学习模型组合到一起实现上面的效果，还需要海量的工程研发工作。没有一个数十人的团队，工作量根本看不到头。然而，OpenAI 通过一个包含 1750 亿参数的大语言模型，就能理解自然的语言输入，直接完成各种不同的问题。而这个让人惊艳的表现，也是让很多人惊呼“通用人工智能（AGI）要来了”的原因。 这两个例子虽然简单，但是咱们暂时先到此为止，记得课后好好练习。 请将今天课程中提供的示例代码，在你搭建的开发环境中运行一下。 你可以去看一下 OpenAI 提供的示例，找几个你感兴趣的用途，在上面的开发环境里运行体验一下，你也可以脑洞大开，尝试一些你想用 AI 解决的问题，看看 AI 能不能给出你想要的结果。 推荐阅读 推荐阅读如果你想知道 GPT 系列大模型到底是怎么回事儿，我推荐你去看一下李沐老师讲解 GPT 系列论文的视频 GPT、GPT-2、GPT-3 论文精读，这个视频深入浅出，能够让你理解为什么现在 GPT 那么火热。","link":"/Enter-the-door-of-AI-learn-to-communicate-with-it/"},{"title":"扩展欧几里德","text":"PE -05欧几里德算法 又名「辗转相除」法 迄今为止已知的最古老的算法, 距今(2017年)2317年 用于快速计算两个数字的最大公约数 还可以用于快速求解ax + by = 1方程的一组整数解 扩展欧几里德 已知算法上推导其它算法的流程, 思想过程才是最重要的;","link":"/Extended-Euclid-algorithm/"},{"title":"Exploring the Potential and Challenges of Hybrid Machine Learning Systems in AI","text":"随着机器学习和深度学习的飞速发展，人工智能（AI）正取得飞跃性进展。然而，越来越多的研究者一致认为，AI演进的下一个阶段在于开发混合机器学习系统。这篇博客文章将探讨这个新兴领域，讨论它的潜力、挑战以及对AI未来的影响。 什么是混合机器学习系统？ 混合机器学习系统结合了两个或更多的机器学习模型或技术，创建出一个更强大、更灵活的AI解决方案。这些系统可以发挥每个组成模型的优势，同时弥补它们各自的弱点。组合机器学习模型的方法有多种，例如： 集成学习：将多个基础模型结合成一个更强大的模型。这可以通过bagging、boosting和stacking等技术来实现。 多模态学习：集成不同的数据来源（例如文本、图像和音频）以创建更丰富的数据表示，并提高整体性能。 迁移学习：利用从一个领域或任务中获得的知识，以改善另一个领域或任务中的性能。 元学习：训练模型学习如何学习，使它们能够更快地适应新任务。 混合系统的潜力 混合机器学习系统有潜力彻底改变AI，为解决复杂问题开辟新的可能性。这些系统的一些关键优势包括： 改善性能：通过结合多个模型，混合系统可以实现比任何单个模型更好的性能。对于过于复杂以至于单个模型无法有效解决的问题尤为如此。 鲁棒性：混合系统可以更好地抵御噪声、过拟合和其他影响单个模型的问题。这在现实世界中的应用中尤其重要，因为数据通常是嘈杂和不完美的。 通用性：混合系统可以处理各种问题、数据类型和任务，这使它们高度适应各种行业和应用。 可迁移性：混合系统可以更轻松地利用从一个领域或任务中获得的知识，以改善另一个领域或任务中的性能，这使它们非常适合具有有限训练数据的任务。 开发混合系统的挑战 尽管有潜力，混合机器学习系统也面临着一些挑战。其中最显著的障碍包括： 复杂性：设计和实施混合系统可能比单个模型更加复杂。研究人员和实践者需要仔细考虑如何最好地组合模型和技术，以创建一个有效的系统。 可伸缩性：混合系统的增加的复杂性可能使它们更难以扩展，无论是在计算资源方面还是处理大量数据的能力方面。 可解释性：混合系统可以更具挑战性地解释和说明，因为它们涉及多个模型和技术的交互。这可能使得理解系统如何做出决策并确保其正确运行变得更加困难。 训练和适应：与训练单个模型相比，训练混合系统可能需要更多的计算资源和时间。此外，将这些系统适应到新任务或不断变化的条件可能需要大量的工作。 总之，混合机器学习系统代表了AI未来的一个有希望的方向。通过利用多个模型和技术的优势，这些系统有潜力在性能、鲁棒性和通用性方面取得重大进展。然而，实现这个潜力需要克服与复杂性、可伸缩性、可解释性和训练相关的挑战。随着研究人员和实践者继续探索这个令人兴奋的领域，我们可以期待在各种行业和应用中看到混合机器学习系统的大量进展。","link":"/Exploring-the-Potential-and-Challenges-of-Hybrid-Machine-Learning-Systems-in-AI/"},{"title":"04 GPT-3&#x2F;4对比其他模型胜在哪？","text":"大家好，我是茶桁。 在前两节课中，我们一起体验了 OpenAI 提供的 GPT-3.5 系列模型的两个核心接口。一个是获取文本的 Embedding 向量，另一个是根据提示语生成补全的文本内容。通过这两种方法，我们可以在零样本或少样本的情况下进行情感分析任务。然而，你可能会有两个疑问。首先，Embedding 不就是将文本转换为向量吗？为什么不直接使用开源模型（如Word2Vec、Bert）而要调用 OpenAI 的 API 呢？在我们的情感分析任务中，我们进行了一些简化处理。一方面，我们排除了相对中性的评分（3分）；另一方面，我们将1分、2分和4分、5分合并，将原本需要判断5个分类的问题简化了。那么，如果我们想要准确预测多个分类，是否也能如此简单呢？ 在本节中，我们将通过代码和数据来回答第一个问题，尝试使用常见的开源模型，看看是否可以通过零样本学习的方式取得良好的效果。至于第二个问题，我们将在下节课中探讨，探索如何进一步利用 Embedding 结合机器学习算法来更好地处理情感分析问题。 什么是预训练模型？ 预训练模型是指通过在大规模文本数据上进行学习而生成的模型，能够将文本转化为语义丰富的向量表示。OpenAI 的 GPT-3 是一种超大规模的预训练模型，其英文全称为“Generative Pre-trained Transformer”，即生成式预训练 Transformer。通过预训练模型，我们可以在没有见过具体问题的情况下，利用大量可用的文本数据进行学习。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/GPT-3-VS-Other-Model/"},{"title":"系列课程：从零开始接触人工智能大模型（介绍）","text":"整个系列课程内容虽然为自己所写，但是参考了bothub 创始人徐文浩的课程《AI 大模型之美》 人工智能是计算机科学领域中最具前瞻性和影响力的技术之一。它是一种智慧型算法，能够模拟人类的思维过程，处理大量的数据和信息，从而发现隐藏在其中的规律和趋势。人工智能的应用范围非常广泛，包括语音识别、图像识别、自然语言处理、机器翻译、智能推荐、智能问答、自动驾驶等等。 人工智能的发展历程可以追溯到上个世纪五六十年代。当时，计算机科学家们开始思考如何让计算机能够像人类一样思考和行动。1956年，美国达特茅斯学院举办了一次名为“人工智能”的会议，正式提出了人工智能的概念。自此以后，人工智能的研究和应用就成为了计算机科学领域的一项重要任务。 随着科技的不断进步，人工智能技术得到了长足的发展。各种机器学习算法、深度学习算法、开源的软件包以及云平台提供的解决方案不断涌现，为企业提供了各种智能化的产品和服务。例如，通过图像识别技术，我们可以将人脸识别、车牌识别、病变识别等技术应用于安防、交通、医疗等领域；通过自然语言处理技术，我们可以实现智能客服、智能翻译、智能问答等功能，提升用户体验和效率；通过机器学习技术，我们可以实现推荐系统、广告精准投放等功能，提高市场竞争力。 然而，人工智能领域仍然存在着挑战和困难。其中一个难点就是“有多少人工就有多少智能”这个诅咒。很多“智能”都来自于大量的人工数据标注和硬编码的业务规则，导致人工智能在某些特殊情况下表现得像“人工智障”。因此，如何提高人工智能的效率和性能，仍然是人工智能领域需要解决的问题之一。 去年 12 月，我第一次尝试与 ChatGPT 进行交互。一开始我并没有对这个新的 AI 聊天机器人抱有太高的期待，毕竟以前的聊天机器人总是表现得像“人工智障”。但是，ChatGPT 证明了我的想法是错误的。在与它交流了几分钟后，我决定让它帮我写一些 Bash 脚本和 SQL 代码。我很惊讶地发现，它不仅完全理解了我的需求，还精确地写出了我需要的复杂 SQL 代码。这次体验让我对人工智能的潜力有了新的认识，也让我更加期待未来人工智能的发展。 对于复杂的需要窗口函数的 SQL，ChatGPT 写得比我快多了。 从年前到目前（5月份）为止，我一直在体验市面上大部分的人工智能工具，例如最近非常火热的ChatGPT等。这样的体验让我对人工智能的潜力有了新的认识，也让我开始思考各行业未来的前景，并提出了一些担忧。当我们看到人工智能技术不断取得突破性进展的同时，也会担心它是否会取代人类的工作，进而给社会带来不稳定的因素。然而，我相信人工智能的发展是为了更好地服务人类，而非取代人类。我们需要更多人去了解和掌握人工智能技术，这样才能更好地应对未来的挑战，发挥人工智能技术的最大价值。 基于此，我想让更多人开始接触人工智能，并且学会如何利用人工智能，更甚为学习新一代AI应用编程。因此，我想介绍一门系列课程：从零开始接触人工智能大模型。该课程将介绍人工智能的基本概念、常见应用场景以及如何利用最新的AI技术构建自己的AI应用。我们将从浅入深地讲解人工智能相关的知识，帮助每个人都能够轻松上手，并且学会如何应用到自己的领域中去。不仅仅是算法工程师和机器学习研究人员，每个工程师都可以快速学习并参与开发新一代的AI应用。我相信，学会开发新一代AI应用是每个软件开发行业从业者都值得学习的，无论是产品经理还是工程师，乃至于行业之外的业务人员，都应该拥抱新的AI浪潮。 在学习的过程中，我们将讨论人工智能的应用场景，例如语音识别、图像识别、自然语言处理、机器翻译、智能推荐、智能问答、自动驾驶等等。这将有助于我们了解人工智能技术在不同领域的应用，从而更好地把握未来的发展方向。同时，我们也会学习最新的人工智能技术，例如大模型、自监督学习等等。这些技术的出现，为人工智能的应用提供了更加广阔的空间和深度。 在这个充满挑战和机遇的时代，我们需要准备好迎接未来的挑战。学习人工智能技术，是每个软件开发行业从业者都需要掌握的技能。无论是产品经理还是工程师，乃至于行业之外的业务人员，都应该拥抱新的AI浪潮，学习开发新一代的AI应用。我相信，通过学习新一代的AI应用编程，我们能够更好地应对未来的挑战，为我们的生活和工作带来更多的便利和机遇。 学习成本那么高，给我一个理由先 这个应该是普遍的一个想法，其实对于此，我将不仅给你一个理由，而是给你三个： 1. 开发门槛降低，人人可学习AI应用开发 人人都应该学习如何开发新一代 AI 应用，因为这一轮的 AI 浪潮里，开发新的 AI 应用的门槛大大降低了。过去，AI 应用开发是一个门槛比较高的领域，需要掌握大量的机器学习和深度学习的知识，了解各种基础模型，使用各种机器学习的编程框架，以及理解在实际应用里锤炼机器学习的各种实战技巧和模型。对于没有相关经验的人来说，不花上个一两年时间，你可能很难用 AI 算法做出有价值的产品。 但是现在，随着预训练好的大型基础模型的出现，以及这些模型的能力通过开放 API 的形式提供出来，即使没有任何机器学习的理论知识，你只需要一两天时间，就能做出一个能解决实际问题的 AI 应用。比如，最近在 GitHub 上就能看到很多工程师，花上 1-2 天时间就做出来的图书翻译、人工智能语音对话的应用。 这样的开发方式，让更多的人有机会参与到 AI 应用的开发中来。无论你是产品经理、UI/UE 设计师、前端开发、后端开发还是大数据团队的人员，都可以通过学习一些基本的 AI 应用开发技能，为自己的职业生涯增添新的技能和竞争力。特别是在当前的科技革命和数字化转型浪潮下，AI 技术已经逐渐渗透到各个行业中，很多企业已经开始了 AI 落地实践，而能够掌握 AI 技术的人才也逐渐成为各个行业中的稀缺资源。因此，学习如何开发新一代 AI 应用，也是提升自己职业竞争力的一种重要途径。 最后，学习如何开发新一代 AI 应用还可以让我们更好地了解 AI 技术的本质和应用，拓宽我们的知识面和视野。AI 技术正在改变我们的生活和工作方式，了解和掌握这些技术，也有助于我们更好地适应未来的发展和变化。 学习如何开发新一代 AI 应用对于个人的职业发展和未来非常重要，因为 AI 技术已经开始在各个行业得到广泛应用。无论你从事什么行业，都可以利用 AI 技术来提高效率、降低成本、提供更好的服务，并在竞争中脱颖而出。掌握 AI 技术也可以让你在未来的就业市场上更有竞争力，拥有更广泛的职业选择。因此，学习如何开发新一代 AI 应用可以为个人的职业发展和未来打下坚实的基础。 2. 站在巨人的肩膀上 随着人工智能技术的迅猛发展，AI应用开发的范围也越来越广泛，涉及到自然语言处理、计算机视觉、语音识别等多个领域。这一轮的AI浪潮已经开始让我们看到了通用人工智能（AGI）的雏形，AI应用的覆盖领域被大大扩展了，几乎任何一个问题都有机会通过AI来解决优化。 过去，机器学习模型的应用通常局限于某一个细分领域上的进步，而且对于每一个具体问题都要单独收集数据、训练单独的机器学习模型来解决里面某一个小问题。然而，随着计算能力的提高和模型规模的增加，现在拥有海量参数的大模型已经开始成为主流。例如，2020年发布的GPT-3模型拥有1750亿个参数，可以无需任何微调，就能解决情感分析、命名实体识别、机器翻译等一系列的自然语言处理问题。同时，对于很多AI没有见过的问题，只要通过自然语言给到AI几个例子，通过\"小样本学习\"，AI就能给出正确的回答。这意味着，一个大模型可以一次性解决不同类型的很多问题。 在计算机视觉上，像2021年OpenAI发布的CLIP模型也有类似的效果。通过4亿个（图片、文本）对的训练，对于图片的分类可以任意扩展，而不需要预先标注。这样的模型使得我们对于图片的分类不再局限于预先的人工数据标注的类别，而是可以扩展到任何类别上去。 这种发展趋势使得AI应用开发的门槛逐渐降低，使得普通人也能够参与到AI应用的开发中来。无论你所在的行业和领域，都有机会通过简单的AI应用开发，提升效率和产出。同时，了解和掌握AI技术也成为了一种职业竞争力，可以帮助你更好地适应未来的工作环境。 总之，AI技术的广泛应用和快速发展已经让AI应用开发成为一个非常重要的技能。了解AI技术的人，无论是在工作中还是在日常生活中，都会受益匪浅。 3. 人工智能对我们的工作的影响 人工智能（AI）已经开始以多种方式改变我们的生活。我们已经习惯了依赖AI进行日常任务，如编写代码、翻译文本，甚至为文章生成图像。然而，AI的影响超出了我们的个人生活，它将对我们的工作产生重大影响。 随着AI的不断发展，它不可避免地将接管许多以前由人类执行的任务。公司已经在使用AI来优化产品描述、搜索算法和其他曾经是人工工作者领域的任务。这无疑会导致工作的流失，并改变许多人的工作性质。 尽管存在工作流失的可能性，但那些拥抱AI的人无疑将会获得好处。使用AI的团队和公司很可能会看到更高的效率和生产力，从而导致更大的产出和成功。无论您是产品经理、工程师、运营专家还是平面设计师，AI的出现都将从根本上改变您的工作性质。 AI将作为助手，帮助我们完成简单的基于知识的任务，甚至提供创造性的灵感。事实上，有些人将AI的发展与工业革命相比，标志着我们生活和工作方式的根本变化。虽然这可能会对一些工人造成危机，但有机会拥抱这种变化并学习在以AI为驱动的未来需求的新技能。 正如2008年App Store的发布创造了对移动应用程序开发人员的需求一样，AI革命为那些愿意学习和适应的人带来了新的机会。无论是获得机器学习、数据科学还是其他与AI相关的领域的专业知识，那些积极应对不断变化的就业市场的人无疑会蓬勃发展。 AI对我们的工作的影响不容小觑。尽管有些人可能将其视为危机，但有机会拥抱这种变化并学习在以AI为驱动的未来需求的新技能。每个人都可以决定如何应对这种变化，以及他们是否会抓住它带来的机会。 如何学习呢？ 新一代AI应用开发是一个快速发展的领域，需要不断更新自己的知识和技能。而通过实践学习是最有效的方法之一，因为它可以让你在实际解决问题的过程中学习和掌握技能。 这门课程采用实践教学的方式，让学生能够亲自动手解决一系列实际问题，如情感分析、记忆聊天机器人、图像搜索等。学生们将通过编写几行或几十行的代码来解决这些问题，并在在线Notebook的环境下进行代码运行，无需搭建复杂的开发环境。即使你是产品经理或业务方，也可以轻松地体验到新一代AI应用的开发过程，从而更好地理解和掌握其工作原理。 除了OpenAI的API外，这门课程还涵盖了语音、视觉等应用场景，包括语音识别、语音合成、AI绘画等。学生们将了解到如何使用开源模型以及如何根据自己的数据微调这些模型，从而更好地满足不同场景下的需求。 此外，这门课程还将探讨AI应用的套路和方法，例如分类、搜索、推荐、问答等问题。学生们将学习如何使用现有模型的能力来解决这些问题，并将这些方法和套路应用到现有的业务系统中，以提高应用的体验和效率。 随着课程的深入，学生们还将学习如何组合多个API、开源模型和开源库来解决复杂的真实问题。例如，如果你想实现一个电商客服，不仅需要检索知识库和问答的能力，还需要连接现有的订单和物流信息的能力。学生们将学习如何在AI应用的开发过程中将复杂的业务流程串联起来，以更好地应对实际问题。 拥抱新时代，接受“通用人工智能” 随着人工智能技术的不断进步，越来越多的人开始认识到“通用人工智能”（AGI）的重要性和可能性。如今，AGI已经不再是一个遥不可及的概念，而是一个即将到来的现实。我们可以看到，各个领域的科学家和工程师正在努力推进AGI的研究和应用，希望通过人工智能技术的创新和发展来实现这一目标。 在这个变化迅速的时代，我们需要尽快拥抱AGI。AGI可以帮助我们解决许多复杂的问题，并且能够极大地改善我们的生活质量。例如，我们可以使用AGI来开发更为智能的医疗设备，提高医疗诊断的准确性和效率；我们也可以利用AGI来优化城市交通，减少交通堵塞和污染；此外，AGI还能为我们提供更好的教育和娱乐体验，让我们的生活更加丰富多彩。 因此，我们需要尽快投入时间和精力来学习和应用AGI技术。学习AGI不仅可以让我们掌握更加先进的技能和知识，还可以激发我们的创新和热情，让我们更好地适应这个变化迅速的时代。我们需要通过学习和实践，尽快将AGI技术应用到我们的工作和生活中，让其发挥最大的价值。 最后，我希望每一个人都能积极地拥抱AGI技术，努力学习和应用这项技术，为推动人工智能技术的发展贡献自己的力量。相信在不久的将来，AGI将成为我们生活中不可或缺的一部分，让我们共同期待并努力实现这一目标。 目录 注： 这里将会是我未来所有系列教程的目录，便于大家更快找到相关章节 导读：了解AI并使用它/他/她们","link":"/Gettin_started_with_large-scale_artificial_intelligence_models_from_scratch/"},{"title":"09 使用Embedding实现语义检索","text":"Hi，我是茶桁。 过去的8讲，你已熟悉Embedding和Completion接口。Embedding适合用于机器学习中的分类、聚类等传统场景。Completion接口可以用作聊天机器人，也可以用于文案撰写、文本摘要、机器翻译等工作。 然而，很多同学可能认为这与他们的日常工作无关。实际上，我们通常在搜索、广告、推荐等业务中使用自然语言处理技术。因此，我们今天来看看如何使用OpenAI的接口来帮助这些需求。 当涉及到优化搜索结果时，OpenAI的Embedding接口可以提供有价值的功能。Embedding接口能够将文本转换为表示其语义特征的向量，这些向量可以用于比较文本之间的相似性，从而优化搜索结果的排序和相关性。 首先，使用OpenAI的嵌入接口，您可以将搜索查询和搜索结果中的文本转换为嵌入向量。通过比较查询向量与结果向量之间的相似度，您可以重新排列搜索结果，以提供更相关和有用的结果。这可以帮助用户更快地找到他们想要的信息，并提供更好的搜索体验。 其次，OpenAI的嵌入接口还可以帮助您改进搜索结果的相关性。通过将用户的上下文和历史记录与搜索查询结合起来，您可以生成更具个性化和定制化的搜索结果。使用嵌入接口，您可以将用户的上下文信息转换为嵌入向量，并与查询向量进行比较，以确定最相关的结果，并在搜索结果中突出显示这些个性化的内容。 此外，OpenAI的嵌入接口还可以用于相似性搜索和聚类分析。您可以使用嵌入向量来比较不同文本之间的相似性，并将相似的文本聚集在一起。这有助于在搜索结果中提供更多相关的选项，并帮助用户发现相关但可能未被明确搜索的内容。 下面，就让我们来一步步的实现： 生成实验数据 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Implementing-semantic-retrieval-using-Embedding/"},{"title":"2. 初识Python脚本","text":"Python的系列课程是写给零基础的小白看的，如果基础比较好的小伙伴可以暂时先不关注，等待后续课程。 Hi， 大家好，我是茶桁。 之前的课程已经给大家讲解了Python相关特性和基本语法。那么这节课呢，我们尝试着从最简单的脚本来开始认识Python。 在开始这节课之前呢，我是默认大家已经安装好了VSCode，并且配置好了Jupyter环境和Python的基本环境。如果在这一步有疑问的小伙伴，可以留言私信我。 我们在终端里输入: 12$ python -VPython 3.10.11 可以看到自己的Python版本。 这个时候，我们其实已经可以在终端里进行Python的代码编辑了，如下： 123$ python&gt;&gt;&gt; print(&quot;Hello AI Cheats&quot;)Hello AI Cheats 如下图： 我们这里需要理解一下，编写python程序的文件，称为python的脚本或程序。要求我们当前的python脚本的文件后缀名必须是.py，如果是Jupyter的文件，后缀是.ipynb print() 输出函数 print可以在程序中输出一些内容，如字符串，数字等等。 函数就是为了完成一些功能，例如： print就是为了输出数据。 变量 变量就是用一个英文字符串来记录或标记一些数据，并且这个被标记的数据是可以变化的。 比如 num = 10，就是把数据10赋值给了变量num来使用，之后就可以使用num来代替这个10的数据。 命名规范 这里我们强调一下Python的命名规范，所有在教授编程的教程中最初一定都会强调规范性。 变量的命名规范如下： 变量名可以使用字母，数字，下划线_， 不能以数字开头 严格区分大小写 不要使用中文 不要使用关键字 if else True False print 当然具体关键字并不仅仅是这些，这里列出了Python的关键字，大家可以自行查看一下，命名的时候需要进行避免。 变量的定义方式 在遵循了变量命名规范之下，我们可以有以下几种定义方式： 123456# 第一种定义方式a = 10b = 20# 第二种定义方式a,b = 30, 40 这里我们来思考一个问题，如何实现两个变量的数据相互交换呢？ 123456# 定义两个变量a = 10b = 20# 交换两个变量的值... 如果使用最普通的方式完成变量数据的交换，那么我们可以使用如下步骤： 把a变量的值 赋值给c ，此时 c变量中 就是 10 把b变量的值 赋值给a ， 此时 a变量中 就是 20 把c变量的值 赋值给b ， 此时 b变量中 就是 10 123456789# 普通方式交换数据a = 10b = 20c = aa = bb = cprint(a, b) 输出结果： 120, 10 我们还可以利用python定义比变量的语法来实现变量的数据交换 12345# 定义比变量的语法方式a = 10b = 20a,b = b,aprint(a, b) 输出结果： 120,10 Python的数据类型 数据类型就是数据的表现形式，比如 “你好” 就是一个字符串，200 就是一个数字。 在程序当中除了这种常用的字符和数字外还有很多其它的数据表现形式。 在Python中，我们可以使用type()函数来返回当前数据的数据类型： 123s = 'ilovechaheng'res = type(s)print(res) 输出结果： 1&lt;class 'str'&gt; 字符串类型 单双引号都可以定义字符串 三引号也可以定义字符串 单双引号定义的字符串不能随意换行，需要在换行时指明换行符 字符串中的引号可以互相嵌套，但是不能嵌套自己（例如不能在单引号中嵌套单引号，除非转义） 字符串中可以使用转义字符，如 .. 如果不想在字符串中实现转义字符可以在字符定义时 加 love = r'\\nihao \\shijie' 123456789# 单引号和双引号进行定义love = 'iloveyou'hello = &quot;你好 世界&quot;# 使用三引号实现大字符串的定义，一般用于大文本字符串的定义，并且大字符串，可以换行s = '''比如说这一个很长很长的文章内容。。。''' 数字类型 int 整型 float 浮点类型 complex 复数 bool 布尔类型（True，False） 12345678910111213141516171819# 数字类型 Numbervarn = 521varn = -1111varn = 3.1415926varn = 0x10 # 十六进制varn = b'001100111' # bytes# 复数varn = 5+6j # complex# 布尔类型 boolvarn = Truevarn = False# print(varn,type(varn))# 数值类型可以参与运算a = 10b = 20print(a+b) # 输出结果 30 List列表类型 列表用来表示一系列数据，例如： 需要记录一组数字或其它数据 列表中存储的数据可以是任意类型的 在需要记录多个数据时，可以使用中括号进行定义 [], 并且每个数据之间使用逗号分隔 , 例如以下数据，定义了几组数字 列表中存储的每一组数据，称为元素 列表中存储的数据，可以通过下标的方式进行获取 那么列表中元素的值可不可以存储一个列表,称为 二级列表（二维列表） 或者 多级列表 （多维列表） 关于列表中的下标，正读和反读的正负号是不一样的： 12345678# 关于列表中的下标''' 0 1 2 3 4 ['a','b',521,'pai',3.1415926] -5 -4 -3 -2 -1'''a = ['a','b',521,'pai',3.1415926]print(a[-3]) 输出结果： 1521 tuple 元组类型的定义 在定义多个数据内容时，可以选择使用List列表类型 还可以使用元组类型来定义， 元组和列表非常像，都时用于存储多个数据时使用 元组使用小括号进行定义（），列表使用中括号进行定义 元组的最大特点就是值不能被改变 123vart = (1,2,3,'a','b')# 元组的其它定义方式vart = 1,2,3 注意在定义元组时，如果元组中只有一个元素，那么需要加, 不然就不是元组类型了 Dict字典类型 字典也是用于存储一组或多组数据时使用，使用大括号 {}来定义 字典是 键值对 的存储方式 name ：admin 键和值之间使用冒号进行分隔，多组键值对之间使用逗号分隔 键必须是字符串或数字类型，值可以是任意类型 键名不能重复，值可以重复 12345678910# 比如需要记录一本书的相关数据 书名，作者，价格，。。。vard = {'title':'&lt;&lt;鬼谷子&gt;&gt;','author':'鬼谷子','price':'29.99'}# print(vard,type(vard))# {'title': '&lt;&lt;鬼谷子&gt;&gt;', 'author': '鬼谷子', 'price': '29.99'} &lt;class 'dict'&gt;# 获取字典中的值print(vard['title'])# 字典中的键不能重复使用，否则会覆盖vard = {'a':10,'b':10,'c':20,'a':'aa',1:'abcdef','2':'2222'}print(vard) 输出结果： 12&lt;&lt;鬼谷子&gt;&gt;{'a': 'aa', 'b': 10, 'c': 20, 1: 'abcdef', '2': '2222'} 在python之前的版本中，字典是无序的 set集合类型 set集合是一个 无序且元素不重复的 集合的数据类型 set集合使用 中括号或者set()方法来定义 如果需要定义一个空集合时 只能使用 set()方法,因为大括号时定义的空字典 集合主要用于运算，交集，差集，并集，对称集合 123456789101112131415161718a = {1,2,3,'a'}# 给集合添加元素# a.add('b')# 无法获取集合中的单个元素，但是可以添加和删除# a.discard('a')# print(a)# 检查当前的元素是否在集合中# print(1 in a)# 集合主要用于运算，交集，差集，并集，对称集合a = {1,2,3,'a','b'}b = {1,'a',22,33}print(a &amp; b) # 交集 {1, 'a'}print(a - b) # 差集 {'b', 2, 3} a 集合有，b集合没有的print(a | b) # 并集 {1, 2, 3, 33, 'a', 'b', 22} 两个集合，放到一起，并且去重print(a ^ b) # 对称差集 {33, 2, 3, 'b', 22} 输出结果： 1234{1, 'a'}{2, 3, 'b'}{1, 2, 3, 'a', 33, 22, 'b'}{33, 2, 3, 22, 'b'} 总结 最后，让我们来进行总结一下，关于Python的数据类型可以查看如下列表： 12345678910111213141516字符串 string数字类型 Number 整型 int 浮点 float 复数 布尔 bool列表 list元组 tuple字典 dict集合 set可变数据类型：列表，字典，集合不可不数据类型： 字符串，数字，元组容器类型数据 ： 字符串，列表，元组，集合，字典非容器类型数据： 数字，布尔类型 数据类型转换 什么是数据类型转换？ 把一个数据类型转换为另一个数据类型，例如 字符串转为数字 为什么需要数据类型转换？ 因为不同的数据类型之间不能运算 数据类型转换的形式？ 自动类型转换 强制类型转换 自动类型转换 当两个不同的值进行运算时，结果会向更高的精度进行计算：True ==&gt; 整型 ==&gt; 浮点 ==&gt; 复数 12345a = 123b = True # 在和数字运算时 True转为数字1，False转为数字 0print(a+b)print(12.5+22)print(True+3.14) 输出结果： 12312434.54.140000000000001 强制类型转换 python中的每个数据类型都有对应的方法，可以对数据类型进行转换 str()可以把所有的其它数据类型转换为字符串类型 int()字符串转数字类型时，如果字符串中是纯数字，可以转换，其它容器类型不能转为数字int类型 float()浮点类型的转换和int类型一样，不过转换的结果是浮点类型 bool() 可以把其它类型转换布尔类型的True或False 以下情况转bool的结果是 False: '',0,0.0,False,[],{},(),set() list()列表 数字类型是 非容器类型，不能转换为列表 字符串 转换为列表时 会把字符串中的每一个字符当做列表的元素 集合 可以转换为 list列表类型 元组 可以转换为 list列表类型 字典 可以转换为 list列表类型,只保留了字典中的键 tuple()元组 数字类型 非容器类型，不能转换为元组 其它容器类型的数据进行转换时，和列表一样 set()集合 数字类型 非容器类型，不能转换为 集合 字符串,列表，元组 可以转为 集合 结果是无序的 字典转换为集合时，只保留了字典的键 key dict()字典 数字类型 非容器类型，不能转换为 字典 字符串不能直接转换为 字典 列表可以转换为字典，要求是一个二级列表，并且每个二级元素只能有两个值 元组可以转换为字典，要求是一个二级元组，并且每个二级元素只能有两个值","link":"/Introduction-to-Python-scripting/"},{"title":"在 Apple Silicon M1&#x2F;M2 Mac 上安装和运行Stable Diffusion","text":"说实话，我找了好多关于如何在 M1/M2 上安装和运行 Stable Diffusion 的教程和帖子，发现相互之间借鉴的不少，但是能用的确实没几个。 寻找一番后，发现其实没那么复杂。也不知道为什么网上的那么多教程搞得那么复杂，又是这个又是那个的一大堆，简单实现的方式有好几种： 1. Diffuers 这是可以在 App Store 上直接搜索并下载的一个 App，看评分和排名似乎都不太好，开发者却是「Hugging Face」，其实在官方的 Github 上就有其下载链接：‣ 这应该是体验 Stable Diffusion最简便的方式了吧。而且还支持选择 Model， 不过有点遗憾的点是没办法调整参数。 2. DiffusionBee 这是出现的比较早的一款第三方 App，使用起来也是特别简单，直接下载安装就行了： DiffusionBee - Stable Diffusion App for AI Art DiffusionBee is the easiest way to generate AI art on your computer with Stable Diffusion. Completely free of charge. https://diffusionbee.com/download 目前不止是 MacOS，还有对应 Windows 64 Bit 的版本，而且，你可以选择下载 HQ Version 版本。官方对其说的是速度慢两倍，但是图像质量更好。 以上两个 App 第一次使用的时候都是需要下载 Model 的，之后就可以直接开心的玩耍了，相比较而言，DiffusionBee 在参数选择上要多一点。支持 Text to Image, Image To Image等。 # 安装 AUTOMATIC1111 这个方法是需要有一点动手能力了，不过相比较而言，也不是那些网站上介绍的那么繁琐。其实就只需要几步而已。 1. 下载 Homebrew 一个包管理器，不太明白的朋友不需要管那些，操作就行了 打开你的终端，不明白什么是终端直接在你的搜索框里输入”终端”，或者”Terminal”, 就能看到了。 然后直接把下面的代码粘贴进去，回车，看着他跑就行了 1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; 2. 安装一些必须的软件包 等上面步骤跑完之后，再复制下面的代码，一样粘贴进去回车看着他跑： 1brew install cmake protobuf rust python@3.10 git wget 3. 下载 AUTOMATIC1111 存储库 等上面步骤跑完，在你的终端输入一下代码并回车 1cd ~ 以上命令是为了让你进入你在 Mac 电脑上的账户主目录，就是这个地址 1/User/xx/ 然后我们接下来的操作会在你这个目录下下载一个文件夹，名字叫 「stable-diffusion-webui」，这贴下面代码到你的终端里，然后回车 1git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui 命令跑完后，你就会渐渐自己名字的目录下多了一个 stable-diffusion-webui 的目录了，然后在终端进入这个目录，操作方法和上面一样 1cd ~/stable-diffusion-webui/ 在你的访达里你也进入这个目录，然后继续进入 models/Stable-diffusion， 这里是存放 Model 的地方。现在你需要下载一个 Model 存放进去，你可以直接在这里下载 1.5 model, 当然，如果你需要 2.1 的或者其他 model，可以去点击下面的链接进去自己下载一个合适的，然后扔到目录里。 Models - Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/models?sort=downloads&amp;search=stable+diffusion 然后你的目录应该会是这样： 然后你就可以尝试着跑你的 stable diffusion 了，刚才我们在终端里进入了 ~/stable-diffusion-webui/， 假设你还在这个位置，我们就可以直接输入： 1./webui.sh 然后去干些自己的事情吧，喝杯茶，看看书。要跑一会呢，特别是你网络不好的情况。 等到终端命令全部跑完后，打开你的 Safari，输入：http://127.0.0.1:7860/ 好了，可以把玩了。 4. 其他 是的，还没结束，还有一些要说的，其实在 Mac App Store 里搜索的话，你还能看到一些其他的 App 可以直接使用，比如： 反正都是免费的，尽量多试试，找到一个自己满意的。 另外，不管你用那种方法，你都需要知道一些good prompts for Stable Diffusion, 这里有一个地方可以看些别人的例子，不过不是那么容易打开： arthub.ai https://arthub.ai/ 还有啊，自己可以多测试一些 model，下下来把玩下。 祝你玩的愉快。全文完。","link":"/How-to-install-and-run-Stable-Diffusion-on-Apple-Silicon/"},{"title":"Knock 升级 -- 快速输入管理员密码","text":"本文知乎专栏 还记得之前介绍过的Knock么？只需要敲击两下就能快速解锁Mac的app。 其实这款App还是 @Rachel 介绍给我的，当时就觉得很酷，可是用下来之后，并没有觉得有很高的实用性。 不过，这次Knock升级了，除了解锁Mac之外，还可以再你需要root权限的时候免去输入管理员密码的麻烦，你所需要的，仅仅是敲击两下你的iPhone。 好吧，现在可以为自己的Mac设定一个超级复杂的密码了。","link":"/Knock-update/"},{"title":"16. Langchain让AI拥有记忆力","text":"你好，我是茶桁。 在之前的两讲中，我们深入了解了 Langchain 的 LLMChain 核心功能，它可以帮助我们链式地调用一系列命令，包括直接调用 OpenAI 的 API、调用其他外部接口或自己实现的 Python 代码。但这只是完成一个小任务所需的调用序列。除了这些，LangChain 还封装了许多其他功能，以便于我们开发 AI 应用。例如，让 AI 有“记忆力”，即记住我们的聊天上下文。我们在第 6 讲中制作的聊天机器人的例子就是这样。为了让 ChatGPT 知道整个聊天的上下文，我们需要将历史对话记录传递给它。但由于 Token 数量有限，我们只能保留最后几轮对话。最终，我们将此功能抽象为一个 Conversation 类。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Langchain%E8%AE%A9AI%E6%8B%A5%E6%9C%89%E8%AE%B0%E5%BF%86%E5%8A%9B/"},{"title":"LLMs的实用介绍","text":"在实践中使用LLMs的3个级别 这是关于在实践中使用大型语言模型（LLMs）系列文章的第一篇。在这里，我将介绍LLMs并提出三个使用它们的级别。未来的文章将探讨LLMs的实际方面，例如如何使用OpenAI的公共API、Hugging Face Transformers Python库、如何微调LLMs以及如何从头构建LLMs 什么是LLM？ LLM 是 Large Language Model 的缩写，是人工智能和机器学习中的最新创新。这种强大的新型人工智能在2022年12月随着 ChatGPT 的发布而迅速传播开来。 对于那些生活在人工智能热潮和技术新闻周期之外的人来说，ChatGPT 是运行在名为 GPT-3 的 LLM 上的聊天界面（现在在撰写本文时已升级到 GPT-3.5 或 GPT-4）。 如果你使用过 ChatGPT，显然这不是来自 [AOL Instant Messenger]（https://en.wikipedia.org/wiki/AIM_(software)) 或你的信用卡客服的传统聊天机器人。 这个聊天机器人感觉不同。 什么使得LLM“大”？ 当我听到“大型语言模型”这个术语时，我的第一个问题是，这与“常规”语言模型有何不同？ 语言模型比大型语言模型更通用。就像所有正方形都是矩形，但并非所有矩形都是正方形一样。所有LLM都是语言模型，但不是所有语言模型都是LLM。 所以LLM是一种特殊的语言模型，但是什么使它们与众不同呢? 有2个关键属性区分LLMs与其他语言模型。一个是数量上的，另一个则是质量上的。 数量上，LLM的区别在于模型中使用的参数数量。目前的LLM大约有10-1000亿个参数[1]。 质量上，当语言模型变得“大”时，会发生一些非凡的事情。它会展示出所谓的*** emergent properties***例如零-shot学习[1]。这些是当语言模型达到足够大的规模时，似乎突然出现的特性。 零样本学习 GPT-3（以及其他LLM）的主要创新在于它能够在各种情境下进行零样本学习[2]。这意味着ChatGPT可以执行一个任务，即使它没有被明确训练过。 尽管这对我们这些高度进化的人类来说可能不是什么大不了的事情，但是这种零样本学习能力与之前的机器学习范例形成了鲜明对比。 以前，为了获得良好的性能，模型需要明确地在它所要完成的任务上进行明确的训练。这可能需要1k-1M个预标记的训练示例。 例如，如果你想让计算机进行语言翻译、情感分析和识别语法错误。每个任务都需要一个专门的模型，它需要在大量标记示例的基础上进行训练。然而，现在，LLM可以在没有明确训练的情况下完成所有这些任务。 LLM如何工作？ 训练大多数最先进的LLM所使用的核心任务是单词预测。换句话说，给定一序列单词，下一个单词的概率分布是什么？ 例如，给定序列Listen to your ____，最有可能的下一个单词可能是：heart，gut，body，parents，grandma等。这可能看起来像下面显示的概率分布。 有趣的是，这是许多（非大型）语言模型过去被训练的方式（例如GPT-1）[3]。然而，由于某种原因，当语言模型超过一定大小（例如~10B个参数）时，这些（新生的）能力，例如零-shot学习，开始出现[1]。 尽管目前还没有明确的答案，解释为什么会发生这种情况（只有推测），但明显LLM是一种强大的技术，具有无数的潜在用例。 使用LLM的3个层次 现在我们来看看如何在实践中使用这种强大的技术。虽然有无数的LLM用例，但在这里，我将它们按所需的技术知识和计算资源排序为3个层次。我们从最容易使用的开始。 一级：提示工程 使用LLM的第一级别是“提示工程”，我将其定义为“任何使用LLM的开箱即用方式”，即不更改任何模型参数。虽然许多技术倾向的个人似乎对提示工程的想法不屑一顾，但这是实际中使用LLM（在技术和经济上）最可访问的方法。 有两种主要的提示工程方式： 简单方式 和 较不简单方式。 简单方式：ChatGPT（或其他方便的LLM UI） - 这种方法的关键好处是方便。像ChatGPT这样的工具提供了一种直观，免费且无代码的使用LLM的方法（没有比这更容易的方法了）。 然而，方便通常是有代价的。在这种情况下，这种方法有两个主要缺点。第一个是缺乏功能。例如，ChatGPT不容易使用户自定义模型输入参数（例如温度或最大响应长度），这些值调节LLM输出。第二，与ChatGPT UI的交互不能轻松地自动化，因此无法应用于大规模使用情况。 虽然这些缺点可能是某些用例的杀手级应用，但如果我们将提示工程向前推进一步，这两个缺点都可以得到改善。 较不简单方式：直接与LLM交互 - 我们可以通过编程接口直接与LLM进行交互来克服ChatGPT的一些缺点。这可以通过公共API（例如OpenAI的API）或在本地运行LLM（使用像Transformers这样的库）来实现。 虽然这种提示工程方式不太方便（因为它需要编程知识和潜在的API成本），但它提供了一种可定制，灵活和可扩展的使用LLM的方法。本系列文章将讨论付费和免费的方法来进行此类提示工程。 尽管提示工程（如此定义）可以处理大多数潜在的LLM应用程序，但依赖通用模型可能会导致特定用例的次优性能。对于这些情况，我们可以进入使用LLM的下一个级别。 等级 2：模型微调 使用 LLM 的第二个等级是模型微调，我定义为对现有 LLM 进行微调以用于特定用例，通过改变至少一个（内部）模型参数，即权重和偏差。在此类别中，我还将在此处将迁移学习即使用现有 LLM 的某些部分来开发另一个模型。 微调通常包括两个步骤。步骤 1：获得预先训练的 LLM。步骤 2：基于给定的特定任务更新模型参数（通常是数千个）高质量标记的示例。 模型参数是定义 LLM 对输入文本的内部表示的。因此，通过针对特定任务调整这些参数，内部表示变得针对微调任务进行了优化（或者至少是这样的想法）。 这是一种强大的模型开发方法，因为相对较少的示例和计算资源可以产生出色的模型性能。 然而，缺点是它需要比提示工程更多的技术专业知识和计算资源。在未来的一篇文章中，我将尝试通过审查微调技术并共享示例 Python 代码来缓解这种缺点。 虽然提示工程和模型微调可能可以处理 LLM 应用程序的 99％，但有时必须走得更远。 等级 3：构建自己的 LLM 在实践中使用 LLM 的第三种最终方法是构建自己的。在模型参数方面，这是您从头开始制定所有模型参数的地方。 LLM 主要是其训练数据的产物。因此，对于某些应用程序，可能需要策划自定义的高质量文本语料库进行模型训练，例如医学研究语料库，用于开发临床应用程序。 这种方法最大的优点是您可以完全自定义 LLM 以适用于您的特定用例。这是终极的灵活性。但是，通常情况下，灵活性的代价是方便性。 由于LLM 性能的关键是规模，因此从头开始构建 LLM 需要巨大的计算资源和技术专业知识。换句话说，这不会是一个个人周末项目，而是一个完整的团队工作数月甚至数年，预算达到 7-8F。 尽管如此，在我未来文章中，我希望探讨从头开始开发 LLM 的流行技术。 最后让我们来总结一下： 虽然LLM现在被吹得足够大，但它们是AI领域的一项强大创新。在这里，我提供了有关LLMs是什么以及如何在实践中使用它们的入门指南。日后我希望写一些文章提供初学者指南，帮助大家启动下一个LLM用例。 资源 链接：「个人博客」 社交：「推特」|「微博」| 「领英」|「油管」 之后我会出一些AI相关的具体视频教程，目前还未找到合适的平台托管，敬请期待。关注我，我会第一时间通知到家。 在我的公众号内的文章大部分是免费阅读的（除非有实际成本支出），如果您觉得对您有帮助，可以给我赞赏一下以表支持。 引用 [1] 大型语言模型调查。 arXiv:2303.18223 [cs.CL] [2] GPT-3论文。 arXiv:2005.14165 [cs.CL] [3] Radford，A.，&amp; Narasimhan，K。（2018）。通过生成式预训练改善语言理解。 （GPT-1论文）","link":"/LLMs%E7%9A%84%E5%AE%9E%E7%94%A8%E4%BB%8B%E7%BB%8D/"},{"title":"02 大语言模型做情感分析","text":"上一节中，我们介绍了大型语言模型的接口非常简单，仅提供了Complete和Embedding两个接口。但这样看似简单的接口，实际上可以解决很多自然语言处理问题。例如，情感分析、文本分类、文章聚类、摘要生成、搜索等问题，都可以使用大型语言模型解决。接下来的几节课中，我们将介绍如何使用这两个简单的API来解决传统的自然语言处理问题。本节我们将从最常见的自然语言处理问题“情感分析”开始介绍，看看如何使用大型语言模型。 传统的二分类方法：朴素贝叶斯与逻辑回归 朴素贝叶斯与逻辑回归可以用来解决“情感分析”问题。这些算法的基本思想是，根据给定的标记数据，学习一个分类器，用来将新的输入数据进行分类。对于情感分析问题，分类器的目标是将一段文字分为正面或负面情感。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Large-language-model-for-sentiment-analysis/"},{"title":"Foundation of Artificial Intelligence - Lecture 1","text":"Algorithm --&gt; Data Structure No obvious solution ==&gt; Algorithm engineers do it If there is a clear implementation path ==&gt; the person who develops the project will do it What's the Algorithm? {Ace of hearts, 10 of spades, 3 of spades, 9 of hearts, 9 clubs, 4 of diamonds, J} First: Hearts&gt; Diamonds&gt; Spades&gt; Clubs Second: Numbers are arranged from small to large Some people put the colors together first Some people arrange the size first, and extract the colors one by one \\[ 1024 --&gt; 10^3 --&gt; 1k \\] \\[ 1024 * 1024 --&gt; 10^6 --&gt; 1M \\] \\[ 1024 * 1024 * 1024 --&gt; 10^9 --&gt; 1G \\] 123456struction-0 00011101struction-1 00011111 struction-2 00011100struction-3 00011101struction-4 00011100struction-5 00011001 2.6G Hz 12345def fac(n): # return n! if n == 1: return 1 # 返回操作 else: return n * fac(n-1) # 乘法操作 + 返回操作 + 函数调用 12345678910fac(1)&gt; 1fac(100)&gt; 93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000fac_100 = &quot;&quot;&quot;93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000&quot;&quot;&quot;len(fac_100)&gt; 158 12345?? N --&gt; fac(n)# 乘法操作 + 返回操作 + 函数调用?? (N - 1)--&gt; fac(n-1)?? N == 100 fac(N) ??? 99 1234Object ` N --&gt; fac(n)` not found.Object ` (N - 1)--&gt; fac(n-1)` not found.Object ` N == 100 fac(N)` not found.Object `? 99` not found. \\[ Time(N) - Time(N-1) = constant \\] \\[ Time(N-1) - Time(N-2) = constant \\] \\[ Time(N-2) - Time(N-3) = constant \\] \\[ Time(2) - Time(1) = constant \\] \\[ Time(N) - Time(1) == (N-1)constant \\] \\[ Time(N) == (N-1)constant + Time(1) \\] \\[ Time(N) == N * constant + (Time(1) - constant) \\]","link":"/Lecture_1/"},{"title":"M1安装Homebrew(ARM)","text":"?&gt; 详情可见作者说明 安装 ARM版本Homebrew必须安装在/opt/homebrew路径下 123cd /optsudo mkdir homebrewsudo curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C homebrew 如果不进行sudo授权，则会报错； 环境变量 本人使用zsh, 所以编辑文件~/.zshrc. 添加如下内容： 12path=('/opt/homebrew/bin' $path) export PATH ?&gt; 如果是使用bash，请修改~/.bashrc 在终端内执行: 1source ~/.zshrc 现在可以试试执行brew install graphviz试试看能否正常安装回归树可视化模块； 软件包和迁徙 软件包依然需要使用X86版Homebrew 1arch -x86_64 启用一个X86模式中端，之后运行的命令都在X86模式下运行，再次安装Homebrew 1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; !&gt; 注意：要将ARM版本Homebrew环境变量设置到最前面，此时两个版本共存时会有限启动ARM版本，需要运行X86版本时，需要手动输入完整路径arch -x86_64 /usr/local/bin/brew 可以在配置文件中设置alias 12abrew='/opt/homebrew/bin/brew' # ARM Homebrewibrew='arch -x86_64 /usr/local/bin/brew' # X86 Homebrew 如果对已有软件包做迁徙，则： 1ibrew bundle dump 此时在目录下就得到一个名为Brewfile的备份文件，导入内容并安装 1abrew bundle --file /path/to/Brewfile !&gt; 执行之前需要编辑Brewfile文件，将cask和mas开头的记录删除掉；","link":"/M1_install_homebrew/"},{"title":"5. 模块化编程","text":"HI, 大家好。我是茶桁。 上一节中我们学习了Python基本的流程控制，并且预告了这一节的内容，就是将要学习「模块化编程」。那什么是模块化编程呢？按照维基百科的说法： 模块化编程（英语：modular programming），是强调将计算机程序的功能分离成独立的、可相互改变的“模块)”（module）的软件设计技术，它使得每个模块都包含着执行预期功能的一个唯一方面（aspect）所必需的所有东西。 说的简单一点，就是把程序进行封装（函数封装、面向对象、文件...） OK，话不多说，让我们开始吧。 函数 什么是函数？ 函数的英文单词为function, 我们将其翻译过来，就是“函数，功能”。 其实，函数就是一个具有特定功能的代码块。 函数的作用 函数的目的是封装，将特定功能的代码块进行封装，其目的是提高代码的重用性，从而提高开发效率，并且降低了后期的维护成本。 函数的定义和使用 函数的定义其实非常简单，我们用代码来写一下： 12345# 定义函数[基本结构]def 函数名([参数列表]): 当前行数的具体功能的代码 当前行数的具体功能的代码 ... 当然，函数在写完之后并不会自动执行，只是把函数定义了而已。如果想要使用定义完成的函数，需要用语法来进行函数的调用。 那么函数该如何调用呢？如下： 1函数名() 示例： 12345678# 函数的定义格式def love(): print('i') print('love') print('u')# 函数的调用love() 当前程序运行输出结果： 123iloveu 以上代码可以得到函数的第一个特征：函数定义后，不调用不执行。还记得咱们上节课强调的流程控制吗？代码最基本流程顺序是自上而下的，所以，这个时候我们如果调用放在上方，例如： 12345678# 函数的调用love()# 函数的定义格式def love(): print('i') print('love') print('u') 此时因为love()调用的时候函数还未被定义，所以会执行报错： 1NameError: name 'love' is not defined 所以我们需要注意：不能在函数定义前调用函数。 另外，我们需要注意，函数的调用不受次数的影响，比如，我们定义好函数后，这个时候在后面调用三次： 123love()love()love() 那执行后的结果应该是连着打印了三次结果。 和变量一样，函数的命名也是要遵守命名规范的： 字母数字下划线，不能以数字开头 严格区分大小写，且不能使用关键字 命名最好有意义，且不要使用中文 现在我们想想，在love()函数被定义后，我们再来定义一个同名的函数会怎么样？ 我们尝试一下，在刚才定义好的函数下方重复写一个同名的函数： 1234567891011121314# 函数的定义格式def love(): print('i') print('love') print('u')def love(): print('u') print(&quot;don't&quot;) print('love') print('me')# 函数的调用love() 直接结果： 1234udon'tloveme 那，我们得到了实验结果：同样的函数名被再次定义之后，冲突的函数会被覆盖。 所以，最后我们总结一下函数的特征及注意事项： 123456781. 函数定义后，不调用不执行2. 不能在函数定义前调用函数3. 函数的调用不受次数影响4. 函数的命名要遵守命名规范 - 字母数字下划线，不能以数字开头 - 严格区分大小写，不能使用关键字 - 命名最好有意义，且不要使用中文5. 函数名不要冲突，冲突后会被覆盖 函数的参数 在定义函数的时候，我们需要注意给函数的参数。可以在参数列表的位置进行定义，这个称为形参。如果一个函数有形参，那么在调用的时候必须传递参数（实参）。实参将值传递给实参的过程，本质上就是变量赋值操作。 函数参数概念及分类 带有参数的函数，该如何定义？ 在定义函数时，在小括号内可以定义形参（形式上的参数） 12345def love(w): print(f'i love {w}')# 调用带有形参的函数时，需要传递参数（实参）love('马户') 执行结果为： 1i love 马户 在这整个函数中，小括号内的w就是形参，在调用的时候的马户就是实参，在调用过程中将值传给了形参w。 那么，如果我在调用的时候没有传递实参，就会直接报错： 123love()TypeError: love() missing 1 required positional argument: 'w' 形参可以是多个，这就是定义带有多个参数的函数： 1234def love(m, n): print(f'{m} love {n}')love('i', 'u') 执行结果： 1i love u 如果形参是多个的话，那么有多少个形参就必须传递几个实参。并且参数都是按照顺序进行传递的。 如果少传一个参数，则同样会被错。 那，能不能多传呢？也不行，如果多传了参数，一样会报错。 至此，我们可以做如下总结： 函数参数：调用时需要传递的数据 函数参数的大类分为形参和实参 形参意思：函数定义时的参数 实参意思：函数调用时的参数 形实关系：函数调用时，形参和实参个数需要一一对应 函数中的参数类型 在确定了什么是形参和实参之后，我们来看看，这两种参数都有哪些类型。 函数参数在类型上，包括： 普通参数 默认参数 收集参数 命名关键字参数 关键字参数收集 普通参数 先来说说普通参数，其实就是位置参数，也叫顺序参数，也是必须传递的参数。 1234def love(m, n): print(f'{m} love {n}')love('i', 'u') 这段代码中，m, n就是普通参数，必须传递。 默认参数 有些函数在定义的时候，行参上就已经定义了默认值，那么这种就叫做默认参数。 在调用函数的时候，默认参数是可以不传值的。当传值之后，默认值就会被改变： 1234def func(x, y=20): print(x, y)func(2) 这段代码中的行参y就是默认参数，我们在调用函数func()只写了一个实参，也就是只传了一个值给函数。这个时候执行结果为： 12 20 我们修改一下，传两个值进去看看结果： 1234def func(x, y=20): print(x, y)func(2, 100) 执行结果： 12 100 可以看到，本来我们定义的行参y的默认值被改变了。 在定义默认参数的时候需要注意，函数中的默认参数只能全部定义在普通参数的后面，否则在调用函数的时候就会报错，比如以下这些情况： 1234567891011121314151617# 第1种错误情况def func(x=100, y=200, z): print(x, y, z)func(100,200,300)# 第2种错误情况def func(x=100, y, z=200): print(x, y, z)func(100, 200, 300)# 第3种错误情况def func(x, y=100, z): print(x, y, z)func(300,200,50) 收集参数 收集参数就是专门收集在函数调用时传递的多余的实参，或者我们可以理解为，不确定需要传递多少个实参，直接用一个行参来接收。 比如，我们现在有个需求就是需要计算用户输入的数字总和，我们按前面那个函数的定义方式为： 1234def func(x, y z=100): print(x+y+z)func(20,30) 这个函数中，我们输入2个值或者3个值都可以，但是当我们只输入一个值或者三个以上的时候，程序就会报错了。 那么有没有什么办法，不管用户输入多少个数字，我们都可以进行相加计算呢？ 1234567def func(x=&quot;+&quot;, *args): if x == '+': print('加法运算', args) else: print('减法运算', args)func(&quot;-&quot;, 2, 3, 4, 5, 6, 7, 8) 这段代码执行结果为： 1减法运算 (2, 3, 4, 5, 6, 7, 8) 虽然中间的运算代码我没有写，但是大家可以看到，已经可以接受不固定的多个参数了。 这个*args就是我们的收集参数。 在定义函数的时候，如果需要收集参数，那么这个形参前面需要加一个*号，例如*args。这里需要注意一点，*args并不是固定写法，你可以随意定义一个，只要前面有*号就可以了。比如下面这样： 1234567def func(x=&quot;+&quot;, *y): if x == '+': print('加法运算', y) else: print('减法运算', y)func(&quot;-&quot;, 2, 3, 4, 5, 6, 7, 8) 一样可以执行并得到一样的结果，这个时候，我们的*y就是收集参数。 收集参数也有两类，一种是普通的收集参数：专门用于收集多余的普通参数，形成一个新的元组。 1语法： 参数前面加*, 例如：*args 还有一种是关键字收集参数：用于专门收集多余关键字实参，形成一个新的字典： 1语法：参数前面加**, 例如：**kwargs 现在我们已经理解了普通的收集参数，那么在学习关键字收集参数之前，我们先来学习一下命名关键字参数 命名关键字参数 命名关键字是放在*号后面的参数，调用的时候强制必须传入制定参数名才能进行调用。 1234def func(a, b, c=3, *args, name): print(a, b, c, &quot;\\n&quot;, *args, &quot;\\n&quot;, name)func(1, 2, 3, 4, 5, 6, 7, 8, name='茶桁') 这段代码执行结果： 1231 2 3 4 5 6 7 8 茶桁 我们特意在中间加了换行字符来清晰的辨别*args和name。 如果在这段代码中我稍微变一下，在执行函数的时候，实参里不标明name可以吗？ 1234def func(a, b, c=3, *args, name): print(a, b, c, &quot;\\n&quot;, *args, &quot;\\n&quot;, name)func(1, 2, 3, 4, 5, 6, 7, 8, '茶桁') 执行之后我们收到了报错： 1TypeError: func() missing 1 required keyword-only argument: 'name' 这段报错明显告诉我们，确实了一个必须的关键字参数name。 那为什么会出现这种报错呢？这是因为在关键字参数之前，我们使用了*args来进行收集参数，那么无论你写多少，这些值都会被*args接收变成元组，那么后面的name自然就无法接受到值了。 让我们再来做一个实验，给命名关键字参数加上一个默认值，那么我们就能明显的看出问题： 1234def func(a, b, c=3, *args, name='_茶桁'): print(a, b, c, &quot;\\n&quot;, *args, &quot;\\n&quot;, name)func(1, 2, 3, 4, 5, 6, 7, 8, '茶桁') 这段代码执行结果： 1231 2 3 4 5 6 7 8 茶桁 _茶桁 可以看到，name给了默认值之后不再出现报错，而我们的实参也并未传到name里，而是全部被*args接收了。最后打印出了name的默认值_茶桁。 利用命名参数的这种定义参数名称接收值的特点，我们就可以打乱之前普通参数传值的顺序性，比如： 12345def func(x, y): print(x, &quot;\\t&quot;, y)func(2, 3)func(y = 2, x = 3) 执行结果为： 122 33 2 还是最开始的普通参数的写法，但是最后执行函数的时候，我们给实参指定了名称，这样传参顺序就没那么重要了。 所以，我们总结一下： 关键字参数定义在收集参数后面 关键字参数必须通过形参的名字来进行传递 关键字参数收集 前面我们在讲收集参数的结尾处提到了关键字参数收集，形式为**kwargs。 1234567def func(a, b, c=3, *args, name, age, **kwargs): print(a, b, c) print(args) # 普通收集参数，会把多余的参数收集为元组 print(name, age) print(kwargs) # 关键字参数收集，会把多余的关键字参数收集为字典func(1, 2, 4, 112, 123, 321, 541, 231, name=&quot;茶桁&quot;, age=18, sex='male', height=185, x='x', y='y') 执行结果： 12341 2 4(112, 123, 321, 541, 231)茶桁 18{'sex': 'male', 'height': 185, 'x': 'x', 'y': 'y'} 从执行结果上我们可以看到，在name和age之后的所有参数都被传递到了**kwargs里，然后作为字典打印了出来。 在声明这个函数和执行函数的时候需要注意，这些参数都是有顺序的，如果在执行函数的时候再多传一个非关键字参数，这个时候程序就会报错，如果是关键字参数，则照样会被**kwargs接收。 在我们介绍完这些参数之后，我们最后再说明一下： 形参声明的位置顺序：普通参数 -&gt; 默认参数 -&gt; 收集参数 -&gt; 命名关键字参数 -&gt; 关键字收集参数 def func(a, b, c=1, *args, d, **kw) 这段声明中，a,b为普通参数，c是默认参数，args是收集参数，d是命名关键字参数，kw是关键字收集参数 极少情况下会同时出现上面五种形参，一般情况下为：普通参数，收集参数，关键字收集参数 所有参数的摆放问题: 实参：普通实参在前，关键字参数在后 形参：关键字收集参数一定在最后出现，收集参数推荐在普通参数之后使用。 推荐顺序：普通形参、收集参数、关键字收集参数 函数的返回值 一个函数除了可以完成一定功能之外，还可以用来安需要返回一些内容。在函数中，使用return关键字来制定返回数据，可以返回任意类型的数据。 函数的返回值会把数据返回到调用的地方，可以使用变量进行接收，或者作其他处理。 函数可以分为两类： 执行过程函数：函数体内完成一定的功能即可，没有返回值 具有返回值的函数：函数体内完成一定的功能，并且返回一个结果到函数调用处。 比如： 12def func(a, b): print(f'{a} love {b}') 以上函数就是一个没有返回值的函数，这个函数只是为了完成打印这句话的功能。 那么有返回值的函数是什么样子？ 如果需要在函数中制定返回内容，我们需要使用return关键字。 12345678# 有返回值的函数def func(a, b): res = f'{a} love {b}' # 可以在函数体内，使用return返回内容 return resr = func('老鼠', '布丁')print(r) 执行结果为： 1老鼠 love 布丁 在这段代码中，我们在func()的函数体内最后利用关键字return返回了任意内容，并且使用变量r接收了这个返回值，最后讲r打印了出来。 在调用func()这个函数的时候，函数中的返回值会返回到函数调用处。 我们再来研究一下return这个关键字，我们在return的前后都加上一段打印代码，看看会发生什么。 12345678def func(a, b): res = f'{a} love {b}' print('这是return前') return res print('这是return后')r = func('老鼠','布丁')print(r) 执行结果： 12这是return前老鼠 love 布丁 看到结果我们可以清楚，return之后的的代码并未继续执行，也就是说，我们如果要在函数体内执行其他任务，必须放在return之前执行，否则根本不会执行。那么我们可以得出结论：return必须放在一个定义函数的最后面。 Tips: 其实，即便没有return关键字或者returen之后没有任何内容，也有返回值，只是返回的是None值。 None是一个特殊的数据，表示什么都没有。查询类型可以看到返回 &lt;class ‘NoneType’&gt; 变量的作用域 作用域就是当前起作用，可用的范围区域。也就是变量的有效范围。 变量按作用域可以分为： 局部变量： 在函数内部可以使用的变量 全局变量：在函数内外都可以使用的变量 局部变量 让我们尝试下，如果函数内定义的变量在函数外使用会如何： 1234def func(): a = 20 print(a) 执行结果： 1NameError: name 'a' is not defined 被告知a并未被定义。 可以看到，函数内定义的变量，在函数外连获取都做不到。这种在函数内定义的这个变量a，就是局部变量，它在函数外不能使用。 再让我们来看看将变量定义在函数外会是怎样的一种情况： 12345num = 10def func(): print(num)func() 执行结果： 110 在func函数内，我们获取到了变量num的值并打印。那说明，在函数内我们可以获取函数外部的变量。 我们继续在继续看： 123456num = 10def func(): num += 20 print(num)func() 执行后报错： 1UnboundLocalError: local variable 'num' referenced before assignment 这样我们可以看到，变量num在函数内虽然可以使用，但是无法进行更改。 那在函数外定义的所有变量都是如此吗？再让我们试试看： 123456items = [1, 2, 3, 4, 5]def func(): items[0] = 20 print(items)func() 执行结果: 1[20, 2, 3, 4, 5] 由此我们看出，并不是所有的变量都不可在函数内进行更改。 其实，变量是分为两种的： 可变数据类型：在函数外定义的变量，在函数内可以使用并改变 不可变数据类型：在函数外定义的变量，在函数内只可以访问而无法改变 可变数据类型有列表和字典，其他的都是不可变数据类型。 全局变量 之前我们介绍的都是局部变量，那怎样定义全局变量呢？ 在函数内使用global直接定义的变量，就是全局变量，函数内外都可以直接使用。 在函数外定义的变量，在函数内使用global关键字进行声明，那么也是全局变量。 例如： 1234567num = 20def func(): global num num += 10 print(num)func() 这个时候我们可以得到执行结果： 130 那有小伙伴就问了，如果我在函数外直接使用global定义全局变量可以吗？让我们来试试看就知道了： 1234567global numnum = 20def func(): num += 10 print(num)func() 执行之后得到报错： 1UnboundLocalError: local variable 'num' referenced before assignment 这样我们就得到了结果：不可以。 在整个程序中，我们可以使用两个函数方法来获取数据： globals()用来获取全局数据，locals()用来获取当前作用域的数据 讲到这里，我们再来多看一组代码： 12345678# 函数的作用域def outer(): print('this is outer func...') def inner(): print('this is inner func...')outer()inner() 这段代码执行结果为： 123this is outer func...NameError: name 'inner' is not defined 正常执行了outer()内的打印，然后又报了一个错误，提示inner函数未定义。 说明，不只是变量有作用域，函数一样也有作用域。要想inner函数内的打印也起作用，我们需要在函数内就调用执行inner()。 1234567# 函数的作用域def outer(): print('this is outer func...') def inner(): print('this is inner func...') inner() # 在函数内执行outer() 这样，我们执行的结果就是： 12this is outer func...this is inner func... 如果我们在外层函数中定义一个局部变量，能在内层函数中使用吗？ 12345678910# 函数的作用域def outer(): a = 2 print('this is outer func...') def inner(): a += 1 print('this is inner func...') print(a) inner()outer() 执行之后得到报错： 1UnboundLocalError: local variable 'a' referenced before assignment 说明并不可以。 nonlocal关键字 那么，到底有没有什么办法在内函数中使用上一层函数中的局部变量呢？答案是有办法。 在内函数中如果想要使用外层函数的变量，那么需要使用nonlocal关键字，可以引用上一层函数中定义的局部变量。 123456789101112# 定义一个外层函数def outer(): # 外函数的局部变量 num = 10 # 内函数, 局部函数, 在函数的内部定义的函数 def inner(): # nonlocal 关键字在局部函数中使用 nonlocal num # 可以引用上一层函数中定义的局部变量 num += 1 print(num) inner()outer() 执行后返回结果： 111 至此，我们通过使用nonlocal关键字，成功拿到了外层函数定义的变量num并使用。最后打印出使用的结果。 这里我们要注意，nonlocal虽然可以引用上一层函数中定义的局部变量，但是这并不代表提升为了全局变量。 既然我们有了global关键字可以提升变量为全局变量，为什么还需要一个nonlocal关键字呢？是不是有点多此一举？ 这两者的功能上并不相同。global关键字修饰变量后标识该变量是全局变量，对该变量进行修改就是修改全局变量，而nonlocal关键字修饰变量后标识该变量是上一级函数中的局部变量，如果上一级函数中不存在该局部变量，nonlocal位置会发生错误（最上层的函数使用nonlocal修饰变量必定会报错）。 关于函数的文档 我们在一个未定义任何变量和函数的空白文档中打印一下全局数据： 1print(globals()) 执行结果： 1234567891011121314151617181920212223242526272829{ '__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': &lt;module 'builtins' (built-in)&gt;, '__builtins__': &lt;module 'builtins' (built-in)&gt;, '_ih': ['', 'print(globals())'], '_oh': {}, '_dh': [PosixPath('/Users/xx/git/AI_Cheats/Python'), PosixPath('/Users/xx/git/AI_Cheats/Python')], 'In': ['', 'print(globals())'], 'Out': {}, 'get_ipython': &lt;bound method InteractiveShell.get_ipython of &lt;ipykernel.zmqshell.ZMQInteractiveShell object at 0x105be29e0&gt;&gt;, 'exit': &lt;IPython.core.autocall.ZMQExitAutocall object at 0x105be3280&gt;, 'quit': &lt;IPython.core.autocall.ZMQExitAutocall object at 0x105be3280&gt;, 'open': &lt;function open at 0x10488e710&gt;, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/Users/xx/git/AI_Cheats/Python/globals.ipynb', '_i': '', '_ii': '', '_iii': '', '_i1': 'print(globals())'} 我们来看这个打印出来的json 类似于__name__这种前后有__ __的数据，称之为“魔术变量”。我们并未定义，但是已经存在了。 如果脚本作为主程序，那么__name__值是__main__，如果是当作一个模块在另外一个脚本中引用去使用，那么值就是当前文件的命名。 __doc__当前脚本的文档说明，在当前脚本当中的第一个三引号注释就是当前脚本的说明文档。比如，我们在这个空白的文档中写一段三引号注释 12345&quot;&quot;&quot;这里是整个文档的说明部分。&quot;&quot;&quot;def func(): pass 然后我们直接将doc打印出来查看： 1print(__doc__) 可以看到输出内容： 1这里是整个文档的说明部分。 这种文档其实不止适用于python文件，对于定义的函数依然适用。比如，我们执行定义了一个函数，并在函数内部用三引号进行注释： 12345678910def func(): &quot;&quot;&quot; 这里是让你写一写函数的文档说明的。 需要说明当前函数的作用， 如果当前函数还有形参，那么也需要对形参进行一一说明。 name: 这个是一个name参数，用于接收姓名 age: 这个参数是表示年龄 :return: 此处说明当前函数的返回值 &quot;&quot;&quot; pass 这个时候，我们在下方执行： 1print(func.__doc__) 可以看到我们在注释内定义的说明文档被打印出来了： 这样，我们不仅可以在自己写函数的时候在上方清晰的写明当前函数的作用及参数，我们还可以使用此方法，查找其他人所写的函数的一些说明。 在我们平时写代码的时候，养成一个好习惯是非常有必要的。 总结一下： print(__name__): 获取当前脚本的文件名 print(__doc__): 获取当前脚本的说明文档 print(func.__doc__)： 获取当前函数的说明文档 练习：函数封装 我们上一讲中的练习中，我们打印了乘法表，矩形图形，还计算了12生肖。这里我们就将乘法表来封装成函数，实现我们上节课留的其中一道思考题：反向打印。 我们先将打印乘法表封装起来： 123456789# 定义函数，打印九九乘法表def multiply_table(): &quot;&quot;&quot; 当前函数的功能是打印出乘法表 &quot;&quot;&quot; for x in range(1, 10): for y in range(1, x+1): print(f'{x}X{y}={x*y}', end=&quot; &quot;) print() 这样，我们在其他地方执行multiply_table()函数的时候，就可以直接打印出乘法表。 现在让我们给这个函数多加一些功能： 123456789101112131415# 定义函数，打印九九乘法表def multiply_table(i=0): &quot;&quot;&quot; 当前函数的功能是打印出乘法表 i=0; i 这个参数可以用来控制正向输出和方向输出，0的时候正向，1的时候反向,默认为0 &quot;&quot;&quot; if i: rs = range(9, 0, -1) else: rs = range(1, 10) for x in rs: for y in range(1, x+1): print(f'{x}X{y}={x*y}', end=&quot; &quot;) print() 我们执行函数的时候，输入1来试试看： 1multiply_table(1) 输出结果： 1234567899X1=9 9X2=18 9X3=27 9X4=36 9X5=45 9X6=54 9X7=63 9X8=72 9X9=81 8X1=8 8X2=16 8X3=24 8X4=32 8X5=40 8X6=48 8X7=56 8X8=64 7X1=7 7X2=14 7X3=21 7X4=28 7X5=35 7X6=42 7X7=49 6X1=6 6X2=12 6X3=18 6X4=24 6X5=30 6X6=36 5X1=5 5X2=10 5X3=15 5X4=20 5X5=25 4X1=4 4X2=8 4X3=12 4X4=16 3X1=3 3X2=6 3X3=9 2X1=2 2X2=4 1X1=1 可以看到，我们的控制结果被成功打印出来。至此，这个有一些小功能的九九乘法表就封装完成了。 那么这一节课就不留思考题了，大家课后熟练掌握一下封装函数和变量的作用域，我们下节课来学习一些高阶函数， 好，下课。","link":"/Modular-programming/"},{"title":"03 提示语，做个聊天机器人","text":"大家好，我是茶桁。 在本次讲座中，我们将研究 OpenAI 提供的 Completion API 接口。你可能已经体验过与 ChatGPT 进行聊天的乐趣，或是利用它帮我们编写各种文本材料的便利。现在，我们可以从这个需求出发，进一步探索 Completion API 的具体用法，以及如何利用它来生成更加优质的文本。 AI 客服 在我了解人工智能和自然语言生成之前，我听说过智能客服，然而我并没有亲身体验过。我想象中，智能客服的回答应该是按照固定的模板进行生成的，这样的缺点就是每次回答都会是一样的。虽然可以设计多个模板来轮换着表达相同的意思，但是最多也就是三四个模板，整体的体验可能会比较呆板。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Let-s-Build-a-Chatbot/"},{"title":"Power for Mac","text":"很多人都会使用\"Pow\"来进行本地静态页面开发环境。对于其配置确实简单到有爱。 不过Yosemite上\"Pow\"都不能正常工作，之前我参照官方的办法写了一篇如何在Yosemite上设置\"Pow\"的方法。有兴趣的可以参看我原文:http://hivan.me/setting-pow-at-Yosemite/ 不过一直用的好好的\"Pow\"近期又开始出现 404 错误，短暂的解决无果以后，我开始寻找一些快速能解决的办法，便遇到了\"Power\"，和\"Pow\"一样，都是建立快速开发环境，并且经过测试，在Yosemite 10.10.1下正常工作。 使用\"Pow\"来做开发环境的，可以暂时用这样一个替代方案，毕竟开发中没有时间去多做研究了，暂时不知道两者的区别，不过\".dev\"正常访问已经没有问题。 Power 项目地址: https://github.com/HackPlan/power/","link":"/Power-for-Mac/"},{"title":"1. Python的特性和语法","text":"千里之行始于足下。 大家好，我是茶桁，这里是我们《AI秘籍》的第一节，让我们先从Python来开始好好的打好基础。 第一堂课，我们先从最基础的Python特性开始，当然，还有一些基本语法。 上来就开始讲特性和语法，说明我们将会遗弃惯用的“环境搭建”等更基础的内容，那些内容网上已经很丰富了，一查一大堆，并且相对来说内容都比较独立，所以希望还不太会搭建开发环境的同学可以自己去搜索看看。或者，其实从我这篇《Apple M1的AI环境搭建》也能完全搭建起一个完整的Python开发环境。（Windows和Linux的同学就只能在网上搜索一下看看了。） 总体来说，Python语言的使用率越来越高，它不仅可以用于GUI开发、Web开发，还能进行数据预处理、数据探索性分析（EDA），更是进行数据挖掘、机器学习、深度学习等任务的首选语言。 基于Python的包也越来越丰富，各种优秀的库层出不穷。根据\"Tiobe编程语言排行榜\"的最新统计，Python结束了自己攀爬的势头，已经开始长期占据榜首。Python的发展势头让人们看到了它在各个领域都有着广阔的应用前景。 得益于Python语言的简洁语法和高效开发能力，使得集成系统变得非常方便。 与此同时，Python相关的就业机会也非常丰富，待遇也相当优渥。 因此，无论从易用性、就业机会还是薪酬待遇来看，Python都是IT从业者必备的编程语言之一。 课程说明 本课程所使用语言为Python3 本课程将会有一些案例，用于辅助学习和理解知识点。 基本所有案例均使用Jupyter Notebook做演示 一些项目会用到软件工程和设计模式的思想。 本课程无任何文学色彩，重点在于简单通俗易懂。 接下来，让我们真实开始吧。 Python的两大特性 一句话总结，就是Python是一门动态的、强类型语言。 动态语言 在了解“动态语言”之前，我们先来了解一下“类型检查”。 类型检查是验证变量或表达式的数据类型是否符合语言规定的类型约束的过程。它用于确保程序在运行时不会出现类型错误，例如将一个字符串与一个整数相加或将一个数字与一个布尔值进行比较。类型检查旨在捕捉潜在的类型不匹配错误，并在编译时或运行时提供相应的警告或错误信息。 如果类型检查发生在程序运行阶段（运行时），则称为\"动态类型语言\"（dynamically typed languages）。常见的动态语言包括： Python JavaScript Ruby PHP Lua Perl Shell脚本 有动态语言，则必然会有其相对的“静态语言”，常见的“静态类语言”包含： C C++ Java C# Swift Kotlin Rust TypeScript 当然，这些都只是一部分常见的动态语言和静态类型语言，还有许多其他编程语言也属于这两个类别。在实际开发中，选择使用动态语言还是静态类型语言取决于项目的需求、开发团队的喜好和项目的规模等因素。每种类型的语言都有其独特的特点和适用场景，因此选择合适的语言是非常重要的。 强类型语言 强类型语言（Strongly Typed Language）要求在编程过程中严格定义和遵守数据类型规则。在强类型语言中，变量必须明确地声明其数据类型，并且在运行时不能隐式地改变数据类型。这意味着变量在使用之前必须进行类型转换，以确保数据的一致性和安全性。 在强类型语言中，编译器或解释器会对数据类型进行严格的检查，如果发现不符合类型规则的操作，就会报错并拒绝执行。这样可以防止一些潜在的类型错误，确保程序的稳定性和正确性。 强类型语言的主要特点包括： 静态类型检查：在编译时或解释时进行类型检查，检查数据类型是否匹配，避免类型不匹配的错误。 显式类型转换：在进行类型转换时，必须显式地指定转换的方式，例如强制类型转换。 不支持隐式类型转换：强类型语言不允许在不明确声明的情况下将一个数据类型隐式地转换为另一个数据类型。 这么说可能并不直接，我们来拿个示例，我们输入两行代码： 12a = 5a = a + 's' 然后我们会看到程序抛出TypeError异常： 这个异常意思是不支持int变量和str变量相加。 常见的强类型语言包括： Java C++ C# Python Swift Kotlin TypeScript Rust Pascal Ada Delphi 一样的，对应的就是弱类型语言。弱类型语言容易与其他类型混合计算，其代表是JavaScript。（有一说一，我还挺喜欢JS的这个特性的） 弱类型语言包括： JavaScript PHP Perl Ruby Tcl Bash AWK MATLAB (在一些操作上可以被视为弱类型) 当然，对于这个划分其实并不是所有人都一致的，有些人还是会把Python归结为弱类型语言，而通常意义上，大家会把C++划分到弱类型。这里我们不去争论，仅仅记住动态/静态、弱类型/强类型的区别就行了。 基本语法 基本语法里，我们介绍一下Python的命名规则、缩进原则、特殊关键字和特殊运算符四个方面。 命名规则 Python的变量命名规则包括以下几条： 允许包括英文、数字以及下划线（_），不能以数字开头。 名称区分大小写，例如\"myVar\"和\"myvar\"是两个不同的变量。 以单下划线（_）开头的变量通常用作受保护的变量，表示应该将其视为私有，不建议直接访问。虽然Python没有严格的访问控制，但这是一种约定俗成的做法。 以双下划线（__）开头和结尾的变量是Python中的特殊标识符，具有特殊的意义，如类的私有成员或专用标识符。 Python的变量命名习惯一般遵守蛇形命名法（snake case）： 一般变量命名使用小写字母，多个单词之间用下划线连接，例如：book_id、book_store_count。 类名首字母大写，如Python内置模块collections.abc中的Iterable类，我们自定义的Book类等。 类方法名也使用小写字母，多个单词之间用下划线连接，例如：get_store_count()。 与Java的命名方法不同，Java通常使用驼峰命名法（camel case）来命名变量和方法名，其中第一个单词首字母小写，后续单词首字母大写，例如：myVar、getStoreCount()。而Python则更倾向于使用蛇形命名法。这是因为Python社区普遍认可了蛇形命名法，使得代码在风格上更加一致和易读。 缩进原则 Python最具特色的特点之一是使用缩进来表示代码的逻辑层次，而不是像Java和C++中使用{}。Python的缩进层级结构非常重要，它代表着代码的逻辑结构。 通常情况下，Python的缩进为4个空格字符。例如，在定义一个Book类并重写__add__方法来计算两本书的库存量时，代码如下所示： 12345678910111213141516171819class Book(object): # 定义类的参数 def __init__(self, b_id, b_name, b_store_count): self.b_id = b_id self.b_name = b_name self.b_store_count = b_store_count # 重写加法操作 def __add__(self, book): return self.b_store_count + book.b_store_count # 创建两个Book类的实例python_intro_book = Book(1, '金瓶梅', 100)ml_intro_book = Book(2, '玉蒲团', 200)# 求两本书的总销量sales_cnt = python_intro_book + ml_intro_bookprint(sales_cnt) 上述代码定义了一个Book类，包括初始化方法__init__和重写的加法操作__add__。通过这种缩进结构，我们可以清晰地看到代码的层次结构和逻辑。 在Python编码中，缩进格式、行间空行数、变量和等号之间的空格等都遵循PEP8（Python Enhancement Proposal 8）规范。可以使用autopep8包来自动实现PEP8规范，保持代码的规范和易读性。 特殊关键字 Python有35个关键字，这些关键字具有特殊的含义，不能用于自定义变量名，否则会引起语法错误。以下是Python的关键字列表： 1234567False await else import passNone break except in raiseTrue class finally is returnand continue for lambda tryas def from nonlocal whileassert del global not withasync elif if or yield 这些关键字在Python编程中扮演着重要的角色。其中，True和False用于表示布尔值的真和假，类似于Java中的true和false；None表示空值，类似于Java中的null；Python使用not表示逻辑反操作，而Java使用!；Python使用and表示两个条件同时满足，Java使用&amp;&amp;；Python使用or表示两个条件满足其一，Java使用||；Python使用elif，而Java使用else if。 还有一些比较特殊的关键字，例如： del用于删除可迭代对象中的某个元素； def用于定义函数； 带有yield的关键字用于定义生成器（generator）函数； global和nonlocal是在Python函数式编程的闭包场景中使用的。 pass关键字用于占位，当你在定义函数或类时暂时不想添加具体的实现时，可以使用pass关键字。 这些关键字的具体用法将在后续文章中更详细地介绍。在此，先对它们有一个整体的认识即可。 特殊运算符 Python的运算符包括： 123+ - * ** / // % @&lt;&lt; &gt;&gt; &amp; | ^ ~ :=&lt; &gt; &lt;= &gt;= == != 大部分运算符在其他编程语言中也是常见的，不过这里重点介绍三个比较特殊的运算符：//、**和:=。 //运算符用于两个数值相除并向下取整，类似于Python的math.floor()功能： 12print(5 // 2) # 输出: 2print(5 // 4.5) # 输出: 1.0 **运算符用于进行幂运算： 1print(2 ** 3) # 输出: 8 :=运算符是在Python 3.8版本中引入的，被形象地称为“海象运算符”。它可以在表达式中同时为变量赋值和比较： 123n = len(a)if n &gt; 10: print(f&quot;{n}大于10&quot;) 可以用“海象运算符”简化为： 12if (n := len(a)) &gt; 10: print(f&quot;{n}大于10&quot;) Python的比较运算符还支持链式比较，使得编码更加方便： 123i = 3print(1 &lt; i &lt; 3) # 输出: Falseprint(1 &lt; i &lt;= 3) # 输出: True 此外，运算符@用于装饰器功能，本专栏会深入解释它的本质，并提供相关案例，帮助你学会使用装饰器。 总结 在本文中，我们一起学习了Python这门功能强大的编程语言。Python的两大特性是动态语言和强类型语言。 动态语言意味着在运行时执行类型检查，而不是在编译时。这使得Python更加灵活和易于使用，允许我们在代码中动态创建和修改变量。Python的动态特性使其成为进行数据预处理、数据探索性分析、数据挖掘、机器学习和深度学习等任务的首选语言。 另一方面，强类型语言意味着变量的类型在声明时就已经确定，并且不能进行隐式类型转换。这确保了代码的稳定性和安全性，帮助我们避免一些常见的错误。 在Python的基本语法方面，我们学习了变量命名规则，缩进原则，特殊关键字和特殊运算符。Python的命名规则允许使用英文、数字和下划线，但不能以数字开头，并且区分大小写。特殊关键字包括Python的35个关键字，如if、else、for、while等等，它们有着特定的含义和用途。特殊运算符中，//用于整数除法，**用于幂运算，:=是在Python 3.8版本中引入的“海象运算符”，使得在表达式中同时为变量赋值和比较变得更加方便。 通过学习Python的特性和基本语法，我们已经具备了编写简单到复杂的程序的基础知识。Python的易用性、丰富的库和社区支持，使其成为一个优秀的编程语言，适用于各种应用场景。无论是初学者还是有经验的开发者，Python都是一个值得深入学习和探索的语言。 希望本文能够为读者提供了一个对Python的初步认识，并激发了你继续学习和研究的兴趣。在接下来的学习过程中，我们可以更深入地了解Python的各种功能和应用领域，并用Python来解决更复杂的问题。 好了，我是茶桁，咱们下节见。","link":"/Python-features-and-syntax/"},{"title":"3. Python3 运算符","text":"Hi，大家好。我是茶桁。 前两节我们学习了基本的Python特性和语法，并且认识了一些基本的Python脚本。今天，我们来学习一下Python的运算符，而我们选择的版本为Python3。 什么是运算符 为了能让我们的学习顺利进行下去，首先我们需要先弄明白：什么是运算符。 这里举一个简单的栗子：\\(4 + 5 = 9\\), 在这个简单的数学计算栗子中，4和5倍称为操作数，+就被成为是运算符, 最后9就是它的运算结果。 到这里，我们对于运算符应该有了一个基本的认知，那么Python语言都支持哪些运算符呢？如下列表： 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 接下来，就让我们来一个个的学习Python的运算符 算术运算符 运算符 描述 实例 + 加 - 两个对象相加 a + b 输出结果 31 - 减 - 得到负数或是一个数减去另一个数 a - b 输出结果 -11 * 乘 - 两个数相乘或是返回一个被重复若干次的字符串 a * b 输出结果 210 / 除 - x 除以 y b / a 输出结果 2.1 % 取模 - 返回除法的余数 b % a 输出结果 1 ** 幂 - 返回x的y次幂 a**b 为10的21次方 // 取整除 - 向下取接近除数的整数 &gt;&gt;&gt; 9//2 4 &gt;&gt;&gt; -9//2 -5 1234567891011# 算术运算符a = 10b = 21print(&quot;a+b=&quot;, a+b)print(&quot;a-b=&quot;, a-b)print(&quot;a*b=&quot;, a*b)print(&quot;b/a=&quot;, b/a)print(&quot;b%a=&quot;, b%a)print(&quot;a**b=&quot;, a**b)print(9//2)print(-9//2) 输出结果： 12345678a+b= 31a-b= -11a*b= 210b/a= 2.1b%a= 1a**b= 10000000000000000000004-5 比较运算符 运算符 描述 实例 == 等于 - 比较对象是否相等 (a == b) 返回 False。 != 不等于 - 比较两个对象是否不相等 (a != b) 返回 True。 &gt; 大于 - 返回x是否大于y (a &gt; b) 返回 False。 &lt; 小于 - 返回x是否小于y。所有比较运算符返回1表示真，返回0表示假。这分别与特殊的变量True和False等价。注意，这些变量名的大写。 (a &lt; b) 返回 True。 &gt;= 大于等于 - 返回x是否大于等于y。 (a &gt;= b) 返回 False。 &lt;= 小于等于 - 返回x是否小于等于y。 (a &lt;= b) 返回 True。 1234567# 比较运算符print(&quot;a==b:&quot;, a==b)print(&quot;a!=b:&quot;, a!=b)print(&quot;a&gt;b:&quot;, a&gt;b)print(&quot;a&lt;b:&quot;, a&lt;b)print(&quot;a&gt;=b:&quot;, a&gt;=b)print(&quot;a&lt;=b:&quot;, a&lt;=b) 输出结果： 123456a==b: Falsea!=b: Truea&gt;b: Falsea&lt;b: Truea&gt;=b: Falsea&lt;=b: True 赋值运算符 运算符 描述 实例 = 简单的赋值运算符 c = a + b 将 a + b 的运算结果赋值为 c += 加法赋值运算符 c += a 等效于 c = c + a -= 减法赋值运算符 c -= a 等效于 c = c - a *= 乘法赋值运算符 c = a 等效于 c = c a /= 除法赋值运算符 c /= a 等效于 c = c / a %= 取模赋值运算符 c %= a 等效于 c = c % a **= 幂赋值运算符 c = a 等效于 c = c a //= 取整除赋值运算符 c //= a 等效于 c = c // a 1234567891011121314151617# 赋值运算符c = a+bprint(c)c+=aprint(c)c-=aprint(c)c*=aprint(c)c/=aprint(c)c%=aprint(c)c=aprint(c)c//=aprint(c) 输出结果： 1234567831413131031.01.0101 位运算符 按位运算符是把数字看作二进制来进行计算的。bin()函数可以把数字转为二进制。 Python中的按位运算法则如下： 下表中变量 a 为 60，b 为 13二进制格式如下： 12345678910111213a = 0011 1100b = 0000 1101-----------------a&amp;b = 0000 1100a|b = 0011 1101a^b = 0011 0001~a = 1100 0011 运算符 描述 实例 &amp; 按位与运算符：参与运算的两个值,如果两个相应位都为1,则该位的结果为1,否则为0 (a &amp; b) 输出结果 12 ，二进制解释： 0000 1100 | 按位或运算符：只要对应的二个二进位有一个为1时，结果位就为1。 (a | b) 输出结果 61 ，二进制解释： 0011 1101 ^ 按位异或运算符：当两对应的二进位相异时，结果为1 (a ^ b) 输出结果 49 ，二进制解释： 0011 0001 ~ 按位取反运算符：对数据的每个二进制位取反,即把1变为0,把0变为1。~x 类似于 -x-1 (~a ) 输出结果 -61 ，二进制解释： 1100 0011， 在一个有符号二进制数的补码形式。 &lt;&lt; 左移动运算符：运算数的各二进位全部左移若干位，由\"&lt;&lt;\"右边的数指定移动的位数，高位丢弃，低位补0。 a &lt;&lt; 2 输出结果 240 ，二进制解释： 1111 0000 &gt;&gt; 右移动运算符：把\"&gt;&gt;\"左边的运算数的各二进位全部右移若干位，\"&gt;&gt;\"右边的数指定移动的位数 a &gt;&gt; 2 输出结果 15 ，二进制解释： 0000 1111 1234567891011# 位运算符print(bin(20))a = 60b = 13print(&quot;a = &quot;, bin(a), &quot;, b = &quot;, bin(b))print(&quot;a&amp;b =&quot;,bin(a&amp;b))print(&quot;a|b =&quot;,bin(a|b))print(&quot;a^b =&quot;,bin(a^b))print(&quot;~a =&quot;,bin(~a))print(&quot;a&lt;&lt;2 = &quot;,bin(a&lt;&lt;2))print(&quot;a&gt;&gt;2 = &quot;,bin(a&gt;&gt;2)) 输出结果： 123456780b10100a = 0b111100 , b = 0b1101a&amp;b = 0b1100a|b = 0b111101a^b = 0b110001~a = -0b111101a&lt;&lt;2 = 0b11110000a&gt;&gt;2 = 0b1111 逻辑运算符 Python语言支持逻辑运算符，以下假设变量 a 为 10, b为 20: 运算符 逻辑表达式 描述 实例 and x and y 布尔\"与\" - 如果 x 为 False，x and y 返回 False，否则它返回 y 的计算值。 (a and b) 返回 20。 or x or y 布尔\"或\" - 如果 x 是 True，它返回 x 的值，否则它返回 y 的计算值。 (a or b) 返回 10。 not not x 布尔\"非\" - 如果 x 为 True，返回 False 。如果 x 为 False，它返回 True。 not(a and b) 返回 False 123456# 逻辑运算符a = 10b = 20print(&quot;a and b = &quot;, a and b)print(&quot;a or b = &quot;, a or b)print(&quot;not(a and b) = &quot;, not(a and b)) 输出结果： 123a and b = 20a or b = 10not(a and b) = False 成员运算符 除了以上的一些运算符之外，Python还支持成员运算符，测试实例中包含了一系列的成员，包括字符串，列表或元组。 运算符 描述 实例 in 如果在指定的序列中找到值返回 True，否则返回 False。 x 在 y 序列中 , 如果 x 在 y 序列中返回 True。 not in 如果在指定的序列中没有找到值返回 True，否则返回 False。 x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 12345# 成员运算符x = [0, 1, 2, 3, 4, 5, 6, 7]y = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]print(&quot;x in y :&quot;, x in y)print(&quot;x not in y :&quot;, x not in y) 输出结果： 12x in y : Falsex not in y : True 身份运算符 身份运算符用于比较两个对象的存储单元 运算符 描述 实例 is is 是判断两个标识符是不是引用自一个对象 x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False is not is not 是判断两个标识符是不是引用自不同对象 x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 注： id() 函数用于获取对象内存地址。 1234567891011# 身份运算符x = 10y = xprint(&quot;x is y:&quot;, x is y)x = 10y = 10print(&quot;x is y:&quot;, x is y)print(&quot;id(x) == id(y)&quot;, id(x) == id(y))print(&quot;x is not y:&quot;, x is not y)id(x) 输出结果： 123456x is y: Truex is y: Trueid(x) == id(y) Truex is not y: False4312793616 is 与 == 区别： is 用于判断两个变量引用对象是否为同一个， == 用于判断引用变量的值是否相等。 运算符优先级 以下表格列出了从最高到最低优先级的所有运算符： 运算符 描述 ** 指数 (最高优先级) ~ + - 按位翻转, 一元加号和减号 (最后两个的方法名为 +@ 和 -@) * / % // 乘，除，取模和取整除 + - 加法减法 &gt;&gt; &lt;&lt; 右移，左移运算符 &amp; 位 'AND' ^ | 位运算符 &lt;= &lt; &gt; &gt;= 比较运算符 == != 等于运算符 = %= /= //= -= += *= **= 赋值运算符 is is not 身份运算符 in not in 成员运算符 not and or 逻辑运算符 注意：Pyhton3 已不支持 &lt;&gt; 运算符，可以使用 != 代替 本教程相关代码可在此查看 ​","link":"/Python-operators/"},{"title":"4. Python的流程控制","text":"Hi，大家好。我是茶桁。 在前面几节课的基础之上，我们今天开始尝试在Python中控制流程。这中间，让我们来做一些实际的练习。 Python语句的分类 让我们先了解一下Python语句的分类。 在Python中，可分为单行代码和代码块/组, 顾名思义，单行代码就是一行的Python代码，而代码块是以冒号作为开始，用缩进划分相同作用域，这样的结构称之为代码块，是一个整体。 12345678# 单行代码a = 123# 代码块if a == 123: print('True')else: print('False') 以上代码中输出结果为： 1True 在输入代码块的时候，我们要注意使用缩进。在其他语言中代码块可能是{}，但是在Python中严格遵守的缩进规则就是代码块。缩进可以是一个Tab距离或者四个空格，可是注意绝对不能混合使用，必须自使用一种方式缩进。 流程控制的分类 什么是流程？流程就是计算机执行代码时候的顺序。 流程可以被分为以下几类： 顺序结构 分支结构/选择结构 循环结构 顺序结构 顺序结构是系统的默认程序结构，自上而下进行执行。 分支结构 分支结构可以让代码走向不同的方向，不同的分支区间。 分支结构中又包含了单向分支，双分支和多分支以及巢状分支。 单向分支 单向分支就是在条件满足之后，执行后续任务。条件不满足的情况下，则不执行。 比如： 123456if 条件表达式： 一条python代码 a = Trueif a: print(&quot;True&quot;) 执行结果： 1True 一个经典案例： 程序员下班前女朋友打电话：下班路上买十个包子回来，如果看到卖西瓜的买一个 123456baozi = 10mxg = Falseif mxg: baozi = 1 print(&quot;买 %s 个包子&quot; %(baozi)) 输出结果： 1买 10 个包子 正常情况下，我们是直接买了10个包子回家，那如果我们看到了卖西瓜的呢？那么这段代码中等于是我们重新赋值了mxg, 就变成： 12345678910baozi = 10mxg = False# 走在路上看到了卖西瓜的，重新赋值mxg = Trueif mxg: baozi = 1 print(&quot;买 %s 个包子&quot; %(baozi)) 输出结果： 1买 1 个包子 双分支 双分支就是在单向分支的基础之上，又多了一个“否则”的选项，当条件不满足的时候执行其他操作。 123456789101112if 条件表达式: 一条python代码else: 另外一条python代码 person = 'girl'if person == 'girl': # 真区间 print(&quot;上前搭讪：美女，能加个微信吗？&quot;)else: # 假区间 print(&quot;直接走开。&quot;) 执行结果： 1上前搭讪：美女，能加个微信吗？ 以上就是一个双向的流程控制，这里面的含义为：表达式成立则执行真区间，如果不成立则执行假区间。 多分支 多分支就是在双分支的基础之上再增加其他可能出现的判断条件，用于执行更多的其他操作。 1234567891011121314if 条件表达式： 一条python代码 ...elif 条件表达式： 一条python代码 ...elif 条件表达式： 一条python代码 ......else: 一条python代码 ... 这段代码中的elif就是可能出现的不同条件，示例如下: 1234567891011score = 59if score &gt;= 90 and score &lt;= 100: print(&quot;奖励一个手机&quot;)elif score &gt;= 80 and score &lt; 90: print(&quot;今晚吃一顿好的奖励一下&quot;)elif score &gt;= 70 and score &lt; 80: print(&quot;鼓励：下次努力加油。&quot;)elif score &gt;= 60 and score &lt; 70: print(&quot;盯紧复习，争取下次进步。&quot;)else: print(&quot;奖励一顿‘竹笋炒肉’&quot;) 执行结果： 1奖励一顿‘竹笋炒肉’ 可以看到以上代码中，是从上到下依次进行判断条件，当所有条件都没有满足的时候，最后走到了else区间。 这就是多分支，需要判断多个表达式的结果，会自行其中符合条件的一个。 巢状分支 巢状分支，也就是嵌套分支。也就是if条件语句的嵌套： 12345678if 条件表达式： 代码语句 if 条件表达式: 代码语句 else: 代码语句else： 代码语句 示例： 1234567891011121314age = 25height = 177sex = 'male'if sex == 'male': # 可以往后判断 if age &gt;= 22 and age &lt;= 35: # 年龄比较合适 if height &gt;= 175: print(&quot;处一下试试...&quot;) else: print(&quot;拉到...&quot;)else: print('当闺蜜吧。') 输出结果： 1处一下试试... 在嵌套分支中我们需要注意，3 ～ 5层嵌套就是极限了，不要再往后嵌套。如果这个层数无法解决你的问题，那么可以重新梳理一下逻辑。基本大部分时候都是逻辑上有问题了。 分支 练习：十二生肖 当用户输入一个四位数的年份，计算当前这个年份对应的生肖： 申猴 酉鸡 戌狗 亥猪 子鼠 丑牛 寅虎 卯兔 辰龙 已蛇 午马 未羊 我们先来做一个用户输入的操作 123# 获取用户输入的年份year = input(&quot;请输入四位数的年份: &quot;)print(year, type(year)) 添加一个type()函数是为了验证用户输入之后的数据类型，当我们输入2023之后，可以看到输出结果为： 12023 &lt;class 'str'&gt; 证明虽然我们输入的是数字，但是被转成了字符串，那这个时候，我们就需要处理一下了： 123# 获取用户输入的年份year = int(input(&quot;请输入四位数的年份: &quot;))print(year, type(year)) 输出结果： 12023 &lt;class 'int'&gt; 这下就对了。 原本我们是需要讲位数，以及范围都控制在合理的数据内的。因为时间关系，在这整个示例中，我就不再去做更多的验证判断了。 123456789101112131415161718192021222324252627282930313233343536# 获取用户输入的年份year = int(input(&quot;请输入四位数的年份: &quot;))#print(year%12)num = year % 12&quot;&quot;&quot;申猴 酉鸡 戌狗 亥猪 子鼠 丑牛 寅虎 卯兔 辰龙 巳蛇 午马 未羊&quot;&quot;&quot;if num == 0: print(f'{year}年是 ==&gt; 申猴')elif num == 1: print(f'{year}年是 ==&gt; 酉鸡')elif num == 2: print(f'{year}年是 ==&gt; 戌狗')elif num == 3: print(f'{year}年是 ==&gt; 亥猪')elif num == 4: print(f'{year}年是 ==&gt; 子鼠')elif num == 5: print(f'{year}年是 ==&gt; 丑牛')elif num == 6: print(f'{year}年是 ==&gt; 寅虎')elif num == 7: print(f'{year}年是 ==&gt; 卯兔')elif num == 8: print(f'{year}年是 ==&gt; 辰龙')elif num == 9: print(f'{year}年是 ==&gt; 巳蛇')elif num == 10: print(f'{year}年是 ==&gt; 午马')elif num == 11: print(f'{year}年是 ==&gt; 未羊')else: print(&quot;您为输入正常的年份&quot;) 当我输入2023的时候，程序输出结果： 12023年是 ==&gt; 卯兔 程序是正常运行了（排除我没做特殊处理可能会出现的BUG），但是我们看这段代码，已经不能用丑陋来形容了。 让我们再改动一下，还记得咱们第二节课程中所学的list吗？这段代码中我们去判断的num是不是和list的下标是一模一样？OK，让我们利用下标来重新写一下这段代码： 123456# 获取用户输入的年份year = int(input('请输入四位数的年份：'))# 定义十二生肖 列表items = ['申猴', '酉鸡', '戌狗', '亥猪', '子鼠', '丑牛', '寅虎', '卯兔', '辰龙', '巳蛇', '午马', '未羊']print(f'{year}年是%s年' %(items[year % 12])) 这段代码输出结果为： 12023年是卯兔年 是不是比起第一段代码来优雅多了？ 循环结构 在完成了分支结构之后，我们来看一下循环结构。循环结构非常重要，必须熟练掌握。 为什么我们需要循环呢？先来看一段代码： 123456print(1)print(2)print(3)print(4)print(5).... 这段代码中，我们重复做了很多次打印的工作。这种事情，其实完全没必要重复去做，交给循环就可以了。 目前在Python中常用的循环有两个，while循环和for...in循环。 while 循环 12345while 条件表达式: 代码内容 代码内容 代码内容 ... 在while循环中，我们通常都会写带有条件变化的循环 1234num = 1while num &lt;=10: print(f'num为{num}') num += 1 输出结果： 12345678910num为1num为2num为3num为4num为5num为6num为7num为8num为9num为10 在这样一段代码中，在进入循环的时候，判断了一下当前条件是否成立。我们先设定了num的值为1，满足进入循环的条件，所以就进入了循环体，然后输出了num的值。 之后，每循环一次我们都对num做一次+1的处理. 也就是更改了变量。当变量更改后，会重新走到循环体的开始去判断条件。在循环11次之后，num就变成了11，不符合进入循环的条件了，循环自然被终止。也可以说，更该变量也是在朝着循环结束的方向在前进。 那么如果我们没有设定这个num的条件变化呢，自然就是无限的循环下去，最终导致内存溢出。 for循环 通常来说，for循环是用来遍历一个容器类型的数据。 1234for 自定义变量 in 容器数据: 代码内容,可以使用自定义变量 代码内容,可以使用自定义变量 代码内容,可以使用自定义变量 使用for...in循环遍历容器类型数据，那么中间的自定义变量就是当前容器类型中的每一个元素。 示例： 123n = '123456789'for i in n: print(i) 输出结果： 123456789123456789 在整个for...in循环体内，我们经常使用range()函数来迭代输出一个范围，比如： 12for i in range(0, 10): print(i) 输出结果为： 123456789100123456789 可以看到我们输出了从0开始，一直到9结束， 一共输出了10个数字。 从结果中，我们大致可以猜到range()函数中(a, b)的含义为：从a开始循环输出，输出到b（不包含b）为止, 比如，我们将刚才的数字改为range(1,8)，那么我们最后输出的内容就会是： 12345671234567 其他流程控制语句 在循环体中，我们还经常应用一些其他的控制语句，用于程序的正常执行和中止。这其中包括 continue语句， 用于跳过当前这一次循环 break语句，用于结束或者跳出 pass语句， 用于占位 12345678num = 1while num &lt;= 10: num += 1 # 判断当前的num是否为偶数 if num % 2 == 0: continue # 跳过本次循环，执行下一次循环 print(num) 输出结果： 12345357911 可以从结果中看到，每次num为偶数的时候，打印并未执行，被跳过了。 让我们来更改一下这一段代码： 123456789num = 1while num &lt;= 10: num += 1 # 判断当前的num是否为偶数 if num % 2 == 0: continue # 跳过本次循环，执行下一次循环 if num == 7: break # 跳出并结束循环，不再继续执行。 print(num) 输出结果为： 1235 结果中我们可以看到，代码只输出到了5。我们来剖析一下整个代码，当代码为5的时候，print()函数还是正常执行了一次，然后再进来的时候，num在最前方+1变为了6，执行了continue，跳出了本次循环。再进入循环之后，num +1 变成了7，这个时候进入了第二个if判断，直接执行了break语句，跳出并结束了整个循环。这样，print()函数这无法再继续执行下去了。 特殊语句 exit() quit() 这两个特殊语句，均是用于结束程序的执行，exit()和quit()之后的代码不会执行。在单纯的循环结构中的作用与break很像，但是完全不能混为一谈。这两个语句是用于结束并退出当前python解释器的，而break仅用于结束当前的循环体。 练习 打印矩形 让我们循环出十行十列 ★ ☆ ，隔一行换色，再做一个隔一列换色。 在最开始，我们先思考一下，十行十列，那就是完成100次打印。我们先把这部分实现一下： 输出结果因为占篇幅，我就不写了，大家自行执行就可以了。 1234num = 0while num &lt; 100: print('☆', end = &quot; &quot;) num += 1 在这之后，我们需要考虑一下，既然是十行十列，那说明我们每隔10个就需要一次换行： 1234567num = 0while num &lt; 100: print('☆', end = &quot; &quot;) # 判断是否需要换行 if num % 10 == 9: print('\\n') num += 1 现在打印出了十行☆, 每一行十个。第一步我们已经实现了，那么现在，让我们来尝试一下隔一行打印一个不同的。思考一下，其实就是奇偶数的问题，想明白之后，接下来就好办了： 123456789101112# 隔列换色num = 0while num &lt; 100: # 判断当前是基数还是偶数 if num % 2 == 0: print('☆', end = &quot; &quot;) else: print('★', end = &quot; &quot;) # 判断是否需要换行 if num % 10 == 9: print('\\n') num += 1 隔列换色实现之后，我们再来考虑一下隔行换色，让我们从隔列换色上找一点灵感。既然隔列换色是奇偶数的问题，那么隔行换色，是不是就是每一行的奇偶数问题？ 那么我们如何对行数做判断呢？其实很简单，我们只要对当前数字做取整数操作：num // 10，然后获取到的整数再来取余就行了。 那么我们就可以这样来实现： 123456789101112# 隔行换色num = 0while num &lt; 100: # 以当前行数为基数，对2取余，判断奇偶 if num // 10 % 2 == 0: print('☆ ', end = &quot; &quot;) else: print('★', end = &quot; &quot;) # 判断是否需要换行 if num % 10 == 9: print('\\n') num += 1 大家可以执行去操作一下试试，建议使用Jupyter Notebook，这种实验性的代码块，很方便得到结果。 打印乘法口诀表 这也是Python教学中经常被拿来进行教学的一个经典案例，和上一个练习一样，我们一边分析，便来完善代码。 整个代码中，我们用到了刚才学到的for...in循环以及range()函数。 首先我们利用range()函数，输出1到9，每输出一个换一次行： 1234# 乘法口诀表for x in range(1, 10): # 换行 print() 然后我们在每一行内再做一次循环，输出每一行的序列, 当然还是从1开始。 12345678# 乘法口诀表for x in range(1, 10): # 第二层循环，内循环 # 内循环负责当前行的函数，第一行 1列 2行 2列....9行 9列 for y in range(1, 10): print(f'{x}x{y}={x*y}', end=&quot; &quot;) # 换行 print() 这里需要注意，就乘法表而言，我们最大列不能大于这一行的被乘数, 那么我们range()需要调整一下： 12345678# 乘法口诀表for x in range(1, 10): # 第二层循环，内循环 # 内循环负责当前行的函数，第一行 1列 2行 2列....9行 9列 for y in range(1, x+1): print(f'{x}x{y}={x*y}', end=&quot; &quot;) # 换行 print() 斐波那契数列 再来让我们多做一个练习，斐波那契数列。 在做这个练习之前，首先我们需要了解什么是斐波那契数列。我这里应用一下维基百科的解释： 斐波那契数所形成的数列称为斐波那契数列。这个数列是由意大利数学家斐波那契在他的《算盘书》中提出。在数学上，斐波那契数是以递归的方法来定义： \\(F_0=0\\) \\(F_1=1\\) \\(F_n=F_{n-1}+F_{n-2}(n&gt;=2)\\) 用文字来说，就是斐波那契数列由0和1开始，之后的斐波那契数就是由之前的两数相加而得出。首几个斐波那契数是：1、 1、 2、 3、 5、 8、 13、 21、 34、 55、 89、 144、 233、 377、 610、 987…… 了解之后，让我们来分析一下： 10, 1, 1, 2, 3, 5, 8, 13... 第0项如果是0，那么第一项是1， 第二项也是1， 之后的第三项开始，每一项都是前面两个数的和。 因为这个数列是一个无限递归下去的数列，我们不能无限的计算下去，所以需要先知道自己计算多少项： 12# 获取用户输入的数据num = int(input('你需要计算多少项？')) 之前我们分析得到，第三项开始，每一项是前面两个数的和，那么，我们需要定义两个变量用来承载相加的两个数，再多定义一个初始值，用于判断是否执行循环： 1234num = int(input('你需要计算多少项？'))n1 = 0n2 = 1count = 2 然后，让我们开始进入正题，需要先判断用户输入的数字是否正整数，我们先不搞那么复杂，只需要简单判断一下是否大于等于0，然后再判断用户输入是否为1， 因为如果是只输出1项，那么就不需要计算了，直接输出n1就好了： 1234567891011num = int(input('你需要计算多少项？'))n1 = 0n2 = 1count = 2# 从之后的数字开始计算if num &lt;= 0: print('请输入一个正整数。')elif num == 1: print(f'斐波那契数列: {n1}')else: pass # 占位 然后，让我们正式进入循环计算, 现在n1为第一项，n2就是第二项，直接输出就可以了 1234567891011num = int(input('你需要计算多少项？'))n1 = 0n2 = 1count = 2# 从之后的数字开始计算if num &lt;= 0: print('请输入一个正整数。')elif num == 1: print(f'斐波那契数列: {n1}')else: print(f'斐波那契数列: {n1}, {n2}', end = &quot;, &quot;) 之后，我们去判断count是否小于用户输入的数字，如果小于，就进入循环。然后再循环内定义一个变量n3， 用来承载相加之后得到的结果，作为当前项输出。再讲n1, n2重新赋值。不要忘了给count加值。 1234567891011121314151617num = int(input('你需要计算多少项？'))n1 = 0n2 = 1count = 2# 从之后的数字开始计算if num &lt;= 0: print('请输入一个正整数。')elif num == 1: print(f'斐波那契数列: {n1}')else: print(f'斐波那契数列: {n1}, {n2}', end = &quot;, &quot;) while count &lt; num: n3 = n1 + n2 print(n3, end = &quot;, &quot;) # 更新数据 n1, n2 = n2, n3 count += 1 当我们输入9的时候，输出结果： 1斐波那契数列: 0, 1, 1, 2, 3, 5, 8, 13, 21, 百钱买百鸡 让我们先来说明一下这个题目： 123一共有100块钱，需要买100只鸡公鸡 3元钱一只，母鸡1元钱一只，小鸡5毛钱一只。要求计算，100块钱买100只鸡，一共有多少种方案 在这个题目里，我们可以计算如果只买一种，这公鸡可以有33只，母鸡有100只，小鸡这可以买200只。 这里面我们可以思考一下，这里我们一共需要3个变量和2个常量，变量为公鸡，母鸡以及小鸡；2个常量为100块钱和总共100只鸡。 先让我们从循环体来开始写： 12345678910num = 0for gj in range(1, 34): for mj in range(1, 101): for xj in range(1, 201): # 判断是否为100只，是否话费100元 if gj + mj + xj == 100 and gj*3 + mj + xj*0.5 == 100: print(f'公鸡{gj}只，母鸡{mj}只，小鸡{xj}只， 共花费{gj*3 + mj + xj*0.5}元') num += 1print(num) 这里，我们是计算了三只都买的情况，那么其实还有一种额外的情况，就是我们一开始说的，100块钱都买母鸡的情况，也正好是100块钱100只鸡。所以，我们的num要从1开始计数 12345678910num = 1for gj in range(1, 34): for mj in range(1, 101): for xj in range(1, 201): # 判断是否为100只，是否话费100元 if gj + mj + xj == 100 and gj*3 + mj + xj*0.5 == 100: print(f'公鸡{gj}只，母鸡{mj}只，小鸡{xj}只， 共花费{gj*3 + mj + xj*0.5}元') num += 1print(num) 输出结果（只看num）： 120 也就是说，我们目前一共有20种组合方案。具体有哪些方案，有兴趣的小伙伴可以执行我所写的代码，会打印出来。 虽然解决问题了，可是这并不是最好的写法。 看看这团垃圾的效率：第一层需要计算34次， 第一层每次计算，第二层都要计算100次，第二层每跑一遍，第三层需要计算200次.... 这简直就是一堆米田共。当我们加上一个计数变量稍微统计一下到底计算了多少次 1234567count = 0... for xj in range(1, 201): count += 1 # 判断是否为100只，是否话费100元...print(count) 可以得到最后结果为： 1660000 是不是很恐怖？让我们改动一下代码，优化性能： 让我们来思考一下，100只鸡这个总数是不是固定不变的？那么公鸡，母鸡的计算得到之后，是不是小鸡的数量就得到了。还有必要在进入一次循环吗？肯定没必要了对不对？所以我们这样改动： 12345678910111213count = 0num = 1for gj in range(1, 34): for mj in range(1, 101): xj = 100 - gj - mj count += 1 # 判断是否为100只，是否话费100元 if gj + mj + xj == 100 and gj*3 + mj + xj*0.5 == 100: print(f'公鸡{gj}只，母鸡{mj}只，小鸡{xj}只， 共花费{gj*3 + mj + xj*0.5}元') num += 1print(f'一共有{num}种组合方式。')print(f'当前一共计算了{count}次') 最后得到的计算结果： 12一共有20种组合方式。当前一共计算了3300次 从660000次一下下降到了3300次，这个性能的提示是很大的了。 所以，很多问题我们不要只追求解决，要善于多思考。 那么至此，我们这节课也就结束了。让我们最后放几个思考题： 思考题 对于我们打印矩阵，完成了隔行上色和隔列上色的问题，我们思考一下该如何解决三角形和菱形； 对于乘法表，思考下我们如何完成反向打印。 解决了思考题的小伙伴，可以在评论区留言。期待看到大家的想法。我是茶桁，咱们下次见，下一节课，我们进入「模块化编程」，开始学习函数。","link":"/Python-process-control/"},{"title":"06 快速建立一个AI应用","text":"Hi，我是茶桁。 在过去的两讲中，我们已经使用 OpenAI 提供的 Embedding 接口完成了文本分类的功能。现在，我们回到 Completion 接口，这一讲将带你更深入地了解该接口的使用。除此之外，我们还将快速搭建一个有界面的聊天机器人，这将让你更好地理解 Completion 接口的应用场景。在这个过程中，你将第一次使用 HuggingFace 这个平台，它是目前最流行的深度学习模型社区。通过 HuggingFace，你可以下载到最新的开源模型，查看其他人提供的示例代码，并参与到社区的交流中。 价廉高质的ChatGPT 我们在第三讲里介绍了Completion接口，并且通过它实现了一个聊天机器人的功能。在那个时候，我们采用的是自己将整个对话拼接起来，将整个上下文都发送给 OpenAI 的 Completion API 的方式。不过，在 3 月 2 日，因为 ChatGPT 的火热，OpenAI 放出了一个直接可以进行对话聊天的接口。这个接口叫做 ChatCompletion，对应的模型叫做 gpt-3.5-turbo，不但用起来更容易了，速度还快，而且价格也是我们之前使用的 text-davinci-003 的十分之一，可谓是物美价廉了。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Quickly-build-an-AI-application/"},{"title":"08 改写和审核","text":"Hi, 我是茶桁。 我们已经介绍了 OpenAI 的主要接口。这是基础知识系列的最后一讲，我们将讨论 OpenAI GPT 系列模型的其他接口。你可能不会经常使用其中一些接口，但了解它们不会有任何坏处，说不定你会在某些需求中用到它们。 在这篇文章中，我们将一起探讨 OpenAI 为文本改写和内容审核提供的功能，以及 GPT 系列模型的种类、区别和应用场景。 文本改写教程 我猜你可能已经用过许多基于 AI 大型语言模型的产品了。其中很常见的一类应用是写作助手，比如 Notion AI。它可以帮助你在文章中选择一段内容，并让 AI 帮你修改它，例如缩短文本或改变语气等。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Rewriting-and-Reviewing/"},{"title":"AI ability practice","text":"Including core capabilities, BI and algorithm related, Code warehouse: 【AI Basic](https://github.com/hivandu/practise/tree/master/AI-basic)","link":"/README/"},{"title":"SQL练习1","text":"-- UPPER 是转换大写的函数 1SELECT emp_name,salary * 12, UPPER(email) FROM employee; -- 使用别名 1234SELECT e.emp_name AS &quot;姓名&quot;, salary * 12 AS &quot;12月的工资&quot;, UPPER(email) &quot;电子邮箱&quot;FROM employee AS e; -- 无表查询 1234567SELECT 1+1;-- Oracle 实现，dual只有一个字段且只包含一行数据SELECT 1+1FROM dual;SELECT * FROM employee WHERE emp_name = '刘备'; -- 比较运算符 123select * from employee where sex &lt;&gt; '男'select * from employee where salary BETWEEN 5000 and 7000select * from employee where emp_name IN('刘备') -- 一个时间段之后入职 1234567select emp_name, hire_date from employee where hire_date &gt; DATE '2018-01-01'SELECT emp_name,hire_date,manager from employee where manager IS NULLSELECT emp_name, sex, salary from employee where sex = '女' AND salary &gt; 10000SELECT emp_name, sex, salary FROM employee WHERE emp_name = '刘备' OR emp_name = '张飞' OR emp_name = '赵云' -- 短路运算 short-circuit evaluation 12345select 'AND' FROM employee WHERE 1 = 0 AND 1/0 = 1;SELECT 'OR' FROM employee where 1 = 1 OR 1/0 = 1;-- NOTselect emp_id,emp_name FROM employee WHERE emp_name NOT IN('刘备','张飞','赵云') -- 运算符优先级 123SELECT emp_name,dept_id,bonusFROM employeeWHERE (dept_id = 2 OR dept_id = 3) AND bonus IS NOT NULL; -- 去处重复值 12SELECT DISTINCT SEX FROM employeeSELECT DISTINCTROW sex from employee / 查找 2018 年 1 月 1 日之后入职，月薪小于 5000，并且奖金小于 1000（包括没有奖金）的员工。 / 12345SELECT emp_id,emp_name,salary,hire_date,bonus FROM employee WHERE hire_date &gt; '2018-01-01' AND salary &lt; 5000 AND (bonus &lt; 1000 OR bonus IS NULL) -- LIKE运算符 1234567SELECT emp_id,emp_name,sexFROM employeeWHERE emp_name LIKE '赵%'SELECT emp_name,emailFROM employeeWHERE email NOT LIKE 'dengzh_@shuguo.com'; -- 转义字符 1234567891011CREATE TABLE t_like(c1 VARCHAR(20))INSERT INTO t_like(c1) VALUES ('进度:25% 已完成')INSERT INTO t_like(c1) VALUES ('日期:2019年5月25日')SELECT c1FROM t_likeWHERE c1 LIKE &quot;%25\\%%&quot;SELECT c1FROM t_likeWHERE c1 LIKE &quot;%25#%%&quot; ESCAPE '#' -- 大小写匹配 123SELECT emp_name,emailFROM employeeWHERE email LIKE 'M%' -- 正则表达式 判断邮箱 1234567891011/*以字母或者数字开头；后面是一个或者多个字母、数组或特殊字符（ . _ - ）；然后是一个 @ 字符；之后包含一个或者多个字母、数组或特殊字符（ . - ）；最后是域名，即 . 以及 2 到 4 个字母。^[a-zA-Z0-9]+[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}$*/SELECT email FROM t_regexpWHERE REGEXP_LIKE (email, BINARY '^[a-z0-9]+[a-z0-9._-]+@[a-z0-9.-]+\\\\.[a-z]{2,4}$','i'); -- 降序排序 1234select emp_name,salary,hire_datefrom employeewhere dept_id = 4ORDER BY salary DESC; -- 多列排序(工资，入职先后) 1234select emp_name,salary,hire_dateFROM employeewhere dept_id = 4ORDER BY salary DESC, hire_date; -- 按照SELECT顺序 1234SELECT emp_name, salary, hire_dateFROM employeeWHERE dept_id = 4ORDER BY 2 desc, 3 中文排序 -- CONVERT 是一个函数，用于转换数据的字符集编码。以下转换为中文GBK字符集 1234SELECT emp_namefrom employeeWHERE dept_id = 4ORDER BY CONVERT(1 USING GBK) -- 空值排序 1234select emp_name,bonusfrom employeewhere dept_id = 2ORDER BY 2 -- 其他空值排序方法 12345-- COALESCE函数将控制转换为一个指定的值SELECT emp_name, COALESCE(bonus,0) AS bonusFROM employeewhere dept_id = 2ORDER BY COALESCE(bonus,0); / 第六节练习: 查询所有的员工信息，按照员工的总收入（年薪加奖金）从高到低进行排序，总收入相同再按照姓名的拼音顺序排序。 / 1234SELECT emp_name, salary, bonus, (salary+bonus) as sumFROM employeeWHERE dept_id &gt;= 0ORDER BY (salary+bonus) DESC, CONVERT(1 USING GBK) -- TopN 排行榜 123456789101112131415161718-- 标准FETCH语法,此语法MySQL不支持，Oracle, PostgreSQL支持SELECT emp_name, salaryFROM employeeORDER BY salary DESCOFFSET 0 ROWSFETCH FIRST 5 ROWS ONLY;-- LIMIT实现TOPN排行榜SELECT emp_name, salaryFROM employeeORDER BY salary DESCLIMIT 5 OFFSET 0;-- 第二种写法SELECT emp_name,salaryFROM employeeORDER BY salary DESCLIMIT 0,5 -- SQL 实现分页查询 1234SELECT emp_name, salaryFROM employeeORDER BY salary DESCLIMIT 10,5 -- 员工排名第3高 123456789select emp_name,salary FROM employeeWHERE salary = ( select salary from employee ORDER BY salary DESC LIMIT 2,1)SELECT emp_name,salary FROM employeeORDER BY salary desc limit 5 offset 10; / 练习：使用LIMIT和OFFSET找出员工表中月薪排名第三高的所有员工 / 123456select emp_name, salary FROM employeewhere salary = ( select salary FROM employee ORDER BY salary desc limit 1 offset 2)","link":"/SQL_ext_1/"},{"title":"SVM-based Text Classification in Practice","text":"The source code: SVM-based Text Classification in Practice 'cnews.train.txt' data cannot be uploaded because it is too large, so it needs to be decompressed and imported after compression. Use SVM to implement a simple text classification based on bag of words and support vector machine. import data 1234# importimport codecsimport osimport jieba Chinese news data is prepared as a sample data set. The number of training data is 50,000 and the number of test data is 10,000. All data is divided into 10 categories: sports, finance, real estate, home furnishing, education, technology, fashion, current affairs, games and entertainment . From the training text, you can load the code, view the data format and samples: 1234567891011data_train = './data/cnews.train.txt' # training data file name data_test = './data/cnews.test.txt' # test data file namevocab = './data/cnews.vocab.txt' # dictionarywith codecs.open(data_train, 'r', 'utf-8') as f: lines = f.readlines()# print sample contentlabel, content = lines[0].strip('\\r\\n').split('\\t')content Take the first item of the training data as an example to segment the loaded news data. Here I use the word segmentation function of LTP, you can also use jieba, and the segmentation results are displayed separated by \"/\" symbols. 123# print word segment resultssegment = jieba.cut(content)print('/'.join(segment)) To sort out the above logic a bit, implement a class to load training and test data and perform word segmentation. 123456789101112131415161718192021222324# cut datadef process_line(idx, line): data = tuple(line.strip('\\r\\n').split('\\t')) if not len(data)==2: return None content_segged = list(jieba.cut(data[1])) if idx % 1000 == 0: print('line number: {}'.format(idx)) return (data[0], content_segged) # data loading methoddef load_data(file): with codecs.open(file, 'r', 'utf-8') as f: lines = f.readlines() data_records = [process_line(idx, line) for idx, line in enumerate(lines)] data_records = [data for data in data_records if data is not None] return data_records# load and process training datatrain_data = load_data(data_train)print('first training data: label {} segment {}'.format(train_data[0][0], '/'.join(train_data[0][1])))# load and process testing datatest_data = load_data(data_test)print('first testing data: label {} segment {}'.format(test_data[0][0], '/'.join(test_data[0][1]))) After spending some time on word segmentation, you can start building a dictionary. The dictionary is built from the training set and sorted by word frequency. 12345678910111213141516171819202122def build_vocab(train_data, thresh): vocab = {'&lt;UNK&gt;': 0} word_count = {} # word frequency for idx, data in enumerate(train_data): content = data[1] for word in content: if word in word_count: word_count[word] += 1 else: word_count[word] = 1 word_list = [(k, v) for k, v in word_count.items()] print('word list length: {}'.format(len(word_list))) word_list.sort(key = lambda x : x[1], reverse = True) # sorted by word frequency word_list_filtered = [word for word in word_list if word[1] &gt; thresh] print('word list length after filtering: {}'.format(len(word_list_filtered))) # construct vocab for word in word_list_filtered: vocab[word[0]] = len(vocab) print('vocab size: {}'.format(len(vocab))) # vocab size is word list size +1 due to unk token return vocabvocab = build_vocab(train_data, 1) In addition, according to category, we know that the label itself also has a \"dictionary\": 12345678910def build_label_vocab(cate_file): label_vocab = {} with codecs.open(cate_file, 'r', 'utf-8') as f: for lines in f: line = lines.strip().split('\\t') label_vocab[line[0]] = int(line[1]) return label_vocablabel_vocab = build_label_vocab('./data/cnews.category.txt')print(f'label vocab: {label_vocab}') Next, construct the id-based training and test sets, because we only consider the bag of words, so the order of words is excluded. Constructed to look like libsvm can eat. Note that because the bag of word model 12345678910111213141516171819202122def construct_trainable_matrix(corpus, vocab, label_vocab, out_file): records = [] for idx, data in enumerate(corpus): if idx % 1000 == 0: print('process {} data'.format(idx)) label = str(label_vocab[data[0]]) # label id token_dict = {} for token in data[1]: token_id = vocab.get(token, 0) if token_id in token_dict: token_dict[token_id] += 1 else: token_dict[token_id] = 1 feature = [str(int(k) + 1) + ':' + str(v) for k,v in token_dict.items()] feature_text = ' '.join(feature) records.append(label + ' ' + feature_text) with open(out_file, 'w') as f: f.write('\\n'.join(records))construct_trainable_matrix(train_data, vocab, label_vocab, './data/train.svm.txt')construct_trainable_matrix(test_data, vocab, label_vocab, './data/test.svm.txt') Training process The remaining core model is simple: use libsvm to train the support vector machine, let your svm eat the training and test files you have processed, and then use the existing method of libsvm to train, we can change different parameter settings . The documentation of libsvm can be viewed here, where the \"-s, -t, -c\" parameters are more important, and they decide what you choose Svm, your choice of kernel function, and your penalty coefficient. 1234567891011121314from libsvm import svmfrom libsvm.svmutil import svm_read_problem,svm_train,svm_predict,svm_save_model,svm_load_model# train svmtrain_label, train_feature = svm_read_problem('./data/train.svm.txt')print(train_label[0], train_feature[0])model=svm_train(train_label,train_feature,'-s 0 -c 5 -t 0 -g 0.5 -e 0.1')# predicttest_label, test_feature = svm_read_problem('./data/test.svm.txt')print(test_label[0], test_feature[0])p_labs, p_acc, p_vals = svm_predict(test_label, test_feature, model)print('accuracy: {}'.format(p_acc)) After a period of training, we can observe the experimental results. You can change different svm types, penalty coefficients, and kernel functions to optimize the results.","link":"/SVM-based_text_classification_in_practice/"},{"title":"11 用好开源模型节约成本","text":"Hi， 大家好，我是茶桁。 直奔主题，我们来谈谈成本这件事。 大家应该都知道，ChatGPT对免费用户是有5美元的API调用额度的，说是这么说，可是那是以前，现在新注册的小伙伴应该都发现自己的API Key根本无法调用API，原因是这个免费额度似乎已经失效了。而我可以直接说，在我从第一节到第10节的课程中所用到的金额，已经超过这个数目了。也就是说，我这10节课API调用成本就已经超过了40元人民币。 看到这大家大概能理解我这个课程为什么改为付费课程了吧？ 对于 ChatCompletion 的接口来说，为了更好地使用它，我们需要传入更多的上下文信息，以便更准确地进行文本生成。不过要注意的是，实际消耗的 Token 数量可能比我们感觉的要多。此外，除了费用之外，数据安全也是我们需要考虑的一个问题。由于每个国家的数据监管要求不同，不是所有的数据都适合通过 OpenAI 的 API 来处理。因此，我们需要寻找一个除 OpenAI 以外的解决方案。幸运的是，有一些开源的大语言模型可以帮助我们解决这个问题。通过利用这些开源的模型，中小型公司也可以轻松地获得更准确、更安全的文本生成服务。 在 Colab 中使用 GPU 在本课中，我们需要使用一些开源模型。但是，并不是所有人的电脑都配备了强劲的 NVIDIA GPU。因此，我建议您使用 Colab 运行相应的笔记本，并注意将运行环境设置为 GPU。 如下图，选择 代码执行程序-&gt;更改运行时类型,然后在硬件加速器上选择 GPU 就可以了。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Save-costs-with-an-open-source-model/"},{"title":"「泰坦尼克」生存预测","text":"最好的学习就是输出,所以虽然这个预测很多人做过了,我还是在这里再做一遍,纯粹是为了自己学习. 前言 这次预测使用的是Sklearn中的决策树模型: 1clf = DecisionTreeClassifier(criterion='entropy') 其中criterion是标准,决定了构造分类树是采用ID3分类树还是CART分类树,对应的取值分别是entropy和gini entropy: 基于信息熵,也就是ID3算法, 实际结果与C4.5相差不大; gini: 默认参数,基于基尼系数. CART算法是基于基尼系数做属性划分的,所以criterion=gini时, 实际上执行的是CART算法. 其完整参数: 123456DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best') 参数代表的含义如下表: 参数表 作用 criterion 在基于特征划分数据集合时，选择特征的标准。默认是 gini,也可以是entropyo splitter 在构造树时，选择属性特征的原则，可以是best或者 random。默认是best,best代表在所有的特征中选择最 好的，random代表在部分特征中选择最好的。 max_depth 决策树的最大深度，我们可以控制决策树的深度来防止 决策树过拟合 max_features 在划分数据集时考虑的最多的特征值数量。为int或float类型。其中int值是每次split时最大特征数；float值是百 分数，即特征数=max_features * n_featureso min_samples_split 当节点的样本数少于min_samples_split时，不再继续分 裂。默认值为2 min_samples_leaf 叶子节点需要的最少样本数。如果某叶子节点数目小于 这个阈值，则会和兄弟节点一起被剪枝。 min_samples_leaf的取值可以是int或float类型。 int类型：代矗小样本数； float类型：表示一个百分比，这是最小样本数 =min_samples_leaf乘以样本数量，并向上取整。 max_leaf_nodes 最大叶子节点数。int类型，默认为None。 默认情况下是不设置最大叶子节点数，特征不多时，不 用设置。特征多时，可以通过设置最大叶子节点数，防 止过拟合。 min_impurity_decrease 节点划分最小不纯度。float类型，默认值为0。 节点的不纯度必须大于这个阈值，否则该节点不再生成 子节点。通过设置，可以限制决策树的增长。 minjmpurity_split 信息増益的阀值。信息増益必须大于这个阀值，否则不 分裂。 class_weight 类别权重。默认为None,也可以是diet或balanced。 diet类型：指定样本各类别的权重，权重大的类别在决策 树构造的时候会进行偏倚。 balanced:算法自己计算权重，样本量少的类别所对应 的样本权重会更高。 presort bool类型，默认是false,表示在拟合前，是否对数据进 行排序来加快树的构建。当数据集较小时，使用 presort=true会加快分类器构造速度。当数据集庞大 时，presort=true会导致整个分类非常缓慢。 在构造决策树分类器后,我们可以使用fit方法让他分类器进行拟合, 使用predict方法对新数据进行预测, 得到预测的分类结果, 也可以使用score方法得到分类器的准确率. fit、predict和score方法的作用如下表: 方法表 作用 fit(features, labels) 通过特征矩阵, 分类表示,让分类器进行拟合 predict(features) 返回预测结果 score(features, labels) 返回准确率 本次数据集一共两个,一个是train.csv, 用于训练, 包含特征信息和存活与否的标签, 一个是test.csv, 测试数据集, 只包含特征信息. 训练集中,包括了以下字段: 字段 描述 Passengerld 乘客编号 Survived 是否幸存 Pclass 船票等级 Name 乘客姓名 Sex 乗客性别 SibSp 亲戚数虽（兄妹、配偶数） Parch 亲戚数虽（父母、子女数） Ticket 船票号码 Fare 船票价格 Cabin 船舱 Embarked 登陆港口 流程 整个流程可以划分为三个阶段: 获取数据 准备阶段 数据探索 数据清洗 特征选择 分类阶段 决策树模型 模型评估&amp;预测 决策树可视化 获取数据 这一步还包含了引入所需依赖 123456789101112# 引入依赖import pandas as pdfrom sklearn.feature_extraction import DictVectorizerfrom sklearn.tree import DecisionTreeClassifierimport os# 准备工作path = os.path.expanduser('~/data/python/Titanic_Data/')# 获取数据train_data = pd.read_csv(path + 'train.csv')test_data = pd.read_csv(path + 'test.csv') 准备阶段 对数据进行探索,分析数据质量,并对数据进行清洗,然后通过特征选择对数据进行降维, 以便于之后进行分类运算; 数据探索 123456train_data.info() # 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度train_data.describe() # 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等train_data.describe(include=['O']) #查看字符串类型 (非数字) 的整体情况train_head(5) # 查看前几行数据 (默认是前 5 行)train_tail(5) # 查看后几行数据 (默认是最后 5 行)train_sample(5) # 查看随机几行数据 (默认是随机1行) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 运行结果&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId 891 non-null int64Survived 891 non-null int64Pclass 891 non-null int64Name 891 non-null objectSex 891 non-null objectAge 714 non-null float64SibSp 891 non-null int64Parch 891 non-null int64Ticket 891 non-null objectFare 891 non-null float64Cabin 204 non-null objectEmbarked 889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.6+ KBNone------------------------------ PassengerId Survived ... Parch Farecount 891.000000 891.000000 ... 891.000000 891.000000mean 446.000000 0.383838 ... 0.381594 32.204208std 257.353842 0.486592 ... 0.806057 49.693429min 1.000000 0.000000 ... 0.000000 0.00000025% 223.500000 0.000000 ... 0.000000 7.91040050% 446.000000 0.000000 ... 0.000000 14.45420075% 668.500000 1.000000 ... 0.000000 31.000000max 891.000000 1.000000 ... 6.000000 512.329200[8 rows x 7 columns]------------------------------ Name Sex ... Cabin Embarkedcount 891 891 ... 204 889unique 891 2 ... 147 3top Peter, Mrs. Catherine (Catherine Rizk) male ... B96 B98 Sfreq 1 577 ... 4 644[4 rows x 5 columns]------------------------------ PassengerId Survived Pclass ... Fare Cabin Embarked0 1 0 3 ... 7.2500 NaN S1 2 1 1 ... 71.2833 C85 C2 3 1 3 ... 7.9250 NaN S3 4 1 1 ... 53.1000 C123 S4 5 0 3 ... 8.0500 NaN S[5 rows x 12 columns]------------------------------ PassengerId Survived Pclass ... Fare Cabin Embarked886 887 0 2 ... 13.00 NaN S887 888 1 1 ... 30.00 B42 S888 889 0 3 ... 23.45 NaN S889 890 1 1 ... 30.00 C148 C890 891 0 3 ... 7.75 NaN Q[5 rows x 12 columns]------------------------------ PassengerId Survived Pclass ... Fare Cabin Embarked619 620 0 2 ... 10.5000 NaN S330 331 1 3 ... 23.2500 NaN Q647 648 1 1 ... 35.5000 A26 C716 717 1 1 ... 227.5250 C45 C860 861 0 3 ... 14.1083 NaN S[5 rows x 12 columns] 数据清洗 探索之后, 我们发现Age、Cabin这两个字段的数据有缺失. 其中, Cabin为船舱, 有大量的缺失值, 在训练集和测试集中的缺失率分别为77%和78%, 无法补齐, Age可以获取平均值进行补齐, 而Embarked是登陆港口, 这个字段也有少量(2个)缺失值, 可以使用最大数据进行补齐. 12345train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)test_data['Age'].fillna(test_data['Age'].mean(), inplace=True)train_data['Embarked'].fillna(train_data['Embarked'].value_counts().idxmax(), inplace=True)test_data['Embarked'].fillna(test_data['Embarked'].value_counts().idxmax(), inplace=True) 分类阶段 特征选择 需要选择有用的字段作为特征,这一步其实很重要: 123456789# 特征选择train_data.columns# 从上一句的结果中选择特征字段features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Parch', 'Embarked']train_features = train_data[features]test_features = test_data[features]train_labels = train_data['Survived'] 这中间有一些事字符串字段, 是不适合进行后续运算的, 需要在这里转变为数值类型,比如Sex字段, 男女两种取值需要转变成0和1 再比如Embarked有S, C, Q三种可能, 我们可以改成Embarked=S, Embarked=C, Embarked=Q三个字段,然后用数值0和1来表示, 其中sklearn特征选择中的DictVectorizer类(上面已引入依赖), 可以处理符号化的对象, 将符号转变为0/1进行表示: 12dvec=DictVectorizer(sparse=False)train_features=dvec.fit_transform(train_features.to_dict(orient='record')) fit_transform这个函数可以讲特征向量转化为特征值矩阵, 我们查看下: 1dvec.feature_names_ 123# 运行结果:['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp'] 我们讲Embarked转化为三列 (['Embarked=C', 'Embarked=Q', 'Embarked=S']), Sex变为了两列 ([Sex=female', 'Sex=male']) 决策树模型 1234# 构造ID3决策树clf=DecisionTreeClassifier(criterion='entropy')# 决策树训练clf.fit(train_features, train_labels) 模型预测 &amp; 评估 我们首先得到测试集的特征值矩阵, 然后使用训练好的决策树clf进行预测, 得到预测结果: 123test_features=dvec.transform(test_features.to_dict(orient='record'))# 决策树预测pred_labels=clf.predict(test_features) 模型评估中,决策树提供了score函数直接得到准确率,但是我们并不知道真实的预测结果,所以无法用预测值和真实的预测结果做比较, 需要使用训练机中的数据进行模型评估, 可以使用决策树自带的score函数计算: 123# 得到决策树准确率acc_decision_tree=round(clf.score(train_features, train_labels), 6)acc_decision_tree 12# 运行结果0.982043 其实,以上准确率评估并不准确,因为我们用训练集做了训练,再用训练集做准确率评估, 并不能代表决策树分类器的准确率. 要统计决策树分类器的准确率, 可以使用K折交叉验证, cross_val_score 函数中的参数 cv 代表对原始数据划分成多少份，也就是我们的 K 值，一般建议 K 值取 10，因此我们可以设置 CV=10 1234import numpy as npfrom sklearn.model_selection import cross_val_score# 使用K折交叉验证, 统计决策树准确率np.mean(cross_val_score(clf, train_features, train_labels, cv=10)) 12# 输出结果0.7778901373283394","link":"/Titanic/"},{"title":"导读：了解AI并使用它&#x2F;他&#x2F;她们","text":"如果你想开始学习AI应用开发，那么在学习之前，有一些学前提醒需要注意。在当今AI爆发的时代，学习AI应用开发需要的学习方法和策略也发生了变化。本课程的目标是通过多尝试、多体验、多做头脑风暴的学习方法，帮助学生在短时间内掌握AI应用开发的基本技能。我们并不会传授过于深奥的数学和理论知识，而是会通过简单易学的API，让学生能够快速上手开发实用价值的AI应用。因此，在学习本课程的过程中，我们鼓励学生多尝试、多体验、多做头脑风暴，以更加轻松、快速地掌握AI应用开发的技能。如果你不知道如何开始，你可以使用Jupyter或者Golab这样的工具来帮助自己入门。 多练习，多尝试，多交流 1. 多尝试运行和修改代码 为什么要多尝试运行和修改代码？因为这是学习AI最有效的方式之一。通过自己亲手运行代码并进行修改，可以更深入地理解算法和模型背后的原理，并从中学到许多实用的技巧。此外，尝试运行和修改代码也能够帮助你更好地掌握编程语言和工具。 而为了方便地进行代码实验，我们可以使用一些开源的工具。例如，Jupyter Notebook 是一个广泛使用的交互式笔记本工具，它支持多种编程语言，并且可以在本地运行。如果你不知道如何搭建环境， 除了本地启动Jupyter之外，你也可以直接使用微软的VSCode，可以直接调用本地Jupyter环境（推荐）。 也可以使用 Google 的 Colaboratory（简称 Colab） 这样的云端工具，只需要一个 Google 账号即可使用。 自然，这也不是全无门槛的，学会如何科学上网是必备技能。这一部分请原谅我无法教授，还是需要自行查找资料。 2. 多体验不同的AI工具 在学习 AI 应用开发的过程中，你需要了解当前市场上涌现的海量 AI 应用。通过体验这些 AI 应用，你可以更好地了解 AI 的能力和应用场景，也能够了解到当前 AI 技术的发展状况。 例如，你可以尝试使用一些人工智能工具来完成自己的工作，比如使用自然语言处理的工具来帮助你写作、使用机器学习的工具来进行数据分析、使用计算机视觉的工具来进行图像处理等等。此外，你还可以体验一些常用的 AI 应用，比如语音助手、智能家居、智能客服、智能医疗等等。通过这些体验，你可以深入了解 AI 技术在实际场景中的应用和效果，从而更好地理解 AI 技术的价值和未来发展方向。 在课程中，你还可以从我推荐中了解到一些最新的 AI 应用。你可以注册账号、下载应用，多去体验一下这些 AI 应用，这不仅能够激发你学习课程的动力，也能够打开你自己利用 AI 大模型能力的思路。 总之，多去体验各类 AI 应用能够帮助你更好地了解 AI 技术的应用和发展现状，也能够激发你对 AI 技术的兴趣和热情，从而更好地进行学习和实践。 本地搭建Stable Diffusion 这里给大家一个小小的建议，尽量不要用线上的图片生成AI去生成商业图片，会有法律隐患的。而如果是在本地架设的情况下，这种问题基本就不存在了。 3. 多交流 与周围的人以及朋友一起多做做头脑风暴，尝试寻找有趣的新产品的机会。事实上，这是一个非常好的建议。AI应用已经涌现出许多，但是有些应用只是简单地使用了现有的API，缺乏创意和创新。然而，还有很多应用具有独特的想法，有些甚至可以直接商业化。学习AI的目的在于学以致用，可以与身边对新一代AI应用有兴趣的人一起探讨，看看课程中介绍的各种方法和技巧能否用于不同的场景和角度。这才是学习这门课程的真正价值。当然，如果你对AI大模型的底层原理有兴趣，可以深入研究其中的数学原理和各种深度学习模型。现在，有能力构建大模型的人实在是太少了，而不是太多了。能够推动通用人工智能向前发展一小步，相信是所有AI从业者都梦寐以求的事情。所以，学习AI，不仅要学习知识，更要发挥创造力，发掘新的应用场景，才能真正做到学以致用。 Stable Diffution 生成的填色图，完全可以变成一项生意，生成多张图做本书：Link 使用AI工具改变你现在的工作方式 随着 ChatGPT、Whisper 和 Stable Diffusion 等强大的 AI 技术的出现，我们的学习和工作方式也需要跟着改变。现在，利用 AI 工具来改造自己的学习和工作流程已经成为一种趋势。通过将 AI 技术应用到各个方面，我们可以获得更加沉浸式的学习体验，同时也能够提高日常生活和工作的效率。在过去的几个月里，我自己也不断地研究和学习新技术，并通过 AI 工具来提高自己的效率。在这篇文章中，我想和大家分享一些我所使用的 AI 工具和优化流程。 ChatGPT，这是一种基于 GPT 技术的人工智能语言模型。我常常利用 ChatGPT 来帮助自己解决问题，比如在学习编程的过程中，我会输入一些代码，然后让 ChatGPT 来帮我检查代码的错误。ChatGPT 还可以用来进行翻译、摘要和生成文章等等。这种 AI 工具可以帮助我们更加高效地学习和工作。 Whisper，这是一种人工智能笔记工具。与传统笔记工具不同的是，Whisper 可以将我们所输入的笔记和文本转化为自然语言，并通过 AI 技术来优化笔记的布局和结构。这样一来，我们可以更加快速和方便地记录学习和工作中的重要信息，并将其整理成易于理解的形式。 Stable Diffusion，这是一种用于大规模数据处理和分析的 AI 工具。在我的研究工作中，我常常需要处理海量的数据，并对数据进行分析和建模。Stable Diffusion 的出现让我能够更加高效地处理数据，并且能够利用 AI 技术来进行数据建模和预测。 利用 AI 工具来改造学习和工作流程已经成为一种趋势。通过利用这些工具，我们可以更加高效地学习和工作，并且可以更加快速地解决问题。当然，这些工具只是 AI 技术应用的冰山一角，未来还将有更多更加强大和智能的 AI 工具出现，让我们拭目以待。 如何使用 ChatGPT 进行学习 随着 AI 技术的发展，ChatGPT 成为了许多人学习知识的“助教”。但是，有些人觉得 ChatGPT 没有多大用处，这可能是因为他们询问了过于宽泛的问题。实际上，ChatGPT 的作用是为我们提供有针对性的回答，只需询问具体问题即可。 当我们学习新知识时，我们可以请 ChatGPT 帮助我们解释我们不理解的内容。与搜索不同，ChatGPT 可以根据我们的追问提供更深入的解释，直到我们完全理解这个知识点为止。此外，ChatGPT 不仅可以解释概念，还可以解释代码。我们可以将需要解释的代码段粘贴到 ChatGPT 中，它将为我们提供详细的讲解。 此外，ChatGPT 作为一个 AI “助教”，它的知识广博、不知疲倦，极其耐心。我们不需要担心问题过于简单或产生心理压力，因为 ChatGPT 不会嫌麻烦或不耐烦。因此，与查找资料或询问他人相比，使用 ChatGPT 可以更高效地解决问题。 问题具体化，将思考的过程交给自己，而获取知识交给ChatGPT，是我最常用的方式 在 Poe 平台中，我们可以选择不同的语言模型，不仅可以使用 ChatGPT，还可以使用其他大型语言模型。这样，我们可以选择最适合自己的模型。 学会使用工具获取额外资料 在当今的数字时代，英文资料已经成为许多行业中获取第一手信息的主要来源。特别是在技术领域，大量的技术文档、API文档和博客文章都是用英文写成的。虽然英文阅读能力是每个人在学习和工作中必须掌握的技能，但对于许多人来说，英文阅读还是比较吃力的。然而，随着机器翻译技术的不断提高，人们已经越来越多地使用翻译插件，将英文资料转换为中英对照版本。 DeepL是一种在线翻译工具，它使用了深度学习技术，可以对英文文本进行准确的翻译。使用DeepL，只需将需要翻译的文本复制粘贴到工具中，它就可以快速将其翻译成目标语言。另外，DeepL还可以通过浏览器插件的形式直接嵌入到浏览器中，当您访问英文网页时，它会自动将其翻译成您的语言。 使用翻译插件可以帮助我们快速浏览英文资料，同时避免了语言障碍。当我们遇到一些翻译不准确的地方，可以快速查看英文原文，确保对资料的理解和应用。同时，翻译插件的中英对照形式也让我们能够更快速地阅读英文资料，从而提高我们的阅读效率。 除了文本资料外，现在还有越来越多的音视频资料，如播客和视频等。这些最新的资料往往只有音频或视频版，而没有文字版。但是通过语音识别和文本摘要技术，我们也可以快速将音频和视频转换为文本，并且生成一个摘要。这样，我们就可以先快速浏览一遍摘要，决定是否值得去完整地听或看。 现在市面上有许多这样的浏览器插件，例如 Glarity，可以帮助我们快速总结视频内容，再来决定是否要看。对于像约翰卡马克这样的大神的访谈，我们可以使用 ChatGPT 背后的语言模型来生成一个摘要，以快速浏览视频内容，确定是否值得花时间去看。 如何通过AI来阅读论文 在科技飞速发展的今天，不断学习新知识，跟上最新的技术进展是非常必要的。读论文是获取新知识的好方法。然而，阅读一篇论文是一项费时费力的工作，因为它通常包含大量的专业术语、公式和图表。但是，随着大型语言模型的出现，我们现在可以借助AI来阅读论文。 现在有很多工具可以帮助我们阅读论文。例如，scispace是一个网站，可以将要阅读的论文上传到其中。然后，我们可以向AI提出问题，以快速了解论文讲解了什么内容。scispace内置了许多你可能会关心的问题，并且可以直接选择回答的语言。此外，在阅读过程中，AI可以对公式、图表等内容进行详细解释，这些工具都可以大大降低阅读论文的门槛，提高掌握这些复杂知识的效率。 除了scispace之外，还有许多其他的工具可以帮助我们阅读论文。例如，ChatPDF是一个可以对PDF文件进行小结和提问的工具。将各种分析报告上传至ChatPDF中，可以快速获取所需的信息。 利用 AI 写代码 Demo：提高生产效率的新工具 GitHub Copilot 是一个利用 AI 技术帮助工程师写代码的工具。通过将需求描述给 ChatGPT，它可以快速生成可用的代码，帮助工程师节省时间和精力。使用 Copilot 写代码的体验非常棒，只需输入注释或代码的开头，Copilot 就能为你生成完整的代码。 对于一些简单的函数调用等胶水代码，Copilot 十有八九是能帮上忙的。即使有些代码不够完美，以它为基础改造比从头开始写更快。当你需要使用一些不熟悉的包时，Copilot 尤其有用。 如果你是一个工程师，安装 Copilot 并使用它写代码是提高生产效率的好方法。此外，使用 ChatGPT 和 Copilot 来帮助写 Demo 代码，可以帮助你快速实验需求，而不必费时查找文档和阅读教程。这些工具能让你更轻松地完成工作，提高你的生产力和效率。 在实际使用中，你可能需要花一些时间来熟悉和调整 Copilot 生成的代码。但是，一旦你熟悉了 Copilot，它将成为你编程工作中最有价值的助手之一。 当然，我是一个穷人，写代码并不是我的主业，所以我一般都使用ChatGPT来完成我为数不多的需求： ChatGPT和Copilot的代码正确率以及BUG率比多数工程师都要来的优秀，自然，也包括我自己。 使用AI获取灵感 现在随着人工智能技术的快速发展，越来越多的AI写作工具涌现出来，让人们在创作过程中更加得心应手。其中，最让人印象深刻的就是AI如何帮助我们获取灵感。虽然AI还没有完全替代人类的思维，但在寻找灵感方面，它们已经展现出了惊人的能力。 在实际的写作中，很多人并不依赖AI产生内容，但是当缺少灵感的时候，AI可以作为一个非常好的助手。例如，当你在围绕一个主题思考写作内容时，可以尝试使用notion.ai等工具寻找灵感。虽然这些工具的很多主意并不新颖，但它们往往可以给你带来意想不到的角度和思路。 此外，你还可以尝试通过人设的不同来让AI从另一个角度帮助你思考问题。例如，为ChatGPT设置一个不同的人设，让它模拟某个领域的专家来帮助你做头脑风暴。这个时候，虽然你只有一个人在思考，但是你却可以组织一个各路大神汇聚的团队帮助你思考问题，让你得到更多有价值的点子。 不仅如此，对于不同领域的人群，还有一些专门的AI工具可以帮助他们获取灵感。例如，Midjourney、Dall-E 2等工具可以让设计师在创作过程中更加得心应手，快速地制作出优秀的设计作品。 接受它，别被它替代 随着科技的快速发展，现代社会正在经历着巨大的变革。许多传统行业和工作岗位正在面临被自动化和数字化取代的风险。因此，我们必须积极地适应新的技术，以免被时代抛弃。尤其是人工智能的发展，不仅给我们的生活带来了诸多便利，也对我们的工作和职业规划产生了重大影响。因此，了解人工智能的基本原理和应用场景，具备使用和开发人工智能的能力，成为未来职场竞争力的关键。同时，我们也需要不断地学习和更新知识，跟上时代的步伐，不断提升自己的技能和能力，以应对快速变化的社会和市场。","link":"/Understanding_and_Utilizing_AI/"},{"title":"05 为文本分类","text":"Hi， 我是茶桁。 在前一讲中，我们观察到大型模型的确表现出良好效果。在情感分析任务中，通过使用 OpenAI API 提供的 Embedding，我们得到的结果要比能在单个机器上运行的较小模型（如T5-base）要好得多。然而，我们之前所选择的问题确实有些过于简单。我们将5个不同的评分分成了正面、负面和中性，同时还排除了相对难以判断的\"中性\"评价，因此我们判断准确率的提高相对较容易实现。但是，如果我们想要准确预测具体的分数呢？这将是我们接下来需要探索的问题。 训练机器学习模型 一种最简单的方法是利用我们获得的文本 Embedding 向量。这次我们将不再直接使用向量之间的距离，而是采用传统的机器学习方法进行分类。这样做的原因在于，仅仅使用向量之间的距离无法最大化利用已标注好的分数信息。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Text-Classification/"},{"title":"利用AI写一个『贪吃蛇游戏』","text":"在完成一些简单的任务之后，我觉得ChatGPT可以帮我完成一些更为复杂，甚至于可能我并不是特别擅长的任务，而我想到了，也许，可以帮我写一个简单的游戏。 贪多嚼不烂的道理我还是明白的，于是，我给自己提前就设定了一个目标： 游戏可以运行实现简单的规则减少BUG的存在可以开始和结束 带着这四个最朴素的愿景，让我们开始吧。 一贯的，一上来我就将问题抛给了chatGPT 1请用Python3.10 编写一个贪吃蛇的游戏，给我完整的代码。 这里我们需要注意一下，你的描述越清晰，得到的结果越有可能是你想要的。在这里，我告诉了chatGPT，我希望用Python写一个游戏，而且版本号是3.10。并且，我告诉了chatGPT，请给我完整的代码。因为如果我不这么说的话，那很有可能chatGPT会先给我一堆的步骤而不是实际的代码。 在之后，我得到了chatGPT的答复： 完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import pygameimport random# 游戏窗口的大小WINDOW_WIDTH = 800WINDOW_HEIGHT = 600# 蛇身和食物的大小BLOCK_SIZE = 20# 定义颜色WHITE = (255, 255, 255)BLACK = (0, 0, 0)RED = (255, 0, 0)# 初始化 Pygamepygame.init()# 创建游戏窗口window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))pygame.display.set_caption(&quot;贪吃蛇游戏&quot;)clock = pygame.time.Clock()def game_loop(): game_over = False # 蛇的初始位置和速度 snake_x = WINDOW_WIDTH // 2 snake_y = WINDOW_HEIGHT // 2 snake_x_change = 0 snake_y_change = 0 # 食物的初始位置 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE while not game_over: for event in pygame.event.get(): if event.type == pygame.QUIT: game_over = True elif event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT: snake_x_change = -BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_RIGHT: snake_x_change = BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_UP: snake_y_change = -BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_DOWN: snake_y_change = BLOCK_SIZE snake_x_change = 0 # 更新蛇的位置 snake_x += snake_x_change snake_y += snake_y_change # 绘制游戏窗口 window.fill(BLACK) pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.draw.rect(window, WHITE, [snake_x, snake_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.display.update() # 碰撞检测 if snake_x == food_x and snake_y == food_y: food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE clock.tick(20) # 控制游戏的帧率 pygame.quit()# 启动游戏game_loop() 从代码里我们可以看出来，文件引用了库pygame以及random, 其中，pygame需要我们单独安装一下: 1conda install pygame 建立了一个snake_v1.py的文件，并把这段代码粘贴到文件里之后，我开始尝试运行它： 1python ~/xx/game/snake_v1.py 事情果然没有想的那么简单，根本就是毫无动静。我开始询问chatGPT chatGPT给到的答案似乎没有什么意义，这些事情在开始之前我就已经做好了。在短暂的思考之后，我忽然想到，是否因为我是Mac系统，从而导致了窗口不出现，于是我继续问问题： 然后继续问： 这一次，我抓到了重点。不明白为什么刚才我提到我是Mac系统的时候他不告诉我，先不管这些，在其中添加这段代码后，游戏终于可以运行了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import pygameimport random# 游戏窗口的大小WINDOW_WIDTH = 800WINDOW_HEIGHT = 600# 蛇身和食物的大小BLOCK_SIZE = 20# 定义颜色WHITE = (255, 255, 255)BLACK = (0, 0, 0)RED = (255, 0, 0)# 初始化 Pygamepygame.init()# 创建游戏窗口window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))pygame.display.set_caption(&quot;贪吃蛇游戏&quot;)pygame.display.flip()clock = pygame.time.Clock()def game_loop(): game_over = False # 蛇的初始位置和速度 snake_x = WINDOW_WIDTH // 2 snake_y = WINDOW_HEIGHT // 2 snake_x_change = 0 snake_y_change = 0 # 食物的初始位置 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE while not game_over: for event in pygame.event.get(): if event.type == pygame.QUIT: game_over = True elif event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT: snake_x_change = -BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_RIGHT: snake_x_change = BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_UP: snake_y_change = -BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_DOWN: snake_y_change = BLOCK_SIZE snake_x_change = 0 # 更新蛇的位置 snake_x += snake_x_change snake_y += snake_y_change # 绘制游戏窗口 window.fill(BLACK) pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.draw.rect(window, WHITE, [snake_x, snake_y, BLOCK_SIZE, BLOCK_SIZE]) pygame.display.update() # 碰撞检测 if snake_x == food_x and snake_y == food_y: food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE clock.tick(20) # 控制游戏的帧率 pygame.quit()# 启动游戏game_loop() 不知道各位看出什么问题了没有，问题如下： 动作太快了，完全是地狱级别的难度； 蛇似乎不会增长，吃了物品没有变长； 撞到墙壁后游戏虽然停止了，但是没任何提示，蛇就这么消失了； 于是，我只能让chatGPT再帮我优化代码： 当然，又有了其他问题， 蛇在撞到自己之后没有办法结束游戏 游戏无法退出 游戏无法重新开始 Game Over之后显示的是乱码 当然，有编程经验的人大概能明白，乱码的话一定是因为中文显示不正确，而目前，我也没打算支持中文。干脆用英文好了。 于是，我又开始提需求，在连续三四轮这种询问之后，终于来到了最后一次： 然后我得到了如下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137import pygameimport random# 游戏窗口的大小WINDOW_WIDTH = 800WINDOW_HEIGHT = 600# 蛇身和食物的大小BLOCK_SIZE = 20# 定义颜色WHITE = (255, 255, 255)BLACK = (0, 0, 0)RED = (255, 0, 0)# 初始化 Pygamepygame.init()# 创建游戏窗口window = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))pygame.display.set_caption(&quot;贪吃蛇游戏&quot;)pygame.display.flip()clock = pygame.time.Clock()def game_loop(): game_over = False game_restart = False game_paused = False while not game_restart: if game_over: show_game_over() # 蛇的初始位置和速度 snake_x = WINDOW_WIDTH // 2 snake_y = WINDOW_HEIGHT // 2 snake_x_change = 0 snake_y_change = 0 # 蛇身的初始位置和长度 snake_body = [] snake_length = 1 # 食物的初始位置 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE score = 0 while not game_over: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() quit() elif event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT and snake_x_change != BLOCK_SIZE: snake_x_change = -BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_RIGHT and snake_x_change != -BLOCK_SIZE: snake_x_change = BLOCK_SIZE snake_y_change = 0 elif event.key == pygame.K_UP and snake_y_change != BLOCK_SIZE: snake_y_change = -BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_DOWN and snake_y_change != -BLOCK_SIZE: snake_y_change = BLOCK_SIZE snake_x_change = 0 elif event.key == pygame.K_SPACE: game_paused = not game_paused elif event.key == pygame.K_q: pygame.quit() quit() if game_paused: continue # 更新蛇的位置 snake_x += snake_x_change snake_y += snake_y_change # 碰撞检测 if snake_x == food_x and snake_y == food_y: # 蛇吃到了食物，增加长度和得分 snake_length += 1 score += 1 food_x = round(random.randrange(0, WINDOW_WIDTH - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE food_y = round(random.randrange(0, WINDOW_HEIGHT - BLOCK_SIZE) / BLOCK_SIZE) * BLOCK_SIZE # 更新蛇的身体 snake_head = [snake_x, snake_y] snake_body.append(snake_head) if len(snake_body) &gt; snake_length: del snake_body[0] # 检测碰撞 if snake_head in snake_body[:-1] or snake_x &lt; 0 or snake_x &gt;= WINDOW_WIDTH or snake_y &lt; 0 or snake_y &gt;= WINDOW_HEIGHT: game_over = True # 绘制游戏窗口 window.fill(BLACK) pygame.draw.rect(window, RED, [food_x, food_y, BLOCK_SIZE, BLOCK_SIZE]) for body_part in snake_body: pygame.draw.rect(window, WHITE, [body_part[0], body_part[1], BLOCK_SIZE, BLOCK_SIZE]) # 显示实时分数 font = pygame.font.Font(None, 28) text = font.render(&quot;Score: &quot; + str(score), True, WHITE) window.blit(text, (10, 10)) pygame.display.update() clock.tick(10) # 控制游戏的帧率 # 重新开始游戏 game_loop()def show_game_over(): font = pygame.font.Font(None, 36) text = font.render(&quot;Game Over. Press Space to restart&quot;, True, WHITE) text_rect = text.get_rect(center=(WINDOW_WIDTH // 2, WINDOW_HEIGHT // 2)) while True: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() quit() elif event.type == pygame.KEYDOWN and event.key == pygame.K_SPACE: game_loop() window.fill(BLACK) window.blit(text, text_rect) pygame.display.update() clock.tick(10)# 启动游戏game_loop() 大家可以看看效果： 我们可以看到，这个游戏相对来说是比较完整了，速度正常了，有计分系统，蛇能正常增长，碰到自己游戏会结束，碰到墙壁后也会Game Over，并且可以通过SPACE来重新开始。并且，我还加入了按Q键的时候退出的功能。 当然，还可以继续完善，比如随着时间的流逝，速度上可以渐渐加快等等。就看你怎么想，然后其他的交给chatGPT。 通过这次的示例演示，其中重点不是教大家如何做一个贪吃蛇游戏，而是教大家如何利用chatGPT来解决你需要解决的问题。当然，我需要收回我开头说的话，chatGPT并不能帮你解决你不熟悉的问题。就比如，如果我完全不懂这其中内容的话，可能我窗口都打不开，我完全都不知道我什么时候才能解决Mac系统中不一样的部分，而也正是因为有一些简单的经验，才让我考虑的那个层面，从而针对性提问解决了问题。 所以要记住，AI并不能帮你解决你完全不懂的问题，起码，你要知道你想问什么，也要知道问题大概卡在哪里了，针对性继续提问。 最后，友情提示一下，不要用API来完成这一次次的对话，经验之谈，去买个Plus，比API交互便宜多了。你看那一串串的代码重复的给你写出来，你完全不知道会耗费多少Token。那些宝贵的Token，还是用在聊天窗无法完成的任务上比较合适。","link":"/Use-AI-to-write-a-snake-game/"},{"title":"10 利用AI索引并分析文献和图片","text":"Hi, 我是茶桁。 看到我这篇文章的读者们不知道有多少人是接触过ChatGPT或者其他人工智能产品的。 市面上目前充斥着大量的人工智能产品，从聊天，文案，脚本，音乐，绘画等方方面面都涵盖了。但是不知道有多少人遇到过以下的场景不知道该如何解决： 我需要针对一篇很长的文章（可以是论文，可以是小说）进行总结或者分析的时候，就开始无从下手。因为ChatGPT在接收长度上是有限制的，这个长度我大概测试过，如果你用的是WebGPT，那么应该中文应该是在2500字左右，多一个字都会告诉你长度超出限制。而我们一篇论文，起码来说都是5000字以上的。分两段来喂给ChatGPT当然可以，但是上下文关联有时候会遇到问题，ChatGPT也会给你胡编乱造。 有的时候我从客户那里接收到的是一张图片，也许是截图，也许就是拍的一张照片。那么，怎样利用ChatGPT去分析这张图片上的内容，然后根据我的需求给我相应的答案呢？ 以上这两点，估计是很多人遇到想解决的。而今天这篇文章，就是从这两点入手教你如何解决。 大语言模型的不足 让我们打开ChatGPT来问一些常识性的问题，这个问题对于大部分上过学的中国人来说，都能从课本上了解到： “鲁迅先生在日本学习医学的老师是谁？” 结果如下图，这个“嘉泽源之助”到底是谁呢？我也不知道，得到这个答案的时候，我还特意去Google了一下，根本找不到相关资料。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Use-AI-to-index-and-analyze-documents-and-images/"},{"title":"13 使用多步提示语让AI帮你写测试","text":"Hi，大家好，我是茶桁。 很遗憾在上一讲，也就是第12讲的时候，咱们对于利用AI写一个VBA宏来执行Excel任务的过程并不顺利，仔细想来既然大家都在这里看这个系列文章了，应该也基本都会Python的，所以一个Excel自动化也并无太大影响，毕竟，这种商业软件的集成一定是早晚的事情，咱们也不必在这里死磕这一个问题。 那么本节课程呢，我们会通过chatGPT的不断交互，去完成一个测试任务。 在很多时候，我们探索性开发一些功能可以极大提高我们的效率，但是这个过程并不能做成一个完整的产品。我们理想中的产品应该是“自动化”的，我们只需要用自然语言输入自己的需求，对应的代码就自动写出来了。 那么如果中间出现了问题怎么办？当然是AI可以自己拿到反馈自己更正自己了，完全不需要人工去介入调试。 下面，让我们开始吧。 代码的起源 让AI自己调试自己的需求听起来是不是很不可思议？随着GPT-4的发布，还有就是未来模型能力的进一步增强，这个骑士并不是遥不可及。是的，我又在这里贩卖焦虑了，那些低廉的测试们，想要自己的退路了吗？ 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/Use-multi-step-prompts-to-ask-AI-to-write-tests-for-you/"},{"title":"使用Telnet数据流看世界杯","text":"使用Telnet数据流创造了正在进行比赛的ASCII影像, 尽管这难以令人想象. 看看他是怎么做的… 在比赛开始10分钟以前, 简单的打开Windows开始-运行窗口, 输入 telnet ascii-wm.net 2006 , 你将会看到 “现场直播” 视频流. 明显的, 这是互联网应用的又一创新. 来试一下吧! http://ascii-wm.net/ 由于我家里只有电脑没有电视，所以全程我都是这么干的。。。可惜，经常与主机断开连接。。而且，很多时候我看不懂。。。郁闷。只能知道个大概。。有兴趣的可以去看看＠","link":"/Use-the-telnet-data-stream-to-watch-the-World-Cup/"},{"title":"VH 情怀黄铜原子笔","text":"「知乎专栏地址」 收到 @罗文森 赠送的笔已经很多天了，快两个星期了吧。 一直在找时间想写一篇评测出来，终究是没抽出时间，况且，我不是写手！眼看时间一天天过去，心里也越来越愧疚。 其实最主要的也不单单是写不出什么，而是总归要用它画两幅画出来，才会感觉的出来到底合不合心意。 对于文具的偏爱，估计是从学生时代就开始了。记得高中的时候缠着老爸给买了第一个奢侈品：一支派克。虽然很珍惜，却并不好用。自此对文具就更是挑剔。 一支好笔，真的能陪一个人好久好久。 打拆包之前，就一股浓浓的逼格。 不过问题来了，我不知道怎么打开包装！XD - 一直不敢使劲，生怕损坏了什么，折腾了好久，才明白，原来连着盒盖的那张纸，是用来撕的。 打开之后，金色的笔体和其皮外套分开躺在盒内，逼格更甚了。 喜欢盒子内部的那句话：“合适的形态总有他合适的作用，还原物的本质，至真至纯，用一支笔唤起书写的初心。”， 好吧，又是一个走情怀的产品。不过对于这个包装和笔本身的设计来说，这个情怀我还是蛮受用的。 而关于细节上，笔的打磨很是花费了一番功夫，那种黄铜的质感以及似乎岁月沉淀的感觉，让人爱不释手。（这些天只要是要签字的机会，我基本都拿出它来装逼）。 而关于旋转笔头，旋转的过程中特别的舒服。到位后的力回馈也刚刚好，让我转来转去玩了好久。额，给玩的不灵光了。（看来关于使用强度，长家还是需要加强）。 关于那件小皮装，似乎有点小了，装进去是用了一些力气的。额，拿出来就更费劲了，太紧了。不过相信用一段时间，也就松了。对于这样一直精致的情怀笔，这件皮衣还是相当必要的。 笔当然还是要用才能知道是否合心意, VH 这支黄铜笔，拿在手上的感觉，重量十足，根本是那些塑料笔没办法比的。但是这个重量不大不小，真的正合适。想起厂商吹的牛逼：“打造了最佳的配重”。 这牛逼不为过。 至于握感上，这个笔对于我来说似乎纤细了一点，可能也是因为我握惯了粗的笔，包括我最爱的那支53Pencil，也是比较粗的。而在厂家原装的那支笔芯，使用起来也是非常顺畅（就喜欢这样顺畅的笔芯）。不过用完需要更换笔芯的时候，就需要买好一点了，要不可惜了这么好的笔。 最后随笔画了两幅画来试笔。 太久不画画了，大家将就看吧！ 有想买的，链接在这里 : 首页-vh企业店 PS：VH家的那个无线充电器，也是逼格满满。打算入一个。。。","link":"/VH-brass-pen/"},{"title":"VPS 设置 Hexo","text":"首先需要感谢@lucifr，我现在这篇文就是在iPad上登录VPS完成的。最后还是忍不住入手了下边两个APP: 当然我其实到现在并不完美，因为rsync和自动执行generate的代码我没有完成。安装incrond的时候总会出错，于是无法执行集群文件同步.所以现在还是在终端里执行generate和cp -rf /home/xxxx/* /home/xxxx 我这里并不是要教设置步骤，因为其实@lucifr 已经在他的这篇文里写的很清楚了，我就写几点注意事项 搞定VPS操作和基本的Linux命令很重要。 要搞定lnmp，参照这里的lnmp详细介绍 新版本的Hexo有更改，在同一目录里是找不到/cli/generate.js的，更别说console.log语句了 @lucifr所说的新建立一个Dropbox账户，意思是在VPS主机上建立一个账户用来执行Dropbox同步，而不是新建立一个Dropbox账户。 其他… 好吧，写其他是因为iPad上用VI进行编辑实在有点难受，现在先这样了，以后有时间了再写一个更详细的。","link":"/VPS-setting-hexo/"},{"title":"VSCO FILM 00 FREE STARTER PACK","text":"本文知乎专栏 VSCOCam是iOS上一款滤镜相机，其最著名的地方就是他们的胶片滤镜。 而其实VSCO在这款APP之前，就一直在做胶片滤镜，有OS X和WIN两个平台的版本。最主要的是作为LightRoom的插件存在。 而现在提供免费下载的这款滤镜就是针对LightRoom的滤镜插件，有OS X和WIN两个版本提供。 We are thrilled to announce VSCO Film 00, our first-ever FREE starter pack. Perfect for anyone who uses VSCO Cam and is looking to take the next step, VSCO Film 00 brings beautiful presets, custom camera profiles, and the familiar VSCO editing experience to your desktop in Adobe Lightroom. VSCO Film 00 includes two of our most popular film stocks (Kodak Gold 100 from Film 05 and Kodak Tri-X from Film 06) and is available for download now. 有需要的可以点击原文链接进行下载，不要感谢我，我是雷锋。 PS: 关于之前我提到的Enlight导出图片丢失数据的部分，我在此向读者和开发者致歉，原因是导出的如果是PNG图片是不能保存那些信息的，可以将图片的质量从Pro调小到Hight, 保存的就是JPEG图片，那么导出的图片地理位置和相机信息就会一并保存了。","link":"/VSCO-FILM-00-FREE-STARTER-PACK/"},{"title":"关于我","text":"茶桁是我笔名，80后, 老派思想者, 固守着年代感的东西... 和大部分这个年代成长起来的孩子一样, 怀念着龙珠, 灌篮高手, 追着海贼王. Github: @hivandu Twitter: @hivan Design: @hivandu Notes: @DUART 对新奇的事物仍然抱持敏感而探索的心境, 并且喜欢有条不紊. 坚信有生之年能看到人类和AI和平共处的景象, 坚信有生之年可以看到第一批星际移民. 这里, 我们谈谈思维, 哲学, 经济以及信息管理. 还会夹杂着我的一些私货, 那都是一些遥远的技能, 已经离我而去很久了. 说了那么多, 先搬家吧. 另外, 在我另一个试验田里, 有着自己对产品和技术的追求, 也欢迎大家围观; 这篇博客是简历在Hexo + Github Pages + vercel上, 有兴趣的小伙伴可以自行Google一下相关教程, 这里就不引述了.","link":"/about/"},{"title":"Alfred 2 Plugin -- open in Atom","text":"用于在Alfred 2中用命令快速调用Atom打开所见文档，类似于Open in Sublime， 为了自己方便建立了一个workflow，有需要的下载吧！ Open in Atom.alfredworkflow","link":"/alfred-2-plugin----open-in-atom/"},{"title":"android 2.1版本无法开机解决","text":"不含刷的变砖的机子！ 这个版本的rom是一定需要SPL的，比起HTC hero的rom来说速度有提升！但是没那么华丽(当然我说的是源生系统，我对theme之类的不太有兴趣！)。。。 但是刷SPL有风险，请慎重行事！ 关于刷机2.1版本后无限火花不开机的情况，主要是Recovery的问题，刷成V2.5应该都可以解决！本人测试OK。 手机端安装可以下载程序：flashrec.apk PC端安装需要USB驱动，然后通过更新工具更新。这里一定要注意区别清楚机型版本，不同机型是不能通用的，再次提示，一定要注意看清楚，千万不要着急。 点击此行下载用于 G1 和 沃达丰版 G2 使用的更新程序 大小: 5369404 字节 MD5: 8CA35537D253EB19CC0D28A45D153FC4 SHA1: 88C3E3ED444A1B96C59594215A81B5C5F589C42E CRC32: 97089632 点击此行下载用于 HTC 版 G2 使用的更新程序 大小: 4848659 字节 MD5: 7217E7DCCFEAB4BFB254B86778FFBD5C SHA1: 8373BDB338D3E6EBA8622CDC19EFBF7AE3484451 CRC32: A9BD3E5E 再次提醒，千万别下错了！ 保证手机与电脑连接无误后，双击更新程序运行即可。","link":"/android-2-1/"},{"title":"Android 2.2 App2sd 问题","text":"其实比起以前版本的app2sd来说,设置是一样的!只是多了一个步骤,就是需要给rom添加一个sdext.然后所有的设置就和以前的版本一模一样了. 首先当然需要有一个已经分好区的sdcard,具体设置可以查看我以前的文,有ubuntu下进行分区的和windows下的! 然后需要最近版本的SPL和Radio,这个本人不提供了,可以自行解决!伸手党可以留下自己的邮箱,我提供下载地址! 准备工作做足后,第一步就是需要在手机上建立一个sdext访问.这里提供一个文本文档:下载： fr-patch134.zip 放在sdcard根目录,然后在连接手机的情况下在终端如下操作: adb shell # sh /sdcard/fr-patch134.txt sdext busybox df -h 如果看到有/sd-ext分区,OK,以下的事情就顺理成章了,参考我以前发布的app2sd步骤操作就好了! 至此所有问题解决! 此处为后续更新,由于之前忽略了点东西,所以这里做一个补充! 由于2.2rom和以前版本的一些差别,在做app2sd之前,需要挂在system,sd-ext和data分区,这是需要注意的一点!挂在命令为:mount 例子: mount system 有什么不明白的再问吧!","link":"/android-2-2-app2sd/"},{"title":"Android 2.2 for G1","text":"親愛的Android,我回歸了...說實話,後兩個月我真的有些壓抑.現在感覺解脫了! Android 2.2 For G1 已經被C神放出,貌似解決了相機問題,而其他問題暫時沒有進行測試,因為本人也正在下載中...萬分期待! 先提供下載地址,等待我適用後再放試用報告.. http://drop.io/ionstorm/asset/defcon-dream-ota-eng-t1-signed-zip PS:如果沒有語言要求可以下載這個版本就OK了,如果強烈需求中文,需要下載中文補丁包 试用报告: 仍然没有中文 系统默认没有中文输入法(这个到不是问题) 没有Google Map,市场无法下到.并且安装了4.2开发版后打开就崩溃(这点对于喜欢Buzz的朋友是个致命伤) 3D图库效果有. 相机不能用,和2.1不同的是,就算拍照有图片,打开来看也是一个android小人.... WIFI正常使用 蓝牙没有测试. 速度真的比2.1快了不少,没有一点卡的感觉. Settings里有CyanogenMod settings,可以直接更新Rom,可是我这里链接失败. Apn需要自行设置,具体的设置方法可在网上Google Vpn使用正常 因为地图没有打开,所以GPS模块没有测试. 其他不进行补充了,由于以上有些原因是我无法接受的.所以打算刷回1.6rom,以前刷2.1是为了绑定自己和公司双帐号,现在不必了!不用忍受2.1的速度了...","link":"/android-2-2-for-g1/"},{"title":"Android 3.0 Preview","text":"整个界面都显得很陌生啊!大家可以从视频中看到. 锁屏界面更改很大,目前不知道是解锁手势还是横向滑动还是画圈. HomeScreen主屏幕，可以看到四角的设计，左上给搜索按钮，支持语音搜索，右上为应用菜单按钮，左下为返回、Home、菜单按钮，右下角则是状态标志，屏幕中间为程序快捷菜单和桌面工具 有一个社交工具的集合，可以查看不同来源的好友更新.不过这个在大陆的情况就...你们懂得! 有个桌面管理器,可以进行桌面的各项设置. 拥有全新的浏览器，可以查看各种完整版网页 拥有全新的Android版本Gmail界面! Gtalk界面也是全新的,并且支持双向视频聊天!不过流量上....不敢想! 全新的Youtube视频墙,不过在国内,我们都是墙内的,它在墙外而已! &lt;li&gt;全新的Google地图界面,支持3D模式导航!&lt;/li&gt; 此外还会有Google图书等更多全新内容!","link":"/android-3-0-preview/"},{"title":"Android 3.0","text":"好吧,7.1号Eldar Murtazin在其博客上已经放出一个可信度比较高的谣言,就是Android3.0将于十月份发布,代号姜饼(Gingerbread) 据传,3.0rom将会针对高端市场,分辨率达到了1280X760,支持此系统的最低配置将为1Ghz处理器,512M内存.可以这么说,在3.0系统发布以后,就可以正式宣布G1被彻底淘汰了. Android 3.0 Gingerbread will be released in mid- October (around 15 -16th), 2010. First handsets shipping in November/December – for the Holiday Season. Minimum hardware requirements for Android 3.0 devices are: 1GHZ CPU, 512MB or RAM, displays from 3.5” and higher. (We all, of course, heard that Android handsets with 2GHz CPU’s are coming) New 1280×760 resolution available for the devices with displays of 4” and higher. (Anyone thinking about Android tablets now? ) Completely revamped user interface. If you want to get a feeling of what Android 3.0 Gingerbread UX is like, check out the Gallery App on Nexus One. The same overall feel, light animated transitions,etc. Natively, through all the UI. Android’s split into 2 branches becomes official. 3.0 for top of the line/high end devices. Cheap, low-end mass market handsets will keep Android 2.1/2.2 详情可以参看:http://www.unwiredview.com/2010/06/30/android-3-0-gingerbread-details-1280x760-resolution-1ghz-minimum-specs-mid-oct-release/","link":"/android-3-0/"},{"title":"Android 4.0 通讯录与Google+的深度整合","text":"在4.0以前,我记得Android里的通讯录名称是“contact”.而在4.0之后,我的手机上是4.0.3,将其更改为“people”了. 而更改的不仅仅是这小小的名称!大家都记得在之前版本的contact里,拨打界面和通讯录界面是整合在一起的.所以那个时候我第一屏单单放一个拨打电话就OK了.需要查找通讯录的话,可以切换tab. 1.理念: 而在4.0里面,你在people里面找不到拨打电话界面,而在拨打电话的界面里也找不到通讯录.Google将这两个单独分立了开来!貌似不方便了,其实不是,反而变得异常容易整理和操作!Google在Android4.0里的理念就是将电话和联系人完全分离开,这本来就不是一回事!智能手机时代,谁说的联系人就一定只能是打电话和发短信的? 2.整合: 说了些小变化,下面切入正题,就是Android 4.0 之后通讯录与Google+的深度整合! 这也是我昨晚没事整理联系人的时候才注意到的,不知道是不是4.0之处就是这样的,还是到了4.0.3的改变! 当我们进入一个单独的联系人界面的时候,发现除了常用的PHONE,EMAIL等选项之后,会有一个CONNECTIONS项目选项,这里会显示手机上安装过的SNS程序里联系人的关联账户! 比如WhatsApp,Twitter或Facebook等! 当然,你也有可能看到一个Add connection选项. 这个选项,其实就是和Google+整合的选单! 可以选择添加联系人到自己的Google+ 圈子里!当然,有可能你的联系人根本没有在Google+注册过! 我猜测,Google以后有可能会将Google+的圈子代替联系人来使用!也就是说,你所有的联系人都是要圈养的.不管他是否注册过Google+,利用圈子的概念管理联系人,其实比较起来而言,比Gmail里的通讯录要高效一点! 接下来,才是深度整合的重点,以我自己为例: 用过4.0的朋友都知道,联系人选单是可以向左拖动的,就是右边还有一块和联系人有关的“update”选项!当然,这一块内容需要对方联系人有Gtalk或者Google+才会显现出来! 平时就会显示Gtalk上的签名状态,而对于有Google+的联系人,则会显示他在Google+中的信息! 我们大家应该都用过MSN或者QQ,应该可以想象一下,MSN当初和Space深度整合,QQ和QQ空间深度整合的情形! 当你查看某个联系人的时候,他在自己空间里发表的文章或相片,都会在其信息里显示出来!当然,QQ有的时候会闹点小情绪,来一两次大姨妈…我们可能没那么及时看到!所以说这个概念并不新奇,只是这次Google借用到了手机上而已! 哈哈,想象一下,够方便吧? 3.想法: 写到这里,我到觉得Google有些小气了. 右边状态栏完全可以让出来给其他的SNS APP来使用而不只是Google+,譬如Twitter和Facebook,或者是QQ空间!既然有了这样一个功能,而又是在消费者手上的,那么决定权就该交给消费者!否则,自己联系人里没多少用Google+的,右边信息栏岂不是浪费? 我想,再以后,这块地方应该会被其他SNS APP占用吧!届时,通过手机通讯录,大家就可以看到某人最近在做什么了!而无需再登录单独的App去查看! 4.后记: 当然,除了Contact以外,Gallery等都和Google的产品有深度整合!而这些,和其他APP整合的可能性是有的!比如Gallery里直接查看Flickr里的相片而不只是Picasa.不过在Google推销自己的Google+这段时间里,我看是不太可能了!","link":"/android-4-contacts-the-depth-integration-with-google-plus/"},{"title":"Android G1 的优化","text":"我是第一批使用android的用户,那个时候没有别的选择,只有G1好选择.所幸买的是英版全白.这个机型据说是很少的.包装里带彩贴的那种! 可是时间长了,G1的诟病也就出现了,系统不断升级,虽然有很多自制包提供下载,解决了官方不在支持G1的问题,可是速度上和原来的设计问题没有办法解决! 不过好在网民的智慧是无穷的,今天带给大家三个特殊的小东西,用来优化你得G1. 10m rom HACK 刷新方法，拷贝boot-cm_2629-dp_mem-xtra.img文件到sd卡，启动到recovery模式下，进入console： mount -a flash_image boot /sdcard/boot-cm_2629-dp_mem-xtra.img JIT enabled Dalvik VM 进入recovery直接升级zip文件就好了 Audio Hack v3.2 apk 这个是mark里2欧元的付费软件.用来加大默认铃音. 下载地址: [download id=\"1\"]","link":"/android-g1-optimization/"},{"title":"Make Android Your Own-Androidify","text":"Androidify,由Google推出的一个app，可以让Android使用者在手机上创造属于自己的形象Android。是一套单纯的纸娃娃系统,可以自定义包括肤色、发色、衣服、裤子、鞋子、饰品等众多套件，还可变更身体的比例，其实还颇好玩的哩。只要直接在Android Market搜寻Androidify就能找到这个可爱的小软件。 跳转有Google针对这个app推出的小短片。不过还不如自己赶快下载回来玩比较实在。","link":"/android-ownandroidify/"},{"title":"","text":"在安卓中叫兽曾经写过两个换肤教程.而教程中写的是替换原文件包...其实这个完全没必要.只要在模拟器的快捷方式中加入一段代码就好了... -avd avdname -skin skinname 而皮肤文件可以分开来放.如图:","link":"/android-skin/"},{"title":"Android 简易访问Twitter,youtube,facebook方法","text":"其实就是修改hosts文件. 而这个hosts文件我是已经修改完毕的...直接cat到手机内覆盖原文件就可以了! 执行之前请将hosts复制到sdcard的根目录,然后cmd,cd如adb目录,然后执行其下代码: adb remount cat /sdcard/hosts &gt; /etc/hosts 一切搞定! 请对于hosts上的IP地址低调传播,谢谢! PS:本hosts修改大法已经基本完全失效,基本所有有效果的IP地址都被屏蔽,如果有条件,自己建立一个VPN吧!不过对于联通的用户我要给你一个大大的警告:联通屏蔽VPN.....","link":"/android-twitteryoutubefacebook/"},{"title":"Auto-save-photo-to-qqmail","text":"前言 为什么是QQ Mail? 因为它大,而且不断自动扩容,你想把它装满暂时是不太现实. 而且来说,QQ邮箱的体验还是非常不错的!过滤规则也很能满足要求,归档搜索查找都不错!一些不涉及隐私而又想保存的文件或者照片或者其他什么东东,存在QQ邮箱里还是不错的!比如:XXX 如果想同步到Google+请看完文章后看最后部分的更新说明! 如何实现 通过众所周知的ifttt 其实,这主要是一个我为了保存自己照片的方式!(爱信不信,不相信拉倒!) 建立一个task, if Instagram 设定条件:New Liked photo, then Gmail 设定条件:Send an email. 好了,填上 To address: xxx@qq.com 就OK了! 简单吧? 如果你熟悉某个联系人,那么建立规则by Username,然后包含此用户名关键词的主题都标上相应的关键词.比如by ladiiprang在邮箱规则里就可以加上\"妹子\"的tags. 注意点 如果你不想后期被众多的新邮件搞得头昏脑胀的话,那么你一开始就要设定好过滤条件! 在ifttt中,Send an email的时候Subject记得填写上一些关键词,比如From Instagram,这样,对于主题内有关键词的邮件就好管理的多了.添加过滤规则就好了!将来自己发送邮箱Gmail的邮件主题包含From和Instagram的都自动移动到一个新建的文件夹内,Ex:Photo DB,完工! 后记 同理,我们也可以建立来自Flickr的发送规则,原理是一样的!注意Gmail邮箱里的过滤条件要建好!否则Gmail爆满是迟早的事情!运用这样的规则,我们还可以发送Dropbox里的文档到QQ邮箱内保存,不过规则限定发送的只能是Public内的文档! 其实一开始我不确定是发送文件还是只有地址!所以我开始的方法很绕,就是将相片想Save到Dropbox,然后再通过Dropbox建立if.then.Gmail.不过试验下来既然能直接发送文件,建立task就简单多了! 还等什么,快去你的Instagram和Flickr上收藏妹子到邮箱内吧! 更新 本来因为标题的原因,这点是不加在这里的!但是想着再写一篇一样意义的文章很没意思,所以就在这里说明一下好了! 在我这篇文章发布之后,G+上看到了电脑玩物的作者+esor huang 的一篇讲解Instagram同步到Dropbox和Google+的说明!以及这篇E文!说起来,这样的同步方式确实很笨拙.你打算电脑24小时开着picasa来为你同步么?那么,我从我这篇文的基础上考虑可行方案!记得之前Picasaweb给每个人都有一个邮箱推送地址!就是类似username.password@picasaweb.com这样的地址!好吧,有邮箱地址就简单了.不过这个地址需要你再登录picasaweb.com去找,在Google+页面上是找不到的!同理,Flickr也有类似的推送地址! 该怎么做我想你已经清楚了吧! 最后,我想到了是否同样可以传送到QQ相册!毕竟和邮箱最大的不同就是相册是用来分享的,而邮箱是用来保存的!可惜,QQ没有针对相册的推送,而推送到QQ空间的XXX@qzone.qq.com这个地址也是必须QQ邮箱内部发才行!是的,和你们一样,我又想到了邮箱转发规则.用QQ邮箱收到邮件后转发到QQ空间邮箱去不就好了!测试后,果然. ... 果然没那么简单,这次我失误了,特么的QQ小气到不允许自己的QQ邮箱转发邮件到QQzone的邮箱来自动推送文章!提示这是一个无效地址!不过也无所谓了,毕竟是推送文章的邮件地址,你也不想自己的QQ空间全是大片的文章,而且每篇文章里只有一张相片吧 ? 这个说明本来是在G+上有提出的,但是有基友测试成功后给的是这里的url,所以我就想,补上这个说明!谁说是一样的到底,但是如果不提Picasaweb有邮件推送地址,估计很多人都已经忘记了!为了找同步到G+上的朋友会看得糊里糊涂!","link":"/auto-save-photo-to-qqmail/"},{"title":"Auto operation Weibo","text":"The code address of this article is: auto operation weibo Chromedrive download: Taobao Mirror , need to be consistent with your Chrome version auto operation weibo 123456789101112131415161718192021222324252627from selenium import webdriverimport timedriver = webdriver.Chrome('/Applications/chromedriver')# login weibodef weibo_login(username, password): # open weibo index driver.get('https://passport.weibo.cn/signin/login') driver.implicitly_wait(5) time.sleep(1) # fill the info: username, password driver.find_element_by_id('loginName').send_keys(username) driver.find_element_by_id('loginPassword').send_keys(password) time.sleep(1) # click login driver.find_element_by_id('loginAction').click() time.sleep(1)# set username, passwordusername = 'ivandoo75@gmail.com'password = 'ooxx'# Mobile phone verification is required here, but still can’t log in fully automaticallyweibo_login(username, password) follow user 12345678910111213141516171819202122def add_follow(uid): driver.get('https://m.weibo.com/u/' + str(uid)) time.sleep(1) # driver.find_element_by_id('follow').click() follow_button = driver.find_element_by_xpath('//div[@class=&quot;btn_bed W_fl&quot;]') follow_button.click() time.sleep(1) # select group group_button = driver.find_element_by_xpath('//div[@class=&quot;list_content W_f14&quot;]/ul[@class=&quot;list_ul&quot;]/li[@class=&quot;item&quot;][2]') group_button.click() time.sleep(1) # cancel the select cancel_button = driver.find_element_by_xpath('//div[@class=&quot;W_layer_btn S_bg1&quot;]/a[@class=&quot;W_btn_b btn_34px&quot;]') cancel_button.click() time.sleep(1)# 每天学点心理学UIDuid = '1890826225'add_follow(uid) create text and publish 1234567891011121314151617181920212223242526272829303132333435def add_comment(weibo_url, content): driver.get(weibo_url) driver.implicitly_wait(5) content_textarea = driver.find_element_by_css_selector('textarea.W.input').clear() content_textarea = driver.find_element_by_css_selector('textarea.W.input').send_keys(content) time.sleep(2) comment_button = driver.find_element_by_css_selector('.W_btn_a').click()# post the textdef post_weibo(content): # go to the user index driver.get('https://weibo.com') driver.implicitly_wait(5) # click publish button # post_button = driver.find_element_by_css_selector('[node-type=&quot;publish&quot;]').click() # input content word to textarea content_textarea = driver.find_element_by_css_selector('textarea.W_input[node-type=&quot;textEl&quot;]').send_keys(content) time.sleep(2) # click publish button post_button = driver.find_element_by_css_selector(&quot;[node-type='submit']&quot;).click() time.sleep(1)# comment the weiboweibo_url = 'https://weibo.com/1890826225/HjjqSahwl'content= 'here is Hivan du, Best wish to u.'# auto send weibocontent = 'Learning is a belief!'post_weibo(content)","link":"/auto_operation_weibo/"},{"title":"针对某一个项目自动切换node版本","text":"nvm作为node的版本管理器，并不具备自动切换版本切换的功能，有的时候我们需要针对某一个项目切换当前的node版本，这个时候就需要用到其他工具了。比如avn 举例项目:project 因为最近Node更新到10之后，我将系统默认版本切换到了10，有不更新不舒服斯基强迫症 而project 编译的版本为8，否则会出现编译出错。 123$ brew install nvm$ nvm i -g avn$ avn steup 之后在project根目录中添加一个文件.node-version 123$ touch .node-version$ echo v8 &gt;&gt; .node-version #node需要切换的版本$ echo `source &quot;$HOME/.avn/bin/avn.sh&quot; # load avn` &gt;&gt; ~/.zshrc 这样就可以了。 不过不排除报错的情况，如果是brew 安装的nvm, 则默认nvm.sh并不在~/.nvm目录内，这个时候可能需要在执行一下某段脚本。一样添加到~/.zshrc内 1$ echo `[[ -s &quot;$(brew --prefix nvm)/nvm.sh&quot; ]] &amp;&amp; source $(brew --prefix nvm)/nvm.sh` &gt;&gt; ~/.zshrc 再切换一下项目目录 12$ cd $project$ avn activated v8.11.2 (avn-nvm v8.11.2) 至此完成了！","link":"/avn-change-node-version-for-a-project/"},{"title":"Boston house analysis","text":"The source code: Boston House 1234567891011121314151617181920212223# Import package# Used to load the Boston housing price data setfrom sklearn.datasets import load_boston# pandas toolkit If you are unfamiliar with pandas, you can refer to the official 10-minute tutorial: https://pandas.pydata.org/pandas-docs/stable/10min.htmlimport pandas as pdimport numpy as np# seaborn for drawingimport seaborn as snsimport matplotlib.pyplot as plt# Show drawing%matplotlib inlinedata = load_boston() # load datasedata.keys() # Fields inside datadf = pd.DataFrame(data['data'])# Looking at the first 5 rows of the dataframe, we can see that the column names are numbersdf.head(5)data['feature_names'] # Feature name The Table params and chinese info 123456789101112131415params chinese infoCRIM 住房所在城镇的人均犯罪率ZN 住房用地超过 25000 平方尺的比例INDUS 住房所在城镇非零售商用土地的比例CHAS 有关查理斯河的虚拟变量（如果住房位于河边则为1,否则为0 ）NOX 一氧化氮浓度RM 每处住房的平均房间数AGE 建于 1940 年之前的业主自住房比例DIS 住房距离波士顿五大中心区域的加权距离RAD 离住房最近的公路入口编号TAX 每 10000 美元的全额财产税金额PTRATIO 住房所在城镇的师生比例B 1000(Bk-0.63)^2,其中 Bk 指代城镇中黑人的比例LSTAT 弱势群体人口所占比例MEDV 业主自住房的中位数房价（以千美元计） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# Replace numeric column names with feature namesdf.columns = data['feature_names']df.head(5)# The target is the house price, which is also our target value. We assign the target value to the dataframedf['price'] = data['target']df.head(5)# View the correlation coefficient between the feature and price, positive correlation and negative correlationsns.heatmap(df.corr(), annot=True, fmt='.1f')plt.scatter(df['RM'], df['price'])plt.figure(figsize=(20, 5))# View the data distribution display of some features and pricefeatures = ['LSTAT', 'RM']target = df['price']for i, col in enumerate(features): plt.subplot(1, len(features), i+1) x = df[col] y = target plt.scatter(x, y, marker = 'o') plt.title('{} price'.format(col)) plt.xlabel(col) plt.ylabel('price')# Simple example: univariate forecast pricex = df['RM']y = df['price']history_notes = {_x: _y for _x, _y in zip(x,y)}history_notes[6.575]# Find the top three prices that are closest to RM:6.57,similary_ys = [y for _, y in sorted(history_notes.items(), key=lambda x_y: (x_y[0] - 6.57) ** 2)[:3]]similary_ys# Calculate the average of threenp.mean(similary_ys) Use historical data to predict data that has never been seen before, the most direct method K-Neighbor-Nearst 12345678def knn(query_x, history, top_n = 3): sorted_notes = sorted(history.items(), key = lambda x_y: (x_y[0] - query_x)**2) similar_notes = sorted_notes[:top_n] similar_ys = [y for _, y in similar_notes] return np.mean(similar_ys)knn(5.4, history_notes) In order to obtain results faster, we hope to obtain predictive power by fitting a function \\[ f(rm) = k * rm + b \\] Random Approach \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} (\\hat{y_i} - y_i) ^ 2 \\] \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] 1234567891011121314151617181920212223def loss(y_hat, y): return np.mean((y_hat - y)**2)import randommin_loss = float('inf')best_k, best_b = None, Nonefor step in range(1000): min_v, max_v = -100, 100 k, b = random.randrange(min_v, max_v), random.randrange(min_v, max_v) y_hats = [k * rm_i + b for rm_i in x] current_loss = loss(y_hats, y) if current_loss &lt; min_loss: min_loss = current_loss best_k, best_b = k, b print(f'{step}, we have func f(rm) = {k} * rm + {b}, lss is :{current_loss}')plt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x]) Monte Carlo simulation(蒙特卡洛模拟) Supervisor \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{k}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i) * rm_i \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{b}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i)\\] 123456789101112131415161718192021222324252627def partial_k(k, b, x, y): return 2 * np.mean((k*x+b-y) * x)def partial_b(k, b, x, y): return 2 * np.mean(k*x+b-y)k, b = random.random(), random.random()min_loss = float('inf')best_k, best_b = None, Nonelearning_rate = 1e-2for step in range(2000): k, b = k + (-1 * partial_k(k, b, x, y) * learning_rate), b + (-1 * partial_b(k, b, x, y) * learning_rate) y_hats = k * x + b current_loss = loss(y_hats, y) if current_loss &lt; min_loss: min_loss = current_loss best_k, best_b = k, b print(f'setp {step}, we have func f(rm) = {k} * rm + {b}, lss is :{current_loss}')best_k, best_bplt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x]) Supervised Learning We turn the forecast of housing prices into a more responsible and sophisticated model. What should we do? \\[ f(x) = k * x + b \\] \\[ f(x) = k2 * \\sigma(k_1 * x + b_1) + b2 \\] \\[ \\sigma(x) = \\frac{1}{1 + e^(-x)} \\] 12345678910111213141516171819def sigmoid(x): return 1 / (1+np.exp(-x))sub_x = np.linspace(-10, 10)plt.plot(sub_x, sigmoid(sub_x))def random_linear(x): k, b = random.random(), random.random() return k * x + bdef complex_function(x): return (random_linear(x))for _ in range(10): index = random.randrange(0, len(sub_x)) sub_x_1, sub_x_2 = sub_x[:index], sub_x[index:] new_y = np.concatenate((complex_function(sub_x_1), complex_function(sub_x_2))) plt.plot(sub_x, new_y) We can implement more complex functions through simple, basic modules and repeated superposition For more and more complex functions? How does the computer seek guidance? What is machine learning? The shortcomings of this method of KNN, what is the background of the proposed linear fitting How to get faster function weight update through supervision method The combination of nonlinear and linear functions can fit very complex functions Deep learning we can fit more complex functions through basic function modules Assigment: \\[ L2-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{(\\hat{y} - y)}^2 \\] \\[ L1-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{|(\\hat{y} - y)|} \\] L2-Loss becomes L1Loss and achieves gradient descent Realize L1Loss gradient descent from 0 1. import package 12import numpy as npimport pandas as pd 2. load data 1234567891011from sklearn.datasets import load_bostondata = load_boston()data.keys()data_train = data.datadata_traget = data.targetdf = pd.DataFrame(data_train, columns = data.feature_names)df.head()df.describe() # Data description, you can view the statistics of each variable 3. Data preprocessing Normalization or standardization can prevent a certain dimension or a few dimensions from affecting the data too much when there are very many dimensions, and secondly, the program can run faster. There are many methods, such as standardization, min-max, z-score, p-norm, etc. How to use it depends on the characteristics of the data set. Further reading-数据标准化的迷思之深度学习领域 12345678910111213from sklearn.preprocessing import StandardScaler# z = (x-u) / s u is the mean, s is the standard deviationss = StandardScaler() data_train = ss.fit_transform(data_train)# For linear models, normalization or standardization is generally required, otherwise gradient explosion will occur, and tree models are generally not requireddata_train = pd.DataFrame(data_train, columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'])data_train.describe() # y=Σwixi+# Because the derivation of b is all 1, add a bias b to the data and set it to 1, as a feature of the data and update the gradient wi*b=widata_train['bias'] = 1data_train Divide the data set, where 20% of the data is used as the test set X_test, y_test, and the other 80% are used as the training set X_train, y_train, where random_state is the random seed 1234567from sklearn.model_selection import train_test_splittrain_x, test_x, train_y, test_y = train_test_split(data_train, data_traget, test_size = 0.2, random_state=42)print('train_x.shape, train_y.shape', train_x.shape, train_y.shape)print('test_x.shape, test_y.shape', test_x.shape, test_y.shape)train_x = np.array(train_x) Model training and gradient update 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def l1_cost(x, y, theta): &quot;&quot;&quot; x: 特征 y: 目标值 thta: 模型参数 &quot;&quot;&quot; k = x.shape[0] total_cost = 0 for i in range(k): total_cost += 1/k * np.abs(y[i] -theta.dot(x[i, :])) return total_costdef l2_cost(x, y, theta): k = x.shape[0] total_cost = 0 for i in range(k): total_cost += 1/k * (y[i] -theta.dot(x[i,:])) ** 2 return total_costnp.zeros(10).shapedef step_l1_gradient(x, y, learning_rate, theta): &quot;&quot;&quot; Function to calculate the gradient of the MAE loss function Return the gradient value 0 for the non-differentiable point at 0 X:特征向量 y：目标值 learing_rate:学习率 theta:参数 &quot;&quot;&quot; n = x.shape[0] # print(n) e = y - x @ theta gradients = - (x.T @ np.sign(e)) / n # sign is a sign function thata = theta - learning_rate * gradients return thetadef step_l2_gradient(x, y, learning_rate, theta): k = x.shape[0] n = x.shape[1] gradients = np.zeros(n) for i in range(k): for j in range(n): gradients[j] += (-2/k) * (y[i] - (theta.dot(x[i, :]))) * x[i, j] theta = theta - learning_rate * gradient return theta# def step_gradient(X, y, learning_rate, theta):# &quot;&quot;&quot;# X:特征向量# y：目标值# learing_rate:学习率# theta:参数# &quot;&quot;&quot;# m_deriv = 0# N = len(X)# for i in range(N):# # 计算偏导# # -x(y - (mx + b)) / |mx + b|# m_deriv += - X[i] * (y[i] - (theta*X[i] + b)) / abs(y[i] - (theta*X[i] + b))# # We subtract because the derivatives point in direction of steepest ascent# theta -= (m_deriv / float(N)) * learning_rate# # theta = theta - learning_rate * gradients# return thetadef gradient_descent(train_x, train_y, learning_rate, iterations): k = train_x.shape[0] n = train_x.shape[1] theta = np.zeros(n) # Initialization parameters loss_values = [] # print(theta.shape) for i in range(iterations): theta = step_l1_gradient(train_x, train_y, learning_rate, theta) loss = l1_cost(train_x, train_y, theta) loss_values.append(loss) print(i, 'cost:', loss) return theta, loss_values# Training parameterslearning_rate = 0.04 # Learning rateiterations = 300 # Number of iterationstheta, loss_values = gradient_descent(train_x, train_y, learning_rate, iterations)","link":"/boston_analysis/"},{"title":"Boston house price CART regression tree","text":"On the code 12345678910111213141516171819202122232425262728293031323334353637383940# CART regression tree predictionfrom sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_bostonfrom sklearn.metrics import r2_score,mean_absolute_error, mean_squared_errorfrom sklearn.tree import DecisionTreeRegressor,export_graphvizimport graphviz# Prepare data setboston = load_boston()# Explore dataprint(boston.feature_names)# Get feature set and pricefeatures = boston.dataprices = boston.target# Randomly extract 33% of the data as the test set, and the rest as the training settrain_features, test_features, train_price, test_price = train_test_split(features,prices,test_size=0.33)# Create CART regression treedtr = DecisionTreeRegressor()# Fitting and constructing CART regression treedtr.fit(train_features, train_price)# Predict housing prices in the test setpredict_price = dtr.predict(test_features)grap_data = export_graphviz(dtr, out_file=None)graph = graphviz.Source(grap_data)# Result evaluation of test setprint(f'Regression tree mean squared deviation:',mean_squared_error(test_price, predict_price))print(f'Regression tree absolute value deviation mean:',mean_absolute_error(test_price, predict_price))# Generate regression tree visualizationgraph.render('Boston') !&gt; Before running this code, please ensure that the relevant dependencies have been installed;","link":"/boston_data_CART/"},{"title":"bye-google-reader","text":"今早一打开网页,满篇都是关于Google将于7月1日正式关闭Reader的消息. 再见吧,Reader. 这八年,基本每天,我都会去看看你... 可是又能如何,到了该走的时候了.即便这个理由让我完全无法接受. Bye, 我亲爱的GReader...","link":"/bye-google-reader/"},{"title":"CM6 test0 32b","text":"好吧,熟悉的人看到标题应该能猜到这就是Android 2.2 的CM版本.没想到这么快会出现,和之前发布的版本不同,这次编译与CM大神的版本!没想到在和儿子过生的当间就发布了... 基于7月5日最新CM6源码编译，新增了电池百分比显示的开关，在Cyanogenmod 设置里进行更改 默认关闭jit，在32a/32b等低端机上开启jit对性能没有改善，反倒更占内存，故在这个版本中关闭jit 修改ADW的默认设置，使其常驻内存，改善从其它程序退回桌面的速度 修改ADW的壁纸图库，用AOSP的图库替换了CM的图库 修改了framework.jar，使用了geesun的代码试其支持中文运营商显示 使用了最新的FRF91的GAPPS CM6自带的contact文件有不少bug，故换成了aosp的contact，虽然相对cm6的功能更少，但非常稳定 重新编译了kernel，个人感觉比默认的kernel更稳定 进一步汉化了framework和superuser等程序 新增32a的支持，32a的用户也可以使用 Known Issues： 第一次启动时可能会意外重启，完成设置后就不会出现这个问题 相机中按0x变焦按键会使相机fc 摄像无法正常使用 由于Gapps都是Nexus专用的，故其素材的尺寸都很大，特别是gmail，显示出来很大，这个暂时无法解决 Dream和Magic因为性能问题，不支持flash，即便刷了2.2也不可能运行flash，所以不要去市场下载flash程序了，不会起作用的，另外也不要再求使用flash的方法了，在地球上不存在解决方法，除非你换手机 App2sd： rom支持app2sd，但默认没有开启。 开启方法： 在超级终端中输入： su pm setInstallLocation 2 即可开启app2sd，不需要有ext分区就可以使用，但官方的这个app2sd还不太稳定，不建议使用。 Download： Dream/Magic 32b:http://thesoloblack.com/rom/cm6-test0-32b-0705-fixed.zip Magic 32a:http://thesoloblack.com/rom/cm6-test0-32a-0705-fixed.zip 本更新信息和下载链接均来源于机锋网!","link":"/cm6-test0-32b/"},{"title":"CnBloggerCon 2012","text":"一年一度的中文网志年会与今晨八点开始，不过没想到的是昨晚就已经完成了开幕式。在Google+上由Isaac Mao主持开幕Hangout视频。可惜我没有看到！ 以上的年会开放营的设计示意图本来我是想稍加修改一下再传到自己的Flickr上的！结果一直到今天开幕都没有太多时间来做这件事情，其实这篇博客都应该是昨天晚上就生产出来的才对。 今年的年会与以前的年会有所不同，没有设置会场，而启用了“云智慧”的概念。而Google+中的会议视频以及才推出没多久的Events为年会此种形式提供了可行性！ 第一天的会议主题为“公民媒体”，可以在如下两个地址内参与讨论： +《云平行会： 麦康瑞在全球之声2012峰会》 +《云访谈：老虎庙和佐拉谈公民记者》 Google+上的Events讨论地址：中文网志年会2012 Events 还请各位中文Blogger们积极参加，因为要翻墙，So,记得戴套，或自备安全工具！你们懂得。","link":"/cnbloggercon-20/"},{"title":"关于设备转向后的自适应","text":"关于移动端的适配，都知道其实rem是比较好的一个适配方案，但是rem是根据根目录的字体大小来调解的，那么，我们在做网页的时候，屏幕旋转后，能否让根目录的字体跟着变化呢？ 先上代码： 1234$(function(){ var size = $(window).width() / 25; $('html').css('font-size': size);}); 这样在css中用rem单位是没什么问题，但是如果屏幕旋转之后，你就会发现，真的不能看了就。原因就是屏幕旋转以后，根上的字体并没有随之变化。 所以我们来加上 12345678910111213// 监视设备方向window.addEventListener(&quot;orientationchange&quot;, function() { media();}, false);function media(argument) { // 因为获取尺寸出错，需要延迟获取 setTimeout(function(){ var size = $(window).width() / 25; console.log('the device size: '+size); $('html').css('font-size', size); }, 200); }","link":"/css-rem-and-javascript/"},{"title":"自定义文件上传框","text":"其实这根本就不值得写出来，只是可能前几步大家都做了，只是最后一步就忽略了。 我们在自定义input:file的时候，一般来说都是外边包一层，里边在写一个&lt;input type=\"file\"&gt;, 然后将其透明值设置成0,然后再定义外层的样式来达到自定义的目的。 HTML： &lt;div class=&quot;upfileOutWrap&quot;&gt; &lt;div class=&quot;upfileWrap&quot;&gt;&lt;input type=&quot;file&quot;&gt;&lt;/div&gt; &lt;div class=&quot;upfileBG&quot;&gt;upload image&lt;/div&gt; &lt;/div&gt; CSS： .upfileOutWrap { cursor: pointer; width: 199px; height: 42px; line-height: 42px; position: relative; } .upfileWrap{ width: 100%; height: 100%; position: absolute; top:-1; left: -1; z-index:2; } .upfileWrap input{ opacity: 0; filter: alpha(opacity=0); cursor: pointer; width: 100%; height: 100%; font-size: 32px; } .upfileBG{ width:100%; height:100%; background: url(./images/upload.png) no-repeat; font-size: 14px; color: white; position: absolute; top:-1; left: -1; padding-left:10px; z-index:1; } 可是这个时候还是有点问题，就是万恶的IE下边。 IE下边的input标签默认都是有光标的，:file也不例外，而且IE下边必须要点击”Browse”或者双击input输入框才会有效果。那么这个时候在IE下就会出现如图的莫名其妙的问题，注意左边的光标，并且还需要双击才会弹出文件选择窗口。 这个时候如果你把input透明度设置成100显示出来，就会发现原来是这样的。 所以这个时候，如果是其他标准浏览器，那么设置好input的高宽就搞定了，而IE下边，还必须考虑如何让”Browse”按钮能铺满我们所自定的div样式。这样我们才能实现IE下不出现光标，而且单击弹出文件选择窗口。 这个时候，看似毫无办法，其实我们可以选择增加字体的大小。当字体变成32px的时候，就是这个样子的。 好了，这样我们就搞定了，将input:file 继续设置为完全透明。那个可恶的光标不见了，我们也可以实现IE下单击。当然，字体到底用多大的，要视你自己定义的视觉效果来看，自己调试吧。 Final CSS: .upfileOutWrap { cursor: pointer; width: 199px; height: 42px; line-height: 42px; position: relative; } .upfileWrap{ width: 100%; height: 100%; position: absolute; top:-1; left: -1; z-index:2; } .upfileWrap input{ opacity: 0; filter: alpha(opacity=0); cursor: pointer; width: 100%; height: 100%; } .upfileBG{ width:100%; height:100%; background: url(./images/upload.png) no-repeat; font-size: 14px; color: white; position: absolute; top:-1; left: -1; padding-left:10px; z-index:1; }","link":"/custom-inputfile/"},{"title":"Debian安装SSH","text":"要求： 安装Debian 8.* 64位操作系统 分区要求: 不要分区 Partitioning mehod: use entire disk Partitioning scheme: All files in one partition 选择源镜像 (mirror country) 请选择china 然后选择ftp.cn.debian.org 程序和服务需求 debian 默认最小安装，安装的时候不用安装桌面环境和标准系统实用程序(以下两个不需要勾选): - Debian destop environment - Standard system utilities 如果有SSH server选项，请务必勾选，会省很多麻烦 安装SSH debian最小安装默认是没有配置apt-get源的，这个时候无法实用apt-get install命令，所以在安装SSH之前，我们需要先配置apt-get: 配置apt-get 终端内操作 1234567# 首先我们需要备份原有配置文件cp /etc/apt/sources.list /etc/apt/sources.listbak# 然后对资源列表文件进行编辑vi /etc/apt/sources.list# 当然也可以实用nano命令nano /etc/apt/source.list PS: 如果对VI操作不熟悉的，可以看这里 vi编辑器常见命令实用 如果安装的时候按照之前我给的步骤来，那么这会的sources.list应该是这样的 对文件进行更改，将以下命令加入文件并保存: 12deb http://ftp.cn.debian.org/debian/ jessie main contrib non-free deb-src http://ftp.cn.debian.org/debian/ jessie main contrib non-free 更改后的文件如图: main, contrib, non-free 分属不同的源，添加后可以从不同的源仓库更新文件索引 至此apt-get源就配置完毕，接下来我们就可以安装SSH了 安装SSH 在终端内输入以下命令: 12345678# 更新apt-get源apt-get update# 更新系统apt-get upgrade# 安装SSHapt-get install ssh 这样就好了，SSH安装完毕 注意 如果一直安装不能成功，请往下看: 首先，请ping ftp.cn.debin.org 和 ping mirrors.163.com 来测试一下能否ping的通域名，如果ping不通，请往下看: 有的时候机房安装debian后会出现域名解析问题,这又是另外一个问题。比如ping 123.111.123.111 是OK的，但是如果ping对应的域名如: ping mirrors.163.com就会出现unknow host的问题。 似乎linux很大一部分都会出现这种问题，能ping的通IP但是ping不通域名。那么请查看以下原因解决: 1. 查看DNS解析是否有问题，确定设置了域名服务器: cat /etc/resolv.conf 123nameserver 114.114.114.114 nameserver 8.8.8.8 nameserver 8.8.4.4 2. 确保网关已设置 grep GATEWAY /etc/sysconfig/network-scripts/ifcfg* 1/etc/sysconfig/network-scripts/ifcfg-eth0:GATEWAY=192.168.40.1 如果未设置，则通过以下方法增加网关 route add default gw 192.168.40.1 或者手工编写/etc/sysconfig/network-scripts/ifcfg* 然后重启network服务: service network restart 3. 确保可用dns解析 grep hosts /etc/nsswitch.conf 文件打开后为: 1hosts: files dns 4. 查看是否防火墙的问题 因为域名解析用到了53端口,需要把下面设置配置到防火墙里: 1234iptables -A INPUT -p udp --sport 53 -j ACCEPT iptables -A OUTPUT -p udp --dport 53 -j ACCEPT iptables -A INPUT -p udp --dport 53 -j ACCEPT iptables -A OUTPUT -p udp --sport 53 -j ACCEPT 如果找不到原因或者不知道怎么设置，那么就用以下最笨的方法: 如果出现这样的问题，更新sources.list后会无法更新也无法安装ssh. 如果出现这样的问题，更新sources.list地址为一下地址: 12345678deb http://123.58.173.186/debian/ jessie main non-free contribdeb http://123.58.173.186/debian/ jessie-updates main non-free contribdeb http://123.58.173.186/debian/ jessie-backports main non-free contribdeb-src http://123.58.173.186/debian/ jessie main non-free contribdeb-src http://123.58.173.186/debian/ jessie-updates main non-free contribdeb-src http://123.58.173.186/debian/ jessie-backports main non-free contribdeb http://123.58.173.186/debian-security/ jessie/updates main non-free contribdeb-src http://123.58.173.186/debian-security/ jessie/updates main non-free contrib 利用IP地址代替域名，但是测试下来只有163的镜像可以这样做。来源为网易镜像的帮助文档: Debian镜像使用帮助","link":"/debian-install-ssh/"},{"title":"Digits recognition","text":"The code address of this article is: digit recognition Convolution operation demo 123456789101112131415import pylabimport numpy as npfrom scipy import signal# set imgimg = np.array([[10, 10, 10, 10, 10],[10, 5, 5, 5, 10], [10, 5, 5, 5, 10], [10, 5, 5, 5, 10], [10, 10, 10, 10, 10]])# set convolutionfil = np.array([[-1, -1, 0], [-1, 0, 1], [0, 1, 1]])# convolution the imgres = signal.convolve2d(img, fil, mode='valid')# output the resultprint(res) output 123[[ 15 10 0] [ 10 0 -10] [ 0 -10 -15]] A image demo 1234567891011121314151617181920212223import matplotlib.pyplot as pltimport pylabimport cv2import numpy as npfrom scipy import signal# read the imageimg = cv2.imread('./data/weixin.jpg', 0) # Any picture# show the imageplt.imshow(img, cmap='gray')pylab.show()# set the convolutionfil = np.array([[-1,-1,0], [-1, 0, 1], [0, 1, 1]])# convolution operationres = signal.convolve2d(img, fil, mode='valid')print(res)# show convolution imageplt.imshow(res, cmap = 'gray')pylab.show() use LeNet model to recognize Mnist handwritten digits 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import kerasfrom keras.datasets import mnistfrom keras.layers import Conv2D, MaxPooling2Dfrom keras.layers import Dense, Flattenfrom keras.models import Sequentialimport warningswarnings.filterwarnings('ignore')# load data(train_x, train_y), (test_x, test_y) = mnist.load_data()train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)train_x = train_x / 255test_x = test_x / 255train_y = keras.utils.to_categorical(train_y, 10)test_y = keras.utils.to_categorical(test_y, 10)# create sequential modelsmodel = Sequential()# The first convolutional layer: 6 convolution kernels, the size is 5*5, relu activation functionmodel.add(Conv2D(6, kernel_size = (5,5), activation='relu', input_shape=(28, 28, 1)))# the second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size = (2, 2)))# the third convolutional layer: 16 convolution kernels, the size is 5*5, relu activation functionmodel.add(Conv2D(16, kernel_size = (5, 5), activation = 'relu'))# the second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size = (2, 2)))# Flatten the parameters, which is called a convolutional layer in leNet5. in fact, this layer is a one-dimensional vector, the same as the fully connected layermodel.add(Flatten())model.add(Dense(120, activation = 'relu'))# Fully connected layer, the number of output nodes is 84model.add(Dense(84, activation = 'relu'))# The output layer uses the softmax activation function to calculate the classification probabilitymodel.add(Dense(10, activation='softmax'))# set the loss function and optimizer configurationmodel.compile(loss = keras.metrics.categorical_crossentropy, optimizer = keras.optimizers.Adam(), metrics = ['accuracy'])# Incoming training data for trainingmodel.fit(train_x, train_y, batch_size = 128, epochs = 2, verbose = 1, validation_data = (test_x, test_y))# Evaluate the resultsscore = model.evaluate(test_x, test_y)print('Error: %.4lf' % score[0])print('Accuracy: ', score[1]) 12345678Train on 60000 samples, validate on 10000 samplesEpoch 1/260000/60000 [==============================] - 37s 616us/step - loss: 0.3091 - accuracy: 0.9102 - val_loss: 0.1010 - val_accuracy: 0.9696Epoch 2/260000/60000 [==============================] - 36s 595us/step - loss: 0.0876 - accuracy: 0.9731 - val_loss: 0.0572 - val_accuracy: 0.981410000/10000 [==============================] - 3s 328us/stepError: 0.0572Accuracy: 0.9814000129699707","link":"/digits_recognition/"},{"title":"使用Plotnine制作元素周期表","text":"首先需要了解元素周期表以及元素数据: 维基百科的元素周期表词条 元素数据 元素周期表基本构成如下: 族：表中的每一列就是一族，从左向右依次为 1、2……18 族。 周期：表中的行。 元素：每个方框表示一个元素，其中包括元素符号、名称、原子序数、原子量。 在主表下面还有镧系元素和锕系元素表。 用颜色区分金属、非金属等常见的物质状态。 最终呈现: 其他形状元素周期表 导入和处理数据 1234567# 导入依赖import pandas as pdimport numpy as npfrom plotnine import *# 读取数据elements = pd.read_csv('~/data/cbcpv/elemanets/elements.csv') 研究数据集 1234567891011121314151617181920212223242526272829303132elements.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 118 entries, 0 to 117Data columns (total 21 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 atomic number 118 non-null int64 1 symbol 118 non-null object 2 name 118 non-null object 3 atomic mass 118 non-null object 4 CPK 118 non-null object 5 electronic configuration 118 non-null object 6 electronegativity 97 non-null float64 7 atomic radius 71 non-null float64 8 ion radius 92 non-null object 9 van der Waals radius 38 non-null float64 10 IE-1 102 non-null float64 11 EA 85 non-null float64 12 standard state 99 non-null object 13 bonding type 98 non-null object 14 melting point 101 non-null float64 15 boiling point 94 non-null float64 16 density 96 non-null float64 17 metal 118 non-null object 18 year discovered 118 non-null object 19 group 118 non-null object 20 period 118 non-null int64 dtypes: float64(8), int64(2), object(11)memory usage: 19.5+ KB&quot;&quot;&quot; 特征group就是该元素所在的族，但是，如果用elements['group'] 查看所有内容，会发现有的记录中用 '-' 标记，说明它不属于任何族，说明它们应该是镧系元素或者锕系元素。根据数据分析的通常要求，'-' 符号最好用数字表示，这里用 ﹣1 转化数据集 123456789101112131415161718# 转换族elements['group'] = [-1 if g=='-' else int(g) for g in elements['group']]elements['group']&quot;&quot;&quot;0 11 182 13 24 13 ..113 14114 15115 16116 17117 18Name: group, Length: 118, dtype: int64&quot;&quot;&quot; 特征 bonding type、metal 都是分类数据，因此在类型上进行转化。 123# 转化分类数据elements['bonding type'] = elements['bonding type'].astype('category')elements['metal'] = elements['metal'].astype('category') 将原本的整数型atomic number特征,转化为字符串类型 1elements['atomic_number'] = elements['atomic number'].astype(str) 元素周期表有两个部分,上面一部分每个元素是属于某一个族的,即group特征中的1-18, 而对于值是-1的则表示这些元素应该在下面的镧系或者锕系元素表中。下面分别用 top 变量和 bottom 变量引用这两部分元素集合. 123## 分别用top和bottom变量引用上下部分元素集合top = elements.query('group != -1').copy()bottom = elements.query('group == -1').copy() 元素周期表中横向表示的是族（group），纵向表示的是周期（period），用下面的方式在 top 中创建两个特征，分别为“族”和“周期”的值。 1234567891011121314151617181920212223242526272829303132333435363738## 在top中区分“族”(group)和“周期”(period)的值&quot;&quot;&quot;横向表示族,纵向表示周期&quot;&quot;&quot;top['x'] = top.grouptop['y'] = top.periodtop['x']&quot;&quot;&quot;0 11 182 13 24 13 ..113 14114 15115 16116 17117 18Name: x, Length: 90, dtype: int64&quot;&quot;&quot;top['y']&quot;&quot;&quot;0 11 12 23 24 2 ..113 7114 7115 7116 7117 7Name: y, Length: 90, dtype: int64&quot;&quot;&quot; 除了上面的部分之外，下面的锕系和镧系元素也要做类似的配置。不过，横坐标不能用 group 特征的值，因为前面设置为 ﹣1。 12345678nrows = 2&quot;&quot;&quot;hshift 和 vshift 分别表示横、纵间距，这样就为每个锕系和镧系元素增加了横纵坐标值。&quot;&quot;&quot;hshift = 3.5vshift = 3bottom['x'] = np.tile(np.arange(len(bottom) // nrows), nrows) + hshiftbottom['y'] = bottom.period + vshift 每个元素都占了一个小方块,所以,这个小方块(元素块)的大小要设置一下 123## 设置元素占据的小矩形tile_width = 0.95tile_height = 0.95 开始画图 1234(ggplot(aes('x', 'y')) + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height))) 这里只有美学映射,没有传入数据集.因为在图层对象中,要传入不同的数据集: “top”和“bottom”. top表示主表中的, bottom表示下面的锕、镧系元素 geom_tile绘制安放元素块图层,并使用top数据集,在引入一个图层,绘制bottom对应的图层. 但是我们发现表反了, 所以需要实现在Y轴方向上的坐标轴翻转. 123456(ggplot(aes('x', 'y')) +geom_tile(top, aes(width=tile_width, height=tile_height)) +geom_tile(bottom, aes(width=tile_width, height=tile_height)) # 在Y轴上进行翻转 +scale_y_reverse() # new) 基本样式已经有了。 前面已经把特征“metal”的数据转换为分类数据，下面用这些数据对不同元素的小矩形（以后简称“元素块”）上色。 1234567(ggplot(aes('x', 'y')) # 对数据不同的元素块进行上色 + aes(fill='metal') # new + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + scale_y_reverse() ) 然后,我们要将化学元素的有关信息写到这些元素块上,这里要写到元素块上的包括: 原子序数，对应着数据集中的特征是“atomic number”； 元素符号，对应着数据集中的特征是“symbol”； 元素名称，对应着数据集中的特征是“name”； 原子量，对应着数据集中的特征是“automic mass” 在这里,我们要绘制四个图层,以便安放四个元素信息, 每个图层上面一个特征,并且每个图层的位置、字号大小等都不相同. 为此我们写一个函数方法来实现: 1234567891011121314151617181920&quot;&quot;&quot;nudge_x: 文本在水平方向上的相对位置nudge_y: 文本在竖直方向上的相对位置ha: 可选'left', 'center', 'right', 标示水平方向的对齐方式va: 可选'top', 'center', 'bottom', 表示竖直方向的堆砌方式size: 字号大小fontweight: 字族中的字体粗细&quot;&quot;&quot;def inner_text(data): layers = [geom_text(data, aes(label='atomic_number'), nudge_x=-0.40, nudge_y=-.40, ha='left', va='top', fontweight='normal', size=6), geom_text(data, aes(label='symbol'), nudge_y=.1, size=9), geom_text(data, aes(label='name'), nudge_y=-0.125, fontweight='normal', size=4.5), geom_text(data, aes(label='atomic mass'), nudge_y=-.3, fontweight='normal', size=4.5) ] return layers 然后我们将函数inner_text应用到绘图流程中去 12345678910111213&quot;&quot;&quot;分别调用两次是因为有top和bottom两个数据&quot;&quot;&quot;(ggplot(aes('x', 'y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) # 绘制上部分图层 + inner_text(top) # new # 绘制下部分图层 + inner_text(bottom) # new + scale_y_reverse()) 是不是觉得图很难看,原因在于我们还没对其进行调整,下面我们就要细微的调整图层,包括大小等 12345678910111213(ggplot(aes('x', 'y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) + scale_y_reverse() # coord_equal作用是设置坐标系的横轴和纵轴 # expand=False, 意味着坐标系的大小由制图所用数据决定 + coord_equal(expand=False) # new # 一个新主题,规定了图纸的尺寸 + theme(figure_size=(12,6)) # new) 在默认的主题中，横纵坐标的图上长度相等，也就是图像是呈现在一张正方形的图纸上，coord_equal 的作用就是设置坐标系的横轴和纵轴，它与 coord_fixed 是完全等效的，能够改变图纸的大小和长宽比例。参数 expand 的值是布尔值，如果为 False，则意味着坐标系的大小（即图纸的大小）由制图所用数据决定。 新增的第二个图层对象是一个新的主题，在其中规定了图纸的尺寸。 我们仔细研究元素周期表,发现Lu和Lr两个元素比较特殊,其实它们不是单独的元素,而是对应着下部分两行的,因此要对这两个进行处理,以区分出与其他元素的不同. 我们将其分为两半,使用过PS作图的同学应该能想到两个不同颜色的图层叠加,上面的图层只有下面图层的一半,那么看起来就像是被分成了两半. 123456# split_df 是绘制新元素块所需要的数据集。split_df = pd.DataFrame({ 'x': 3-tile_width/4, 'y': [6,7], 'metal': pd.Categorical(['lanthanoid', 'actinoid'])}) 123456789101112(ggplot(aes('x','y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) # 将新的数据集用于叠加Lu和Lr的图层上进行遮挡 + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) # new + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) + scale_y_reverse() + coord_equal(expand=False) + theme(figure_size=(12, 6))) 基本制作完成了,下面来美化一下: 123456789101112131415161718(ggplot(aes('x', 'y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) + scale_y_reverse() # 对元素块填充色进行转换 + scale_fill_brewer(type='qual', palette=3) + coord_equal(expand=False) # 增加了一个经典的主题图层对象 + theme_void() + theme(figure_size=(12, 6), # 增加一个主题图层,并设置了该图层的尺寸和背景色 plot_background=element_rect(fill='white') )) 到最后了,我们要解决主表中的元素表上族和周期的问题 观察主表中的每一列,注意我们已经把Y轴映射反序了,如果在H元素的元素块上标注族的序号为“1”, 那么这个“1”的Y轴坐标应该是y=1, 同样,Sc元素块上标注族的需要“3”, 那么“3”的Y轴坐标应该是y=4. 这样,我们就可以创建每列及其对应的Y轴坐标了. 1234567## 创建每列(即:族, 编号为1-18)及其对应的Y轴坐标groupdf = pd.DataFrame({ 'group': range(1, 19), 'y': np.repeat([1,2,4,2,1], [1,1,10,5,1])})groupdf group y 0 1 1 1 2 2 2 3 4 3 4 4 4 5 4 5 6 4 6 7 4 7 8 4 8 9 4 9 10 4 10 11 4 11 12 4 12 13 2 13 14 2 14 15 2 15 16 2 16 17 2 17 18 1 让我们来标注族的序号 123456789101112131415161718192021222324252627## 标注族序号(ggplot(aes('x','y')) + aes(fill='metal') + geom_tile(top, aes(width=tile_width, height=tile_height)) + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) + geom_tile(bottom, aes(width=tile_width, height=tile_height)) + inner_text(top) + inner_text(bottom) # 标注每一列族序号的文本图层 # aes('group', 'y', label='group') 重写了X轴和Y轴的映射 # inherit_aes=False, 不继承映射配置 + geom_text(groupdf, aes('group', 'y', label='group'), color='gray', nudge_y=.525, va='bottom', fontweight='normal', size=9, inherit_aes=False ) # 以Y轴调转坐标轴 + scale_y_reverse() # 对元素块填充色进行转换 + scale_fill_brewer(type='qual', palette=3) + coord_equal(expand=False) # 增加了一个经典的主题图层对象 + theme_void() + theme(figure_size=(12, 6), # 增加一个主题图层,并设置了该图层的尺寸和背景色 plot_background=element_rect(fill='white'), )) 最终,我们标注玩周期就完成了. 周期是对每一行的标注,一共7行,因为标注在左侧,可以把它看成是左侧的Y轴标示,可以在图层上通过对Y轴标示的设置完成周期的标注. 1234567891011121314151617181920212223242526272829303132333435363738## 开始标注周期, 最终完成(ggplot(aes('x', 'y')) # 把特征'metal'的数据转换为分类数据,进行元素块上色 + aes(fill='metal') # 创建上部元素块 + geom_tile(top, aes(width=tile_width, height=tile_height)) # 创建Lu和Lr的半个元素块 + geom_tile(split_df, aes(width=tile_width/2, height=tile_height)) # 创建下部元素块 + geom_tile(bottom, aes(width=tile_width, height=tile_height)) # 创建文字图层, 把化学元素的有关信息写到元素块中 + inner_text(top) + inner_text(bottom) # 标注每一列族序号的文本图层 # aes('group', 'y', label='group') 重写了X轴和Y轴的映射 # inherit_aes=False, 不继承映射配置 + geom_text(groupdf, aes('group', 'y', label='group'), color='gray', nudge_y=.525, va='bottom', fontweight='normal', size=9, inherit_aes=False ) # 以Y轴调转坐标轴, 增加了纵坐标主刻度标示数字。 + scale_y_reverse(breaks=range(1, 8), limits=(0, 10.5) ) # 对元素块填充色进行转换 + scale_fill_brewer(type='qual', palette=3) + coord_equal(expand=False) # 增加了一个经典的主题图层对象 + theme_void() + theme(figure_size=(12, 6), # 增加一个主题图层,并设置了该图层的尺寸和背景色 plot_background=element_rect(fill='white'), # 增加了参数 axis_text_y，对 Y 轴标示的显示格式进行了设置。 axis_text_y=element_text(margin={'r':5}, color='gray', size=9) )) 完成...","link":"/elements_by_plotnine/"},{"title":"enlight VS snapseed2","text":"本文知乎专栏地址 在Enlight出来之前，Snapseed曾经是我最主要的修图工具。 当然，Enlight确实是大而全的一款App，但是使用中，却让我有了无法忍受的一点。那就是丢失部分图片信息。 导出的时候，Enlight导出所用的格式是PNG，而Snapseed仍然是jpg导出。自然大家都知道PNG图片所用的格式是无损的，这就是说，Snapseed导出的图像相比Enlight要小很多，因为iPhone默认拍照所保存的都是jpg格式，所以即便再导出PNG格式，那所增加的部分应该是软件通过算法添加进去的。 而，Enlight在导出的PNG格式图片里，却丢失了相机信息，地理位置信息和更重要的拍照时间信息。照片的时间被导出的时间所代替。 同样一张照片，我们先使用Enlight进行修图 然后使用Snapseed 2 修图 以下是使用Metapho查看的照片信息。 1. 首先我们来看Enlight导出的照片信息: 可以看到Camera和Location信息都没有了，照片被导出成PNG格式，而且照片大了很多。 2. 我们再来看看Snapseed导出的照片信息 可以看到Camera和Location信息都保存完好。 这也是Enlight最让我无法接受的一点，其实Enlight在设置里是可以选择增加地理信息的，也就是会调用GPS模块，但是当导出的时候这些信息就全都没有了，即便是用Enlight来拍照。不知道下一版本的Enlight是否能够改善这点。如果对这些信息无所谓的同学，Enlight倒是现今为止最强大的修图工具，Snapseed虽然更新到版本2了，但是似乎依然没有加入曲线。Snapseed2的具体更新，可以点击原文链接查看「领客」内的文。 ———","link":"/enlight-VS-snapseed2/"},{"title":"A Preliminary Study of Machine Learning","text":"Gradient 123456789101112131415def loss(k): return 3 * (k ** 2) + 7 * k -10# -b / 2a = -7 / 6def partial(k): return 6 * k + 7k = ramdom.randint(-10, 10)alpha = 1e-3 # 0.001for i in range(1000): k = k + (-1) * partial(k) * alpha print(k, loss(k))","link":"/example_002/"},{"title":"Introduction to Artificial Intelligence","text":"The code address of this article is: Example 01 The source code is in ipynb format, and the output content can be viewed. rule based 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import random from icecream import ic#rules = &quot;&quot;&quot;#复合句子 = 句子 , 连词 句子#连词 = 而且 | 但是 | 不过#句子 = 主语 谓语 宾语#主语 = 你| 我 | 他 #谓语 = 吃| 玩 #宾语 = 桃子| 皮球# #&quot;&quot;&quot;rules = &quot;&quot;&quot;复合句子 = 句子 , 连词 复合句子 | 句子连词 = 而且 | 但是 | 不过句子 = 主语 谓语 宾语主语 = 你| 我 | 他 谓语 = 吃| 玩 宾语 = 桃子| 皮球 &quot;&quot;&quot;def get_grammer_by_description(description): rules_pattern = [r.split('=') for r in description.split('\\n') if r.strip()] target_with_expend = [(t, ex.split('|')) for t, ex in rules_pattern] grammer = {t.strip(): [e.strip() for e in ex] for t, ex in target_with_expend} return grammer#generated = [t for t in random.choice(grammer['句子']).split()]#test_v = [t for t in random.choice(grammer['谓语']).split()]def generate_by_grammer(grammer, target='句子'): if target not in grammer: return target return ''.join([generate_by_grammer(grammer, t) for t in random.choice(grammer[target]).split()])if __name__ == '__main__': grammer = get_grammer_by_description(rules) #ic(generated) #ic(test_v) #ic(generate_by_grammer(grammer)) ic(generate_by_grammer(grammer, target='复合句子')) water pouring 12345678910111213141516171819202122232425262728293031323334353637383940def water_pouring(b1, b2, goal, start=(0, 0)): if goal in start: return [start] explored = set() froniter = [[('init', start)]] while froniter: path = froniter.pop(0) (x, y) = path[-1][-1] for (state, action) in successors(x, y, b1, b2).items(): if state not in explored: explored.add(state) path2 = path + [(action, state)] if goal in state: return path2 else: froniter.append(path2) return []def successors(x, y, X, Y): return { ((0, y+x) if x + y &lt;= Y else (x + y - Y, Y)): 'X -&gt; Y', ((x + y, 0) if x + y &lt;= X else (X, x + y - X)): 'X &lt;- Y', (X, y): '灌满X', (x, Y): '灌满Y', (0, y): '倒空X', (x, 0): '倒空Y', }if __name__ == '__main__': print(water_pouring(4, 9, 5)) print(water_pouring(4, 9, 5, start=(4, 0))) print(water_pouring(4, 9, 6))","link":"/example_01/"},{"title":"Initial exploration of machine learning","text":"The code address of this article is: Example 02 The source code is in ipynb format, and the output content can be viewed. ## Gradient 123456789101112131415161718192021222324import randomdef loss(k): return 3 * (k ** 2) + 7 * k - 10 # -b / 2a = -7 / 6def partial(k): return 6 * k + 7k = random.randint(-10, 10)alpha = 1e-3 # 0.001for i in range(1000): k = k + (-1) * partial(k) *alpha print(k, loss(k)) # out&quot;&quot;&quot;7.959 124.32404299999999-7.918246 122.66813714954799show more (open the raw output data in a text editor) ...-1.1833014444482555 -14.082503185837805&quot;&quot;&quot; Cutting Problem All the dynamic programming: sub-problems Overlapping sub-problems parse solution 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from collections import defaultdictfrom functools import lru_cache# least recent usedprices = [1, 5, 8, 9, 10, 17, 17, 20, 24, 30, 33]complete_price = defaultdict(int)for i, p in enumerate(prices): complete_price[i+1] = p solution = {}cache = {}#&lt;- if when n .... is huge. size(cache)# keep most important information.@lru_cache(maxsize=2**10)def r(n): # a very classical dynamic programming problem # if n in cache: return cache[n] candidates = [(complete_price[n], (n, 0))] + \\ [(r(i) + r(n-i), (i, n - i)) for i in range(1, n)] optimal_price, split = max(candidates) solution[n] = split # cache[n] = optimal_price return optimal_pricedef parse_solution(n, cut_solution): left, right = cut_solution[n] if left == 0 or right == 0: return [left+right, ] else: return parse_solution(left, cut_solution) + parse_solution(right, cut_solution)if __name__ == '__main__': print(r(19)) print(parse_solution(19, solution)) # out&quot;&quot;&quot;55[11, 6, 2]&quot;&quot;&quot; Dynamic 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748from collections import defaultdictfrom functools import wrapsfrom icecream import icoriginal_price = [1,5,8,9,10,17,17,20,24,30,33]price = defaultdict(int)for i, p in enumerate(original_price): price[i+1] = p def memo(func): cache = {} @wraps(func) def _wrap(n): if n in cache: result = cache[n] else: result = func(n) cache[n] = result return result return _wrap @memodef r(n): max_price, split_point = max( [(price[n],0)] + [(r(i) + r(n-i), i) for i in range(1, n)], key=lambda x: x[0] ) solution[n] = (split_point, n-split_point) return max_price def not_cut(split): return split == 0def parse_solution(target_length, revenue_solution): left, right = revenue_solution[target_length] if not_cut(left): return [right] return parse_solution(left, revenue_solution) + parse_solution(right, revenue_solution) solution = {}r(50)ic(parse_solution(20,solution))ic(parse_solution(19,solution))ic(parse_solution(27,solution))# out&quot;&quot;&quot;ic| parse_solution(20,solution): [10, 10]ic| parse_solution(19,solution): [2, 6, 11]ic| parse_solution(27,solution): [6, 10, 11][6, 10, 11]&quot;&quot;&quot; Gradient descent 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npimport matplotlib.pyplot as pltimport randomfrom icecream import icdef func(x): return 10 * x**2 + 32*x + 9def gradient(x): return 20 *x + 32 x = np.linspace(-10, 10)steps = []x_star = random.choice(x)alpha = 1e-3for i in range(100): x_star = x_star + -1*gradient(x_star)*alpha steps.append(x_star) ic(x_star, func(x_star))fig, ax = plt.subplots()ax.plot(x, func(x))&quot;&quot;&quot;ic| x_star: 9.368, func(x_star): 1186.3702400000002ic| x_star: 9.14864, func(x_star): 1138.732618496show more (open the raw output data in a text editor) ...ic| x_star: -0.1157435825983131, func(x_star): 5.430171125980905[&lt;matplotlib.lines.Line2D at 0x7fd6d19545d0&gt;]&quot;&quot;&quot;for i, s in enumerate(steps): ax.annotate(str(i+1), (s, func(s))) plt.show() k-means-finding-centers K-means 123456789101112131415161718192021222324252627282930313233343536373839404142from pylab import mplmpl.rcParams['font.sans-serif'] = ['FangSong'] # Specify the default fontmpl.rcParams['axes.unicode_minus'] = False # Solve the problem that the minus sign'-' is displayed as a square in the saved imagecoordination_source = &quot;&quot;&quot;{name:'兰州', geoCoord:[103.73, 36.03]},{name:'嘉峪关', geoCoord:[98.17, 39.47]},{name:'西宁', geoCoord:[101.74, 36.56]},{name:'成都', geoCoord:[104.06, 30.67]},{name:'石家庄', geoCoord:[114.48, 38.03]},{name:'拉萨', geoCoord:[102.73, 25.04]},{name:'贵阳', geoCoord:[106.71, 26.57]},{name:'武汉', geoCoord:[114.31, 30.52]},{name:'郑州', geoCoord:[113.65, 34.76]},{name:'济南', geoCoord:[117, 36.65]},{name:'南京', geoCoord:[118.78, 32.04]},{name:'合肥', geoCoord:[117.27, 31.86]},{name:'杭州', geoCoord:[120.19, 30.26]},{name:'南昌', geoCoord:[115.89, 28.68]},{name:'福州', geoCoord:[119.3, 26.08]},{name:'广州', geoCoord:[113.23, 23.16]},{name:'长沙', geoCoord:[113, 28.21]},//{name:'海口', geoCoord:[110.35, 20.02]},{name:'沈阳', geoCoord:[123.38, 41.8]},{name:'长春', geoCoord:[125.35, 43.88]},{name:'哈尔滨', geoCoord:[126.63, 45.75]},{name:'太原', geoCoord:[112.53, 37.87]},{name:'西安', geoCoord:[108.95, 34.27]},//{name:'台湾', geoCoord:[121.30, 25.03]},{name:'北京', geoCoord:[116.46, 39.92]},{name:'上海', geoCoord:[121.48, 31.22]},{name:'重庆', geoCoord:[106.54, 29.59]},{name:'天津', geoCoord:[117.2, 39.13]},{name:'呼和浩特', geoCoord:[111.65, 40.82]},{name:'南宁', geoCoord:[108.33, 22.84]},//{name:'西藏', geoCoord:[91.11, 29.97]},{name:'银川', geoCoord:[106.27, 38.47]},{name:'乌鲁木齐', geoCoord:[87.68, 43.77]},{name:'香港', geoCoord:[114.17, 22.28]},{name:'澳门', geoCoord:[113.54, 22.19]}&quot;&quot;&quot; Feacutre Extractor 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768city_location = { '香港': (114.17, 22.28)}test_string = &quot;{name:'兰州', geoCoord:[103.73, 36.03]},&quot;import repattern = re.compile(r&quot;name:'(\\w+)',\\s+geoCoord:\\[(\\d+.\\d+),\\s(\\d+.\\d+)\\]&quot;)for line in coordination_source.split('\\n'): city_info = pattern.findall(line) if not city_info: continue # following: we find the city info city, long, lat = city_info[0] long, lat = float(long), float(lat) city_location[city] = (long, lat)city_location# output&quot;&quot;&quot;{'香港': (114.17, 22.28), '兰州': (103.73, 36.03),show more (open the raw output data in a text editor) ... '澳门': (113.54, 22.19)}&quot;&quot;&quot;import mathdef geo_distance(origin, destination): &quot;&quot;&quot; Calculate the Haversine distance. Parameters ---------- origin : tuple of float (lat, long) destination : tuple of float (lat, long) Returns ------- distance_in_km : float Examples -------- &gt;&gt;&gt; origin = (48.1372, 11.5756) # Munich &gt;&gt;&gt; destination = (52.5186, 13.4083) # Berlin &gt;&gt;&gt; round(distance(origin, destination), 1) 504.2 &quot;&quot;&quot; lon1, lat1 = origin lon2, lat2 = destination radius = 6371 # km dlat = math.radians(lat2 - lat1) dlon = math.radians(lon2 - lon1) a = (math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon / 2) * math.sin(dlon / 2)) c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)) d = radius * c return d Vector Distances 余弦距离 Cosine Distance 欧几里得距离 Euclidean Distance 曼哈顿距离 Manhattan distance or Manhattan length 12345678910111213import matplotlib.pyplot as pltimport networkx as nximport warningswarnings.filterwarnings('ignore')%matplotlib inline# set plt, show chineseplt.rcParams['font.sans-serif'] = ['Arial Unicode MS']plt.rcParams['axes.unicode_minus'] = Falsecity_graph = nx.Graph()city_graph.add_nodes_from(list(city_location.keys()))nx.draw(city_graph, city_location, with_labels=True, node_size=30) K-means: Initial k random centers 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394k = 10import randomall_x = []all_y = []for _, location in city_location.items(): x, y = location all_x.append(x) all_y.append(y)def get_random_center(all_x, all_y): r_x = random.uniform(min(all_x), max(all_x)) r_y = random.uniform(min(all_y), max(all_y)) return r_x, r_yget_random_center(all_x, all_y)# output&quot;&quot;&quot;(93.61182991130997, 37.01816228131414)&quot;&quot;&quot;K = 5centers = {'{}'.format(i+1): get_random_center(all_x, all_y) for i in range(K)}from collections import defaultdictcloset_points = defaultdict(list)for x, y, in zip(all_x, all_y): closet_c, closet_dis = min([(k, geo_distance((x, y), centers[k])) for k in centers], key=lambda t: t[1]) closet_points[closet_c].append([x, y])import numpy as npdef iterate_once(centers, closet_points, threshold=5): have_changed = False for c in closet_points: former_center = centers[c] neighbors = closet_points[c] neighbors_center = np.mean(neighbors, axis=0) if geo_distance(neighbors_center, former_center) &gt; threshold: centers[c] = neighbors_center have_changed = True else: pass ## keep former center return centers, have_changeddef kmeans(Xs, k, threshold=5): all_x = Xs[:, 0] all_y = Xs[:, 1] K = k centers = {'{}'.format(i+1): get_random_center(all_x, all_y) for i in range(K)} changed = True while changed: closet_points = defaultdict(list) for x, y, in zip(all_x, all_y): closet_c, closet_dis = min([(k, geo_distance((x, y), centers[k])) for k in centers], key=lambda t: t[1]) closet_points[closet_c].append([x, y]) centers, changed = iterate_once(centers, closet_points, threshold) print('iteration') return centerskmeans(np.array(list(city_location.values())), k=5, threshold=5)# output&quot;&quot;&quot;iterationiterationiterationiterationiteration{'1': array([99.518, 38.86 ]), '2': array([117.833, 39.861]), '3': array([91.11, 29.97]), '4': array([106.81, 27. ]), '5': array([116.87166667, 27.6275 ])}&quot;&quot;&quot;plt.scatter(all_x, all_y)plt.scatter(*zip(*centers.values())) 12for c, points in closet_points.items(): plt.scatter(*zip(*points)) 1234567891011121314151617181920212223city_location_with_station = { '能源站-{}'.format(i): position for i, position in centers.items()}city_location_with_station# output&quot;&quot;&quot;{'能源站-1': (108.82946246581274, 26.05763939719317), '能源站-2': (97.96769355736322, 22.166113183141032), '能源站-3': (114.05390380408154, 38.7698708467224), '能源站-4': (118.49242085311417, 28.665716162786204), '能源站-5': (125.08287617496866, 25.55784683330647)}&quot;&quot;&quot;def draw_cities(citise, color=None): city_graph = nx.Graph() city_graph.add_nodes_from(list(citise.keys())) nx.draw(city_graph, citise, node_color=color, with_labels=True, node_size=30)%matplotlib inlineplt.figure(1,figsize=(12,12)) draw_cities(city_location_with_station, color='green')draw_cities(city_location, color='red') About the dataset This contains data of news headlines published over a period of 15 years. From the reputable Australian news source ABC (Australian Broadcasting Corp.) Site: http://www.abc.net.au/ Prepared by Rohit Kulkarni 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as np import pandas as pd import matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.feature_extraction import textfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.cluster import KMeansfrom nltk.tokenize import RegexpTokenizerfrom nltk.stem.snowball import SnowballStemmerimport warningswarnings.filterwarnings('ignore')%matplotlib inlinedata = pd.read_csv(&quot;./data/abcnews-date-text.csv&quot;,error_bad_lines=False,usecols =[&quot;headline_text&quot;])data.head()# output&quot;&quot;&quot;headline_text0 aba decides against community broadcasting lic...1 act fire witnesses must be aware of defamation2 a g calls for infrastructure protection summit3 air nz staff in aust strike for pay rise4 air nz strike to affect australian travellers&quot;&quot;&quot;data.to_csv('abcnews.csv', index=False, encoding='utf8')data.info()# output&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 1103665 entries, 0 to 1103664Data columns (total 1 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 headline_text 1103665 non-null objectdtypes: object(1)memory usage: 8.4+ MB&quot;&quot;&quot; Deleting dupliate headlines(if any) 12data[data['headline_text'].duplicated(keep=False)].sort_values('headline_text').head(8)data = data.drop_duplicates('headline_text') NLP Preparing data for vectorizaion However, when doing natural language processing, words must be converted into vectors that machine learning algorithms can make use of. If your goal is to do machine learning on text data, like movie reviews or tweets or anything else, you need to convert the text data into numbers. This process is sometimes referred to as “embedding” or “vectorization”. In terms of vectorization, it is important to remember that it isn’t merely turning a single word into a single number. While words can be transformed into numbers, an entire document can be translated into a vector. Not only can a vector have more than one dimension, but with text data vectors are usually high-dimensional. This is because each dimension of your feature data will correspond to a word, and the language in the documents you are examining will have thousands of words. TF-IDF In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf. Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification. One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model. 123456789101112131415punc = ['.', ',', '&quot;', &quot;'&quot;, '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',&quot;%&quot;]stop_words = text.ENGLISH_STOP_WORDS.union(punc)desc = data['headline_text'].valuesvectorizer = TfidfVectorizer(stop_words = stop_words)X = vectorizer.fit_transform(desc)word_features = vectorizer.get_feature_names()print(len(word_features))print(word_features[5000:5100])# output&quot;&quot;&quot;96397['abyss', 'ac', 'aca', 'acacia', 'acacias', 'acadamy', 'academia', 'academic', 'academics', 'academies', 'academy', 'academys', 'acai', 'acapulco', 'acars', 'acason', 'acasuso', 'acb', 'acbf', 'acc', 'acca', 'accan', 'accc', 'acccc', 'acccs', 'acccused', 'acce', 'accedes', 'accelerant', 'accelerants', 'accelerate', 'accelerated', 'accelerates', 'accelerating', 'acceleration', 'accelerator', 'accen', 'accent', 'accents', 'accentuate', 'accentuates', 'accentuating', 'accenture', 'accept', 'acceptability', 'acceptable', 'acceptably', 'acceptance', 'acceptances', 'accepted', 'accepting', 'acceptor', 'acceptors', 'accepts', 'accerate', 'acces', 'access', 'accessary', 'accessed', 'accesses', 'accessibility', 'accessible', 'accessing', 'accessories', 'accessory', 'accesss', 'acci', 'accid', 'accide', 'acciden', 'accidenatlly', 'accidenbt', 'accident', 'accidental', 'accidentally', 'accidently', 'accidents', 'acciona', 'accis', 'acclaim', 'acclaimed', 'acclamation', 'acclimatise', 'acco', 'accolade', 'accolades', 'accom', 'accomm', 'accommoda', 'accommodate', 'accommodated', 'accommodates', 'accommodating', 'accommodation', 'accomo', 'accomodation', 'accomommodation', 'accompanied', 'accompanies', 'accompaniment']&quot;&quot;&quot; Stemming Stemming is the process of reducing a word into its stem, i.e. its root form. The root form is not necessarily a word by itself, but it can be used to generate words by concatenating the right suffix. For example, the words fish, fishes and fishing all stem into fish, which is a correct word. On the other side, the words study, studies and studying stems into studi, which is not an English word. Tokenizing Tokenization is breaking the sentence into words and punctuation, 12345stemmer = SnowballStemmer('english')tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')def tokenize(text): return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())] Vectorization with stop words(words irrelevant to the model), stemming and tokenizing 123456789101112131415vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)X2 = vectorizer2.fit_transform(desc)word_features2 = vectorizer2.get_feature_names()print(len(word_features2))print(word_features2[:50]) # output&quot;&quot;&quot;65232[&quot;'a&quot;, &quot;'i&quot;, &quot;'s&quot;, &quot;'t&quot;, 'aa', 'aaa', 'aaahhh', 'aac', 'aacc', 'aaco', 'aacta', 'aad', 'aadmi', 'aag', 'aagaard', 'aagard', 'aah', 'aalto', 'aam', 'aamer', 'aami', 'aamodt', 'aandahl', 'aant', 'aap', 'aapa', 'aapt', 'aar', 'aaradhna', 'aardman', 'aardvark', 'aargau', 'aaron', 'aaronpaul', 'aarwun', 'aat', 'ab', 'aba', 'abaaoud', 'ababa', 'aback', 'abadi', 'abadon', 'abal', 'abalon', 'abalonv', 'abama', 'abandon', 'abandond', 'abandong']&quot;&quot;&quot;vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)X3 = vectorizer3.fit_transform(desc)words = vectorizer3.get_feature_names() For this, we will use k-means clustering algorithm. ### K-means clustering (Source Wikipedia) Elbow method to select number of clusters This method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified. Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance. Basically, number of clusters = the x-axis value of the point that is the corner of the \"elbow\"(the plot looks often looks like an elbow) 123456789101112from sklearn.cluster import KMeanswcss = []for i in range(1,11): kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0) kmeans.fit(X3) wcss.append(kmeans.inertia_)plt.plot(range(1,11),wcss)plt.title('The Elbow Method')plt.xlabel('Number of clusters')plt.ylabel('WCSS')plt.savefig('elbow.png')plt.show() As more than one elbows have been generated, I will have to select right amount of clusters by trial and error. So, I will showcase the results of different amount of clusters to find out the right amount of clusters. 123456print(words[250:300])# output&quot;&quot;&quot;['decis', 'declar', 'defenc', 'defend', 'delay', 'deliv', 'demand', 'deni', 'despit', 'destroy', 'detent', 'develop', 'die', 'director', 'disabl', 'disast', 'discuss', 'diseas', 'dismiss', 'disput', 'doctor', 'dog', 'dollar', 'domest', 'donald', 'donat', 'doubl', 'doubt', 'draw', 'dri', 'drink', 'drive', 'driver', 'drop', 'drought', 'drown', 'drug', 'drum', 'dump', 'dure', 'e', 'eagl', 'earli', 'eas', 'east', 'econom', 'economi', 'edg', 'educ', 'effort']&quot;&quot;&quot; 3 Clusters 12345678910111213kmeans = KMeans(n_clusters = 3, n_init = 20, n_jobs = 1) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)kmeans.fit(X3)# We look at 3 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid)) # output&quot;&quot;&quot;0 : new, say, plan, win, council, govt, australia, report, kill, fund, urg, court, warn, water, australian, nsw, open, chang, year, qld, interview, wa, death, face, crash1 : polic, investig, probe, man, search, offic, hunt, miss, arrest, death, car, shoot, drug, seek, attack, assault, say, murder, crash, charg, driver, suspect, fatal, raid, station2 : man, charg, murder, court, face, jail, assault, stab, die, death, drug, guilti, child, sex, accus, attack, woman, crash, arrest, car, kill, miss, sydney, alleg, plead&quot;&quot;&quot; 5 Clusters 123456789101112131415kmeans = KMeans(n_clusters = 5, n_init = 20, n_jobs = 1)kmeans.fit(X3)# We look at 5 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))# output&quot;&quot;&quot;0 : man, plan, charg, court, govt, australia, face, murder, accus, jail, assault, stab, urg, drug, death, attack, child, sex, die, woman, guilti, say, alleg, told, car1 : new, zealand, law, year, plan, open, polic, home, hospit, centr, deal, set, hope, australia, look, appoint, announc, chief, say, south, minist, govt, rule, servic, welcom2 : say, win, kill, report, australian, warn, interview, open, water, fund, nsw, crash, death, urg, year, chang, wa, sydney, claim, qld, hit, attack, world, set, health3 : council, plan, consid, fund, rate, urg, seek, new, merger, water, land, develop, reject, say, mayor, vote, chang, elect, rise, meet, park, push, want, govt, approv4 : polic, investig, man, probe, search, offic, hunt, miss, arrest, death, car, charg, shoot, drug, seek, attack, assault, murder, crash, say, driver, fatal, suspect, raid, woman&quot;&quot;&quot; 6 Clusters 12345678910111213141516kmeans = KMeans(n_clusters = 6, n_init = 20, n_jobs = 1)kmeans.fit(X3)# We look at 6 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid)) # output&quot;&quot;&quot;0 : council, govt, australia, report, warn, urg, fund, australian, water, nsw, chang, qld, wa, health, elect, rural, countri, hour, sa, boost, climat, govern, servic, south, consid1 : man, charg, murder, court, face, jail, assault, stab, die, death, drug, guilti, child, sex, accus, attack, woman, crash, arrest, car, kill, miss, sydney, plead, alleg2 : polic, investig, probe, man, search, offic, hunt, miss, arrest, death, car, shoot, drug, seek, attack, crash, assault, murder, charg, driver, say, fatal, suspect, raid, warn3 : win, kill, court, interview, crash, open, death, sydney, face, year, claim, hit, attack, world, set, final, day, hous, die, home, jail, talk, return, cup, hospit4 : new, zealand, law, year, plan, open, council, polic, home, hospit, centr, deal, set, hope, australia, appoint, look, announc, chief, say, govt, south, minist, mayor, welcom5 : say, plan, council, govt, water, need, group, chang, labor, minist, govern, opposit, public, mp, health, union, green, hous, develop, resid, report, expert, cut, australia, mayor&quot;&quot;&quot; 8 Clusters 123456789101112131415161718kmeans = KMeans(n_clusters = 8, n_init = 20, n_jobs = 1)kmeans.fit(X3)# Finally, we look at 8 the clusters generated by k-means.common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]for num, centroid in enumerate(common_words): print(str(num) + ' : ' + ', '.join(words[word] for word in centroid)) # output&quot;&quot;&quot;0 : polic, say, man, miss, arrest, jail, investig, car, search, murder, attack, crash, kill, probe, die, hunt, shoot, assault, offic, drug, stab, accus, fatal, guilti, bodi1 : death, hous, polic, toll, investig, man, probe, inquest, rise, woman, coron, blaze, price, public, white, babi, sentenc, famili, road, spark, jail, prompt, blame, custodi, report2 : plan, council, govt, water, new, say, develop, hous, group, chang, unveil, reject, park, urg, centr, public, expans, green, resid, health, reveal, labor, govern, opposit, power3 : court, face, man, accus, told, hear, murder, high, case, appear, rule, charg, alleg, appeal, drug, jail, woman, death, assault, order, sex, stab, challeng, teen, polic4 : australia, govt, kill, report, warn, australian, urg, fund, nsw, interview, water, open, crash, qld, chang, wa, year, day, claim, hit, attack, sydney, set, health, world5 : new, council, zealand, law, fund, year, consid, water, urg, open, say, seek, rate, centr, mayor, govt, elect, look, develop, land, deal, hope, set, push, home6 : win, award, cup, titl, open, gold, stage, world, final, tour, elect, australia, lead, seri, aussi, claim, second, australian, big, england, grand, m, battl, race, record7 : charg, man, murder, face, assault, drug, polic, child, sex, woman, teen, death, stab, drop, alleg, attack, rape, men, guilti, shoot, bail, sydney, fatal, driver, yo&quot;&quot;&quot; Because even I didn't know what kind of clusters would be generated, I will describe them in comments. Other discussions 1234567891011121314151617181920212223242526272829303132333435363738394041import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom nltk.corpus import stopwordsfrom sklearn.feature_extraction.text import CountVectorizerimport gensimfrom collections import Counterimport stringfrom nltk.stem import WordNetLemmatizer, PorterStemmerfrom nltk.tokenize import word_tokenizeimport pyLDAvis.gensim_modelsfrom wordcloud import WordCloud, STOPWORDSfrom textblob import TextBlobfrom spacy import displacyimport nltkimport warningswarnings.filterwarnings('ignore')# set pltplt.rcParams['font.sans-serif'] = ['Arial Unicode MS']plt.rcParams.update({'font.size': 12})plt.rcParams.update({'figure.figsize': [16, 12]})# plt.figure(figsize = [20, 20])plt.style.use('seaborn-whitegrid')df = pd.read_csv('../data/abcnews-date-text.csv', nrows = 10000)df.head()# output&quot;&quot;&quot;publish_date headline_text0 20030219 aba decides against community broadcasting lic...1 20030219 act fire witnesses must be aware of defamation2 20030219 a g calls for infrastructure protection summit3 20030219 air nz staff in aust strike for pay rise4 20030219 air nz strike to affect australian travellers&quot;&quot;&quot; The data set contains only two columns, the release date and the news title. For simplicity, I will explore the first 10,000 rows in this dataset. Since the titles are sorted by publish_date, they are actually two months from February 19, 2003 to April 7, 2003. Number of characters present in each sentence Visualization of text statistics is a simple but insightful technique. They include: Word frequency analysis, sentence length analysis, average word length analysis, etc. These really help to explore the basic characteristics of text data. For this, we will mainly use histograms (continuous data) and bar graphs (categorical data). First, let me look at the number of characters in each sentence. This can give us a rough idea of the length of news headlines. 1df['headline_text'].str.len().hist() number of words appearing in each news headline The histogram shows that the range of news headlines is 10 to 70 characters, usually between 25 and 55 characters. Now, we will continue to explore the data verbatim. Let's plot the number of words that appear in each news headline. 1df['headline_text'].str.split().map(lambda x: len(x)).hist() Analysing word length Obviously, the number of words in news headlines is in the range of 2 to 12, and most of them are between 5 and 7. Next, let's check the average word length in each sentence. 1df['headline_text'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x)).hist() The average word length is between 3 and 9, and the most common length is 5. Does this mean that people use very short words in news headlines? Let us find out. One reason that may not be the case is stop words. Stop words are the most commonly used words in any language (such as \"the\", \"a\", \"an\", etc.). Since the length of these words may be small, these words may cause the above graphics to be skewed to the left. Analyzing the number and types of stop words can give us some in-depth understanding of the data. To get a corpus containing stop words, you can use the nltk library. Nltk contains stop words from multiple languages. Since we only deal with English news, I will filter English stop words from the corpus. Analysing stopwords 1234567891011121314151617181920212223242526272829# Fetch stopwordsimport nltknltk.download('stopwords')stop=set(stopwords.words('english'))# output&quot;&quot;&quot;[nltk_data] Downloading package stopwords to /Users/xx/nltk_data...[nltk_data] Package stopwords is already up-to-date!&quot;&quot;&quot;# Create corpuscorpus=[]new= df['headline_text'].str.split()new=new.values.tolist()corpus=[word for i in new for word in i]from collections import defaultdictdic=defaultdict(int)for word in corpus: if word in stop: dic[word]+=1 # Plot top stopwordstop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] x,y=zip(*top)plt.bar(x,y) Draw popular stop words Most common words We can clearly see that in the news headlines, stop words such as \"to\", \"in\" and \"for\" dominate. So now that we know which stop words appear frequently in our text, let's check which words other than these stop words appear frequently. We will use the counter function in the collection library to count the occurrence of each word and store it in a list of tuples. This is a very useful feature when we are dealing with word-level analysis in natural language processing. 12345678910counter=Counter(corpus)most=counter.most_common()x, y=[], []for word,count in most[:40]: if (word not in stop): x.append(word) y.append(count) sns.barplot(x=y,y=x) Wow! In the past 15 years, \"America\", \"Iraq\" and \"War\" have dominated the headlines. \"We\" here may mean the United States or us (you and me). We are not a stop word, but when we look at the other words in the picture, they are all related to the United States-the Iraq War and \"we\" here may mean the United States. Ngram analysis Ngram is a continuous sequence of n words. For example, \"Riverbank\", \"Three Musketeers\" and so on. If the number of words is two, it is called a double word. For 3 characters, it is called a trigram, and so on. Viewing the most common n-grams can give you a better understanding of the context in which the word is used. Bigram analysis To build our vocabulary, we will use Countvectorizer. Countvectorizer is a simple method for labeling, vectorizing and representing corpora in an appropriate form. Can be found in sklearn.feature_engineering.text Therefore, we will analyze the top news in all news headlines. 123456789101112def get_top_ngram(corpus, n=None): vec = CountVectorizer(ngram_range=(n, n)).fit(corpus) bag_of_words = vec.transform(corpus) sum_words = bag_of_words.sum(axis=0) words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) return words_freq[:10]top_n_bigrams=get_top_ngram(df['headline_text'],2)[:10]x,y=map(list,zip(*top_n_bigrams))sns.barplot(x=y,y=x) Trigram analysis We can observe that dualisms such as \"anti-war\" and \"killed\" related to war dominate the headlines. How about triples? 123top_tri_grams=get_top_ngram(df['headline_text'],n=3)x,y=map(list,zip(*top_tri_grams))sns.barplot(x=y,y=x) We can see that many of these hexagrams are a combination of \"face the court\" and \"anti-war protest.\" This means that we should spend some effort on data cleaning to see if we can combine these synonyms into a clean token. Topic modelling Use pyLDAvis for topic modeling exploration Topic modeling is the process of using unsupervised learning techniques to extract the main topics that appear in the document set. Latent Dirichlet Allocation (LDA) is an easy-to-use and efficient topic modeling model. Each document is represented by a topic distribution, and each topic is represented by a word distribution. Once the documents are classified into topics, you can delve into the data for each topic or topic group. But before entering topic modeling, we must do some preprocessing of the data. we will: Tokenization: The process of converting sentences into tokens or word lists. remove stopwordslemmatize: Reduce the deformed form of each word to a common base or root. Convert to word bag: word bag is a dictionary where the key is the word (or ngram/tokens) and the value is the number of times each word appears in the corpus. With NLTK, you can easily tokenize and formalize: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import nltknltk.download('punkt')nltk.download('wordnet')# output&quot;&quot;&quot;[nltk_data] Downloading package punkt to /Users/xx/nltk_data...[nltk_data] Package punkt is already up-to-date![nltk_data] Downloading package wordnet to /Users/xx/nltk_data...[nltk_data] Unzipping corpora/wordnet.zip.True&quot;&quot;&quot;def preprocess_news(df): corpus=[] stem=PorterStemmer() lem=WordNetLemmatizer() for news in df['headline_text']: words=[w for w in word_tokenize(news) if (w not in stop)] words=[lem.lemmatize(w) for w in words if len(w)&gt;2] corpus.append(words) return corpus corpus = preprocess_news(df)# Now, let's use gensim to create a bag of words modeldic=gensim.corpora.Dictionary(corpus)bow_corpus = [dic.doc2bow(doc) for doc in corpus]# We can finally create the LDA model:lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 4, id2word = dic, passes = 10, workers = 2)lda_model.show_topics()# output&quot;&quot;&quot;[(0, '0.010*&quot;say&quot; + 0.007*&quot;cup&quot; + 0.006*&quot;war&quot; + 0.005*&quot;world&quot; + 0.005*&quot;back&quot; + 0.005*&quot;plan&quot; + 0.005*&quot;green&quot; + 0.004*&quot;win&quot; + 0.004*&quot;woman&quot; + 0.004*&quot;new&quot;'), (1, '0.010*&quot;govt&quot; + 0.009*&quot;war&quot; + 0.009*&quot;new&quot; + 0.007*&quot;may&quot; + 0.005*&quot;sars&quot; + 0.005*&quot;call&quot; + 0.005*&quot;protest&quot; + 0.005*&quot;boost&quot; + 0.005*&quot;group&quot; + 0.004*&quot;hospital&quot;'), (2, '0.018*&quot;police&quot; + 0.015*&quot;baghdad&quot; + 0.014*&quot;man&quot; + 0.005*&quot;missing&quot; + 0.005*&quot;claim&quot; + 0.005*&quot;court&quot; + 0.005*&quot;australia&quot; + 0.004*&quot;move&quot; + 0.004*&quot;murder&quot; + 0.004*&quot;charged&quot;'), (3, '0.030*&quot;iraq&quot; + 0.015*&quot;war&quot; + 0.007*&quot;iraqi&quot; + 0.007*&quot;council&quot; + 0.006*&quot;troop&quot; + 0.005*&quot;killed&quot; + 0.004*&quot;crash&quot; + 0.004*&quot;soldier&quot; + 0.004*&quot;open&quot; + 0.004*&quot;say&quot;')]&quot;&quot;&quot; Theme 0 represents things related to the Iraq war and the police. Theme 3 shows Australia's involvement in the Iraq War. You can print all the topics and try to understand them, but there are tools that can help you run this data exploration more effectively. pyLDAvis is such a tool, it can interactively visualize the results of LDA. Visualize the topics 123pyLDAvis.enable_notebook()vis = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dic)vis On the left, the area of each circle represents the importance of the topic relative to the corpus. Because there are four themes, we have four circles. The distance between the center of the circle indicates the similarity between themes. Here you can see that Topic 3 and Topic 4 overlap, which indicates that the themes are more similar. On the right, the histogram of each topic shows the top 30 related words. For example, in topic 1, the most relevant words are \"police\", \"new\", \"may\", \"war\", etc. Therefore, in our case, we can see many war-related words and topics in the news headlines. Wordclouds Wordcloud is a great way to represent text data. The size and color of each word appearing in the word cloud indicate its frequency or importance. It is easy to create a wordcloud using python, but we need to provide data in the form of a corpus. 123456789101112131415161718192021stopwords = set(STOPWORDS)def show_wordcloud(data, title = None): wordcloud = WordCloud( background_color='white', stopwords=stopwords, max_words=100, max_font_size=30, scale=3, random_state=1 ) wordcloud=wordcloud.generate(str(data)) fig = plt.figure(1, figsize=(12, 12)) plt.axis('off') plt.imshow(wordcloud) plt.show() show_wordcloud(corpus) Similarly, you can see that terms related to war are highlighted, indicating that these words often appear in news headlines. There are many parameters that can be adjusted. Some of the most famous are: stopwords: stop a group of words appearing in the image. max_words: Indicates the maximum number of words to be displayed. max_font_size: Maximum font size. There are many other options to create beautiful word clouds. For more detailed information, you can refer to here. Text sentiment Sentiment analysis is a very common natural language processing task in which we determine whether the text is positive, negative or neutral. This is very useful for finding sentiments related to comments and comments, allowing us to gain some valuable insights from text data. There are many projects that can help you use python for sentiment analysis. I personally like TextBlob and Vader Sentiment. 1234567from textblob import TextBlobTextBlob('100 people killed in Iraq').sentiment# output&quot;&quot;&quot;Sentiment(polarity=-0.2, subjectivity=0.0)&quot;&quot;&quot; Textblob Textblob is a python library built on top of nltk. It has been around for a while and is very easy to use. The sentiment function of TextBlob returns two attributes: Polarity: It is a floating-point number in the range of [-1,1], where 1 means a positive statement and -1 means a negative statement. Subjectivity: refers to how personal opinions and feelings affect someone’s judgment. The subjectivity is expressed as a floating point value with a range of [0,1]. I will run this feature on news headlines. TextBlob claims that the text \"100 people killed in Iraq\" is negative, not a view or feeling, but a statement of fact. I think we can agree to TextBlob here. Now that we know how to calculate these sentiment scores, we can use histograms to visualize them and explore the data further. 123456def polarity(text): return TextBlob(text).sentiment.polaritydf['polarity_score']=df['headline_text'].\\ apply(lambda x : polarity(x))df['polarity_score'].hist() You will see that the polarity is mainly between 0.00 and 0.20. This shows that most news headlines are neutral. Let's categorize news as negative, positive, and neutral based on the scores for a more in-depth study. Postive , Negative or Neutral ? 1234567891011121314def sentiment(x): if x&lt;0: return 'neg' elif x==0: return 'neu' else: return 'pos' df['polarity']=df['polarity_score'].\\ map(lambda x: sentiment(x)) plt.bar(df.polarity.value_counts().index, df.polarity.value_counts()) Yes, 70% of news is neutral, only 18% of positive news and 11% of negative news. Let's look at the positive and negative headlines. 1234567891011df[df['polarity']=='neg']['headline_text'].head(5)# output&quot;&quot;&quot;7 aussie qualifier stosur wastes four memphis match23 carews freak goal leaves roma in ruins28 council chief executive fails to secure position34 dargo fire threat expected to rise40 direct anger at govt not soldiers crean urgesName: headline_text, dtype: object&quot;&quot;&quot; Vader The next library we are going to discuss is VADER. Vader is better at detecting negative emotions. It is very useful in the context of social media text sentiment analysis. The VADER or Valence Aware dictionary and sentiment reasoner is an open source sentiment analyzer pre-built library based on rules/dictionaries and is protected by the MIT license. The VADER sentiment analysis class returns a dictionary that contains the possibility that the text appears positive, negative, and neutral. Then, we can filter and select the emotion with the highest probability. We will use VADER to perform the same analysis and check if the difference is large. 1234567891011121314151617181920212223from nltk.sentiment.vader import SentimentIntensityAnalyzernltk.download('vader_lexicon')sid = SentimentIntensityAnalyzer()def get_vader_score(sent): # Polarity score returns dictionary ss = sid.polarity_scores(sent) #return ss return np.argmax(list(ss.values())[:-1]) &quot;&quot;&quot;[nltk_data] Downloading package vader_lexicon to[nltk_data] /Users/xx/nltk_data...&quot;&quot;&quot;df['polarity']=df['headline_text'].\\ map(lambda x: get_vader_score(x))polarity=df['polarity'].replace({0:'neg',1:'neu',2:'pos'})plt.bar(polarity.value_counts().index, polarity.value_counts()) Yes, the distribution is slightly different. There are even more headlines classified as neutral 85%, and the number of negative news headlines has increased (to 13%). Named Entity Recognition Named entity recognition is an information extraction method in which entities existing in the text are classified into predefined entity types, such as \"person\", \"location\", \"organization\" and so on. By using NER, we can gain insight into the entities that exist in a given text data set of entity types. Let us consider an example of a news article. In the above news, the named entity recognition model should be able to recognize Entities, such as RBI as an organization, Mumbai and India as Places, etc. There are three standard libraries for named entity recognition: Stanford Nell space NLTK I will use spaCy, which is an open source library for advanced natural language processing tasks. It is written in Cython and is known for its industrial applications. In addition to NER, spaCy also provides many other functions, such as pos mark, word to vector conversion, etc. SpaCy’s Named Entity Recognition has been published in OntoNotes 5 has been trained on the corpus and supports the following entity types There are three kinds of pre-trained models for English in SpaCy. I will use en_core_web_sm to complete our task, but you can try other models. To use it, we must first download it: 12345678910111213141516171819202122232425# !python -m spacy download en_core_web_sm# Now we can initialize the language model:import spacyfrom spacy import displacyimport en_core_web_smnlp = en_core_web_sm.load()# nlp = spacy.load(&quot;en_core_web_sm&quot;)# One of the advantages of Spacy is that we only need to apply the nlp function once, and the entire background pipeline will return the objects we needdoc=nlp('India and Iran have agreed to boost the economic \\viability of the strategic Chabahar port through various measures, \\including larger subsidies to merchant shipping firms using the facility, \\people familiar with the development said on Thursday.')[(x.text,x.label_) for x in doc.ents] &quot;&quot;&quot;[('India', 'GPE'), ('Iran', 'GPE'), ('Chabahar', 'GPE'), ('Thursday', 'DATE')]&quot;&quot;&quot; We can see that India and Iran are confirmed as geographic locations (GPE), Chabahar is confirmed as a person, and Thursday is confirmed as a date. We can also use the display module in spaCy to visualize the output. 123from spacy import displacydisplacy.render(doc, style='ent') This can make sentences with recognized entities look very neat, and each entity type is marked with a different color. Now that we know how to perform NER, we can further explore the data by performing various visualizations on the named entities extracted from the data set. First, we will run named entity recognition on news headlines and store entity types. NER Analysis 123456789101112def ner(text): doc=nlp(text) return [X.label_ for X in doc.ents] ent=df['headline_text'].apply(lambda x : ner(x))ent=[x for sub in ent for x in sub]counter=Counter(ent)count=counter.most_common()# Now, we can visualize the entity frequency:x,y=map(list,zip(*count))sns.barplot(x=y,y=x) Now we can see that GPE and ORG dominate the headlines, followed by the PERSON entity. We can also visualize the most common tokens for each entity. Let's check which places appear the most in news headlines. Most common GPE 12345678910def ner(text,ent=&quot;GPE&quot;): doc=nlp(text) return [X.text for X in doc.ents if X.label_ == ent] gpe=df['headline_text'].apply(lambda x: ner(x,&quot;GPE&quot;))gpe=[i for x in gpe for i in x]counter=Counter(gpe)x,y=map(list,zip(*counter.most_common(10)))sns.barplot(y,x) I think we can confirm the fact that \"America\" means America in news headlines. Let's also find the most common names that appear on news headlines. Most common person 123456per=df['headline_text'].apply(lambda x: ner(x,&quot;PERSON&quot;))per=[i for x in per for i in x]counter=Counter(per)x,y=map(list,zip(*counter.most_common(10)))sns.barplot(y,x) Saddam Hussein and George Bush served as presidents of Iraq and the United States during the war. In addition, we can see that the model is far from perfect to classify \"vic govt\" or \"nsw govt\" as individuals rather than government agencies. Pos tagging Use nltk for all parts of speech markup, but there are other libraries that can do the job well (spaacy, textblob). 123456789101112import nltknltk.download('averaged_perceptron_tagger')sentence=&quot;The greatest comeback stories in 2019&quot;tokens=word_tokenize(sentence)nltk.pos_tag(tokens)# Notice:# You can also use the spacy.displacy module to visualize the sentence part of the speech and its dependency graph.doc = nlp('The greatest comeback stories in 2019')displacy.render(doc, style='dep', jupyter=True, options={'distance': 90}) We can observe various dependency labels here. For example, the DET tag indicates the relationship between the word \"the\" and the noun \"stories\". You can check the list of dependency labels and their meanings here. Okay, now that we know what a POS tag is, let's use it to explore the title data set. Analysing pos tags 1234567891011def pos(text): pos=nltk.pos_tag(word_tokenize(text)) pos=list(map(list,zip(*pos)))[1] return pos tags=df['headline_text'].apply(lambda x : pos(x))tags=[x for l in tags for x in l]counter=Counter(tags)x,y=list(map(list,zip(*counter.most_common(7))))sns.barplot(x=y,y=x) We can clearly see that nouns (NN) dominate in news headlines, followed by adjectives (JJ). This is typical for news reports, and for art forms, higher adjective (ADJ) frequencies may happen a lot. You can investigate this in more depth by investigating the most common singular nouns in news headlines. Let us find out. Nouns such as \"war\", \"Iraq\", and \"person\" dominate the news headlines. You can use the above functions to visualize and check other parts of the voice. Most common Nouns 123456789101112131415def get_adjs(text): adj=[] pos=nltk.pos_tag(word_tokenize(text)) for word,tag in pos: if tag=='NN': adj.append(word) return adjwords=df['headline_text'].apply(lambda x : get_adjs(x))words=[x for l in words for x in l]counter=Counter(words)x,y=list(map(list,zip(*counter.most_common(7))))sns.barplot(x=y,y=x) Dependency graph 12doc = nlp('She sells seashells by the seashore')displacy.render(doc, style='dep', jupyter=True, options={'distance': 90}) Text readability Textstat 12from textstat import flesch_reading_easedf['headline_text'].apply(lambda x : flesch_reading_ease(x)).hist() complex headlines? Almost all readability scores exceed 60. This means that an average of 11-year-old students can read and understand news headlines. Let's check all news headlines with a readability score below 5. 123456789101112131415x=[i for i in range(len(reading)) if reading[i]&lt;5] &quot;&quot;&quot;rror loading preloads:Failed to fetch dynamically imported module: https://file+.vscode-resource.vscode-webview.net/Users/xx/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/datascience-ui/errorRenderer/errorRenderer.js&quot;&quot;&quot;news.iloc[x]['headline_text'].head() &quot;&quot;&quot;Error loading preloads:Failed to fetch dynamically imported module: https://file+.vscode-resource.vscode-webview.net/Users/xx/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/datascience-ui/errorRenderer/errorRenderer.js&quot;&quot;&quot; Final thoughts In this article, we discussed and implemented various exploratory data analysis methods for text data. Some are common and little known, but all of them can be an excellent addition to your data exploration toolkit. Hope you will find some of them useful for your current and future projects. To make data exploration easier, I created a \"exploratory data analysis of natural language processing templates\", which you can use for your work. In addition, you may have seen that for each chart in this article, there is a code snippet to create it. Just click the button below the chart. Happy exploring! From: https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr","link":"/example_02/"},{"title":"Machine Learning Part-01","text":"Linear Regression Example Implement Linear Regression for Boston House Price Problem 123456789import randomimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsimport numpy as npfrom sklearn.datasets import load_bostonfrom matplotlib.animation import FuncAnimationimport re Part-01: Linear Regression 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200housing_price = load_boston()dataframe = pd.DataFrame(housing_price['data'])dataframe.columns = housing_price['feature_names']dataframe['price'] = housing_price['target']# sns.heatmap(dataframe.corr(), annot=True, fmt='.1f')# plt.show()print(dataframe.columns) &quot;&quot;&quot;Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'price'], dtype='object')&quot;&quot;&quot;rm = dataframe['RM']lst = dataframe['LSTAT']target = dataframe['price']def model(x, w, b): return np.dot(x, w.T) + bdef loss(yhat, y): return np.mean( (yhat - y) ** 2)def partial_w(x1, x2, y, yhat): return np.array([2 *np.mean((yhat - y) * x1), 2 * np.mean((yhat - y) * x2)])def partial_b(x1, x2, y, yhat): return 2 * np.mean((yhat - y))w = np.random.random_sample((1, 2))print(w)b = 0alpha = 1e-5epoch = 200history = []history_k_b_loss = [] &quot;&quot;&quot;[[0.76646144 0.3095512 ]]&quot;&quot;&quot;for e in range(epoch): losses = [] for batch in range(len(rm)): random_index = random.choice(range(len(rm))) x1, x2 = rm[random_index], lst[random_index] y = target[random_index] yhat = model(np.array([x1, x2]), w, b) loss_v = loss(yhat, y) w = w - partial_w(x1, x2, y, yhat) * alpha b = b - partial_b(x1, x2, y, yhat) * alpha losses.append(loss_v) history_k_b_loss.append((w, b, loss_v)) if batch % 100 == 0: print('Epoch: {}, Batch: {}, loss: {}'.format(e, batch, np.mean(losses))) history.append(np.mean(losses)) &quot;&quot;&quot;Epoch: 0, Batch: 0, loss: 151.86271856102778Epoch: 0, Batch: 100, loss: 263.5872813250959show more (open the raw output data in a text editor) ...Epoch: 199, Batch: 500, loss: 28.308274447364248&quot;&quot;&quot;````## Logstic Regression```pythonhousing_price = load_boston()dataframe = pd.DataFrame(housing_price['data'])dataframe.columns = housing_price['feature_names']dataframe['price'] = housing_price['target']rm = dataframe['RM']lst = dataframe['LSTAT']price = dataframe['price']print(np.percentile(price, 66)) &quot;&quot;&quot;23.53&quot;&quot;&quot;# plt.hist(target)# plt.show()dataframe['expensive'] = dataframe['price'].apply(lambda p: int(p &gt; np.percentile(price, 66)))expensive = dataframe['expensive']# print(dataframe.head())print(dataframe['expensive']) &quot;&quot;&quot;0 11 0 ..505 0Name: expensive, Length: 506, dtype: int64&quot;&quot;&quot;def logistic(x): return 1 / (1 + np.exp(-x))def model(x, w, b): return logistic(np.dot(x, w.T) + b)def loss(yhat, y): return -1 * np.sum(y*np.log(yhat) + (1 - y) * np.log(1 - yhat))def partial_w(x1, x2, y, yhat): return np.array([np.sum((yhat - y) * x1), np.sum((yhat - y) * x2)])def partial_b(x1, x2, y, yhat): return np.sum(yhat - y) w = np.random.random_sample((1, 2))print(w) &quot;&quot;&quot;[[0.69565948 0.90768813]]&quot;&quot;&quot;b = 0alpha = 1e-5epoch = 200history = []history_k_b_loss = []for e in range(epoch): losses = [] for batch in range(len(rm)): random_index = random.choice(range(len(rm))) x1, x2 = rm[random_index], lst[random_index] y = expensive[random_index] yhat = model(np.array([x1, x2]), w, b) loss_v = loss(yhat, y) w = w - partial_w(x1, x2, y, yhat) * alpha b = b - partial_b(x1, x2, y, yhat) * alpha losses.append(loss_v) history_k_b_loss.append((w, b, loss_v)) if batch % 100 == 0: print('Epoch: {}, Batch: {}, loss: {}'.format(e, batch, np.mean(losses))) history.append(np.mean(losses)) &quot;&quot;&quot;Epoch: 0, Batch: 0, loss: 3.14765267665445e-06Epoch: 0, Batch: 100, loss: 13.555508645878497show more (open the raw output data in a text editor) ...Epoch: 199, Batch: 500, loss: 0.31372698791846687&quot;&quot;&quot;predicated = [model(np.array([x1, x2]), w, b) for x1, x2 in zip(rm, lst)]true = expensivedef accuracy(y, yhat): return sum(1 if i == j else 0 for i, j in zip(y, yhat)) / len(y) print(accuracy(true, predicated)) &quot;&quot;&quot;0.0&quot;&quot;&quot; decision boundary Linear Regression: Regression is implemented, including the definition of linear functions, why use linear functions, the meaning of loss, the meaning of gradient descent, stochastic gradient descent Use Boston house price dataset. The data set of Beijing housing prices in 2020, why didn’t I use the data set of Beijing housing prices? Boston: room size, subway, highway, crime rate have a more obvious relationship, so it is easier to observe the relationship Beijing's housing prices:! Far and near! Room Condition ==》 School District! ! ! ! =&gt; Very expensive Haidian District 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import randomimport numpy as npimport pandas as pdfrom sklearn.datasets import load_bostondataset = load_boston()data = dataset['data']target = dataset['target']columns = dataset['feature_names']dataframe = pd.DataFrame(data)dataframe.columns = columnsdataframe['price'] = target# print(dataframe.corr()) # show the correlation of dataframe variables# correlation =&gt; If one value increases, it will cause another value to increase, and the correlation coefficient is closer to 1 if it increases in a certain proportion.# correlation =&gt; 0 means there is no relationship between the two# correlation =&gt; -1 One value increases, the other value must decrease, and the decrease is in equal proportion# sns.heatmap(dataframe.corr())# plt.show()# RM: The average number of bedrooms in the community# LSTAT: Percentage of low-income people aroundrm = dataframe['RM']lstat = dataframe['LSTAT']def linear(x, w, b): # vectorized model return np.dot(x, w.T) + bdef loss(yhat, y): # numpy broadcast numpy广播方法 return np.mean( (yhat - y) ** 2)def partial_w(x, y, yhat): return np.array([2 * np.mean((yhat - y) * x[0]), 2 * np.mean((yhat - y) * x[1])])def partial_b(x, y, yhat): return 2 * np.mean((yhat - y))def optimize(w, b, x, y, yhat, pw, pb, learning_rate): w = w + -1 * pw(x, y, yhat) * learning_rate b = b + -1 * pb(x, y, yhat) * learning_rate return w, b def train(model_to_be_train, target, loss, pw, pb): w = np.random.random_sample((1, 2)) # w normal b = np.random.random() # 0 深度学习的时候会和大家详细解释 learning_rate = 1e-5 epoch = 200 losses = [] for i in range(epoch): batch_loss = [] for batch in range(len(rm)): # batch training index = random.choice(range(len(rm))) rm_x, lstat_x = rm[index], lstat[index] x = np.array([rm_x, lstat_x]) y = target[index] yhat = model_to_be_train(x, w, b) loss_v = loss(yhat, y) batch_loss.append(loss_v) w, b = optimize(w, b, x, y, yhat, pw, pb, learning_rate) if batch % 100 == 0: print('Epoch: {} Batch: {}, loss: {}'.format(i, batch, loss_v)) losses.append(np.mean(batch_loss)) return model_to_be_train, w, b, losses if __name__ == &quot;__main__&quot;: import matplotlib.pyplot as plt target = dataframe['price'] model, w, b, losses = train(linear, target, loss, partial_w, partial_b) plt.plot(losses) predicate = model(np.array([19, 7]), w, b) print(predicate) plt.show() &quot;&quot;&quot;Epoch: 0 Batch: 0, loss: 165.0318036522631Epoch: 0 Batch: 100, loss: 1936.2111196826459show more (open the raw output data in a text editor) ...Epoch: 199 Batch: 500, loss: 0.024829543832110872[88.74340551]&quot;&quot;&quot; Logstic Regression Linear Regression: Regression is implemented, including the definition of linear functions, why use linear functions, the meaning of loss, the meaning of gradient descent, stochastic gradient descent Use Boston house price dataset. The data set of Beijing housing prices in 2020, why didn’t I use the data set of Beijing housing prices? Boston: room size, subway, highway, crime rate have a more obvious relationship, so it is easier to observe the relationship Beijing's housing prices:! Far and near! Room Condition ==》 School District! ! ! ! =&gt; Very expensive Haidian District Harder than deep learning: 1. compiler 2. programming language &amp; automata 3. computer graphic 4. complexity system 5. computing complexity 6. operating system 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130from sklearn.datasets import load_bostonimport pandas as pdimport numpy as npdataset = load_boston()data = dataset['data']target = dataset['target']columns = dataset['feature_names']dataframe = pd.DataFrame(data)dataframe.columns = columnsdataframe['price'] = target# print(dataframe.corr()) # show the correlation of dataframe variables# correlation =&gt; If one value increases, it will cause another value to increase, and the correlation coefficient is closer to 1 if it increases in a certain proportion.# correlation =&gt; 0 means there is no relationship between the two# correlation =&gt; -1 One value increases, the other value must decrease, and the decrease is in equal proportion# sns.heatmap(dataframe.corr())# plt.show()# RM: The average number of bedrooms in the community# LSTAT: Percentage of low-income people aroundrm = dataframe['RM']lstat = dataframe['LSTAT']price = dataframe['price']greater_then_most = np.percentile(price, 66)dataframe['expensive'] = dataframe['price'].apply(lambda p: int(p&gt; greater_then_most))target = dataframe['expensive']print(dataframe[:20]) &quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX \\0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 5 0.02985 0.0 2.18 0.0 0.458 6.430 58.7 6.0622 3.0 222.0 6 0.08829 12.5 7.87 0.0 0.524 6.012 66.6 5.5605 5.0 311.0 7 0.14455 12.5 7.87 0.0 0.524 6.172 96.1 5.9505 5.0 311.0 8 0.21124 12.5 7.87 0.0 0.524 5.631 100.0 6.0821 5.0 311.0 9 0.17004 12.5 7.87 0.0 0.524 6.004 85.9 6.5921 5.0 311.0 10 0.22489 12.5 7.87 0.0 0.524 6.377 94.3 6.3467 5.0 311.0 11 0.11747 12.5 7.87 0.0 0.524 6.009 82.9 6.2267 5.0 311.0 12 0.09378 12.5 7.87 0.0 0.524 5.889 39.0 5.4509 5.0 311.0 13 0.62976 0.0 8.14 0.0 0.538 5.949 61.8 4.7075 4.0 307.0 14 0.63796 0.0 8.14 0.0 0.538 6.096 84.5 4.4619 4.0 307.0 15 0.62739 0.0 8.14 0.0 0.538 5.834 56.5 4.4986 4.0 307.0 16 1.05393 0.0 8.14 0.0 0.538 5.935 29.3 4.4986 4.0 307.0 17 0.78420 0.0 8.14 0.0 0.538 5.990 81.7 4.2579 4.0 307.0 18 0.80271 0.0 8.14 0.0 0.538 5.456 36.6 3.7965 4.0 307.0 19 0.72580 0.0 8.14 0.0 0.538 5.727 69.5 3.7965 4.0 307.0 PTRATIO B LSTAT price expensive 0 15.3 396.90 4.98 24.0 1 1 17.8 396.90 9.14 21.6 0 2 17.8 392.83 4.03 34.7 1 3 18.7 394.63 2.94 33.4 1 4 18.7 396.90 5.33 36.2 1 5 18.7 394.12 5.21 28.7 1 6 15.2 395.60 12.43 22.9 0 7 15.2 396.90 19.15 27.1 1 8 15.2 386.63 29.93 16.5 0 9 15.2 386.71 17.10 18.9 0 10 15.2 392.52 20.45 15.0 0 11 15.2 396.90 13.27 18.9 0 12 15.2 390.50 15.71 21.7 0 13 21.0 396.90 8.26 20.4 0 14 21.0 380.02 10.26 18.2 0 15 21.0 395.62 8.47 19.9 0 16 21.0 386.85 6.58 23.1 0 17 21.0 386.75 14.67 17.5 0 18 21.0 288.99 11.69 20.2 0 19 21.0 390.95 11.28 18.2 0 &quot;&quot;&quot;def sigmoid(x): return 1 / (1 + np.exp(-x))def model(x, w, b): return sigmoid(np.dot(x, w.T) + b)def loss(yhat, y): return -np.sum(y*np.log(yhat) + (1 - y)*np.log(1 - yhat))def partial_w(x, y, yhat): return np.array([np.sum((yhat - y) * x[0]), np.sum((yhat - y) * x[1])])def partial_b(x, y, yhat): return np.sum((yhat - y)) model, w, b, losses = train(model, target,loss, partial_w, partial_b)random_test_indices = np.random.choice(range(len(rm)), size=100)decision_boundary = 0.5 &quot;&quot;&quot;Epoch: 0 Batch: 0, loss: 5.380792320433632Epoch: 0 Batch: 100, loss: 4.821708458450062show more (open the raw output data in a text editor) ...Epoch: 199 Batch: 500, loss: 0.052809537616594626&quot;&quot;&quot;for i in random_test_indices: x1, x2, y = rm[i], lstat[i], target[i] predicate = model(np.array([x1, x2]), w, b) predicate_label = int(predicate &gt; decision_boundary) print('RM: {}, LSTAT: {}, EXPENSIVE: {}, Predicated: {}'.format(x1, x2, y, predicate_label)) &quot;&quot;&quot;RM: 5.701, LSTAT: 18.35, EXPENSIVE: 0, Predicated: 0RM: 4.973, LSTAT: 12.64, EXPENSIVE: 0, Predicated: 0show more (open the raw output data in a text editor) ...RM: 6.678, LSTAT: 6.27, EXPENSIVE: 1, Predicated: 1&quot;&quot;&quot; One thing left is to check the accuracy of our model! ! How to measure the quality of the model: 1. accuracy precision recall f1, f2 score AUC-ROC curve Introduce a very very important concept: -&gt; over-fitting and under-fitting (over-fitting and under-fitting) The entire machine learning process is constantly adjusting over-fitting and under-fitting!","link":"/example_03/"},{"title":"Machine Learning Part-02","text":"Data Pre-processing Feature-Extractor Split Training, Test, Validation Build Model Gradient Descent Evaluation Predicat Analysis House Price Regression 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129## load datafrom sklearn.datasets import load_boston## ususlly will load in csvdata = load_boston()print(data['DESCR'])&quot;&quot;&quot;_boston_dataset:Boston house prices dataset---------------------------**Data Set Characteristics:** :Number of Instances: 506 show more (open the raw output data in a text editor) ...Morgan Kaufmann.&quot;&quot;&quot;import pandas as pdimport numpy as npdf = pd.DataFrame(data['data'])df.columns = data['feature_names']df[df['CHAS'] == 1]&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT142 3.32105 0.0 19.58 1.0 0.8710 5.403 100.0 1.3216 5.0 403.0 14.7 396.90 26.82... 1.1296 24.0 666.0 20.2 347.88 8.88&quot;&quot;&quot;## Pre-processingdf.std()&quot;&quot;&quot;CRIM 8.601545ZN 23.322453INDUS 6.860353CHAS 0.253994NOX 0.115878RM 0.702617AGE 28.148861DIS 2.105710RAD 8.707259TAX 168.537116PTRATIO 2.164946B 91.294864LSTAT 7.141062dtype: float64&quot;&quot;&quot;df['CHAS'] = df['CHAS'].astype('int')df['CHAS'] = df['CHAS'].astype('category')df['RAD'] = df['RAD'].astype('int')df['RAD'] = df['RAD'].astype('category')df&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98...505 0.04741 0.0 11.93 0 0.573 6.030 80.8 2.5050 1 273.0 21.0 396.90 7.88506 rows × 13 columns&quot;&quot;&quot;df['RAD']&quot;&quot;&quot;0 11 22 2...505 1Name: RAD, Length: 506, dtype: categoryCategories (9, int64): [1, 2, 3, 4, ..., 6, 7, 8, 24]&quot;&quot;&quot;from sklearn.preprocessing import OneHotEncoderonehoter = OneHotEncoder()chas_and_rad_vec = onehoter.fit_transform(df[['CHAS', 'RAD']])## Standarlizefrom sklearn.preprocessing import StandardScalerss = StandardScaler()df.shape&quot;&quot;&quot;(506, 13)&quot;&quot;&quot;real_vec = ss.fit_transform(df.drop(columns = ['CHAS', 'RAD']))chas_and_rad_vec[0].toarray()&quot;&quot;&quot;array([[1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])&quot;&quot;&quot;import numpy as npnp.mean(real_vec, axis = 0)&quot;&quot;&quot;array([-1.12338772e-16, 7.89881994e-17, 2.10635198e-16, -1.96592852e-16, -1.08828186e-16, -1.47444639e-16, -8.42540793e-17, 0.00000000e+00, -4.21270397e-16, -7.44244367e-16, -3.08931624e-16])&quot;&quot;&quot;np.std(real_vec, axis = 0)&quot;&quot;&quot;array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])&quot;&quot;&quot;real_vec.shape&quot;&quot;&quot;(506, 11)&quot;&quot;&quot;chas_and_rad_vec.shape&quot;&quot;&quot;(506, 11)&quot;&quot;&quot;## Feature-ExtractorX = np.concatenate((real_vec, chas_and_rad_vec.toarray()), axis = 1)y = data['target']## Split Training, Test, Validationdef split_train_val_test(X, y, test_ratio = 0.2, val_ratio = 0.2): indices = np.random.choice(range(len(X)), size = len(X), replace=False) train_indices = indices[:int(len(X) * (1-test_ratio) * (1 - val_ratio))] val_indices = indices[int(len(X)*(1-test_ratio) * (1-val_ratio)): int(len(X) * (1-test_ratio))] test_indices = indices[int(len(X) * (1-test_ratio)):] return (X[train_indices], y[train_indices]), (X[val_indices], y[val_indices]), (X[test_indices], y[test_indices])(X_train, y_train), (X_val, y_val), (X_test, y_test) = split_train_val_test(X, y) sklearn.model_selection.train_test_split also could be used Build-Model 1234567from sklearn.linear_model import LinearRegressionregression = LinearRegression()regression.fit(X_train, y_train)&quot;&quot;&quot;LinearRegression()&quot;&quot;&quot; Question: If overfittiing or underfitting? Explain: Why validation set is more useful in deep learning Gradient Descent Evaluation 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152regression.score(X_train, y_train)&quot;&quot;&quot;0.7477980609064946&quot;&quot;&quot;regression.score(X_val, y_val)&quot;&quot;&quot;0.7611715890963341&quot;&quot;&quot;regression.score(X_test, y_test)&quot;&quot;&quot;0.711869928554872&quot;&quot;&quot;## Interpreterregression.coef_&quot;&quot;&quot;array([-1.04208922, 1.30263494, 0.29143618, -2.31827512, 2.40383155, 0.25013857, -3.55953868, -1.68823412, -2.37743843, 0.74411049, -3.79489254, -0.79143926, 0.79143926, -2.51995654, -2.20671004, 0.65594998, -0.31683083, -0.07929752, -2.15244627, -0.06686364, 1.93167854, 4.75447632])&quot;&quot;&quot;regression.intercept_&quot;&quot;&quot;22.070279554739386&quot;&quot;&quot;### PredictX_test[0]&quot;&quot;&quot;array([ 1.68404594, -0.48772236, 1.01599907, 1.07378711, 0.21279502, 1.11749449, -0.93188642, 1.53092646, 0.80657583, -3.61192313, 2.29842066, 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. ])&quot;&quot;&quot;regression.predict([X_test[0]])&quot;&quot;&quot;array([9.64589284])&quot;&quot;&quot;import matplotlib.pyplot as pltfor i in range(5): plt.scatter(X[:, 5], y) plt.scatter(X[:, 5], regression.predict(X))plt.show() 1234567891011121314151617import matplotlibmatplotlib.colors%matplotlib inlinedef show_predication_result(x, target): width = 3 fig,ax = plt.subplots(x.shape[1]//width + 1, width, figsize = (40,40)) for i in range(x.shape[1]): ix = np.unravel_index(i, ax.shape) plt.sca(ax[ix]) ax[ix].title.set_text('Feature-{}'.format(i)) plt.scatter(x[:, i], target) plt.scatter(x[:, i], regression.predict(x)) show_predication_result(X_train, y_train) 1show_predication_result(X_val, y_val) 1show_predication_result(X_test, y_test) Outliers Part-02 Logstic Regression Data Pre-processing Feature-Extractor Split Training, Test, Validation Build Model Gradient Descent Evaluation Predicat Analysis Pre-processing 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom struct import unpackdef loadmnist(imagefile, labelfile): # Open the images with gzip in read binary mode images = open(imagefile, 'rb') labels = open(labelfile, 'rb') # Get metadata for images images.read(4) # skip the magic_number number_of_images = images.read(4) number_of_images = unpack('&gt;I', number_of_images)[0] rows = images.read(4) rows = unpack('&gt;I', rows)[0] cols = images.read(4) cols = unpack('&gt;I', cols)[0] # Get metadata for labels labels.read(4) N = labels.read(4) N = unpack('&gt;I', N)[0] # Get data x = np.zeros((N, rows*cols), dtype = np.uint8) #Initialize numpy array y = np.zeros(N, dtype = np.uint8) # Initialize numpy array for i in range(N): for j in range(rows*cols): tmp_pixel = images.read(1) # Just a single byte tmp_pixel = unpack('&gt;B', tmp_pixel)[0] x[i][j] = tmp_pixel tmp_label = labels.read(1) y[i] = unpack('&gt;B', tmp_label)[0] images.close() labels.close() return (x, y) X_train, y_train = loadmnist('~/data/course_data/t10k-images-idx3-ubyte','~/data/course_data/t10k-labels-idx1-ubyte') X_test, y_test = loadmnist('~/data/course_data/train-images-idx3-ubyte','~/data/course_data/train-labels-idx1-ubyte') X_train.shape &quot;&quot;&quot; (10000, 784) &quot;&quot;&quot; X_test &quot;&quot;&quot; array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]], dtype=uint8) &quot;&quot;&quot; y_test &quot;&quot;&quot; array([5, 0, 4, ..., 5, 6, 8], dtype=uint8) &quot;&quot;&quot; plt.figure(figsize = (20, 4))for index, (image, label) in enumerate(zip(X_train[0:5], y_train[0:5])): plt.subplot(1, 5, index+1) plt.imshow(np.reshape(image, (28, 28))) plt.title('Traininng: %i\\n' % label, fontsize = 20) We only choose label with 0 and 6 1234567891011121314151617181920212223242526272829303132333435zero_train_indices = np.where(y_train == 0)one_train_indices = np.where(y_train == 6)train_indices = np.concatenate((zero_train_indices[0], one_train_indices[0]))zero_test_indices = np.where(y_test == 0)one_test_indices = np.where(y_test == 6)test_indices = np.concatenate((zero_test_indices[0], one_test_indices[0]))train_indices = np.random.choice(train_indices, size = len(train_indices), replace=False)test_indices = np.random.choice(test_indices, size = len(test_indices), replace=False)val_ratio= 0.2train_indices = train_indices[: int(len(train_indices) * (1 - val_ratio))]val_indices = train_indices[int(len(train_indices) * (1 - val_ratio)):]binary_x_train = X_train[train_indices]binary_x_test = X_test[test_indices]binary_x_val = X_train[val_indices]binary_y_train = y_train[train_indices]binary_y_test = y_test[test_indices]binary_y_val = y_train[val_indices]import randombinary_y_train&quot;&quot;&quot;array([6, 0, 0, ..., 6, 0, 0], dtype=uint8)&quot;&quot;&quot;plt.imshow(np.reshape(binary_x_train[1], (28,28)))plt.title('Training: %i\\n' % binary_y_train[1], fontsize =20)&quot;&quot;&quot;Text(0.5, 1.0, 'Training: 0\\n')&quot;&quot;&quot; 123456789101112131415from collections import CounterCounter(binary_y_train)&quot;&quot;&quot;Counter({6: 768, 0: 782})&quot;&quot;&quot;Counter(binary_y_test)&quot;&quot;&quot;Counter({6: 5918, 0: 5923})&quot;&quot;&quot;Counter(binary_y_val)&quot;&quot;&quot;Counter({0: 148, 6: 162})&quot;&quot;&quot; Build model 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from sklearn.linear_model import LogisticRegressionclf = LogisticRegression(random_state = 0, solver = 'lbfgs')# L-BFGS-B - Software for Large-scale Bound-constrained Optimizationimport warningswarnings.filterwarnings('ignore')clf.fit(binary_x_train, binary_y_train)&quot;&quot;&quot;LogisticRegression(random_state=0)&quot;&quot;&quot;clf.coef_&quot;&quot;&quot;array([[ 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,show more (open the raw output data in a text editor) ... 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])&quot;&quot;&quot;clf.intercept_&quot;&quot;&quot;array([0.00016519])&quot;&quot;&quot;clf.score&quot;&quot;&quot;&lt;bound method ClassifierMixin.score of LogisticRegression(random_state=0)&gt;&quot;&quot;&quot;clf.score(binary_x_train, binary_y_train)&quot;&quot;&quot;1.0&quot;&quot;&quot;clf.score(binary_x_val, binary_y_val)&quot;&quot;&quot;1.0&quot;&quot;&quot;binary_x_test.shape&quot;&quot;&quot;(11841, 784)&quot;&quot;&quot;binary_y_test.shape&quot;&quot;&quot;0.9865720800608057&quot;&quot;&quot;predicated_result = clf.predict(binary_x_test)np.where(binary_y_test != predicated_result)&quot;&quot;&quot;(array([ 17, 45, 66, 137, 260, 279, 323, 453, 529, 739, 753, 947, 1034, 1248, 1290, 1422, 1434, 1444, ... 10677, 10739, 10750, 10979, 11010, 11058, 11104, 11113, 11366, 11389, 11421, 11458, 11528, 11659, 11760]),)&quot;&quot;&quot;lookup_index = 1184plt.imshow(np.reshape(binary_x_test[lookup_index], (28, 28)))plt.title('Actual Value: {} ; Predict Value: {} \\n'.format(binary_y_test[lookup_index], predicated_result[lookup_index]), fontsize = 20)&quot;&quot;&quot;Text(0.5, 1.0, 'Actual Value: 6 ; Predict Value: 6 \\n')&quot;&quot;&quot; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from sklearn import metricsbinary_y_test[0]&quot;&quot;&quot;6&quot;&quot;&quot;predicated_result[0]&quot;&quot;&quot;6&quot;&quot;&quot;metrics.precision_score(binary_y_test, predicated_result, average = 'macro')&quot;&quot;&quot;0.9865879016517065&quot;&quot;&quot;metrics.precision_score(binary_y_test, predicated_result, pos_label = 6)&quot;&quot;&quot;0.9837056946077608&quot;&quot;&quot;metrics.recall_score(binary_y_test, predicated_result, pos_label = 6)&quot;&quot;&quot;0.9895234876647516&quot;&quot;&quot;fpr, tpr, threshold = metrics.roc_curve(binary_y_test, predicated_result, pos_label = 6)metrics.auc(fpr, tpr)&quot;&quot;&quot;0.9865733258009728&quot;&quot;&quot;cm = metrics.confusion_matrix(binary_y_test, predicated_result)import seaborn as snsfrom sklearn.metrics import confusion_matrixdata = confusion_matrix(binary_y_test, predicated_result)data&quot;&quot;&quot;array([[5826, 97], [ 62, 5856]])&quot;&quot;&quot;df_cm = pd.DataFrame(data, columns = np.unique(binary_y_test), index = np.unique(binary_y_test))plt.figure(figsize = (10, 7))sns.set(font_scale=1.4) # for label sizesns.heatmap(df_cm, cmap='Blues', annot=True, annot_kws = {'size': 16}) # font size&quot;&quot;&quot;&lt;AxesSubplot:&gt;&quot;&quot;&quot; 1234567df_cm.index.name = 'Actual'df_cm.columns.name = 'Predicted'plt.figure(figsize = (10, 10))sns.heatmap(df_cm, cmap='Blues', annot=True, annot_kws={'size': 16})&quot;&quot;&quot;&lt;AxesSubplot:xlabel='Predicted', ylabel='Actual'&gt;&quot;&quot;&quot; Boston code reproduction and reference answers 123456789101112131415161718192021222324252627282930313233# Import package# Used to load the Boston housing price data setfrom sklearn.datasets import load_boston# pandas toolkit For students who are new to pandas, please refer to the official 10-minute tutorial: https://pandas.pydata.org/pandas-docs/stable/10min.htmlimport pandas as pd# seaborn for drawingimport seaborn as snsimport numpy as np # numpy# Show drawing%matplotlib inlinedata = load_boston()data.keys()&quot;&quot;&quot;dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])&quot;&quot;&quot;df = pd.DataFrame(data['data'])df.head()&quot;&quot;&quot; 0 1 2 3 4 5 6 7 8 9 10 11 120 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.981 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.142 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.033 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.944 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33&quot;&quot;&quot;data['feature_names']&quot;&quot;&quot;array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='&lt;U7')&quot;&quot;&quot; Field meaning 名称 中文描述 CRIM 住房所在城镇的人均犯罪率 ZN 住房用地超过 25000 平方尺的比例 INDUS 住房所在城镇非零售商用土地的比例 CHAS 有关查理斯河的虚拟变量（如果住房位于河边则为1,否则为0 ） NOX 一氧化氮浓度 RM 每处住房的平均房间数 AGE 建于 1940 年之前的业主自住房比例 DIS 住房距离波士顿五大中心区域的加权距离 RAD 离住房最近的公路入口编号 TAX 每 10000 美元的全额财产税金额 PTRATIO 住房所在城镇的师生比例 B 1000(Bk-0.63)^2,其中 Bk 指代城镇中黑人的比例 LSTAT 弱势群体人口所占比例 MEDV 业主自住房的中位数房价（以千美元计） 1234567891011121314151617181920df.columns = data['feature_names']df.head()&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.981 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.142 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.033 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.944 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33&quot;&quot;&quot;df['price'] = data['target']df.head(2)&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT price0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.9 4.98 24.01 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.9 9.14 21.6&quot;&quot;&quot;sns.heatmap(df.corr(), annot=True, fmt='.1f') 12345import matplotlib.pyplot as pltplt.scatter(df['RM'], df['price'])&quot;&quot;&quot;&lt;matplotlib.collections.PathCollection at 0x7fe0f984f810&gt;&quot;&quot;&quot; 12345678910111213plt.figure(figsize = (20, 5))features = ['LSTAT', 'RM']target = df['price']for i, col in enumerate(features): plt.subplot(1, len(features), i+1) x = df[col] y = target plt.scatter(x, y, marker = 'o') plt.title('{} vs price'.format(col)) plt.xlabel(col) plt.ylabel('price') 1234567891011121314151617181920x = df['RM']y = df['price']history_notes = {_x: _y for _x, _y in zip(x, y)}history_notes[6.575]&quot;&quot;&quot;24.0&quot;&quot;&quot;# Find the top three prices closest to RM:6.57,similary_ys = [y for _, y in sorted(history_notes.items(), key=lambda x_y: (x_y[0]-6.57) ** 2)[:3]]similary_ys&quot;&quot;&quot;[23.8, 24.0, 24.8]&quot;&quot;&quot;np.mean(similary_ys) # Calculate the average of three&quot;&quot;&quot;24.2&quot;&quot;&quot; Using historical data to predict data that has never been seen before, the most direct method K-Neighbor-Nearst 1234567891011def knn(query_x, history, top_n=3): sorted_notes = sorted(history.items(), key=lambda x_y: (x_y[0] - query_x) ** 2) similar_notes = sorted_notes[:top_n] similar_ys = [y for _, y in similar_notes] return np.mean(similar_ys)knn(5.4, history_notes)&quot;&quot;&quot;15.700000000000001&quot;&quot;&quot; In order to obtain results faster, we hope to obtain predictive power by fitting a function \\[ f(rm) = k * rm + b \\] Random Approach \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} (\\hat{y_i} - y_i) ^ 2 \\] \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] 12345678910111213141516171819202122232425262728293031323334def loss(yhat, y): return np.mean((yhat - y) **2)import randommin_loss = float('inf')best_k, bes_b = None, Noneprint(min_loss)min_loss = float('inf')best_k, bes_b = None, Nonefor step in range(1000): min_v, max_v = -100, 100 k, b = random.randrange(min_v, max_v), random.randrange(min_v, max_v) y_hats = [k * rm_i + b for rm_i in x] current_loss = loss(y_hats, y) if current_loss &lt;min_loss: min_loss = current_loss best_k, best_b = k, b print('In step {}, we have obtained the function f(rm) = {} * rm + {}, at this time loss is: {}'.format(step, k, b, current_loss))&quot;&quot;&quot;In step 0, we have obtained the function f(rm) = 14 * rm + -78, at this time loss is: 212.87040239525695In step 70, we have obtained the function f(rm) = 10 * rm + -47, at this time loss is: 88.70654683794466In step 256, we have obtained the function f(rm) = 13 * rm + -55, at this time loss is: 68.45390542094862In step 526, we have obtained the function f(rm) = 10 * rm + -37, at this time loss is: 54.977297826086954&quot;&quot;&quot;plt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x])&quot;&quot;&quot;&lt;matplotlib.collections.PathCollection at 0x7fe0980f37d0&gt;&quot;&quot;&quot; Monte Carlo simulation Supervisor \\[ Loss(k, b) = \\frac{1}{n} \\sum_{i \\in N} ((k * rm_i + b) - y_i) ^ 2 \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{k}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i) * rm_i \\] \\[ \\frac{\\partial{loss(k, b)}}{\\partial{b}} = \\frac{2}{n}\\sum_{i \\in N}(k * rm_i + b - y_i)\\] 12345678910111213141516171819202122232425262728293031323334def partial_k(k, b, x, y): return 2 * np.mean((k*x+b-y) *x)def partial_b(k, b, x, y): return 2*np.mean(k*x+b-y) k, b = random.random(), random.random()min_loss = float('inf')best_k, best_b = None, Nonelearning_rate = 1e-2for step in range(2000): k,b = k+(-1*partial_k(k,b,x,y) * learning_rate), b+(-1*partial_b(k,b,x,y) * learning_rate) y_hats = k * x +b current_loss = loss(y_hats, y) if current_loss &lt; min_loss: min_loss = current_loss best_k, best_b = k, b print('On the {} step, we have func f(rm) = {} * rm + {}, loss is {} now'.format(step, k, b, current_loss))&quot;&quot;&quot;On the 0 step, we have func f(rm) = 6.968714597804018 * rm + -21.099847342593957, loss is 45.86961514375004 nowOn the 1 step, we have func f(rm) = 6.9692276199804555 * rm + -21.103110737199852, loss is 45.86852398135223 nowshow more (open the raw output data in a text editor) ...On the 1999 step, we have func f(rm) = 7.783005326604901 * rm + -26.279646762684518, loss is 44.468037178267025 now&quot;&quot;&quot;best_k, best_b&quot;&quot;&quot;(10, -37)&quot;&quot;&quot;plt.scatter(x, y)plt.scatter(x, [best_k * rm + best_b for rm in x]) Supervised Learning We turn the forecast of housing prices into a more responsible and sophisticated model. What should we do? \\[ f(x) = k * x + b \\] \\[ f(x) = k2 * \\sigma(k_1 * x + b_1) + b2 \\] \\[ \\sigma(x) = \\frac{1}{1 + e^(-x)} \\] 1234def sigmoid(x): return 1 / (1 + np.exp(-x))sub_x = np.linspace(-10, 10)plt.plot(sub_x, sigmoid(sub_x)) 12345678910def random_linear(x): k, b = random.random(), random.random() return k * x + bdef complex_function(x): return (random_linear(x))for _ in range(10): index = random.randrange(0, len(sub_x)) sub_x_1, sub_x_2 = sub_x[:index], sub_x[index:] new_y = np.concatenate((complex_function(sub_x_1), complex_function(sub_x_2))) plt.plot(sub_x, new_y) We can implement more complex functions through simple, basic modules and repeated superposition For more and more complex functions? How does the computer seek guidance? What is machine learning? The shortcomings of the KNN method, what is the background of the proposed linear fitting How to obtain faster function weight update through supervision method The combination of nonlinear and linear functions can fit very complex functions Deep learning we can fit more complex functions through basic function modules Assigment \\[ L2-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{(\\hat{y} - y)}^2 \\] \\[ L1-Loss(y, \\hat{y}) = \\frac{1}{n}\\sum{|(\\hat{y} - y)|} \\] Change L2-Loss in the code to L1Loss and implement gradient descent Realize L1Loss gradient descent from 0 1 Import package 12import numpy as npimport pandas as pd 2 Load data set 123456789101112131415161718192021222324252627282930313233from sklearn.datasets import load_bostonboston = load_boston()boston.keys()&quot;&quot;&quot;dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])&quot;&quot;&quot;X = boston.datay = boston.targetdf = pd.DataFrame(boston.data, columns = boston.feature_names)df.head()&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.981 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.142 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.033 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.944 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33&quot;&quot;&quot;df.describe() # Data description, you can view the statistics of each variable&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTATcount 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000mean 3.613524 11.363636 11.136779 0.069170 0.554695 6.284634 68.574901 3.795043 9.549407 408.237154 18.455534 356.674032 12.653063std 8.601545 23.322453 6.860353 0.253994 0.115878 0.702617 28.148861 2.105710 8.707259 168.537116 2.164946 91.294864 7.141062min 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 187.000000 12.600000 0.320000 1.73000025% 0.082045 0.000000 5.190000 0.000000 0.449000 5.885500 45.025000 2.100175 4.000000 279.000000 17.400000 375.377500 6.95000050% 0.256510 0.000000 9.690000 0.000000 0.538000 6.208500 77.500000 3.207450 5.000000 330.000000 19.050000 391.440000 11.36000075% 3.677083 12.500000 18.100000 0.000000 0.624000 6.623500 94.075000 5.188425 24.000000 666.000000 20.200000 396.225000 16.955000max 88.976200 100.000000 27.740000 1.000000 0.871000 8.780000 100.000000 12.126500 24.000000 711.000000 22.000000 396.900000 37.970000&quot;&quot;&quot; 3 Data preprocessing Normalization or standardization can prevent a certain dimension or a few dimensions from affecting the data too much when there are very many dimensions, and secondly, the program can run faster. There are many methods, such as standardization, min-max, z-score, p-norm, etc. How to use it depends on the characteristics of the data set. Extended reading-the deep learning field of the myth of data standardization 12345678910111213141516171819from sklearn.preprocessing import StandardScalerss = StandardScaler() # z = (x-u) / s u is the mean, s is the standard deviationX = ss.fit_transform(df) # For linear models, normalization or standardization is generally required, otherwise there will be a gradient explosion, which is generally not required for tree modelsdf = pd.DataFrame(X, columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX' ,'PTRATIO','B','LSTAT'])df.describe()&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTATcount 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02 5.060000e+02mean 2.808469e-17 6.599903e-16 -4.633974e-16 -4.353127e-16 1.404235e-16 -1.755293e-17 2.176564e-16 -1.685082e-16 -5.055245e-16 8.987102e-16 -1.067218e-15 4.493551e-16 -2.246775e-16std 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00 1.000990e+00min -4.197819e-01 -4.877224e-01 -1.557842e+00 -2.725986e-01 -1.465882e+00 -3.880249e+00 -2.335437e+00 -1.267069e+00 -9.828429e-01 -1.313990e+00 -2.707379e+00 -3.907193e+00 -1.531127e+0025% -4.109696e-01 -4.877224e-01 -8.676906e-01 -2.725986e-01 -9.130288e-01 -5.686303e-01 -8.374480e-01 -8.056878e-01 -6.379618e-01 -7.675760e-01 -4.880391e-01 2.050715e-01 -7.994200e-0150% -3.906665e-01 -4.877224e-01 -2.110985e-01 -2.725986e-01 -1.442174e-01 -1.084655e-01 3.173816e-01 -2.793234e-01 -5.230014e-01 -4.646726e-01 2.748590e-01 3.811865e-01 -1.812536e-0175% 7.396560e-03 4.877224e-02 1.015999e+00 -2.725986e-01 5.986790e-01 4.827678e-01 9.067981e-01 6.623709e-01 1.661245e+00 1.530926e+00 8.065758e-01 4.336510e-01 6.030188e-01max 9.933931e+00 3.804234e+00 2.422565e+00 3.668398e+00 2.732346e+00 3.555044e+00 1.117494e+00 3.960518e+00 1.661245e+00 1.798194e+00 1.638828e+00 4.410519e-01 3.548771e+00&quot;&quot;&quot; \\[ y=Σwixi+b \\] Because the derivation of b is all 1, add a bias b to the data and set it to 1, as a feature of the data and update the gradient wi*b=wi 1234567891011121314151617df['bias'] = 1df&quot;&quot;&quot; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT bias0 -0.419782 0.284830 -1.287909 -0.272599 -0.144217 0.413672 -0.120013 0.140214 -0.982843 -0.666608 -1.459000 0.441052 -1.075562 11 -0.417339 -0.487722 -0.593381 -0.272599 -0.740262 0.194274 0.367166 0.557160 -0.867883 -0.987329 -0.303094 0.441052 -0.492439 12 -0.417342 -0.487722 -0.593381 -0.272599 -0.740262 1.282714 -0.265812 0.557160 -0.867883 -0.987329 -0.303094 0.396427 -1.208727 13 -0.416750 -0.487722 -1.306878 -0.272599 -0.835284 1.016303 -0.809889 1.077737 -0.752922 -1.106115 0.113032 0.416163 -1.361517 14 -0.412482 -0.487722 -1.306878 -0.272599 -0.835284 1.228577 -0.511180 1.077737 -0.752922 -1.106115 0.113032 0.441052 -1.026501 1... ... ... ... ... ... ... ... ... ... ... ... ... ... ...501 -0.413229 -0.487722 0.115738 -0.272599 0.158124 0.439316 0.018673 -0.625796 -0.982843 -0.803212 1.176466 0.387217 -0.418147 1502 -0.415249 -0.487722 0.115738 -0.272599 0.158124 -0.234548 0.288933 -0.716639 -0.982843 -0.803212 1.176466 0.441052 -0.500850 1503 -0.413447 -0.487722 0.115738 -0.272599 0.158124 0.984960 0.797449 -0.773684 -0.982843 -0.803212 1.176466 0.441052 -0.983048 1504 -0.407764 -0.487722 0.115738 -0.272599 0.158124 0.725672 0.736996 -0.668437 -0.982843 -0.803212 1.176466 0.403225 -0.865302 1505 -0.415000 -0.487722 0.115738 -0.272599 0.158124 -0.362767 0.434732 -0.613246 -0.982843 -0.803212 1.176466 0.441052 -0.669058 1506 rows × 14 columns&quot;&quot;&quot; Divide the data set, where 20% of the data is used as the test set X_test, y_test, and the other 80% are used as the training set X_train, y_train, where random_state is the random seed 1234567891011from sklearn.model_selection import train_test_splitX_train, X_test, y_train,y_test = train_test_split(df, y, test_size = 0.2, random_state = 42)print('X_train.shape, y_train.shape:', X_train.shape, y_train.shape)print('X_test.shape, y_test.shape', X_test.shape, y_test.shape)&quot;&quot;&quot;X_train.shape, y_train.shape: (404, 14) (404,)X_test.shape, y_test.shape (102, 14) (102,)&quot;&quot;&quot;X_train = np.array(X_train) Model training and gradient update 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697def l1_cost(X, y, theta): &quot;&quot;&quot; X: 特征 y: 目标值 theta: 模型参数 &quot;&quot;&quot; k = X.shape[0] total_cost = 0 for i in range(k): total_cost =+ 1/k * np.abs(y[i] - theta.dot(X[i, :])) return total_cost def l2_cost(X, y, theta): k = X.shape[0] total_cost = 0 for i in range(k): total_cost += 1/k * (y[i] - theta.dot(X[i, :])) ** 2 return total_cost np.zeros(10).shape&quot;&quot;&quot;(10,)&quot;&quot;&quot;def step_l1_gradient(X, y, learning_rate, theta): &quot;&quot;&quot; Function to calculate the gradient of the MAE loss function Return the gradient value 0 for the non-differentiable point at 0 X: feature vector y: target value learing_rate: learning rate theta: parameter &quot;&quot;&quot; n = X.shape[0] print(n) e = y-X @ theta gradients = -(X.T @ np.sign(e)) / n theta = theta-learning_rate * gradients return theta def step_l2_gradient(X, y, learning_rate, theta): k = X.shape[0] x = X.shape[1] gradients = np.zeros(n) for i in range(k): for j in range(n): gradients[j] += (-2/k) * (y[i] - (theta.dot(X[i, :]))) * X[i, j] theta = theta - learning_rate * gradients return theta def step_gradient(X, y, learning_rate, theta): &quot;&quot;&quot; X: feature vector y: target value learing_rate: learning rate theta: parameter &quot;&quot;&quot; m_deriv = 0 N = len(X) for i in range(N): # Calculate the partial derivative # -x(y-(mx + b)) / |mx + b| m_deriv +=-X[i] * (y[i]-(theta*X[i] + b)) / abs(y[i]-(theta*X[i] + b)) # We subtract because the derivatives point in direction of steepest ascent theta -= (m_deriv / float(N)) * learning_rate# theta = theta-learning_rate * gradients return thetadef gradient_descent(X_train, y_train, learning_rate, iterations): k = X_train.shape[0] n = X_train.shape[1] theta = np.zeros(n) loss_values = [] print(theta.shape) for i in range(iterations): theta = step_l1_gradient(X_train, y_train, learning_rate, theta) loss = l1_cost(X_train, y_train, theta) loss_values.append(loss) print(i, 'cost:', loss) return theta, loss_values # Training parameterslearning_rate = 0.04 # Learning rateiterations = 300 # number of iterationstheta ,loss_values = gradient_descent(X_train, y_train, learning_rate, iterations)&quot;&quot;&quot;(14,)4040 cost: 0.045943991727139124041 cost: 0.045848379493882215404show more (open the raw output data in a text editor) ...299 cost: 0.017838215258874083&quot;&quot;&quot; Heart Practise 1234import pandas as pdpath = '~/data/'dataPath = path + 'heart.csv'train_data = pd.read_csv(dataPath) Field meaning 字段名 含义 age 年龄 sex 性别(1 = 男性, 0 = 女性) cp 胸部疼痛类型(值1：典型心绞痛，值2：非典型性心绞痛，值3：非心绞痛，值4：无症状） trestbps 血压 chol 胆固醇 fbs 空腹血糖（&gt; 120 mg/dl，1=真；0=假） restecg 心电图结果（0=正常，1=患有ST-T波异常，2=根据Estes的标准显示可能或确定的左心室肥大） thalach 最大心跳数 exang 运动时是否心绞痛（1=有过；0=没有） oldpeak 运动相对于休息的ST slop 心电图ST segment的倾斜度(值1:上坡，值2:平坦，值3:下坡） ca 透视检查看到的血管数 thal 缺陷种类（3=正常；6=固定缺陷；7=可逆缺陷） target 是否患病（0=否，1=是） Print a brief summary of the data set 12345678910111213141516171819202122232425262728293031train_data.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 303 entries, 0 to 302Data columns (total 14 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 303 non-null int64 1 sex 303 non-null int64 2 cp 303 non-null int64 3 trestbps 303 non-null int64 4 chol 303 non-null int64 5 fbs 303 non-null int64 6 restecg 303 non-null int64 7 thalach 303 non-null int64 8 exang 303 non-null int64 9 oldpeak 303 non-null float64 10 slope 303 non-null int64 11 ca 303 non-null int64 12 thal 303 non-null int64 13 target 303 non-null int64 dtypes: float64(1), int64(13)memory usage: 33.3 KB&quot;&quot;&quot;train_data.target.value_counts()&quot;&quot;&quot;1 1650 138Name: target, dtype: int64&quot;&quot;&quot; Change the \"sex\" column to two columns \"sex_0\" and \"sex_1\". 1sex = pd.get_dummies(train_data['sex'], prefix = &quot;sex&quot;) Add \"sex_0\" and \"sex_1\" to the data set. 1train_data = pd.concat([train_data,sex], axis = 1) And delete the sex column 1train_data = train_data.drop(columns = ['sex']) Print out the first five lines. Check whether sex_0, sex_1 are added successfully, and whether sex is deleted successfully. 123456789train_data.head()&quot;&quot;&quot; age cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target sex_0 sex_10 63 3 145 233 1 0 150 0 2.3 0 0 1 1 0 11 37 2 130 250 0 1 187 0 3.5 0 0 2 1 0 12 41 1 130 204 0 0 172 0 1.4 2 0 2 1 1 03 56 1 120 236 0 1 178 0 0.8 2 0 2 1 0 14 57 0 120 354 0 1 163 1 0.6 2 0 2 1 1 0&quot;&quot;&quot; Get sample label 12345y_data = train_data.target.valuestrain_data.shape&quot;&quot;&quot;(303, 15)&quot;&quot;&quot; Get sample feature set 12345x_data = train_data.drop(['target'],axis=1)x_data.shape&quot;&quot;&quot;(303, 14)&quot;&quot;&quot; Divide the data set Parameters: test_size=0.3, random_state=33 12from sklearn.model_selection import train_test_splitX_train,X_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=33) Normalization Import the StandardScaler package and initialize 12from sklearn.preprocessing import StandardScalerstandardScaler = StandardScaler() fit function/module is used to train model parameters 1standardScaler.fit(X_train) Standardize the training set and test set 12X_train = standardScaler.transform(X_train)X_test = standardScaler.transform(X_test) Define logistic regression model 123from sklearn.linear_model import LogisticRegression log_reg = LogisticRegression()log_reg.fit(X_train,y_train) Calculate the training set score 1234log_reg.score(X_train,y_train)&quot;&quot;&quot;0.8537735849056604&quot;&quot;&quot; Calculate the test set score 1234log_reg.score(X_test,y_test)&quot;&quot;&quot;0.8461538461538461&quot;&quot;&quot; Use the classification_report function to display a text report of the main classification indicators 12345678910111213from sklearn.metrics import classification_reporty_predict_log = log_reg.predict(X_test)print(classification_report(y_test,y_predict_log))&quot;&quot;&quot; precision recall f1-score support 0 0.93 0.78 0.85 50 1 0.78 0.93 0.84 41 accuracy 0.85 91 macro avg 0.85 0.85 0.85 91weighted avg 0.86 0.85 0.85 91&quot;&quot;&quot;","link":"/example_04/"},{"title":"Machine Learning Part-03","text":"Decision trees Machine learning basics - use decision trees to make predictions about coupons In order to get close to real life and applications, the processing of actual data sets is the main focus. From January 1, 2016 to June 30, 2016, real online and offline consumption behaviors are predicted to be used by users within 15 days after receiving coupons in July 2016. Note: In order to protect the privacy of users and businesses, all data is anonymized, and biased sampling and necessary filtering are used. Data set ccf_offline_stage1_train.csv (training data) Field Description User_id 用户ID Merchant_id 商户ID Coupon_id 优惠券ID：null表示无优惠券消费，此时Discount_rate和Date_received字段无意义 Discount_rate 优惠率：x 代表折扣率；x:y表示满x减y。单位是元 Distance user经常活动的地点离该merchant的最近门店距离是x*500米（如果是连锁店，则取最近的一家门店），x\\(\\in[0,10]\\)；null表示无此信息，0表示低于500米，10表示大于5公里； Date_received 领取优惠券日期 Date 消费日期：如果Date=null &amp; Coupon_id != null，该记录表示领取优惠券但没有使用，即负样本；如果Date!=null &amp; Coupon_id = null，则表示普通消费日期；如果Date!=null &amp; Coupon_id != null，则表示用优惠券消费日期，即正样本； 123456789101112131415161718192021222324252627282930313233343536373839404142# load pluginimport pandas as pdimport numpy as np# load datatrain_data = pd.read_csv('~/data/ccf_offline_stage1_train.csv')train_data.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 1754884 entries, 0 to 1754883Data columns (total 7 columns): # Column Dtype --- ------ ----- 0 User_id int64 1 Merchant_id int64 2 Coupon_id float64 3 Discount_rate object 4 Distance float64 5 Date_received float64 6 Date float64dtypes: float64(4), int64(2), object(1)memory usage: 93.7+ MB&quot;&quot;&quot;train_data.head()&quot;&quot;&quot; User_id Merchant_id Coupon_id Discount_rate Distance Date_received Date0 1439408 2632 NaN NaN 0.0 NaN 20160217.01 1439408 4663 11002.0 150:20 1.0 20160528.0 NaN2 1439408 2632 8591.0 20:1 0.0 20160217.0 NaN3 1439408 2632 1078.0 20:1 0.0 20160319.0 NaN4 1439408 2632 8591.0 20:1 0.0 20160613.0 NaN&quot;&quot;&quot;print(train_data.shape)data = train_data.dropna(how = 'any')print(train_data.shape)&quot;&quot;&quot;(1754884, 7)(1754884, 7)&quot;&quot;&quot; Discount_rate是object类型的，object在pandas中代表字符串，字符串类型不能输入模型中，所以需要改为数值类型 123456789101112print('Discount_rate 类型: \\n', data['Discount_rate'].unique())# [0,1] 表示折扣率# x:y 表示满 x 减 y&quot;&quot;&quot;Discount_rate 类型: ['20:1' '20:5' '30:5' '50:10' '10:5' '50:20' '100:10' '30:10' '50:5' '30:1' '100:30' '0.8' '200:30' '100:20' '10:1' '200:20' '0.95' '5:1' '100:5' '100:50' '50:1' '20:10' '150:10' '0.9' '200:50' '150:20' '150:50' '200:5' '300:30' '100:1' '200:10' '150:30' '0.85' '0.6' '0.5' '300:20' '200:100' '300:50' '150:5' '300:10' '0.75' '0.7' '30:20' '50:30']&quot;&quot;&quot; Convert Discount_rate into numerical features Discount type x:y 表示满 x 减 y 将 x:y 类型的字符串设为1 [0,1] 表示折扣率 将 [0,1] 类型的字符串设为 0 12345678910111213141516171819202122232425262728293031323334353637def getDiscountType(row): if ':' in row: return 1 else: return 0data['Discount_rate'] = data['Discount_rate'].apply(getDiscountType)&quot;&quot;&quot;See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy import sys&quot;&quot;&quot;data.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;Int64Index: 67165 entries, 6 to 1754880Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 User_id 67165 non-null int64 1 Merchant_id 67165 non-null int64 2 Coupon_id 67165 non-null float64 3 Discount_rate 67165 non-null int64 4 Distance 67165 non-null float64 5 Date_received 67165 non-null float64 6 Date 67165 non-null float64dtypes: float64(4), int64(3)memory usage: 4.1 MB&quot;&quot;&quot;# load plugin# Import DecisionTreeClassifier modelfrom sklearn.tree import DecisionTreeClassifier# Import train_test_split, used to divide the data set and test setfrom sklearn.model_selection import train_test_split# Import accuracy_score accuracy indexfrom sklearn.metrics import accuracy_score add label row to the dataset Labeling Label Label which samples are positive samples y=1 and which are negative samples y = -1 Forecast goal: the user's consumption within 15 days after receiving the coupon (Date-Date_received &lt;= 15) means to receive the coupon and use it within 15 days, that is, a positive sample, y = 1 (Date-Date_received&gt; 15) means that the coupon has not been used within 15 days, that is, a negative sample, y = 0 pandas tutorial on time https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html 1234567891011def label(row): if row['Date'] != 'null': td = pd.to_datetime(row['Date'], format = '%Y%m%d') - pd.to_datetime(row['Date_received'], format = '%Y%m%d') if td &lt;= pd.Timedelta(15, 'D'): return 1 return 0data['label'] = data.apply(label, axis = 1)&quot;&quot;&quot;See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy&quot;&quot;&quot; Statistics positive and negative samples 123456print(data['label'].value_counts())&quot;&quot;&quot;1 570600 10105Name: label, dtype: int64&quot;&quot;&quot; Divide the data set 80% training set 20% test set 80% train 20% test 1X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.2, random_state=3) Check the number and category distribution of training samples 12345678y_train.value_counts()&quot;&quot;&quot;2751537 966641735 86 ..4461556 1Name: User_id, Length: 34984, dtype: int64&quot;&quot;&quot; Check the number and type distribution of test samples 12345678y_test.value_counts()&quot;&quot;&quot;6641735 272751537 22 ..89464 1Name: User_id, Length: 11405, dtype: int64&quot;&quot;&quot; Initialize the classification decision tree model, the depth is 5 layers 1model = DecisionTreeClassifier(max_depth=6, random_state = 1) Model training 1model.fit(X_train, y_train) Model prediction 1y_pred = model.predict(X_test) Model evaluation 1234accuracy_score(y_test, y_pred)&quot;&quot;&quot;0.011315417256011316&quot;&quot;&quot; Change the standard of the model selection feature to entropy 1model = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=2) Model training 1model.fit(X_train, y_train) predict 1y_pred = model.predict(X_test) Evaluate 1accuracy_score(y_test, y_pred) In addition to the above key steps, you can explore the data by yourself, as well as any other forms of feature preprocessing methods and feature engineering processing. I hope to focus on understanding the development process of machine learning tasks. For the skills and methods of data processing, it is encouraged to invest more time to explore. iris 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifier, export_graphviz iris = load_iris()X = iris.data y = iris.targettree_clf = DecisionTreeClassifier()tree_clf.fit(X, y)export_graphviz( tree_clf, out_file=&quot;~/data/course_data/iris_tree.dot&quot;, feature_names=iris.feature_names, class_names=iris.target_names, rounded=True, filled=True)for line in open('~/data/course_data/iris_tree.dot'): print(line)&quot;&quot;&quot;digraph Tree {node [shape=box, style=&quot;filled, rounded&quot;, color=&quot;black&quot;, fontname=helvetica] ;edge [fontname=helvetica] ;0 [label=&quot;petal length (cm) &lt;= 2.45\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]\\nclass = setosa&quot;, fillcolor=&quot;#ffffff&quot;] ;1 [label=&quot;gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]\\nclass = setosa&quot;, fillcolor=&quot;#e58139&quot;] ;0 -&gt; 1 [labeldistance=2.5, labelangle=45, headlabel=&quot;True&quot;] ;2 [label=&quot;petal width (cm) &lt;= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]\\nclass = versicolor&quot;, fillcolor=&quot;#ffffff&quot;] ;0 -&gt; 2 [labeldistance=2.5, labelangle=-45, headlabel=&quot;False&quot;] ;3 [label=&quot;petal length (cm) &lt;= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]\\nclass = versicolor&quot;, fillcolor=&quot;#4de88e&quot;] ;2 -&gt; 3 ;4 [label=&quot;petal width (cm) &lt;= 1.65\\ngini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]\\nclass = versicolor&quot;, fillcolor=&quot;#3de684&quot;] ;3 -&gt; 4 ;5 [label=&quot;gini = 0.0\\nsamples = 47\\nvalue = [0, 47, 0]\\nclass = versicolor&quot;, fillcolor=&quot;#39e581&quot;] ;show more (open the raw output data in a text editor) ...16 [label=&quot;gini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]\\nclass = virginica&quot;, fillcolor=&quot;#8139e5&quot;] ;12 -&gt; 16 ;}&quot;&quot;&quot; Salient Features 1tree_clf.feature_importances_ Build Decision Tree: CART 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import pandas as pdmock_data = { 'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'], 'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'], 'family_number': [1, 1, 2, 1, 1, 1, 2], 'bought': [1, 1, 1, 0, 0, 0, 1],}dataset = pd.DataFrame.from_dict(mock_data)import numpy as npfrom collections import Counterdef entropy(elements): counter = Counter(elements) probabilities = [counter[e] / len(elements) for e in elements] return -sum(p * np.log10(p) for p in probabilities)def find_the_min_spilter(training_data: pd.DataFrame, target: str) -&gt; str: x_fields = set(training_data.columns.tolist()) - {target} spliter = None min_entropy = float('inf') for f in x_fields: elements = set(training_data[f]) for e in elements: sub_spliter_1 = training_data[dataset[f] == e][target].tolist() entropy_1 = entropy(sub_spliter_1) sub_spliter_2 = training_data[dataset[f] != e][target].tolist() entropy_2 = entropy(sub_spliter_2) entropy_v = entropy_1 + entropy_2 if entropy_v &lt; min_entropy: min_entropy = entropy_v spliter = (f, e) print('spliter is: {}'.format(spliter)) print('the min entropy is: {}'.format(min_entropy)) return spliterfind_the_min_spilter(dataset, 'bought')&quot;&quot;&quot;spliter is: ('income', '+10')the min entropy is: 0.7176797562470717('income', '+10')&quot;&quot;&quot;dataset[dataset['income'] == '-10']&quot;&quot;&quot; gender income family_number bought1 F -10 1 16 M -10 2 1&quot;&quot;&quot;dataset[dataset['income'] != '-10']&quot;&quot;&quot; gender income family_number bought0 F +10 1 12 F +10 2 13 F +10 1 04 M +10 1 05 M +10 1 0&quot;&quot;&quot;","link":"/example_05/"},{"title":"Machine Learning Part-04","text":"SVM 12345678910111213import numpy as nplabel_a = np.random.normal(6, 2, size = (50,2))label_b = np.random.normal(-6, 2, size = (50,2))import matplotlib.pyplot as plta = [1, 2, 3]b = [-1,-2, -3]plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))plt.show() 12345678910111213141516171819202122232425262728293031323334353637label_a_x = label_a[:, 0]label_b_x = label_b[:, 0]def f(x, k, b): return k*x -b k_and_b = []for i in range(100): k, b = (np.random.random(size = (1,2)) * 10 - 5)[0] if np.max(f(label_a_x, k, b)) &lt;= -1 and np.min(f(label_b_x, k, b)) &gt;= 1: print(k, b) k_and_b.append((k, b))&quot;&quot;&quot;-3.4732670434285517 -2.3248316389039325-3.654276254462583 0.01110189858052646-2.4609031871010014 -0.3932180655739925-2.9206497777762843 0.2595456609552631-4.07589152330003 -0.6463313059119606-3.1950366475236835 -1.8558958669742989-4.316670785852706 -3.1033030808371653-4.124339773909792 -1.5741734685470687-4.20817621470405 0.4368323022696625-3.7098120657624003 -0.38196175566618784-3.2053446683533315 0.12822700803583054-4.534694169094692 1.143734501297419-4.8124714209376425 0.8707258703100704&quot;&quot;&quot;plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))for k, b in k_and_b: x = np.concatenate((label_a_x, label_b_x)) plt.plot(x, f(x, k, b))plt.show() 1234567plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))k,b = sorted(k_and_b, key = lambda t: abs(t[0]))[0]x = np.concatenate((label_a_x, label_b_x))plt.plot(x, f(x, k, b))plt.show() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from sklearn.datasets import load_bostondatasets = load_boston()data, target = datasets['data'], datasets['target']import pandas as pddf = pd.DataFrame(data)df.columns = datasets['feature_names']import randomdef random_select(df, drop_num = 4): columns = random.sample(list(df.columns), k = len(df.columns) - drop_num) return df[columns] from sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeRegressorsample_x = random_select(df)regressioner = DecisionTreeRegressor()(X_train, X_test, y_train, y_test) = train_test_split(sample_x, target, test_size = 0.3)regressioner.fit(X_train, y_train)regressioner.score(X_train, y_train)&quot;&quot;&quot;1.0&quot;&quot;&quot;regressioner.score(X_test, y_test)&quot;&quot;&quot;0.8110635350395325&quot;&quot;&quot;def random_tree(train_X, train_y, test_X, test_y, drop_n = 4): train_sample = random_select(train_X, drop_num = drop_n) regressioner = DecisionTreeRegressor() regressioner.fit(train_sample, train_y) train_score = regressioner.score(train_sample, train_y) test_score = regressioner.score(test_X[train_sample.columns], test_y) print('train score = {}; test score = {}'.format(train_score, test_score)) y_predicat = regressioner.predict(test_X[train_sample.columns]) return y_predicat def random_forest(train_X, train_y, test_X, test_y, tree_n = 4): predicat = np.array([random_tree(train_X, train_y, test_X, test_y) for _ in range(tree_n)]) return np.mean(predicat, axis = 0) (X_train, X_test, y_train, y_test) = train_test_split(df, target, test_size = 0.3)forest_predict = random_forest(X_train, y_train, X_test, y_test)&quot;&quot;&quot;train score = 1.0; test score = 0.5367061884031776train score = 1.0; test score = 0.4983695562874999train score = 1.0; test score = 0.6715869370883646train score = 1.0; test score = 0.6210922529610217&quot;&quot;&quot;forest_predict&quot;&quot;&quot;array([10.925, 21.1 , 30.625, 28.025, 22.525, 17.65 , 20.6 , 17.325, 29.175, 14.95 , 40.775, 19.55 , 12.175, 23.675, 10.775, 22.1 , ... 15.575, 20.5 , 22.775, 30.725, 18.975, 16.45 , 22.05 , 18.925])&quot;&quot;&quot;from sklearn.metrics import r2_scorer2_score(y_test, forest_predict)&quot;&quot;&quot;0.7840500839091215&quot;&quot;&quot; Entropy: 熵 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npfrom collections import Counterfrom icecream import icfrom functools import lru_cachedef pr(es): counter = Counter(es) def _wrap(e): return counter[e] / len(es) return _wrapdef entropy(elements): # Information Entropy p = pr(elements) return -np.sum(p(e) * np.log(p(e)) for e in set(elements))def gini(elements): p = pr(elements) return 1-np.sum(p(e) ** 2 for e in set(elements)) pure_func = giniic(pure_func([1, 1, 1, 1, 1, 0]))ic(pure_func([1, 1, 1, 1, 1, 1]))ic(pure_func([1, 2, 3, 4, 5, 8]))ic(pure_func([1, 2, 3, 4, 5, 9]))ic(pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'c']))ic(pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'd']))&quot;&quot;&quot;ic| pure_func([1, 1, 1, 1, 1, 0]): 0.2777777777777777ic| pure_func([1, 1, 1, 1, 1, 1]): 0.0ic| pure_func([1, 2, 3, 4, 5, 8]): 0.8333333333333333ic| pure_func([1, 2, 3, 4, 5, 9]): 0.8333333333333333ic| pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'c']): 0.44897959183673464ic| pure_func(['a', 'b', 'c', 'c', 'c', 'c', 'd']): 0.61224489795918370.6122448979591837&quot;&quot;&quot; Random forest 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879from sklearn.datasets import load_bostonfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitimport numpy as npimport pandas as pdfrom sklearn.metrics import r2_scorehouse = load_boston()X = house.datay = house.targetx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)tree_reg = DecisionTreeRegressor()tree_reg.fit(x_train, y_train)print('whole dataset train acc: {}'.format(tree_reg.score(x_train, y_train)))print('whole dataset test acc: {}'.format(tree_reg.score(x_test, y_test)))&quot;&quot;&quot;whole dataset train acc: 1.0whole dataset test acc: 0.6776520888466615&quot;&quot;&quot;def random_forest(train_x, train_y, test_x, test_y, drop_n=4): random_features = np.random.choice(list(train_x.columns), size=len(train_x.columns)-drop_n) sample_x = train_x[random_features] sample_y = train_y reg = DecisionTreeRegressor() reg.fit(sample_x, sample_y) train_score = reg.score(sample_x, sample_y) test_score = reg.score(test_x[random_features], test_y) print('sub sample :: train score: {}, test score: {}'.format(train_score, test_score)) y_predicated = reg.predict(test_x[random_features]) return y_predicated, test_score with_feature_names = pd.DataFrame(X)with_feature_names.columns = house['feature_names']x_train, x_test, y_train, y_test = train_test_split(with_feature_names, y, test_size=0.3, random_state=0)tree_num = 4predicates = []for _ in range(tree_num): predicated, score = random_forest(x_train, y_train, x_test, y_test) predicates.append((predicated, score))&quot;&quot;&quot;sub sample :: train score: 1.0, test score: 0.5640870175410873sub sample :: train score: 1.0, test score: 0.29024437819534354sub sample :: train score: 1.0, test score: 0.37812117132843814sub sample :: train score: 1.0, test score: 0.5650888856735524&quot;&quot;&quot;predicates_value = [v for v, s in predicates]forest_scores = [s for v, s in predicates]print('the score of forest is : {}'.format(r2_score(y_test, np.mean(predicates_value, axis=0))))&quot;&quot;&quot;the score of forest is : 0.680193104551715&quot;&quot;&quot;weights = np.array(forest_scores) / np.sum(forest_scores)weights_score = np.zeros_like(np.mean(predicates_value, axis=0))for i, v in enumerate(predicates_value): weights_score += v * weights[i] print('the score of weighted forest is : {}'.format(r2_score(y_test, weights_score)))&quot;&quot;&quot;the score of weighted forest is : 0.6956613076019385&quot;&quot;&quot; Show SVM 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npimport matplotlib.pyplot as pltlabel_a = np.random.normal(6, 2, size=(50, 2))label_b = np.random.normal(-6, 2, size=(50, 2))plt.scatter(*zip(*label_a))plt.scatter(*zip(*label_b))label_a_x = label_a[:, 0]label_b_x = label_b[:, 0]def f(x, w, b): return w * x + b k_and_b = []for i in range(100): k, b = (np.random.random(size=(1, 2)) * 10 - 5)[0] if np.max(f(label_a_x, k, b)) &gt;= -1 and np.min(f(label_b_x, k, b)) &gt;= 1: print(k, b) k_and_b.append((k, b))&quot;&quot;&quot;0.17732109082579406 3.9508645615428843-0.8649868307954458 1.7349996177756957...-2.2969567032985783 2.171321001904926&quot;&quot;&quot;for k, b in k_and_b: x = np.concatenate((label_a_x, label_b_x)) plt.plot(x, f(x, k, b)) print(k_and_b)&quot;&quot;&quot;[(0.17732109082579406, 3.9508645615428843), (-0.8649868307954458, 1.7349996177756957), (-0.818317924604357, 0.352843348193578), (-0.19730603224472976, 4.002168852007262), ...(-2.2969567032985783, 2.171321001904926)]&quot;&quot;&quot;w, b = min(k_and_b, key=lambda k_b: k_b[0])all_x = np.concatenate((label_a_x, label_b_x))plt.plot(all_x, f(all_x, w, b), 'r-o')plt.show() Integrated learning Ensemble learning is a machine learning paradigm that solves the same problem by training multiple models. In contrast to ordinary machine learning methods that try to learn a hypothesis from training data, ensemble methods try to construct a set of hypotheses and use them in combination. Next, we will use the decision tree and its integrated version to model the classic data set Mnist and observe the differences in different integration methods. 123456789!ls!unzip mnist_test.csv.zip &amp;&amp; unzip mnist_train.csv.zipimport numpy as npimport pandas as pdfrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier Build a data set The Mnist data set used this time is not in the original format. In order to more easily adapt to this training, the 28 * 28 pictures in the original data set are flatten operation, it becomes 784 features, the columns in the DataFrame below: 1x1, 1x2, ..., 28x28, representing the i row and j column in the picture The pixel value of is a grayscale image, so the pixel value is only 0 and 1 1234567891011train_df = df = pd.read_csv('~/data/mnist_train.csv')train_df.head()&quot;&quot;&quot; label 1x1 1x2 1x3 1x4 1x5 1x6 1x7 1x8 1x9 ... 28x19 28x20 28x21 28x22 28x23 28x24 28x25 28x26 28x27 28x280 5 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 01 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 02 4 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 03 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 04 9 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 05 rows × 785 columns&quot;&quot;&quot; View training data information:, whether there is NaN, how many pieces of data are there... 1234567891011121314151617181920212223242526272829train_df.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 60000 entries, 0 to 59999Columns: 785 entries, label to 28x28dtypes: int64(785)memory usage: 359.3 MB&quot;&quot;&quot;test_df = df = pd.read_csv('~/data/mnist_test.csv')test_df.head()&quot;&quot;&quot; label 1x1 1x2 1x3 1x4 1x5 1x6 1x7 1x8 1x9 ... 28x19 28x20 28x21 28x22 28x23 28x24 28x25 28x26 28x27 28x280 7 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 01 2 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 02 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 03 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 04 4 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 05 rows × 785 columns&quot;&quot;&quot;test_df.info()&quot;&quot;&quot;&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 10000 entries, 0 to 9999Columns: 785 entries, label to 28x28dtypes: int64(785)memory usage: 59.9 MB&quot;&quot;&quot; Build training and test data 12345678910X_train = train_df.iloc[:, 1:]y_train = train_df.iloc[:, 0]X_test = test_df.iloc[:, 1:]y_test = test_df.iloc[:, 0](X_train.shape, y_train.shape), (X_test.shape, y_test.shape)&quot;&quot;&quot;(((60000, 784), (60000,)), ((10000, 784), (10000,)))&quot;&quot;&quot; Decision Tree First train a simple decision tree to see how it performs 1234567891011121314151617181920dtc = DecisionTreeClassifier()dtc.fit(X_train, y_train)dtc.score(X_train, y_train)&quot;&quot;&quot;1.0&quot;&quot;&quot;dtc.score(X_test, y_test)&quot;&quot;&quot;0.8753&quot;&quot;&quot;dtc = DecisionTreeClassifier(min_samples_leaf=8)dtc.fit(X_train, y_train)dtc.score(X_train, y_train), dtc.score(X_test, y_test)&quot;&quot;&quot;(0.9311666666666667, 0.8795)&quot;&quot;&quot; From the above results, we can see that by adjusting the parameter min_samples_leaf, the overfitting situation has been alleviated. What does this parameter mean? Why increasing it can alleviate the overfitting problem? The meaning of min_samples_leaf is the minimum number of samples contained in the leaf nodes of the decision tree. By increasing this parameter, the decision tree can not capture any of the subtle features of the training data during training, resulting in excessive training data. Fitting: The large number of samples of leaf nodes can also play a role in voting and enhance the generalization performance of the model. You can try to continue to increase the value of this parameter and try to find the best parameter. In addition to this parameter, you can also try to adjust the parameters such as min_samples_split and max_features. For the specific meaning, please refer to sklearn documentation Second question: Try to adjust other parameters to see the performance of the decision tree on the test set Random Forest Take a look at the bagging version of the decision tree and how the random forest performs! 1234567rfc = RandomForestClassifier(n_estimators = 10)rfc.fit(X_train, y_train)rfc.score(X_train, y_train), rfc.score(X_test, y_test)&quot;&quot;&quot;(0.99905, 0.9513)&quot;&quot;&quot; It is worthy of the integrated version. It basically achieves better performance under the default parameters. The accuracy of the test set is about 7% higher than that of the ordinary decision tree. However, comparing the training and test results, it can be found that there is still a certain degree of overfitting. , Try to adjust some parameters below 1234567rfc = RandomForestClassifier(n_estimators = 20)rfc.fit(X_train, y_train)rfc.score(X_train, y_train), rfc.score(X_test, y_test)&quot;&quot;&quot;(0.9999, 0.96)&quot;&quot;&quot; After increasing the parameter n_estimators, the accuracy of the test set has increased by about 1%. The meaning of this parameter is to train 20 decision trees at the same time, and finally integrate the results. The increase of this parameter can be simply regarded as voting The number of people increases, so the final result will inevitably be more robust. You can try to continue to increase this parameter, or adjust other parameters such as max_samples, appropriately less than the total amount of training data, which can increase the difference between different sub-models and further improve the generalization performance. It can also adjust the parameters of the base learner (decision tree). For the meaning of the parameters, see sklearn documentation GBDT Let's compare the performance of the boosting version of the decision tree GBDT! 1234567gbc = GradientBoostingClassifier(n_estimators=10)gbc.fit(X_train, y_train)gbc.score(X_train, y_train), gbc.score(X_test, y_test)&quot;&quot;&quot;(0.8423, 0.846)&quot;&quot;&quot; As expected, the performance has been greatly improved, and the indicators of the training set are basically the same as those of the test set, and there is no overfitting, so it should be possible to continue to try to improve this parameter. Generally, in the absence of overfitting, we only need to consider continuing to increase the complexity of the model. This is the fastest way to improve performance. When the complexity of the model increases to the point of over-fitting, we then consider using some methods to reduce over-fitting. Bagging The aforementioned random forest and GBDT are ensemble learning algorithms based on decision trees, but it should be noted that ensemble learning is not exclusive to decision trees. Any other learner can be used as a base learner for ensemble learning, such as Logistic regression, support vector machine. Bagging is short for \"bootstrap aggregating\". This is a meta-algorithm, which takes M sub-samples (with replacement) from the initial data set, and trains the prediction model on these sub-samples. The final model is obtained by averaging all sub-models, which usually produces better results. The main advantage of this technique is that it combines regularization, all you need to do is choose good parameters for the base learner. The following uses the general api provided by sklearn to construct an integrated learning algorithm 12345678# Still use decision tree as base learnerbgc = BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=1.0, n_estimators=20)bgc.fit(X_train, y_train)bgc.score(X_train, y_train), bgc.score(X_test, y_test)&quot;&quot;&quot;(0.9935166666666667, 0.9506)&quot;&quot;&quot; Third question Logistic regression as a base learner 1234567bgc = BaggingClassifier(LogisticRegression(max_iter = 500), max_samples=0.5, max_features=1.0, n_estimators=20)bgc.fit(X_train, y_train)bgc.score(X_train, y_train), bgc.score(X_test, y_test)&quot;&quot;&quot;(0.9421166666666667, 0.9228)&quot;&quot;&quot; Above we have successfully used logistic regression as the base learner to complete integrated learning. You can try to use only logistic regression for training, and compare the performance of the single model with the bagging version of logistic regression. Boosting Boosting refers to a series of algorithms that can transform a weak learner into a strong learner. The main principle of boosting is to combine a series of weak learners (only better than random guessing). For those samples that were misclassified in the early stages of training, the boosting algorithm will give more attention. Then combine the predictions by weighted majority voting (classification) or weighted sum (regression) to produce the final prediction. 123456abc = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=10, learning_rate=0.01)abc.fit(X_train, y_train)abc.score(X_train, y_train), abc.score(X_test, y_test)&quot;&quot;&quot;(1.0, 0.875)&quot;&quot;&quot; Comparing the boosting integrated version of decision tree and logistic regression, we can find that logistic regression has better generalization ability, and decision tree is easier to overfit 123456abc = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=8), n_estimators=10, learning_rate=0.01)abc.fit(X_train, y_train)abc.score(X_train, y_train), abc.score(X_test, y_test)&quot;&quot;&quot;(0.9981833333333333, 0.9532)&quot;&quot;&quot; In fact, over-fitting is not a bad thing. If your model cannot be over-fitted, it means that it cannot fit the training data well. Therefore, the decision tree is very over-fitted at the beginning, which also shows its potential. , You can see that after the above parameters are adjusted, the boosting version of the decision tree easily exceeds the boosting version of the logistic regression","link":"/example_06/"},{"title":"RNN","text":"Simple RNN Define function Import the required libraries 12345678import ioimport osimport unicodedataimport stringimport globimport torchimport random 123# alphabet small + capital letters + &quot;.,;'&quot;ALL_LETTERS = string.ascii_letters + &quot;.,;'&quot;N_LETTERS = len(ALL_LETTERS) Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427 123456def unicode_to_ascii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in ALL_LETTERS ) 123456789101112131415161718192021def load_data(): # Build the category_lines dictionary, a list of names per language category_lines = {} all_categories = [] def find_files(path): return glob.glob(path) # Read a file and split into lines def read_lines(filename): lines = io.open(filename, encoding = 'utf-8').read().strip().split('\\n') return [unicode_to_ascii(line) for line in lines] for filename in find_files('~/data/course_data/names/*.txt'): category = os.path.splitext(os.path.basename(filename))[0] all_categories.append(category) lines = read_lines(filename) category_lines[category] = lines return category_lines, all_categories To represent a single letter, we use a “one-hot vector” of size &lt;1 x n_letters&gt;. A one-hot vector is filled with 0s except for a 1 at index of the current letter, e.g. \"b\" = &lt;0 1 0 0 0 ...&gt;. To make a word we join a bunch of those into a 2D matrix &lt;line_length x 1 x n_letters&gt;. That extra 1 dimension is because PyTorch assumes everything is in batches - we’re just using a batch size of 1 here. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# Find letter index from all_letters, e.g. &quot;a&quot; = 0def letter_to_index(letter): return ALL_LETTERS.find(letter) # Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensordef letter_to_tensor(letter): tensor = torch.zeros(1, N_LETTERS) tensor[0][letter_to_index(letter)] = 1 return tensor # Turn a line into a &lt;line_length x 1 x n_letters&gt;,# or an array of one-hot letter vectorsdef line_to_tensor(line): tensor = torch.zeros(len(line), 1, N_LETTERS) for i, letter in enumerate(line): tensor[i][0][letter_to_index(letter)] = 1 return tensor def random_training_example(category_lines, all_categories): def random_choice(a): random_idx = random.randint(0, len(a) - 1) return a[random_idx] category = random_choice(all_categories) line = random_choice(category_lines[category]) category_tensor = torch.tensor([all_categories.index(category)], dtype = torch.long) line_tensor = line_to_tensor(line) return category, line, category_tensor, line_tensor if __name__ == '__main__': print(ALL_LETTERS) print(unicode_to_ascii('Ślusàrski')) category_lines, all_categories = load_data() print(category_lines['Italian'][:5]) print(letter_to_tensor('J')) # [1, 57] print(line_to_tensor('Jones').size()) # [5, 1, 57] &quot;&quot;&quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,;'Slusarski['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])torch.Size([5, 1, 56])&quot;&quot;&quot; Second Example 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# Import the required librariesimport torchimport torch.nn as nnimport matplotlib.pyplot as pltclass RNN(nn.Module): # implement RNN from scratch rather than using nn.RNN def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(input_size + hidden_size, hidden_size) self.i2o = nn.Linear(input_size + hidden_size, output_size) self.softmax = nn.LogSoftmax(dim = 1) def forward(self, input_tensor, hidden_tensor): combined = torch.cat((input_tensor, hidden_tensor), 1) hidden = self.i2h(combined) output = self.i2o(combined) output = self.softmax(output) return output, hidden def init_hidden(self): return torch.zeros(1, self.hidden_size)category_lines, all_categories = load_data()n_categories = len(all_categories)n_hidden = 128rnn = RNN(N_LETTERS, n_hidden, n_categories)# one stepinput_tensor = letter_to_tensor('A')hidden_tensor = rnn.init_hidden()output, next_hidden = rnn(input_tensor, hidden_tensor)print(output.size())print(next_hidden.size())&quot;&quot;&quot;torch.Size([1, 18])torch.Size([1, 128])&quot;&quot;&quot;# whole sequence/nameinput_tensor = line_to_tensor('Albert')hidden_tensor = rnn.init_hidden()output, next_hidden = rnn(input_tensor[0], hidden_tensor)print(output.size())print(next_hidden.size())&quot;&quot;&quot;torch.Size([1, 18])torch.Size([1, 128])&quot;&quot;&quot;def category_from_output(output): category_idx = torch.argmax(output).item() return all_categories[category_idx] print(category_from_output(output))&quot;&quot;&quot;German&quot;&quot;&quot;criterion = nn.NLLLoss()learning_rate = 0.005optimizer = torch.optim.SGD(rnn.parameters(), lr = learning_rate)def train(line_to_tensor, category_tensor): hidden = rnn.init_hidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_to_tensor[i], hidden) loss = criterion(output, category_tensor) optimizer.zero_grad() loss.backward() optimizer.step() return output, loss.item() current_loss = 0all_losses = []plot_steps, print_steps = 1000, 5000n_iters = 100000for i in range(n_iters): category, line, category_tensor, line_tensor = random_training_example(category_lines, all_categories) output, loss = train(line_tensor, category_tensor) current_loss += loss if (i + 1) % plot_steps == 0: all_losses.append(current_loss / plot_steps) current_loss = 0 if (i + 1) % print_steps == 0: guess = category_from_output(output) corrent = 'CORRECT' if guess == category else f'WRONG ({category})' print(f'{i+1} {(i+1) / n_iters *100} {loss:.4f} {line} / {guess} {corrent}') &quot;&quot;&quot;5000 5.0 2.5063 Bureau / Scottish WRONG (French)10000 10.0 1.4726 Bitar / Arabic CORRECT15000 15.0 1.9405 Bazilevitch / Russian CORRECT20000 20.0 1.5565 Dupont / French CORRECT25000 25.0 0.1202 Majewski / Polish CORRECT30000 30.0 1.1579 Kucharova / Czech CORRECT35000 35.0 1.0075 Sheng / Chinese CORRECT40000 40.0 0.8343 Masih / Arabic CORRECT45000 45.0 0.5371 Fan / Chinese CORRECT50000 50.0 0.3260 Vinh / Vietnamese CORRECT55000 55.00000000000001 2.5464 Pahlke / Polish WRONG (German)60000 60.0 1.5921 Clark / Scottish CORRECT65000 65.0 4.3648 Paulis / Greek WRONG (Dutch)70000 70.0 1.3289 Thian / Vietnamese WRONG (Chinese)75000 75.0 2.2715 Kelly / English WRONG (Irish)80000 80.0 1.0069 Siu / Korean WRONG (Chinese)85000 85.0 0.8168 Kan / Chinese CORRECT90000 90.0 0.2283 Dinh / Vietnamese CORRECT95000 95.0 2.0048 Abbascia / Japanese WRONG (Italian)100000 100.0 0.6310 O'Shea / Irish CORRECT&quot;&quot;&quot;plt.figure()plt.plot(all_losses)plt.show() 12345678910111213141516171819202122232425262728293031def predict(input_line): print(f'\\n &gt; {input_line}') with torch.no_grad(): line_tensor = line_to_tensor(input_line) hidden = rnn.init_hidden() for i in range(line_tensor.size()[0]): output, hidden = rnn(line_tensor[i], hidden) guess = category_from_output(output) print(guess) while True: sentence = input('Input: ') if sentence == 'quit': break predict(sentence)&quot;&quot;&quot; &gt; ChineseIrish &gt; EnglishEnglish &gt; JapaneseFrench &gt; FrenchGerman&quot;&quot;&quot; LSTM Modeling trigonometric functions Use LSTM to fit sine and cosine functions Use numpy to build time series data based on sine function Use keras to build a simple regression network, mainly using the LSTM network structure to fit the periodicity of the sine function, and visualize the fitted sine function image and the real function image Related knowledge points Time series data construction and forecasting Time series model building, training, evaluation and visualization based on keras LSTM 123456789101112131415161718# Import necessary libraries# Build dataimport numpy as np# Build a modelfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Inputfrom tensorflow.keras.layers import LSTMfrom tensorflow.keras.layers import Dense# Printing progress barfrom tqdm import tqdm# Visualizationimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline 1. Construct a data set This module will use numpy to construct time series data. There are two main steps: Define the sine function (cosine function) Select historical data window size to construct time series data 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def ground_func(x): &quot;&quot;&quot; sine / cosine function Args: x: numpy.ndarray return: sin(x) or cos(x) &quot;&quot;&quot; y = np.sin(x) return ydef build_data(sequence_data, n_steps): &quot;&quot;&quot; Use sine function data to build X, y Args: sine_data: numpy.ndarray n_steps: history data window size return: X: numpy.ndarray, y: numpy.ndarray &quot;&quot;&quot; # init X, y = [], [] seq_len = len(sequence_data) for start_idx in tqdm(range(seq_len), total=seq_len): end_idx = start_idx + n_steps if end_idx &gt;= seq_len: break cur_x = sequence_data[start_idx: end_idx] cur_y = sequence_data[end_idx] X.append(cur_x) y.append(cur_y) X = np.array(X) y = np.array(y) X = X.reshape(*X.shape, 1) return X, y # Construct the original sine/cosine function sequencexaxis = np.arange(-50 * np.pi, 50 * np.pi, 0.1)sequence_data = ground_func(xaxis)len(sequence_data)# Take 1000 data for visualizationplt.figure(figsize = (20, 8))plt.plot(xaxis[:1000], sequence_data[:1000]) 1234567n_steps = 20X, y = build_data(sequence_data, n_steps)X.shape, y.shape&quot;&quot;&quot; 99%|█████████▉| 3122/3142 [00:00&lt;00:00, 1557955.63it/s]((3122, 20, 1), (3122,))&quot;&quot;&quot; 2. Build the model This module builds a timing model based on the LSTM and Dense layer in keras. The following points need to be noted: 1. Choose the right hidden size 2. Choose a suitable activation function, such as relu, tanh 3. The optimizer chooses sgd, adam, etc. 3. The loss function chooses cross entropy loss function (cross_entropy) or mean square error (mse), etc. 123456789101112131415161718192021222324252627282930313233343536def create_model(): &quot;&quot;&quot; Build a LSTM model fit sine/cosine function. hints: 1. a LSTM fit time pattern (ref: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) 2. a Dense for regression (ref: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) &quot;&quot;&quot; model = Sequential() model.add(Input(shape = (20, 1))) model.add(LSTM(32, activation='tanh')) model.add(Dense(1, activation='tanh')) model.compile(optimizer = 'adam', loss = 'mse') return model# Initialize the model and print related informationmodel = create_model()model.summary()&quot;&quot;&quot;Instructions for updating:Call initializer instance with the dtype argument instead of passing it to the constructorModel: &quot;sequential&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================lstm (LSTM) (None, 32) 4352 _________________________________________________________________dense (Dense) (None, 1) 33 =================================================================Total params: 4,385Trainable params: 4,385Non-trainable params: 0_________________________________________________________________&quot;&quot;&quot; 3. Model training 12345678910111213141516# Try to change epochs and add callbacks, such as EarlyStopping (https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)history = model.fit(X, y, batch_size = 32, epochs = 25, verbose = 1)plt.plot(history.history['loss'], label='loss')plt.legend(loc ='upper right') # draw the loss image&quot;&quot;&quot;Instructions for updating:Use tf.where in 2.0, which has the same broadcast rule as np.whereEpoch 1/253122/3122 [==============================] - 4s 1ms/sample - loss: 0.1433Epoch 2/253122/3122 [==============================] - 3s 879us/sample - loss: 0.0072show more (open the raw output data in a text editor) ...Epoch 25/253122/3122 [==============================] - 3s 858us/sample - loss: 2.2191e-05&quot;&quot;&quot; 4. Forecast This module uses a function different from the training data to construct test data to verify the generalization performance of the model. The main steps are as follows: 1. Define a new function (sine/cosine) 2. Use the trained model to make predictions 3. Visually compare model prediction results with real values 123456789101112131415161718192021222324252627282930313233343536def test_func(x): &quot;&quot;&quot; sine/cosine function, different from ground_func above. Args: x: numpy.ndarray return: sin(x) or cos(x) &quot;&quot;&quot; y = np.cos(x) return y test_xaxis = np.arange(0, 10 * np.pi, 0.1)test_sequence_data = test_func(test_xaxis)# Use the initial n_steps of historical data to start forecasting, and the subsequent data will use the predicted data as historical data for further forecastingy_preds = test_sequence_data[:n_steps]# Step by step forecastfor i in tqdm(range(len(test_xaxis)-n_steps)): model_input = y_preds[i: i+n_steps] model_input = model_input.reshape((1, n_steps, 1)) y_pred = model.predict(model_input, verbose = 0) y_pred = np.append(y_preds, y_pred)plt.figure(figsize = (10,8))plt.plot(test_xaxis[n_steps:], y_preds[n_steps:], label ='predictions')plt.plot(test_xaxis, test_sequence_data, label ='ground truth')plt.plot(test_xaxis[:n_steps], y_preds[:n_steps], label ='initial sequence', color ='red')plt.legend(loc ='upper left')plt.ylim(-2,2)plt.show()&quot;&quot;&quot;100%|██████████| 295/295 [00:01&lt;00:00, 183.91it/s]&quot;&quot;&quot; Recurrent Neural Networks source 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import pandas as pd# load datatimeserise_revenue = pd.read_csv('~/data/course_data/time_serise_revenue.csv')sales_data = pd.read_csv('~/data/course_data/time_serise_sale.csv')timeserise_revenue.head()&quot;&quot;&quot; Unnamed: 0 day_1 day_2 day_3 day_4 day_5 day_6 day_7 day_8 day_9 ... day_51 day_52 day_53 day_54 day_55 day_56 day_57 day_58 day_59 day_600 0 2.622866 2.657832 2.771121 2.815845 2.876267 2.859229 2.844758 2.793797 2.736443 ... 1.228701 1.290414 1.474886 1.563295 1.736197 1.797285 1.978940 2.198979 2.277908 2.403300...4 4 1.702631 1.825995 2.038047 2.194083 2.313903 2.417883 2.567613 2.650782 2.729691 ... 1.258760 1.137150 1.109007 1.104999 1.150137 1.204513 1.221350 1.327023 1.387304 1.5573635 rows × 61 columns&quot;&quot;&quot;def sample_from_table(sample_size, dataframe): sample_row = dataframe.sample().values[0] begin_column = random.randint(0, len(sample_row) - sample_size - 1) return (sample_row[begin_column: begin_column + sample_size], sample_row[begin_column + 1: begin_column + sample_size + 1]) import torchimport torch.nn as nnfrom torch.nn import functional as Ffrom torch.autograd import Variablefrom torch import optimimport numpy as npimport math, randomimport matplotlib.pyplot as pltimport seaborn as sns# Generating a noisy multi-sin waveclass FullyConnected(nn.Module): def __init__(self, x_size, hidden_size, output_size): super(FullyConnected, self).__init__() self.hidden_size = hidden_size self.linear_with_tanh = nn.Sequential( nn.Linear(10, self.hidden_size), nn.Tanh(), nn.Linear(self.hidden_size, self.hidden_size), nn.Tanh(), nn.Linear(self.hidden_size, output_size) ) def forward(self, x): yhat = self.linear_with_tanh(x) return yhat class SimpleRNN(nn.Module): def __init__(self, x_size, hidden_size, n_layers, batch_size, output_size): super(SimpleRNN, self).__init__() self.hidden_size = hidden_size self.n_layers = n_layers self.batch_size = batch_size # self.inp = nn.Linear(1, hidden_size) self.rnn = nn.RNN(x_size, hidden_size, n_layers, batch_first=True) self.out = nn.Linear(hidden_size, output_size) # 10 in and 10 out def forward(self, inputs, hidden=None): hidden = self.__init__hidden() # print('Forward hidden {}'.format(hidden.shape)) # print('Forward inps {}'.format(inputs.shape)) output, hidden = self.rnn(inputs.float(), hidden.float()) # print('Out1 {}'.format(output.shape)) output = self.out(output.float()) # print('Forward outputs {}'.format(output.shape)) return output, hidden def __init__hidden(self): hidden = torch.zeros(self.n_layers, self.batch_size, self.hidden_size, dtype = torch.float64) return hidden # Set datasetsource_data = sales_data# Fully Connected Modeln_epochs = 100n_iters= 50hidden_size = 2 # try to change this parametersn_layers = 2batch_size = 5seq_length = 10n_sample_size = 50x_size = 1fc_model = FullyConnected(x_size, hidden_size, output_size = seq_length)fc_model = fc_model.double()criterion = nn.MSELoss()optimizer = optim.SGD(fc_model.parameters(), lr = 0.01)losses = np.zeros(n_epochs)plt.imshow(fc_model.state_dict()['linear_with_tanh.0.weight'])plt.show() 1234567891011121314151617181920212223242526272829303132333435for epoch in range(n_epochs): for iter_ in range(n_iters): _inputs, _targets = sample_from_table(n_sample_size, source_data) inputs = Variable(torch.from_numpy(np.array([_inputs[0:10], _inputs[10:20], _inputs[20:30], _inputs[30:40], _inputs[40:50]], dtype = np.double))) targets = Variable(torch.from_numpy(np.array([_targets[0:10], _targets[10:20], _targets[20:30], _targets[30:40], _targets[40:50]], dtype = np.double))) outputs = fc_model(inputs.double()) optimizer.zero_grad() loss = criterion(outputs, targets) loss.backward() optimizer.step() losses[epoch] += loss if iter_ % 10 == 0: plt.clf() plt.ion() plt.title('Epoch {}, iter {}'.format(epoch, iter_)) plt.plot(torch.flatten(outputs.detach()), 'r-', linewidth = 1, label = 'Output') plt.plot(torch.flatten(targets), 'c-', linewidth = 1, label = 'Label') plt.plot(torch.flatten(inputs), 'g-', linewidth = 1, label = 'Input') plt.draw() plt.pause(0.05) A total of 5 * 99 pictures were rendered in the middle, so I won’t show them one by one. RNN Model 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263n_epochs = 100n_iters = 50hidden_size = 2 # try to change this parametersn_layers = 2batch_size = 5seq_length = 10n_sample_size = 50x_size = 1output_size = 1rnn_model = SimpleRNN(x_size, hidden_size, n_layers, int(n_sample_size / seq_length), output_size)criterion = nn.MSELoss()optimizer = optim.SGD(rnn_model.parameters(), lr = 0.01)losses = np.zeros(n_epochs)for epoch in range(n_epochs): for iter in range(n_iters): _inputs, _targets = sample_from_table(n_sample_size, source_data) inputs = Variable(torch.from_numpy(np.array([_inputs[0:10], _inputs[10:20], _inputs[20:30], _inputs[30:40], _inputs[40:50]], dtype = np.double)).unsqueeze(2)) targets = Variable(torch.from_numpy(np.array([_targets[0:10], _targets[10:20], _targets[20:30], _targets[30:40], _targets[40:50]], dtype = np.double)).unsqueeze(2).float()) # [49] # print('Inputs {}, targets {}'.format(inputs.shape, targets.shape)) # Use teacher forcing 50% of the time # force = random.random() &lt; 0.5 outputs, hidden = rnn_model(inputs.double(), None) optimizer.zero_grad() loss = criterion(outputs, targets) loss.backward() optimizer.step() losses[epoch] += loss if iter % 10 ==0: plt.clf() plt.ion() plt.title('Epoch {}, iter {}'.format(epoch, iter)) plt.plot(torch.flatten(outputs.detach()), 'r-', linewidth = 1, label = 'Output') plt.plot(torch.flatten(targets), 'c-', linewidth = 1, label = 'Label') plt.plot(torch.flatten(inputs), 'g-', linewidth = 1, label = 'Input') plt.draw() plt.pause(0.05)# if epoch &gt; 0:# print(epoch, loss) A total of 5 * 99 pictures were rendered in the middle, so I won’t show them one by one. 12plt.plot(losses[20:])plt.show()","link":"/example_08/"},{"title":"Advanced Deep Learning","text":"Different optimer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npimport torchx = np.random.random(size=(100, 8))linear = torch.nn.Linear(in_features=8, out_features=1)sigmoid = torch.nn.Sigmoid()linear2 = torch.nn.Linear(in_features=1, out_features=1)model = torch.nn.Sequential(linear, sigmoid, linear2).double()train_x = torch.from_numpy(x)print(model(train_x).shape)yture = torch.from_numpy(np.random.uniform(0, 5, size=(100, 1)))# print(x)print(yture.shape)&quot;&quot;&quot;torch.Size([100, 1])torch.Size([100, 1])&quot;&quot;&quot;loss_fn = torch.nn.MSELoss()optimer = torch.optim.SGD(model.parameters(), lr=1e-5)for e in range(100): for b in range(100 // 1): # stochastic gradient descent # for b in range(100 // 10): # mini-batch gradient descent # for b in range(100 // 100): # batch gradient descent batch_index = np.random.choice(range(len(train_x)), size=20) yhat = model(train_x[batch_index]) loss = loss_fn(yhat, yture[batch_index]) loss.backward() print(loss) optimer.step()&quot;&quot;&quot;tensor(5.0873, dtype=torch.float64, grad_fn=&lt;MseLossBackward&gt;)tensor(3.4337, dtype=torch.float64, grad_fn=&lt;MseLossBackward&gt;)show more (open the raw output data in a text editor) ...tensor(2.1481, dtype=torch.float64, grad_fn=&lt;MseLossBackward&gt;)&quot;&quot;&quot; Matrix dimension 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from torch import nnimport torchimport numpy as npx = torch.from_numpy(np.random.random(size=(4, 10)))print(x.shape)&quot;&quot;&quot;torch.Size([4, 10])&quot;&quot;&quot;model = nn.Sequential( nn.Linear(in_features=10, out_features=5).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Sigmoid(), nn.Linear(in_features=8, out_features=8).double(), nn.Softmax())ytrue = torch.randint(8, (4, ))print(ytrue)&quot;&quot;&quot;tensor([4, 0, 7, 7])&quot;&quot;&quot;loss_fn = nn.CrossEntropyLoss()print(model(x).shape)print(ytrue.shape)loss = loss_fn(model(x), ytrue)print(torch.randint(5, (3, )))loss.backward()for p in model.parameters(): print(p, p.grad) Advanced deep learning 123456789101112131415# Basic computing libraryimport numpy as np# Deep learning libraryimport torchimport torch.nn as nnimport torch.optim as optimimport torchvisionimport torch.nn.functional as Fimport torchvision.transforms as transforms# Auxiliary drawing galleryimport matplotlib.pyplot as plt# Time operation libraryimport time# Progress bar control libraryfrom tqdm import tqdm Project 1: Forward propagation of simple neural network Question 1: Define the initial parameters and activation function You need to use numpy to implement the forward propagation process of the neural network and calculate the final output result of the output layer. In order to complete the above tasks, we need to make the following assumptions: 1. The value entered is [3,5] 1. The two weights of the hidden layer h1 are [2,4], [4,-5] 1. The two weights of the hidden layer h2 are [-1,1], [2,2] 1. The weight of the output layer is [-3,7] 1. All layers do not use bias 1. All hidden layers need to add tanh activation function 12345678910111213141516# TODO: Define a numpy array with the input data of the neural network:input_data = np.array([3, 5])# TODO: Define a numpy array with the content of the hidden layer and output layer weights of the neural network:# Tips: The weight dictionary has been built, you only need to fill in the corresponding value according to the hidden layer nameweights = {'h11': np.array([2, 4]), 'h12': np.array([4, -5]), 'h21': np.array([-1, 1]), 'h22': np.array([2, 2]), 'out': np.array([-3, 7])}# TODO: Improve the following tanh activation function:def tanh(x): return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) Question 2: Calculate the neural network output layer by layer In the calculation of the neural network, it is necessary to first multiply the weight of the layer to be calculated with its input data, and then sum, and then through the operation of the activation function, it can be output to the next layer. Below we will use the layer as the unit to perform calculations: The first is the first hidden layer. You need to multiply, sum, and input the data of the input layer and the weight of the hidden layer into the activation function. 123456789101112131415161718print(input_data * weights['h11'])a = tanh(input_data * weights['h11']).sum()b = tanh((input_data * weights['h11']).sum())print(a,b)&quot;&quot;&quot;[ 6 20]1.9999877116507956 1.0&quot;&quot;&quot;# TODO: multiply, sum, and input the data of the input layer and the weight of the first hidden layer into the activation function.hidden_11_value = tanh(input_data * weights['h11']).sum()hidden_12_value = tanh(input_data * weights['h12']).sum()hidden_1_output = np.array([hidden_11_value, hidden_12_value])&quot;&quot;&quot;1.9999877116507956-7.550282621338056e-11[ 1.99998771e+00 -7.55028262e-11]&quot;&quot;&quot; Next is the second hidden layer, the operation of this layer is exactly the same as the previous layer. 12345# TODO: multiply, sum, and input the data output by the upper layer and the weight of the second hidden layer into the activation function.hidden_21_value = tanh(hidden_1_output * weights['h21']).sum()hidden_22_value = tanh(hidden_1_output * weights['h22']).sum()hidden_2_output = np.array([hidden_21_value, hidden_22_value]) Finally, there is the output layer. At this time, there is only one node that needs to be calculated, and there is no need to add an activation function. 12# TODO: multiply and sum the data output by the upper layer and the weight of the output layeroutput = (hidden_2_output * weights['out']).sum() At this point, you have completed all the calculations. Now let's print out the output of these layers and have a look. 1234print(output)&quot;&quot;&quot;9.887385002294863&quot;&quot;&quot; Project 2: CIFAR-10 Image Classification Preparation The data set used in this project can be directly exported from the torchvision library. Here are some basic data operations (data download may take a few minutes, please be patient). 1234567891011121314151617181920212223242526272829##Define various transformation operations on the image, including converting the array to tensor, and regularizing the image#transforms.Compose is mainly used for some common graphics transformations, such as cropping and rotation#Traverse the list array and perform each transforms operation on the img in turntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.48216, 0.44653), (0.24703, 0.24349, 0.26159))))#Export the CIFAR10 data set in torchvision. The root is the directory where the data is stored after downloading. The train controls whether it is in the training phase, the download controls whether it needs to be downloaded, and the transform passes in a series of image transformations.trainset = torchvision.datasets.CIFAR10(root='~/data/course_data/', train=True, download=True, transform=transform)testset = torchvision.datasets.CIFAR10(root='~/data/course_data/', train=False, download=True, transform=transform)#Used to divide the training data into multiple groups, this function throws a group of data each time.trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)#Used to divide the test data into multiple groups, this function throws a group of data each time.testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False)&quot;&quot;&quot;Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ~/data/course_data/cifar-10-python.tar.gz170499072it [02:24, 1181561.38it/s] Extracting ~/data/course_data/cifar-10-python.tar.gz to ~/data/course_data/Files already downloaded and verified&quot;&quot;&quot; After the data download is complete, we can simply check the data label to see if it is correct with the data set in the exercise description. 12345678910111213trainset.classes&quot;&quot;&quot;['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']&quot;&quot;&quot; Let's check the data image again. 123456789101112131415161718192021222324252627282930#Display the pictures visually#Define drawing functiondef imshow(inp, title = None): &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot; # Define the canvas for drawing fig = plt.figure(figsize = (30, 30)) # Convert the dimensions of the picture inp = inp.numpy().transpose((1,2,0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) # Standardize the picture inp = std * inp + mean # The value of the entire image array is limited to the specified value a_min, and a_max inp = np.clip(inp, 0, 1) # Visual display of pictures plt.imshow(inp,)# Get a batch of datainputs, classes = next(iter(trainloader))# Display in grid format, the function is to combine several images into one imageout = torchvision.utils.make_grid(inputs)# plt.imshow() can display the picture and also display its formatimshow(out, title = [trainset.classes[x] for x in classes]) Question 1: Build a simple neural network After the data is ready, you need to build a simple neural network. 12345678910111213141516# TODO: define a layer 3 fully connected neural network, the input dimension is 32*32*3, the output dimension of the first layer is 1000, the output dimension of the second layer is 500, and the output dimension of the third layer is 10class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(32*32*3, 1000) self.fc2 = nn.Linear(1000, 500) self.fc3 = nn.Linear(500, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) return self.fc3(x)# Instantiate the neural network classnet = Net() After the model structure is defined, the loss function and optimizer need to be determined. 12345# Define loss function-cross entropycriterion = nn.CrossEntropyLoss()# Define the optimizer, pass the parameters of the neural network to the optimizer, and define the learning rateoptimizer = optim.Adam(net.parameters(), lr = 3e-4) Question 2: Neural Network Training The main content of the model has been completed, and the training can be carried out below. In the process of model training, the following steps are generally followed: Big for loop-epochs, used to manage a set of data loop training several times Small for loop-step, used to retrieve data from dataloader in batchsize unit Clear the gradient of the optimizer Read in data and label, and perform shape transformation (can be done or not) Run the forward propagation process of the model Generate the final result based on the model output Calculate the loss Calculate the gradient based on the loss Update parameters based on gradient 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# TODO: training modelnum_epochs = 10since = time.time()net.train()for epoch in range(num_epochs): print(f'Epoch {epoch + 1} / {num_epochs}') running_loss = 0.0 running_corrects = 0 # Take out each batch of data in a loop from the trainloader for data in tqdm(trainloader): # TODO: Completion code inputs, labels = data inputs = inputs.view(-1, 32 * 32 * 3) optimizer.zero_grad() outputs = net(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) loss.backward() optimizer.step() # Calculation of the loss function of a batch of data running_loss += loss.item() * inputs.size(0) # Calculation of the accuracy of a batch of data running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / trainloader.dataset.data.shape[0] epoch_acc = running_corrects.double() / trainloader.dataset.data.shape[0] print('train loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc)) print('-' * 20)time_elapsed = time.time()-sinceprint('Trainning complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed% 60))&quot;&quot;&quot;Epoch 1 / 10100%|██████████| 3125/3125 [01:04&lt;00:00, 48.74it/s]train loss: 1.6377 Acc: 0.4185--------------------Epoch 2 / 10100%|██████████| 3125/3125 [01:04&lt;00:00, 48.15it/s]train loss: 1.4254 Acc: 0.4962--------------------Epoch 3 / 10100%|██████████| 3125/3125 [01:06&lt;00:00, 47.29it/s]train loss: 1.3065 Acc: 0.5372--------------------Epoch 4 / 10100%|██████████| 3125/3125 [01:04&lt;00:00, 48.76it/s]train loss: 1.2026 Acc: 0.5729--------------------Epoch 5 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 49.98it/s]train loss: 1.1129 Acc: 0.6033--------------------Epoch 6 / 10100%|██████████| 3125/3125 [01:01&lt;00:00, 51.17it/s]train loss: 1.0252 Acc: 0.6343--------------------Epoch 7 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 49.67it/s]train loss: 0.9373 Acc: 0.6668--------------------Epoch 8 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 49.63it/s]train loss: 0.8545 Acc: 0.6936--------------------Epoch 9 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 50.02it/s]train loss: 0.7770 Acc: 0.7242--------------------Epoch 10 / 10100%|██████████| 3125/3125 [01:02&lt;00:00, 50.16it/s]train loss: 0.7020 Acc: 0.7492--------------------Trainning complete in 10m 33s&quot;&quot;&quot; Question 3: Model evaluation After completing the model training, the model needs to be evaluated to verify the accuracy of the model on the test set. Tips: In the model training log, the accuracy acc is also printed, but this is the accuracy of the model on the training set, not the accuracy on the test set. You can observe the accuracy of the training set and the accuracy of the test set to see if there is any difference. 12345678910111213141516# TODO: Complete model evaluationcorrect, total = 0, 0net.eval()for data in tqdm(testloader): inputs, labels = data inputs = inputs.view(-1, 32 * 32 * 3) outputs = net(inputs) _, predicted = torch.max(outputs, 1) total += labels.size(0) correct += (predicted == labels).sum().item()print('The testing set accuracy of the network is: %d %%'% (100 * correct / total))&quot;&quot;&quot;100%|██████████| 625/625 [00:03&lt;00:00, 157.71it/s]The testing set accuracy of the network is: 53 %&quot;&quot;&quot;","link":"/example_07/"},{"title":"CNN","text":"The source code: example_09: CNN CNN Principle 123456789101112import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom struct import unpackfrom torchvision.datasets import MNISTfrom sklearn.linear_model import LogisticRegressionimport torchfrom PIL import Imagefrom torch import nnmnist_dataset_train = MNIST(root = '~/data/course_data', train=True, download = True)mnist_dataset_test = MNIST(root = '~/data/course_data', train=False, download = True) The first machine vision problem: Let the computer automatically distinguish between 0 and 6 123456789101112X_train = mnist_dataset_train.data.numpy()y_train = mnist_dataset_train.targets.numpy()X_test = mnist_dataset_test.data.numpy()y_test = mnist_dataset_test.targets.numpy()&quot;&quot;&quot;Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gzDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ~/data/course_data/MNIST/raw/train-images-idx3-ubyte.gz9913344it [00:02, 4759648.85it/s] ...5120it [00:00, 12492633.21it/s]Extracting ~/data/course_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ~/data/course_data/MNIST/raw&quot;&quot;&quot; Explain CNN principles 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140def conv(image, filter_): # Convolution operation print(image.shape) print(filter_.shape) assert image.shape[-1] == filter_.shape[-1] test_image = image height, width = filter_.shape[0], filter_.shape[1] filter_result = np.zeros(( test_image.shape[0]-height + 1, test_image.shape[1]-width + 1 )) for h in range(test_image.shape[0]-height + 1): for w in range(test_image.shape[1]-width + 1): sub_windows = test_image[h: h + height, w: w + width, :] op = np.sum(np.multiply(sub_windows, filter_)) filter_result[h][w] = op return filter_result# Part 2: Strides&quot;&quot;&quot;Try to modify stride in Conv Function&quot;&quot;&quot;# Part3: Pooling&quot;&quot;&quot;Create a pooling cell for conv&quot;&quot;&quot;# Part4: Volume&quot;&quot;&quot;Create 3-d volume filter&quot;&quot;&quot;# Part5: Fully Connected Layers&quot;&quot;&quot;Create Fully Connected Layer, to flatten&quot;&quot;&quot;# Part6: Cross-Entropy&quot;&quot;&quot;Create Cross-Entropy cell to get loss value&quot;&quot;&quot;# Part7: ResNet&quot;&quot;&quot;Why we need resNet, and its functions&quot;&quot;&quot;class ResBlock(nn.Module): &quot;&quot;&quot; A very basic ResNet unit The unit passed: batch normal The output value retains the original input value, so that our result does not dissipate &quot;&quot;&quot; def __init__(self, n_channel): super(ResBlock, self).__init__() self.conv = nn.Conv2d(n_channel, n_channel, kernel_size = 3, padding=1, bias = False) self.bath_norm = nn.BatchNorm2d(num_features = n_channel) torch.nn.init.constant_(self.bath_norw.weight, 0.5) torch.nn.init.zeros_(self.bath_norm.bias) torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity ='relu') # sum(windows * filter) ==&gt; The larger the windows, the larger the added value, the smaller the windows, the smaller the value def forward(self, x): out = self.conv(x) out = self.conv(out) out = self.bath_norm(out) out = torch.relu(out) return out + x if __name__ == '__main__': image = Image.open('~/data/course_data/doo.jpeg') image_array = np.array(image) plt.imshow(image_array) # Robert 算子 rebert_1_kernel = np.array([ [1, 0], [0, -1] ]) robert_2_kernel = np.array([ [0, 1], [-1, 0] ]) #Sobel 算子 sobel_x_kernel = np.array([ [-1, 0, 1], [-2, 0, 2], [-1, 0, 1] ]) sobel_y_kernel = np.array([ [-1, -2, -1], [0, 0, 0], [1, 2, 1] ]) # Laplacian 算子 laplacian_kernel = np.array([ [0, 1, 0], [1, -4, 1], [0, 1, 0] ]) filters = [ np.array([sobel_x_kernel] * 3), np.array([sobel_y_kernel] * 3), np.array([laplacian_kernel] * 3) ] for i, f in enumerate(filters): print('applying filter: {}'.format(i)) plt.subplot(3, 3, i * 3 + 1) plt.imshow(image_array) filter_result = conv(image_array, f) plt.subplot(3, 3, i * 3 + 2) plt.imshow(filter_result) plt.subplot(3, 3, i * 3 + 3) plt.imshow(filter_result, cmap = 'gray')plt.show()#ResNet&quot;&quot;&quot;applying filter: 0(1931, 1931, 3)(3, 3, 3)applying filter: 1(1931, 1931, 3)(3, 3, 3)applying filter: 2(1931, 1931, 3)(3, 3, 3)&quot;&quot;&quot; Identification codes Train a model to classify and recognize the characters in the verification code, and finally complete the verification code recognition The data set used contains a total of 36 characters from 0-9 and AZ. There are 50 pictures for each character in the training set, and 10 pictures for each character in the verification set. The verification code data set is composed of 4 character pictures taken out randomly. become. Related knowledge points Data Reading Use torch to build, train, and verify models Model prediction and image segmentation analyze Question 1-Establish a character comparison table We can reverse each pair of keys and values by traversing the dictionary and store them in a new dictionary. The sample code is as follows: 1new_dict = {v: k for k, v in old_dict.items()} #### Question 2-Define datasets and dataloader In opencv-python, you can use image = cv2.medianBlur(image, kernel_size) for median filtering. #### Question 3-Define the network structure In torch, the convolution and fully connected layers are defined as follows: 12conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)fc = nn.Linear(in_features, out_features, bias) #### Question 4-Define the model training function The model training process of the torch framework includes operations such as clearing the gradient, forward propagation, calculating the loss, calculating the gradient, and updating the weight, among which: 1. Clear the gradient: the purpose is to eliminate the interference between step and step, that is, use only one batch of data loss to calculate the gradient and update the weight each time. Generally can be placed first or last; 1. Forward propagation: use a batch of data to run the process of forward propagation to generate model output results; 1. Calculate the loss: use the defined loss function, model output results and label to calculate the loss value of a single batch; 1. Calculate the gradient: According to the loss value, calculate the gradient value required in this optimization in the ownership of the model; 1. Update weight: Use the calculated gradient value to update the value of all weights. The sample code of a single process is as follows: 12345&gt;&gt;&gt; optimizer.zero_grad() # Clear the gradient (can also be placed in the last line)&gt;&gt;&gt; output = model(data) # forward propagation&gt;&gt;&gt; loss = loss_fn(output, target) # Calculate loss&gt;&gt;&gt; loss.backward() # Calculate the gradient&gt;&gt;&gt; optimizer.step() # update weight Programming Import the library to be used in this project 123456789101112131415import osimport cv2import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.utils.data import Dataset, DataLoaderimport torchvisionimport torchvision.transforms as transformsimport numpy as npimport pickleimport PILimport matplotlib.pyplot as pltfrom PIL import Imageos.environ['KMP_DUPLICATE_LIB_OK'] = 'True' Understanding the data set Define the data path 123train_data_dir = '~/data/course_data/train_data.bin'val_data_dir = '~/data/course_data/val_data.bin'verification_code_dir = '~/data/course_data/verification_code_data.bin' The data set used is stored in a binary file, and we need to define a function to read the picture in the binary file. 1234def load_file(file_name): with open(file_name, mode ='rb') as f: result = pickle.load(f) return result See what the data set looks like: 123456789101112train_data = load_file(train_data_dir)img_test = list()for i in range(1, 1800, 50): img_test.append(train_data[i][1])plt.figure()for i in range(1, 37): plt.subplot(6, 6, i) plt.imshow(img_test[i-1]) plt.xticks([]) plt.yticks([])plt.show() View single big picture 12345# plt.subplot(6, 6, i)plt.imshow(train_data[500][1])plt.xticks([])plt.yticks([])plt.show() It can be seen that there is a lot of noise in the character picture, and the noise will have an adverse effect on the model prediction result, so we can use a specific filter to eliminate the picture noise during data preprocessing. Question 1-Establish a character comparison table A simple observation shows that there are no duplicates in the key and value in the character dictionary just defined. Therefore, the key and value in the dictionary can be reversed so that we can use the value to find the key (convert the model prediction result into a readable character) Now you need to complete the following code to reverse the keys and values in the dictionary (for example: dict={'A':10,'B':11} and get new_dict={10:'A ',11:'B'} 12345char_dict = {'0':0,'1':1,'2':2,'3':3,'4':4,'5':5,'6':6,'7':7,'8':8,'9':9,\\ 'A':10,'B':11,'C':12,'D':13,'E':14,'F':15,'G':16,'H':17,'I':18,'J':19,'K':20,'L':21,'M':22,\\ 'N':23,'O':24,'P':25,'Q':26,'R':27,'S':28,'T':29,'U':30,'V':31,'W':32,'X':33,'Y':34,'Z':35 }new_char_dict = {v : k for k, v in char_dict.items()} Question 2-Define datasets and dataloader We need to use torch.utils.data.Dataset as the parent class to define our own datasets in order to standardize our own datasets. 123456789101112131415class iDataset(Dataset): def __init__(self, file_name, transforms): self.file_name = file_name # file name self.image_label_arr = load_file(self.file_name) # read binary file self.transforms = transforms # Image converter def __getitem__(self, index): label, img = self.image_label_arr[index] img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) # Convert the picture to grayscale img = cv2.medianBlur(img, 5) # Use median blur to remove image noise img = self.transforms(img) # Transform the image return img, char_dict[label[0]] def __len__(self): return len(self.image_label_arr) Now we can define transform and dataloader. 12345678910transform = transforms.Compose([transforms.ToPILImage(), transforms.Resize([28, 28]), # Adjust the image size to 28*28 transforms.ToTensor(), # Convert the picture to tensor transforms.Normalize(mean = [0.5], std = [0.5])]) # Perform normalization processingtrain_datasets = iDataset(train_data_dir, transform)train_loader = DataLoader(dataset=train_datasets, batch_size=32, shuffle = True)val_datasets = iDataset(val_data_dir, transform)val_loader = DataLoader(dataset=val_datasets, batch_size = 32, shuffle = True) Question 3-Define the network structure After the data is ready, we need to define a simple convolutional neural network. The input of the neural network is [batchsize,chanel(1),w(28),h(28)], and the output is 36 categories. Our neural network will use 2 convolutional layers with 2 fully connected layers. The parameter settings of these four layers are shown in the following table (the default parameters can be used directly if they are not marked): 1. conv1: in_chanel=1, out_chanel=10, kernel_size=5 1. conv2: in_chanel=10, out_chanel=20, kernel_size=3 1. fc1: in_feature=2000, out_feature=500 4. fc2: in_feature=500, out_feature=36 1234567891011121314151617181920212223class ConvNet(nn.Module): def __init__(self): super().__init__() # TODO: self.conv1 = nn.Conv2d(1, 10, 5) self.conv2 = nn.Conv2d(10, 20, 3) self.fc1 = nn.Linear(20 * 10 * 10, 500) self.fc2 = nn.Linear(500, 36) def forward(self, x): # inputsize: [b, 1, 28, 28] in_size = x.size(0) # b out = self.conv1(x) out = F.relu(out) out = F.max_pool2d(out, 2, 2) out = self.conv2(out) out = F.relu(out) out = out.view(in_size, -1) out = self.fc1(out) out = F.relu(out) out = self.fc2(out) out = F.log_softmax(out, dim = 1) return out Question 4-Define the model training function Next, we need to complete the model training function to achieve the following operations: 1. Clear the gradient 1. Forward propagation 1. Calculate the gradient 1. Update weights 12345678910def train(model, train_loader, optimizer, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if(batch_idx + 1) % 10 == 0: print('Train Epoch: {} [{} / {} ({:.0f} %)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) Define model test function 123456789101112def test(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: output = model(data) test_loss += F.nll_loss(output, target, reduction = 'sum') pred = output.max(1, keepdim = True)[1] correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%) \\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) Define model and optimizer We define the model structure we just built as model and choose to use the Adam optimizer. 12model = ConvNet()optimizer = optim.Adam(model.parameters()) Model training and testing We can first set the number of epochs to 3 and perform model training to see how accurate the model is and whether it meets the requirements of verification code recognition. If the model accuracy is not enough, you can also try to adjust the number of epochs and retrain. 123456789101112131415161718192021222324252627282930EPOCHS = 3for epoch in range(1, EPOCHS + 1): train(model, train_loader, optimizer, epoch) test(model, val_loader) &quot;&quot;&quot;Train Epoch: 1 [288 / 1800 (16 %)] Loss: 3.340514Train Epoch: 1 [608 / 1800 (33 %)] Loss: 2.872326Train Epoch: 1 [928 / 1800 (51 %)] Loss: 1.977929Train Epoch: 1 [1248 / 1800 (68 %)] Loss: 1.098688Train Epoch: 1 [1568 / 1800 (86 %)] Loss: 0.535660Test set: Average loss: 0.2888, Accuracy: 328/360 (91%) Train Epoch: 2 [288 / 1800 (16 %)] Loss: 0.072813Train Epoch: 2 [608 / 1800 (33 %)] Loss: 0.139866Train Epoch: 2 [928 / 1800 (51 %)] Loss: 0.109487Train Epoch: 2 [1248 / 1800 (68 %)] Loss: 0.058259Train Epoch: 2 [1568 / 1800 (86 %)] Loss: 0.013144Test set: Average loss: 0.0099, Accuracy: 360/360 (100%) Train Epoch: 3 [288 / 1800 (16 %)] Loss: 0.010245Train Epoch: 3 [608 / 1800 (33 %)] Loss: 0.004797Train Epoch: 3 [928 / 1800 (51 %)] Loss: 0.002203Train Epoch: 3 [1248 / 1800 (68 %)] Loss: 0.006250Train Epoch: 3 [1568 / 1800 (86 %)] Loss: 0.005230Test set: Average loss: 0.0028, Accuracy: 360/360 (100%)&quot;&quot;&quot; Define model test function 123456789101112def test(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: output = model(data) test_loss =+ F.nll_loss(output, target, reduction = 'sum') pred = output.max(1, keepdim = True)[1] correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print('\\nTest set: Average loss: {:.4f}, Accuracy : {}/{} ({:.0f}%) \\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) Define model and optimizer We define the model structure we just built as model and choose to use the Adam optimizer. 12model = ConvNet()optimizer = optim.Adam(model.parameters()) Model training and testing We can first set the number of epochs to 3 and perform model training to see how accurate the model is and whether it meets the requirements of verification code recognition. If the model accuracy is not enough, you can also try to adjust the number of epochs and retrain. 1234567891011121314151617181920212223242526272829EPOCHS = 3for epoch in range(1, EPOCHS + 1): train(model, train_loader, optimizer, epoch) test(model, val_loader)&quot;&quot;&quot;Train Epoch: 1 [288 / 1800 (16 %)] Loss: 3.508450Train Epoch: 1 [608 / 1800 (33 %)] Loss: 3.288610Train Epoch: 1 [928 / 1800 (51 %)] Loss: 2.584805Train Epoch: 1 [1248 / 1800 (68 %)] Loss: 1.180833Train Epoch: 1 [1568 / 1800 (86 %)] Loss: 0.564084Test set: Average loss: 0.0088, Accuracy : 316/360 (88%) Train Epoch: 2 [288 / 1800 (16 %)] Loss: 0.173177Train Epoch: 2 [608 / 1800 (33 %)] Loss: 0.043262Train Epoch: 2 [928 / 1800 (51 %)] Loss: 0.054462Train Epoch: 2 [1248 / 1800 (68 %)] Loss: 0.052596Train Epoch: 2 [1568 / 1800 (86 %)] Loss: 0.013714Test set: Average loss: 0.0006, Accuracy : 360/360 (100%) Train Epoch: 3 [288 / 1800 (16 %)] Loss: 0.004590Train Epoch: 3 [608 / 1800 (33 %)] Loss: 0.007654Train Epoch: 3 [928 / 1800 (51 %)] Loss: 0.004135Train Epoch: 3 [1248 / 1800 (68 %)] Loss: 0.003140Train Epoch: 3 [1568 / 1800 (86 %)] Loss: 0.003019Test set: Average loss: 0.0001, Accuracy : 360/360 (100%) &quot;&quot;&quot; The model has been trained! Does the test set accuracy of the last epoch exceed 99%? Identification codes After successfully implementing the digital recognition, we can start the verification code recognition! First, import the verification code data set: 1verification_code_data = load_file(verification_code_dir) Let's choose a picture at random (Figure 6) to see what the verification code looks like. 1234image = verification_code_data[6]IMG = Image.fromarray(cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB))plt.imshow(IMG)plt.show() Let's take a look at what effect the median filter can have on the captcha image. 123img = cv2.medianBlur(image.copy(), 5)plt.imshow(img)plt.show() Finally, let us look at the actual results of verification code recognition: 1234567891011121314151617181920212223242526272829IMAGES = list()NUMS = list()for img in verification_code_data: IMAGES.append(img) img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) image_1 = img[:, :80] image_2 = img[:, 80:160] image_3 = img[:, 160:240] image_4 = img[:, 240:320] img_list = [image_1, image_2, image_3, image_4] nums = [] for one_img in img_list: one_img = transform(one_img) one_img = one_img.unsqueeze(0) output = model(one_img) nums.append(new_char_dict[torch.argmax(output).item()]) NUMS.append('Verification_code: '+ ''.join(nums))plt.figure(figsize = (20, 20))plt.subplots_adjust(wspace = 0.2, hspace=0.5)for i in range(1, 11): plt.subplot(5, 2, i) plt.title(NUMS[i-1], fontsize = 25, color = 'red') plt.imshow(IMAGES[i - 1]) plt.xticks([]) plt.yticks([])plt.show()","link":"/example_09/"},{"title":"Business Intelligence(BI)","text":"Use LeNet model to recognize Mnist handwritten digits 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import tensorflow as tf#print(tf.__version__)from tensorflow.keras import layersfrom tensorflow.keras.datasets import mnistfrom tensorflow.keras.layers import Conv2D, MaxPooling2Dfrom tensorflow.keras.layers import Dense, Flattenfrom tensorflow.keras.models import Sequentialimport numpy as npimport warningswarnings.filterwarnings('ignore')# Data loading#(train_x, train_y), (test_x, test_y) = mnist.load_data() #Download the data set from the Internetdata = np.load('~/data/course_data/mnist.npz') #Read data set from local#print(data.files)train_x, train_y, test_x, test_y = data['x_train'], data['y_train'], data['x_test'], data['y_test']warnings.filterwarnings('ignore')# Input data is mnist data settrain_x = train_x.reshape(train_x.shape[0], 28, 28, 1)test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)train_x = train_x / 255test_x = test_x / 255train_y = tf.keras.utils.to_categorical(train_y, 10)test_y = tf.keras.utils.to_categorical(test_y, 10)# Create sequential modelmodel = Sequential()# The first layer of convolutional layer: 6 convolution kernels, the size is 5*5, relu activation functionmodel.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))# The second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size=(2, 2)))# The third layer of convolutional layer: 16 convolution kernels, size 5*5, relu activation functionmodel.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))# The second pooling layer: maximum poolingmodel.add(MaxPooling2D(pool_size=(2, 2)))# Flatten the parameters, which is called a convolutional layer in LeNet5. In fact, this layer is a one-dimensional vector, the same as the fully connected layermodel.add(Flatten())model.add(Dense(120, activation='relu'))# Fully connected layer, the number of output nodes is 84model.add(Dense(84, activation='relu'))# The output layer uses the softmax activation function to calculate the classification probabilitymodel.add(Dense(10, activation='softmax'))# Set the loss function and optimizer configurationmodel.compile(loss=tf.keras.metrics.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])# Pass in training data for trainingmodel.fit(train_x, train_y, batch_size=128, epochs=2, verbose=1, validation_data=(test_x, test_y))# Evaluate the resultsscore = model.evaluate(test_x, test_y)print('error:%0.4lf' %score[0])print('Accuracy:', score[1])&quot;&quot;&quot;Train on 60000 samples, validate on 10000 samplesEpoch 1/260000/60000 [==============================] - 39s 643us/sample - loss: 0.3172 - acc: 0.9096 - val_loss: 0.1105 - val_acc: 0.9626Epoch 2/260000/60000 [==============================] - 39s 652us/sample - loss: 0.0892 - acc: 0.9725 - val_loss: 0.0664 - val_acc: 0.979010000/10000 [==============================] - 4s 358us/sample - loss: 0.0664 - acc: 0.9790error:0.0664Accuracy: 0.979&quot;&quot;&quot; Use LR to classify MNIST handwritten digits 12345678910111213141516171819202122232425262728293031323334from sklearn.model_selection import train_test_splitfrom sklearn import preprocessingfrom sklearn.metrics import accuracy_scorefrom sklearn.datasets import load_digitsfrom sklearn.linear_model import LogisticRegressionimport matplotlib.pyplot as plt# Download Datadigits = load_digits()data = digits.data# Data Explorationprint(data.shape)# View the first imageprint(digits.images[0])# The meaning of the numbers represented by the first imageprint(digits.target[0])# Display the first imageplt.gray()plt.title('Handwritten Digits')plt.imshow(digits.images[0])plt.show()&quot;&quot;&quot;(1797, 64)[[ 0. 0. 5. 13. 9. 1. 0. 0.] [ 0. 0. 13. 15. 10. 15. 5. 0.] [ 0. 3. 15. 2. 0. 11. 8. 0.] [ 0. 4. 12. 0. 0. 8. 8. 0.] [ 0. 5. 8. 0. 0. 9. 8. 0.] [ 0. 4. 11. 0. 1. 12. 7. 0.] [ 0. 2. 14. 5. 10. 12. 0. 0.] [ 0. 0. 6. 13. 10. 0. 0. 0.]]0&quot;&quot;&quot; 12345678910111213141516# Split the data, use 25% of the data as the test set, and the rest as the training settrain_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)# Adopt Z-Score standardizationss = preprocessing.StandardScaler()train_ss_x = ss.fit_transform(train_x)test_ss_x = ss.transform(test_x)# Create LR classifierlr = LogisticRegression()lr.fit(train_ss_x, train_y)predict_y=lr.predict(test_ss_x)print('LR accuracy rate: %0.4lf'% accuracy_score(predict_y, test_y))&quot;&quot;&quot;LR accuracy rate: 0.9644&quot;&quot;&quot;","link":"/example_12/"},{"title":"FREEDOM","text":"许很多的影片我都应该从新温习一遍。不只是因为我学的就是这个，更重要的是每次看一遍都能理解一些新的东西。 勇敢的心，1995年电影界最成功影片。先不去评论其他技术上的细节。我只是为一个名族英雄折服。freedom。 中国古话里就说过：生命诚可贵，爱情价更高，若为自由故，两者皆可拋。 不知道从什么时候开始，这句话被滥用了。人们总是以此来津津乐道自由的重要。歪曲的道理不能称之为道理。这里的自由，我根本不原意理解成为个人的自由。至少国有国法，家有家规这话我还不会去颠覆它。 任何环境总是有规矩才会成方圆，才会有乐趣。 自由，更深层的含义是民族上的。。。为本民族的自由而战，豪情万丈。所以梅尔.吉普森最后的”freedom”如此震撼人心。。 至此，我仍然相信，民族利益高于一切。一切政治上的形式主义都可以扔到一边。for my people.I will…","link":"/freedom/"},{"title":"Natural Language Processing NLP","text":"Resnet Visualize 12345678910111213141516171819202122232425262728293031323334353637383940414243import torchvisionimport torch.nn.functional as Ffrom torchvision.transforms import transformsfrom torch import nnimport torchimport matplotlib.pyplot as pltfrom icecream import icfrom PIL import Imageimport numpy as npdef visualize_model(model, input_, output): width = 8 fig, ax = plt.subplots(output[0].shape[0] // width, width, figsize=(20, 20)) for i in range(output[0].shape[0]): ix = np.unravel_index(i, ax.shape) plt.sca(ax[ix]) ax[ix].title.set_text('filter-{}'.format(i)) plt.imshow(output[0][i].detach()) plt.show()preprocess = transforms.Compose([ transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),])resnet = torchvision.models.resnet18(pretrained=True) # transfer step 1: load pretrained modelconv_model = [m for _, m in resnet.named_modules() if isinstance(m, torch.nn.Conv2d)]&quot;&quot;&quot;Downloading: &quot;https://download.pytorch.org/models/resnet18-f37072fd.pth&quot; to /Users/lilithgames/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth100%|██████████| 44.7M/44.7M [00:34&lt;00:00, 1.36MB/s]&quot;&quot;&quot;for m in conv_model: m.register_forward_hook(visualize_model)myself = preprocess(Image.open('~/data/course_data/doo.jpeg'))with torch.no_grad(): resnet(myself.unsqueeze(0)) # un-squeeze for convert myself to [ [myself] ] Only some pictures are posted here Transfer Example 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import torchvisionimport torch.nn.functional as Ffrom torchvision.transforms import transformsfrom torch import nnimport torchimport matplotlib.pyplot as pltfrom icecream import icpreprocess = transforms.Compose([ transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),])cifar_10 = torchvision.datasets.CIFAR10('~/data/course_data/', download=False, transform=preprocess)train_loader = torch.utils.data.DataLoader(cifar_10, batch_size=128, shuffle=True)resnet = torchvision.models.resnet18(pretrained=True) # transfer step 1: load pretrained modelfor param in resnet.parameters(): param.requires_grad = False # frozen weights feature_num = resnet.fc.in_featuresresnet.fc = nn.Linear(feature_num, 10) # rewrite fc classifieric(resnet(cifar_10[0][0].unsqueeze(0)))criterion = nn.CrossEntropyLoss()optimizer = torch.optim.SGD(resnet.parameters(), lr=1e-3, momentum=0.9)epochs = 2losses = []&quot;&quot;&quot;return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)ic| resnet(cifar_10[0][0].unsqueeze(0)): tensor([[-0.0763, -0.4537, 0.8168, 0.2136, -0.0465, 0.4844, -0.4026, 0.8763, -0.7048, -0.7375]], grad_fn=&lt;AddmmBackward&gt;)&quot;&quot;&quot;for epoch in range(epochs): epoch_loss = 0 for i, (images, labels) in enumerate(train_loader): ic(epoch, i) output = resnet(images) loss = criterion(output, labels) optimizer.zero_grad() loss.backward() optimizer.step() epoch_loss += loss.item() if i &gt; 0: print('Epoch: {} batch:{}, loss ==&gt; {}'.format(epoch, i, epoch_loss / i)) losses.append(epoch_loss / i)&quot;&quot;&quot;ic| epoch: 0, i: 0ic| epoch: 0, i: 1ic| epoch: 0, i: 2Epoch: 0 batch:1, loss ==&gt; 5.118020296096802ic| epoch: 0, i: 3Epoch: 0 batch:2, loss ==&gt; 3.8235710859298706ic| epoch: 0, i: 4...ic| epoch: 0, i: 203Epoch: 0 batch:202, loss ==&gt; 1.4433288293899875...&quot;&quot;&quot;plt.plot(losses)plt.show()&quot;&quot;&quot;Because the last time is too long to run, the losses are not assigned&quot;&quot;&quot; Resnet Transfer Learning 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import torchvisionimport torch.nn.functional as Fcifar_10 = torchvision.datasets.CIFAR10('~/data/course_data', download=False, transform=preprocess)train_loader = torch.utils.data.DataLoader(cifar_10, batch_size=512, shuffle=True)plt.imshow(cifar_10[10][0].permute(1, 2, 0))for param in res_net.parameters(): param.requires_grad = False# Parameters of newly constructed modules have requires_grad=True by defaultnum_ftrs = res_net.fc.in_featuresres_net.fc = nn.Linear(num_ftrs, 10) # only update this part parameters criterion = nn.CrossEntropyLoss()# Observe that only parameters of final layer are being optimized as# opposed to before.optimizer_conv = optim.SGD(res_net.fc.parameters(), lr=0.001, momentum=0.9)# Decay LR by a factor of 0.1 every 7 epochslosses = []epochs = 10for epoch in range(epochs): loss_train = 0 for i, (imgs, labels) in enumerate(train_loader): print(i) outputs = res_net(imgs) loss = criterion(outputs, labels) optimizer_conv.zero_grad() loss.backward() optimizer_conv.step() loss_train += loss.item() if i &gt; 0 and i % 10 == 0: print('Epoch: {}, batch: {}'.format(epoch, i)) print('-- loss: {}'.format(loss_train / i)) losses.append(loss_train / len(train_loader)) Show Resnet 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import cv2import numpy as npimport torchfrom torchvision import transformsfrom torchvision.models import resnet18from torchsummary import summaryimport matplotlib.pyplot as pltdef show_one_model(model, input_, output): width = 8 fig, ax = plt.subplots(output[0].shape[0] // width, width, figsize=(20, 20)) for i in range(output[0].shape[0]): ix = np.unravel_index(i, ax.shape) plt.sca(ax[ix]) ax[ix].title.set_text('Filter-{}'.format(i)) plt.imshow(output[0][i].detach()) # plt.pause(0.05) input('this is conv: {}, received a {} tensor, press any key to continue: '.format(model, input_[0].shape)) plt.show() def main(img): &quot;&quot;&quot; Forward propagation, print feature maps during the transfer process &quot;&quot;&quot; # Define device, transforms transform = transforms.Compose([transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.ToTensor(), ]) # Process pictures, define models img = transform(img).unsqueeze(0) model = resnet18(pretrained=True) # Print model summary, which can be used for convolutional layer comparison summary(model, (3, 224, 224)) for p in model.parameters(): print(p) conv_models = [m for _, m in model.named_modules() if isinstance(m, torch.nn.Conv2d)] for conv in conv_models: conv.register_forward_hook(show_one_model) with torch.no_grad(): model(img) # conv_models = [m for _, m in model.named_modules() if isinstance(m, torch.nn.Conv2d)] # # first_conv = conv_models[0] # # show_one_model(first_conv, img, output=first_conv(img)) if __name__ == '__main__': img = cv2.imread('~/data/course_data/doo.png') main(img) &quot;&quot;&quot;---------------------------------------------------------------- Layer (type) Output Shape Param #================================================================ Conv2d-1 [-1, 64, 112, 112] 9,408 BatchNorm2d-2 [-1, 64, 112, 112] 128show more (open the raw output data in a text editor) ... -2.5093e-02, 6.7847e-03, -1.7868e-02, -7.8250e-04, -6.3448e-03], requires_grad=True)&quot;&quot;&quot;","link":"/example_10/"},{"title":"G1 app2sd 完全教程","text":"声明1:你所需要的软件在这里可以下载的到! 声明2.app2sd虽然可以省却手机内存,但是也有许多不便的地方!操作后SDcard就是机子的一部分,不能随便摘取.我用的4G的卡,在机子挂在后存储有问题!不知道其他卡如何.所以在存储文件和音乐的时候还是需要用到读卡器,而这个时候我必须选择关机!直接卸载SDcard会造成机子程序出错!而不得不从新执行一遍app2sd的过程!并且执行过后也会存在一些不可知的问题!如果对稳定性比较看重的人这里可以飘过了! 声明3.我的sdcard已经在手机内通过!懒得再刷,所以没有用我的card抓图!本教程图片多为网上现成图片来完成!而图片不是一个地方抓取的!所以图片上的容量会有差距.但是刷机过程没有错 从新格盘,正好用自己的图!顺便说一下,ubuntu下的默认抓图真恶心!每抓一张都要从新启动一次程序! 所需要的准备的工作: 1.SDcard(必须) 2.分区软件(必须,windows下可以使用Acronis Disk Director Suite,支持vista.linux下可以直接利用终端分区!) 3.Android SDK(非必须,可以再网上下载Terminal Emulator.apk,安装后在手机上输入adb下的指令完成操作!) 首先我们要将SDcard分区,分成fat32和ext2,至于ext3是否可行我没有测试过,有兴趣的可以试试并且留言告诉我测试报告! 我选择的是在ubuntu的终端执行,这样操作比较靠谱.而在windows下的分区软件不是很稳定!会造成诸多不可见的错误! windows下的分区软件有Acronis Disk Director Suite以及PartitionManager,至于分区魔术师可以略过,因为它不支持分区SDcard.Acronis Disk Director Suite软件分区可以移步到此查看! 以USB内存卡方式插上电脑，或者用读卡器插上电脑 像我的ubuntu，它会自动挂载你的卡。 把东西备份好，然后卸载。一定要卸载，不然无法分区 启动ubuntu或者您的linux系统,在终端内输入如下代码: dmesg //查看所连接的设备! 可以看到sdb或者sdc之类的设备名称!假设我以下操作都为sdc设备! sudo fdisk /dev/sdc //这里需要说明,如果linux下非root,必须要输入sudo来取得root权限进行操作.以下类同! p是显示当前分区 n是创建 d是删除 w是应用你的操作 doo@ubuntu:~# sudo fdisk /dev/sdc Command (m for help): d &lt; ==删除当前分区 Command (m for help): p &lt;==显示一下，确定已删除 Disk /dev/sdc: 3965 MB, 3965714432 bytes 122 heads, 62 sectors/track, 1024 cylinders Units = cylinders of 7564 * 512 = 3872768 bytes Disk identifier: 0x9dfd42a5 Device Boot Start End Blocks Id System Command (m for help): Command (m for help): m &lt; ==查看帮助 Command action a toggle a bootable flag b edit bsd disklabel c toggle the dos compatibility flag d delete a partition l list known partition types m print this menu n add a new partition o create a new empty DOS partition table p print the partition table q quit without saving changes s create a new empty Sun disklabel t change a partition's system id u change display/entry units v verify the partition table w write table to disk and exit x extra functionality (experts only) Command (m for help): n &lt;==新建分区，选择主分区 Command action e extended p primary partition (1-4) p Partition number (1-4): 1 &lt;==指定该主分区为1号 First cylinder (1-1024, default 1): &lt;==敲回车，直接使用SD卡的最开头 Using default value 1 Last cylinder or +cylinders or +sizeK(K,M,G) (1-1024, default 1024): +3300M &lt;==填入分区的大小 Command (m for help): n &lt;==新建分区，选择扩展分区(所有逻辑分区加起来就是扩展分区) Command action e extended p primary partition (1-4) p Partition number (1-4): 2 &lt;==扩展分区的序号是2 First cylinder (895-1024, default 895): &lt;==敲回车，直接接着剩余空间的最开头 Using default value 895 Last cylinder or +cylinders or +sizeK(K,M,G) (895-1024, default 1024): &lt;==敲回车，用默认的，使用全部剩余空间 Using default value 1024 Command (m for help): Command (m for help):p Disk /dev/sdc: 3965 MB, 3965714432 bytes 122 heads, 62 sectors/track, 1024 cylinders Units = cylinders of 7564 * 512 = 3872768 bytes Device Boot Start End Blocks Id System /dev/sdc1 1 894 733792+ 83 Linux /dev/sdc2 729 1024 272128+ 83 Linux 创建好两个分区后, 我们还需要用命令t修改分区卷标, 选择分区1改卷标为c 命令为 Command (m for help):t t &lt; ==修改卷标 partition number (1-4): 1 &lt;==输入1来制定第一个分区. Hex code (type L to List codes): c &lt;==输入C来制定卷标 Changed system type of partition 1 to c (W95 FAT32 (LBA)) Command (m for help): w &lt;==将缓冲写入SD卡,应用你的操作 The partition table has been altered! Calling ioctl() to re-read partition table. WARNING: If you have created or modified any DOS 6.X partitions, please see the fdisk manual page for additional information. Syncing disks. doo@ubuntu:~# doo@ubuntu:~# sudo ls /dev/sdc* &lt; ==查看分区情况 /dev/sdc /dev/sdc1 /dev/sdc2 doo@ubuntu:~#sudo mkfs.vfat /dev/sdc1 &lt;==格式化第一个主分区。 mkfs.vfat 3.0.1 (23 Nov 2008) doo@ubuntu:~# sudo mkfs.ext2 /dev/sdc2 &lt;==格式化第二个分区 mke2fs 1.41.4 (27-Jan-2009) warning: 139 blocks unused Filesystem laber= OS type: Linux Block size=1024 (log=0) Fragment size=1024 (log=0) 123360 inodes, 491521 blocks 24583 blocks (5.00%) reserved for the super user First data block=1 Maximum filesyetem blocks=67633152 68 block groups 8192 blocks per group, 8192 fragments per group 2856 inodes per group Superblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409 Writing inode tables: done Writing superblocks and filesystem accounting information: done This filesystem will be automatically checked every 38 mounts or 180 days.whichever comes first. Use tune2fs -c or -i to override. doo@ubuntu:-$ 分区完毕后ubuntu会自动挂在两个盘符.表示成功! 然后需要手机必须为app2sd版本的rom,在windows 命令提示符下输入命令查看: 以下步骤必须安装android sdk.(其实一下步骤不一定需要在windows cmd下进行,在网上下载一个android的终端Terminal Emulator.apk,然后启动此程序在手机内输入以下指令是一样的!只是在sdcard的系统盘下建立app文件夹并挂载到android rom上! ) C:and Settings&gt;cd c:&lt; ==cd到sdk adb.exe C:&gt;adb devices &lt; ==查看连接的硬件和设备 List of devices attached 000000000000 device &lt;==分区过硬盘以后连接会显示000000000000 的硬件号 C:&gt;adb shell # su &lt; ==如果你还没有取得root权限,那么这一步通不过. su # ls /system &lt;==查看一下system目录下的文件夹 ls /system lib framework media fonts etc customize build.prop usr bin xbin app sd lost+found busybox df -h &lt; ==查看系统盘情况!如果分区成功,那么会在android的系统下显示分区.如下我的385.8M的分区在android的系统内!再往下是sdcard的系统!如果没有那表示分区失败.当然还有一种可能就是你的手机不是app2sd rom busybox df -h Filesystem Size Used Available Use% Mounted on tmpfs 48.3M 0 48.3M 0% /dev tmpfs 4.0M 12.0k 4.0M 0% /sqlite_stmt_journals /dev/block/mtdblock3 67.5M 67.5M 0 100% /system /dev/block/mtdblock5 74.8M 30.4M 44.3M 41% /data /dev/block/mtdblock4 67.5M 1.2M 66.3M 2% /cache /dev/block/mmcblk0p2 385.8M 2.0k 366.5M 0% /system/sd &lt; ==由于在ubuntu下分区后手机内读取sdcard出错,所以后便又分了一次!但是没有抓图,所以容量上和上图有差距.再者本身linux和windows读取SDcard的容量上就有不同! /dev/block//vold/179:1 3.3G 4.0k 3.3G 0% /sdcard # mkdir /system/sd/app &lt;==建立sdcard分区上的app文件夹!如果以前sdcard曾做过app2sd,那么这个文件夹是存在的!会有命令符提示文件夹存在! mkdir /system/sd/app # cd /data cd /data # cp -a app /system/sd/app cp -a app /system/sd/app # rm -r app rm -r app # ln -s /system/sd/app /data/app ln -s /system/sd/app /data/app # reboot reboot 手机自动重启后就OK了.放心安装你所想要的apk程序吧! 顺便说一句:ubuntu的9.04快要放出正式版了!欢迎大家下载试用.","link":"/g1-app2sd/"},{"title":"G1上打造Hero!(更新tips&amp;app)","text":"这本来是安卓上发布的一片帖子,我写在这里主要是为了为自己增加点浏览量!顺便解解眼馋.但是我并不打算在这里提供教程和下载! (由于flickr最近不稳定,所以图片显现不出来!)是由于博客上的flickr插件的原因.. 有兴趣的看这里吧.... 教程中需要的是C6的卡,但是我的是金士顿东京原厂8G卡.试试用一下吧!稍后发试用报告!","link":"/g1-hero-tipsapp/"},{"title":"G1 提权刷机顺序","text":"本人主要是写给在我博客中留言的@leo 以及和他一样刷机无从入手的朋友!技术是一直在更新的,老的技术文档虽然不太适用了,但是参考价值是一定有的!所以也可以翻看我之前的文档,对比本文参看! 长久不用就是会忘记哈...唉..已经很久没有从源头开始刷起了! 拿起公司的黑色美版G1,按一直以来的刷机程序来刷,傻眼了....recovery的版本不对,是最早的版本,机子虽然是1.6版本,但是没有提权,无法获得root权限.也就根本无从刷机. 所以需要从新捡起以前的知识来从源头开始刷起..这就需要一个逻辑性的顺序问题.也就是我们需要将G1降级到RC29或者RC7版本,RC29对应的是美版第一个文件,而我的白色英版对应的则是RC7. 顺序上应该是: 将手机降级并获取root权限 push recovery.img,获取testkey 恢复出厂设置 升级所需要版本. 升级RC版本 升级G2版本 升级APP2SD版本 在第一步就卡着了,源头版本文件没有保存.各大论坛也都没有存档了!找的比较费劲,所以有些文件还是需要保存的.... 具体可以参看博客内的相关链接! 另外,关于技术文档上比较重要的一点就是,在获取testkey以后,最好更换recovery的版本,因为以前的版本选项比较少,只能将刷机包更名为update.zip来进行刷机,而在后期的版本中加入了list,可以选择根目录内的zip包来进行刷机而无需命名为update,并且可以wipe system以及sdcard等多个内容.这就是所说的full wipe,现在很多rom都需要full wipe才可以刷机,避免出错! 并且更重要的是,在后期版本中的recovery中加入了ext3 to ext4的更换分区功能.玩过linux的应该对这个分区很熟悉!而在黑色美版中我刷入了1.61版的recovery,依旧刷的1.6版本rom.","link":"/g1-mentioned-order-of-the-right-brush/"},{"title":"G1 蓝牙传输更新 Bluetooth File Transfer 1.4","text":"不知道有没有人和我遇到了一样的问题!在G1rom更新到固件1.6以后,app2sd的Bluex1.12版本无法安装,而无app2sd版本的在安装后只能发送文件而无法打开蓝牙端口接收! 有兴趣的朋友可以到这里下载以后试试. 而这次寻到一个新的软件,版本为1.4版本,相信不是以前的bluex的版本更新,其实我自己也不确定是否为更新版本,因为图标已经换的很彻底了,并且内部结构也与以前大不一样!最重要的一点,原来支持文档类别存放,而现在却统一放在一个文件夹下!确实有些很不方便... 不过这个apk只需要在sdcard内直接安装就可以使用,需要提供root权限...而传输和接受都OK,并且比较顺畅! 另外,此软件我没有在hero的固件上测试,不知道是否支持hero,有兴趣的自己试试吧!而hero上的蓝牙传输其实有解决办法了...如下: More progress 11:04pm CST 8/26/09 Tracked down what calls the BTIP service, it's /system/lib/libandroid_runtime.so . Tried replacing it with a cupcake build, rebooted and ran into the issue where /system/framework/framework.jar is still referencing calls that were in the Hero libandroid_runtime.so . So replaced framework.jar and framework.odex from cupcake build and got the following error. D/AndroidRuntime( 1517): &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; AndroidRuntime START &lt; &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; D/AndroidRuntime( 1517): CheckJNI is OFF I/dalvikvm( 1517): DexOpt: mismatch dep signature for '/system/framework/core.odex' E/dalvikvm( 1517): /system/framework/framework.jar odex has stale dependencies I/dalvikvm( 1517): Zip is good, but no classes.dex inside, and no valid .odex file in the same directory D/libc-abort( 1517): abort() called in pid 1517 Any \"educated\" ideas? Questions &amp; Progress 01:09pm CST 8/17/09 So lately what I've been trying to do is find where a reference is made to actually call the BTIPS service. I've been lookiing in /system/framework and /data/app_s/Settings.apk but haven't found it yet. What I'm hoping to do is modify the file and have it call BT the same way cupcake did. Has anyone else found where a reference to \"btips\" is at? Settings.apk, which is what pops up when on home screen and you hit menu-&gt;settings, only makes a call to \"android:targetClass=\"com.android.settings.bluetoo th.BluetoothSettings\" Anywho, if you find it in any system libraries or framework files let me know. Please no PM's or posts about where you \"THINK\" it may be at. I've already tried the random guessing stuff, now I'm going through libraries one by one trying to find it. Some more notes 12:30pm CST 7/24/09 Here are some notes of interest. There are two versions of the /system/bin/bts daemon that are floating around on the Hero builds md5sum bts 29ffa46f12c01e3690690752b4e2d58d bts md5sum bts 5aeaca42d67d3b3c64ceda9ee4bfec1a bts There are also two versions of the TIInit_5.3.53.bts firmware files. One is actually just the brf6300.bin file renamed to match what Hero is looking for in /etc/firmware md5sum TIInit_5.3.53.bts d7a214bdb9b4fbc2b4e2dd7e3ab95df0 TIInit_5.3.53.bts md5sum TIInit_5.3.53.bts cb3d2ecbfc97c026a0dcceb8c959b7db TIInit_5.3.53.bts If you run \"strings\" on /system/bin/bts and grep for \"TII\" you'll be able to tell which firmware files that version supports TIInit_3.4.27.bts TIInit_4.2.38.bts TIInit_5.2.34.bts TIInit_5.3.53.bts TIInit_6.2.31.bts Nice picture illustrating BT architecture in Android 7:04pm CST 7/17/09 A note for ROM devs 02:27pm CST 7/17/09 Something to note, Hero does not use any of the following legacy services and therefore they can be removed from init.rc &amp; init.trout.rc . This is mainly something the ROM cookers should pay attention to. The btips service actually handles all of this now. REMOVE THE FOLLOWING: service hcid /system/bin/hcid -s -n -f /etc/bluez/hcid.conf socket bluetooth stream 660 bluetooth bluetooth socket dbus_bluetooth stream 660 bluetooth bluetooth # init.rc does not yet support applying capabilities, so run as root and # let hcid drop uid to bluetooth with the right linux capabilities group bluetooth net_bt_admin misc disabled service hciattach /system/bin/hciattach -n -s 115200 /dev/ttyHS0 texas 4000000 flow user bluetooth group bluetooth net_bt_admin disabled service hfag /system/bin/sdptool add --channel=10 HFAG user bluetooth group bluetooth net_bt_admin disabled oneshot service hsag /system/bin/sdptool add --channel=11 HSAG user bluetooth group bluetooth net_bt_admin disabled oneshot Found something 01:48pm CST 7/17/09 I was looking through init.trout.rc and noticed the following lines chown bluetooth bluetooth /sys/devices/platform/msm_serial_hs.0/serial_lock_cpu chmod 0660 /sys/devices/platform/msm_serial_hs.0/serial_lock_cpu This may not seem like much but this node does not actually exist in our builds. It's possible, and probably likely, that HTC modified their kernel to support the changes that were made in the bts (btips) daemon. We all are pretty much not using the HTC kernel, we're using custom compiled kernels from JAC or Cyanogen. I tried using the RUU kernel but couldn't boot at all. Is anyone able to get their phone booting off the RUU kernel and NOT one of the custom kernels that are floating around in these ROMs? If so, can you check if this device node exists? I believe booting off that kernel could be the answer to the UART clock issues I'm getting and missing devices in /sys . NEXT I have been toying around with the following value in init.rc that seems to affect whether or not I get an error. /proc/sys/net/unix/max_dgram_qlen The default is 10, the RUU release of Hero sets it to 999. If I change that to 10000 then it pauses the BT services and just sits there. If I revert to default I get the same error that I see when its set to 999. Wondering if there's a happy medium in queue length (qlen). Just me thinking out loud. Latest progress 11:43pm CST 7/15/09 I wanted to post some newer results I've been having with BT debugging on Hero. I found out how to circumvent the UART disable error. This is done by having the service btips statement in init.rc to look as follows service btips /system/bin/bts socket bluetooth stream 666 bluetooth bluetooth socket dbus_bluetooth stream 666 bluetooth bluetooth group bluetooth net_bt_admin root misc disabled oneshot The most important part is \"oneshot\" which tells Android to NOT restart the btips service after it dies. If you leave this off then it will relaunch btips service and tie up the I2C bus. The newest error I'm getting is the inability to launch HCI. This is hopefully the LAST error before I can get BT functional! Anyways, just wanted to update everyone that I have not stopped working on bluetooth. 1247718990.888806 BTSTACK(778) INFO | UATRAN: HCI Command was not acknowledged with an event [ vendor/ti/btips-linux/B_TIPS/btstack/hcitrans/uart/uarttran.c:298 ] 1247718990.889935 BTSTACK(778) INFO | HCI: HCI_Process detected transport failure [ vendor/ti/btips-linux/B_TIPS/btstack/stack/hci/hci_proc.c:1596 ] 1247718990.890179 BTSTACK(778) INFO | RADIOMGR: RmgrHciCallback: 0x6 [ vendor/ti/btips-linux/B_TIPS/btstack/stack/radiomgr.c:364 ] 1247718990.890362 BTSTACK(778) INFO | RADIOMGR: HCI init failed (retrying) [ vendor/ti/btips-linux/B_TIPS/btstack/stack/radiomgr.c:386 ] 1247718990.890484 BTSTACK(778) INFO | RADIOMGR: HCI init error [ vendor/ti/btips-linux/B_TIPS/btstack/stack/radiomgr.c:335 ] 1247718990.890637 BTSTACK(778) INFO | ME: HCI Init complete status: 22 [ vendor/ti/btips-linux/B_TIPS/btstack/stack/me/me.c:1220 ] 1247718990.890789 BTSTACK(778) INFO | CMGR: Received event HCI_INIT_ERROR [ vendor/ti/btips-linux/B_TIPS/btstack/profiles/common/conmgr.c:591 ] 1247718990.890942 BTSTACK(778) INFO | Dbus | inside _BTBUS_COMMON_BTL_callback with event: 6 0[ vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_common.c:62 ] 1247718990.893536 BTSTACK(778) INFO | sending dbus message from BTBUS_COMMON_BTL_callback in {vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_common.c:84}[ vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_utils.c:189 ] 1247718990.898022 BTSTACK(778) INFO | Dbus | _BTBUS_COMMON_BTL_callback signal sent: 6 0[ vendor/ti/btips-linux/EBTIPS/apps/btbus_wrap_common.c:87 ] 1247718990.898358 BTSTACK(778) FATAL | HCI Init Status Received while neither FM nor BT On in progress[ vendor/ti/btips-linux/EBTIPS/btl/ti_chip_mngr/ti_chip_mngr.c:1232 ] 1247718990.898541 BTSTACK(778) Assert | 0[ vendor/ti/btips-linux/EBTIPS/btl/ti_chip_mngr/ti_chip_mngr.c:1232 ] 1247718990.899121 BTSTACK(778) FATAL | signal 11 sent to our program from address 0xdeadbaad and code 1[ vendor/ti/btips-linux/EBTIPS/apps/btt_task.c:102 ] I'll update this main post as I, or others, come up with progress or advancements. The directories for this are already created in the latest Hero init.rc . Just need to create the ddb file touch /data/btips/TI/BtDeviceDb.ddb chmod 666 /data/btips/TI/BtDeviceDb.ddb The results of making these changes is you are able to get ALL bluetooth services and sockets created. Bluetooth is working from the commandline, just not on the frontend where we need it. PS:xda那边似乎有人已经放出hero shippment rom, 蓝牙问题应该已经解决了....静待佳音吧! :[download id=\"6\"] | skydrive 下载","link":"/g1-bluetooth-file-transfer-1-4/"},{"title":"Google  Android 4.0","text":"10,19日上午十点左右,Google,三星在香港联合举办了发布会,推出了下一代Android系统(Ice Cream Sandwich)Android4.0,以及下一代Nexus智能手机,三星Galaxy Nexus Prime. 这次的软件开发包也在这次大会宣布可以下载使用,在developer,android.com上. 这次更新重新设计了浏览器的界面,改进了Gmail功能,增强了照相和摄影.可以进入编辑模式,使用各种系统自带的滤镜效果和其他编辑操作! 当然,我最欣赏的改进就是这个全新分享功能 一种全新的分享方法，Andriod Bean，使得我们分享变得非常简单。比如说现在他有一个Andriod的手机，把手机对过去想和我分享，就是通过Andriod Bean来分享，把两个手机背靠背的放在一起内容就过来了，这个文章就已经过来了，就是这样轻轻一碰。我想看一下Google Map，现在我看到的是一个地图，在香港弥敦道上面的，现在要把这个地图直接发到我的手机上，同样是把两个手机背靠背放在一起，我手机上也出现了同样的地图。再和大家分享最后一个例子，他在玩一个很酷的游戏，我手机上没有，但是我也很想玩这个游戏，虽然我也可以到Andriod市场上浏览、下载，但是我选择用更酷的方式来做。也是把手机背靠背的放在一起，然后就直接把我带到Andriod市场上的这个游戏页面上，我就不用浏览了，直接下载就可以了。Andriod Bean可以让我们加入某一个社交群体、分享图片、内容，极大的释放了我们的想象力。 关于更新问题,怀疑这次N1这个亲儿子估计也应该可以赶上末班车,因为Google官方不是声明,但凡运行Android2.3的机子,都可以更新并运行4.0么!现在能做的就是坐等CM大神了!","link":"/google-android-40/"},{"title":"Google+ 研究","text":"已經忘記今天是第幾天試用Google+了，這兩天圈內當然免不了的大部分都在討論Google+，而什麼所謂的G+應用技巧，什麼25條關於G+的，還有什麼50條，100條類似的！反正不止是G+內部，包括twitter，facebook，微博以及博客圈內，大家都在樂此不彼的討論Google+！ 可是我這裡不想討論一些技術上的東西，也不想討論它是否真的能夠打敗Facebook或者twitter！我只想從真正的社交上去分析一下我自己的對G+內部細微的探討！ 找不到好圖，又懶得自己做。所以借用一下+lucifr Liu的圖！反正都做了，表浪費！原圖有 PSD文檔可以下載！我按照自己的標準稍稍修改了一下！ 讓我們先來探討一下G+中的circles，雖然所有的社交網絡中都有這樣一種概念，但是沒有Google那樣去深挖它，延伸它！在多部分的社交網絡中，或許都是在模仿Facebook，或許就是在模仿Twitter！而這兩者的基本機制就是相互添加對方為好友，才算開始真正的社交！當然，Twitter的機制要鬆散一點，即便雙方沒有相互fo對方或者只有一方fo了，一樣可以互相聊天！而Facebook對於社交圈子的概念就相對封閉了一點，如果只有一方fo了對方，那麼一樣不構成相互溝通的條件！ 而比較尷尬的是，Facebook作為一個最大的SNS網站，基本上老老少少都在上邊進行基礎社交，這也構成了現在年輕人逐步放棄Facebook的主因！因為這個如果都相互fo了對方，那麼這個circles的範圍就太大了。只要我發佈了一條消息，我的老師，我的父母，我的同學以及我的基友，大家都可以看到這條消息！而在真實的社交範圍中，這是不可想像的！比如，包二奶這裡就是個十分不適合的場所！當然，我們可以私信！！！！！！好吧，我舉的例子有些屎，我承認！那麼讓我們想像一下，總有些事情是不希望父母或者另外的人知道的，而只想在小範圍內的圈子內傳播的！那麼，Google+給了你這個實現的方式，就是circles概念的區分和交集！這個還需要你對聯繫人進行具體的整理才可以！有些人，即是朋友又是同事，這就是兩個circles的交集，那麼我一條信息不管發佈在我的朋友圈還是同事圈，這個人都可以收到我的信息！加入說，同事圈裏又有些話題需要討論而不想讓老闆看到該如何，讓我們建立一個將老闆排除在外的同事圈！ 大概瞭解了Circles的概念以後，我想討論的就是Google+的一些細節的地方！我們都知道，無論是微博還是SNS社交網站比如開心，人人網都會有Share分享功能！這個功能的作用在於，看到好的東西以後，可以直接分享給自己的社交圈裏的所有人觀看！這也是網絡病毒營銷的根源！而在Google+中，我們注意到一個細節，並且進行了一番討論。Share這個功能被分割了。我們以大家熟悉的微博為例，當我Share某話題以後，如果原帖被修改或者刪除，那麼我Share過去的內容也會跟著變動！而且原帖會被注明轉帖和評論了多少次！在G+中就完全不是這樣，當我Share某個帖子後，這個帖子就變成我複製過去的了！只會聲明原帖出自什麼人。並且評論不會跟著原帖一起變動，而我在新Share的帖子下邊回覆，也不被記錄在原帖的評論裏！這就是我們所討論的被分割！可是我們思考一下真實社交圈的情況，我們從某個人那裡聽到了一個消息，然後四處散播！當那個人再對同樣的圈子內修正了這個消息或者否定了這個消息以後，我所散播出去的圈子并不能收到這樣一個反饋，那麼，只有我自己去再去修正一下自己所說過的話！那麼也就是，每個人都必須對自己所說的話和所散播的言論負責！而當你所散播了消息以後，即便和你之前聽到的是一模一樣的，那麼這個討論也只限制在你自己的圈子內而和原來的圈子無關！這個說明，Google+在細節上真的是在充分考慮真實社交的環境而在努力模擬再現！ 大家也都發現，G+可以被當作郵箱來用！當你發表一篇消息給某個特定的人或者一個圈子的人。那麼對方會收到一個消息。這個消息最終如何接受有三種情況！第一種情況，對方沒有G+帳號，那麼這條消息肯定是直接發佈到對方郵箱裏！第二種情況，對方有G+帳號而你所寫接收方是聯繫人或者圈子，並且對方有關閉郵件提醒，那麼對方會在G+上接收到這條信息。第三種情況就是對方有G+帳號並且沒有關閉郵件提醒，那麼就是G+和郵箱都會收到這條消息！ 不過，對於給沒有G+帳戶的人用G+發佈邀請還可以，如果要當作邮箱来用，還是奉勸大家不要這樣做！因為G+發佈郵件的郵箱並不是你的郵箱，是隨機生成的一個郵箱地址，並不是固定的！也就是說對方即便收到也無法回覆！而我做了一個實驗，給我的163，QQ，以及yahoo和gmail郵箱都各發了一個郵件！有的郵件無影無蹤了，而即便收到的郵件，也是等了很久以後才收到的！所以，如果要發郵件的話，請點擊黑又長上邊+You後邊的Gmail選項，老老實實的用Gmail發郵件吧！ PS：此處之後還會不斷更新，分享我對Google+的一些想法！當然，是有了感悟後才會來更新！以後關於Google+的研究如無特殊情況也都在本帖內討論！想到哪寫到哪，不分那麼多條條框框了！ 其實，關於Google+，還有很多值得研究的細節，Google也在不斷完善！可以看得出這真的是一個很有誠意的互聯網產品！相應號召，大家都搬家吧！","link":"/google-research/"},{"title":"Gmail 的转发上限","text":"在Gmail中创建转发过滤器是有上限的,这个估计没有多少人知道!我想也是因为没有多少人有实际的需求… 今天我在Twitter上抱怨Gmail里的邮件过多,占用了很大一部分空间,以至于我的Gmail空间已经临近上限… 也终于迫使我开始清理邮件! ###问题产生:### 在清理的时候发现,除了一些比较大附件的邮件以外,大部分占用空间的邮件都是一些广告邮件! 我很少设置过滤器删除广告邮件,特别是一些推送服务的广告,不过大多都是直接略过收件箱存档而已,有空了还可以去看看…这也使得我这几年来邮箱里布满了此类邮件! 当然,清理的过程还是比较顺利的,毕竟这些邮件的时效性的原因,这些邮件删除起来一点也不心疼! 那么,对于新邮件该怎么处理呢,总不能继续占用存储空间吧.也不好直接删除,毕竟有的时候我还是要看看的. 那么,过滤器这个时候就起作用了! 我的想法很简单,既然QQ邮箱空间是无上限的,而广告邮件又无关乎隐私问题,所以可以放心的转发到QQ空间进行保存!所以对一些广告邮件设置了forward filter…然后问题来了,当我建立到一定的数量的时候,Gmail开始提醒我“转发邮件地址过多,无法创建过滤器 这可怎么办,总该是有解决办法的!于是查阅Gmail的帮助手册发信,Gmail是支持布尔运算符来查找邮件的!而这些布尔运算符一样可以创建到filter里. OK,问题到这里就变得简单了… ###解决方案:### 比如说,我要将 renren.com kaixin001.com 高朋 卓越 美团 拉手等邮件过滤直接forward到QQ邮箱里,那么其实创建一条filter就够了!按如下格式renren.com OR kaixin001.com OR 高朋 OR 卓越 OR 美团 OR 拉手,其中“OR”是Gmail中所支持的布尔运算符,而且必须大写. 延伸阅读: 希望这篇文可以帮助更多的人更好的利用Gmail.更详尽的运算符可以查阅Gmail的帮助手册","link":"/gmail-forward-upper-limit/"},{"title":"Gphone SD卡分区(更新)","text":"以前写过一个Gphone app2sd的完全教程,在那个教程当中有完整的分区步骤,可是那个分区模式是需要linux系统才可以执行,而我当时用的是ubuntu. 现在可行的分区模式也就是linux下,然后windows下的分区软件.譬如PartitionManager,AcronisDiskDirector,不要去想PQ魔术师,那个不支持sd卡分区!就这么多了么?...其实,如果你有Gphone手机的话,完全可以用windows dos来分区! 先说条件: 1.Gphone手机 2.SDcard 3.android sdk 4.一条usb连接线 步骤: 首先开机进入recovery模式，按ALT+X进入“console” 打开cmd,输入: adb shell parted /dev/block/mmcblk0 print 可以看到分区的情况,一般来说都是一个分区,如果以前做过app2sd那么就是两个...删除这两个分区.如下图为三个分区: 然后输入代码删除分区: rm 1 rm 2 rm 3 (如果只有两个分区或者一个分区的,执行一步操作就可以了,也就是rm&lt; 数字&gt;) 在完成后就是一个未分区的SDcard 然后对SDcard重新分区,注意需要根据你的卡大小来分配各分区的大小,一般linux-swap最大32M.ext分区500M足够了,最大不要超过1GB.不过似乎有将linux-swap分成96Mb的...输入以下代码分区: mkpartfs primary fat32 0 7445 mkpartfs primary ext2 7445 7873 300-500都可 mkpartfs primary linux-swap 7873 7969 务必是96M 不然你有C6卡也不能全速体验HERO了... 至此,分区工作就完成了.可以输入print来检查一下. 以下为可执行可不执行步骤...就是将ext2转换为ext3/ext4 转换为ext3输入以下命令: upgrade_fs 转换ext4输入以下命令: tune2fs -O extents,uninit_bg,dir_index /dev/block/mmcblk0p2 e2fsck -fpDC0 /dev/block/mmcblk0p2 upgrade_fs 结束以后,输入 parted /dev/block/mmcblk0 print 验证是否升级到ext3/ext4 然后quit退出,重启手机. 本教程参考了安卓网上安装hero的步骤","link":"/gphone-sdcard/"},{"title":"Logistic regression to diagnose heart disease","text":"The preject source code url : Heart load data 1234567891011121314import pandas as pdfrom sklearn.linear_model import LogisticRegressionfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_splitdata = pd.read_csv('./data/heart.csv')# the csv url: https://github.com/hivandu/colab/blob/master/AI_Data/data/heart.csv# Print a brief summary of the data setdata.info()data.shapedata.target.value_counts() The params meaning 123456789101112131415Params Meaning age 年龄 sex 性别(1 = 男性, 0 = 女性)cp 胸部疼痛类型(值1：典型心绞痛，值2：非典型性心绞痛，值3：非心绞痛，值4：无症状）trestbps 血压 chol 胆固醇 fbs 空腹血糖（&gt; 120 mg/dl，1=真；0=假） restecg 心电图结果（0=正常，1=患有ST-T波异常，2=根据Estes的标准显示可能或确定的左心室肥大） thalach 最大心跳数 exang 运动时是否心绞痛（1=有过；0=没有）oldpeak 运动相对于休息的STslop 心电图ST segment的倾斜度(值1:上坡，值2:平坦，值3:下坡） ca 透视检查看到的血管数 thal 缺陷种类（3=正常；6=固定缺陷；7=可逆缺陷）target 是否患病（0=否，1=是） Perform analysis 12345678910111213141516171819202122232425# Change the &quot;sex&quot; column into two columns &quot;sex_0&quot; and &quot;sex_1&quot;sex = pd.get_dummies(data['sex'], prefix = 'sex') # Add &quot;sex_0&quot; and &quot;sex_1&quot; to the data set. data = pd.concat([data, sex], axis = 1)# And delete the sex column.data = data.drop(columns = ['sex'])# Print out the first five lines. Check whether sex_0, sex_1 are added successfully, and whether sex is deleted successfully.data.head()# Get sample labeldata_y = data.target.valuesdata_y.shape# Get sample feature setdata_x = data.drop(['target'], axis = 1)data_x.shape# Divide the data settrain_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size = 0.3, random_state=33) Normalization 1234567891011121314151617181920212223# initializess = StandardScaler()# The fit function/module is used to train model parametersss.fit(train_x)# Standardize the training set and test settrain_x = ss.transform(train_x)test_x = ss.transform(test_x)# Define a logistic regression modellr = LogisticRegression()lr.fit(train_x, train_y)# Calculate the training set scorelr.score(train_x, train_y)# Calculate test set scorelr.score(test_x, test_y)# Use the classification_report function to display a text report of the main classification indicatorspredict = lr.predict(test_x)print(classification_report(test_y, predict))","link":"/heart/"},{"title":"Hello 2012","text":"以2012起头,其实主要是想要写2011 今天是2011年的最后一天了,明天正式步入毁灭年!谁知道去,也许是真的呢...这样的社会,早点灭了的好! 总体来说,今年是非常不顺的一年.不管是对世界,对中国,还是对我来说... 由于微博的兴旺,各种事情仿佛一瞬间被放大,被拉近...而我们忽然发现,世界原来可以如此精彩...不过这精彩的代价,确实让人有些心痛!这一年里,最欣喜的事情就是在Google Plus里看@变态辣椒的漫画.真是一个好地方啊... 不过这个月初开始不太愿意在那里发言,主要原因还是发现其实放在什么地方,都会有脑残的出现! 而最让人头疼的是,脑残们都不会觉得自己是脑残,根本说不通道理...而他们那些偏激的想法,让我感到恐惧! 是的,如果人类经历一次灭顶之灾,而忽然发现自己周围仍然在酝酿这种灭顶之灾的苗头,你会感到恐惧! 所幸,临到结尾,@韩寒 抛出了三篇惹人口风的文章...看来丫是想给大家留个值得咀嚼的话题在年内!而在问内提到里\"清算\"这样的字眼... 唉,说多了,其实也是废话! 况且说, 我这里真不适合谈论这些.","link":"/hello-2012/"},{"title":"Bluehost上安裝Habari","text":"Habari我還在試用階段...不得承認wordpress確實是一個好的博客程序,但是對於他的日進臃腫我有點微詞... 可憐我不是代碼編寫出身,所以很多問題不得不求助於人,還好身邊有一個好老師\"天佑\"給我提供了很多幫助! 以下就寫寫近期的一些內容. 因為bluehost原本就只願PHP5,而其他的一些標準我不太懂,但是據我安裝下來基本都已經全部滿足,唯一的就是需要自己開啟pdo for mysql. 在這之前你最好是下載一個requirements.php來測試一下你的主機是否已經為你的habari做好了準備. 將requirements.php上傳到自己需要安裝的文件夾下,輸入http://youblogurl/requirements.php 如果切OK,那么就會如下顯示: 如若不然,就會有如下顯示: 我這裡提示的是需要安裝或者打開pdo,如果你也是bluehost,那么基本就是這個提示了! 其實到這步是不需要聯繫客服幫你打開的,默認bluehost就已經安裝了pdo,主要是需要打開而已! 在自己的服務器上建立php.ini文檔,在CP上點擊PHP Config,然後選擇install php.ini master file 不出意外在根服務器上就已經建立php.ini了,加入如下語句: extension_dir = \"/usr/lib/php/extensions/no-debug-zts-20060613\" extension=pdo.so extension=pdo_mysql.so 到這裡當然還沒有結束,需要將php.ini複製到你所要安裝的habari文件夾... 這步一定要做.我不知道原理,所以不要問我,我只是如此操作了,成功了.反而刪除后就會出現需要激活pdo的提示. 然後自然就是下載habari,上傳到安裝目錄,輸入路徑...然後就是和wordpress的安裝順序一樣了! 這裡要提示一點:在網上有說安裝habari要將其目錄設定為777,我的經驗是不要這樣設置,這樣會造成訪問此目錄的時候出現505錯誤頁面...\"天佑\"幫我測試的時候就是如此!...後來我更改回來后就正常訪問了... 當然,很多人都是wordpress的用戶,所以如果你需要導入wordpress的原始數據的話需要做如下工作: 安裝一個插件,用以取消自動保存. 禁用後臺的多版本保存. 刪除數據庫中多餘的文章版本. 完成一上步驟后在habari導入的時候才不會有重複文章導入! 當然,在我的habari測試中還出現了亂碼問題,不僅僅是導入的時候出現亂碼,在輸入博客標題,寫新文章都會出現中文亂碼問題,在發此文的時候這個問題還沒有得到解決,只能讓各位待續了! 再次感謝\"天佑\"的幫助!","link":"/install-habari-at-bluehost/"},{"title":"Google正式推出代码搜索 Code Search","text":"来自小熊在线 Google新一轮的发布热潮在黄金周涌现。这就是Google刚推出的代码搜索，即Google Code Search。根据Garett Rogers的介绍，Google代码搜索结果来自Google的索引。也就是说理论上Google能找到的代码，你都可以利用Google代码搜索找到。 你可以利用Google代码搜索来搜索各种函数的定义以及相关的示例代码，还可以直接使用正则表达式搜索以获得更精确的结果。另外，你还可以限定搜索某种语言、许可或文件名。 对于程序员而言，这个代码搜索工具应用挺实用的。Google Code Search允许编程人员搜索代码用法范例，以更好地理解代码功能。该服务中索引了数十亿行代码，来源是Web上保存的文档以及SourceForge、Google Code等开源软件项目库。 Google还提供了一个API允许第三方开发人员将代码搜索框整合到他们的开发工具中。 点击进入Google Code Search","link":"/google-code-search/"},{"title":"habari的時區和more","text":"在這之前還一直在煩惱habari的時區問題和more代碼截斷. fireyy給了我一些幫助. habari最新的版本已經加上了時區的調整.而這個版本是需要svn方式獲取才可以.基於bluehost默認沒有開通ssh方式.那么我就只有選擇等待habari下一版本的正式發布,又或者聯繫bluehost增加ssh訪問,并花一定時間來學習和熟悉svn獲取的方式! 另外一個就是代碼截斷.在habari總一樣可以試用more代碼來對文章進行截斷,在所用模板總加入: // Only uses the tag, with the 'more' as the link to full post Format::apply_with_hook_params( 'more', 'post_content_out', 'more' ); 這樣在編寫entry的時候就與wordpress的效果是相同的了. 而整個habari的模板代碼和wordpress是比較類似的!頭疼的是plugin和wordpress有很多不一樣的地方!所以對於不懂代碼的我來說,就無從著手了... 奇怪的一點是我現在說使用的habari模板激活后有一個setting的選項,點擊以後并無反應! 我想本身是可以修改的... 比較丟人的地方在於,前邊的文章總提到了歸檔頁面,其實并不是真的在頁面中直接添加代碼就可以了!而是我所用的模板中已經加入了兼容代碼...對於如何建立頁面來實現仍然不清楚! 2009-2-21 01:53 BTW:好吧,我承認我自己又傻了一回...并不是需要在服務器上安裝SVN才可以的..也可以將svn服務器上的程序下載到本地然後再上傳,效果是一樣的!而我現在說安裝的habari就更新到了0.6beta.這樣就完全解決了時區問題!並且增加了zh_tw的文件包!雖然漢化并不完全,但是已經很好了!","link":"/habaris-time-zone-and-more/"},{"title":"Debian上安装Node服务器","text":"本教程只针对我自己，记录用，而且并不是完整版，不对其他人负责。请尽量不要参照本文 最近一直在学习NodeJS，本地上玩的差不多了，总要去架设个服务器跑一下，选择了digitalocean加的$5/mo 服务，安装了个Debian，至于为什么是Debian，好吧，因为。。。 言归正传，一遍操作一遍记录一下，好记性不如烂笔头嘛。 至于怎么注册DigitalOcean这里就不在详述了，从SSH登录开始吧。 SSH登录 未免麻烦，最好是选择SSH登录，官方有详细的介绍： How To Use SSH Keys with DigitalOcean Droplets 这里要说一下，DigitalOcean每次登录的时候都会告诉我密码过期，害得我重置了无数次密码。如果你也遇到这种问题，那么就先选择Conole Access, 然后在弹出的窗口控制台进行操作，修改root密码后再在本地操作。 12345678910111213141516171819202122// 创建SSH keyssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (~/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in ~/.ssh/id_rsa.Your public key has been saved in ~/.ssh/id_rsa.pub.The key fingerprint is:4a:dd:0a:c6:35:4e:3f:ed:27:38:8c:74:44:4d:93:67 demo@aThe key's randomart image is:+--[ RSA 2048]----+| .oo. || . o.E || + . o || . = = . || = S = . || o + = + || . o + o . || . o || |+-----------------+ 1cat ~/.ssh/id_rsa.pub 然后添加到digitalocean的SSH Keys里，Name随便起 之后我们就可以链接服务器了 1cat ~/.ssh/id_rsa.pub | ssh root@[your.ip.address.here] &quot;cat &gt;&gt; ~/.ssh/authorized_keys&quot; 然后，就可以直接登录了: 1ssh root@[your.ip.address.here] 安装Node 我选择的方式是源码安装 1234567891011121314151617// update system$ sudo apt-get update$ sudo apt-get install git-core curl build-essential openssl libssl-dev// Clone node$ cd /usr/local/src$ git clone https://github.com/nodejs/node$ cd node// select checkout$ git tag$ git checkout v4.4.7// install$ ./configure$ make$ sudo make install 漫长的等待，然后就可以查询了$ node -v， 这会就会出现安装的node version 安装NPM 12345$ wget https://npmjs.org/install.sh --no-check-certificate$ chmod 777 install.sh$ ./install.sh$ npm -v3.10.5 安装zsh(不是必要) 123456789sudo apt-get zshgit clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh// copy defult zshrccp ~/.zshrc ~/.zshrc.bak// set oh-my-zsh to usecp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrccash -s /bin/zshsudo shutdown -r now 安装Ruby 安装rbenv 1234567891011git clone git://github.com/sstephenson/rbenv.git ~/.rbenv# 用来编译安装 rubygit clone git://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build# 用来管理 gemset, 可选, 因为有 bundler 也没什么必要git clone git://github.com/jamis/rbenv-gemset.git ~/.rbenv/plugins/rbenv-gemset# 通过 gem 命令安装完 gem 后无需手动输入 rbenv rehash 命令, 推荐git clone git://github.com/sstephenson/rbenv-gem-rehash.git ~/.rbenv/plugins/rbenv-gem-rehash# 通过 rbenv update 命令来更新 rbenv 以及所有插件, 推荐git clone git://github.com/rkh/rbenv-update.git ~/.rbenv/plugins/rbenv-update# 使用 Ruby China 的镜像安装 Ruby, 国内用户推荐git clone git://github.com/AndorChen/rbenv-china-mirror.git ~/.rbenv/plugins/rbenv-china-mirror 12345echo 'export PATH=&quot;$HOME/.rbenv/bin:$PATH&quot;' &gt;&gt; ~/.bashrcecho 'eval &quot;$(rbenv init -)&quot;' &gt;&gt; ~/.bashrc#rbenv install 2.3.1 特么什么都能遇到，远端locale和本地不符，提示无法安装 1sudo locale-gen en_US.UTF-8 // or 1sudo dpkg-reconfigure locales 1vim /etc/ssh/ssh_config 注释或删除AcceptEnv LANG LC_* (服务器SSH配置) 然后断开SSH重新登录，不行重启一下服务器，就好了。 1sudo shutdown -r now 继续: 1rebnv install 2.3.1 部署Nginx 1$ sudo apt-get install nginx 其实Nginx也不是必要装的，Node自己可以跑服务！ 1nohup node app.js 但是如果要多域名的话，需要用到Nginx反代，额，这部分还不懂。再去研究下！ 顺便，加一个删除Nginx的步骤: 1234567sudo apt-get --purge remove nginxsudo apt-get autoremovedpkg --get-selections|grep nginx// 罗列出与nginx相关的软件， nginx-common deinstall 然后sudo apt-get --purge remove nginx-common","link":"/install-node-to-Debian/"},{"title":"hexo-generator-feed","text":"前段时间想通过QQ邮箱将自己的写的博客里的文章转发到QQ空间去。众所周知，这种转发似乎只有通过QQ邮箱发邮件转发一条路。要不就只有Ctrl+C，Ctrl+V。我是很厌恶这种方式的。 可是在QQ邮箱里，并没有收到自己从使用hexo以后的任何更新。好吧，我知道问题了，我失去了feed订阅路径。而Google的结果是，hexo自身并不带feed订阅，如果要支持订阅需要安装插件，也就是“hexo-generator-feed\" 这货。 后来，良久之后@lucifr也是这么告诉我的。 前段时间家里有事，离开上海，回来以后又因为工作需要恶补，一直没时间弄！这刚刚才弄好。。。。 说一下这货，其实是蛮简单的，我也就不再细述了，官方文档也将如何安装写的很清晰。自己查阅一下就好。","link":"/hexo-generator-feed/"},{"title":"King of pop 2014","text":"I MISS U.","link":"/king-of-pop-2014/"},{"title":"Mac MAX_open","text":"在使用Hexo的过程里，经常会卡在deploy指令上，错误原因之一可能是因为Mac的MAX_open数小的原因，Linux默认为1024，而Mac上只有256，所以只要修改MAX_open数就可以了。指令如下： 1234567891011$ sudo sysctl -w kern.maxfiles=20480kern.maxfiles: 12288 -&gt; 20480$ sudo sysctl -w kern.maxfilesperproc=18000kern.maxfilesperproc: 10240 -&gt; 18000$ ulimit -S -n 2048bubbyroom.com$ ulimit -n2048 其中，$ ulimit -n是用于查看Mac的MAX_open数的指令。只执行修改之前可以先执行此指令查看一下。 后记：在Terminal中修改了MAX_open仅适用于当前窗口，新建Tab，窗口后在新的Tab和窗口里都会失效。","link":"/mac-max_open/"},{"title":"Mac隐藏&#x2F;显示桌面图标","text":"Shell: 1defaults write com.apple.finder CreateDesktop -bool FALSE;killall Finder &amp; 1defaults delete com.apple.finder CreateDesktop;killall Finder AppleScript: 1234567display dialog &quot;桌面图标设置为可见或隐藏?&quot; buttons {&quot;可见&quot;, &quot;隐藏&quot;} with icon 2 with title &quot;Switch to presentation mode&quot; default button 1set switch to button returned of resultif switch is &quot;隐藏&quot; then do shell script &quot;defaults write com.apple.finder CreateDesktop -bool FALSE;killall Finder&quot;else do shell script &quot;defaults delete com.apple.finder CreateDesktop;killall Finder&quot;end if","link":"/mac-False-Desktop-icon/"},{"title":"mac apache","text":"经常忘记命令，还是自己稍微记录一下好，免得每次都去Google。 打开 apache sudo apachectl start 關閉 Apache： sudo apachectl stop 重開 Apache： sudo apachectl restart","link":"/mac-apache/"},{"title":"mac:port occupied","text":"查询: 1$ lsof -i:3000 显示: 12COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEnode 2243 du 12u IPv6 0xc9b8c91a94a8da89 0t0 TCP *:hbci (LISTEN) 结束: 1$ kill -9 2243","link":"/mac-port-occupied/"},{"title":"Mac显示隐藏文件","text":"转载自Mac疯，记录而已，不长用，老是记不住，每次都要Google，郁闷！ 显示文件： defaults write com.apple.finder AppleShowAllFiles -bool true 隐藏文件 defaults write com.apple.finder AppleShowAllFiles -bool false 记得KillAll Finder来重启Finder","link":"/macxian-shi-yin-cang-wen-jian/"},{"title":"「马拉松跑步数据","text":"先引入数据,准备进行分析 1234# 引入数据import pandas as pddata = pd.read_csv('~/data/cbcpv/marathon/marathon.csv')data.sample(5) OUT: age gender split final 19841 34 M 01:55:25 04:50:03 11002 28 W 01:55:00 04:11:00 11619 26 M 01:40:28 04:13:52 4068 34 M 01:38:30 03:30:21 6922 35 M 01:37:44 03:48:37 这个数据集有以下几个特征： age，运动员的年龄 gender，运动员的性别 split，半程所用时间 final，全程所用时间，即最终成绩 自然,要先了解下数据的具体情况 1234567891011121314data.info()# 输出结果&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 37250 entries, 0 to 37249Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 37250 non-null int64 1 gender 37250 non-null object 2 split 37250 non-null object 3 final 37250 non-null objectdtypes: int64(1), object(3)memory usage: 1.1+ MB 可以看到并没缺失值, 不过split和final特征中的数据不实数字类型, 用字符串表示了所用时间长度, 所以我们需要进行转化: 1234import datetimedef convert_time(s): h,m,s=map(int, s.split(':')) return datetime.timedelta(hours=h, minutes=m, seconds=s) 使用完成的方法进行数据转换,我们从新读取一下数据: 123456789101112131415df=pd.read_csv( '~/data/cbcpv/marathon/marathon.csv', converters={ 'split': convert_time, 'final': convert_time })df.dtypes# 输出结果age int64gender objectsplit timedelta64[ns]final timedelta64[ns]dtype: object 这次数据已经转换为timedelta64类型, 下面我们就要转化时间为整数, 一般的做法都是秒或者毫秒数: 1234567d = datetime.timedelta(hours=1, minutes=0, seconds=0)df2 = pd.DataFrame({'time':[d]})df2.astype(int)# 输出结果 time0 3600000000000 我们看到的输出结果,是“纳秒“(ns)单位: \\[1s=10^9ns\\] 我们还需要转化为秒: 1234567d=datetime.timedelta(hours=1, minutes=0, seconds=0)df2=pd.DataFrame({'time':[d]})df2.astype(int) * 1e-9# out time0 3600.0 然后我们要讲split和final两个特征的数据进行转化 123df['split_sec']=df['split'].astype(int) * 1e-9df['final_sec']=df['final'].astype(int) * 1e-9df.sample(5) OUT: age gender split final split_sec final_sec 11725 35 M 0 days 01:53:53 0 days 04:14:19 6833.0 15259.0 19815 24 M 0 days 01:58:45 0 days 04:49:57 7125.0 17397.0 5754 49 M 0 days 01:42:39 0 days 03:41:05 6159.0 13265.0 33166 46 M 0 days 02:31:37 0 days 06:06:17 9097.0 21977.0 9226 36 W 0 days 01:49:06 0 days 04:01:55 6546.0 14515.0 现在多了两个特征split_sec和final_sec, 都是以秒为单位的浮点数. 描述统计 先了解数据: 1df.describe() OUT: age split final split_sec final_sec count 37250.000000 37250 37250 37250.000000 37250.000000 mean 40.697369 0 days 02:03:54.425664429 0 days 04:48:09.303597315 7434.425664 17289.303597 std 10.220043 0 days 00:22:55.093889674 0 days 01:03:32.145345151 1375.093890 3812.145345 min 17.000000 0 days 01:05:21 0 days 02:08:51 3921.000000 7731.000000 25% 33.000000 0 days 01:48:25 0 days 04:02:24 6505.000000 14544.000000 50% 40.000000 0 days 02:01:13 0 days 04:44:25 7273.000000 17065.000000 75% 48.000000 0 days 02:16:11 0 days 05:27:36 8171.000000 19656.000000 max 86.000000 0 days 04:59:49 0 days 10:01:08 17989.000000 36068.000000 居然年龄上最大的数据是86, 让我们看看特征的数据分布: 123%matplotlib inlineimport seaborn as snsax=sns.boxplot(x=df['age']) 这个箱线图反应了, 数据里确实有一些“离群值”. 数据分布 研究下数据分布, 看看split_sec和final_sec 1sns.displot(df['split_sec']) 1sns.displot(df['final_sec']) 整体看来,两个特征下的数据都符合正态分布, 但是final_sec的分布图比较胖. 这次我们把gender这个分类特征添加进来: 1sns.violinplot(x='gender', y='final_sec', data=df) 这些看到, 男性运动员在总体上还是比女性运动员要快一些. 寻找优秀的原因 跑马拉松或者了解这项运动的人都清楚, 运动员很关注整个赛程中前后半程的时间比较,好的选手是后半程用时和前半程近似. 因此, 我们来研究下, 这些运动员前后半程用时情况. 12345g=sns.jointplot('split_sec', 'final_sec', data=df, kind='hex')# 绘制一条直线, 作为参考import numpy as npg.ax_joint.plot(np.linspace(4000, 16000), np.linspace(8000, 32000), ':k') 横坐标是splict_sec特征, 即半程用时. 纵轴表示final_sec特征, 全程用时. 途中可以看出, 的确是越优秀的运动员,前半程用时越接近全程用时的一半, 甚至还有少数后半程跑的更快的. 我们做个计算来深入研究下: 12df['split_frac']=1-2*df['split_sec']/df['final_sec']df.sample(5) OUT: age gender split final split_sec final_sec split_frac 2065 35 W 0 days 01:31:41 0 days 03:14:40 5501.0 11680.0 0.058048 9001 43 W 0 days 01:58:19 0 days 04:00:44 7099.0 14444.0 0.017031 30039 34 M 0 days 02:25:17 0 days 05:39:21 8717.0 20361.0 0.143755 27456 62 W 0 days 02:13:28 0 days 05:25:01 8008.0 19501.0 0.178709 13335 41 M 0 days 01:45:36 0 days 04:21:00 6336.0 15660.0 0.190805 用直方图再增加一个参考线来看看split_frac特征中的数据分布: 1234import matplotlib.pyplot as pltsns.displot(df['split_frac'], kde=False)# 垂直于 x 轴的直线，0 表示 x 轴位置plt.axvline(0, color='k', linestyle='--') 从这张图中, 更清晰的看到全体参赛者的运动安排. 再来探究下不同特征之间的关系: 12345sns.pairplot( data=df, vars=['age','split_sec','final_sec','split_frac'], hue='gender') 让我们来看下80岁选手的数量: 1234(df.age&gt;=80).sum()# OUT15 下面, 我们划分下年龄段,看看各年龄段的成绩分布: 12345678910df['age_dec']=df['age'].map(lambda age: 10*(age//10))sns.violinplot( x='age_dec', y='split_frac', hue='gender', data=df, split=True, inner='quartile', palette=['lightblue', 'lightpink']) 看这张图, 我们发现,不同性别的运动员的split_frac特征数据分布中, 年龄越大,前后端的时间分布比相对集中. 再看看全程用时分布比较: 123456789sns.violinplot( x='age_dec', y='final_sec', hue='gender', data=df, split=True, inner='quartile', palette=['lightblue', 'lightpink']) 从30岁往后, 明显年纪越大,用时越长.","link":"/marathon/"},{"title":"NFC和O2O","text":"总体来说,这是我最看好的移动应用之一! 注意,是之一. 首先,我要说的是SNS网站真的没有再大的作为了!人们已经开始从那些疯狂中开始慢慢冷却了下来,而真正实际用到SNS网站的人也是小部分而已.造势,炒作,宣传,营销...当然,我不能否认这些平台在作为传统媒体的延伸甚至是替代品上的功效. 可是又真有多少人会真的坚持活在这种亢奋的状态下.除非有大批的粉丝不停的为自己打鸡血吧? 所以我总认为,移动社交产品可以打住了.至于查找附近聊友的功能,诸如微信以及添加新功能的米聊以及后来居上的一些产品,不要看宣传的标语很美,实际上还不是沦为宅男腐女查找就近炮友的必备品而已! 好吧,对SNS产品的吐槽结束了,我真正要说的是NFC以及O2O这两个东西! 首先我们需要扫盲一下,NFC就是Near Field Communication,翻译过来就是近距离无线通信，是一种短距离的高频无线通信技术，允许电子设备之间进行非接触式点对点数据传输，在十厘米（3.9英吋）内，交换数据。这个技术由免接触式射频识别（RFID）演变而来.而我们平时所用的交通卡等一类射频卡种,都是RFID技术. &lt;而O2O(Online to Offline)就不是什么技术了,而是一种互联网商业模式,就是把线上的消费者带到现实中的商店中去在线支付购买线下商品和服务,再到线下去享受服务. 而实际上,团购,就是一种O2O模式.可是我心里的O2O模式,完全不是像团购那样的垃圾.这种模式在中国,已经早早的走入了死胡同,一条死路!我所要说的O2O,是一种集合所有商家优惠折扣的消费资源,并一卡通吃的卡片发行商. 而也终于让我遇到一个,这就是么卡.有兴趣的可以去看看,我可没有要推销么卡的意思,也无意为他写什么行销软文.只是在这里纯探讨而已! 好吧,下面让我们实际展望一下,现在很多手机已经开始内置NFC技术.比如iPhone 4S,三星盖世兔等等,那么,你能理解我说什么了,我们以后也就完全可以不必身上大卡小卡的带在身上,一个手机就OK了.交通卡,银行卡,还有各大商场卖场的贵宾卡折扣卡,都可以完完全全的装载我们的手机里! 这其实已经应该不是什么新技术,手机代替交通卡和信用卡,日本早就开始做先行者了!而实际上,O2O模式在中国也才起头而已,而重点,并不是技术怎样怎样,而是线下的资源掌握的如何.要不团购团队中实际上最大的是营销团队呢. 而像么卡这样将所有会员卡全部装到手机里的模式,一样取决于,谁掌握了最大的市场!当你的卡通吃了全世界大大小小的商家卖场的时候,相信所有人都愿意只带着一部手机走遍天下的感觉! 好了,展望结束...大家想去吧,哈哈!","link":"/nfc-and-o2o/"},{"title":"nitrous.io","text":"世界，真心是美好的啊！ 好吧，再使用nitrous之前，我一直都不知道，原来世界可以如此美好。 不需要Dropbox君，不需要git的master了，而且可以随时随地hexo。 妈妈再也不用担心我的md文档丢失了。。。 这货不仅仅是node.js,还有Rails，python以及Go环境哦。。。 亲们，还在等什么。如果windows下的Ruby环境配置让你想砸机子的冲动，那么赶快来nitrous.io吧！","link":"/nitrousio/"},{"title":"Note about my ubuntu","text":"这是我为了便于自己以后方便而记录的一些关于我ubuntu上设置所需要的内容！当然其中内容都是通用的，只是如果你们要拿去用的话记得将路径以及一些变量变成你自己的！ about system $ sudo gedit /etc/apt/sources.list 12345678910deb http://mirrors.163.com/ubuntu/ karmic main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ karmic-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ karmic-bac. kports main restricted universe multiverse $ sudo apt-get update and $ sudo apt-get upgrade and hosts $ gedit /etc/hosts # My Dropbox have new hosts file about ATI Donload form This link 12cd **sudo sh **.run That's ok about Keepass 2 so...apt-get 123$ sudo apt-add-repository ppa:jtaylor/keepass$ sudo apt-get update$ sudo apt-get install keepass2 10.04 所需软件包 1 install mono #[link](http://mono-project.com/DistroPackages/Ubuntu) 1234567Click on &quot;System&quot;, &quot;Administration&quot;, &quot;Software Sources&quot;.Click on the &quot;Other Software&quot; tab.Click on &quot;Add...&quot;, and enter the line:**deb http://badgerports.org lucid main**Click on &quot;Add Source&quot;Click on &quot;Authentication&quot;, then on &quot;Import Key File&quot;Download this [GPG key file](http://badgerports.org/directhex.ppa.asc), ID 0E1FAD0C, and select it in the &quot;Import Key File&quot; windowClick on &quot;Close&quot;, then &quot;Reload&quot; when the pop-up appears. You're all set! 2 $ sudo apt-add-repository ppa:jtaylor/keepass 3 $ sudo apt-get update 4 $ sudo apt-get install keepass2 about Java Download from This link and 1234$ cd **$ tar **.tar.gz(64bit)$ sudo update-alternatives --install &quot;/usr/bin/java&quot; &quot;java&quot; &quot;/home/hivan/software/jdk1.7.0_04/bin/java&quot; 1$ sudo update-alternatives --config java 其实就是配置一下默认路径就OK了！ 或者以下方法： $ gedit ~/.bashrc 末尾添加变量 1234JAVA_HOME=/home/hivan/software/jdk1.7.0_04JRE_HOME=${JAVA_HOME}/jreCLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libPATH=${JAVA_HOME}/bin:$PATH and 123456789$ source ~/.bashrc$ sudo update-alternatives --install /home/hivan/software/jdk1.7.0_04/bin/java 300 $ sudo update-alternatives --install /home/hivan/software/jdk1.7.0_04/bin/javac 300 $ sudo update-alternatives --install /home/hivan/software/jdk1.7.0_04/bin/jar 300 $ sudo update-alternatives --config java #用于替换Java，可能会跳出选择框$ java -version #测试java version &quot;1.7.0_04-ea&quot;Java(TM) SE Runtime Environment (build 1.7.0_04-ea-b19)Java HotSpot(TM) 64-Bit Server VM (build 23.0-b20, mixed mode) about goagent 1234567$ cd$ mkdir bin$ cd bin建立一个proxy.sh file$ gedit proxy.sh输入： python /***/goagent/local/proxy.py save and quit$ chmod 700 proxy.sh about ruby 12345678$ sudo apt-get install apache2 curl git libmysqlclient-dev mysql-server nodejs$ bash -s stable &lt; &lt;(curl -s https://raw.github.com/wayneeseguin/rvm/master/binscripts/rvm-installer)$ echo '[[ -s &quot;$HOME/.rvm/scripts/rvm&quot; ]] &amp;&amp; . &quot;$HOME/.rvm/scripts/rvm&quot; # Load RVM function' &gt;&gt; ~/.bashrc$ source .bashrc$ rvm requirements$ sudo apt-get install build-essential openssl libreadline6 libreadline6-dev curl git-core zlib1g zlib1g-dev libssl-dev libyaml-dev libsqlite3-0 libsqlite3-dev sqlite3 libxml2-dev libxslt-dev autoconf libc6-dev ncurses-dev automake libtool bison subversion$ rvm install 1.9.3$ rvm 1.9.3 –-default #有可能出错**RVM is not a function, selecting rubies with 'rvm use ...' will not work.** 则执行：`$ rvm alias create default 1.9.3 about Rails 12$ gem install bundler rails rdoc #rdoc 为Octopress所需组建$ rails -v #如果“**程序“rvm”尚未安装。**”则检查一下.bashrc的路径配置 about python 123456789$ python -VPython 2.6.6$ curl -kL http://github.com/utahta/pythonbrew/raw/master/pythonbrew-install | bash$ . $HOME/.pythonbrew/etc/bashrc$ pythonbrew install 2.7.1$ pythonbrew switch 2.7.1Switched to Python-2.7.1$ python -VPython 2.7.1 ubuntu 12中默认是2.7，如果要安装3.2和上述步骤一样！改一下版本号 about Sublime Text 2 12345678910#!/usr/bin/env xdg-open[Desktop Entry]Name=Sublime Text 2Comment=Sublime Text 2Exec=/home/hivan/software/&quot;Sublime Text 2&quot;/sublime_textIcon=/home/hivan/software/Sublime Text 2/Icon/128X128/sublime_text.pngTerminal=falseType=ApplicationCategories=Application;Development;StartupNotify=true 设置权限：可执行文件！ and 12$ cd /home/hivan/software/&quot;Sublime Text 2&quot;/$ sudo cp &quot;Sublime Text 2.desktop&quot; /usr/share/applications Ctrl+` 1import urllib2,os;pf='Package Control.sublime-package';ipp=sublime.installed_packages_path();os.makedirs(ipp) if not os.path.exists(ipp) else None;open(os.path.join(ipp,pf),'wb').write(urllib2.urlopen('http://sublime.wbond.net/'+pf.replace(' ','%20')).read()) Ctrl+Shift+P 1 ZenCoding 2 Alignment 3 Markdown 4 setting user { \"ignored_packages\": [] } download setting from this link Finally, input right, you can only choose scim,so... 12$ sudo apt-get install scim$ sudo apt-get install scim-pinyin ... 12scim设置－&gt;全局设置－&gt;将预编辑字符串嵌入到客户端中勾去掉scim设置-&gt;gtk-&gt;嵌入式候选词标 勾去掉 about Retext 123sudo add-apt-repository ppa:mitya57sudo apt-get updatesudo apt-get install retext or This link about Octopress Go to This Link add and remove ppa 12$ sudo add-apt-repository ppa:name/name$ sudo add-apt-repository -r ppa:name/name Temporary end, to be continued...","link":"/note-about-my-ubuntu/"},{"title":"拿福能千人挑战活动","text":"如何加入: 在之前@mg12 那里有看到一个博客广告.右下角的地方有一个拿福能的广告. 就我所见,吴钊童鞋很少会在页面上添加广告的,除非他认为可以做的.这和我的页面上那么多广告不同.所以他的页面一直很清爽!这次少见的添加了拿福能,我想一定有些搞头.所以就去注册了,并且,我从@Denis那里也看到了相应的广告,还看到了一篇拿福能博主北京聚会的Post,而且对其大加赞赏!So,我想,这个广告提供商有搞头. 拿福能是做什么的 总的说来,拿福能是一个广告提供商,可以称作网络营销公司.说起来Feedsky以前做过付费软文大家应该都不陌生了.不过我知道众多的博主都被Feedsky的付费评论给搞伤了,很难相信同类的公司能做的起来.自然,我也有些将信将疑.不过对于博主们来说,有多一份的收入总比没有的强.在国内,写独立博客确实是一件非常让人心酸的事情,在国外独立博主获得丰厚的回报的时候,我们从几年前就开始在探讨国内博客的生存之道,到今天为止仍然还是没有找到合适的出路.那么这次拿福能既然为博主们提供了如此好的一个机遇,大家不妨做做看! 并且,拿福能有一些和其他不同的是,他们有一些线下的付费活动.就如@mg12 介绍的,他们会提供一些电影或者之类的观赏活动,然后在博主回来之后写影评或者剧透.当然,这基本属于一种电影宣传活动!更重要的是,有无数的独立博主可以相互交流.不管你是哪一类的博客,总有一个主题是适合你的! 活动内容 据说,拿福能在国外已经做了好几年了.并且有一些非常成功的案例,而此次开发国内市场,当然是期望在国内也有所作为.所以搞了这么一个千人挑战活动,分发50,000元奖金来招募一千个博主.届时,人满之后这一千个博主将会平分这50,000元奖金. 有兴趣的,可以去拿福能的千人挑战活动主页参看!","link":"/nuffnang-challenge-1000/"},{"title":"pokemon","text":"引入依赖和数据 123456789import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as pltimport ospath = os.path.expanduser('~/data/cbcpv/pokemon/')df = pd.read_csv(path + 'pokemon.csv', index_col=0, encoding='cp1252') 探索数据 1df.sample(5) OUT: Name Type 1 Type 2 Total HP Attack Defense Sp. Atk Sp. Def Speed Stage Legendary # 75 Graveler Rock Ground 390 55 95 115 45 45 35 2 False 82 Magneton Electric Steel 465 50 60 95 120 70 70 2 False 79 Slowpoke Water Psychic 315 90 65 65 40 40 15 1 False 123 Scyther Bug Flying 500 70 110 80 55 80 105 1 False 9 Blastoise Water NaN 530 79 83 100 85 105 78 3 False 对比并了解下数据集的各个特征类型: 12345678910111213141516171819202122df.info()# OUT&lt;class 'pandas.core.frame.DataFrame'&gt;Int64Index: 151 entries, 1 to 151Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 151 non-null object 1 Type 1 151 non-null object 2 Type 2 67 non-null object 3 Total 151 non-null int64 4 HP 151 non-null int64 5 Attack 151 non-null int64 6 Defense 151 non-null int64 7 Sp. Atk 151 non-null int64 8 Sp. Def 151 non-null int64 9 Speed 151 non-null int64 10 Stage 151 non-null int64 11 Legendary 151 non-null bool dtypes: bool(1), int64(8), object(3)memory usage: 14.3+ KB 可以看到Type 2这个特征有缺失值, 其他的没有, 而且显示的为正数型, 很符合数据分析的要求. 接下来用散点图研究特征Attack和 Defense的关系 1234567sns.lmplot( x='Attack', y='Defense', data=df, fit_reg=False, hude='Stage') 我们这里参数使用了fit_reg=False, 隐藏了回归线. 在Seaborn中是没有单独绘制散点图的方法的,但是通过参数设置,实现了散点图的绘制.如果此参数设置为True 接下来用箱线图看下各特征数据分布: 1sns.boxplot(data=df) 这个结果显示出, Total, Stage以及Legendary特征的数据是不适合在这里绘制散点图的, 需要对特征进行适当选择 12stats_df=df.drop(['Total', 'Stage', 'Legendary'], axis=1)sns.boxplot(data=stats_df) 这样,比较清晰的看出几个特征的数据分布情况了, 非数字的特征自动摒弃. 在研究Seaborn, 我们知道还有用i中研究数据分布的函数sns.violinplot, 我们尝试用它绘制特征Attack相对于特征Type 1的数据(这是一个分类行特征)的分布. 123456df['Type 1'].unique()# OUTarray(['Grass', 'Fire', 'Water', 'Bug', 'Normal', 'Poison', 'Electric', 'Ground', 'Fairy', 'Fighting', 'Psychic', 'Rock', 'Ghost', 'Ice', 'Dragon'], dtype=object) 上面显示了特征Type 1中唯一数据, 即数据的值. 123456789101112131415161718192021222324252627282930313233343536373839sns.set( style='whitegrid', rc={ 'rigure.figsize':(11.7, 8.27) # 设置了画布的尺寸 })pkmn_type_colors=[ '#78C850', # Grass '#F08030', # Fire '#6890F0', # Water '#A8B820', # Bug '#A8A878', # Normal '#A040A0', # Poison '#F8D030', # Electric '#E0C068', # Ground '#EE99AC', # Fairy '#C03028', # Fighting '#F85888', # Psychic '#B8A038', # Rock '#705898', # Ghost '#98D8D8', # Ice '#7038F8', # Dragon]sns.violinplot( x='Type 1', y='Attack', data=df, inner=None, # 去掉提琴图中的竖线 palette=pkmn_type_colors)sns.swarmplot( x='Type 1', y='Attack', color='k', # 数据的点的颜色 alpha=0.7 )plt.title('Attack by Type') pkmn_type_colors是一个列表, 列出的颜色对应着特征Type 1中的唯一值. 因为去掉了提琴图内部的竖线,所以整个图没有太乱, 想知道有竖线的是什么样子, 可以注释掉inner=None这个参数. 之前我们删除了三个特征得到了一个变量stats_df引用的数据集: 1stats_df.sample() OUT: Name Type 1 Type 2 HP Attack Defense Sp. Atk Sp. Def Speed # 128 Tauros Normal NaN 75 100 95 40 70 110 数据结果中看出来, 特征HP Attack Defense Sp.Atk Sp.Def Speed都是整数, 在df.info()中也能看出来.现在有需求, 如果把这些特征分布进行可视化, 而且要放到一个坐标系中进行比较? 参考: 先使用pd.melt函数, 将所指定的特征进行归并 123456melted_Df=pd.melt( stats_df, id_vars=['Name', 'Type 1', 'Type 2'], # 保留的特征 var_name='Stat' # 其余特征规定到这一列内)melted_df.sample(10) OUT: Name Type 1 Type 2 Stat value 291 Kabutops Rock Water Attack 115 406 Marowak Ground NaN Defense 110 821 Machoke Fighting NaN Speed 45 129 Gyarados Water Flying HP 95 281 Lapras Water Ice Attack 85 586 Vaporeon Water NaN Sp. Atk 110 483 Nidoqueen Poison Ground Sp. Atk 75 93 Gengar Ghost Poison HP 60 791 Vulpix Fire NaN Speed 65 481 Nidoran‰ªÛ Poison NaN Sp. Atk 40 这样,在melted_df数据集中的Stat特征中的数据就是分类数据, 值是stats_df中被归并的特征名称. 12345melted_df['Stat'].unique()# OUTarray(['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed'], dtype=object) 在此基础上, 我们绘制反应分类特征数据分布的图示. 12345sns,swarmplot( x='Stat', y='value', data=melted_df) 还可以在此基础上,再叠加一层分类: 1234567sns.swarmplot( x='Stat', y='value', data=melted_df, hue='Type 1' # 叠加一层分类)plt.legend(bbox_to_anchor=(1, 1), loc=2)","link":"/pokemon/"},{"title":"picasaweb","text":"这一下就经过了很多日子，本来前些天是想写一些教程的。 现在脑子里的想法大多都被时间冲淡了。只是随便写写吧。 由于我的机子重新装了系统的原因。迅雷里的原始连接我也找不到了，有兴趣的朋友可以自己去找找看 picasa2和picasaweb有很多的不同，最大的不一样大概就是现在没有推出中文版本。一切都是E文。自信能看得懂的朋友可以下载来试试。 picasaweb是和自己的google账号绑定的。google的picasaweb服务在网上有250M的存储空间。可以利用picasaweb直接上传到自己的账号上。我去试验了一下，服务很不错，而且有很多新的功能，可惜的是不支持外连，估计快可以了，因为每一张图片都有自己的地址。 Picasaweb和原版本还有很多新加的功能。其中我比较感兴趣的就是色相搜索。输入color:xxx(比如red)就会有相应色相的相片被搜索出来。效果大家可以自己下载以后自己看看@。在我使用过程中，虽然E文给我找了不少麻烦，但是总体上感觉这次的升级还是有很多实用的功能在里边。 好了，软件谈完了，说说最近看的电影。。我这个人不喜欢在网上下载，质量太侮辱我眼睛。所以我一般都是等到碟版出来以后买回来看。。。。等到现在才看了达芬奇密码和碟三，但是我并不是要向大家说这两部片子，而是我看的另外一部:《东京审判》这是一部设计历史题材和政治题材的片子。1945年日本签署投降书的以后，联盟国法官团赶赴东京对日本的20多位战犯进行审判。长达817次审判和最后一次宣判共818次的审判历程。 印象最深的是曾志伟饰演的日本人。从他嘴里说出来的:狗日的日本鬼子。 不知道这句话是因为深受其害还是因为别的原因。再怎么样，一个日本人不会如此辱骂自己的国家吧。就象中国人自己，如果说了一句中国猪，反映会是怎样的？ so。。。最后还是不得不感叹战争带来的危害。所有的一切，也都仅仅是战争引起来的而已 不过再如何，日本到现在为止的历史观确实让我恶心。我到现在才知道靖国神社原来一共供奉着2000多位战犯，而其中有7位是甲级战犯，而最大的也是最可恶的战端挑起者东条英机，也被供奉在里边。这，是对人类文明的一种侮辱。 OK。。。写到这里了。。该去看书了","link":"/picasaweb/"},{"title":"解决Mac M1 原生Photoshop找不到CEP扩展面板","text":"研究这个问题, 也属于是撞上了! 我现在使用的PS版本为: 在Photoshop内, 我画画时一直使用的是第三方的色轮插件Coolorus, 长这样: 好用与否, 可以说, 谁用谁知道. 前些日子发现Photoshop2022有M1原生版本, 不用再使用转译版本, 我心想, 应该速度上会快很多吧! 兴奋的更新完后才发现, Coolorus面板无论如何找不到了, 原本应该在窗口下的“扩展(旧版)”菜单也找不到. 无论怎么折腾都不行, 而且设置面板里的增效工具也是灰色无法设置: 正当我心灰意冷准备返回2021时, 忽然想到, Adobe早就在PS中启用UXP插件了, 而CEP插件因为历史原因一直也无法完全取消, 那么既然是早就做的事情, 为什么2022版本里全给抹杀了呢, 重点是, 面板里设置项虽然不可点击, 但是还在? 问题应该不是出在版本上, 而是出在M1原生的问题上, 我试着去设置了转译, 再次重新打开PS, 果然不出所料, 扩展(旧版)菜单又回来了. 好吧, 这回知道是怎么回事了, 也就好解决了. 方法一: 返回PS 22.21版本, 简单粗暴 方法二: 讲PS2022设置为Rosetta转译打开方式, 同样简单粗暴! 回答一下可能大家问到的问题: 速度上, 感觉不出有什么太大的变化 是的, Coolous照样无法使用, 我快崩溃了, 正在纠结到底是放弃Coolous使用Photoshop原始色轮, 还是回到2021 反正问题是这么个问题, 解决方案也有了!大家自行抉择吧!","link":"/ps-find-cep-for-m1/"},{"title":"重裝「Yosemite」","text":"重裝系統之後，很多東西需要重裝，特別是開發環境。而開發環境的先後順序和設置，一直是我頭疼的事情。這次就着從新安裝了一遍，把很多東西都記錄下來。之前那個環境被我搞的亂七八糟，並且恢復不回來了。 多大多數次序參照 @mrzhang 系統偏好設置 更改電腦名稱 共享 允許安裝任何來源APP 安全性與隱私 --》通用 設置快捷鍵 鍵盤 --》 快捷鍵 配置VPN以及SSH 很重要，因爲很多源都在牆外了 安裝輸入法 下載並安裝\"Squirrel\" 下載並安裝\"SCU\" 安裝Sublime Text 3 設置package Ctrl+ , and setting: 12345678910111213141516171819202122232425262728293031{ &quot;caret\\_style&quot;: &quot;phase&quot;, &quot;color\\_scheme&quot;: &quot;Packages/Color Scheme - Default/Solarized (Light).tmTheme&quot;, &quot;font\\_face&quot;: &quot;Monaco&quot;, &quot;font\\_size&quot;: 13.0, &quot;hightlight\\_line&quot;: true, &quot;hightlight\\_modified\\_tabs&quot;: true, &quot;ignored\\_packages&quot;: [ &quot;Vintage&quot; ], &quot;indent\\_to\\_bracket&quot;: true, &quot;draw\\_centered&quot;: false, //居中显示 &quot;line\\_numbers&quot;: true, //显示行号 &quot;gutter&quot;: true, //显示行号边栏 &quot;fold\\_buttons&quot;: true, //显示折叠按钮 &quot;fade\\_fold\\_buttons&quot;: true, //始终显示折叠按钮 &quot;rulers&quot;: [], //列显示垂直标尺，在中括号里填写数字，宽度按字符计算 &quot;spell\\_check&quot;: false, //拼写检查 &quot;hot\\_exit&quot;: true, //保留未保存内容 &quot;line\\_padding\\_bottom&quot;: 1, &quot;line\\_padding\\_top&quot;: 1, &quot;scroll\\_past\\_end&quot;: true, //文本最下方缓冲区 &quot;tab\\_size&quot;: 2, // Tab制表宽度 &quot;translate\\_tabs\\_to\\_spaces&quot;: true, //缩进和遇到Tab键用空格替代 &quot;wide\\_caret&quot;: true, &quot;word\\_wrap&quot;: true, &quot;match\\_tags&quot;: true, //HTML下突出显示光标所在标签的两端。 &quot;match\\_selection&quot;: true, //全文高亮当前选中字符 &quot;wrap\\_width&quot;: 80} 編輯設置 /etc/paths 123456/usr/local/bin/usr/local/sbin/usr/bin/usr/sbin/bin/sbin 安裝Xcode 1xcode-select --install 安裝Homebrew 1ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/homebrew/go/install)&quot; PS: 這裏可能會很長時間的等待 設置Sublime終端鏈接 1ln -s /Applications/Sublime\\ Text.app/Contents/SharedSupport/bin/subl /usr/local/bin/sm Git, autojump 1brew install git autojump Oh My Zsh 1curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | sh 設置 ~/.zshrc: 1234export NVM\\_NODEJS\\_ORG\\_MIRROR=&quot;http://npm.taobao.org/dist&quot;[[ -s &quot;$HOME/.nvm/nvm.sh&quot; ]] &amp;&amp; . &quot;$HOME/.nvm/nvm.sh&quot;export NODE\\_PATH=$NVM\\_DIR/$(nvm\\_ls current)/lib/node\\_modules 安裝 NodeJS 12nvm install 0.11.15nvm alias default 0.11.15 安裝rbenv 123456789git clone git://github.com/sstephenson/rbenv.git \\~/.rbenv# 用来编译安装 rubygit clone git://github.com/sstephenson/ruby-build.git \\~/.rbenv/plugins/ruby-build# 用来管理 gemset, 可选, 因为有 bundler 也没什么必要git clone git://github.com/jamis/rbenv-gemset.git \\~/.rbenv/plugins/rbenv-gemset# 通过 gem 命令安装完 gem 后无需手动输入 rbenv rehash 命令, 推荐git clone git://github.com/sstephenson/rbenv-gem-rehash.git \\~/.rbenv/plugins/rbenv-gem-rehash# 通过 rbenv update 命令来更新 rbenv 以及所有插件, 推荐git clone https://github.com/rkh/rbenv-update.git \\~/.rbenv/plugins/rbenv-update 設置~/.zshrc 12export PATH=&quot;$HOME/.rbenv/bin:$PATH&quot;eval &quot;$(rbenv init -)&quot; 其他 安裝Ruby 1234rbenv install -l # list all available versionsrbenv install 2.1.5 # install a Ruby versionrbenv global 2.1.5 # set the global versionrbenv versions # list all installed Ruby versions 配置gem源 1234gem sources -a http://ruby.taobao.org/ -r https://rubygems.org/echo 'gem: --no-document' \\&gt;\\&gt; \\~/.gemrcgem updategem update --system 安装 MongoDB, MySQL 1brew install mongodb mysql 設置開機自啓動「可選」 123mkdir -p \\~/Library/LaunchAgentsln -sfv /usr/local/opt/mongodb/\\*.plist \\~/Library/LaunchAgentsln -sfv /usr/local/opt/mysql/\\*.plist \\~/Library/LaunchAgents 安装 Pow 12curl get.pow.cx | shgem install powder Powder 是一套管理工具 SSH-KeyGen 12ssh-keygen -t rsacat \\~/.ssh/id\\_rsa.pub 安裝Rails, sass, compass 以及 hexo 12gem install rails sass compassnpm install -g hexo 安裝必要工具 1234gem install mysql2gem install capistranogem install capistrano-ext Snippets - Download 安裝其他APP","link":"/reinstall-mac-osx-Yosemite/"},{"title":"How to set up networkx in Chinese","text":"Problem description Hi, everynone, when we use networkx to display Chinese, we will find that Chinese cannot be displayed. Solution Download the font in the attachment; https://github.com/hivandu/practise/blob/master/resource/SimHei.ttf Execute in jupyter notebook 12import matplotlibprint(matplotlib.__path__) Find the path to matplotlib, and then cd to this path, after cd to this path, continue cd, cd to the path map-data/fonts/ttf. Then replace the file DejaVuSans,ttf with the file we just. 1$ mv SimHei.ttf nx.draw(city_graph, city_location, with_labels = True, node_size = 10).ttf Among them, the ttf font used. I have uploaded it to everyone.","link":"/set_chinese_for_networkx/"},{"title":"在Yosemite中设置Pow","text":"在Yosemite中，Pow安装和启动是有问题的。这是因为ipfw被移除了，所以如果要在Yosemite中跑Pow，需要做些设置才可以。 1, 添加文件com.pow到/etc/pf.anchors/目录内 sudo vim /etc/pf.anchors/com.pow 2, 在文件内添加代码: 1234rdr pass on lo0 inet proto tcp from any to any port 80 -&gt; 127.0.0.1 port 20559rdr pass on en0 inet proto tcp from any to any port 80 -&gt; 127.0.0.1 port 20559rdr pass on en9 inet proto tcp from any to any port 80 -&gt; 127.0.0.1 port 20559 NOTE: 代码后一行必须要有换行符，否则会出现语法错误 3, 打开文件/etc/pf.conf 4, 添加代码: rdr-anchor \"pow\"，需要添加到rdr-anchor \"com.apple/*\"下一行 5, 打开文件/etc/pf.anchors/com.apple, 并添加代码: 12load anchor &quot;pow&quot; from &quot;/etc/pf.anchors/com.pow&quot; NOTE: 一样必须有换行符 6, 终端执行: sudo pfctl -f /etc/pf.conf 7, 好了，现在可以打开pf了: sudo pfctl -e","link":"/setting-pow-at-Yosemite/"},{"title":"模拟器的app2sd","text":"一个读者@XXX(因为个人意愿隐掉名字) 发mail询问我关于windows下模拟器app2sd的问题,先不说有没有必要,说一下我测试的结果! 本来我给他回邮件是让他试一下apptosd.apk的,后来越想越不对劲,所以自己做了下测试!结果如下: 然后通过adb shell也无法操作. # mkdir /system/sd/app mkdir /system/sd/app mkdir failed for /system/sd/app, No such file or directory 不过仔细想想,app2sd必须满足的条件我们在windows上根本就不存在,首先我们必须要一个app2sd的支援固件!然后我们需要sdcard分出一个ext2的分区... 而这两个条件全部都不满足!那基本上可以说没有办法! 看以后有没有高手可以实现模拟器上安装修改固件,那么可以安装一个app2sd的固件,而另外一个必须满足的条件就是必须将建立的虚拟sdcard分出一个ext2分区来! 满足了这两个条件,那么所有的都会水到渠成! BTW:下午这位读者给我的回复: 非常感谢！ 我测试的结果跟您是一样的，不过后面那个建目录的不一样。下面是我对您博文上的一点分析。 然后通过adb shell也无法操作. 【XXX】google好象改过linux内核，adb shell登录以后几种命令都有权限限制。在虚拟sd卡上建立文件夹有所有权限，然而用adb push传上去的就没有可执行的权限，使用chmod命令修改权限也不成功。 # mkdir /system/sd/app 12mkdir /system/sd/appmkdir failed for /system/sd/app, No such file or directory 【XXX】在system目录下adb shell 命令是没有写权限的。你这个尝试如果是mkdir /system/sd就会报“mkdir failed for sd, Read-only file system”的错误，但是在data目录下就能够创建目录。 不过仔细想想,app2sd必须满足的条件我们在windows上根本就不存在,首先我们必须要一个app2sd的支援固件!然后我们需要sdcard分出一个ext2的分区… 而这两个条件全部都不满足!那基本上可以说没有办法! 看以后有没有高手可以实现模拟器上安装修改固件,那么可以安装一个app2sd的固件,而另外一个必须满足的条件就是必须将建立的虚拟sdcard分出一个ext2分区来! 【XXX】appsd的固件这个是什么概念？用mksdcard创建的虚拟sdcard不就是对应的手机上的sdcard么？虚拟sdcard为什么要分出一个ext2分区呢？ext2分区一个什么概念，sdcard要分出ext2分区的原理是什么？ 能否简单介绍一下？或者介绍一下相关的资料？谢谢。 另：对您给我传的那个apk我不是很了解，这个文件从哪里来的，它都做了些什么事？ 下面是我adb shell后ls –l查看到的各文件夹权限，是有加载虚拟sdcard的。 123456789101112131415drwxrwxrwt root root 2009-08-10 04:45 sqlite_stmt_journalsdrwxrwx--- system cache 2009-07-21 09:01 cached---rwxrwx system system 2009-08-10 04:52 sdcardlrwxrwxrwx root root 2009-08-10 04:45 etc -&gt; /system/etcdrwxr-xr-x root root 2009-05-15 00:53 systemdrwxr-xr-x root root 1970-01-01 00:00 sysdrwxr-x--- root root 1970-01-01 00:00 sbindr-xr-xr-x root root 1970-01-01 00:00 proc-rwxr-x--- root root 9075 1970-01-01 00:00 init.rc-rwxr-x--- root root 1677 1970-01-01 00:00 init.goldfish.rc-rwxr-x--- root root 106568 1970-01-01 00:00 init-rw-r--r-- root root 118 1970-01-01 00:00 default.propdrwxrwx--x system system 2009-05-15 00:58 datadrwx------ root root 1970-01-01 00:00 rootdrwxr-xr-x root root 2009-08-10 04:46 dev 这里请注意system，sdcard和data它们的权限以及各自的意义。 123d---rwxrwx system system 2009-08-10 04:52 sdcarddrwxr-xr-x root root 2009-05-15 00:53 systemdrwxrwx--x system system 2009-05-15 00:58 data 下面说下我对各个信息的理解。首先第一列这是表示的各用户的权限，d代表这是文件夹，rwx分别代表读、写、执行权限。 d后面第一组三个字符表示当前用户的读写执行权限，第二组代表group用户的权限，第三组表示other用户的权限。 然后是第二列，表示当前用户对该文件夹的权限级别，第三列代表该文件夹的当前用户。 如果我对这组信息的含义理解方式正确的话，那么这里我就有疑问了： 1. linux下面有system这个权限级别吗？我有个同事说只有root、group和other，所以我很奇怪这里的system这个权限级别是怎么回事，它有什么样的权限，能做到怎么样。 2. sdcard这个目录非常奇怪，自己的用户权限都没有，group和other用户却有所有权限，在sdcard目录里面建立的目录权限跟sdcard的权限一样。 3. 我们自己写的应用程序，不知道是属于什么样的权限级别，是作为什么样的用户来访问各目录包括sd卡的，手机sd卡和虚拟sd卡。","link":"/simulator-app2sd/"},{"title":"Run xgboost on Mac and Regression data","text":"The source code: xgboost_regression update at 2021-09-07: Install xgboost on Apple M1 1234567git clone --recursive https://github.com/dmlc/xgboostmkdir xgboost/my_buildcd xgboost/my_buildCC=gcc-11 CXX=g++-11 cmake ..make -j4cd ../python_package/Users/xx/miniforge3/envs/tf/bin/python setup.py install u must install miniforge for M1, conda create -n tf python=3.9.5 Run xgboost In the process of using xgboost, I encountered a small obstacle, that is, xgboost cannot be run normally on the M1 of the Mac. It needs to be tossed. The following is the installation process: 1. Homebrrew is required first 2. Install gcc and cmake 123brew install gccbrew install cmakebrew install libomp 3. Download xgboost package Yes, you cannot use the network package to install, you need to download, compile and install by yourself. Fortunately, the process is not troublesome: Source: http://mirrors.aliyun.com/pypi/simple/xgboost/ I downloaded xgboost-1.4.2.tar.gz 4. Installation Enter cd ~/download/ run 1pip install xgboost-1.4.2.tar.gz Okay, you can introduce it to try 12from xgboost import XGBClassifierxb = XGBClassifier() xgboost Regression load data 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import pandas as pdimport warnings%pylab inlinewarnings.filterwarnings('ignore')# load data from urldf = pd.read_csv('./data/Titanic.txt', sep=',', quotechar='&quot;', encoding='ISO 8859-15')df.info()df.head()# Filter some featuresfeatures = df[['pclass', 'age', 'sex']]# Labellabel = df['survived']features.info()# Missing values ​​are filled with meanfeatures['age'].fillna(df['age'].mean(), inplace=True)features.info()# Divide the datasetfrom sklearn.model_selection import train_test_splittrain_x, test_x, train_y, test_y = train_test_split(features, label, test_size = 0.25, random_state=33)# Feature vectorizationfrom sklearn.feature_extraction import DictVectorizervec = DictVectorizer(sparse = False)train_x = vec.fit_transform(train_x.to_dict(orient='record'))test_x = vec.transform(test_x.to_dict(orient='record'))# Random forest training and predictionfrom sklearn.ensemble import RandomForestClassifierrfc = RandomForestClassifier()rfc.fit(train_x, train_y)print('The accuracy of random Forest Classifier on testing set:', rfc.score(test_x, test_y))&quot;&quot;&quot;The accuracy of random Forest Classifier on testing set: 0.7781155015197568&quot;&quot;&quot;# xgboost training and predictionfrom xgboost import XGBClassifierxb = XGBClassifier()xb.fit(train_x, train_y)print(f'The accuracy:', xb.score(test_x, test_y))&quot;&quot;&quot;The accuracy: 0.7750759878419453&quot;&quot;&quot;","link":"/run_xgboost_on_M1_and_regression/"},{"title":"sketch中打开高版本文件","text":"sketch也不知道什么时候开始年费化了，也不能打开高版本文件了。（妈蛋） 据说是为了促进销量和保护版本。 打开包文件，然后打开包内的meta.json 替换头部: 1{&quot;commit&quot;:&quot;335a30073fcb2dc64a0abd6148ae147d694c887d&quot;,&quot;appVersion&quot;:&quot;43.1&quot;,&quot;build&quot;:39012 替换尾部 1&quot;commit&quot;:&quot;335a30073fcb2dc64a0abd6148ae147d694c887d&quot;,&quot;build&quot;:39012,&quot;appVersion&quot;:&quot;43.1&quot;,&quot;variant&quot;:&quot;NONAPPSTORE&quot;,&quot;version&quot;:88},&quot;version&quot;:88,&quot;saveHistory&quot;:[&quot;NONAPPSTORE.39012&quot;],&quot;autosaved&quot;:0,&quot;variant&quot;:&quot;NONAPPSTORE&quot;} 这里实际有几个key:commit, appVersion, build, version,NONAPPSTORE value替换成相应的值就OK了。","link":"/sketch-open-hight-version-file/"},{"title":"SOLOVE Air-M20000","text":"第一次为产品写评测吧我这是，也不太记得了。不过以下这个电源，觉得值得写上一篇。 「SOLOVE移动电源 Air-M20000」 照例先来几张开箱图 本来应该再早一个月拿到这款电源的，因为产能的原因，跳票了。好吧，后来又因为快递单的遗漏，跳票了更久。不过好东西都是值得等待的。 第一次打开包装盒的时候，拿在手上满满的质感，身躯娇小，容量却很大。不过说起来，因为控制了高宽的原因（高宽仅与一张信用卡大小相仿），厚度不太理想。达到了一枚1元硬币的厚度，虽然握在手上的手感十分舒服，但是如果对于想揣在衣服口袋里的人来说，可能这个厚度稍稍有点不甚让人满意，说起来，10000mAh的容量，能做到如此地步已经很不容易了。 而对于SOLOVE Air-M20000的设计，相信也是能俘获很多人的心。电源灯的效果真的是很漂亮。 也许是因为控制尺寸的原因吧，SOLOVE Air-M20000并没有像其他大容量移动电池一样配备两个输出插孔，只有一个。输入Micro-USB, 输出为USB-A. 不过联想多数时候，我的小米电源另一个输出口都空的情况，其实一个已经足够了，重点是轻便易携带。 上周五这款电源已经到货了，没有第一时间拿来写当然是为了接受下周末的检验。 实际使用中，周五的晚上电源满电，周六出门12点半到下午6点，iPhone 6从3点开始电量百分之15，接入SOLOVE，1小时后达到92%，期间不停的在发微信。然后取下电源，周六没有为SOLOVE充电，周日12点多左右出门，到下午6点多回来，iPhone 6经历了两次空电的情况。而两次SOLOVE都将电量充满。 实际使用情况下，SOLOVE的容量还是不错的，iPhone 6来回充满三到四次应该是不成问题，更重要的是，充电速度很快。在不断使用过程中，一个小时基本就可以完全充满。当然，每个不同型号的手机可能都有差别。 所以SOLOVE的Air-M20000, 无论是从设计，做工还是续航情况，都还是蛮值得入手的。唯一遗憾的点，为什么给了我一个红色的，而不是黑色或白色的。。。。","link":"/solove/"},{"title":"玩转Stable Diffusion WebUI 各类模型","text":"Stable Diffusion WebUI 最有意思的地方不是在安装好之后生成图像，而是各种各样的模型。 提前警告：如果你的硬盘空间不够大的话，还是不要随便玩模型了，随随便便就是好几 G，又得甚至于 10 多个 G。 目前我仅留了最常用的 SD V1.5 和 SD V2.1两个模型，大小为 13G。 另外还需要说明一点，就是我曾经测试过用 NAS 来存储模型使用，完全不能用，暂时没有时间具体去研究到底什么原因。只有老老实实的继续在本地硬盘上跑。所以 NAS 上存了大量模型，真需要用到的时候再复制过来。 写这篇文章也是因为近期玩模型过程中打算整理一波，一是方便自己，二么也算是对其他小伙伴做些贡献。 Stable Diffusion 各种模型层出不穷，要说完估计需要费一番功夫，所以我摒弃其他小模型，只整理收集大模型，就是 ckpt 和 safetensors。如果你也打算跟着我一起玩模型但是还未安装，可以先参看我之前的文章： 在 Apple Silicon M1/M2 Mac 上安装和运行Stable Diffusion 说实话，我找了好多关于如何在 M1/M2 上安装和运行 Stable Diffusion 的教程和帖子，发现相互之间借鉴的不少，但是能用的确实没几个。 寻找一番后，发现其实没那么复杂。也不知道为什么网上的那么多教程搞得那么复杂，又是这个又是那个的一大堆，简单实现的方式有好几种： https://www.hivan.me/How%20to%20install%20and%20run%20Stable%20Diffusion%20on%20Apple%20Silicon 还是先从最基础的模型开始： Stable Diffusion 其他多数模型基本上都是从这个基础模型上再次训练得到的。 Stable Diffusion v2.1 SDv2.1提升了人物生成能力，因为SDv2.0大量增加了风景、建筑物和动物的数据集，减少了人物的学习量。 SDv2.1提高了NSFW过滤器准确度，因为SDv2.0的成人过滤器过滤的太狠，错误判定很多 即使是极端长宽比的图像也能顺利生成。 解剖学的身体和手（特别是手掌）的描写精度提高。 512 X 512 model : stabilityai/stable-diffusion-2-1-base · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-1-base 768 X 768 model: stabilityai/stable-diffusion-2-1 · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-1 img2img model stabilityai/stable-diffusion-2-depth · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-depth 重绘model stabilityai/stable-diffusion-2-inpainting · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-2-inpainting 超分 model stabilityai/stable-diffusion-x4-upscaler · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler Stable Diffusion V 1.5 runwayml/stable-diffusion-v1-5 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main Stable Diffusion V 1.4 CompVis/stable-diffusion-v-1-4-original · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/CompVis/stable-diffusion-v-1-4-original NovelAI 大名鼎鼎的 NovelAI，属于商业泄露模型。经过人在回路精细微调，可以生成高质量的二次元图像。但是千万时刻记得这个可是商用泄露模型，要注意避免法律风险： pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/animefull-latest.tar Waifu Diffusion 基于 Stable Diffusion 模型训练得到，增加了动漫及人物训练得到的模型，基本平时各种公开场合看到 WD 就是他。 WD 和 NovelAI 模型有些同质化，但是 NovelAI 实际是商用模型泄露，在某些使用情况下是有风险的。而 WD 不是，不过也不是说他绝对安全，毕竟 WD 也使用 Danbooru 进行学习，所以如果你关心这个需要注意一点。 Waifu Diffusion V1.5 这个模型使用是需要一个 yaml 文件的，究其原因是这个模型是基于 SD V2 得出的，需要把和 Model 同名的 yaml 文件放在模型所在的文件夹下，目前 1.5 模型是 beta2 版本，持续迭代 ing… Waifu Diffusion v1.5 beta waifu-diffusion/wd-1-5-beta · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/waifu-diffusion/wd-1-5-beta VAE(1.4 VAE 通用) vae/kl-f8-anime2.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt YAML waifu-diffusion/wd-1-5-beta2 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/waifu-diffusion/wd-1-5-beta2/tree/main/checkpoints Waifu Diffusion V1.4 和 1.5 版本一样，基于 SD V2得到的，依然需要下载 yaml 文件放在 model 同文件夹下。 Waifu Diffusion V 1.4 wd-1-4-anime_e1.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e1.ckpt wd-1-4-anime_e2.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e2.ckpt VAE(1.5 通用） vae/kl-f8-anime2.ckpt · hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt YAML e2 和 e1 是通用的，但是需要改名 hakurei/waifu-diffusion-v1-4 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hakurei/waifu-diffusion-v1-4/tree/main *Elysium Anime* 生成偏真实风格的动漫图片，风格比较偏向西式，光影还不错。 模型推荐写下面这些负面提示，可有效提升质量。 1lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry Elysium_V1 偏真实风的模型，手画的还不错，模型底稿基本是以西方人为主，所以生成的脸也偏西方人。 Elysium_V1.ckpt · hesw23168/SD-Elysium-Model at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hesw23168/SD-Elysium-Model/blob/main/Elysium_V1.ckpt *SD_Elysium_Kuro_Model* 与Anything 4.0、WD 1.4等合并后经过微调的二次元用模型。已经包含 WD 的“kl-f8-anime2”VAE 文件，因此无需使用额外的 VAE 文件 Elysium_Kuro_Anime_V1.safetensors · hesw23168/SD_Elysium_Kuro_Model at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hesw23168/SD_Elysium_Kuro_Model/blob/main/Elysium_Kuro_Anime_V1.safetensors *Elysium_Anime_V3* 动漫的附加学习模型，NSFW化相当严重，有更清晰的轮廓和轻微的三维效果。基于Elysium_V1 Elysium_Anime_V3.safetensors · hesw23168/SD-Elysium-Model at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/hesw23168/SD-Elysium-Model/blob/main/Elysium_Anime_V3.safetensors *Anything系列* Anything是个神奇的二次元模型，据说是基于几十种模型融合+未知图片训练而来，随便写几个提示，就能到的不错的结果。不过这个模型整个就是一团混沌，实际训练模型，过程，方法，作者全部都是未知的。模型容易过拟合，非专业人士，请不要在此基础上训练模型。 Anything v3.0 “应该”是基于NAI模型+WD+SD等几十种模型+手部图片强化训练得出的。实际训练模型，过程，方法，作者全部都是未知的。如果没有.vae.pt，图片整体颜色浓度（饱和度）会更很浅。PS：Anything v3.0 的 .vae.pt 文件可以用于 NAI。 Anything V3.0 fp16: magnet:?xt=urn:btih/:45cd353ac4fa87098db5e3a6a349539710a3a1f5&amp;dn=Anything-V3.0-fp16.zip Anything v3.0 fp32: magnet:?xt=urn:btih/:d9db662ab5ace77004b3348c23c9381380c27156&amp;dn=Anything-V3.0-fp32.zip Anything v3.0 full-ema: magnet:?xt=urn:btih/:80460036625fb61dce4bc6e7dab744744309a2a0&amp;dn=Anything-V3.0-fullema.zip huggingface.co https://huggingface.co/Linaqruf/anything-v3-better-vae/tree/main Anything v4 自称是Anything最新版本的模型，实际一切都是未知的。仅需几个提示即可生成详细的 2D 插图的能力以及使用 danbooru 标签的能力。整体比过拟合的v3更自然，人物姿势等更容易操作。 anything-v4.0-pruned.safetensors · andite/anything-v4.0 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/andite/anything-v4.0/blob/main/anything-v4.0-pruned.safetensors Anything v4.5 貌似是Anything v4的进化，但实际一切都是未知的。比v4画风更柔和一点。 anything-v4.5-pruned.safetensors · andite/anything-v4.0 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/andite/anything-v4.0/blob/main/anything-v4.5-pruned.safetensors Zeipher 生成更符合真人解剖结构的真人模型，训练集以女性图像为主官方网站是 https://ai.zeipher.com，已经关闭。请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 F222 f222.safetensors · acheong08/f222 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/acheong08/f222/blob/main/f222.safetensors F111 f111.ckpt · Reachout/F111 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/Reachout/F111/blob/main/f111.ckpt 3DKX 因为Zeipher官方已经GG，这是热心网友创建的衍生3DKX模型如果你想让你的 3D 角色有一张更“二次元”的脸，提示词最开始写 “3d cartoon of”，或者如果你想要经典的 3D 渲染外观，写“a 3d render of”高分辨率模型，推荐分辨率为 1152 x 768 或更高 3DKX_1.0b f111.ckpt · Reachout/F111 at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/Reachout/F111/blob/main/f111.ckpt R34 从网站“rule34.xxx”的 150,000 张图像中进行训练。rule34.xxx几乎全是NSFW图片，所以你懂的 r34_e4 1.99 GB file on MEGA https://mega.nz/file/yJgDUCzA#zOD2yeE6QLBqPEjEpIi2b4FWOlb64yVUveOd_eW6teI 磁力链接：magnet:?xt=urn:btih/:ed9f0e3f849d7119107ef4e072c6abeb129e1a51&amp;dn=r34_e4.ckpt EVT pixiv排行榜模型 基于pixiv排行图片训练，夹杂有部分R18排行图片 Evt_V4_e10_ema Evt_V4_e10_ema.safetensors · haor/Evt_V4-preview at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/haor/Evt_V4-preview/blob/main/Evt_V4_e10_ema.safetensors EVT_V3 haor/Evt_V3 · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/haor/Evt_V3 EVT_V2 haor/Evt_V2 · Hugging Face We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/haor/Evt_V2 Basil_mix 逼真的真人模型，基于亚洲风格训练，支持Danbur标签提示词需要加载VAE，不然画面色彩浓度和边缘会很淡提示词应尽可能简单不要堆砌大量质量标签和负面提示，不然会适得其反。请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 basil_mix basil mix.ckpt · nuigurumi/basil_mix at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/nuigurumi/basil_mix/blob/main/basil%20mix.ckpt VAE vae-ft-mse-840000-ema-pruned.ckpt · stabilityai/sd-vae-ft-mse-original at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.ckpt Chillout Mix 逼真的真人模型，基于亚洲风格训练，支持Danbur标签提示词请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 chillout mix _ NiPruned Fp32 Fix chilloutmix_NiPrunedFp32Fix.safetensors · Inzamam567/useless_Chillout_mix at main We’re on a journey to advance and democratize artificial intelligence through open source and open science. https://huggingface.co/Inzamam567/useless_Chillout_mix/blob/main/chilloutmix_NiPrunedFp32Fix.safetensors Uber Realistic Porn Merge 如名字所说，逼真的真人Porn模型，简称 URPM 模型请不要用真人模型画明星和未成年的NSFW内容，不然你可能会遇到很麻烦的法律问题 Uber Realistic Porn Merge Uber Realistic Porn Merge (URPM) | Stable Diffusion Checkpoint | Civitai For early access builds and to support daily work on URPM, please check out my patreon! https://www.patreon.com/uber_realistic_porn_merge , or disc... https://civitai.com/models/2661/uber-realistic-porn-merge-urpm","link":"/stable-diffusion-webUI-models/"},{"title":"vue 2.0 自定义filter并挂载到全局使用","text":"其实早几天前就进入了第三部分,而因为第二部分面向对象程序设计糊里糊涂,搞得再接下来的学习里有很多实例根本看不懂,或者很难回忆起所学的知识点.不得不回头又一个字一个字的老老实实的看了一遍,并且将每个字都敲出来更新到了Sites上!至此算是比较牢固了..而更新的部分,相信对于想进入Java世界的新手们也算是一个好的参考. 其中前两部分分为八章,第一部分为基础程序设计,第二部分为面向对象程序设计 ,第三部分则是实战的Java应用程序设计.我询问过一个朋友,说是对于Android开发来说,Java的前两部分属于基础,非常重要.而第三部分也就无所谓了.不过想来,还是将所有的东西全部抓牢以后再开始.毕竟Java已经学到这一步,规规矩矩做一个Java开发者也未尝不可.下面给出Java前两部分的链接,算是再次推广我的Learn Wiki.而第三部分将不会在此更新了.有兴趣的自己跟着我在Sites上的更新翻看吧,更新速度要视我自己的学习速度而定!现在手上的教程为:《Java 开发与实战经典》 第一部分:Java基础程序设计 1.Java概述及开发环境搭建 2.简单的Java程序 3.Java基础程序设计 4.数组与方法 第二部分:Java面向对象程序设计 5.面向对象(基础篇) 6.面向对象(高级篇) 7.异常的捕获与处理 8.包及访问控制权限 第三部分将不会继续在这里给出链接.请自行查看 http://learn.hivan.me/index/home/java-learn/Java-DAP","link":"/the-third-part-of-the-java-learning/"},{"title":"vue 2.0 自定义filter并挂载到全局使用","text":"vue 2.0 开始，取消默认filter, 需要自定义。 而自定义之后每次在需要使用的组件内引用也确实蛮麻烦的。 所以我们就来将定义的filter挂载到全局使用。 vue2.0 filter相关文档 定义 引用 挂载 使用 /src/filters/ - format.js 123export default function(val){ ...} index.js 12345import format from &quot;./format&quot;;export default{ format: format,} /src/ - main.js 123456789...import commonFiltes from './filters/index'Object.keys(commonFiltes).forEach(function (key, index, arr) { Vue.filter(key, commonFiltes[key]);})... /src/components/ - xxx.vue 1234567&lt;template&gt;...&lt;div&gt;{{ data | format }}&lt;/div&gt;&lt;/template&gt;&lt;script&gt;...&lt;/script&gt;","link":"/vue2.0-custom-filter-to-global/"},{"title":"vux更改Tabbar选中状态","text":"在vux的文档和示例中，都没有明确的说明tabbar上v-model的使用 文档中将v-model说明放在了TabbarItem示例下，但是其实这个应该是放在Tabbar上 1234567891011121314151617181920212223&lt;template&gt; &lt;router-view class=&quot;view&quot; v-on:changeTab=&quot;changeTab&quot;&gt;&lt;/router-view&gt; &lt;tabbar v-model=&quot;index&quot;&gt; &lt;tabbar-item&gt;&lt;/tabbar-item&gt; ... &lt;tabbar-item&gt;&lt;/tabbar-item&gt; &lt;/tabbar&gt;&lt;/template&gt;&lt;script&gt;data(){ return{ index:0, ... }}methods:{ changeTab(num){ ... this.index = num; ... }}&lt;/scirpt&gt; 然后子组件中调用 123mounted(){ this.$emit('changeTab', 2)} 这样就便于在不同的组件内都可以更改Tabbar选中状态","link":"/vux-tabbar-selected/"},{"title":"什么是ifttt,ifttt怎么玩?","text":"原帖出自師北寰的網絡日誌，當中很多條目內容也都是來自於Twitter！由於原帖地處偏僻，需要翻山越嶺，所以為了照顧國內用戶，特轉此貼！這也是我為數不多的轉帖之一。當然，我知道，這個服務可能過段時間完全開放後又會和國內無緣！就像G+才出世兩天就被牆了一樣！不過這也充分證明了G+的優秀！而現階段，最好玩的網絡應用就是G+和ifttt。所以一般都是if oo then xx! 什么是ifttt？ ifttt是一坨网站：ifttt.com 即If This Then That，你可以在ifttt上设定一个条件，当达到你设定的一个条件时，便触发一个（你指定）动作。这里的「条件」和「动作」是指开放的互联网服务，比 如flickr，twitter，facebook，youtube等。别问我有没有新浪微博、人人网，优秀且可靠的互联网服务都在国外。 国内没法抄ifttt。ifttt最重要是服务稳定，就国内普遍鸡贼的情况来看，if端服务时不时封闭一下是常事。创业企业抄好了，大公司眼红把自己网站接口关闭再抄一个出来，创业公司就得玩完。大公司抄的话，竞争对手们也会关闭自己的服务，这玩意儿在国内没法玩。 ifttt的稳定性是关键，可以定制一连串的if…then任务出来，但如果中间某一个服务出问题，后面的任务就全失效了（当然，这么定制也挺笨的）。 ifttt非常重要的一个优点是，将常用服务（twitter，加星或分享的Google reader条目，加过标签的instagram和flickr照片）中的重要资料，全部发送到一个存储服务（Dropbox，evernote，Gmail），需要用的时候检索起来将非常方便。 ifttt可以怎么玩？ 好玩的可以有：if某女谈论「失恋」、「男友+讨厌」、「伤心」、「难过」，then 发送一条短信。ifttt泡妞必备… 非常实用的应用可以有：New fav tweets to evernote ifttt还解决了我以前在北京十分急需的一个功能：if 北京美国大使馆空气监测站的空气质量指数超过250，then 发送一条短信…当然，其它方法也能实现，但ifttt方便太多了。 未来的应用有：ifttt的出现真可以实现未来你挂了也能一个人办丧事：if三十天未发推，then启动一系列任务：1.发邮件告诉殡仪馆来收尸（亲，你可以看我的google location）；2.自动转账；3.发表遗书告诉亲友可于30天后到某处悼念；4.分享生前录好的视频，最后再操一遍GFW。 在未来，随着越来越多社交服务的出现，以及多条件任务功能的推出，玩ifttt的花样将呈爆发式增长，乐趣无穷。 ifttt还可以有什么玩法？ @mranti: ifttt应用举例：if 某男A和某女B同时check in同一个地方，then 短信我的手机：“A和B有奸情，而且正在进行”。八卦利器啊！ @hecaitou: 理想状态下的ifttt应用场景：一旦老婆的推上出现“加班”字样，立即激活一条手机短信通知。同时，自动检测谷歌日历，找出几个今晚没有事情的老友。随后，在FB上新建一个活动“今晚喝大酒”，一旦超过3人同意，触发一条订餐消息给餐厅。餐厅查询Evernote，找到这群人最喜欢的菜和酒。on Twitter: http://twitter.com/hecaitou/status/85927850749857792 @mranti: ifttt应用举例：if 明天下雨，发推DM给自己的心仪女友：“亲爱的，明天出门带伞，我是你的阳光”。 on Twitter: http://twitter.com/mranti/status/85927810924937219 最后这两条和菜头和安替的推是我在twitter fav之后，在写这篇文章过程中自动保存到evernote的，服务十分流畅，文章写起来太方便了有没有？ —-以下内容为下午四点十五分更新—- 出去溜达了一圈回来，脑子里一直在想ifttt，ifttt简直是个太科幻的产品了，第一次觉得人工智能——不对，是机器智能——离自己这么近。ifttt比Google、Facebook都要伟大得太多。 ifttt就是一个反射，它把你想象得到的任何一个动作反射为另外一个你能想象得到的动作，并且它不像生物体一样会被躯干束缚。ifttt上将出现拉马克进化？ @hecaitou: ifttt里面，如果在Channel之上，提供一个Task的自由市场。让各种Geek做出各种奇奇怪怪的Task来，用户添加Task而不是点选Channel，那就连盈利的问题都解决了。 on Twitter: http://twitter.com/wuyagege/status/85959272638324736 @mranti: 在ifttt的世界里面，各位姑娘小心了，什么恋爱短信啊、花啊、DM关怀啊、贴心礼品啊，都可能是程序的Task算出来的。而且ifttt的世界中，一个人死了，他对一个女生的关心也可以一直持续下去，仿佛天天都在。 on Twitter: http://twitter.com/mranti/status/85974845216665600 @boatman: ifttt神就神在即使被墙，只要设置好this和that的关联性，墙并无法阻止this触发that，除非GFW把所有的channel全部封锁才有可能抑制ifttt，但当ifttt支持自定义channel时，就是神也难救方滨兴。 on Twitter: http://twitter.com/Ryan_XxOo/status/85975132866220032 @Doriscafe: 我死后，请你替我照顾她。每天给她发短信叫早，订花，在推特上mention她，赞她，天气好提醒加衣，天气不好提醒带伞，请你替我照顾她，只要服务器不倒下，就直到永远。#ifttt on Twitter: http://twitter.com/Doriscafe/status/85975909429018624 @juicy_luna: 我个人觉得吧，#ifttt 就是把生物里的神经反射运用进了网路里，甚至还会扩展到物质生活。也就是说，它担负起神经链的作用，将能把一切行动串联起来，形成纵横的网络。。奇妙的世界。。 on Twitter: http://twitter.com/juicy_luna/status/85976819626549248 @duck_1984: 超级多米诺啊 蝴蝶效应啊 ifttt毁灭世界啊 自寻死路啊愚蠢的人类 还有更多⋯⋯你来补充。","link":"/what-is-ifttt-and-how-to-play/"},{"title":"Yosemite访问用户级服务器目录","text":"升级到OSX 10.10(Yosemite)以后，localhost是可以正常访问的，只是localhost/~user无法打开了，提示403错误。 网上查找资料，说是随着系统的更新，Apache本本更新到2.4.9，PHP也更新到了5.5.14，所以Apache的配置就需要做相应的修改。 首先，我们需要确定打开了Apache 1sudo apachectl start 然后设置允许访问用户目录 修改httpd.conf配置 1sudo subl /etc/apache2/httpd.conf command + f 查找代码，并去掉注释符 # 123456LoadModule authz_core_module libexec/apache2/mod_authz_core.soLoadModule authz_host_module libexec/apache2/mod_authz_host.soLoadModule userdir_module libexec/apache2/mod_userdir.soLoadModule php5_module libexec/apache2/libphp5.soInclude /private/etc/apache2/extra/httpd-vhosts.confInclude /private/etc/apache2/extra/httpd-userdir.conf 修改httpd-userdir.conf配置 1sudo subl /etc/apache2/extra/httpd-userdir.conf command + f 查找以下代码，去掉注释符# 1Include /private/etc/apache2/users/*.conf 修改yourUserName.conf配置 1sudo subl /etc/apache2/users/username.conf PS: username为你的用户名称，如果没有该文件则新建一个，然后将内容修改为: 123Options Indexes MultiViewsAllowOverride NoneRequire all granted 然后设置文件权限为755 1sudo chmod 755 /etc/apache2/users/haibor.conf 最后我们需要重启Apache 1sudo apachectl restart","link":"/yosemite-open-usersite/"},{"title":"从美国三大协会说供应链管理的演变","text":"前面说到，供应链管理从采购、运营和物流管理发展而来，它是对从供应商的供应商到客户的客户的产品流、信息流和资金流的集成管理，以实现对客户价值的最大化，以及供应链成本的最小化。企业之间的竞争不再是企业与企业之间的竞争，而是供应链与供应链之间的竞争。过去二三十年来，美国的汽车业在日本汽车大厂的强大攻势下一败涂地，就是一个供应链战胜另一个供应链的例子。 在美国，这种集成的供应链管理概念不是一蹴而就的，而是经过几十年的演进发展而来的。这里我们从采购、运营和物流职业协会的发展历史出发，阐述供应链管理在美国的发展——要知道，判断一个职能的发展，最简单的方式就是看相应行业、职业协会的发展。 简言之，供应链管理不是一个领域，而是三个。过去不是，现在不是，在可以预见的将来仍会保持在多个领域齐头并进。这从目前美国跟供应链管理相关的行业协会可见一斑。 供应管理协会（ISM） 供应管理协会是世界上规模最大、影响最大的供应管理组织，拥有四万多会员。它发布的采购经理人指数（PMI）跟踪生产、库存、订单量等变化，是美国经济的风向标，被新闻媒体、学术研究、华尔街和政府部门广泛引用。它的前身是美国采购经理联合会（NAPM）。在100多年的发展过程中，伴随着供应管理在美国公司的重要性不断提升，该协会的侧重点从采购发展到供应管理，再到供应链管理。 图1-3简单地表述了在美国，采购从采购代理发展到采购管理，再发展到供应管理的过程。最早的采购是采购代理，即内部客户确定了需求，找好了供应商，价格也往往都谈好了，采购负责签合同、下订单，把东西买回来。当时的专业协会就叫“采购代理人协会”（1915年成立）。后来，采购说，我不但可以帮你下订单，而且可以帮你找供应商、管供应商、谈价钱、谈合同，于是就变成采购管理，协会也改名“采购经理人协会”（1968年）。再到后来，采购说，我不但可以帮你找供应商、管供应商，而且可以处理运输、物流、进出口，把一切打点好，直到产品进了我们的仓库，于是就演变成了供应管理，专业组织也改名“供应管理协会”（2002年）。 但是，供应管理协会的核心仍然是采购与供应管理，而不是广义上的供应链管理。2002年前后，采购经理协会改名为供应管理协会时，有人问，为什么不改为“供应链管理协会”？ISM的答复是供应链管理太广泛，还不够定型。是的，在供应链领域，不管你问美国还是中国的职业经理人，你们的头衔是什么，答复大多是采购、运营或物流，而不是供应链管理。即使是供应链经理，他们的职责也往往侧重某个领域，并不是我们真正意义上的大供应链。但毫无疑问，从采购管理到供应管理，是向供应链管理迈进了一大步。 图1-3 采购管理发展到供应链管理 这也体现在职业认证上。供应管理协会原来的认证是注册采购经理（C.P.M.），在30多年的历史中，全球认证人数超过4万，认证内容覆盖价格、质量、交付、合同管理、供应商选择、供应商谈判、国际贸易、公司管理及人力资源管理等。为适应采购向供应管理的发展，供应管理协会在2008年推出“供应管理专业人士认证”（CPSM），以取代C.P.M.，这标志着向供应链管理更进一步。相对而言，CPSM的要求比C.P.M.更高，也反映了供应（链）管理比采购管理要求更高。[1] [1] 对于CPSM认证的细节，可阅读《供应链管理的职业认证》一文，收在我的《供应链管理：实践者的专家之路》一书中，机械工业出版社于2017年出版；或在我的“供应链管理专栏”（www.scm-blog.com）上查询同名文章。 运营管理协会（APICS） APICS是美国生产与库存管理协会的缩写。与供应管理协会侧重采购相对应，APICS历来侧重于生产与库存管理。为适应向供应链管理发展的趋势，该组织在2004年更名为APICS—运营管理协会（具体如图1-4所示）。考虑到APICS在美国乃至世界的影响，运营管理协会仍旧保留APICS字眼，也显示不放弃在生产与库存控制方面的传统优势。 图1-4 APICS更名，进入供应链管理时代 在美国宏观经济中，APICS的地位没有供应管理协会高，比如没有像供应管理协会的PMI那样有影响力的宏观经济指数。但在生产与库存管理领域，APICS享有崇高的声望。它的生产与库存管理认证（CPIM）在生产企业受到普遍重视。其认证内容侧重于生产的计划、控制和实施，即如何把销售计划转变为需求计划、生产主计划，然后细化到生产计划、物料供应计划，再到生产线的排程和控制，内容包罗万象。自1973年首次推出以来，全球总共有10.7万人得到此项认证。[1] 伴随着更名，APICS在2005年推出“供应链专业人士认证”（CSCP）。从字面上看，该认证针对供应链管理；从内容上看，该认证试图覆盖供应管理的三大范畴（采购与供应管理、生产运营管理、物流管理）；从级别上看，该认证比CPIM高，它要求一定年限的相关工作经验，而CPIM则没有。在过去10多年来，该认证与CPIM并存，可视作CPIM向供应链管理的延伸，以实现APICS在供应链管理领域与供应管理协会（ISM）两分天下的目的。 2014年4月30日，APICS宣布与Supply Chain Concil（SCC）合并。这种合并，一方面是为了抱团取暖—2008年金融危机以来，经济低迷，美国专业协会的经费大减；另一方面是为了挽救运营管理在美国日益衰落的局面——外包盛行，供应链全球化下，很多公司的生产制造被外包给低成本国家的供应商，原来由运营部门负责的任务转移到采购部门，原来的生产、库存、计划等专业人士也纷纷转业，而以这些人为基础的运营管理协会也就每况愈下。 这从他们的董事会成员可以看出：APICS的董事会成员中（2018年），12个董事会成员，只有两位是《财富》500强的副总，其余大多是些总监、经理、顾问[2]——生产外包到别的国家，有些生产运营的专家就只能转入咨询业。作为对比，看看其竞争对手供应管理协会的董事会，13个董事会成员中，只有一两个不是《财富》500强的副总裁、首席采购官级别（2018年）[3]。我们说这些，并不是某相声中说的，两个人拿名片打牌，看谁的名片上的头衔大；而是说，这些职业协会的董事会大部分是志愿者，大企业的高管们是否愿意把自己的时间贡献出来，从侧面反映了一个协会在行业的影响力。 好消息是，运营管理协会APICS这些年来继续与别的职业协会合并。2015年7月，它与原来的美国运输和物流协会合并，算是正式跨入物流领域，也给它的认证库里增加了一个新成员：运输与物流认证（CTL）。通过这一系列的兼并，运营管理协会可以说成为美国三大供应链职业协会中涉猎范围最广的一个。当然，兼收并蓄的风险呢，也是可能变成大而杂，丧失聚焦点，变成三不像。 [1] CPIM Transformed for Today’s Busy Professionals. APICS网站，www.apics.org. [2] 2018 APICS Board of Directors. APICS网站，www.apics.org. [3] ISM Board of Directors. ISM网站，www.instituteforsupplymanagement.org. 供应链管理专业人士协会（CSCMP） 这是美国第三个与供应链管理相关，也有相当影响力的协会。它的前身是物流管理协会（CLM）。顺应物流管理向供应链管理的过渡，供应链管理专业人士协会（CSCMP）试图从物流管理的角度出发，来“蚕食”供应链管理这一热门行业。 物流管理协会的影响更多是在物流教育领域，这从他们的主席人选可见一斑：物流专业的一些知名教授曾担任过该组织的主席，例如国内熟悉的鲍尔索克斯（密歇根州立大学教授，1964~1965年任主席）、门泽尔（田纳西大学教授，2000~2001年任主席）等。 相信在短时间内，供应管理专业人士协会很难大幅增加在工业界的影响，也很难成为一个纯粹意义上的供应链管理协会。这从该协会的董事会可见一斑：15位董事会成员中，6位是教授或来自大学。作为对比，运营管理协会APICS的董事会只有1位教授，而供应管理协会ISM则是清一色的大企业高管。 在发展历史上，供应链管理专业人士协会很好地诠释了从小到大、从部分到全部的发展历程。如图1-5所示，1963年，美国实物配送协会成立，表明最早的物流以运输为主，简单地说就是管着一帮卡车司机，做着把东西从A点搬送到B点的“实物配送”。[1]到了20世纪80年代，这些人说，我们不但可以把东西从A点搬到B点，而且可以对付整个过程的仓储、配送、海关等多道手续，以及伴随而来的信息流，这就变成了物流管理。于是在1985年，实物配送协会改为物流管理协会，覆盖运输以外的更多业务。再后来，物流管理说，我们也可以对付采购、运营的事啊。得，这就变成了供应链管理—2004年，物流协会改名供应链管理专业协会，正式从物流跨入供应链领域。不过对我们供应链领域的人而言，一看你是供应链管理职业人协会的，就知道你的前世今生是物流。 图1-5 从实物配送到物流管理到供应链管理 从认证角度而言，长期以来，这个机构一直没有能与供应管理协会和运营管理协会相匹敌的认证。最近注意到，供应链管理专业人士协会推出了SCPro的认证。整个认证分为三级：第一级是供应链管理核心知识，覆盖供应链管理领域的八个方面；第二级是供应链挑战的分析与实施，基于案例来测试学员对供应链管理知识的应用能力；第三级是供应链转型，需要在学术机构导师的指导下，分析具体企业的真实状况，规划一个供应链改进项目，来取得真实的业务成果，比如提高投资回报率、缩短周转周期等。[2]这个认证目前在国内还没有看到，在美国的影响也尚需建立。[3] 从上述美国三大职业协会的发展可以看出，在可预见的未来，供应链管理仍将以一个综合领域的面目存在，在采购、运营和物流的基础上继续发展。但集成的趋势很明显，不但在行业协会，而且在工业界、教育界。 很多公司在集成采购、运营和物流管理三个部门，设立全球供应链部。ERP软件提供者如SAP、Oracle促进了这一趋势：它们的软件使跨职能协作更加容易。学术界也有越来越多的系、专业改名为供应链管理。美国MBA排名中也增设了供应链管理/物流管理专业，与传统的会计、金融、营销、国际管理等分庭抗礼，说明供应链管理作为一个专业已经形成。 资源 CPSM认证由美国供应管理协会提供，详情见www.ism.ws CPIM和CSCP认证由美国运营管理协会提供，详情见www.apics.org 这些认证在国内都可以参加，其中CPSM已经汉化。 延伸阅读 《供应链管理在国内的发展》，节选自我的另一本书《供应链管理：实践者的专家之路》。十几年前，我在申请北美商学院时，第一次听说供应链管理；七八年前，国内的一些大型企业启动供应链转型；最近几年，越来越多的中小企业着眼供应链，解决日益严峻的成本和库存问题。扫描二维码，阅读全文。 [1] 有个学术刊物，名字叫International Journal of Physical Distribution &amp; Logistics Management，翻译过来就是《实物配送和物流管理国际学刊》。从1970年创刊至今，都快半个世纪了，还看不到一点要寿终正寝的样子。 [2] 见供应链管理专业人士协会的网站：http://cscmp.org [3] 判断一个认证的价值，最简单的就是看招聘网站上，有多少职位要求或者建议应聘者有这个认证。我到Monster.com（这是美国的一个主要招聘网站）上，搜索SCPro认证，只发现两个职位；搜索ISM的CPSM认证，发现487个岗位；搜索APICS的CPIM认证，出来700个岗位；搜索CSCP，有358个岗位（2018年10月10日，搜索时不限职位所在的地域）。再搜索“注册供应链管理师”CSCM认证，发现17个岗位—这个认证最近突然在国内冒起来，到处都有人在宣传，说是国家人力资源和社会保障部认可的，引得很多读者三天两头到我这里求证，问这个认证是不是主办者宣称的那样，是个美国主流认证，这里算是一并答复。","link":"/%E4%BB%8E%E7%BE%8E%E5%9B%BD%E4%B8%89%E5%A4%A7%E5%8D%8F%E4%BC%9A%E8%AF%B4%E4%BE%9B%E5%BA%94%E9%93%BE%E7%AE%A1%E7%90%86%E7%9A%84%E6%BC%94%E5%8F%98/"},{"title":"15. 使用LLMChain连接Google和计算器","text":"大家好，我是茶桁. 在上一节课中，我们学习了如何使用LangChain这个Python包链式调用OpenAI的API。通过链式调用，我们可以将需要多轮询问AI才能解决的问题封装起来，将需要多轮自然语言调用才能解决的问题变成一个函数调用。 然而，LangChain对我们的帮助远不止于此。最近，ChatGPT发布了Plugins插件机制。通过Plugins，ChatGPT可以浏览整个互联网，还可以接入诸如Wolfram这样的科学计算工具，能够解决许多大语言模型难以解决的问题。不过，这是需要Plus用户才可享用的，并且每一个小时内的对话Token都是有限制的。 但是，这并不重要，我们通过LangChain也能实现类似的功能。在今天的课程中，我们将继续深入挖掘Langchain，看看它如何解决这些问题。 解决 AI 数理能力的难题 虽然许多人发现 ChatGPT 在回答各种问题时表现得很好，但是当涉及到计算三位数乘法时，它就显得有些力不从心了。它似乎只是快速估算一个数字，而不是真正准确计算。为了解决这个问题，我们需要进一步研究 AI 数学能力的提升。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E4%BD%BF%E7%94%A8LLMChain%E8%BF%9E%E6%8E%A5Google%E5%92%8C%E8%AE%A1%E7%AE%97%E5%99%A8/"},{"title":"使用Python库unstructured揭秘文本数据","text":"在数据的世界里，文本数据是特别复杂的。它不像数字数据那样被分成整齐的行和列。作为一个副业，我正在开发自己的个人人工智能助手。其目的是利用我的笔记和文件中的数据来回答我的问题。重要的好处是，所有的数据处理都将在我的电脑上进行，确保没有文件被上传到云端，而且我的文件将保持隐私。 为了处理这种非结构化的数据，我发现unstructured的Python库非常有用。它是一个灵活的工具，可以处理各种文档格式，包括Markdown、、XML和HTML文档。 从unstructured的开始 你可以通过以下方式轻松安装该库： 1pip install unstructured 装载和分割文件 你想对你的文件做的第一件事是把它分割成更小的部分或章节。这个过程被称为分区，使其更容易分类和提取文本。 以下是你如何做的： 123from unstructured.partition.auto import partitionelements = partition(filename=&quot;example-docs/note.md&quot;) example-docs/note.md： 123## My test titleAnd here is a sample text. 当我们分割一个文档时，输出是一个文档元素对象的列表。这些元素对象代表了源文档的不同组成部分。unstructured库支持各种元素类型，包括Title, NarrativeText, 和ListItem。要访问元素类型，你可以使用category方法： 1234for element in elements: print(f&quot;{element.category}:&quot;) print(element) print(&quot;\\n&quot;) Output: 123456TitleMy test titleNarrativeTextAnd here is a sample text. 文档元素的列表可以用convert_to_dict函数转换为字典的列表： 123from unstructured.staging.base import convert_to_dictdict_data = convert_to_dict(elements) Output: 1234567891011121314151617181920[{'type': 'Title', 'coordinates': None, 'coordinate_system': None, 'layout_width': None, 'layout_height': None, 'element_id': 'a3114599252de55bea36c288aa9aa199', 'metadata': {'filename': 'sample-doc.md', 'filetype': 'text/markdown', 'page_number': 1}, 'text': 'My test title'}, {'type': 'NarrativeText', 'coordinates': None, 'coordinate_system': None, 'layout_width': None, 'layout_height': None, 'element_id': '6e78562ede477550604528df644630e8', 'metadata': {'filename': 'sample-doc.md', 'filetype': 'text/markdown', 'page_number': 1}, 'text': 'And here is a sample text.'}] 但由于我想把这些文本块存储在数据库中，并对数据进行一些探索性分析，所以我用convert_to_dataframe函数把文本元素转换成pandas数据框架： 123from unstructured.staging.base import convert_to_dataframedf = convert_to_dataframe(elements) 获取元数据 unstructured库的一个整洁的特点是它如何跟踪它从文档中提取的元素的各种元数据。例如，你可能想知道哪些元素来自哪个页码。你可以像这样提取某个文档元素的元数据： 12doc_metadata = elements[0].metadata.to_dict()print(doc_metadata) Output: 1{'filename': 'note.md', 'filetype': 'text/markdown', 'page_number': 1} 当源文件中的信息可用时，所有文件类型都会返回以下元数据字段：filename、file_directory、date、filetype和page_number。 筹备Transformers 当你准备将你的文本送入转化器模型进行进一步处理时，你可以使用stage_for_transformers函数。这个函数通过将你的文本元素分割成适合模型注意力窗口的大块来准备。 在下面的例子中，我使用了一个叫做SentenceTransformers的库： 12345from sentence_transformers import SentenceTransformerfrom unstructured.staging.huggingface import stage_for_transformersmodel = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)chunked_elements = stage_for_transformers(elements, model.tokenizer) And now I can load all the notes in a specific directory, so I can convert them to embedding vectors later: 1234567all_elements = []root_dir = '/corpus'for directory, subdirectories, files in os.walk(root_dir): for file in files: full_path = os.path.join(directory, file) all_elements += partition(filename=full_path) unstructured 的局限性 这个库也有一些问题和限制。 当加载和解析docx文件时，它不能正确地将子弹头识别为ListItem，大多数情况下将它们标记为NarrativeText或Title。这使得标题识别也不可靠，因为当你查看输出时，你无法确定每个标题实际上是一个标题还是一个被错误地标记为标题的列表项。(issue on github) 当处理大型文档时，没有办法知道每个段落或标题的父类是什么。这可能是一个非常有用的功能，特别是在将数据反馈给LLM的时候。 (issue on github) 替代品 在玩了unstructured之后，我试图看看是否有更好的替代品可以用python来阅读文档。虽然我需要加载各种格式的文件，但我缩小了搜索范围，首先找到阅读docx文件的替代品（因为这是你从Google Drive下载一大文件夹的文件时得到的格式）。以下是我找到的东西： python-docx 它看起来很强大，但操作起来很复杂。 我试着加载和解析了几个docx文件。我遇到的最大问题是加载任何包含超链接的文本。由于某种未知的原因，超链接的文本在最后的输出中被返回为空。这使得它不能用于我的目的，因为链接文本提供了文本中的宝贵信息。 优点：它能够为标题提供标题级别的信息（如Heading 1、Heading 2等）。 docx2txt 它在hood下使用 python-docx。 只返回加载的文档的一个巨大的全文字符串。这就要求我把我的文档分割成有意义的小块，这可不是一件容易的事。 优点：它对超链接没有任何问题，而且输出的文本是可读的、有用的。 优点：它也非常容易使用。 simplify_docx 它在 python-docx 的基础上工作。 这个库基本上将python-docx的复杂输出转换为更容易使用的json输出。 它对超链接也有同样的问题，当段落中有一个链接时，会返回空文本。 所以我现在会继续使用unstructured。值得一提的是，使用LangChain或其他类似的工具可以更容易地完成这一点。然而，我建立这个个人AI助手的部分动机是学习之旅。通过使用unstructured加载文档和其他类似工具进行嵌入等，我对底层流程有了更深的了解，而不是使用LangChain这样的一站式解决方案。 我将在未来的文章中分享更多关于我在构建个人人工智能助手方面取得的进展，敬请关注「坍缩的奇点」， 或到外网关注「茶桁- MAMT」。","link":"/%E4%BD%BF%E7%94%A8Python%E5%BA%93unstructured%E6%8F%AD%E7%A7%98%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE/"},{"title":"使用 Transformers 进行语音转文本的完整入门指南","text":"我与音频数据打交道的次数比我意识到的要多得多。 世界上充满了音频数据和亟待解决的相关问题。我们可以使用机器学习来解决其中的许多问题。您可能对用于训练机器学习模型的图像、文本和表格数据以及用于解决这些领域问题的机器学习并不陌生。随着Transformer架构的出现，解决音频相关问题的准确性大大高于之前已知的方法。我们将学习音频ML的基础知识，使用变压器将语音转换为文本，并学习使用Huggingface库通过机器学习解决音频相关问题。 了解音频机器学习的基础知识并获得相关背景知识。 了解如何为机器学习收集、存储和处理音频数据。 了解一项常见且有价值的任务：使用机器学习将语音转换为文本。 了解如何使用Huggingface工具和库来完成音频任务--从寻找数据集到训练模型，并使用它们利用Huggingface Python库通过机器学习解决音频问题。 本文作为AI系列文章的附加部分，但是并不放入系列之内，以保证其整体性。 自 2010 年代初期深度学习革命发生以来，AlexNet 在识别物体方面超越了人类的专业知识，Transformer 架构可能是自那时以来最大的突破。Transformers 使以前无法解决的任务成为可能，并简化了许多问题的解决方案。虽然它最初的目的是为了在自然语言翻译中获得更好的结果，但很快它不仅被应用于自然语言处理中的其他任务，而且还被跨领域应用——ViT或视觉变压器用于解决与图像相关的任务，决策变压器用于决策强化学习代理中的制作，最近一篇名为 MagViT 的论文演示了 Transformer 在各种视频相关任务中的使用。 这一切都始于现在著名的论文《Attention is All You Need》，该论文介绍了导致Transformers 诞生的注意力机制。本文并不假设您已经了解 Transformers 架构的内部工作原理。 尽管在公共领域和普通开发人员领域，ChatGPT 和 GitHub Copilot 是非常著名的名字，但深度学习已经在许多领域的许多实际用例中使用——视觉、强化学习、自然语言处理等。 近年来，我们了解了许多其他用例，例如药物发现和蛋白质折叠。音频是深度学习尚未完全解决的迷人领域之一；从某种意义上说，Imagenet 数据集中的图像分类是通过卷积神经网络解决的。 我假设您有使用 Python 的经验。基本的Python知识是必要的。您应该了解库及其常见用法。 我还假设您了解机器学习和深度学习的基础知识。 不需要具备Transformers 知识，但会有所帮助。 关于音频数据的注意事项：该平台不支持插入音频，因此我创建了一个包含所有代码和音频数据的 Colab 笔记本。你可以在这里找到它。在Google Colaboratory中启动它，您可以从笔记本上播放浏览器中的所有音频。 您可能已经见过音频 ML 的实际应用。说“Hi, Siri”或“Okay, Google”就会启动各自平台的助手——这就是与音频相关的机器学习的实际应用。这种特殊的应用被称为“关键字检测”。 但在这个领域中，使用 Transformer 很有可能解决许多问题。但是，在开始使用 Transformer 之前，让我快速告诉您在 Transformer 之前如何解决与音频相关的任务。 在《Transformers 》出现之前，音频数据通常被转换为梅尔谱图——描述手头音频剪辑的图像，并将其视为一幅图像并输入卷积神经网络进行训练。在推理过程中，音频样本首先被转换为梅尔谱图表示，CNN 架构将基于此进行推理。 现在我将快速向您介绍“librosa”Python 包。这是一个处理音频数据非常有用的包。我将生成一个梅尔光谱图，让您了解它们的外观。您可以在网上找到librosa 文档。 首先，通过从终端运行以下命令来安装 librosa 库： 1pip install librosa 然后，在您的笔记本中，您必须像这样简单地导入它： 1import librosa 我们将使用与库捆绑在一起的一些数据来探索该库的一些基本功能。 1array, sampling_rate = librosa.load(librosa.ex(&quot;trumpet&quot;)) 我们可以看到librosa.load()方法返回一个音频数组以及喇叭声音的采样率。 12345import matplotlib.pyplot as pltimport librosa.displayplt.figure().set_figwidth(12)librosa.display.waveshow(array, sr=sampling_rate) 这会将音频数据值绘制成如下图： 在 X 轴上，我们看到时间，在 Y 轴上，我们看到剪辑的幅度。通过以下方式收听： 123from IPython.display import Audio as audaud(array, rate=16_000) 您可以在我为此博文创建的Colab 笔记本中聆听声音。 使用 librosa 直接绘制梅尔谱图。 1234567891011121314151617import numpy as npS = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=128, fmax=8_000)S_dB = librosa.power_to_db(S, ref=np.max)plt.figure().set_figwidth(12)librosa.display.specshow(S_dB, x_axis=&quot;time&quot;, y_axis=&quot;mel&quot;, sr=sampling_rate, fmax=8000)plt.colorbar() 我们使用梅尔谱图而不是其他表示形式，因为它比其他表示形式包含更多的信息——一条曲线中的频率和幅度。您可以访问有关 Analytics Vidhya 的这篇精彩文章，了解有关频谱图的更多信息。 这正是 Transformer 之前的音频 ML 中的大量输入数据的样子，用于训练卷积神经网络。 正如《Attention is All You Need》论文中介绍的那样，注意力机制成功地解决了与语言相关的任务，因为从高层次来看，注意力头在预测下一个序列时决定序列的哪一部分比其他部分更值得关注令牌。 现在，音频是序列数据的一个非常合适的例子。音频自然是由自然界或我们的语音器官（例如人类语音或动物声音）的振动产生的连续信号。但计算机既不能处理也不能存储连续数据。所有数据都是离散存储的。 音频的情况也是如此。仅存储特定时间间隔的值；这些功能足以听歌、看电影以及通过电话或互联网与我们自己交流。 变压器也处理这些数据。 就像NLP（自然语言处理）一样，我们可以根据不同的需求使用不同架构的Transformer。我们将使用编码器-解码器架构来完成我们的任务。 如前所述，我们将在每个流程步骤中使用 Huggingface 库。您可以导航到 Huggingface 数据集中心来查看音频数据集。我们将在这里计算的数据集是 MINDS 数据集。它是来自不同语言的说话者的语音数据的数据集。数据集中的所有示例都带有完整注释。 让我们加载数据集并对其进行一些探索。 首先，安装 Huggingface 数据集库。 1pip install datasets pip install 确保我们下载的数据集库增加了对音频相关功能的支持。 然后我们探索 MINDS 数据集。我强烈建议您浏览数据集的Huggingface 页面并阅读数据集卡。 在 Huggingface 数据集页面上，您可以看到数据集具有非常相关的信息，例如任务、可用语言和使用数据集的许可证。 现在我们将加载数据并了解更多信息。 123456from datasets import load_dataset, Audiominds = load_dataset(&quot;PolyAI/minds14&quot;, name=&quot;en-AU&quot;, split=&quot;train&quot;)minds = minds.cast_column(&quot;audio&quot;, Audio(sampling_rate=16_000)) 请注意数据集的加载方式。名字在前，我们只对澳大利亚口音英语感兴趣，我们只对训练分组感兴趣。 在输入训练或推理任务之前，我们希望所有音频数据具有相同的采样率。这是通过代码中的“Audio”方法完成的。 我们可以研究个别例子，如下所示： 12example = minds[0]example {‘path’: ‘/root/.cache/huggingface/datasets/downloads/extracted/a19fbc5032eacf25eab0097832db7b7f022b42104fbad6bd5765527704a428b9/en-AU~PAY_BILL/response_4.wav’,‘audio’: {‘path’: ‘/root/.cache/huggingface/datasets/downloads/extracted/a19fbc5032eacf25eab0097832db7b7f022b42104fbad6bd5765527704a428b9/en-AU~PAY_BILL/response_4.wav’,‘array’: array([2.36119668e-05, 1.92324660e-04, 2.19284790e-04, …,9.40907281e-04, 1.16613181e-03, 7.20883254e-04]),‘sampling_rate’: 16000},‘transcription’: ‘I would like to pay my electricity bill using my card can you please assist’,‘english_transcription’: ‘I would like to pay my electricity bill using my card can you please assist’,‘intent_class’: 13, ‘lang_id’: 2} 这很容易理解。它是一个带有级别的 Python 字典。我们已经存储了路径和采样率。查看字典中的转录键。当我们对自动语音识别感兴趣时，它包含标签。[“audio”][“aray”]包含我们将用于训练或推断的音频数据。 我们可以轻松收听任何我们想要的音频示例。 123from IPython.display import Audio as audaud(example[&quot;audio&quot;][&quot;array&quot;], rate=16_000) 您可以在Colab Notebook中收听音频。 现在，我们清楚地了解数据的外观及其结构。我们现在可以继续从自动语音识别的预训练模型中进行推断。 Huggingface hub 有许多模型，可用于各种任务，如文本生成、摘要、情感分析、图像分类等。我们可以根据我们想要的任务对中心中的模型进行排序。我们的用例是语音到文本，我们将探索专门为此任务设计的模型。 为此，您应该导航到https://huggingface.co/models，然后在左侧边栏上单击您想要的任务。在这里，您可以找到可以立即使用的模型，或者找到一个很好的候选模型来微调您的特定任务。 在上图中，我已经选择了自动语音识别作为任务，并且我得到了右侧列出的所有相关模型。 注意不同的预训练模型。像 wav2vec2 这样的一种架构可以有许多针对特定数据集进行微调的模型。 您需要进行一些搜索并记住可用于使用该模型或微调的资源。 我认为Facebook 的wav2vec2-base-960h将适合我们的任务。我再次鼓励您访问模型页面并阅读模型卡。 Huggingface 有一个非常友好的 API，可以帮助完成各种与 Transformer 相关的任务。 之前，我们找到了任务所需的模型，现在我们将其与上一节中看到的 Pipeline 方法一起使用。 首先，安装 Huggingface 变压器库。 1pip install transformers 然后，导入 Pipeline 类并选择任务和模型。 1234567from transformers import pipelineasr = pipeline(&quot;automatic-speech-recognition&quot;, model=&quot;facebook/wav2vec2-base-960h&quot;)print(asr(example[&quot;audio&quot;][&quot;example&quot;])) # example is one example from the dataset 输出是： 1{'text': 'I WOULD LIKE TO PAY MY ELECTRICITY BILL USING MY CAD CAN YOU PLEASE ASSIST'} 您可以看到这与我们上面看到的注释非常匹配。 这样，您就可以从任何其他示例中得到推论。 在本指南中，我介绍了音频数据处理和探索的基础知识以及音频机器学习的基础知识。在简要讨论用于音频机器学习的 Transformer 架构之后，我向您展示了如何在 Huggingface 中心使用音频数据集以及如何通过 Huggingface 模型中心使用预训练模型。 您可以使用此工作流程解决许多与音频相关的问题，并通过利用变压器架构来解决这些问题。 音频机器学习涉及通过机器学习技术解决音频领域现实世界中出现的与音频相关的问题。 由于音频数据存储为数字序列，因此可以将其视为与序列相关的问题，并使用我们已有的用于解决其他序列相关问题的工具来解决。 由于 Transformer 成功解决了与序列相关的问题，我们可以使用 Transformer 架构来解决音频问题。 由于语音数据和音频数据通常由于年龄、口音、说话习惯等因素而存在很大差异，因此针对特定数据集使用微调的解决方案总是更好。 Huggingface 拥有许多与音频相关的解决方案，涉及数据集、训练模型以及使用和调整训练和微调的简单方法。 Huggingface Audio ML 课程，了解有关音频机器学习的更多信息 Allen Downey 的《Think DSP》深入研究数字信号处理 Q1. 什么是音频机器学习？ 答：音频机器学习是使用机器学习技术解决与音频数据相关的问题的领域。示例包括：通过关键字检测打开和关闭智能家居中的灯，通过语音转文本向语音助手询问当天的天气等。 Q2。如何收集机器学习的音频数据？ 答：机器学习通常需要大量数据。要收集音频机器学习的数据，必须首先决定要解决什么问题。并收集相关资料。例如，如果您正在创建一个名为“Jarvis”的语音助手，并希望用“Good day, Jarvis”这句话来激活它，那么您需要收集来自不同地区、不同年龄、属于不同国家的人说出的这句话。多种性别 - 并使用适当的标签存储数据。在每个音频任务中，标记数据非常重要。 Q3。什么是机器学习中的音频分类？ 答：音频分类是一项机器学习任务，旨在将音频样本分类为一定数量的预定类别。例如，如果在银行部署音频模型，则可以使用音频分类根据客户的意图对来电进行分类，以将呼叫转发到适当的部门（贷款、储蓄账户、支票和汇票、共同基金） ， ETC。","link":"/%E4%BD%BF%E7%94%A8Transformers%E8%BF%9B%E8%A1%8C%E8%AF%AD%E9%9F%B3%E8%BD%AC%E6%96%87%E6%9C%AC/"},{"title":"14. 使用链式调用简化多步提示语","text":"Hi, 大家好，我是茶桁。 OpenAI 的大语言模型提供了 Completion 和 Embedding 两个核心接口。 我们可以通过增加提示语（Prompt）历史记录来提高模型的回答准确性和自然性。还可以将 Embedding提前索引好存起来，以此做到让AI根据外部知识来回答问题， 在我们多次与AI对话的过程中，讲AI返回的答案放在新的问题里，那么我们就可以让AI帮主我们给自己的代码撰写单元测试了。 以上这些方法是自然语言类应用中常见的模式。为了方便应用开发者使用这些模式，开源社区开发了名为 Langchain 的开源库，使用 Langchain，我们可以更加快速地实现之前利用大语言模型实现过的功能，并且可以更好地将模型集成到我们的业务系统中，实现更加复杂、有价值的功能。 何谓链式调用 在第 11 讲中，我们学习了 llama-index 的使用，并在此过程中已经安装了 Langchain。虽然 Langchain 也有类似 llama-index 的功能，但这不是 Langchain 的主要卖点。Langchain 带来的第一个主要优势就在于它的名字，也就是链式调用。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E4%BD%BF%E7%94%A8%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%E7%AE%80%E5%8C%96%E5%A4%9A%E6%AD%A5%E6%8F%90%E7%A4%BA%E8%AF%AD/"},{"title":"供应链的根本是协作，那为什么不协作","text":"我们知道，供应链是采购把东西买进来，生产来加工增值，物流负责配送给客户。自从有现代企业以来，就有人做采购，有人做生产运营，有人做物流配送。也就是说，采购、运营和物流管理由来已久，至少有上百年的历史，那为什么供应链管理是个新概念，直到20世纪80年代才出现？根本原因：单一指标驱动下，职能之间山头林立，协作度低，形不成供应链。让我细细道来。 职能之间山头林立，协作度低，形不成供应链 传统模式下，企业是职能导向，驱动员工行为的是职能目标，即自上而下的目标。比如对采购来说，就是采购价格最低，因为老板对采购的最大要求就是省钱；对生产来说，就是产能利用率最高，这也是老板最为关注的。你知道，价格没有最低，只有更低，如果牺牲交付、质量和服务的话。生产也类似：为了提高产能利用率，降低单位生产成本，那就减少换线，以延长交付周期，牺牲交付绩效为代价。 也就是说，在传统模式下，驱动员工行为的是竖向的效率型指标（比如成本更低、产能利用率更高），缺乏横向的服务型指标（比如交付更快、质量更好）。在绩效考核的驱动下，每个职能关注的重点是顶头上司，而不是兄弟职能的诉求，这样，职能与职能之间就串不起来，形不成供应链。这也是职能之间协作度低的根本原因。 这也是为什么兄弟职能之间经常互相挖坑，不管是自觉还是不自觉地；而上下级之间就很少互坑，协作也容易多了。这里的关键是强相关的指标：上级的目标百分之百传递给下级，下级的绩效也百分之百汇总给上级，大家是同一条绳子上的蚂蚱。但跨职能之间则不是，职能之间的横向联系不够强。也就是说，纵向指标之下，你关注的是来自上司的需求；横向指标缺失，你就不会那么关注兄弟职能，也就是内部客户的诉求。结果就是山头林立，局部优化盛行。 当然，有人会说，我也有横向指标啊，比如交付和质量。你当然有，但问题是，把你敲得满头是包的，是没完成你老板的事，还是兄弟职能不满意？或者说，让你晚上睡不着觉的，是你老板的事，还是兄弟职能的？如果是前者的话，说明横向的质量、交付等指标还是不够强大；支配我们行为的，还是自上而下的指标。 而解决方案呢，就是建立强相关的横向指标，让职能之间有类似于上下级之间的强联系。对于每个具体的职能、具体的员工来说，他们不但要有纵向的效率指标（比如成本、产能利用率、库存周转率），还要有横向的服务指标（比如按时交货率、质量合格率），这是一对表面矛盾，但实际相辅相成的指标[1]，是打破职能壁垒，促进跨职能协作，形成供应链的关键。 这道理不难，你天生就懂，因为兄弟职能不配合，你的第一大招就是想方设法让对方“背指标”，背的就是横向的服务型指标。那为什么总是给“背”不上呢？或者说名义上“背”上了，实际上却没有呢？ 这里的根本原因有二：其一，横向指标在绩效考核中权重太低，没法起到“强相关”的作用，引导员工的行为改变。其二，企业的管理精细度不够，没法有效客观量化横向指标——没法客观量化，就不知道；不知道，就没法管理，还是没法“强相关”。 第一个原因无须多言。对于第二个原因，让我们拿供应商的按时交货率为例来说明。之所以用按时交货率，是因为这是所有横向指标中最为直观，也最为简单明了的：要么按时，要么不按时，有什么可争辩的？且慢，这问题远没有那么简单。 先说什么是按时。计划说，按时就是能够满足客户需求，也就是说，以客户的需求日期为基准。采购马上就有异议：供应商的正常交期是30天，客户的需求日期只给3天，这怎么能做到呢？不公平。那什么叫公平呢？采购就说按照供应商的正常交期，要么是合同约定，要么是报价时约定。这时候销售、计划、生产马上“跳”起来了：这世界不是个完美的世界，如果客户每次都给我们足够的响应周期，那还要采购干什么？ 就这样，计划基于需求日期，3月1日就要货；采购基于标准交期，3月31日才交付。两个极端，对另一方都不公平。不公平就没有约束力——连法律都有规定，强迫签订的不公平合同不具法律效力。那什么叫公平？供应商说，我理解你们3月1日要货，但物理定律没法违背：车工需要x天，铣工需要y天，最后的精加工需要z天，也就是说，最快也是3月15日，否则要货没有，要命一条。就这样，计划、采购和供应商三方达成一致，3月15日就成为按时不按时的标准，也就是说，基于供应商的承诺日期。 这道理很简单，是不是？没错，对于一个具体的订单来说，这是不难。但想想看，一个公司，每天动辄有几十几百个订单，每个订单都这么来回拉锯，达成三方一致，可不是件容易的事。好不容易达成一致，第二天需求变了，得，又得重新来一次。这工作量有多大，离开电子商务的支持，简直不可想象。 在电子商务发达的企业，采购订单由ERP自动生成，发给供应商；供应商确认交期、数量、单价，通过电子商务传递给采购方；如果供应与需求匹配，这就作为供应商的承诺写入ERP，成为后续判断是否按时的标准，不需要任何人工介入；如果不匹配，系统会自动提醒供应商做出更好的承诺；还不够好的话，采购员、物控员、催货员就人工介入，打电话，发邮件，找老板，督促供应商改进交付，直到供应商做出三方能够达成一致的承诺。第二天需求变了，这样的流程就重来一次。 看得出，有电子商务支持的话，百分之八九十的事儿由信息系统做了，员工只是负责那5%、10%的例外；没有电子商务的话，员工就不得不把所有的情况都当例外，我还没见过一个公司，能人工确认每个订单的供应商承诺日期，并随着需求日期的更新而更新。遗憾的是，大多数企业都没有这样的电子商务，所以就没有能力做精、做细，在订单和料号层面客观统计按时交货率。 没有三方认可的承诺日期，计划就基于需求日期统计，按时交货率自然很差；采购为了自保，就基于正常交期统计，按时交货率自然很好。告到老板那里，老板一看，双方都有道理啊，只好批评教育，再宣教一番“以客户为导向”，这事儿就不了了之了。结果呢，计划只能以内部客户的身份，从道义上给采购压力；而采购呢，虽然“背”着供应商的按时交付指标，但实际上形同虚设，起不到“强相关”的作用，驱动他们的仍然是单一的价格指标。 最简单的按时交付都这么难以客观统计，质量、服务等指标就更难客观量化。就拿供应商的质量问题来说，每一个质量问题，都意味着生产线、质检、供应商以及设计之间无穷尽的扯皮，大量Email乱飞，大多企业根本没有资源来梳理清楚。同理，没法客观统计的就没法管理，这样，供应商质量指标就没法落实到采购头上，驱动采购的呢，依然是自上而下的成本指标。 做不精细，企业没法有效量化横向指标，就不得不借助企业文化来推动跨职能协作，让大家“学雷锋”，但没法从根本上解决问题。 既然职能之间的横向指标难以客观建立，有些企业就采取组织措施，成立集成供应链部门，让采购、运营、物流、计划、客服等职能统一汇报到同一个总监，通过组织措施打通这些职能之间的壁垒。 图1-6所描述的，就是一个本土名企的“集成供应链”。该公司设立首席供应官，与负责营销、产品的两位老总平行，一起汇报给CEO。在首席供应官下，有负责供应商选择的寻源、负责工厂的生产，以及端对端的供应链。 有人或许会问，既然寻源与生产都是供应链的一部分，为什么没有包括在集成供应链里？这里主要有两个原因：其一，生产管理成百成千的员工，有很多琐碎杂务；寻源要跟设计、供应商打交道，有很多商务关系要维护，供应链总监的精力有限，没法对付那么多的事情。其二，成本压力大的时候，企业就倾向于集中采购，把寻源单列出来，在更高层面整合需求，增加规模优势，获取更好的采购价格。 图1-6 集成供应链是通过组织措施打通部门墙 有趣的是，也正是在这个名企，寻源有时候归供应链，有时候又独立出来。其后的驱动因素呢，就是企业的业务需求：当速度不够快的问题更大时，寻源划归供应链，从寻源到订单处理都在同一个职能，以快速响应市场需求；当成本不够低的问题突出时，寻源就单列出来，以获取更大的规模优势。很多企业的采购时而集中，时而分散，后面的驱动因素也是一样。 集成供应链让客服、计划、执行采购、仓储、配送等职能处于同一部门，其好处是，即便职能之间没有客观的横向指标，也可以通过组织措施，促进这些子职能之间的协作，以控制局部优化，推动全局优化。 比如有个公司，原来采购、物流分别汇报给不同的总监，再到不同的副总，最后到同一个高级副总裁。两个职能，两条迥异的汇报线，从员工到经理到总监到副总，都是单一指标驱动：采购希望供应商发货越快越好，物流希望运输成本越低越好，就经常出现互坑的情况，比如采购员动不动就让加急运输，而加急运输费用呢，则由物流部门来买单。 当这个公司成立集成供应链后，采购经理和物流经理都汇报给同一个供应链总监，总监层面既对采购的按时交付负责，也对物流的运输成本负责。在一对相互矛盾的指标驱动下，总监一看到加急运费那么高，就马上找手下采购经理的麻烦；采购经理就找采购员的麻烦，于是采购员也就“理性”多了，再也不敢动不动就24小时加急，超额的物流费用也就得以控制。 集成供应链的另一个好处是“冤有头，债有主”，给销售等内部客户一个解决方案。当没有集成供应链时，销售问责计划，为什么交付不按时；计划一转身，就把问题推给了生产，说生产不及时；生产自然有采购垫背，说供应商没有按时交付；采购就把设计拉出来，说设计变更；而设计，则把球踢给了销售，说都怪客户的需求变更。最后，没有一个职能真正对销售负责。成立集成供应链部门后，责任到此为止：从接到客户订单开始，计划、采购、生产、包装、配送，都归供应链总监负责，供应链总监对这些职能“要打要骂”随便，但最终的交付呢，找供应链总监就行了。 业界人士说 以前PMC、制造、品保各自独立的时候，产线和供应商一旦有问题，马上就暴露出来；现在把这些职能集成到生产事业部（跟集成供应链类似——作者注），供应端的问题反倒不容易暴露，而一旦发现，就是大问题。 刘宝红答 在供应链上，没问题是最大的问题。这就如小孩子们在一起，总是会打打闹闹的；一旦没声音了，有经验的妈妈都知道，完了，肯定是在什么地方干坏事呢。 没有一种组织结构是完美的。一种组织结构解决了一些问题，必然会产生另一些问题。关键是要看解决的问题多，还是制造的多。但如果这种集成能够更好地解决更多问题，集中、集成还是值得的。 组织越是集成、集中，组织内的问题就越不容易暴露。比如当生产和计划分离时，你会经常听到生产抱怨计划；而当生产部门自己做计划、自己做执行的时候，抱怨的声音就小了很多——谁会自己抱怨自己呢？但你知道，那并不是因为生产自己做计划做得更好。 有些企业习惯于独立各职能，目的之一就是暴露问题。我的经验是，越是管理粗放的企业，比如大型央企、国企和内地的一些大型民企，职能之间的集成度越低，职能与职能之间的监督就越强，防止贪腐等行为。当然，这种多权分立会造成别的问题，比如唯一责任人缺失，在后文的“多权分立，供应商成了公共草地”部分还会详细讲到。 实践者问 供应链管理部门得如何设置，才能让供应链更加有效？ 刘宝红答 不知道，因为组织结构一定要跟业务需求联系起来，才能讨论有效无效。比如当成本压力大时，集中采购是很好的组织结构；但是，当速度不够快成为大问题时，集中采购就不是有效的组织形式。 这里我要补充的是，你不一定得有供应链管理部门，才能管理供应链。供应链管理更多是流程型管理，而不是组织型管理。流程稳健，完全可以不要“供应链管理”部门。比如苹果就没有“供应链管理”部。我的老东家也是。流程稳健的标志是职能之间的横向指标完善。比如在一些企业，计划、采购、生产、物流等职能之间设立了强相关的指标，在计划的驱动下，各部门各司其职，也能取得良好的供应链绩效。 [1] 比如对采购来说，既要价格低，又要质量好；对计划来说，既要交付好，又要库存低；对设计来说，既要产品性能好，又要满足目标成本。这些都是表面上相互矛盾，实际上却一致的指标。工作做到位，两者都能达到。比如计划做好了，知道计划客户要的，交付就好；也知道不计划客户不要的，库存就低。这点在《供应链的三道防线：需求预测、库存计划、供应链执行》一书中有详细阐述，刘宝红、赵玲著，机械工业出版社于2018年出版。 小贴士 供应链管理的“儒家”与“法家” 传统的日本供应链是长期关系，或者说，更像“儒家”的做法。[1]在长期关系下，绩效考核相对很次要。这就如一家人，相互之间很少会设定指标。而约束双方行为的呢，也正是长期关系，是未来，因为在长期关系下，双方都有很多可失去的，所以就更加理性。 比如在论资排辈的终身雇用制下（当然，现在的日本也早已不尽如此），员工表现不好，就没有好的晋升机会；跳槽后，又得从头开始，从最基本的做起，损失反倒更大，这促使员工在现有工作上尽职尽责（当然也承受很多委屈和压力，你到日本的地铁上，一眼望去，职业人的满头灰白就是证明）。同理，在长期合作下，供应商知道未来一部分业务是它的，如果不把现在的事做好，风险就是失去未来生意，这也驱使供应商更好地干活。 但是，对于北美和中国企业来说，这就很难适用。美国和中国其实惊人地相似，放在企业行为上，那就是短期关系。如果非要说有什么不同的话，那就是中国比美国更短期罢了。不管是企业之间，还是企业与员工之间，短期关系意味着没有未来；没有未来就意味着没有可失去的；没有可失去的，你自然就没法拿未来来约束对方。那怎么办？就只能推行“法家”的做法，基于契约来管理。 契约有两种：其一，竖向的契约，这是上下级之间的契约，也是最基本的契约，驱动职能内部上下级之间的协作；其二，横向的契约，这是兄弟职能、公司与公司之间的契约，驱动跨部门、跨公司协作。前者体现为纵向指标，后者体现为横向指标。纵向指标大家都熟悉，横向指标的好处呢，就是别光顾着把所有的水都放到自己田里：种好自己的一亩三分地要紧，分点水到邻居的地里也要紧。分多少呢，不是靠发扬风格，而是约定好的，即横向指标。 过去三四十年来，本土企业从没有契约的“大锅饭”，过渡到有竖向契约的市场经济，现在正在建立横向契约的路上。但是，对本土企业来说，契约化还远未完成。一方面是文化原因：传统的文化是基于关系的，要变成冷冰冰的契约关系，会有各种挑战；另一方面是能力原因：企业的管理精细度还不够，没法有效客观量化绩效，建立强相关的横向指标。 于是，很多企业就“儒”“法”并举，一方面大张旗鼓宣扬企业文化，这是儒家的做法；另一方面推行绩效管理，典型的法家做法。而做得好的企业，这两方面都做得不错。 就拿华为和海尔来说，这是一南一北两个非常有代表性的本土名企，在外人看来都是企业文化非常强的企业，似乎靠的就是任正非和张瑞敏的一张嘴。但是，华为和海尔不是靠企业文化吃饭的，如果把它们理解为儒家信徒就大错特错了；企业文化背后，它们靠的是异常严酷的绩效考核，典型的“法家”做法，而这正是局外人不知道，或者不愿意知道的。 比如海尔的“日清日毕”，字面上文质彬彬，翻译成白话可就不了：今天的事儿没做完，晚上你就不要回家。他们甚至为每一个员工独立核算，你对内部客户做了多少事，那是你的营收；内部供应商为你做了多少事，那是你的成本，力求每个人、每件事的账都算得清清楚楚。 再比如华为的能上能下，几年前我跟他们的两位销售高管会面，其中一位总监指着另一位副总说，以前他们两个的职位正好相反，后来因为绩效原因，两个人就倒过来了。能者上，不能者下，这话说起来多么容易，但在注重关系文化的氛围里，有几个企业能真正做到？华为可以说把法家精神发挥到了极点。 这里要补充的是，传统的日本企业虽然是儒家做法，绩效考核不是很严格，但不要忽视它们的绩效统计能力。在我所熟悉的全球企业中，日本企业可以说是数据最齐全，分析最到位的。夸张点说，它们的每一件事都有数据支持，员工的Excel用得烂熟，决策更多的是基于数据，而不是判断。 你可以不算账，但不能没有账。放在绩效管理上，就是你不一定要考核，但不能不统计。要知道，企业大了，几亿元、几十亿元的规模，离开绩效统计，就没人知道真相——不统计就不知道，而不知道就无法管理。管理能力的一大标志就是数据的充分与否。放在古代帝国的文明程度上，就是能否造册征税：中原文明有能力做全国普查，能够按丁、按亩征税；而草原上的野蛮人呢，就像匈奴，称雄北方几个世纪，往往连自己有多少人马都弄不清，最后连片瓦片都没留下，就消失在历史的长河中。 [1] 这里说的“传统”，主要指20世纪日本崛起的那段时间，大致在八九十年代前后。当时美国系统学习日本的做法，现在能看到的关于日本管理方式的文献，大都是那个时段产生的。当然，过去二三十年里，日本经历了显著的变化，管理方式也在变化，在有些做法上与欧美更加趋同，因文献不足，这里就不予探讨了。 案例 找替换供应商时，技术与质量不积极 有位职业人新近晋升供应链经理，全面负责公司的采购、质量和物流管理。摆在他面前的第一要事就是供应商质量问题：有些关键的供应商，质量问题一直没法解决。这位经理的解决方案呢，就是启动供应商淘汰机制，另行选择更好的供应商。但是，技术和质量人员都不够积极。他问我该如何设置绩效考核，提高这些职能与采购协作的意愿，尽快找到替换供应商。 案例企业年度营收十亿元左右，不大也不小，职能之间的部门导向有，但壁垒远远没有大公司的那么高。在开发替换供应商上，技术、质量人员积极性不高，看上去是个“不愿意”的问题，其实是因为供应商没选好、没管好，根本上是个“不能够”的问题。而解决方案呢，要从选好、管好供应商，争取首发命准上找，而不是第一个供应商没选好、没管好，就再找一个，让各部门重复投入资源。 遗憾的是，很多人分不清“不愿意”和“不能够”，误把后者当前者，一味地在绩效考核上做文章，自然没法解决问题。 就拿案例公司来说，供应商选好后，后续管理跟不上，有选择、没管理，好供应商也会变坏（这点后文还会详细谈到）。就质量问题来说，他们先是单纯依靠质量部门来应对，比如驻场管理，自然没法解决；最终变成了整体供应商问题，采购就拿淘汰代替管理，让技术、质量开发替代供应商，意味着更多的技术验证、质量评估工作。质量、技术虽然不知道根本的解决方案，却明白找替代供应商并不能真正解决问题，因为新供应商一进来，也会有老供应商的问题——这两个职能早已吃过很多亏，受过很多苦，对这点有很多切身体会。 所以，在找新供应商上，技术与质量消极反抗，出工不出力，也就不足为奇了。而解决方案呢，不是给这两个职能定指标，让他们更加愿意开发替代供应商；而是要改进供应商的选择与管理，争取第一次就把供应商选到位、管到位。 类似的情况很多。 比如有个企业的供应链老总说，他们的销售预测保守，不愿意建库存，导致供应链赶工加急。怎么才能促使销售多建库存？这不是个绩效考核问题，这是个能力建设问题：该公司采取“销售提需求”的做法，让一线销售人员做需求预测；那么多的一线销售，每人预测自己的客户需求，预测的颗粒度那么小，预测的准确度注定很低；预测准确度低，库存的风险大，销售人员建库存就自然保守。所以解决方案不是给销售绩效考核，而是改善需求预测流程，比如在公司层面集中预测，兼顾关键销售人员的职业判断，提高预测的准确度[1]，让销售能够更有效地管控库存风险，从而更愿意建库存。 延伸阅读 对于跨职能协作，我的基本观点是要通过绩效考核，设立强相关的横向指标来解决不愿协作的问题。当然，过犹不及，绩效考核也有副作用。比如《孔雀效应》一文中讲到的单向选择，还有我们都熟悉的“手术很成功，病人却死了”，都是常见的“绩效考核病”。在我的“供应链管理专栏”（www.scm-blog.com） 上搜索题目，或者扫描二维码，即可阅读《孔雀效应》全文。 [1] 对于一线销售为什么做不好需求预测，详细内容可参见我和赵玲的《供应链的三道防线：需求预测、库存计划、供应链执行》一书，106~117页。","link":"/%E4%BE%9B%E5%BA%94%E9%93%BE%E7%9A%84%E6%A0%B9%E6%9C%AC%E6%98%AF%E5%8D%8F%E4%BD%9C%EF%BC%8C%E9%82%A3%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%8D%8F%E4%BD%9C/"},{"title":"17. 利用LangChain让AI做决策","text":"Hi，大家好。我是茶桁。 在第 11 讲中，我向您介绍了如何将各种资料内容向量化，借助Llama-index建立索引，对我们自己的文本资料进行问答。在过去的3讲中，我们深入了解了如何使用Langchain。该工具可帮助我们整合AI对语言的理解和组织能力、外部各种资料或者SaaS的API，以及您自己编写的代码。通过整合这些功能，我们可以使用自然语言完成更复杂的任务，而不仅仅是闲聊。 但到目前为止，我们所有基于ChatGPT的应用基本上都是“单项技能”，例如前面关于“藤野先生”的问题或上一讲中查询最新天气或通过Python进行算术运算。这本质上是限制AI只针对我们预先索引或实时搜索的数据进行回答。 给AI加上多项选择能力 要做一个能跑在生产环境上的 AI 聊天机器人，需要的不止一个技能。在电商领域，最起码需要以下三个技能： 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E5%88%A9%E7%94%A8LangChain%E8%AE%A9AI%E5%81%9A%E5%86%B3%E7%AD%96/"},{"title":"20. 尝试让机器拥有声音","text":"大家好，我是Hivan。 好久不见了，今天我们来讨论下如何让机器拥有声音。 回顾一下我们上一讲的内容，我们已经成功使用Whisper模型使得AI能够理解我们说的话。这为我们带来了很多应用，例如让AI代替我们收听播客并总结内容。然而，这只是单向的交流模式。现在，让我们探索更深入的可能性，让AI不仅仅能够“听懂”我们的话，而且通过ChatGPT回答我们的问题，并将所有内容合成语音，用声音与我们进行双向交互。 这就是我们本次探索的主题：让AI说话。我们将学习如何使用云端API进行语音合成（Text-To-Speech），同时也会介绍开源模型，使您能够在本地CPU上实现这一功能，让数据安全问题不再是困扰。 让我们一起，给机器赋予声音吧！ 使用 Azure 云进行语音合成 语音合成技术早已迈入成熟阶段，你所听到的许多短视频配音都借助此技术实现。无论是科大讯飞、阿里云、百度、AWS Polly还是Google Cloud，国内外的大公司纷纷提供了类似的云服务。然而，今天我们将带您领略微软Azure云的语音合成API，主要是因为以下两个原因： 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E5%B0%9D%E8%AF%95%E8%AE%A9%E6%9C%BA%E5%99%A8%E6%8B%A5%E6%9C%89%E5%A3%B0%E9%9F%B3/"},{"title":"19. 快速倾听和总结音频内容","text":"Hi，大家好，我是茶桁。 其实到第18章的时候，我们处理文本的内容就全部都结束了，从本节课开始，我们要开始学习如何处理音频和图像。 我不知道有没有人和我一样的习性，就是比起视频和音频文件来说，还是跟喜欢看文本文件。这其中最主要的一个原因就是因为文本内容我们可以准确定位，而对于文本内容的接收速度还取决于我们输入设备（眼睛和处理信息的脑部）速度。而音频或者视频则不然，我们必须听完讲述者所说的话，即便你开到2倍速，速度依然受限，而且无法准确定位。那有没有什么办法能快速完成对音频文件内信息的获取呢，自然就是将语音内容转换成文本的能力。 其实到这一步，类似于Premiere或者剪映等剪辑软件都可以完成，不仅如此，在AI大行其道的今天，市面上应该也有不少Audio2Text的服务或者应用。接下来，我们要讲的就是一个杀手级服务了。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E5%BF%AB%E9%80%9F%E5%80%BE%E5%90%AC%E5%92%8C%E6%80%BB%E7%BB%93%E9%9F%B3%E9%A2%91%E5%86%85%E5%AE%B9/"},{"title":"观点：我们无法通过改造自己摆脱气候危机","text":"让我们面对现实吧——气候变化是人类最大的失误。我们已经知道它近一个世纪了。科学是清楚的。然而，我们什么也没做。真是太尴尬了。 现在，全球领导人终于开始忙着收拾残局。但是，尽管我们需要的大多数气候解决方案已经存在，但我们似乎无法按照所需的速度和规模部署它们。 简而言之，世界正在变暖，而我们却无法让它降温。去年，人类向大气中排放的\\(CO_2\\)比以往任何时候都多（呃……WTF？）。 可以理解的是，领导者们都害怕极了。这促使他们探索一些非常愚蠢且完全危险的想法。他们最糟糕的脑波之一是地球工程——也就是用地球的气候扮演上帝的角色。（这里使用的“地球工程”并不是指碳去除技术，据我们所知，碳去除技术是相当合法的。） 其中一些建议包括增亮云层、改变海洋的化学成分，或者向大气中发射粒子来使太阳的光线变暗——会出现什么问题呢？ 虽然这些提议听起来像是反乌托邦科幻电影中的内容，但改变地球气候实际上非常容易且成本低廉。 太阳能地球工程是这些“解决方案”中最具争议性的一种。其最受欢迎的衍生产品是平流层气溶胶喷射，涉及将灰尘喷射到大气中，以减少照射到地球表面的阳光量。这项技术的灵感来自于火山云，众所周知，火山云在一次大喷发后可以使整个地球冷却多年。 云增加了地球表面的反射率。平流层气溶胶喷射旨在通过将灰尘喷洒到高层大气中来复制这种效果，以期冷却气候。 虽然平流层气溶胶注入对于阻止全球变暖可能非常有效，但它可能会打开潘多拉魔盒的问题。根据联合国最近的一份报告，干扰全球自然气候可能会破坏臭氧层，改变全球降雨模式，并导致严重的地缘政治紧张局势。 尽管支持者称太阳能地球工程将是对抗变暖的短期措施，但《科学美国人》最近发表的一项研究表明，如果政客们确实决定向大气中发射尘埃，他们可能会在“几个世纪或更长时间”内危险地依赖它。 为了向大气中排放足够的灰尘来抑制变暖，每年可能需要数万次高空飞行。这一过程的突然停止可能会导致温度飙升，其速度可能快于生命的适应速度，这一概念被称为“终止休克”。 还有一个道德问题，即技术修复可以减轻政客和企业尽快脱碳的压力。 简而言之，太阳能地球工程相当于气候变化创可贴。 尽管存在风险，美国政府去年还是启动了一项为期五年的研究计划，探索将更多阳光反射回太空的方法，为进一步资助这项新兴技术奠定了基础。 比尔·盖茨、乔治·索罗斯和 Facebook 联合创始人达斯汀·莫斯科维茨等亿万富翁都表达了兴趣，而 60 名著名科学家则希望进行小规模太阳能地球工程现场实验。 甚至还有一家名为 Make Sunsets 的初创公司，基于其向大气中释放二氧化硫以遏制变暖的承诺而预售碳信用额。 美国初创公司 Make Sunsets 因未经批准进行平流层气溶胶注入测试而被逐出墨西哥。《麻省理工科技评论》的几位研究人员谴责了“Make Sunsets”，称其努力“为时过早”。 预防原则——或者对普通人来说“如果有疑问，就不要考虑”——是健全环境决策的基本前提之一，也是我们今后应该注意的原则。 欧盟本周宣布，呼吁就气候地球工程可能使用带来的风险进行“最高国际级别”会谈，这可能是积极的一步。 欧盟官员在周三的联合通讯中表示：“这些技术给人类和生态系统带来了新的风险，同时也可能加剧国家之间的权力失衡，引发冲突并引发无数道德、法律、治理和政治问题。” “我们不能用造成问题的思维方式来解决问题。 尽管欧盟正在采取预防措施，但它并不完全反对这些技术，而是寻求制定管理这些技术的“规则”。 其他人则采取更强硬的立场。 乌得勒支大学哥白尼可持续发展研究所的弗兰克·比尔曼在去年发表的一份声明中警告说，“人们对太阳能地球工程的风险知之甚少，而且永远无法完全了解。” 比尔曼是一群著名气候科学家的领导者，呼吁就太阳能地球工程达成不使用协议。换句话说，全球范围内禁止其开发。 “太阳能地球工程的研究并不是像其倡导者所说的那样，为预防气候灾难而准备 B 计划。相反，它只会推迟和破坏当前的全球气候政策，”他说。 “此外，现有的国际机构体系无法有效监管这项技术在全球范围内的部署。太阳能地球工程不是解决方案。” 我完全同意，教授。通过扮演上帝的角色来操纵气候，我们不仅面临着使我们的困境恶化的风险，而且还发出了一个危险的信息——人类可以简单地通过设计方法来解决问题，而不是从根本上解决问题（想想广泛的文化、社会和政治）变换）。 正如爱因斯坦的一句名言：“我们不能用创造问题的思维方式来解决问题。”","link":"/%E6%88%91%E4%BB%AC%E6%97%A0%E6%B3%95%E9%80%9A%E8%BF%87%E6%94%B9%E9%80%A0%E8%87%AA%E5%B7%B1%E6%91%86%E8%84%B1%E6%B0%94%E5%80%99%E5%8D%B1%E6%9C%BA/"},{"title":"18. 根据垂直需求微调模型","text":"大家好，我是茶桁。 最近事情太多，这一节课更新的有些晚了。 首先我们先了解一下我们本节课讲要讲一些什么，我们之前介绍过 llama-index 和 LangChain，学习了将大语言模型和自己的知识库组合来解决问题的方法。这个方法中，我们不需要调整我们使用的模型，而是使用嵌入向量索引我们的数据，并在需要时查询索引来解决问题。 然而，我们也可以完全利用自己的数据，创建一个新的模型来解决问题。这种方法是OpenAI提供的微调模型功能。这也是我们要探讨的大语言模型的最后一个主题。 如何微调模型 我们都知道，AI其实是建立在大语言模型之上的，而模型再如何补全，也没有办法全知全能。在很多时候，AI所回答的内容常常错漏百出，甚至于一些垂直领域可能完全词不达意。这些其实都是因为缺少了特定领域的训练数据，而我们要做的，就是要补全这一部分数据进行训练，为我们自己的需求微调出一个擅长本领域的模型。 由于本文在其他平台已经成为收费文章，为了避免损害付费会员权益，所以此处做删减处理。 希望继续观看的，请订阅我的公众号。","link":"/%E6%A0%B9%E6%8D%AE%E5%9E%82%E7%9B%B4%E9%9C%80%E6%B1%82%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B/"},{"title":"不要只看供应链管理的一个方面","text":"供应链管理不是软件？ 其实产生疑惑不无道理。供应链管理的范畴非常广泛，对它的认识就像盲人摸象：很多人是IT背景，他们想到的就是供应链管理软件，例如Oracle、SAP；对很多采购人员来说，供应链管理就是采购和供应商管理，即确保供应商保质、保量、按时提供价格合理的产品或服务；对于生产管理出身的人讲，供应链不过是生产管理的延伸罢了；对于物流行业的人来说，供应链管理则往往被等同于物流运输、车辆调度、仓储管理等。 在国内，很多人分不清供应链管理与物流管理。2010年，我拜访一位本土顶尖企业的首席执行官。该首席执行官几次提到物流管理，说ISM可以在物流管理上做出更大贡献。我想他指的应该是供应链管理，因为ISM侧重于供应链管理中的采购与供应管理，跟物流管理还离得比较远。当然，因为远离操作层，在一个上千亿元人民币规模的大公司CEO看来，这些区别或许不怎么重要。 大致在20世纪90年代，物流和供应链管理两个概念先后传入国内。这并不是说以前就没有物流和供应链——有人类的时候就有，只是不一定这么叫罢了。当时的大背景是物流成本太高，约束着本土供应链的效率。物流成本包括仓储、分销、运输、库存、物料搬运、第三方物流等费用，跟一个国家的基础设施息息相关。当时美国的物流成本是GDP的10%左右，得益于发达的高速公路网和信息基础设施；中国的物流成本是GDP的20%左右，跟当时落后的基础设施不无关系。 在国内，物流是供应链的瓶颈，因而就成为供应链管理的重点，乃至一叶障目，不见森林，让很多人误把物流管理当作供应链管理。当然，过去一二十年来，国内大规模投资高铁、高速公路等基础建设，这一情况得到显著改善，物流成本占GDP的百分比逐年下降，比如到2018年有望降到14%左右。作为对比，美国这些年来也是一路下降，达到7.2%，全球平均水平为11.2%。我想，这跟这些年来信息技术的发展不无关系：信息流驱动产品流，信息流的效率高了，物流的效率也会改善。 到底什么是供应链管理呢？ 供应链是从客户的客户到供应商的供应商，供应链管理是对贯穿其中的产品流、信息流和资金流的集成管理，以最大化给客户的价值、最小化供应链的成本。它是一个综合管理、全局优化的思想，以摆脱单个公司、单个职能层面的局部优化，实现供应链条的全局优化为目标。 在实践操作中，供应链管理由三大块构成： 供应管理（寻源） 运营管理（加工） 物流管理（交付） 跨越企业管理中的供、产、销三大块（如图：） 资料来源：Supply Chain Council. 简单地说，就是采购把东西买进来，生产来加工增值，物流来配送给客户。这三大块是执行职能，由计划驱动——也可以说计划是供应链的第四大领域。计划是供应链的引擎。很多执行层面的问题，看上去是没做到，其实往往是没想到——计划不到位造成的。这也是为什么在供应链运营模型（SCOR）中，计划处于采购、运营和物流之上。 从三大职能上讲，供应管理侧重于采购和供应商管理，使供应商成为公司的有机延伸；生产运营管理力求以最有效的方式完成产品、服务的增值过程；而物流管理则力求以最经济、迅捷的方式把货物从A点流动到B点。 从三条流上讲，产品流从供应商向客户流动，是供应链的实物流（如果是从客户向供应商方向的话，则称为逆向物流）；资金流是从客户流向供应商，是供应链的血液；而信息流则是双向流通，构成供应链的神经系统。 在竖向集成盛行的年代，供、产、销大都处于同一个公司内部。例如20世纪早期的福特汽车，从炼铁厂到零部件再到整车组装，都曾试图集中在自己旗下，尽管从来没有真正集成到这一步。最近二三十年以来，竖向集成解体，外包盛行，这三大功能越来越依赖供应商，例如零部件来自供应商，生产靠外包制造商，物流靠第三方物流公司。作为采购方，对这三部分的集成管理是供应链管理的重点。同样的道理，竞争也不再局限于公司与公司之间，竞争变成供应链与供应链之间的竞争。 值得注意的是，任何一个新的领域，都是在已有领域的基础上发展而来的。供应链管理也不例外，它从供应管理、运营管理、物流管理等分别向相邻的领域扩展而成。反映在学术机构，在北美，虽然专业都叫供应链管理，但不同大学的供应链管理专业侧重点不同。比如在亚利桑那州立大学，供应链专业历来以采购见长，而田纳西大学则侧重物流，麻省理工侧重运输。根本原因呢，就是这些学校的供应链管理是从这些具体领域发展而来的。 除了采购、生产和物流外，工业工程也是供应链管理的“近亲”。在亚利桑那州立大学工业工程的很多教授都在供应链管理系任教，后来甚至有一位成为供应链管理系的系主任。一位在密歇根大学工业工程系就读的博士，研究的却是供应链管理，后来到俄勒冈大学的商学院任教，教授的也是供应链管理。 在美国，很多大学的供应链管理专业设在商学院。比如MBA排名中，一个分支就是供应链管理。除此之外，也有很多工学院设立供应链管理的研究生专业。这几年，有好几个大学设立一年制的供应链管理硕士课程，比如马里兰大学、俄亥俄州立大学、南加州大学等，吸引了大批的中国留学生，造成同质化严重，也是个问题。得州大学达拉斯分校也有供应链专业。 在研究领域，有很多杰出的研究者都是从别的领域来的，对供应链管理专业的建立贡献巨大。比如斯坦福大学的李效良（Hau Lee）教授，他关于“牛鞭效应”的研究可以说奠定了供应链的理论基础。但这些研究大都是他在工学院时做的，师承工业工程、管理科学和运筹学，上溯到MIT的系统动力学（System Dynamics）。李效良在担任《管理科学》（Management Science）杂志主编期间（1997~2002年），在这个管理学领域最权威的学术期刊之一上，刊登了大量的供应链管理文章，可以说让供应链管理正式成为一个专业领域。 在工业界，鲜有能够跨越供应链的三个领域的实践者。尽管很多公司试图把采购、运营和物流等职能集中到一起，组成全球供应链或全球运营部，但下面的分支部门仍旧围绕三个职能划分。道理很简单：没有人能够掌握所有的采购、运营、物流，外加计划等众多领域的专业技能。对供应链管理的认识仍会处于“盲人摸象”状态：采购背景的人说是采购的延伸，物流的人说是物流的延伸，而生产部门则认为是运营管理的延伸。一些流程分析、软件背景的人，则更多地从端对端的流程角度出发，理顺供应链的产品流、信息流和资金流，提供了一个全新的供应链管理视角。条条大路通罗马，这些都可成为公司搭建卓越供应链的起点。 就本土企业来说，20世纪90年代后期，华为导入IBM的集成供应链的概念，旨在打通职能部门之间的横向联系，提高供应链的效率，可以说是开了国内集成供应链的先河。华为所在的电信设备行业批量小，品种多，复杂度高，集成供应链确实是关键的解决方案之一。2005年，联想并购IBM的PC业务，全盘接受了IBM的供应链管理体系。 这几年，供应链管理的概念更加深入各行各业，不光是大企业，还有中小企业；不光是制造业，还有建筑业、电商业、餐饮业等。除了制造业外，还有建筑、零售、服装、餐饮、电商等行业。根本原因呢，是这些企业认识到供应链的价值，认识到打通部门之间的壁垒、通过全局优化来提高公司绩效的重要性。 20多年来，供应链管理在中国遍地开花 有个朋友，曾经担任西贝餐饮的副总裁。他说，餐饮看上去是开餐馆，其实比拼的还有供应链实力。比如原材料的获取、储存、加工等——很多原材料有很强的季节性，比如西贝用的羊肉来自内蒙古草原，内蒙古羊肉最好的在秋季，羊一定要在那两三个月内从内蒙古的牧场收齐、宰杀、冰冻，供后面的一整年用，这些都需要一流的供应链计划和执行来支持。因为餐馆所处位置一般为市内黄金地段，租金很贵，所以店面都较小，大多菜的加工其实是在中央厨房完成的，及时运送到店面后，做些简单的最后加工就上菜了。中央厨房就跟制造业的工厂差不多。从这个意义上讲，西贝这样的餐饮业跟生产、零售业没有本质区别。 2000年国内很少听到供应链管理的概念，更不用说有这专业了；十多年后，国内已经有很多大学设立了供应链管理专业，各种各样的供应链公司如雨后春笋，供应链管理的概念也更加深入。尤其是经历二三十年的高速发展后，企业普遍面临“增长陷阱”[1]，越来越多的人意识到，企业要生存，不但需要开发好的产品（主要是设计的责任）、卖个好价钱（销售的任务），而且要以适当的成本、速度生产出来（供应链的责任）。随着整体经济的进一步成熟、放缓，降本增效的压力必将加剧，而作为降本增效的主要源泉，供应链管理任重道远。 在之后篇幅里，会从采购、运营和物流的角度学习供应链管理，希望能有个全面的认识。 !&gt; 这里是脚注。（由于docsify缺乏脚注功能，所以只能直接写了，没有链接回跳） 「1」：增长陷阱”指企业发展到一定阶段，营收增速放缓，不再增长甚至下跌，而成本由于惯性还会继续上升（比如不管使用与否，设备折旧会照旧；不管生意好坏，员工每年的工资总得加几个点），导致利润率越来越低，甚至亏本。详细内容可参考我的另一本书《供应链管理：高成本、高库存、重资产的解决方案》，机械工业出版社于2016年出版。 采购和供应链管理 在供应链管理的三大职能中，供应管理与供应链管理只是一字之差，可以说是供应链管理的“近亲”。但是，供应管理的重点是供应商这一外在战略资源，与运营管理侧重公司内部生产运营、物流管理侧重产品和信息的流通形成对比。 供应管理起源于采购管理。从严格意义上讲，供应管理的范畴远大于采购管理。但为了行文方便，采购管理和供应管理在本书中通用（如果没有特别注明的话）。 在美国，传统上采购的地位不是很高，因为传统上美国公司的竖向集成度挺高，对外来资源依赖度低。作为管理外来资源的采购部门，其主要任务是围绕订单处理日常交易。简单地说，内部客户（如工程师）说，我要买这个，采购的任务就是下订单，确认价格、交期，把货按时拿到。 采购部门的吸引力有限，就成了那些百无一用的人的最后落脚点。就如我在亚利桑那州立大学读书时，一位叫皮尔森的教授曾经说，如果一个人干不了销售、设计、生产等，那只能去做采购了；如果连钱也不会花，那就只能卷起铺盖另谋高就，去祸害我们的竞争对手吧。 美国如此，中国也是：传统的计划经济下，外在资源主要依靠国家统一调配，公司A的产品给B做原材料，价格都由政府规定了，采购自然也就可有可无了。在那些比较封闭的行业，比如军工、航空业，还能看到传统经济的影子：长期以来，中国的航空业是半竖向集成的，整个行业其实就是中航工业和它的子公司们，行政命令历来扮演重要角色。现在为了开发商用大飞机，得跟那么多的全球供应商打交道，用工业界通用的方式做生意，采购面临的挑战可想而知。 在采购管理上，经常听人说，如果你连花钱都不会（做采购），那可真是百无一用了。就如李鸿章对儿子说，如果你连做官都不会，你可就一无是处了（大意）。其实我们都知道，做官的学问可大了，离开了那些官僚，一个国家的运作就会大受影响。采购也是：采购是一个大职业，尤其是在有些行业，产品成本的百分之七八十都来自供应商的情况下，采购已经远远超越持币购物，而是在管理公司百分之七八十的增值活动——供应商表面上在赚我们百分之七八十的钱，实际上在帮我们做百分之七八十的事，而采购呢，则对选择和管理供应商负责，对这些增值活动负责。 更进一步，常言说得好，卖得好不如买得好：采购每节省一块钱的开支，利润就增加一块；销售增加一块钱的销售，利润大致增加一毛。不管是零售业，还是制造业、服务业，采购的重要性毋庸置疑。在有些行业，比如电商和贸易行业，由于没有生产，供应链的所有增值环节都在供应商处，采购的价值就更大了。 随着很多行业转向外包战略，外购额逐渐增长，成为公司开支中的最大一块，公司对供应商的依赖度越来越高。而作为管理供应商的对口职能，采购的重要性也在日益上升。在美国，设置首席运营官的公司越来越少，设置首席采购官的则越来越多，根本原因就在于增值活动以前主要发生在公司内部，由首席运营官负责，现在则越来越多地外包给供应商，由首席采购官负责。 采购的地位提升，其重心也从订单处理转为对供应商的战略管理，过渡到供应管理。20世纪80年代，麦肯锡的一位顾问在《哈佛商业评论》上发表文章，题为《采购必须成为供应管理》[1]，吹响了这一战略转移的号角。但是，整个过程花了二三十年。2002年，美国采购经理人联合会（NAPM）更名为供应管理协会（ISM），是这一过程的里程碑事件，标志着供应管理正式成为主流。 与采购管理的围绕订单处理相对应，供应管理更侧重供应商的战略管理，通过分析开支、确认需求、评估供应商、选择供应商、签订协议、管理供应商绩效来确保以合适的成本保质保量地获取资源。从时间跨度上讲，供应管理向前延伸到设计和新产品开发，向后延伸到产品的生命周期结束；从影响的对象上讲，供应管理延伸到对公司的资产、现金流等的管理，直接影响公司的盈利。 在北美，有些公司已经开始统计供应管理的贡献，例如净利润率是10%，其中0.5%是供应管理通过降低采购成本等来实现的。首席采购官这一头衔能够与首席财务官、首席运营官等相提并论，也反映了采购与供应管理战略地位的提高。 从供应链的角度来看，采购处于公司的内外结合点，是管理供应链的理想选择。作为采购部门，突破对传统职能的认识，在管理供应链上发挥更大作用，也是提升采购在公司地位的一个有效办法。采购对内管理需求（比如设计的新产品寻源、生产部门的量产需求）、对外管理供应商（比如供应商选择和绩效管理），通过理顺需求来理顺供应，其实就是在管理供应链，或者说管理供应链的一大块。 在一些大型国企、央企，以及管理粗放的民营企业，采购并没有意识到这些。他们对自己的定位主要是招投标，以及供应商出了问题后的应急反应。[2]没有了需求管理，很多需求一落地就是紧急需求，给后续的供应链执行带来很大挑战；没有系统的供应商管理，供应商层面的问题没有解决，导致订单层面的问题不断，供应绩效长期在低水平徘徊。这些都是采购面临的大问题，也是供应链管理的大挑战，我们在后面还会详细讲到。 资源 美国高级采购研究中心（CAPS Research，www.capsresearch.org）。该中心是美国供应管理协会与亚利桑那州立大学合作成立的，有一系列专题研究，侧重采购与供应管理，是全球该领域的顶尖研究机构。 资源 微信公众号“宫迅伟采购频道”，有一系列的采购与供应管理方面的原创文章。 延伸阅读 有一个门类专门讲采购管理（http://scm-blog.com/cat-23），可以阅读更多采购管理方面的文章。 [1] 文章英文名为Purchasing Must become Supply Management，发表于1983年9月，作者为Peter Kraljic（中文译名“卡拉杰克”）。卡拉杰克是麦肯锡的咨询顾问，在德国汉诺威工业大学获博士学位，对采购界的影响深远。有名的“卡拉杰克矩阵”就是由他提出的：他参照投资模型，按照收益影响和供应风险两个维度，把采购项分为四类，区别对待。这是采购管理中一个最为根本的模型，有很大的指导意义。更多细节可参考百度百科“卡拉杰克模型”词条。 [2] 可以说，招投标是采购工作的一部分；但如果是采购的主要任务，这个企业的采购注定是“小采购”。采购的主要任务是选择、管理供应商，招投标是供应商选择的一种方法，而且是很不完美的方法。在招投标盛行的企业、机构，伴随着招投标的往往是供应商的有选择、没管理，供应商绩效一塌糊涂。我们在后文还会详细阐述解决方案。 物流管理：从A点到B点 简单讲完了采购和供应管理，我们来看一下物流管理。 原美国物流管理协会、现供应链管理专业人士协会对物流管理的定义如下： 物流管理是供应链管理的一部分，即为满足客户需求，通过计划、实施和控制，促成产品、服务和信息从发源地到消费点的有效流动及储藏。 这定义有点长，拗口，但说明了几点： 第一，物流管理是供应链管理的一部分。作为美国物流管理方面的权威组织，供应链专业人士协会的定义有相当的权威性，确定了物流与供应链的关系。在2004年，该协会名称从物流管理改为供应链管理，也反映了物流管理向供应链管理的延伸。与此类似，运营管理、采购与供应管理也在向供应链管理延伸、靠拢，从它们的更名上可见一斑：运营管理协会以前叫美国库存与生产控制学会（2004年改名），供应管理协会以前叫美国采购经理联合会（2002年改名）。 第二 ，物流管理的对象是产品、服务、信息的流动与储存。简而言之，就是把产品从A点搬到B点，并处理过程中的服务、信息。值得注意的是，它不负责采购（那是供应管理的任务），也不负责生产（那是生产和运营管理的事）。这个界限表明了物流管理想与运营管理、供应管理三分供应链管理的天下。 第三，物流管理不但管理产品、服务、信息的正向流动（从供应商到客户），而且管理其反向流动（从客户到供应商，即逆向物流）。逆向物流日趋重要，是退货、保修、返修等售后服务的重要一环，也更难管理。国内可能还体会不到，如果你在美国，到沃尔玛这样的大超市去看看，节假日后，退货的队跟买货的差不多长，你就知道逆向物流面临的挑战了。 按照上述定义，物流管理的对象包括运输、车队、仓储、物料处理、订单履行、物流网络设计、库存管理，以及对第三方物流服务商的管理等。当然，有时候物流管理也会涉及采购、生产、包装和客户服务等。它不但要优化物流的各环节，而且要考虑与其他职能的集成。 在国内，很多人片面地把物流等同于运输，就是把产品从A点搬到B点，看上去很简单，其实不然。光从它占美国、日本GDP的7%左右，占中国GDP的15%左右来说就不简单。单拿它的分支行业来说，运输业、仓储业等本身就大得不得了，整个物流行业，你能想象有多大吗？ 有趣的是，查一下几十年前的定义，物流管理还包括采购，采购被视作入厂物流[1]的一部分。这也与当时竖向集成为主、采购的地位低下不无关系。在有些公司，比如欧洲的一些公司，负责订单处理的采购员汇报给物流，而不是采购部门。上汽大众也是类似的设置，估计是受德国大众的影响。中国有物流与采购联合会、物流与采购网，都是物流在先，采购在后，一定程度上也反映了采购与物流的关系。 在美国，采购管理领域的研究者，有很多原先也是毕业于物流管理系。例如，美国经典的采购教科书的作者David Burt教授，原来就是毕业于斯坦福大学的物流管理专业。我在亚利桑那州立大学的教授Lisa Ellram呢，虽然研究方向主要是采购，但博士学位却来自俄亥俄州立大学的物流管理。 在国内，物流（logistics）早些年被译作后勤学，又称军事物流学。这跟物流与军事联系由来已久不无关系。诸葛亮六出岐山，据说是一人打仗，需要五人做后勤支持，后勤是最大的挑战，而输也是输在后勤上。左宗棠在西北平叛，“惟秦陇之事，筹饷难于筹兵，筹粮难于筹饷，而筹转运尤难于筹粮，窘迫情形，为各省所未见。”[2]——在陕甘一带，筹钱比招兵难，筹粮比筹钱难，而粮草的运转比筹粮更难，说的也是物流后勤之难。 第二次世界大战后期的诺曼底登陆，表面上是一场战役的成功，不为人所知的是后勤物流的杰作。以前说美军能够在24小时内开赴全球的任何地方；现在呢，美军第82空降师可以在18个小时内到达世界的任何地方，拼的还是物流的实力。2017年，中印的6·18洞朗对峙事件，之所以能够和平解决，与中国的物流运输能力分不开——得益于这些年在西藏的铁路、公路建设，我们能够迅速地在西藏投放大批重型武装，对印度形成有效吓阻。 资源 微信公众号“物流沙龙” www.logclub.com 是物流管理领域的一个交流平台。这个沙龙已有十余年的历史，一直坚持在物流管理领域。 [1] 入厂物流是inbound logistics的翻译，简单地说，就是把原材料、半成品等运入厂区，比如从供应商到工厂。 [2] 胜利在望却甘愿求和，左宗棠西北平乱为何要选马家军做朝廷代理人.百家号“史料不辑”. 运营管理：千遍万遍不走样 运营管理是供应链管理的三大组成之一，当然也可以说供应链管理是运营管理的延伸。那究竟什么是运营管理呢？ 微软的英卡特百科全书对运营管理定义如下：“运营管理是对主要商业活动的管理，即组织和控制最基本的商业活动，为客户提供产品或服务。”这与美国运营管理经典教科书[1]的定义挺接近：“运营管理是对公司相关体系的设计、运作和改进，以制造产品和提供服务。”它是把原材料、人力、技术、资金、设备等转化为产品、服务的增值过程，是每一个管理人员都没法回避的。 运营管理协会，即原来的美国生产与库存控制学会，对运营管理的定义有明显的生产和库存管理的痕迹，但贴切地反映了运营管理的兼容并蓄：“运营管理是对研发、工业工程、管理信息系统、质量管理、生产管理、库存管理、会计等职能的集成，以有效地规划、利用和控制生产或服务机构。” 运营管理不是制造业专有，从“制造与服务业运营管理学会”的名字就可见一斑。在美国，国内生产总值GDP的79.7%来自服务业（2017年）[2]，运营管理的研究重心也在从制造业向服务业转移。很多起源于制造业的概念，也被移植到服务业。麦当劳把流水生产线用到快餐服务，就是一个例子——流水线最早由福特汽车导入，是个制造行业的实践。 一位在戴尔担任过运营经理的朋友说，运营管理都是些琐碎繁杂的事。没错，不过运营管理的这些柴米油盐事，却关系到公司的基本运作，如质量、交货、服务等，任何一件小事都可能让你的生产线停顿下来，所以非常重要。一位纳斯达克100的大公司的首席运营官说，他的全球运营部门是“啥事都牵扯”（in the middle of everything），也是同样的道理。 琐碎繁杂，微不足道，干一遍没什么难，难就难在千遍万遍不走样。这就如麦当劳的炸薯条本身没什么了不起，真正了不起的是，不管在世界什么地方，由什么肤色的人炸，是早晨还是晚上，这薯条都炸得一个样。就如海尔集团首席执行官张瑞敏所说，“不简单，就是将简单的事做千遍万遍做好；不容易，就是将容易的事做千遍万遍做对。”背后没有成套的系统、流程是不可能的。运营管理的价值就体现在对这些系统、流程的设计、运营和改进上。而且只有从日常运营的繁杂琐事中上升到流程、系统的实质问题并改进，运营管理者才能脱颖而出。这点同样适用于供应链管理。 在北美的大公司，运营管理和供应链管理相互搭接。例如，在IBM这类推行集成供应链管理的公司，运营管理是供应链管理的一部分；而在另一些公司，采购、物流等是全球运营部门的一部分，汇报给全球运营部。究竟是运营汇报给供应链，还是供应链汇报给运营，这并不重要：水无定型、法无定法，关键是组织结构要能够满足公司的业务要求，并随着业务的发展而调整。 业界人士说 在一些国企，运营管理部是虚岗，跳出实际的业务流程去进行所谓的运营优化管理。在我看来，这种做法非常低效：一方面，该岗位平时可有可无，即使该部门全部放假也完全不影响业务进程，对从业人没有任何直接的业务压力，没有压力就没有动力，很难出成绩和效果；另一方面，会产生外行指挥内行的现象，反而干扰正常业务流程。如果用跨部门的专项项目，或者类似精益生产的改善小组，效果应该更好。常设专职的工作组、委员会一般都不是解决问题的好组织形式。——米良疯，微信公众号“供应链管理专栏”读者 刘宝红答 其实何止国企，大企业都有这问题。专业分工下，内行埋头干活儿，没时间抬头看路；外行在教人干活儿，但不知道活儿是怎么干的。干活儿的跟教人干活儿的是两层皮，注定效果会打折扣。最早的日本企业的“质量圈”（quality circle），就是干活儿的人自己在改进，两层皮的问题就比较小。 [1] 该书英文名为Operations Management for Competitive Advantage。作者为Richard Chase，F. Robert Jacobs和Nicholas Aquilano，2005年由McGraw-Hill/Irwin出版社出版。该书的更新版本有中文版，名为《运营管理》，由任建标翻译，机械工业出版社2015年出版。 [2] List of Countries by GDP Sector Composition.维基百科，www.wikipedia.org. 供应链管理的几个“小亲戚” 除了采购、运营和物流管理外，供应链管理还有好几个“小亲戚”，比如运筹学、系统动力学、工业工程、信息技术等。 运筹学为供应链的优化提供了工具，比如线性规划、数理统计等。如果你看20世纪60年代以来的文献，库存计划、生产排程、配送网络优化等领域到处都是运筹学的影子。而供应链的真正优化，也离不开这些数理统计模型。国内高校中，有些供应链教授就是运筹学背景。比如上海交大安泰学院的陈晓荣博士，就是个运筹学专家，现负责全球运营领袖MBA课程（交大和MIT合办，在我看来是国内最好的供应链管理MBA）。 运筹学有很多模型和算法，相对北美而言，也是国内教授比较擅长之处。在美国，供应链管理的顶级研究，比如发表在《管理科学》（Management Science）等上面的论文，大多也离不开数理模型，那些杰出的研究者呢，也是以华人和印度裔为主。 供应链管理的另一个“亲戚”，甚至可以说是“近亲”，是系统动力学。该学科源自麻省理工的Jay Forrester教授，着眼供应链条上各个环节之间的互动，力图全局优化，可以说是供应链管理的鼻祖。作为供应链管理的经典游戏，“啤酒游戏”就是由Forrester教授在20世纪60年代开发的，后来演化成多种版本，用来展示供应链上没法回避的“牛鞭效应”[1]，也能在系统动力学上找到起源。 供应链管理最早在制造业发展起来，而制造业离不开IE和IT—供应链管理的另两个“亲戚”。前者是工业工程，可以说是现代管理之母，生产线、仓储配送设施等的优化，都离不开工业工程；后者是信息技术，比如以ERP为核心的信息系统，撑起了企业和供应链的框架。供应链的流程，特别是订单层面的基本流程，其实是固化在信息系统里。 我在硅谷工作的那些年，经常跟负责ERP的分析员们开玩笑，说公司把我们这些负责供应链业务的人都开掉也没关系，只要保留他们那些维护ERP的人员就行了——企业的基本流程，比如订单处理，是固化在ERP中的，而这些分析员最熟悉ERP和业务流程，招些新人，由他们培训就可以了。 也是因为这个原因，有些公司的供应链改进由CIO牵头。比如时不时有公司联系我，希望我来帮助他们改进供应链管理，联系人的头衔中屡屡就有CIO的字眼。有个计算机巨头邀请我去培训他们的IT人员（主要是分析员），主要原因就是他们最熟悉业务流程，需要承担供应链绩效改进的责任。电商、贸易行业，我就见过好几个CIO在负责制定需求预测、库存计划逻辑。 这有很多问题。最主要的是CIO虽说熟悉基本的业务流程，但并不一定熟悉业务本身。打个比方：CIO很熟悉在ERP里，库存如何从一个库存点转移到另一个库存点，但这跟库存控制没有关系——库存控制取决于合理的需求预测、合理的库存计划，光熟悉那些ERP里的指令是远远不够的。所以，CIO在供应链的组织设计、绩效考核、主干流程方面往往经验不足，因而不是主导供应链绩效改进的合适人选。[2] [1] 简单地说，“牛鞭效应”就是由于信息不对称，需求变动沿着供应链传递时会逐级放大，越是远离需求源，放大的幅度越大。后文会详细探讨。 [2] 这方面有个案例，在我和赵玲合著的《供应链的三道防线：需求预测、库存计划、供应链执行》一书中（145~151页）。该案例讲的是在一个本土企业，供应链改进原来由IT驱动，最后转向由集成供应链来负责，因为后者更加熟悉业务机制。","link":"/%E7%9B%B2%E4%BA%BA%E6%91%B8%E8%B1%A1/"},{"title":"6. Python的高阶函数","text":"Hi，大家好。 我是茶桁。 本节课，我们来学习一下Python中的「高阶函数」。 递归函数 让我们先来了解一下，什么是递归函数。 递归函数就是定义一个函数，然后在此函数内，自己调用自己。 既然是自己调用自己，那这个函数必须要有一个结束才行，否则会一直重复的调用下去，直到调用层数越来越多，最终会导致栈溢出。 让我们先写一个雏形： 1234567891011# 初步认识一下递归函数def recursion(num): print(num) recursion(num - 1)recursion(3)# 执行结果3 2 1 0 -1 -2 -3 -4 -5 -6 -7 ....RecursionError: maximum recursion depth exceeded while calling a Python object 最后，导致栈溢出，程序报错。 那么这个程序到底做了什么？ 首先，我们定义了一个函数，然后执行，执行的时候给了一个参数3。 进入程序之后，先将3打印了一遍，然后在函数内部，又调用了一遍自己，参数为3-1，也就是传了一个参数2，在进入函数之后，打了了2， 继续自己调用自己，传参2-1，1-1, 0-1, ...就这样一直循环下去。 那么我们怎么样让这个程序停下来？就是在函数自己调用自己之前，加上一个限制条件： 1234567891011121314# 初步认识一下递归函数 3 2 1 0def recursion(num): print(num) # 检测当前值是否到0 if num &gt; 0: # 调用函数本身 recursion(num - 1)recursion(3)# 执行结果3210 我们给调用之前加了一个条件，如果num &gt; 0才允许继续执行，这样，当程序传递了1-1之后，执行了最后一次打印，然后就不向下执行了。 不过不要以为程序到这里就结束了，我们多加一行代码试试看： 123456789101112# 初步认识一下递归函数 3 2 1 0def recursion(num): print(num, end=&quot; &quot;) # 检测当前值是否到0 if num &gt; 0: # 调用函数本身 recursion(num - 1) print(num, end=&quot; &quot;) # 又加了一个print函数recursion(3)# 执行结果3 2 1 0 0 1 2 3 如果你不知道程序做了什么，我们稍微分析一下： 1234567891011解析当前递归函数的执行过程：```recursion(3) ==&gt; 3 recursion(3-1) ==&gt; 2 recursion(2-1) ==&gt; 1 recursion(1-1) ==&gt; 0 recursion(0) ==&gt; 0 recursion(1) ==&gt; 1 recursion(2) ==&gt;2recursion(3) ==&gt; 3``` 也就是，在递归函数中，程序是一层一层的进入，然后再一层一层的返回。 这就好像是， 我们在上学的时候，你坐在最后一排，但是你有个心仪的女孩坐在最前面。你想要对方电话，这个时候你传递一个纸条给前面的同学，前面的同学再往前传，一直往前传到女孩手里。女孩看完之后，写完回复再一次次的传回来。最后你满怀期待的打开一看：“滚。” 当然，我们的递归函数和这个不同的地方是最后不会多加那个“滚”字。 回调函数 什么是回调函数呢？ 我们首先来思考一个问题： 1234def func(a): print(a)func(a) 在这个简单的函数中，我们已经学会了传值a给到func()，那么参数到底可以传一些什么进去？a可以是什么？能不能是一个函数呢？ 这就引出了我们现在的内容： 1234567891011121314# 带有回调函数参数的函数def func(obj): print(obj, type(obj)) # 并且在函数中调用传递进来的形参函数 obj()def _self(): print(&quot;i am _self&quot;)func(_self)# 执行结果&lt;function _self at 0x111ed4280&gt; &lt;class 'function'&gt;i am _self 可以看到，我们选择执行的是func函数，但是最后打印出了_self函数中语句。原因就是我们在执行func函数的时候，将_self函数作为参数传递给了func的形参obj， 我们在其中打印了obj以及obj的类型，并且最后执行了一下obj， 实际上也就是执行了一遍_self函数。 如果在一个函数中要求传递的参数是一个函数作为参数，并且在函数中使用了传递进来的函数，那么这个函数我们就可以称为是一个回调函数。 我们拿系统内部的一个现成的函数来重新封装一个新的函数来试试： 123456789101112131415# 做一个数学计算的函数def func(x, y, obj): &quot;&quot;&quot; 此函数用来整合其他的数学运算 在当前函数中，需要接收三个参数，前两个为数值，最后一个为函数 x, y: int f: function :return: &quot;&quot;&quot; print(obj(x, y))func(2, 3, pow)# 执行结果8 在日后使用这个函数的时候，就可以传入数值和要做什么计算的方法，就可以了。 当然，这个函数写的并不完善，比如，我们在执行func(2, 3, sum)的时候就会报错，原因是因为sum()函数内部是要进行迭代的的，然而int类型中没有魔法方法__iter__, 所以无法迭代。所以，要想这个函数具有通用性，还需要在内部完成很多工作。 闭包函数 之前我们在回调函数中将函数作为参数进行了传递，那么问题来了，既然函数能作为参数进行传递，那能不能作为参数被return呢？ 12345678910def person(): money = 0 def work(): print(money) return workperson()# 执行结果&lt;function __main__.person.&lt;locals&gt;.work()&gt; 我们可以看到，work函数被成功返回出来了。但是并未继续执行， 因为其内部的print()没起作用。 我们用一个变量来接收这个返回的函数： 1234567891011def person(): money = 0 def work(): print(money) return workres = person()res()# 执行结果0 说明res接收到返回的work()函数，并且最后执行成功了。 好了，让我们继续为这个函数做一点什么，看看有什么变化。 12345678910111213def person(): money = 0 def work(): nonlocal money money += 100 print(money) return workres = person()res()# 执行结果100 这个结合前几节所讲的内容就很好理解了对吧？ nonlocal关键字拿到上一层函数定义的变量，然后在内层函数中进行使用，最后打印出来。 那我们继续执行会如何？让我们多执行几次： 12345678910111213141516171819# 定义一个函数def person(): money = 0 # 函数中定义了一个局部变量 # 定义内函数 def work(): nonlocal money # 在内函数中使用了外函数的临时变量 money += 100 print(money) # 在外函数中返回了内函数，这个内函数就是闭包函数 return workres = person() # return work res = workres() # res() == work()res()res()res()res()# 此时 就不能够在全局中对money这个局部变量进行任何操作了，# 闭包的作用：保护了函数中的变量不受外部的影响，但是又能够不影响使用 你会不会认为会一直打印100? 让我们看看执行结果到底是怎样的： 12345100200300400500 怎么样，是不是完全没想到？这个就是闭包函数的特点。 在一个函数内返回了一个内函数，并且这个返回的内函数还使用了外函数中局部变量，这个就是闭包函数。其特点为： 在外函数中定义了局部变量，并且在内部函数中使用了这个局部变量 在外函数中返回了内函数，返回的内函数就是闭包函数 ⚠️ 主要在于保护了外函数中的局部变量，既可以被使用，又不会被破坏。 检测一个函数是否为闭包函数，可以使用func.__closure__，如果是闭包函数返回cell 匿名函数 -- lambda表达式 首先，我们先弄清楚什么是匿名函数： 匿名函数的意思就是说可以不使用def来定义，并且这个函数也没有名字。 在Python中，我们可以使用lambda表达式来定义匿名函数。我们需要注意，lambda仅仅是一个表达式，并不是一个代码块，所以lambda又称为一行代码的函数。 在lambda表达式中，也有形参，并且不能够访问除了自己的形参之外的任何数据，包括全局变量。其语法如下： 12# 语法：lambda [参数列表]:返回值 让我们来尝试写写看，我们先来定义一个普通的加法运算的函数： 123456# 封装一个函数做加法运算# 普通函数def sum(x, y): return x+yprint(sum(2, 3)) 毫无疑问，执行结果为5。那么接下来，用lambda该怎么写呢？ 123456# 改成lambda表达式来封装res = lambda x, y:x+yprint(res(4, 4))# 执行结果8 结合闭包函数的讲解，这里就应该很容易看懂了吧？一样的地方就是，使用了一个变量res来接收这个返回的函数，然后执行res函数。 让我们再来一段： 12345res = lambda sex:&quot;很man&quot; if sex=='male' else '很nice'print(res('female'))# 执行结果很nice 只看结果的话，我们很清楚这段函数最后执行到了else语句内。但是是如何进入的呢？让我们将这段代码用普通函数的写法展开来看看： 1234567def func(sex): if sex == 'male': return '很man' else: return '很nice'func('female') 这样是不是很清晰了？回过头来让我们看刚才那段lambda表达式，我们可以这样去看： 12345# lambda 参数列表: 真区间 if 表达式判断 else 假区间lambda sex: '很man' if sex=='male' else '很nice'# 然后用一个变量接收函数res = lambda sex: '很man' if sex=='male' else '很nice' 所以可以看出来，其实lambda十分的方便，并且并不难理解，当你习惯了lambda之后，会非常便捷。 迭代器 迭代器是一个很有意思的功能，可以说是Python中最具特色的功能之一，它是访问集合元素的一种方式。 迭代器是一个可以记住访问遍历的位置的对象。从集合的第一个元素开始访问，直到集合中的所有元素被访问完毕。 迭代器只能是从前往后一个一个的遍历，不能后退。 我们把之前一直使用的range()拿过来看： 123456# range(10, 3, -1) 返回一个可迭代的对象for i in range(10, 3, -1): print(i, end=&quot; &quot;) # 执行结果10 9 8 7 6 5 4 表面上来看，似乎range()本身就是一个迭代器，可是我们来尝试做个实验： 12345x = range(5)print(next(x))# 执行结果TypeError: 'range' object is not an iterator 当我们尝试调用next()函数的时候报错了，被告知range不是一个迭代器。 那么，range不是迭代器，究竟是什么呢？这里我们就要先深入研究下迭代器的特性： 严格来说，迭代器是指实现了迭代协议的对象，迭代协议是指实现了iter方法并返回一个实现了next方法的迭代器对象，并通过StopIterator一场标识迭代完成。 iter() iter()能把迭代的对象，转为一个迭代器对象，其参数为可迭代的对象(str, list, tuple, dict)， 返回值为迭代器对象。其中需要注意的一点是：迭代器一定是一个可以迭代的对象，但是可迭代的对象并不一定是迭代器。 我们在迭代器上使用iter会得到相同的对象： 123456789i = iter([1, 2, 3])print(iter(i) is i)print(id(iter(i)))print(id(i))# 执行结果True44255020964425502096 基于此，我们可以这样实现： 12345i = iter([1, 2, 3, 4])list(zip(i, i))# 执行结果[(1, 2), (3, 4)] next() next()函数可以去调用迭代器，并返回迭代器中的下一个数据。 我们使用iter函数可以从任何可迭代对象中获取一个迭代器： 123456789a = iter([1, 2, 3])print(a)print(next(a))print(1 in a)# 执行结果&lt;list_iterator object at 0x124111a50&gt;1False 可以看到，我们使用iter函数可以从任何可迭代对象中获取一个迭代器。而且迭代器有个特点，即每次用完一个元素即消耗掉该元素，不会保留在迭代器中，也就是说，是一次性的。 1234567891011res = iter([1, 2, 3, 4])print(next(res), end=&quot; &quot;)print(next(res), end=&quot; &quot;)print(next(res), end=&quot; &quot;)print(next(res), end=&quot; &quot;)# print(next(res))r = list(res)print(r)# 执行结果1 2 3 4 [] 可以看到，list里面已经空了。 我们用for来取值： 123456789res = iter([1, 2, 3, 4])for i in res: print(i, end=&quot; &quot;)r = list(res)print(r)# 执行结果1 2 3 4 [] for直接将迭代内的元素全部取完了，所以最后打印下一个值的时候也显示空了。所以我们可以得到迭代器的取值方案： 迭代器的取值方案 next()：调用一次获取一次，直到数据被取完。 list()：使用list函数直接取出迭代器中的所有数据。 for：使用for循环遍历迭代器的数据 总结一下： 迭代器是一个可以记住遍历的位置的对象。 迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。 迭代器有两个基本的方法：iter() 和 next()。 字符串，列表或元组对象都可用于创建迭代器： 那么，再回过头来看看range range可以像任何其他可迭代对象一样循环使用，但是它并不具备迭代器中的一些特性，比如，我们之前实验过，range并不能使用next方法，而我们可以从range中得到一个迭代器： 1234iter(range(3))# 执行结果&lt;range_iterator at 0x124184570&gt; 我们在迭代器中使用元素就会消耗掉该元素，但是我们遍历一个range对象并不消耗它， 比如： 很明显我们可以重复使用。 来一个更直接的，我们之前用for获取了迭代里的值，我们对range()也来使用一下看看会不会有不同的结果： 123456789res = range(1,5)for i in res: print(i, end=&quot; &quot;)r = list(res)print(r)# 执行结果1 2 3 4 [1, 2, 3, 4] 一样的代码，对象不同。我们可以明显看到区别，range拿到最后里面的元素并没有减少。这也说明了，range并不是迭代器。 实际上，range的迭代是通过iter协议来实现的，只是一种类似迭代器的鸭子类型，并非真正的迭代器。 其实，有一种可以直接检测迭代器和可迭代对象的方法： 12345678910111213141516171819202122232425# 检测迭代器和可迭代对象from collections.abc import Iterator, Iterable varstr = '123456'res = iter(varstr)r = range(1, 5)# isinstance() 检测一个数据是不是一个指定的类型# Iterable: 迭代对象，Iterator: 迭代器r1 = isinstance(varstr, Iterable) r2 = isinstance(varstr, Iterator)r3 = isinstance(res, Iterable)r4 = isinstance(res, Iterator) r5 = isinstance(r, Iterable) r6 = isinstance(r, Iterator)print(f'varstr 是迭代对象：{r1}, \\t varstr 是迭代器: {r2}')print(f'res 是迭代对象：{r3}, \\t res 是迭代器: {r4}')print(f'r 是迭代对象：{r5}, \\t r 是迭代器: {r6}')# 执行结果varstr 是迭代对象：True, varstr 是迭代器: Falseres 是迭代对象：True, res 是迭代器: Truer 是迭代对象：True, r 是迭代器: False 今天的知识点讲到这就结束了，接下来，让我们来做两个小练习。 练习题 递归查询斐波那契数列位数 还记得我们之前讲过的斐波那契数列吗？不记得没关系，我们来复习一下： 1# 斐波那契数列: 0, 1, 1, 2, 3, 5, 8, 13... 我们这次来实现一个函数，用于查询斐波那契数列中当前位置的数值是多少。 1234567891011121314# 递归实现斐波那契数列def fibonacci(n): if n == 1: return 0 elif n == 2 or n == 3: return 1 else: return fibonacci(n-1) + fibonacci(n-2) res = fibonacci(6)print(res)# 执行结果5 我为大家画了张图，来看看程序内部到底做了些什么： 从这张图中，我们可以看到递归的步骤和返回的结果。 递归实现阶乘 什么是阶乘？比如我们实现7的阶乘，那么就是 \\[ 1\\times2\\times3\\times4\\times5\\times6\\times7 \\] 让我们来试着实现一下: 123456789101112# 实现阶乘def factorial(n): if n == 1: return 1 else: return n*factorial(n-1) res = factorial(7)print(res)# 执行结果5040 验证一下看看： 1234print(1*2*3*4*5*6*7)# 执行结果5040 看来结果没问题，那让我们来看看程序内发生了什么： 12345678910111213141516'''factorial(7) =&gt; 7 * factorial(6) =&gt; 6 * factorial(5) =&gt; 5 * factorial(4) =&gt; 4 * factorial(3) =&gt; 3 * factorial(2) =&gt; 2 * factorial(1) =&gt; factorial(1) = 1 2 * 1 = 2 3 * 2 = 6 4 * 6 = 24 5 * 24 = 120 6 * 120 = 7207 * 720 = 5040''' 虽然实现了，最后还是不得不说几点注意事项： 递归函数的效率并不高，所以尽量能不用就不要用。 一个函数如果调用后没有结束，那么栈空间中就一直存在，直到这个函数运算结束才会销毁。 好了，今天的课程到此结束。大家课后记得多练习。下课！","link":"/Higher-order-functions/"},{"title":"7. Python的内置函数","text":"Hi，大家好。我是茶桁。 讲完了基础函数和高阶函数之后，我们这一节来研究下Python的内置函数，看看Python在安装完毕之后的解释器里，到底都预先给我们提供好了哪些可用的函数。 本节内容着重介绍一些常用函数，并且会做一些应用上的示例。当然，对于Python的内置函数，我们还可以查询官方文档，我这节参照的为3.10版本文档 range()函数 这几节课中，我们频繁使用并且着重介绍过这个函数，那我们就从它开始介绍吧。 一般我们需要遍历一个数值序列的时候，range()函数就会派上用场，它生成算数级数。 12345678910'''range() 函数功能： 能够生成一个置顶的数值序列参数： start: 开始的值，默认为0 stop: 结束的值 [, step]: 可选，步进值， 默认为1返回值： 可迭代的对象，数字序列'''range(start, stop, [, step]) 让我们来看一下： 12345res = range(10)print(res, type(res))-----------------------------range(0, 10) &lt;class 'range'&gt; 可以看到这其实就是一个range的类，其实在我们Python中，任何数据都是一个对象而已。 来看案例： 123456# range函数的使用方式res = range(11)print(list(res))-----------------------------[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 当我们的range内只写一个参数时，这个参数就是stop值，也就是从start的默认值0开始到输入的参数值（stop）之前为止，比如这段代码中，stop会结束到11之前，也就是10。 我们在这段代码中，将range的内容转化成一个list并打印了出来。当然，我们也可以使用循环，依次去除range内的内容： 12345for i in res: print(i, end=&quot; &quot;) -----------------------------0 1 2 3 4 5 6 7 8 9 10 记得上节课我们提到过，range()是不支持next()函数的，不过如果我们将其转成迭代器，就可以使用next()函数调用： 123456789res = iter(range(11))print(next(res))print(next(res))print(list(res))-----------------------------01[2, 3, 4, 5, 6, 7, 8, 9, 10] 可以看到，使用iter转成迭代器之后，可以正常使用next()函数，并且我们再次查看res的内容，0,1已经被拿走，只将剩余内容转化为list打印了出来。 当我们在range中添加两个参数的时候，start就是第一个参数，第二个参数就是stop值。 123456# 添加两个参数for i in range(5, 10): print(i, end=&quot; &quot;)-----------------------------5 6 7 8 9 当我们输入三个参数的时候，第一个参数为start, 第二个参数为stop, 第三个参数就是[, step]， 比如： 123456# 添加三个参数for i in range(1, 10, 3): print(i, end=&quot; &quot;) -----------------------------1 4 7 这段代码的含义就是从1开始, 以3为步进来提取数字，并打印出来，一直到10之前的数字为止。 如果不太理解步进值的可以执行数一遍就理解了，比如我们从1开始顺序往后数3个数，那就是2、3、4，数到了4, 再继续往后数3个数，就是5、6、7，数到了7。再继续往后就是8、9、10。但是，我们代码中的stop值为10，所以到9就结束了，也就是说，我们这段代码就只取出了1, 4, 7三个值。 三种参数值的情况我们都了解之后我们可以思考下，难道我们只能选择顺序取值吗？其实不然，我们还可以倒叙取值，聪明的小伙伴可能想到了，调换一下start和stop值不就可以了嘛？我们从10开始取值，取到0为止： 12for i in range(10, 0): print(i, end=&quot; &quot;) 执行一下，哎，似乎什么都没打印出来。这又是为什么呢？是不是出BUG了？ 其实，什么都没打印出来才是正确的，这是因为，虽然我们给了开始和结束值，但是我们遗忘了一个重要的参数，那就是步进值step，这个值默认可是1，从10开始+1来计数，无论如何也算不到0。所以，我们将步进值改成负数，也就是倒着数了: 12345for i in range(10, 0, -1): print(i, end=&quot; &quot;) -----------------------------10 9 8 7 6 5 4 3 2 1 至此，我们可以得到结论，是否倒叙取值除了开始和结束值，更重要的是看step是正数还是负数。 123456res = range(-10,-20,-1) # [-10, -11, -12, -13, -14, -15, -16, -17, -18, -19]res = range(-20,-10) # [-20, -19, -18, -17, -16, -15, -14, -13, -12, -11]res = range(-10,10)# [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9] zip()函数 zip() 函数可以接受多个可迭代的对象，然后把每个可迭代对象中的第i隔元素组合在一起，形成一个新的迭代器。 12345'''参数： *iterables, 任意个的可迭代对象返回值： 返回一个元组的迭代器'''zip(*iterables) 让我们来直接看示例： 123456789n1 = '1234'n2 = ['a', 'b', 'c']n3 = ['A', 'B', 'C', 'D']# 调用zip函数，合成新的元组迭代器res = zip(n1, n2, n3)print(list(res))-----------------------------[('1', 'a', 'A'), ('2', 'b', 'B'), ('3', 'c', 'C')] 我知道你们看到这个执行结果会有很多疑问，先别着急，我们先看一下它是否包含迭代器的特性： 12for i in res: print(i) 当你执行这段代码的时候就会发现，似乎什么都没发生。 那到底是怎么回事？我们不用for，让我们再转换一次list之后看看里边有什么： 1234print(list(res))-----------------------------[] 列表居然是空的。是不是瞬间想到了什么？ 没错，这个似乎就是迭代器的特性之一，当其中元素被使用之后，会删掉使用过的元素。而我们之前在执行print(list(res))的时候，已经将内部元素都转成list并展现过，所以现在res内的元素都被删掉了。 没事，让我们再重新来定义一次，也就是重新给res内填满元素然后直接for循环一次看看： 12345678910111213n1 = '1234'n2 = ['a', 'b', 'c']n3 = ['A', 'B', 'C', 'D']# 调用zip函数，合成新的元组迭代器res = zip(n1, n2, n3)for i in res: print(i) -----------------------------：('1', 'a', 'A')('2', 'b', 'B')('3', 'c', 'C') 我们可以看到，每次打印i的时候都打印了一个元组，而这个元组就是一个新元素，比如第一行('1', 'a', 'A'), 这整个元组就是一个新元素。 让我们再用next试试（当然我又重新填满了res）： 123456print(next(res))print(next(res))-----------------------------('1', 'a', 'A')('2', 'b', 'B') next函数也能正常执行，那可以说明，zip确实组合成了一个新的迭代器。 现在我们返回来再看一遍代码中的n1,n2,n3, 分别是1234, [‘a’, 'b', 'c'], ['A', 'B', 'C', 'D']。最后组成的迭代器对象为：[('1', 'a', 'A'), ('2', 'b', 'B'), ('3', 'c', 'C')]。 通过分析可以看出来，zip的工作原理是先分别取可迭代对象的第一个元素组合成一个元组，然后再分别取第二个元素组合成一个元组，依次往后取... 可是n1, n3分别都是四个元素，为什么我们最后只组合成了三个元组？那是因为n2中只包含了三个元素，当在其中找不到第四个元素的时候，就会放弃组合。 来，让我们在看一个示例： 1234567n1 = [1, 2, 3, 4]n2 = [22, 33, 44, 55]res = zip(n1, n2)print(list(res))-----------------------------[(1, 22), (2, 33), (3, 44), (4, 55)] 大家看到最后的执行结果有没有觉得很眼熟？可能很多小伙伴一时间想不到，我们来调整一下： 123456[ (1, 22), (2, 33), (3, 44), (4, 55)] 记住这个数据结构，我们在后期做数据分析的时候， 当我们做矩阵运算的时候用的非常多。 不知道大家是否都学过高等数学里的线性代数、微积分，包括概率统计。这些在我们之后做数据分析，数据挖掘，包括机器学习、人工智能这些科学运算里面，非常重要的一些数学功底。 不太记得了也没关系，这些我后面将会专门拿几节出来给大家补一下这方面。 让我们继续，zip还有一种应用方式，当其与*运算符结合使用的时候，可以用来拆解列表： 12345678910# zip 与 * 运算符相结合使用x = [1, 2, 3]y = [4, 5, 6]print(zip(x, y))print(*zip(x, y))-----------------------------&lt;zip object at 0x107b8d200&gt;(1, 4) (2, 5) (3, 6) 可以看到，zip是一个迭代器，*zip这生成了组合好的多个元组数据。 比如： 12345678x1 = [1, 2, 3]y1 = [4, 5, 6]x2, y2 = zip(*zip(x, y))print(x2, y2)-----------------------------(1, 2, 3) (4, 5, 6) 这样，我们就将两个列表转换成了两个元组。当然，其实我们这样操作还不如直接使用tuple函数来的方便快捷一点。 那下面，我们就看看都有哪些数据类型转换相关的内置函数。 数据类型转换相关的内置函数 这些函数的功能非常简单和单一，属于拿来就用的函数，我们就仅列出来，不多做介绍了。 int() 将其它类型数据转为整型 float()转为浮点类型 bool()转为布尔类型 complex()转为复数 str()转为字符串类型 list 转为列表类型 tuple转为元组类型 dict 转为字典类型 set 转为集合类型 变量相关函数 id() 获取当前数据的ID标识 type() 获取当前数据的类型字符串 print()数据的打印 input()获取输入的数据 isinstance()检测是否为指定的数据类型 数学相关函数 abs()获取一个数的绝对值 1234print(abs(-99.99))-----------------------------99.99 sum()求和 从 start 开始自左向右对 iterable 中的项求和并返回总计值 1234print(sum([1,2,3]))-----------------------------6 max() 获取最大值 123456print(max([1,2,3]))print(max(99,12,45))-----------------------------399 min() 获取最小值 123456print(min([2,1,6,-9]))print(min(6,7,1,0,-2))------------------------------9-2 pow(x, y)幂运算 返回 x 的 y 次幂 1234print(pow(2,3)) -----------------------------8 round(x, n) 对x四舍五入，小数点保留n位 123456print(round(3.1415926))print(round(3.1415926,2))-----------------------------33.14 round这个函数不是绝对意义上的四舍五入，在取整这个问题是是奇进偶退： 123456print(round(3.5))print(round(4.5))-----------------------------：44 进制函数及字符集 bin() 将数值类型转为二进制 1234print(bin(123)) -----------------------------0b1111011 int() 将二进制转化为整型 1234print(int(0b1111011))-----------------------------123 oct() 转为八进制数 1234print(oct(123))-----------------------------0o173 hex() 转为十六进制数 1234print(hex(123))-----------------------------0x7b ASCII及字符集 ASCII，全称为美国信息互换标准代码。是一套基于拉丁字母的字符编码，共收录了 128 个字符，用一个字节就可以存储，它等同于国际标准 ISO/IEC 646。它一共有128个支付，最后更新是1986年。 我们要知道的是，ASCII 编码是美国人给自己设计的，他们并没有考虑欧洲那些扩展的拉丁字母，也没有考虑韩语和日语，我大中华几万个汉字更是不可能被重视。计算机也是美国人发明的，起初使用的就是 ASCII 码，只能显示英文字符。各个国家为了让本国公民也能正常使用计算机，开始效仿 ASCII 开发自己的字符编码，例如 ISO/IEC 8859（欧洲字符集）、shift_Jis（日语字符集）、GBK（中文字符集）等。 从65开始到90为止，是大写字母（A ~ Z), 97到122是小写字母(a ~ z)，48到57是0 ~ 9。 而我们经常使用的是GB2312-80, GBK和GBK18030以及Unicode字符集。 GB2312-80 是 1980 年制定的中国汉字编码国家标准。共收录 7445 个字符，其中汉字 6763 个。 GBK 于1995年制定 收录了 21003 个汉字。GBK向下与 GB 2312 编码兼容， GBK18030 2001年的1月正式强制执行，是我国制订的以汉字为主并包含多种我国少数民族文字（如藏、蒙古、傣、彝、朝鲜、维吾尔文等）的超大型中文编码字符集强制性标准，其中收入汉字70000余。 Unicode（统一码、万国码、单一码）是计算机科学领域里的一项业界标准，包括字符集、编码方案等。 它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。1990年开始研发，1994年正式公布。 UTF-8 以字节为单位对Unicode进行编码。 我们现在写代码的时候基本遵循UTF-8编码为主。 有的时候，我们是需要将字符转为ASCII， 也有对应的方法： 1234print(ord('a'))-----------------------------a 将ASCII转为字符也一样： 1234print(chr(65))-----------------------------A 高阶函数 和上一节课不同，我们现在要讲的高阶函数，是Python解释器里的内置高阶函数。 sorted() 很多时候，我们在处理数据的时候都需要对数据进行排序。不管是以序号，名称还是日期的方式。sorted()就是我们最常用的排序函数： 12345678910sorted(iterable, [reverse, key])‘’‘运行原理：把可迭代数据里面的元素，一个一个的取出来，放到key这个函数中进行处理，并按照函数中return的结果进行排序，返回一个新的列表功能：排序参数： iterable:可迭代的数据 （容器类型数据，range数据序列，迭代器） reverse:可选，是否反转，默认为False，不反转， True反转 key:可选， 函数，可以是自定义函数，也可以是内置函数返回值：排序后的结果’‘’ 我们来看几个示例，首先我们先来看看默认的排序方式：从小到大： 123456arr = [3,7,1,-9,20,10]res = sorted(arr) print(res)-----------------------------[-9, 1, 3, 7, 10, 20] 当然，既然我们能从小到大来进行排序，那就可以用从大到小的方式： 12345arr = [3,7,1,-9,20,10]print(sorted(arr,reverse=True))-----------------------------[20, 10, 7, 3, 1, -9] 现在我们得到了从小到大排序，也得到了从大到小排序。然后我们再来作妖：能不能按照所有数字的绝对值大虾哦进行排序呢？哎，还记得我们刚讲过的数学相关的函数里有一个求绝对值的函数嘛？既然sorted()这个函数里的参数key可以接收函数，那让我们结合在一起试试看： 123456arr = [3,7,1,-9,20,10]res = sorted(arr,key=abs)print(res)-----------------------------[1, 3, 7, -9, 10, 20] 果然，我们得到了想要的结果。来分析下内部到底做了什么： 1234[3,7,1,-9,20,10] # 原始列表3 7 1 9 20 10 # 求绝对值1 3 7 9 10 20 # 给绝对值进行排序1 3 7 -9 10 20 # 转换成原本的值 那现在，我再多尝试一下，我试试看自己定义一个函数： 12def func(num): return num % 2 函数定义好了，让我们尝试使用自定义函数对数据进行排序： 12345arr = [3,2,4,6,5,7,9]print(sorted(arr, key = func))-----------------------------[2, 4, 6, 3, 5, 7, 9] 看似起结果了。那到底函数内干了些什么呢？让我们在其中多打印一点东西出来，看个明白： 1234567891011121314151617def func(num): print(num, num % 2, end=&quot; &quot;) print() return num % 2 arr = [3,2,4,6,5,7,9]print(sorted(arr, key = func))-----------------------------3 1 2 0 4 0 6 0 5 1 7 1 9 1 [2, 4, 6, 3, 5, 7, 9] 这样我们就很清晰的看到了对原数字和取余结果，在对取余进行排序之后，再在取余的结果上进行默认的从小到大进行排序，就得到了最后的结果[2, 4, 6, 3, 5, 7, 9] 不过，这种功能大多数时候我们基本是临时用一下，特意写一个方法似乎有点多余。还记得咱们之前讲的匿名函数吧？让我们用匿名函数优化一下： 1234567# 用匿名函数优化arr = [3,2,4,6,5,7,9]res = sorted(arr, key=lambda num:num%2)print(res)-----------------------------[2, 4, 6, 3, 5, 7, 9] 正是我们所要的结果。 从这就能看出来，高阶函数sorted()的key因为能接收自定义函数，所以给了我们很大的可玩空间。小伙伴们还能想到哪些方法，快去做做实验。 map() 这个函数在对传入的可迭代数据中的每一个元素进行处理，然后返回一个新的迭代器： 12345678map(func, *iterables)'''功能： 对传入的可迭代数据中的每个元素放入到函数中进行处理，返回一个新的迭代器参数： func 函数 自定义函数|内置函数 iterables：可迭代的数据返回值：迭代器''' 让我们先来看一个普通的函数： 12345678910# 把一个字符串数字的列表转为整型列表items = ['1', '2', '3', '4']new_list = []for i in items: new_list.append(int(i))print(new_list)-----------------------------[1, 2, 3, 4] 我们将一个内部元素均为字符串的列表，转成了一个整型列表。 不过这个函数看起来似乎还是有些麻烦，让我们再用map试试看： 123456items = ['1', '2', '3', '4']res = map(int, items)print(list(res))-----------------------------[1, 2, 3, 4] 看，是不是简便多啦？当然，最后map最后生成的是迭代器而并不是列表，我们还是需要转化一下数据类型。 这段代码中map的处理方式其实非常简单，它将items里的每一个元素用int方法转换成整型，转换完之后形成一个新的迭代器，然后返回。 再让我们看一个示例感受一下, 这次我们将两段对比写在一起： 123456789101112131415161718# 普通方法进行实现items = [1, 2, 3, 4]res = []for i in items: x = i ** 2 res.append(x)print(res)# 使用map函数处理items = [1, 2, 3, 4]def func(x): return x ** 2res = map(func, items)print(res, list(res))-----------------------------[1, 4, 9, 16]&lt;map object at 0x107ea5030&gt; [1, 4, 9, 16] 我们看到执行结果完全一样，不过使用map()的方式更简洁，逻辑也更清晰。我们要知道，Python本身是自带幂次方方法的。即便是我们自己来实现，其实我们还可以把代码写的更简洁： 123456items = [1, 2, 3, 4]res = map(lambda x:x**2, items)print(res,list(res))-----------------------------&lt;map object at 0x107c98610&gt; [1, 4, 9, 16] 基于map的应用，我们来个小作业吧： 我们现在有这样一个列表：['a','b','c','d'] 要求将其转换成：[65,66,67,68] 最后，再给大家留个课后作业， 我给大家两个函数及其功能介绍，大家自己去尝试看看，然后自己琢磨下其用法。 reduce(func, iterable) 功能：每一次从 iterable 拿出两个元素，放入到func函数中进行处理，得出一个计算结果，然后把这个计算结果和iterable中的第三个元素，放入到func函数中继续运算，得出的结果和之后的第四个元素，加入到func函数中进行处理，以此类推，直到最后的元素都参与了运算 filter(func, iterable) 功能：过滤数据，把 iterable 中的每个元素拿到 func 函数中进行处理，如果函数返回True则保留这个数据，返回False则丢弃这个数据。 这两个函数在处理数据上作用都非常大。 好了，下课。大家有问题记得留言。","link":"/python-Built-in-functions/"},{"title":"8. 数据类型 - 字符串详解","text":"Hi, 大家好。我是茶桁。 前几节课中我们学习了函数，那么这节课开始，我们花几节课返过头来详细的学习一下Python内的数据类型。第一节课，让我们先从字符串开始： 回顾字符串的定义方式 了解转义字符 字符串格式化的方法 字符串相关函数 字符串的定义方式 单引号定义字符串 ‘ ’ 双引号定义字符串“ ” 三引号定义字符串‘’‘内容’‘’或者“”“内容”“” 字符串定义时，引号可以互相嵌套 转义字符 一个普通的字符出现在转义符\\的后面时，实现了另外一种意义。 \\ 转义符，续行符。 作为转义符时，在\\后面出现的字符可能会实现另外一种意义。 作为续行符时，在行尾使用了\\后，可以换行继续书写内容。 123456str = '123'\\ '456'print(str)---123456 打印结果看，并未换行，说明续行符起作用了。 \\n 代表一个换行符 123456str = &quot;岁月是一把杀猪刀， \\n但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪刀， 但是它拿长得丑的人一点办法都没有。 \\r代表光标位置（从\\r出现的位置开始作为光标的起点） 12345str = &quot;岁月是一把杀猪刀， \\r但是它拿长得丑的人一点办法都没有。&quot;print(str)---但是它拿长得丑的人一点办法都没有。 \\t代表一个水平制表符（table 缩进） 12345str = &quot;岁月是一把杀猪刀\\t但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪刀 但是它拿长得丑的人一点办法都没有。 \\b 代表一个退格符 12345str = &quot;岁月是一把杀猪刀\\b但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪但是它拿长得丑的人一点办法都没有。 注意看，并不是毫无改变的打印出来了，整句话中\\b前面的刀这个字被退格了。 \\\\ 反转义\\，输出了\\，取消\\的转义效果 12345str = &quot;岁月是一把杀猪刀\\\\n但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪刀\\n但是它拿长得丑的人一点办法都没有。 第二个\\被前面的\\转义了，所以n就不会再被转义，也就没有换行。 r， 如果我们想把转义字符也作为普通字符输出，那我们可以在字符串的最前面加上r 12345str = r&quot;岁月是一把杀猪刀\\n但是它拿长得丑的人一点办法都没有。&quot;print(str)---岁月是一把杀猪刀\\n但是它拿长得丑的人一点办法都没有。 字符串内的转移字符\\n被打印了出来。 字符串相关的操作 字符串+操作, 将参与运算的字符串相加后组成一个新的字符串。 123456789str=&quot;君不见，黄河之水天上来，奔流到海不复回。&quot;str2 = &quot;君不见，高堂明镜悲白发，朝如青丝暮成雪。&quot;res = '将进酒\\n'+ str + '\\n' + str2print(res)---将进酒君不见，黄河之水天上来，奔流到海不复回。君不见，高堂明镜悲白发，朝如青丝暮成雪。 字符串*操作，str*n就是 将当前字符串重复n遍 1234567str = '重要的话说三遍\\n' * 3print(str)---重要的话说三遍重要的话说三遍重要的话说三遍 字符串[]切片操作 字符串的索引操作，字符串中只能使用[]下标访问，不能修改。 123456str[start:stop:step]功能，获取str的特定下标或者对str进行切片操作参数： start: 可选，开始值，默认为0 stop: 可选，结束值，默认为len(str) step：可选，步进值，默认为1 因为所有参数都是可选项，所以其实我们可以什么参数都不给，直接使用默认值： 12345678str = '凡诗之所谓风者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。'print(str)# 等同于print(str[::])---凡诗之所谓风者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。凡诗之所谓风者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。 当我们写一个值，那就是获取指定下标的元素： 1234print(str[6])---者 但是当我们只写一个值，并且后面跟上符号::, 那含义就是从start开始，向后取完： 1234print(str[6::])---者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。 从这我们可以看出来，当我们只写一个单独的值而没有加::的时候，含义就是从start开始，但是并不向后继续取值，而有了::就是继续向后取值。其实，只写一个:也是一样的，因为只要知道向后取值，step默认值就是为1: 1234print(str[6:])---者，多出于里巷歌谣之作，所谓男女相与咏歌，各言其情者也。 那如果我们在这个基础上加上一个值，那就是从start开始直到stop之前。和range()一样，取不到stop。 1234print(str[2:6])---之所谓风 然后再多加一个值，和range()一样，就是往后数step个数再取值： 1234print(str[2:15:2])---之谓者多于巷谣 其实，这里比较饶的并不是如何取值，二是::这两个符号。当我们将上面讲的这些内容了解通透后，就可以玩转字符串的切片了。 那对应的，如果我们想将字符串完全取值，但是是隔一个取一个，那我们就可以使用start和stop的默认值，而只定义step 1234print(str[::2])---凡之谓者多于巷谣作所男相咏，言情也 那如果我们想让整个字符串倒过来呢？ 1234print(str[::-1])---。也者情其言各，歌咏与相女男谓所，作之谣歌巷里于出多，者风谓所之诗凡 字符串的格式化方法 常用的字符串的格式化方法就是format() 先让我们看看最普通的方式： 12s = '茶桁'str = '乘舟将欲行，忽闻岸上踏歌声。' 我定义了这两个字符串，现在我想将两段字符串合在一起变成一句“茶桁乘舟将欲行，忽闻岸上踏歌声。”（嗯，权吾乃青蓮居士。） 很多小伙伴是不是觉得太简单了，我们之前学了+号，直接拼接不就好了。自然也是可以的，只是我们现在要用更普遍和便捷的方式来完成： 123456s = '茶桁'str = '{}乘舟将欲行，忽闻岸上踏歌声。'.format(s)print(str)---茶桁乘舟将欲行，忽闻岸上踏歌声。 假如说，我们现在只有诗词的大半句，其中少了踏歌行这三个字，那我们又该如何？那我们就往format中传入两个参数，后面那个参数自定义出这三个字符就可以了： 123456s = '茶桁'str = '{}乘舟将欲行，忽闻岸上{}。'.format(s, &quot;踏歌行&quot;)print(str)---茶桁乘舟将欲行，忽闻岸上踏歌行。 看到这里，我们是不是认为字符串使用format就只能顺序传值？第一个答案填入第一个空，第二个答案填入第二个空... 其实不只是如此，字符串后使用format，其中的{}还可以接受索引传参： 123456s = '茶桁'str = '{1}乘舟将欲行，忽闻岸上{0}。'.format(&quot;踏歌行&quot;, s)print(str)---茶桁乘舟将欲行，忽闻岸上踏歌行。 通过索引传参的适用范围毕竟还是有限，我们很容易一不小心就会把参数顺序搞乱。那还有没有其他办法呢？ 我们还可以通过关键字传参： 12345str = '{s2}乘舟将欲行，忽闻岸上{s1}。'.format(s1 = &quot;踏歌行&quot;, s2 = &quot;茶桁&quot;)print(str)---茶桁乘舟将欲行，忽闻岸上踏歌行。 那假如说我们得到的是一个列表数据，是否需要先转换数据？其实也没必要，format支持对容器型数据的传参数： 12345str = '豪放派：{0[0]}，婉约派：{0[1]}，流氓派:{0[3]},蛋黄派：{0[2]}'.format(['李白','辛弃疾','达利园','茶桁'])print(str)---豪放派：李白，婉约派：辛弃疾，流氓派:茶桁,蛋黄派：达利园 那么如果是字典类型的呢？那就更简单了，我们之前提到的关键字传参，不就正好对应字典吗？ 123456dict = {'a':'茶桁', 'b':'蛋黄派'}str = '{a}乘舟将欲行，忽闻岸上{b}'.format(**dict)print(str)---茶桁乘舟将欲行，忽闻岸上蛋黄派 嗯，不错。似乎我们创建了一句新的诗句。 其实，format还有其他的用法，就是直接用关键字f， 比如： 12345str = f'{dict[&quot;a&quot;]}乘舟将欲行，忽闻岸上{dict[&quot;b&quot;]}'print(str)---茶桁乘舟将欲行，忽闻岸上蛋黄派 f是在3.7版本中新增的格式化方法，在使用的过程中，要注意字符串符号“”和‘’的嵌套关系。 在基本使用之外，我们还有一些风骚的特殊用法，比如，我们可以用format直接限定小数的位数： 12345str = '圆周率是多少：{:.5f}'.format(3.1415926)print(str)---圆周率是多少：3.14159 字符串相关函数 在Python中，字符串应该是最常见的数据类型，对应字符串的函数也有不少。大家可以去看看官方的文档 英文字符与字符检测相关函数 我们可以返回字符串的副本，并且将首字母大写，其余小写： 12345str = 'I am a data product manager'str.capitalize()---'I am a data product manager' 因为我在使用Jupyter Notebook，所以即便我么有使用print，依然可以打印出执行结果。只是仅可以打印最后一个执行的函数。 可以把字符串中的一个单词的首字母大写： 1234str.title()---'I Am A Data Product Manager' 可以全部改为大写： 1234str.upper()---'I AM A DATA PRODUCT MANAGER' 把字符串全部改为小写 1234str.lower()---'i am a data product manager' 字符串中的大小写字符转换，大写转小写，小写转大写: 1234str.swapcase()---'i AM A DATA PRODUCT MANAGER' 检测字符是否包含在字符串内： 1234print('o' in 'love')---True 检测字符串是否为全部大写字母组成 1234str.isupper()---False 检测字符串是否为全部小写字母组成 1234str.islower()---False 检测字符串是否符合标题title的要求 1234str.istitle()---False 检测字符串是否由数字和字母组成，如果字符串中包含来非数字字母的其它字符，则返回False 1234str.isalnum()---False 检测字符串是否全部由字符(包含英文字符和中文)组成 1234str.isalpha()---False 检测字符串是否由纯数字字符组成 1234'123'.isdigit()---True 检测当前字符串是否为 空格 字符组成 ' ’ 1234' '.isspace()---True 检测字符串是否以指定的字符开始的，也可以指定开始和结束的位置 1234str.startswith('I')---True 1234str.startswith('a', 5)---True 检测字符串是否以 指定的字符 结束的，也可以指定开始和结束的位置 12345678print(str.endswith('a'))print(str.endswith('a', 5, 11))print(str.endswith('a', 1, 6))---FalseTrueTrue 字符串的查找和操作相关函数（✨ 重点） 前面铺垫了那么多之后，接下来这部分，才是这一节的重点。 让我们先从查找来看： str.find(sub[, start[, end]]) find会返回一个子字符串，找到字符中符合条件的第一个字符出现的索引位置，未找到则返回-1 12345str = &quot;I am a data product manager.&quot;print(str.find('am'))---2 让我们用切片的方式反过来找一下看看： 12345res = str.find('am')str[res:res+2]---'am' 我们从之前可以知道res取值为2，现在等于是str[2:4]， 正好是am所在的位置。 find中有start和end，是支持切片查找的： 123456print(str.find('am', 0, 4))print(str.find('am', 4, 10))---2-1 可以看到，在从4开始找到10的时候找不到am, find有一个功能相同，但是方向不同的方法rfind(), 和find的不同点只是，rfind是从后往前找的。 str.index(sub[, start[, end]]) 类似于find()， 但在找不到子字符串的时候会引发ValueError 1234str.index('python')---ValueError: substring not found str.count(sub[, start[, end]]) 这个函数会在字符串中去查找sub在其中[start, end]范围内非重叠出现的次数。 123456print(str.count('a'))print(str.count('a', 5, 12))---63 接下来让我们看看字符串操作相关的函数： str.split(sep=None, maxsplit=-1) 这个方法可以按照指定的分隔符(sep)，把字符串分隔成列表。 12345str = 'user_admin_id_123'str.split('_')---['user', 'admin', 'id', '123'] 整个方法里的maxsplit是进行多少次拆分，比如1为一次拆分，也就是会返回2个元素。默认值为-1，意思是不限制拆分次数。 1234str.split('_', 1)---['user', 'admin_id_123'] str.rsplit(sep=None, maxsplit=-1) 和split方法相似，只是方向不同。这个是从后向前获取。 1234str.rsplit('_')---['user', 'admin', 'id', '123'] 这段代码可以看到功能上是完全一样的，如果我们把maxsplit加进去，就能看到方向上的不同： 1234str.rsplit('_', 1)---['user_admin_id', '123'] 这样就能清晰看到，rsplit是从后面开始拆分的。 str.join(iterable) join的功能和split可以看成是相反的，是使用指定的字符串，把一个容器中的元素连接成一整个字符串 12345str = ['user', 'admin', 'id', '123']'_'.join(str)---'user_admin_id_123' str.strip([chars]) 去除字符串左右两侧的指定字符, chars参数为置顶要溢出字符的字符串，默认移除空白符。 12345str = ' chaheng 'str.strip(' ')---'chaheng' 这个函数有两个伴生函数，一个是rstrip， 从方法名应该能猜的出来，这是去掉字符串右侧的指定字符，另一个是lstrip， 这是去除左侧的指定字符。 123456str.rstrip(' ')str.lstrip(' ')---' chaheng''chaheng ' len()函数可以获取当前字符串的长度 1234len(str)---9 str.replace(old, new[, count]) 可以替换对应的字符串，将old都替称为new。count则是替换次数。比如一个字符串内出现了十次old， 我`count给的5, 则只替换前5次出现的old字符串。 1234567str = 'abcabcabcabcabcabc'str.replace('a', 'e')str.replace('a', 'e', 2)---'ebcebcebcebcebcebc''ebcebcabcabcabcabc' 可以注意一下两次打印的区别。 这次就不留练习题了，字符串的查询和操作函数属于重中之重，大家最好是多去练习几遍，将其中的方法记会杯熟。 好，今天就到这里。咱们下节课再见。","link":"/Detailed-of-string/"},{"title":"9. 数据类型 - 列表详解","text":"Hi，大家好。我是茶桁。 最近几节课，我们都是在详细讲解Python内的数据类型，上一节课我们详细了解了字符串，这节课，让我们来详解一下列表。 首先，我们先有一个大的概念，列表，其实就是一组有序的数据组合；另外，列表中的数据是可以被修改的。也就是说，列表是一个可变序列类型。 列表定义 如何在Python的定义列表，记住以下几点就可以了： 可以使用中括号进行定义[] 可以使用list()函数定义 还可以使用列表推导式定义: [x for x in iterable] 在定义列表中的元素时，需要在每个元素之间使用逗号（英文逗号），进行分隔。[1, 2, 3] 列表中的元素可以是任意类型，通常用于存放同类项目的集合。 列表的基本操作 让我们先来定义一个列表: 1234567items = [1, 2, 3, 4]items2 = list('1234')print(items,'\\n', items2)---[1, 2, 3, 4] ['1', '2', '4', '5'] 我们使用了最基本的两个方式来定义列表。至于列表推导式， 先不用着急，我们后面会单独讲它。 我们可以看到，刚才我刻意将item和items两个列表定义了不同种类的元素，那他们到底能否拼接在一起？我们尝试一下列表的相加： 1234print(items + items2)---[1, 2, 3, 4, '1', '2', '3', '4'] 没问题，两种不同类型的元素拼接到了一起，组成了一个新的列表。 让我们将这段代码搞的复杂一点，新的列表对于我要的模拟数据来说太少了，我想再增加5倍的长度： 1234print((items + items2) * 5)---[1, 2, 3, 4, '1', '2', '3', '4', 1, 2, 3, 4, '1', '2', '3', '4', 1, 2, 3, 4, '1', '2', '3', '4', 1, 2, 3, 4, '1', '2', '3', '4', 1, 2, 3, 4, '1', '2', '3', '4'] 没毛病，也就是说，我将小学学到的基本数学运算用到这里完全适用。 那如果用到减法呢，虽然难以想象最后的结果，试试中可以： 1234print(items - items2)---TypeError: unsupported operand type(s) for -: 'list' and 'list' 果然是我想多了，完全不支持操作数类型。 那是不是关于列表的操作也就到此为止了？并不是，列表除了利用加和乘进行拼接和循环的操作之外，还有很多其他的基本操作，比如： 12345items[2] = 9print(items, &quot;\\t&quot;,items[3])---[1, 2, 9, 4] 4 这里，我们利用了列表的下标操作修改了列表内的下标[2]的元素（第三个），并且将修改后的列表和列表内下标[3]的元素打印了出来。 有这样一种情况大家想过没有，这个列表呢，我并不知道有多长，但是我知道最后一个数字，现在我就想把最后一个数字取出来该怎么办？用len()获取长度之后再-1? 是不是太麻烦了？ 还记得之前咱们讲过，下标是可以从后往前数的吗？ 1234items[-1]---4 嗯，我想再这个列表添加几个数字： 1234items[4] = 10---IndexError: list assignment index out of range 哎，我似乎想的并不对。本以为原列表下标[3]是最后一个元素，那我多加一个下标就会再多加一个元素，可是似乎并不行。那么我们该怎么在列表内最佳元素呢？ 可以尝试一下专门添加元素的append()函数： 123456items = [1, 2, 3, 4]items.append(2)print(items)---[1, 2, 3, 4, 2] 加是加了，可是我们之前是想加10的，现在不小心加成2了，不行，我要删了它。该怎么办？随便吧，我就记得windows的CMD命令中的删除文件似乎是del，试试看： 12345del items[-1]print(items)---[1, 2, 3, 4] 居然成了... 这就神奇了。看起来，Python并不是很难。不过我们这里不得不说，在Python中还有一个针对列表删除元素的方法：pop() 12345items = [1, 2, 3, 4]items.pop()---[1, 2, 3] pop([index=-1])函数专门用于移除列表中的一个元素，其中参数index为索引值，默认为-1，也就是说默认是从列表移除最后一个值。 1234567# 将索引值改为从前数第一个items = [1, 2, 3, 4]items.pop(0)print(items)---[2, 3, 4] 列表中的切片 在学习了列表的基本操作之后，我们来看看列表中的切片。提前说一声，在数据分析的应用中，对数据整理的过程绝大多数时候都需要用到列表的切片操作， 所以大家这部分要好好理解。 列表的切片操作基本语法其实很简单 list[开始值:结束值:步进值] 看起来很熟悉对吧？在我们之前介绍字符串相关的操作的时候，就是这种方式。其用法和字符串中也是如出一辙： list[开始值:] 从开始值索引到列表的最后 list[:结束值]从列表最前面索引到结束值之前 list[开始值:结束值]按照给到的开始值开始索引，到结束值之前为止。 当然，除了这三个基本的操作之外还有list[::], list[::步进值], list[开始值::步进值], list[:结束值:步进值],list[开始值:结束值:步进值]，我们下面一一的来看看，在字符串相关操作中没有特别理解的没关系，这里再来加深下印象： 12# 先来定义一个列表方便后续操作items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP'] 从开始值索引到最后： 1234print(items[2:])---['Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP'] 从下标[2]开始，也就是从第三个Ruby开始向后索引。 从最前面索引到结束值之前： 1234print(items[:2])---['Python', 'Java'] 现在我们让这两个语言单独露脸，算是对它们进行补偿了。 从开始值索引到结束值之前： 1234print(items[2:3])---['Ruby'] 哎，为什么只索引出来一个值？因为结束值为3，它之前不就是2吗。开始值也是2，那可不就只有一个值而已。 这回，我们把步进值加上： 12345# 加上步进值print(items[0:-1:2])---['Python', 'Ruby', 'C++', 'JavaScript'] 从最前面索引到最后，步进值为2，所以是隔一个索引一个。那为什么PHP没索引到？估计你又忘了，是索引到结束值之前，不包含结束值，自然PHP就没被索引到。 只有步进值会是什么情况？ 12345# 只有步进值print(items[::-2])---['PHP', 'JavaScript', 'C++', 'Ruby', 'Python'] 步进值为负数，那显然是从后向前索引了。隔一个索引一个，等等，为啥第一个Python被索引到了？ 那是因为，当我们开始值和结束值都没取值的情况下，默认是从头到尾索引，现在嘛，应该是从尾到头索引。也就是包含了头尾，不存在最后一个值之前，所以列表内的所有值都索引了一个遍，只是因为有步进值的关系，所以变成隔一个索引一个。 再让我们将所有值都去掉，只留下[::]试试看： 1234567# 删掉所有值试试print(items[::])print(items[:])---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP']['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP'] 从结果上看，中括号内一个冒号和两个冒号出来的结果是一样的。 在索引查找之后，我们来看看，利用切片的方式是否可以对列表内的元素进行更新和删除？ 从指定下标开始，到指定下标前结束，并替换为对应的数据(容器类型数据，会拆分成每个元素进行赋值) 123456items = [1, 2, 3, 4, 5]items[2:4] = [7, 8]print(items)---[1, 2, 7, 8, 5] 指定的切片内的元素被替换掉了。 刚才我们使用切片替换元素的时候元素是一一对应的，那如果我们没有对应的话会发生什么？ 1234567# 切片范围大于添加元素的个数items = [1, 2, 3, 4, 5]items[2:6] = [7]print(items)---[1, 2, 7] 结果并没有报错，而是将切片范围内的元素都移除之后添加了一个元素7。我们再试试其他的： 1234567# # 切片范围小于添加元素的个数items = [1, 2, 3, 4, 5]items[2:3] = [7, 8, 9, 0]print(items)---[1, 2, 7, 8, 9, 0, 4, 5] 可以看到，比起原本的列表，我们的值增加了。原本下标[2]的元素被移除之后，在这个位置插入了[7,8,9,0]四个元素。 以此，我们可以总结切片更新列表，实际上就是删除掉切片范围内的元素，再在原来的位置上插入新加的元素，并且将之后的元素向后移动。 那既然这样的话，我们是不是可以利用这种特性对列表内的元素进行删除？ 123456items = [1, 2, 3, 4, 5]items[2:4] = []print(items)---[1, 2, 5] 没毛病，确实可以这么用。 当然，除了这种插入空列表的方式之外，还有其他方式可以删除列表内的指定元素, 还记得前面我们介绍的del方法吗？ 123456items = [1, 2, 3, 4, 5]del items[2:4]print(items)---[1, 2, 5] 那既然我们可以用添加空列表的方式来删除列表内的元素，del是不是就没用武之地了？实际上，并非如此。del有一个特殊的用法，就是在利用步进值来跳着删除元素： 123456items = [1, 2, 3, 4, 5]del items[0:6:2]print(items)---[2, 4] 那聪明的小伙伴肯定想，添加空列表的方式也加上步进值就不行吗？我们来试试： 123456items = [1, 2, 3, 4, 5]items[0:5:2] = []print(items)---ValueError: attempt to assign sequence of size 0 to extended slice of size 3 报错提示我们，序列分配不正确。说明我们不能这样使用。如果要这样使用的话，替换的元素个数必须对应才行： 123456items = [1, 2, 3, 4, 5]items[0:4:2] = [9, 10]print(items)---[9, 2, 10, 4, 5] 列表相关函数(✨ 重点) 除了以上介绍的关于列表的一些方法之外，Python还为我们提供了一些列表常用的相关函数： len() 这个函数可以检测当前列表的长度，列表中元素的个数： 12345items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']len(items)---9 count() 这个函数可以检测当前列表中指定元素出现的次数： 1234items.count('Python')---1 append() 这个函数前面我们已经介绍过了，就是向列表尾部追加新的元素，返回值为None。 12345items.append('SQL')print(items)---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', 'SQL'] insert() 这个函数可以向列表中指定的索引位置添加新的元素。 12345items.insert(4, 'Go')print(items)---['Python', 'Java', 'Ruby', 'Rust', 'Go', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', 'SQL'] pop() 还记得我们之前删除列表中元素的时候介绍pop()函数吗？其实，pop()函数是对指定索引位置上的元素做出栈操作，然后返回出栈的元素 123456789items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']print(items.pop())print(items.pop(2))print(items)---PHPRuby['Python', 'Java', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node'] 默认的情况下，pop()是把列表的最后一个元素出栈，当给值之后，是将指定索引的元素进行出栈。 remove() 这个函数是专门删除特定元素用的，可以指定列表中的元素进行删除，只删除第一个，如果没有找到，则会报错。 12345678items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.remove('PHP')print(items)items.remove('Go')---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node']ValueError: list.remove(x): x not in list 可以看到，第一个remove成功删除了PHP，但是第二个remove并未在列表中找到Go，所以报错。 index() 这个函数可以查找指定元素在列表中第一次出现的索引位置 12345items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.index('PHP')---8 除此之外，index()还能接收索引值，当输入索引值的时候，index()会在指定范围内查找元素的索引位置： 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']print(items.index('Ruby', 0, 5))items.index('PHP', 0, 5)---2ValueError: 'PHP' is not in list 可以看到，指定范围内没有说要查找的元素的时候就会报错，告知元素不在列表内。 extend() 这个函数接收一个容器类型的数据，把容器的元素追加到原列表中 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']a = ['Go', 'MATLAB']items.extend(a)print(items)---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', 'Go', 'MATLAB'] 看到这，是不是感觉很像两个列表相加？那既然我们可以将两个列表相加了，这个方法似乎有些多余了。 这么想的小伙伴们，我们再来看两段示例： 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']a = (1, 2, 3)items.extend(a)print(items)---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', 1, 2, 3] 另外一段： 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']a = (1, 2, 3)items = items + aprint(items)---TypeError: can only concatenate list (not &quot;tuple&quot;) to list 可以看到，第二段代码直接报错了。那说明，相加这个操作必须两个都是列表才可以，不支持列表和元组相加。可是extend()方法是支持将任意一个容器类型的数据中的元素追加到原列表中的。 我们再来多看一段： 1234567items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']a = '1234'items.extend(a)print(items)---['Python', 'Java', 'Ruby', 'Rust', 'C++', 'Swift', 'JavaScript', 'Node', 'PHP', '1', '2', '3', '4'] 将a定义为一段字符串，一样可以使用extend()来接收并追加到原列表内。 clear() 这个函数比较简单，就是清空列表内容 123456items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.clear()print(items)---[] reverse() 这个函数可以对列表进行翻转 123456items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.reverse()print(items)---['PHP', 'Node', 'JavaScript', 'Swift', 'C++', 'Rust', 'Ruby', 'Java', 'Python'] sort() 该函数将对列表进行排序, 在默认的情况下，会对元素进行从小到大的排序 123456items = ['Python','Java','Ruby','Rust','C++','Swift','JavaScript','Node','PHP']items.sort()print(items)---['C++', 'Java', 'JavaScript', 'Node', 'PHP', 'Python', 'Ruby', 'Rust', 'Swift'] 额，这样似乎并不明显，我们重新换个案例。不过大家也可以想想现在这段代码中，为什么会有这样的结果。 123456items = [9, 3, 5, 2, 1, 7, 8, 0, 6]items.sort()print(items)---[0, 1, 2, 3, 5, 6, 7, 8, 9] 嗯，这回明显了。 除了从小到大排序外，我们还可以将其从大到小排序，利用关键参数reverse来开启： 123456items = [9, 3, 5, 2, 1, 7, 8, 0, 6]items.sort(reverse=True)print(items)---[9, 8, 7, 6, 5, 3, 2, 1, 0] OK，现在让我们来回过头来解释一下第一段代码中的结果：['C++', 'Java', 'JavaScript', 'Node', 'PHP', 'Python', 'Ruby', 'Rust', 'Swift']， 之所以会产生这样的结果，不是因为它按英文字母来排序，当然这么想也对但是不全对，而是因为它的排序依据是ASCII码，之前我们学习过，ASC II码只包含了128个字符，仅仅是美国的标准，128个字符里面都是西文码，那么如果中间包含了中文会怎样呢？ 不如我们直接来看看： 123456items = [9, 3, 5, 2, 1, 7, 8, 0, '茶', '桁']items.sort(reverse=True)print(items)---TypeError: '&lt;' not supported between instances of 'int' and 'str' 完了，直接报错。不过这个似乎和编码无关，而是数据类型的问题，告诉我们字符和整型之间不能排序。别问我怎么看懂的，我也是查字典。 知道是数据类型的问题就好办了，我们将数据类型变成一致的再试试： 123456items = ['9', '3', '5', '2', '1', '7', '8', '0', '茶', '桁']items.sort(reverse=True)print(items)---['茶', '桁', '9', '8', '7', '5', '3', '2', '1', '0'] 居然成功了，那既然是ASCII码，那为什么还会支持中文排序呢？还记得我们除了介绍ASCII码之外，还介绍过一个Unicode编码。那即是说，Python中的sort()排序的依据是Unicode编码。 当然，除了默认规则之外，我们还可以自己对排序进行干预，加上你想要的规则。sort(key)内的key参数可以接收一个函数，按照函数的处理结果进行排序： 123456items = [-5, -3, 5, 2, 0, -9, 12, 14, -1, -6]items.sort(key=abs)print(items)---[0, -1, 2, -3, -5, 5, -6, -9, 12, 14] 这一段是不是让小伙伴们想到之前我们在Python的内置函数中介绍高阶函数的内容？没错，就是一样的。所以，我们这次就不对函数内部排序过程进行分析了，有兴趣的小伙伴可以回去看看第七节的内容。 深拷贝与浅拷贝 接着，让我们来看看关于拷贝的问题，先说浅拷贝。 说到浅拷贝，实际上是仅拷贝了列表中的一维元素，如果列表中存在二维元素或容器，则为引用而不是拷贝。使用copy函数或者copy模块中的copy函数拷贝的都是浅拷贝。 123456items = [1, 2, 3]res = items.copy()print(items, '\\t', res)---[1, 2, 3] [1, 2, 3] copy()之后的新列表和原列表内容上是一样的。 接着让我们来操作一下copy之后的res 123456789items = [1, 2, 3]res = items.copy()del res[2]print(items)print(res)---[1, 2, 3][1, 2] 可以看到，对res进行操作完全不影响原列表的内容。这就说明，copy产生的新列表和原列表并不是一个列表，我们可以验证一下看看： 123456print(id(items))print(id(res))---46363598724636086464 当我们用id()函数的时候，可以看到他们是两个完全不同的id 刚才我们定义的items是一个一维列表，接着让我们再来定义一个多维列表来尝试一下: 123456789items = [1, 2, 3, 4, ['a', 'b', 'c']]res = items.copy()del res[3]print(items)print(res)---[1, 2, 3, 4, ['a', 'b', 'c']][1, 2, 3, ['a', 'b', 'c']] 我们可以看到，做删除操作之后，res内容变了，但是原列表items没变化。似乎和之前的并没有什么不同，让我们再继续试试: 1234567del res[3][1]print(res)print(items)---[1, 2, 3, ['a', 'c']][1, 2, 3, 4, ['a', 'c']] 发生了什么？我们明明是操作的res而不是原列表items， 为什么items也发生了变化？难道是id是相同的吗？来，试试就知道了。 123456print(id(items))print(id(res))---46364272644636085824 似乎并不相同。那既然不是同一个元素，为什么我们操作res的时候，items也会跟着一起变化？ 别着急，让我们接着看下面的操作: 123456print(id(items[4])) # items这个位置是列表['a', 'c']print(id(res[3])) # res这个位置是列表['a', 'c']---46352459524635245952 如何，一模一样对吧？这就说明，在items以及它的copy列表res中，这个嵌套的列表是同一份。这也就能解释为什么我们对res内的嵌套列表进行操作的时候, items也发生了变化。 这个就是我们在一开始说到的，copy仅仅是拷贝了列表中的一维元素，对二维元素和容器仅仅是引用，这个应用对象当然还是原来那个对象。所以，两者的id才是是同一个。 浅拷贝我们理解完之后，才看看什么是深拷贝。 深拷贝和浅拷贝比起来就有深度的多，嗯，这么讲是因为深拷贝不仅仅是拷贝了当前的列表，同时还把列表中的多维元素或容易也拷贝了一份，而不是像浅拷贝一样仅仅是引用。完成深拷贝的函数是copy模块中的deepcopy函数。 12345items = [1, 2, 3, ['a', 'b', 'c']]res = items.deepcopy()---AttributeError: 'list' object has no attribute 'deepcopy' 额，尴尬。居然报错了... 似乎deepcopy并不是和copy函数一样的用法。 细心的小伙伴应该之前就注意到了，在介绍copy函数和deepcopy函数的时候，我都在强调是copy模块中的这句话，确实，我们在使用deepcopy的时候，是需要先引用模块再使用的，并且，使用方式也有一些不同: 1234567import copyitems = [1, 2, 3, ['a', 'b', 'c']]res = copy.deepcopy(items)print(res)---[1, 2, 3, ['a', 'b', 'c']] 没错，我们这就对items完成了深拷贝，生成了新的列表res。 那到底是否是真的深拷贝呢？让我们试一试： 1234567891011print(id(items))print(id(res))print(id(items[3]))print(id(res[3]))---4636282048463479987246362851204637491072 没问题，我们打印出来的id各不一样，包括items内的二维列表以及res内的二维列表，id也都不同，说明确实是深拷贝。 不放心的小伙伴，我们再来更改列表元素测试一下: 12345678del res[3][0]print(res[3])print(items[3])---['b', 'c']['a', 'b', 'c'] 可以看到，当我们更改res内的二维列表时，items并未发生改变。说明二维列表我们也一样完成了拷贝，而不是像浅拷贝一样仅是引用了。 列表推导式 在本文最开始，我们介绍列表的时候提过三种列表生成方式，包括直接定义列表, 用list函数，最后一个就是列表推导式。那我们接下来，就要详细讲讲列表推导式。 列表推导式提供了一个更简单的创建列表的方法，常见的用法是把某种操作应用于序列或可迭代的每个元素上，然后使用其结果来创建列表，或者通过满足某些特定条件元素来创建子序列。 采用一种表达式的当时，对数据进行过滤或处理，并且把结果组成一个新的列表。 哎，最怕就是定义和文字过多，让我们直接上示例吧。 基本的列表推导式使用方式 结果变量 = [变量或变量的处理结果 for 变量 in 容器类型数据] 现在，假设我们想要创建一个平方列表： 123456789# 使用普通方法完成items = []for i in range(10): items.append(i**2)print(items)---[0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 这是用我们所学过的内容来进行创建，当然，我们还学过另外一种方式: 123456# 使用 map函数和list完成items = list(map(lambda x: x**2, range(10)))print(items)---[0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 这里似乎有一点复杂，咱们还是来分析一下吧。 首先，我们创建了一个匿名函数lambda x:x**2, 再创建了一个可迭代对象range(10)。 接着，我们给map函数传入了这两个参数，分别传给了func和*iterables, 关于map函数，我们在第七节：内置函数中有讲解过，完了的小伙伴可以翻看前面复习一下。 map函数在对传入的可迭代数据中的每一个元素进行处理，然后返回一个新的迭代器, 最后用list函数将这个新的迭代器转换成了一个列表。 然后，我们将传入的func函数用一个匿名函数 没错，我们使用map函数和list也可以进行实现。 那么最后，让我们来看看列表推导式如何完成这个需求： 123456# 列表推导式items = [i**2 for i in range(10)]print(items)---[0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 简简单单一句话，比用map函数更简单的逻辑，就完成了我们的需求。这一句话的代码其实逻辑桁清晰，也很好理解： 首先，我们用for来进行循环传值给i， 接着，我们用i**2来得到我们期望的值，最后生成列表。本质上，其实和我们用的第一种普通方法是一样的。 接着我们再来看一个， 我们现在有一个字符串'1234'， 想要得到这样一个列表[2, 4, 6, 8]。照例，从普通方法开始： 12345678910# 普通方法str = '1234'items = []for i in str: items.append(int(i)*2)print(items)---[2, 4, 6, 8] OK，没问题。我们继续： 123456789items.clear()print(items)items = list(map(lambda x:int(x)*2, str))print(items)---[][2, 4, 6, 8] 可以看到，我们先将items清空之后再继续操作的，这次我们用了list+map的方式，一样得到了我们想要的结果。 最后，当然是用列表推导式的方式： 123456789items.clear()print(items)items = [int(i)*2 for i in str]print(items)---[][2, 4, 6, 8] 同样，我们得到了想要的结果。 讲到这里了，我给大家秀一个小技巧，俗称骚操作，就是我们其实可以运用位运算操作符： 123456789items.clear()print(items)items = [int(i) &lt;&lt; 1 for i in str]print(items)---[][2, 4, 6, 8] 具体代码执行的时候发生了什么，就算是给大家留个小思考题。提示：可以回头翻看下我们之前讲到的位运算符。 带有判断条件的列表推导式 除了基本的列表推导式，我们还有一种带有判断条件的列表推导式。 结果变量 = [变量或变量的处理结果 for 变量 in 容器类型数据 条件表达式] 相比起基本的列表推导式，我们现在多了一个条件表达式，那么我们该怎么利用呢？来个需求：从0 ~ 9，求所有的偶数并且形成一个新的列表。这回，我们就只完成常规方法和列表推导式，对比着来观察一下： 12345678910# 常规方式items = []for i in range(10): if i % 2 == 0: items.append(i)print(items)---[0, 2, 4, 6, 8] 很好，我们完成了需求。接下来，大家试试不看我下面写的代码，自己从常规方式思考下该怎么写，然后自己运行一下试试写对了没，最后，再和我写的对比一下看看咱们写的有没有区别。 12345items = [i for i in range(10) if i % 2 == 0]print(items)---[0, 2, 4, 6, 8] 没错，就是这么简单，你做对了吗？ 带有条件判断的多循环推导式 现在有这样一个需求，我们拿到两个列表[1,2,3], [3,1,4], 要将这两个列表中的元素两两组合，要求组合的元素不能重复： 12345678910# 常规方法items = []for x in [1, 2, 3]: for y in [3, 1, 4]: if x != y: items.append((x,y))print(items)---[(1, 3), (1, 4), (2, 3), (2, 1), (2, 4), (3, 1), (3, 4)] 这样，我们就完成了刚才的需求。那用推导式该如何实现呢？ 12345items = [(x, y) for x in [1, 2, 3] for y in [3, 1, 4] if x != y]print(items)---[(1, 3), (1, 4), (2, 3), (2, 1), (2, 4), (3, 1), (3, 4)] 没毛病，我们完全实现了刚才的需求。这个很好理解对吧？ 让我们接着来看最后一个推导式的形式。 对于嵌套循环的列表推导式 这次我们直接写需求，然后上示例。 需求为，现在我们有一个3x4的矩阵，由3个长度为4的列表组成，我们现在要交换其行和列。 注意哦，这个行转列需求在处理数据的时候经常会用到。 1234567891011121314151617# 需求样例'''[ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]==&gt;[ [1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]''' 来，让我们尝试着实现一下： 1234567891011121314151617181920212223242526# 首先，定义初始数据，大家可以直接copy我给到的矩阵数据# 先定义数据arr = [ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]# 使用常规方法items = []for i in range(4): res = [] for row in arr: res.append(row[i]) items.append(res)print(items)# 使用列表推导式, 我们从内向外来写items = [[row[i] for row in arr] for i in range(4)]print(items)---[[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]][[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]] 这样，我们就完成了刚才的需求。我们拆解呢，还是从外层开始讲起： 首先，因为我们发现数据是4列，所以我们设定了一个range(4)来进行4次迭代，将0,1,2,3这四个下标分别传到内层循环。 然后我们开始在arr内循环找到当前的row, 循环会依次去寻找[1,2,3,4]，[5,6,7,8],[9,10,11,12]。然后将每一个row中的寻找当前的row[i]，并且填入一个新列表内。那么这三组列表中的row[i]就会是这样的： row[1]分别为1, 5, 9, row[2]分别为2, 6, 10.... 依次类推。当外层循环完成之后，就正好是组成了新的四个新的列表，最后再将新列表依次传到items这个空列表内，就完成了。 那同样都是两次for循环嵌套，为什么上面那个案例就是顺序写的，内层for循环写在了后面，而下面这个案例的内层for循环就写到了前面呢？ 好的，让我们来看看，如果将下面这个案例的内存for循环写在后面会是怎样的: 12345items = [row[i] for i in range(4) for row in arr]items---[1, 5, 9, 2, 6, 10, 3, 7, 11, 4, 8, 12] 看到了吗？顺序还是对的，只是依次传入了数据，并未形成矩阵。那有小伙伴就说了，那是不是因为没在row[i]上加[]从而形成列表呢？ 好的，让我们再来做一个实验： 12345items = [[row[i]] for i in range(4) for row in arr]items---[[1], [5], [9], [2], [6], [10], [3], [7], [11], [4], [8], [12]] 可以看到，列表是形成了，但是却是一个元素占一个列表，并没形成我们想要的矩阵。 估计小伙伴们看出来了，在推导式中，因为变量或变量的处理结果必须放在前面，所以我们为了要形成矩阵内层新的row，所以才必须将处理结果和内层循环方法放在一起，并加上[]来确保这组结果能形成一个列表, 也就是我们现在这样： 12345items = [[row[i] for row in arr] for i in range(4)]items---[[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]] 看一次没看懂的小伙伴，可以多看看，尝试着自己去分解，将其理解透彻。因为这个方法在我们后续的数据清洗中使用非常频繁。 小练习 为了能达到练习的目的，从这一节开始，所有练习可以不在课程中展示了。大家先做一下，然后可以在我下一节课中的源码中去找答案，然后来看看和自己做的是否一样。 以下所有练习必须使用列表推导式来实现 有些练习不止一个方法，大家尝试用多种方法来实现一下。 做完的小伙伴可以在课程后面留言讨论。 12345678910111213141516171819202122232425# 1. 让我们尝试将字典中的健值对转成`key = value`的数据格式{'user':'admin', 'age':'20', 'phone':'133'} 转为 ['user=admin','age=20','phone=133']# 2. 把列表中的所有字符全部转为小写['A', 'CCCC', 'SHIss', 'Sipoa','Chaheng', 'Python','dsAhio']# 3. x是0～5之间的偶数，y是0~5之间的奇数，把x，y组成一个元组，放到列表中# 4. 使用列表推导式完成九九乘法表# 5. 求M, N中矩阵和元素的乘积'''M = [ [1, 2, 3], [4, 5, 6], [7, 8, 9]]N = [ [2, 2, 2], [3, 3, 3], [4, 4, 4]]''' 最后，大家记得做练习并且留言，下课。","link":"/Detailed-of-list/"},{"title":"10. 数据类型 - 元组详解","text":"Hi，大家好。我是茶桁。 之前两节分别介绍了字符串和列表，今天，我们来讲讲另外一个常用到的数据类型：元组。 元组和列表很像，两者都是一组有序的数据的组合。但是也有很多不同点，比如元组内的元素一旦定义了就不可以再修改，因此元组称为不可变数据类型。 元组定义 元组的定义方式包括以下要点： 定义元组变量 = (), 或者变量 = tuple() 可以使用变量 = (*iterable)定义含有数据的元组 ⚠️ 需要注意：如果元组中只有一个元素时，这唯一的元素后面也必须加逗号，这是为了区分其他元素标识这是一个元组: (1,) 特例： 变量 = 1,2,3， 这种方式也可以定义为一个元组。 元组的相关操作 由于元组是一个不可变的数据类型，因此其在创建之后只能使用索引进行访问，无法进行其他操作。访问方式其实和列表一样，同样可以使用切片方式获取元素。 元组可以进行切片操作，在访问数据这件事情上和列表几乎一样，没有什么区别，所以完全可以借鉴上一节我讲的内容来看，这里就不详细介绍了，仅仅给大家写出一些案例： 1234567891011121314151617181920# 常见的元组切片索引查询操作tup = 1, 2, 3, 4, 5, 5, 4, 3, 2, 1print('[:]:\\t',tup[:]) # 获取全部print('[::]:\\t', tup[::]) # 获取全部print('[1:]:\\t', tup[1:]) # 从索引1开始获取到最后print('[1:3]:\\t', tup[1:3]) # 从索引1开始索引到3之前print('[:3]:\\t', tup[:3]) # 从0开始索引到3之前print('[1:5:2]:\\t', tup[1:5:2]) # 从1开始索引到5之前，步进值为2print('[::2]:\\t', tup[::2]) #从0开始索引到最后，步进值为2print('[5:1:-1]:\\t', tup[5:1:-1]) # 从5开始往前索引到1， 步进值为-1。---[:]: (1, 2, 3, 4, 5, 5, 4, 3, 2, 1)[::]: (1, 2, 3, 4, 5, 5, 4, 3, 2, 1)[1:]: (2, 3, 4, 5, 5, 4, 3, 2, 1)[1:3]: (2, 3)[:3]: (1, 2, 3)[1:5:2]: (2, 4)[::2]: (1, 3, 5, 4, 2)[5:1:-1]: (5, 5, 4, 3) 除了常用的切片操作之外，和列表一样，元组也能使用一些基本函数来完成查询操作 12345678910111213# 获取元组的长度print(len(tup))# 统计一个元素在元组中出现的次数print(tup.count(5))# 获取一个元素在元组内的下标（索引值）print(tup.index(5, 1, 9))---1024 除此之外，元组还可以引用基础的数学运算符+和*来进行加和乘的运算。 1234567# 加和乘操作print((1, 2, 3) + ('a', 'b'))print((1, 2, 3) * 5)---(1, 2, 3, 'a', 'b')(1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3) 有一个同学曾经问过我：既然元组是不可修改的，那为什么还能用加和乘的运算呢？ 不知道在座的小伙伴有没有这种想法？ 这样吧，我重新写一段代码，小伙伴们应该就明白了: 123456789101112# 元组是这样的tup = (1, 2, 3, 4)print(id(tup))tup2 = (3, 4, 5, 6)tup = tup + tup2print(id(tup))---4446580864445062912044506479844459297216 不知道大家看明白没有。解释一下，其实就是说，在进行加法和乘法运算的时候，即便我们的变量名是一样的，实际上也是生成了一个新的元组，而不是之前那一个了。所以这个并非是修改和更新，而是创建。 为了对比，我再写一段更新的代码给大家看： 12345678# 尝试更新元组tup = (1, 2, 3, 4, 5, 6)print(tup[2])del tup[2]print(tup)---TypeError: 'tuple' object doesn't support item deletion 可以看到，报错提示了，tuple对象不支持删除项目 作为对比，我们看看列表的： 123456789# 看看列表更新（只看id）items = [1, 2, 3, 4, 5, 6]print(id(items))del items[2]print(id(items), '\\t',items)---44504159364450415936 [1, 2, 4, 5, 6] 可以看到，不仅是内部元素被删除了，并且id完全没有变化。也就是说，我们是在这个列表本身做了删除动作，并未生成新的列表。关于这部分，我们上一节中的深拷贝和浅拷贝讲的很清楚，大家可以回去好好看看理解一下。 元组推导式 生成器 在起初，我们先来看看元组是否和列表一样支持使用推导式。 12345tup = (i for i in range(10))print(tup)---&lt;generator object &lt;genexpr&gt; at 0x109cfa570&gt; 这段并非是报错，而是打印出了tup的类型：生成器对象。 我们之前学过，使用列表推导式生成的结果是一个列表，但是元组似乎和列表并不一样，生成的结果是一个生成器对象。 12列表推导式 ==&gt; [变量运算 for i in 容器] ==&gt; 结果 是一个 列表元组推导式 ==&gt; (变量运算 for i in 容器) ==&gt; 结果 是一个 生成器 那这里就有个疑问了，什么是生成器？ 生成器是一个特殊的迭代器，生成器可以自定义，也可以使用元组推导式去定义。 生成器是按照某种算法去推算下一个数据或结果，只需要往内存中存储一个生成器，节约内存消耗，提升性能。 语法 里面是推导式，外面是一个()的结果就是一个生成器 自定义生成器，含有yield关键字的函数就是生成器 那么，我们到底应该怎样操作生成器呢？ 既然生成器是迭代器的一种，那我们是否可以使用迭代器的操作方法来操作生成器呢？ 说干就干，让我们直接操作做实验： 1234567891011tup = (i for i in range(10))print(next(tup))print(next(tup))print(next(tup))print(list(tup))---012[3, 4, 5, 6, 7, 8, 9] 没毛病，确实支持next()函数，并且内部元素在使用后也被移除了。 12345# 让我们将其转为元组print(tuple(tup))---() 哎，为什么里面是空的？那是因为，我们上一段代码中的最后一句，已经讲所有迭代器内的元素转为了列表，素衣目前迭代器tup内是没有任何元素了，所以我们转过来必须是空的。 再来生成一个，我们来试试用for对它进行循环： 123456tup = (i for i in range(10))for i in tup: print(i, end=&quot; &quot;) ---0 1 2 3 4 5 6 7 8 9 可以看到结果没有问题。可以推断出，生成器和迭代器没有任何区别，我们在平时使用的时候，就直接将它作为迭代器使用就可以了。 yield关键字 在之前，我们提到了，含有yield关键字的函数就是生成器。 它返回的结果是一个迭代器。我们可以理解为，生成器函数就是一个返回迭代器的函数。 那么yield有哪些需要注意的点呢？我们先在下面列一下，之后再带着大家一起过： yield和函数中的return有点像 共同点： 执行到这个关键字后会把结果返回来 不同点： return会把结果返回，并结束当前函数的调用 yield会返回结果，并记住当前代码执行的位置，下一次调用时会从上一次离开的位置继续向下执行。 上实验： 123456789101112# 定义一个普通函数def func(): print('Hello yield') return 'yield' print('Hello again')func()func()---Hello yieldHello yield 在这个自定义函数内，return执行的时候，就会结束当前函数的调用，而在之前，第一个print()函数正确执行了，但是第二个print()函数因为在return之后，所以并未运行。即便我们一共执行了两次函数，可是也仅仅是讲第一个print()函数执行了两次。 123456789101112131415# 尝试使用yield定义一个生成器函数def func(): print('Hello yield') yield 'yield' print('Hello again') yield 'again'# 调用生成器函数， 返回一个迭代器res = func()next(res)next(res)---Hello yieldHello again 可以看到，当我们使用next()函数的时候，迭代器起作用了，每执行一次，分别调用第一个yield之前和之后的print()，也就是说继续执行了。 那如何验证yield的返回呢？我们将这段代码改造一下： 12345678910111213141516171819# 尝试使用yield定义一个生成器函数def func(): print('Hello yield') yield 'return yield' print('Hello again') yield 'return again'# 调用生成器函数， 返回一个迭代器res = func()str = next(res)print(str)str = next(res)print(str)---Hello yieldreturn yieldHello againreturn again 没问题，依次打印出了返回值return yield和return again。 还记得我们之前教过，使用list函数去调用，可以讲迭代器的返回结果，作为容器的元素，让我们再来改造一下这段代码： 1234567891011121314151617# 尝试使用yield定义一个生成器函数def func(): print('Hello yield') yield 'return yield' print('Hello again') yield 'return again'# 调用生成器函数， 返回一个迭代器res = func()items = list(res)print(items)---Hello yieldHello again['return yield', 'return again'] 我们看见，确实，返回结果被依次放入了一个list容器中。 当然，除了list函数之外，还可以使用for来获取迭代器内容： 1234567891011121314151617181920# 尝试使用yield定义一个生成器函数def func(): print('Hello yield') yield 'return yield' print('Hello again') yield 'return again'# 调用生成器函数， 返回一个迭代器res = func()items = []for i in res: items.append(i)print(items)---Hello yieldHello again['return yield', 'return again'] 我们来分析一下在以上这几段代码中，生成器函数调用时到底是什么过程。 首先，调用生成器函数，返回一个迭代器： 第一次去调用迭代器，走到当前的生成器函数中，遇到第一个yield, 把return yield返回，并且记住当前的执行状态（位置），暂停了执行，等待下一次的调用 第二次去调用迭代器，从上一次遇到的yield位置开始执行，遇到了第二个yield，把return again返回，并重新记录状态，暂停执行，等待下一次调用。 如果最后又调用了迭代器，那么会从上一次的yield位置开始，可是后面没有了，就会超出范围，抛出异常：StopIteration:。 那么这种一次一次调用执行的方式什么时候适用呢？比如说，我们在处理一个非常大的数据，电脑可能吃不住，这个时候我们就可以拆开来一次一次的执行获取结果。 小练习 为了能达到练习的目的，从这一节开始，所有练习可以不在课程中展示了。大家先做一下，然后可以在我下一节课中的源码中去找答案，然后来看看和自己做的是否一样。 以下所有练习必须使用列表推导式来实现 有些练习不止一个方法，大家尝试用多种方法来实现一下。 做完的小伙伴可以在课程后面留言讨论。 上一节的练习已经放到本次教程的源码内，可以在此获取：https://github.com/hivandu/AI_Cheats/tree/main/Python 1今天就一个练习：使用生成器改写斐波那契数列函数","link":"/Detailed-of-tuple/"},{"title":"11. 数据类型 - 字典","text":"Hi，大家好。我是茶桁。 关于Python的数据类型，我们已经详细讲解了三种，字符串，列表和元组。那么今天，我们再来讲一种：字典。 字典也是一种数据的集合，由健值对组成的数据集合，字典中的键是不能重复的。 字典中的键必须是不可变的数据类型，常用的键主要是：字符串，整型... 实际上，在之前字符串和列表的铺垫之后，任何数据类型其实都会感觉差不多，当然，每个数据类型也都有自己的特点以及需要注意的地方，不过在方法，操作上也会有很多类同点。 那么，让我们开始学习字典吧。 字典的定义 字典可以通过把以逗号分隔的key:value对列表包含于花括号之内来创建字典。 也可以通过dict构造器来创建 {'jack': 666, 'stored': 777} 或者{666:'jack', 777:'stored'} 让我们开始写代码来做实验： 使用{}定义: 12345myDict = {'a':1, 'b':2, 'c':2}print(myDict)---{'a': 1, 'b': 2, 'c': 2} 使用dict(key=value, key=value)函数进行定义 12345myDict = dict(name='张三', sex='male', age=22)print(myDict)---{'name': '张三', 'sex': 'male', 'age': 22} 数据类型的转换：dict(二级容器类型) 列表或元组，并且只有二级容器才可以转换 12345myDict = dict([['a',1], ['b',2], ['c',3]])print(myDict)---{'a': 1, 'b': 2, 'c': 3} 让我们来试试如果不是二级容器类型会如何： 12345myDict = dict(['a',1], ['b',2], ['c',3])print(myDict)---TypeError: dict expected at most 1 argument, got 3 报错了，提示我们字典最多一个参数，但是现在里面有3个。 再继续试试其他情况： 12345myDict = dict([[['a',1],['b',2],['c',3]]])print(myDict)---ValueError: dictionary update sequence element #0 has length 3; 2 is required 再次抛出异常，提示字典更新序列元素长度为3，第2位是必填项。 以上可以看出，只有二级容器才能通过dict()函数来做数据类型的转换。 zip压缩函数，dict转类型 123456789ex1 = [1, 2, 3, 4]ex2 = ['a', 'b', 'c', 'd']# 压缩过后做的事情其实就是数据类型的转换myDict = dict(zip(ex1, ex2))print(myDict)---{1: 'a', 2: 'b', 3: 'c', 4: 'd'} 字典的操作 还记得吗，无论是列表还是元组，都支持数学的基本运算符+和*。那字典是不是也同样支持？ 123456ex1 = {'a':1, 'b':2, 'c':3}ex2 = {1:'a', 2:'b', 3:'c', 4:'d'}print(ex1 + ex2)---TypeError: unsupported operand type(s) for +: 'dict' and 'dict' 提示类型错误，*实际上也是一样，这里我们就不占用篇幅再多打印一次错误了。说明，字典并不支持这两个基本的数学运算符。想想我们之前提到的dict中key不能重复其实也就好理解了。如果支持+， 那相加的两个字典内key值如果相同，那到底舍去那一个呢？*法就更容易理解，原本*就是将相同的数据重复乘n份，不支持也就理所应当了。 那么，字典到底支持哪些操作呢？我们接着往下看实验： 首先，让我们尝试获取一下元素，既然字典是key:value形式的，那要想拿到value值，必然是使用key来获取： 12345res = ex1['a']print(res)---1 拿到元素了，那如果我们是要修改元素呢？直接赋值试试： 12345ex1['a'] = 111print(ex1)---{'a': 111, 'b': 2, 'c': 3} 看来是有效的，增删改查，我们现在来试试删除： 12345del ex1['a']print(ex1)---{'b': 2, 'c': 3} 也没毛病。 接下来，当然就是添加元素了： 12345ex1['aa'] = 'aaaaa'print(ex1)---{'b': 2, 'c': 3, 'aa': 'aaaaa'} 之前我们反复说过字典的一个特点，就是字典不能有重复的key，这也是我们无法使用+和*操作字典的原因。那么问题来了，如果我在添加元素的时候key重复了怎么办？ 什么怎么办，添加key重复了，那不就变成修改元素了吗？^_^ 检测和获取 增删改查我们前三个基本都已经讲完了，那剩下的，就是查了。让我们看看如何检测和获取元素。 成员检测，只能检测key， 无法检测value。是否注意到我们之前一直使用的一句代码for i in range(10), 大家应该都能明白这一句代码是做什么吧？其实，我们坚持是否包含的时候，就可以用in来实现： 123456print('AA' in ex1)print('AA' not in ex1)---FalseTrue 获取当前字典的长度，只能检测当前有多少个健值对： 1234print(len(ex1))---3 我们还可以获取当前字典中的所有key键： 1234print(ex1.keys())---dict_keys(['b', 'c', 'aa']) 当然，不只是key。实际上，字典中所有的value值，我们一样可以获取到： 1234print(ex1.values())---dict_values([2, 3, 'aaaaa']) 最后，让我们尝试把key和value一起获取到： 1234print(ex1.items())---dict_items([('b', 2), ('c', 3), ('aa', 'aaaaa')]) 字典的遍历 当我们谈到对字典的遍历时，实际上和检测、获取时一样的。只是写进了遍历循环里而已，让我们来看看吧： 在我们遍历当前字典时，只能获取当前的key, 但是我们可以通过获取到的key来完成获取当前key的value: 12345for i in ex1: print(i, ':', ex1[i], end=&quot;; &quot;) ---b : 2; c : 3; aa : aaaaa; 这种获取方式就显得略微繁琐一点，既然我们之前有提到一个将key和value一起获取到的函数方法，那我们在for里一样可以使用它来将key和value一起获取到，只是，我们需要用到两个参数来接收： 12345for k, v in ex1.items(): print(k, ':', v, end=&quot;; &quot;) ---b : 2; c : 3; aa : aaaaa; 既然之前介绍的获取上我们可以单独获取key和value， 当然这里也通通能用： 12345678910111213# 遍历所有的keyfor k in ex1.keys(): print(k, end=&quot;; &quot;)print()# 遍历所有的valuefor v in ex1.values(): print(v, end=&quot;; &quot;)---b; c; aa; 2; 3; aaaaa; 字典的相关函数 和列表、元组一样，字典也有一些相关函数。有些嘛，一看到就很熟悉，在其他地方也能用，可是也有一些事字典专用的。 len(dict): 获取字典的健值对个数 dict.keys() 获取当前字典的所有key键，组成的列表 dict.values() 获取当前字典的所有value值，组成的列表 dict.items()返回由字典项（（键，值）对）组成一个新视图 iter(dict)返回以字典的键为元素的迭代器。 1234567res = iter(ex1)print(next(res))print(list(res))---b['c', 'aa'] 接下来，让我们重新定义一个新的字典来继续下面的函数学习： 1myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5} dict.pop(key) 通过key从当前字典中弹出健值对，删除。 123456myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}myDict.pop('a')print(myDict)---{'b': 2, 'c': 3, 'd': 4, 'e': 5} 这里我们需要注意一个点，就是pop()这个函数其实是有返回值的，会返回当前删除的健值对的value, 我们拿一个变量来接收一下返回值看看： 123456myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}res = myDict.pop('a')print(res)---1 可以看到，res接收到了pop()方法的返回值1 dict.popitem(): 后进先出（LIFO）的方式删除健值对，我们这里需要理解一下什么叫后进先出，就是最后一个加入字典的元素，先出来。 123456myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}myDict.popitem()print(myDict)---{'a': 1, 'b': 2, 'c': 3, 'd': 4} 和pop方法一样，popitem方法也会有一个返回值，不过是返回一个元组。 12345res = myDict.popitem()print(res)---('e', 5) 上面我们在讲获取的时候提到，可以直接使用key来获取元素的value， 不过如果字典内如果没有这个key的话，程序会报错。除了使用key来直接获取，字典里还有一个get()方法可以用来获取一个元素，用get获取元素存在就返回，不存在也不回报错，而是回返回None 12345678910111213myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}print(myDict.keys('f'))---TypeError: dict.keys() takes no arguments (1 given)============# get方法获取myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}print(myDict.get('f'))---None 字典的update方法可以更新对字典进行更新，如果这个key存在的话，就是更新。如果key不存在，则会进行添加。update可是使用key = value的形式更新，也可以直接获取一个新字典进行更新。 1234567myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}myDict.update(a=11, b=22)myDict.update({'c':33, 'f':66})print(myDict)---{'a': 11, 'b': 22, 'c': 33, 'd': 4, 'e': 5, 'f': 66} 实际上可以这么理解，update方法在获取其他字典更新原字典就有点像使用数学运算符的+, 区别只是，update是强制把最终确定值定为 + 号后方的值。 字典中还有一个方法setdefault(), 完整的写法为:dict.setdefault(key[, default])这个方法会去字典中找寻存在的key，并且会返回它的值。如果这个key不存在，这会插入一个值为default的key， 并且返回default: 1234567891011myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}res = myDict.setdefault('aa', '123')print(res)res = myDict.setdefault('a', 2)print(res)print(myDict)---1231{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'aa': '123'} 需要注意的是，如果这个key在字典中本来就存在，则并不会修改原本key的值，即便你在后面设定了一个default。并且返回的也会是字典内原本的value。也就是说，这个方法只能用来查询和新增。 字典推导式 和之前介绍的数据类型一样，字典也可以使用推导式来实现一些功能。比如： 字典中的健值对位置进行交换，先用普通的方法实现： 12345678910myDict = {'a':1, 'b':2, 'c':3}newDict = {}for k, v in myDict.items(): newDict[v] = kprint(newDict)---{1: 'a', 2: 'b', 3: 'c'} 然后再让我们看看字典推导式如何完成： 123456myDict = {'a':1, 'b':2, 'c':3}newDict = {v:k for k, v in myDict.items()}print(newDict)---{1: 'a', 2: 'b', 3: 'c'} 有的小伙伴可能会在推导式前方只写了一个变量来进行接收，那会变成什么样呢？我们来看看： 123456myDict = {'a':1, 'b':2, 'c':3}newDict = {v for k, v in myDict.items()}print(newDict, type(newDict))---{1, 2, 3} &lt;class 'set'&gt; 可以看到，最终打印的字典似乎看起来怪怪的，不是key:value的对形式，而是只有一个值。我们type一下能看到，类型并非是字典，而是set， 也就是说这是一个集合。 来让我们再看一个案例，让我们把一个字典中的value值有偶数的对保留下来，并且交换健值对的位置，一样的，让我们先用普通方式做一遍： 12345678myDict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':6}# 使用普通方式完成newDict = {}for k,v in myDict.items(): if v%2 == 0: newDict[v] = kprint(newDict) 再让我们使用字典推导式来完成 12345newDict = {v:k for k,v in myDict.items() if v%2 == 0}print(newDict)---{2: 'b', 4: 'd', 6: 'f'} OK，关于字典的东西基本上也就这么多。在前面学习过字符串和列表之后，是不是其他的容器类数据就没那么难了？很多东西都是普遍适用的，所以我们要活学活用，多思考。 那今天就不留练习题了，咱们下节课是数据类型最后一节了，之后我们开始讲解具体实际应用。字符串和容器类数据是Python中的基础也是重点，大家一定要好好的巩固。 下一节：集合。咱们下节课再见。","link":"/Detailed-of-dictonary/"},{"title":"12. 数据类型 - 集合详解","text":"Hi， 大家好。我是茶桁 通过最近几节课的内容，我们已经了解到了大部分的容器类数据的特性和应用，今天这一节课是容器类数据的最后一部分。让我们今天来详细了解一下「集合」。 集合是确定的一组无序的数据的组合。注意这一句话中的几个概念： 首先是「确定的」，当前集合中的元素的值是不能重复的。 集合是由多个数据组合的容器类型数据 集合中的数据没有先后顺序 集合的作用大多数时候是为了从成员检测、从无序列中去除重复项。还有就是数学中的集合类计算，例如交集、并集、差集一集对称差集等等。 集合的定义 集合的定义和字典类数据的定义非常像，包含了三种定义方式： 可以直接使用{}来定义集合 可以使用set()进行集合的定义和转换 使用集合推导式来完成集合的定义 ⚠️ 需要注意：集合中的元素不能重复，集合中存放的数据为：Number, String, Tuple，冰冻集合 冰冻集合 在集合的定义部分，其他数据类型我们都能理解，唯独多出来一个冰冻集合似乎没有见过，也难以理解。 冰冻集合的定义，需要且仅能使用frozenset()函数来进行定义。故名思义，冰冻集合一旦定义之后，是不能进行修改的，只能做一些集合相关的运算，比如交集，差集等等。 回过头来看冰冻集合的定义函数frozenset()， 这个函数本身是一个强制转换类的函数，可以把其他任何容器类型的数据转为冰冻集合，然后参与集合运算。 123456789# 定义一个冰冻集合mySets = frozenset({'love', 666, 333, 2, 'a', 1, 2, 'MAMT','55IW'})# 遍历集合for i in mySets: print(i, end=&quot;, &quot;)---1, 2, 55IW, love, 333, MAMT, 666, a, 也是可以看到，打印的结果完全没有任何顺序。 冰冻集合当然也可以使用集合推导式： 12345res = frozenset({i&lt;&lt;1 for i in range(6)})print(res)---frozenset({0, 2, 4, 6, 8, 10}) 可以进行拷贝： 12345res = res.copy()print(res)---frozenset({0, 2, 4, 6, 8, 10}) 当然冰冻集合也可以进行集合的运算，不过这部分我们将在后面讲解集合的时候来学习。暂时我们只是对「冰冻集合」的概念有个了解就可以了。 集合的基本操作和常规函数 以往的几节，我们都是将集合的操作和函数分开来讲，而这次我们放在一起讲。其实也没其他原因，就是因为这部分的内容并没有多少，并且很容易理解。 123456# 定义集合mySets = {123,'abc',False,'love',True,(1,2,3),0,3.1415,'123',1}mySets---{(1, 2, 3), 123, '123', 3.1415, False, True, 'abc', 'love'} 打印的结果再一次验证了集合无序，除此之外，我们可以看到打印出来的集合比我们进行定义的时候似乎少了,这又是为什么呢？ 原来，在集合内布尔类型的数据其实就是0和1， True表示为1， False表示为0，而集合内的值是不能重复的，所以，布尔值和0,1就只能存在一个。 我们来尝试检测一下集合中的值，和其他容器类数据一样，我们可以直接使用for...in： 1234567# 检测集合中的值print('123' in mySets)print('123' not in mySets)---TrueFalse 然后一样，可以使用len()检测长度： 1234print(len(mySets))---8 遍历的方法依然是用for 123456789101112for i in mySets: print(i, type(i)) ---False &lt;class 'bool'&gt;True &lt;class 'bool'&gt;3.1415 &lt;class 'float'&gt;(1, 2, 3) &lt;class 'tuple'&gt;love &lt;class 'str'&gt;abc &lt;class 'str'&gt;123 &lt;class 'int'&gt;123 &lt;class 'str'&gt; 为什么我这次会将数据类型也打印出来呢？是因为我想让大家好好记住这些类型，目前集合就只支持这些数据类型，其他的并不支持放入。比如说列表，是无法进入集合内的： 12345# 看看列表是否能放入mySets = {123,'abc',False,'love',True,(1,2,3),0,3.1415,'123',1,['list', 2, 3, 4, 5]}---TypeError: unhashable type: 'list' 可以看到报错信息提示，不支持类型：列表。 那我们如何像集合中追加元素呢？可以使用add()： 12345678# 定义集合mySets = {123,'abc',False,'love',True,(1,2,3),0,3.1415,'123',1}res = mySets.add('茶桁')print(mySets, '\\nres:', res)---{False, True, 3.1415, (1, 2, 3), 'love', '茶桁', 'abc', 123, '123'} res: None 可以看到我们在其中追加了一个字符串茶桁, 但是我们再一次验证了集合的无序，新加入的字符串并没有和其他数据类型一样新加入的元素放在最末端。 并且我们注意到了，我用res来接收了add()的返回值，返回了一个None。 除了追加之外，当然我们也可以对集合进行删除元素的处理： 123456res = mySets.pop()print(mySets, '\\nres:', res)---{True, 3.1415, (1, 2, 3), 'love', '茶桁', 'abc', 123, '123'} res: False 用pop()删除集合内的元素是随机的，并且，会将删除的元素返回。 如果想指定删除集合中的元素有没有办法呢？其实也有，remove()和discard()都可以做到，但是两者又有些区别，我们接着看代码： 123456789# 使用remove()res = mySets.remove('abc')print(mySets)res = mySets.remove('aaa')---{True, 3.1415, (1, 2, 3), 'love', '茶桁', 123, '123'} res: NoneKeyError: 'aaa' 能看到，remove()确实可以删除集合内的指定元素，并给一个返回值None。不过当集合内没有此元素的时候，就会报错，提示关键词错误。 那让我们再来看看discard() 12345678910# 使用discardmySets = {123,'abc',False,'love',True,(1,2,3),0,3.1415,'123',1}res = mySets.discard('123')print(mySets, f'res:{res}')res = mySets.discard('aaa')print(mySets, f'res:{res}')---{False, True, 3.1415, (1, 2, 3), 'love', 'abc', 123} res:None{False, True, 3.1415, (1, 2, 3), 'love', 'abc', 123} res:None 和remove()一样，也删除了一个指定元素，并且返回了None。不同的是，当我们使用discard删除一个不存在的元素时，discard虽然没有删除任何内容，但是也没有报错。 一个个删除太麻烦了，这个集合我就想让它变成一个空集合，好办，用clear()做清空处理呗，和字典一样： 12345678mySets = {123,'abc',False,'love',True,(1,2,3),0,3.1415,'123',1}print(mySets)mySets.clear()print(mySets)---{False, True, 3.1415, (1, 2, 3), 'love', 'abc', 123, '123'}set() 空集合拿到了，可以放入我们喜欢的元素了。依然和字典一致，我们可以使用update: 12345678res = mySets.update({1, 2, 3, 4, 5})print(mySets, f'res:{res}')res = mySets.update({2, 3, 4, 5, 6})print(mySets, f'res:{res}')---{1, 2, 3, 4, 5} res:None{1, 2, 3, 4, 5, 6} res:None 结果中显示，我们更新成功了，新添加了一些元素进入集合。那第二次添加，为什么就只有6添加进去了呢？还记得么？集合不能有重复值，就跟字典不能有重复的key一样。在字典中使用update，遇到相同key后面的value会被更新，那其实集合也是一样的，只是因为只有一个值，所以更新完不还是这个值么。 在冰冻集合的时候我们用到过一次copy， 这里我们要单独拿出来说说，因为集合中的元素都是不可变的，包括元组和冰冻集合，所以当前集合的浅拷贝并不存在深拷贝的问题。换句话说，就是不存在在拷贝后，对集合中不可变的二级容器进行操作的问题。 123456mySets = {123,'abc',False,'love',True,(1,2,3),0,3.1415,'123',1}res = mySets.copy()print(res)---{False, True, 3.1415, (1, 2, 3), 'love', 'abc', 123, '123'} 集合是没有deepcopy方法的。 集合的运算和检测 集合的主要运算有四种，以下将列出这四种以及他们的方法: 交集 &amp;, set.intersection(), set.intersection_update() 并集 |, union(), update() 差集 -, difference(), difference_update() 对称差集 ^, symmetric_difference(), symmetric_difference_update() 我们先来看看符号运算： 123# 先定义两个集合mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'} 然后让我们先求交集&amp;: 123456# 求两个集合交集res = mySet1 &amp; mySet2print(res)---{'Python'} 求两个集合并集（求并集的时候会去除重复项）| 12345res = mySet1 | mySet2print(res)---{'Java', 'C', 'Rust', 'C++', 'Go', 'Python', 'Ruby', 'JavaScript', 'Swift'} 求两个集合差集- 12345678res = mySet1 - mySet2res2 = mySet2 - mySet1print(res)print(res2)---{'Go', 'Rust', 'C++', 'Swift'}{'Java', 'C', 'Ruby', 'JavaScript'} 这段代码结果中res和res2的区别在于，res是mySet1中有，而mySet2中没有， res2是mySet2中有，而mySet1中没有。 求两个集合对称差集^ 12345res = mySet1 ^ mySet2print(res)---{'Java', 'C', 'C++', 'Swift', 'Rust', 'Go', 'Ruby', 'JavaScript'} 看完符号运算，我们可以再来看看函数运算 交集的运算函数为set.intersection(), set.intersection_update(), 那这两个函数又有什么区别呢？ 12345678res = mySet1.intersection(mySet2)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---{'Python'}mySet1:{'Go', 'Python', 'Rust', 'C++', 'Swift'}, mySet2:{'Java', 'C', 'Python', 'Ruby', 'JavaScript'} 让我们先记住intersection()的结果, mySet1和mySet2并没有发生变化，而返回值为两个集合相同的内容。然后我们再来看看另外一个函数： 12345678res = mySet1.intersection_update(mySet2)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---NonemySet1:{'Python'}, mySet2:{'Java', 'C', 'Python', 'Ruby', 'JavaScript'} 首先我们就能看到，返回值为None, 并且mySet1发生了变化。也就是说，set.intersection_update()是将两者的交集重复赋值给到了头部的变量，这里就是mySet1，然后返回一个None值。 接着我们来看一下并集运算函数: union(), update() 1234567891011mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'}res = mySet1.union(mySet2)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---{'Java', 'C', 'Rust', 'C++', 'Go', 'Python', 'Ruby', 'JavaScript', 'Swift'}mySet1:{'Go', 'Python', 'Rust', 'C++', 'Swift'}, mySet2:{'Java', 'C', 'Python', 'Ruby', 'JavaScript'} 我们首先看到了返回值，正事两个集合的并集，两个原始集合也没有发生变化。 再来看看update() 1234567891011mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'}res = mySet1.update(mySet2)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---NonemySet1:{'Java', 'C', 'Rust', 'C++', 'Go', 'Python', 'Ruby', 'JavaScript', 'Swift'}, mySet2:{'Java', 'C', 'Python', 'Ruby', 'JavaScript'} 可以很明显看到区别：返回值为None，并集的计算结果被复制给了第一个变量，这里是mySet1。 再看完并集之后，就轮到差集了, 分别是这两个函数difference(),difference_update()： 1234567891011mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'}res = mySet1.difference(mySet2)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---{'Go', 'Rust', 'C++', 'Swift'}mySet1:{'Go', 'Python', 'Rust', 'C++', 'Swift'}, mySet2:{'Java', 'C', 'Python', 'Ruby', 'JavaScript'} 返回值为差集的计算结果, 这里是mySet1有的而mySet2没有的。那不用问，按照一贯的惯例， difference_update()一定是将计算结果返回给第一个变量，这回我们换一下，将mySet2换成第一个变量试试： 1234567891011mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'}res = mySet2.difference_update(mySet1)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---NonemySet1:{'Go', 'Python', 'Rust', 'C++', 'Swift'}, mySet2:{'Java', 'C', 'Ruby', 'JavaScript'} 果然就跟料想的一样，最终的计算结果赋值给了mySet2。 最后当然就是对称差集函数symmetric_difference() symmetric_difference_update()： 1234567891011mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'}res = mySet2.symmetric_difference(mySet1)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---{'Java', 'C', 'C++', 'Swift', 'Rust', 'Go', 'Ruby', 'JavaScript'}mySet1:{'Go', 'Python', 'Rust', 'C++', 'Swift'}, mySet2:{'Java', 'C', 'Python', 'Ruby', 'JavaScript'} res接收了计算结果，成为了一个新集合。 接下来，大家应该能猜到了吧？不过还是要做做实验才知道，万一和自己想的不一样呢。 1234567891011mySet1 = {'Python','Go','Rust', 'Swift', 'C++'}mySet2 = {'C','JavaScript', 'Ruby', 'Java', 'Python'}res = mySet2.symmetric_difference_update(mySet1)print(res)print(f'mySet1:{mySet1}, \\nmySet2:{mySet2}')---NonemySet1:{'Go', 'Python', 'Rust', 'C++', 'Swift'}, mySet2:{'Java', 'C', 'C++', 'Swift', 'Rust', 'Go', 'Ruby', 'JavaScript'} 嗯，这个结果，一点都没有惊喜和意外。 好吧，运算我们学完之后，接着我们要看看集合的检测方法, 一共有三个，记住用法就可以了： issuperset()检测是否为超集 issubset()检测是否为子集 isdisjoint()检测是否不相交 为了更好的说明，我们不能在用之前的集合，这回，我们定义三个集合来看： 123mySet1 = {1, 2, 3, 4, 5, 6}mySet2 = {1, 2, 3}mySet3 = {6, 7, 8} 接下来从第一个检测开始： 12345678print(mySet1.issuperset(mySet2))print(mySet2.issuperset(mySet1))print(mySet1.issuperset(mySet3))---TrueFalseFalse 不知道大家在数学里有没有学过「超集」的概念，我们从最后的打印结果可以看出来，mySet1是mySet2的超集，反过来则不是，并且，mySet1也不是mySet3的超集。观察三个集合内的元素我们可以得出结论，如果集合a是另外一个集合b的超集，那么集合b内的元素一定在集合a中都找得到。 再来检测子集： 12345678print(mySet1.issubset(mySet2))print(mySet2.issubset(mySet1))print(mySet3.issubset(mySet1))---FalseTrueFalse 从这个结果中我们能看到，子集的概念就和超集完全相反了。 最后就是检测两个集合是否相交了，也就是集合中的元素有没有重复的： 12345678910print(mySet1.isdisjoint(mySet2))print(mySet2.isdisjoint(mySet1))print(mySet3.isdisjoint(mySet1))print(mySet2.isdisjoint(mySet3))---FalseFalseFalseTrue 虽然前三个都打印的False， 最后一个打印的True，但是我们从集合中应该知道，只有mySet2和mySet3没有相交关系。所以我们可以知道，isdisjoint()这个函数其实是检测不相交的。也就是说，返回结果为False则证明相交，返回结果为True反而是不相交。 结语 至此，随着我们的集合内容讲完，咱们的容器类数据类型就全部讲完了。 咱们下一节开始，咱们要开始行的篇章。下一节内容预告：Python中File文件的操作。 本节课一样就不布置作业了，大家好好的将最近将的容器类数据好好的回顾一下，将基础打扎实。","link":"/Detailed-of-set/"},{"title":"13. Python的文件操作","text":"Hi，大家好。我是茶桁。 在之前的几节课程中，我们学习了Python的数据类型。和市面上大多数的Python教程不同的是，我先为大家介绍完函数之后才开始介绍数据类型，其中原因就是很多数据类型的方法及理解都需要先搞懂函数的基本语法。 在结束了Python数据类型学习之后，我们今天开始进入一个新的篇章。今天，让我们来详细了解一下在Python中如何去进行文件操作。 我们大家都使用过智能手机，电脑，iPad等电子产品。那我们肯定有打开文件的经验，比如说打开一个Word、Excel文档。最基础的操作实际上就两步，分别是1. 打开文件， 2. 关闭文件。 我们要理解的一点是，文件都是放在存储设备中的，这才是我们能打开它的基础。那我们在存储设备中对文件进行打开之后进行的读写操作，实际上就是文件I/O。 什么是I/O?I代表Input(输入)，O代表Output（输出）。当你打开一个文件的时候，就算你没有对文件进行更改，也依然已经有了I/O操作，毕竟文件只有读取之后，才能显示到你的屏幕上。 那么文件读写到底分了几步呢？让我们引用一下宋丹丹的经典小品中的一段： 问，把大象装进冰箱分几步？ 我们就不在这里进行分步讨论了，因为流程步骤实际上是一模一样的： 打开文件open() ： 打开冰箱。 读取文件read()/ 写入内容write()： 把大象装进冰箱。 关闭文件close()： 关闭冰箱。 可以说，你在你的设备上做的任何操作都逃不开这几步，区别无非就是你有没有写入内容，从哪里打开的，读取的文件是什么类型的。 那么复杂一点的，就是当你打开一个App的时候，这个App执行某项操作的时候去互联网上的服务器找相应的文件然后到本地之后打开，读取。我们不讨论在打开文件之前的一系列例如下载（这个下载动作有时是主动的，有时是被动的）操作，就只说到本地之后读取文件并展示，就一定包含这三步。 理解到这，可以了。我们接着正式来学习Python如何对文件进行操作。 文件操作 open() open函数就是用于最初的打开文件的动作，其基本格式为：open(文件路径, 打开方式, [字符集])， 完整的格式为：open(file, mode='r', buffering=None, encoding=None, errors=None, newline=None, closefd=True) 在大部分时候，我们使用基本格式就足够了。 123456'''打开文件 open() 参数1: 文件路径 参数2: 打开的方式 参数3: 字符集''' 路径，也就是url是一种统一资源定位符。其中包括相对路径和绝对路径。 相对路径，比如说我们被路人问路，我们就说：这条街往前，前面十字路口就是交道口，然后左转，再走100米左右就到了。 绝对路径， 这个就非常好理解了，北京市西城区鼓楼东大街28号，特别准确了对吧？ 这两个路径的描述呢，其实指向的是一个地方。只是一个是针对人所在的位置来告知你怎么走，另外一个是从最上层给到你一个绝对的地址。而电脑里的相对路径和绝对路径也基本就是这么个意思。 我们来看相对路径，主要是使用./和../来进行描述，这两个都有一个共同点，就是以当前文件为准。也就是当前文件向我们问路，我们站在当前文件的地方告诉它该怎么走去到达自己的目的地。 举例, 假设我们现在正在编辑index.py这个文件，也就是说，向我们问路的文件是index.py： 123456789- project | - index.py | - test.txt | img | - person.jpg | - dog.jpg | - cat.jpg- data | - person.csv 这样一个路径关系中： 当我们需要去访问person.jpg并打开的话，那就是index.py同目录下的img目录里面去寻找person.jpg， 那我们相对路径的写法为./img/person.jpg。 当我们要去找person.csv的时候，由于这个csv是存在于上一层目录的同级目录data内，那我们需要向上去寻找，就是../data/person.csv。 这就是./和../的区别，一个是当前目录同级内去寻找，一个是向上一级的目录内去寻找。那如果文件存储于上两层目录中呢？那就向上翻两层呗：../../这样，多层的时候，依次类推。 相对路径介绍完了，我们来看看绝对路径。 绝对路径的前提是必须找到根目录。在windows中我们其实都熟悉一个东西就是盘符。比如说C:\\，不严谨的说，盘符就算是绝对路径的根目录了。那为什么说不严谨的说呢？因为我们输入文件路径的时候可以输入：C:\\data\\person.csv这样去寻找。但是，盘符之上其实是整个硬盘，我们只是将硬盘虚拟成了不同的盘符用于划分空间而已。 在Mac或者Linux中，就是以整个硬盘为准去寻找文件的。比如说/Users/xx/Downloads，就是我们的下载目录。 那我们如果想要打开文件，这两种方式其实都可以，一般来说，为了代码能够适应环境变化，我们都会选择使用相对路径。 说完文件路径，让我们来说说打开方式，我先介绍一个模式，后面咱们再慢慢讲： w模式： write， 写入 如果文件不存在，创建这个文件； 如果文件存在，则打开这个文件，并且清空文件内容。文件打开后，文件的指针在文件的最前面。什么是指针呢？ 可以这么理解，当我们打开一个word文档的时候，我们的光标是不是都在这个文档的最上面？这个光标的位置，就是指针的位置。 write() write()是用于对文件写入内容来使用的，格式为:文件对象.write(内容) close() 格式为: 文件对象.close() , 可以关闭打开的文件。 我们需要注意一点，我们在对文件进行操作的时候，一定记得操作完要关闭它。否则，这个文件就会一直存在于内存地址中。 下面，让我们看看在Python中如何打开操作一个文件的。 以下所有的操作演示都会在../Python/13.ipynb中进行编写，所以我们的操作路径都会以这个文件为准。 让我们现在当前文件的中创建一个文件夹data，然后在其中放入一个文件13-1.txt，我们说要做的事情，就是打开这个文件，然后将我们之前写的内容复制一部分写入到这个txt文件中去，路径关系如下图： 12345678# 打开13-1 并且写入内容fp = open('./data/13-1.txt', 'w')print(fp, type(fp))fp.write('相对路径: 比如说我们被路人问路，我们就说：这条街往前，前面十字路口就是交道口，然后左转，再走100米左右就到了。\\n 绝对路径: 这个就非常好理解了，北京市西城区鼓楼东大街28号，特别准确了对吧？')fp.close()---&lt;_io.TextIOWrapper name='./data/13-1.txt' mode='w' encoding='UTF-8'&gt; &lt;class '_io.TextIOWrapper'&gt; 打印区打印的内容，实际上是我们print函数执行的结果，可以看到，我们打印fp这个变量的时候，显示的是&lt;_io.TextIOWrapper name='./data/13-1.txt' mode='w' encoding='UTF-8'&gt;, 其类型是&lt;class '_io.TextIOWrapper'&gt;。 这些先放在一边，让我们看看文件到底写入没有： 写入是写入了，可是这是什么鬼？ 啊，差点忘了，整个open()方法内后面还有一个参数encoding=， 这个参数是告诉我们这个文件以什么字符集去打开。默认的就是UTF-8，显然，我们保存的这个文件并不是，所以最终导致了乱码。 让我们修改一下代码，在open()内添加一下encoding，其他不变: 12345fp = open('./data/13-1.txt', 'w', encoding='GBK')...---&lt;_io.TextIOWrapper name='./data/13-1.txt' mode='w' encoding='GBK'&gt; &lt;class '_io.TextIOWrapper'&gt; 可以看到，打印出来的fp最后的encoding值已经发生了变化。让我们再去看看文件如何了： 果然没问题，内容能够正确显示而不会乱码了，我们注意到下方文件字符集确实为GBK。 关于字符集编码的问题这里有疑问的，自己回过头再去把我之前讲的课程好好翻腾一下，复习一下。 整段代码中，我们引用了刚才介绍的三个文件操作的函数： open(), write(), close()。 在简单了解了文件的操作步骤之后，我们接下来再继续看文件操作中另外一个比较重要的函数: read()。 read() 在对文件进行操作的时候，一定要记得流程一定是打开open在最前面，close关闭在最后面。至于中间你是要读取，写入还是别的什么操作，那都不违反文件操作的整个流程。 所以在下面一段代码里，我们可以尝试把之前的write()替换为read()，顺便可以学一下如何在代码中看看我们刚修改过的文件： 123456789fp = open('./data/13-1.txt', 'r', encoding='GBK')res = fp.read()fp.close()print(res)---相对路径: 比如说我们被路人问路，我们就说：这条街往前，前面十字路口就是交道口，然后左转，再走100米左右就到了。绝对路径: 这个就非常好理解了，北京市西城区鼓楼东大街28号，特别准确了对吧？ 可以看到，我们讲刚才写入的内容在打印区完整的打印了出来。 不知道小伙伴们有没有注意到，在使用open()函数的时候，其中的第二个参数「打开方式」这次发生了变化，改成了‘r’， 这中打开模式就是专门用于读取文件的，它在打开文件的时候，不会想‘w’的打开方式一样清空文件。 比如，我们讲之前的代码中换一下打开方式来试试： 12345678fp = open('./data/13-1.txt', 'w', encoding='GBK')res = fp.read()fp.close()print(res)---UnsupportedOperation: not readable 报错了，提示不可读。 我们再去直接打开13-1.txt的时候可以看到。文件内空空如也，之前写入的内容全都被清空了。 到这里为止，大家了解了文件操作的四个基本操作函数，在这里我可以教大家一个文件操作中的一些高级技巧，比如，我们可以使用with...as...来进行操作： 1234'''with open(文件路径, 打开模式) as 变量: 变量.操作()''' 让我们直接来看示例： 1234567with open('./data/13-1.txt', 'r+', encoding='GBK') as fp: res = fp.read() print(res) ---相对路径: 比如说我们被路人问路，我们就说：这条街往前，前面十字路口就是交道口，然后左转，再走100米左右就到了。绝对路径: 这个就非常好理解了，北京市西城区鼓楼东大街28号，特别准确了对吧？ 这样，我们也就直接完成了之前读取的操作。 read函数内是有参数的：read(count)， 接收的值为整型，这里是描述当前我要读取几个字节长度： 123456with open('./data/13-1.txt', 'r', encoding='GBK') as fp: res = fp.read(5) print(res) ---相对路径: 有的小伙伴看到我写到这可能就有疑问了，我为什么没有写close()函数，那是不是说，现在这个文件都还一直存在内容地址中。 其实并不是如此。在使用with...as...这个方式去打开一个文件的之后，在整个代码结束的时候会自动对当前打开的文件一遍执行close()函数。 好，让我接着继续： 12345678with open('./data/13-1.txt', 'r+', encoding='GBK') as fp: res = fp.read() print(res) fp.write(res)---相对路径: 比如说我们被路人问路，我们就说：这条街往前，前面十字路口就是交道口，然后左转，再走100米左右就到了。绝对路径: 这个就非常好理解了，北京市西城区鼓楼东大街28号，特别准确了对吧？ 打印区并未发生变化，原因就是我们的写入操作是在print之后进行的，我们直接打开文件来看看： 可以看到，内容确实被写入文件中了。注意我打标记的地方，并没有换行对吧，也就是说，我们在做写入的时候，指针是标记在这个位置的，然后继续往后写入。 另外整个代码中需要注意的就是打开模式了，我们之前已经用过的打开模式有‘w’和``'r'， 现在我们用了‘r+’的模式，那么r+呢就是既可以读，也可以写入。并且，不会一开始就清空文件的内容。 对应‘w’的清空模式，就是‘w+’, 虽然‘w+’也是可读可写的模式，但是它和‘w’的模式一致，打开文件的时候直接清空整个文件的内容。 除了这四个模式之外，还有'a'和‘'a+’模式，是追加写的模式，这种模式的特点是打开文件的时候，指针是放在文件最末尾的。所以这种模式使用read()的时候，是读不到任何内容的。 以为到这里就结束了吗？太单纯了，整个文件操作的打开模式中，还有一个‘x+’的模式，这种模式我们可以称它为异或，什么意思呢？就是这种模式只会新建文件来执行后续操作，否则就会报错： 12345with open('./data/13-1.txt', 'x+', encoding='GBK') as fp: print(fp.read()) ---FileExistsError: [Errno 17] File exists: './data/13-1.txt' 提示文件错误：文件存在。 如果我们操作的是一个本来不存在的文件，才可以正常的往下进行： 1234567891011with open('./data/13-x+.txt', 'x+', encoding='UTF-8') as fp: res = fp.read() print(res) fp.write('这里是&quot;x+&quot;模式下新加入的内容。')with open('./data/13-x+.txt', 'r+', encoding='UTF-8') as fp: res = fp.read() print(res) ---这里是&quot;x+&quot;模式下新加入的内容。 我们在用‘x+’模式打开一个文件的时候，它已经新建了这个文件，我们可以看到读取之后并未读取到任何内容，因为这个文件内还是空的。在进行写入操作之后，我们在下面再一次读取这个文件，可以看到内容已经被写入了。 详谈「打开模式」 其实mode这个参数并不只是我们演示的这么点内容，mode这个参数是接收两种值的，一个是刚才我们一直在讲的读写模式, 而另外一个则是文件格式： 读写模式： 读写模式的参数主要有四种， 分别是r, w,a以及一个特殊+， 其中r, w, a决定了当前文件默认是只读还是只写，还有就是指针位置。+是和前面三个结合使用的，无法单独使用，其主要作用是使的文件读写兼备。 文件格式： 文件格式主要是以两种格式为准，一种是普通的文本文件，一种是二进制格式文件。不要以为二进制格式没什么大不了，我们一般谈到非文本文件都属于二进制格式文件，比如：图片。 这两个格式控制字符一个是t: 以文本格式打开文件（默认值）， 一个是b: 以二进制格式打开文件。 一般来说，我们大部分时候都不会单独使用某一个参数吗，而是结合着一起使用。比如： r+， 打开一个文件用于读写，文件存在就打开，文件不存在则报错。指针在文件头。这种模式要注意，因为指针在文件头，所以新写入的内容会在原内容之前。 w+, 打开一个文件用于读写，文件存在就打开，并且会清空所有内容后进入编辑模式，如果文件不存在则会创建一个新文件。虽然指针也在文件头，但是因为它霸道的清空属性，所以也不存在新写入的内容会在原内容之前了。 a+， 以追加的模式打开一个文件用于读写，如果文件存在就打开，如果文件不存在，则会创建一个新文件用于读写。这种模式下和w+不同的地方在于它会将指针放在文件末尾，写入的时候是从文件尾部开始写。并且，它没那么霸道，要清空原内容才可以。 其他的模式就是在打开文件格式和读写模式的组合，一般我们不写是因为大部分时候我们操作的都是文本文件进行操作，而如果我们需要用二进制格式打开文件的时候，就不能使用默认的t而是b了，一般我们会是这样进行组合：rb, rb+, wb, wb+, ab, ab+。 当然，最后就是我们刚才用到的x+， 其实它也是一种组合形式，原本应该是x, 这种模式是在Python3中新添加的，它在文件不存在的时候它会创建一个新文件用于写入。如果这个文件存在，就会报错。 那么有了x这个参数之后，我们以前为了避免误操作覆盖原文件，那么我们会先去判断一个文件是否存在，然后再去执行后续的写入操作。可是使用x就没那么麻烦了，可以直接操作写入，反正文件如果存在会返回错误。 关于指针位置 那么我们在使用了r+之后，有没有什么办法可以让我们不在原内容之前写入内容而是从后开始写呢？ 答案是有办法，也就是调整指针位置，调整完毕之后再进行写入操作就可以了。 调整指针的方法为seek(offset[, whence])。我们来看一个对比： 12345678910111213141516171819202122232425# 创建一个新文件with open('./data/13-2.txt', 'w', encoding='UTF-8') as fp: fp.write('1. Hello Python.\\n2. Hello C++. \\n3. Hello Ruby.')# 正常状态下with open('./data/13-2.txt', 'r', encoding='UTF-8') as fp: print(fp.readline()) print(fp.readline()) print('--------------')# 设置指针重新偏移到头部with open('./data/13-2.txt', 'r', encoding='UTF-8') as fp: print(fp.readline()) fp.seek(0, 0) print(fp.readline())---1. Hello Python.2. Hello C++. ------------1. Hello Python.1. Hello Python. 在最开始，我们重新创建了一个文件，然后写了三行文字。分别是： 1231. Hello Python.2. Hello C++. 3. Hello Ruby. 然后我们开始用不同的方法进行读取，每次仅读取一行。 正常状态下，readline()这个方法是顺序往下执行的，第一次执行的时候读取的是第一行，第二次执行的时候就是读取的第二行。这种方式是不是感觉有些熟悉，像不像迭代器？ 回过头来，我们再来看两次执行的结果，不同的是，第二次我在两个readline()方法中间加入了一段fp.seek(0,0)来将指针再次调整到头部，别着急，我们一会讲为什么这样写，先来看看结果。 因为有了fp.seek(0,0)的存在，第二次执行和第一次完全不同。第一段内容被读取了两次。这就是seek()的作用，讲指针又调整到了文件头部。 现在，让我们来说说seek()内参数的含义，完整的写法是：seek(offset[, whence])，其中offset是偏移量，而whence是从哪开始。whence就只有三个值， 0, 1, 2， 0就表示是从头部开始偏移，1就表示从当前位置开始偏移，2就代表从文件末尾开始偏移。而我们写的(0,0)意思就是从文件头部开始偏移，偏移量为0。 再来看一段代码： 123456789101112# 设置指针重新偏移到头部with open('./data/13-2.txt', 'r+', encoding='UTF-8') as fp: fp.seek(0, 2) fp.write('这里是使用r+添加到末尾的内容')with open('./data/13-2.txt', 'r', encoding='UTF-8') as fp: print(fp.read()) ---1. Hello Python.2. Hello C++. 3. Hello Ruby.这里是使用r+添加到末尾的内容 通过前面的学习我们知道，r+在打开文件之后，指针是放在头部的，但是我们这里用seek(0,2)将指针调整到了最末尾，并且写入了一段文字。 学到这里，文件的基本操作也就差不多学完了，让我们来分别总结一下： 相关函数 open(), 打开文件, 格式：open(file_name [, access_mode][, buffering]) read(), 读取内容， 格式：fileObject.read([count]) 不设置count是从当前位置读取到文件末尾，设置count这是读取指定长度的字符。 readline(), 读取一行 不设置count是从当前位置读取到这一行末尾，设置count这是读取这一行中指定长度的字符。 readlines(), 读取所有行 不设置参数是表示读取所有汗，每一行作为一个参数，返回了一个列表。设置count是按照行进行读取，可以设置读取的字节数，设置的字节数不足一行按一行来读取。 write()，写入内容, 格式：fileObject.write(string) writelines(), 写入容器类型数据： 写入容器类数据的时候要注意，这个容器类数据必须是可更新的类型。 seek(), 设置文件指针的偏移, 格式: seek(offset[, whence]) close()， 关闭文件 当然，除了这几个之外，文件还有很多其他的函数，但是目前我们用这些进行读写操作就足够了。 打开模式（图） 关于打开模式， 我之前写的那些内容看懂理解了，其实也就不需要现在这两张图了，可是我担心的是有些小伙伴理解不了，那有了下面的图，至少操作的时候可以参考： 模式 描述 t 文本模式 (默认)。 x 写模式，新建一个文件，如果该文件已存在则会报错。 b 二进制模式。 + 打开一个文件进行更新(可读可写)。 U 通用换行模式（不推荐）。 r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。 rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。一般用于非文本文件如图片等。 r+ 打开一个文件用于读写。文件指针将会放在文件的开头。 rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。一般用于非文本文件如图片等。 w 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 w+ 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。 ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。 总结一下最常用的六种模式： 模式 r r+ w w+ a a+ 读 ✓ ✓ ✓ ✓ 写 ✓ ✓ ✓ ✓ ✓ 创建 ✓ ✓ ✓ ✓ 覆盖 ✓ ✓ 指针在开始 ✓ ✓ ✓ ✓ 指针在结尾 ✓ ✓ 下面这张经典的流程图可以告诉你在什么时候需要用什么： 结尾与预告 文件的基本操作就介绍到这里了，大家下课之后记得要去多多的熟悉和练习。 那么下一节课呢，我们会根据我们这之前所讲的所有内容，尝试做一个小demo， 实现一个简单的注册和登录功能。 这里先介绍一下这个demo: 123456实现功能：1. 用户输入用户名和密码以及确认密码2. 用户名不能重复3. 两次密码要一致4. 用户用已经注册的账户登录5. 密码如果错误3次，锁定，无法再登录。 好了，小伙伴们，咱们下节课再见。","link":"/file-operations/"},{"title":"14. 练习：登录注册系统","text":"Hi，大家好。我是茶桁。 上一节课，我们详细的介绍了文件读写的流程和原理，并用Python进行实际操作了一下。 那么这节课呢，我们利用之前所学的内容，尝试做一个小练习：建立一个登录注册系统。上节课我们在结尾的时候讲练习内容贴了出来，还记得要求吗？ 123456实现功能：1. 用户输入用户名和密码以及确认密码2. 用户名不能重复3. 两次密码要一致4. 用户用已经注册的账户登录5. 密码如果错误3次，锁定，无法再登录。 那么这节课呢，因为都是一些讲过的知识点，所以在整个实现过程中我就不详细讲解了，我们重点在于介绍思想和流程。那让我们开始吧。 注册功能 我们先把大的结构写出来，一个注册功能，那首先需要接收两个参数：用户名、密码。 并且，为了防止用户注册时候输错密码导致无法登录，还需要让用户确认一遍密码： 1234567891011121314151617181920# 先实现注册功能# 封装一个函数 完成注册功能def register(): # 用户输入用户名 username = input('欢迎注册，请输入用户名：') # 用户输入密码 password = input('请输入您的密码: ') # 请确认您的密码 re_password = input('请再输入一次您的密码: ') print(username, password, re_password)register()---du 111111 111111 下一步我们来细化这个框架，我们把用户名先放在一边，来挑一挑密码的刺： 首先，我们需要让密码保持安全性，那么我们对位数，组合就要有要求。这里我们简单点，必须输入6位以上吧。 然后，在确认密码的时候，肯定两次密码要一致才行。 OK，让我们补全这几个逻辑关系： 12345678910111213141516171819202122232425262728293031# 先实现注册功能# 封装一个函数 完成注册功能def register(): # 用户输入用户名 username = input('欢迎注册，请输入用户名：') # 用户名需要检测是否已经存在 # 用户输入密码 password = input('请输入您的密码: ') # 检测密码长度不能低于6位 if len(password) &gt;= 6: # 请确认您的密码 re_password = input('请再输入一次您的密码: ') # 检测密码和确认密码是否一致 if re_password == password: # 用户名和密码都正确，可以写入文件 pass else: print('两次输入的密码不同，请重新输入。', username, password, re_password) # 密码长度不够 else: print('密码格式不正确：', username, password)register()---两次输入的密码不同，请重新输入。 du 123456 1234567 可以看得出来，我确认密码的时候输入了其他密码，判断逻辑没有问题。 继续来细化，现在的问题是，当我们输入第一次密码的时候判断有问题，或者两次密码输入不一致的时候，我们没有办法跳到一开始让用户重新输入，那么下面我们就来解决这个问题： 12345678910111213141516171819202122232425262728293031323334353637# 先实现注册功能# 封装一个函数 完成注册功能def register(): # 用户输入用户名 username = input('欢迎注册，请输入用户名：') # 用户名需要检测是否已经存在 # 利用循环，都正确的时候结束循环。 while True: # 用户输入密码 password = input('请输入您的密码: ') # 检测密码长度不能低于6位 if len(password) &gt;= 6: # 请确认您的密码 re_password = input('请再输入一次您的密码: ') # 检测密码和确认密码是否一致 if re_password == password: # 用户名和密码都正确，可以写入文件 print('恭喜你，注册成功', username, password, re_password) # 结束循环 break else: print('两次输入的密码不同，请重新输入。', username, password, re_password) # 密码长度不够 else: print('密码格式不正确：', username, password)register()---密码格式不正确： du 12345两次输入的密码不同，请重新输入。 du 123456 1234567恭喜你，注册成功 du 123456 123456 外面套一层while循环，这样就解决了。 只有当密码正确输入的时候，循环才会结束，否则就会跳到循环的最开始，重新进行输入。 接着，我们就来看看当输入正确之后，我们如何写入文件呢？这个是我们上一节课刚讲解的课程，现在让我们来实现一下： 1234567891011121314151617# 先实现注册功能# 封装一个函数 完成注册功能def register(): ... if re_password == password: # 用户名和密码都正确，可以写入文件 # 打开文件，写入数据 with open('./data/user.txt', 'a+', encoding='UTF-8') as fp: fp.write(f'{username}:{password}\\n') print(f'注册成功，用户名:{username}') # 结束循环 break ...register()---注册成功，用户名:du 这样，我们就将用户名和密码以username:password的格式以一行的形式存储到了一个user.txt文件内。并且，因为我们使用的a+的模式，所以每次打开指针都会是放在文件的末尾进行添加。 来，让我们看看是否成功了： 可以看到，没问题。这样，我们在之后读取的时候就可以直接读取单行，并且以key:value的形式拿到我们说要的用户名和密码。用于验证key是否存在，value是否正确。 说到验证key是否存在，似乎我们还没写这一段验证代码，既然文件里已经有数据了，让我们把这段代码补全吧, 我们在register()这个自定义函数外面写一段读取文件的代码，写在外面是因为，我们读取文件这个动作，和整个注册动作不能说毫无关系，只能说是没有关联。 123456with open('./data/user.txt', 'r', encoding=&quot;utf-8&quot;) as fp: res = fp.readlines() print(res)---['admin:123456\\n', 'du:654321\\n'] 还记得吗？readlines()这个函数的应用，不记得的，看我教程的上一节内容复习下。 看似我们确实成功的读取到了全部内容，可是我们细想一下，这样想有没有问题？还记得我们上一节课上对r这个模式的说明吗？r是读取已有文件，但是如果文件不存在，那就会报错。 可是我们在整个程序运行过程中，不能因为这个原因就不让用户继续往下走了对吧？那么我们怎么去修改呢？来，我们一起尝试下： 12345678# 读取所有的注册信息with open('./data/user.txt', 'a+', encoding=&quot;utf-8&quot;) as fp: fp.seek(0,0) res = fp.readlines() print(res)---['admin:123456\\n', 'du:654321\\n'] 我们利用a+的特性，如果文件不存在则会创建，如果有，这将指针放在文件末尾。这样的话，这一段内容就没有问题了？为什么不用w+? 还不是因为w+太霸道，虽然新人胜旧人，但也不能就直接把旧人干掉吧。 但是a+是将指针放在文件末尾的，我们直接使用的话，什么内容也读不到，所以我们使用可seek(0,0)来将指针放在了文件头的位置。 好了，继续。我们只将文件读取出来没用，需要将其中的数据放在一个变量里供我们检测，所以下一步，我们就需要创建一个变量，并且接收文件中的所有数据： 12345678910111213141516171819# 先实现注册功能# 专门定义数据变量，存放已经注册的用户信息userlist = []pwdlist = []# 读取所有的注册信息with open('./data/user.txt', 'a+', encoding=&quot;utf-8&quot;) as fp: fp.seek(0,0) # 调整指针到文件头 res = fp.readlines() # 按照每一行读取所有用户数据 for i in res: # 循环读取每一行数据 r = i.strip() # 处理每一行尾部的换行符 mydict = r.split(':') # 分隔用户名和密码 userlist.append(mydict[0]) pwdlist.append(mydict[1]) print(userlist, pwdlist) ---['admin', 'du'] ['123456', '654321'] 这样，我们拿到用户名和密码，并且分别放入了两个list变量中。 接下来，我们对于注册那一步就需要进行判断了。 1234567891011121314151617# 封装一个函数 完成注册功能def register(): ... # 用户输入用户名 username = input('欢迎注册，请输入用户名：') # 用户名需要检测是否已经存在 if username in userlist: print('当前用户已经存在，请更换用户名。') else: # 利用循环，都正确的时候结束循环。 while True: # 用户输入密码 password = input('请输入您的密码: ') ...# register() 在代码中，我们判断用户名是否存在于列表中，如果存在，打印已经存在，如果不存在，则继续往下执行。将之前输入密码的while循环放入了else判断逻辑中。 看似问题解决了，不过新的问题又来了。虽然我们现在判断了用户是否存在，并且用户不存在的话可以继续往下执行，可是如果存在呢？我们需要的是让用户重新输入用户名，但是现在是打印完之后就结束了。所以，我们还得继续修改代码，这回简单了，和password的处理用一样的方法就可以了。 12345678910111213141516171819202122def register(): # 执行循环， 用户名操作 while True: # 用户输入用户名 username = input('欢迎注册，请输入用户名：') # 用户名需要检测是否已经存在 if username in userlist: print('当前用户已经存在，请更换用户名。') else: # 利用循环，都正确的时候结束循环。 while True: ... if len(password) &gt;= 6: ... if re_password == password: ... # 结束循环 break else: ...# register() 我们将register()内的所有判断代码放入了一个循环中，那么在if之后，没有向下执行else内的逻辑的时候，就会跳转到最开始的while循环重新执行。 不过这样似乎还是不行，那就是几遍else逻辑全部执行完毕，内部的break也是跳出里面那层while循环，外层循环只能无限的执行下去了。 我们目前所要做的，就是在所有的代码顺利执行到密码正确确认的时候，整个函数执行就全部终止了。那有什么办法吗？ 我们看到了while True， 然后开启了无限循环。那是不是说，如果True那里不为真，就无法进入循环了？ 嗯，既然这样，我们设定一个变量，在需要终止函数的时候，设定变量为Flase就行了。 123456789101112131415161718192021222324252627# 封装一个函数 完成注册功能def register(): # 定义一个变量，用于控制外循环 site = True # 执行循环， 用户名操作 while site: ... if username in userlist: print('当前用户已经存在，请更换用户名。') else: # 利用循环，都正确的时候结束循环。 while True: ... if len(password) &gt;= 6: ... if re_password == password: ... # 结束循环 # 结束外循环 site = False # 结束内循环 break else: ...# register() 这样，在密码确认成功之后，site设定为false， while循环判断条件为假，无法再次进入循环。执行代码，输出注册成功：用户名:du2， 并且跳出了循环不再继续向下执行。说明我们这一步已经没问题了。 那到这里，我们一个简易的可运行的注册方法就完成了，当然，这段代码中其实还有很多可以完善的空间，比如说，判断用户名是否存在之后，还要判断用户名是否合法，密码是否合法等等。还有就是我们一般接收账户密码的变量应该使用key:value形式的字典，不过大多数时候，我们还要考虑效率问题。分别保存成两份是不错的选择，那么这个时候，我们就需要用到获取当前字典中所有key值的方法了。以上代码在关键部位可以这样改： 1234567891011121314151617181920212223242526272829# 专门定义数据变量，存放已经注册的用户信息userdict = {}userlist = []# 读取所有的注册信息with open('./data/user.txt', 'a+', encoding=&quot;utf-8&quot;) as fp: fp.seek(0,0) # 调整指针到文件头 res = fp.readlines() # 按照每一行读取所有用户数据 for i in res: # 循环读取每一行数据 r = i.strip() # 处理每一行尾部的换行符 mydict = r.split(':') userdict.update({mydict[0]:mydict[1]}) userlist = userdict.keys() # 封装一个函数 完成注册功能def register(): # 定义一个变量，用于控制外循环 site = True # 执行循环， 用户名操作 while site: # 用户输入用户名 username = input('欢迎注册，请输入用户名：') # 用户名需要检测是否已经存在 if username in userlist: print('当前用户已经存在，请更换用户名。') else: ...register() 总之，代码的实现是需要一步一步细细琢磨的，而且实现的方式也不是只有一种。要考虑逻辑，效率等等因素。文章最后会给到我的源码文件，现在大家先一步步的跟着我往下走吧。 登录功能 在实现登录功能之前，我们来分析一下这个功能要做的事情： 需要使用已经注册的用户信息登录 密码输入错误3次之后，锁定账户信息（不能再使用这个账户进行登录操作） 这是两个最基本的功能，既然要核对用户信息和密码，那和注册一样，我们还是需要读取user.txt文件，拿到里面的信息后传给相应的变量，用于检测。那好，我们先把注册那边实现的部分功能拿过来，就无需自己再写一遍了： 12345678910111213141516171819# 搞定登录功能# 专门定义数据变量，存放已经注册的用户信息userdict = {}userlist = []# 读取所有的注册信息with open('./data/user.txt', 'a+', encoding=&quot;utf-8&quot;) as fp: fp.seek(0,0) # 调整指针到文件头 res = fp.readlines() # 按照每一行读取所有用户数据 for i in res: # 循环读取每一行数据 r = i.strip() # 处理每一行尾部的换行符 mydict = r.split(':') userdict.update({mydict[0]:mydict[1]}) userlist = userdict.keys() print(f'userdict:{userdict},\\nuserlist:{userlist}')---userdict:{'admin': '123456', 'du': '654321', 'du2': '123456', 'zhang': '123456', 'wang': '123456'},userlist:dict_keys(['admin', 'du', 'du2', 'zhang', 'wang']), 执行过后可以看到，三个变量已经分别存储了一个字典和两个列表，也就是字典存储了健值对的用户密码，以及将用户和密码再分别存储到两个列表内。 数据准备好了，接下来，我们开始定义函数吧： 1234567891011121314# 封装登录函数def login(): # 获取用户登录时输入的用户名 username = input('欢迎登录，请输入您的用户名：') # 检测当前用户名是否存在 if username in userlist: # 让用户输入密码 pass else: # 用户名不存在 print('用户名错误，亲重新输入') # 检测用户是否属于锁定状态 那么大的框架就定义好了。现在让我们逐步开始细化这些代码。 先不看让用户继续输入密码的逻辑，我们先来看看用户名不存在的逻辑该怎么做。 当用户名不存在的时候，我们肯定是先要告知提示用户，接下来应该是让用户重新输入一次用户名。那怎么做呢？之前实现注册那边实际上已经有经验了，用while循环呗，接下来，让我们完善一下： 123456789101112131415161718192021222324252627282930# 封装登录函数def login(): # 自定义变量, 控制登录外循环 isLogin = True # 创建循环 while isLogin: # 获取用户登录时输入的用户名 username = input('欢迎登录，请输入您的用户名：') # 检测当前用户名是否存在 if username in userlist: while True: # 让用户输入密码 pwd = input('请输入您的密码：') # 检测用户输入的密码是否正确 if pwd == userdict[username]: print(pwd) isLogin = False # 结束外循环 break # 结束内循环 else: print('您的密码输入有误。') else: # 用户名不存在 print('用户名错误，亲重新输入') # 检测用户是否属于锁定状态login() 在细化的代码中，我们用了和注册函数一样的方法，创建了一个外循环和一个内循环。原因就是我们分别需要判断用户名和密码，都需要返回来重新输入。 之前注册的循环已经讲的比较详细了，所以这里我们就不细致的讲解了，小伙伴们自己好好琢磨一下逻辑关系。 那到这里，我们还缺什么呢？看看需求，我们似乎还需要判断用户输入密码错误的次数对吧？ 好的，让我们继续，从定义变量开始： 12345678910111213141516171819202122232425262728293031# 封装登录函数def login(): ... # 定义变量，用来记录用户输入密码错误次数 errorNum = 3 # 创建循环 while isLogin: ... if username in userlist: while True: ... if pwd == userdict[username]: ... else: # 密码错误，修改变量次数 errorNum -= 1 # 判断当前密码错误次数 if errorNum == 0: print('给你机会你不中用啊细狗。账户已锁定，请联系管理人员并上供品。') isLogin = False break else: print(f'您的密码输入有误, 您还能再尝试{errorNum}次。') else: ... # 检测用户是否属于锁定状态login() 我们定义了一个errorNum的变量，因为我们只能尝试最多三次，所以我们给这个变量设定了一个3的值。 在之后的循环中，没尝试错误一次，我们就让这个变量-1操作，直到变为0为止。 然后，就是我们需要定义一个黑名单把用户名写入了。 1234567891011121314151617181920212223242526272829303132# 封装登录函数def login(): ... # 创建循环 while isLogin: if username in userlist: while True: ... if pwd == userdict[username]: ... else: # 密码错误，修改变量次数 errorNum -= 1 # 判断当前密码错误次数 if errorNum == 0: print('给你机会你不中用啊细狗。账户已锁定，请联系管理人员并上供品。') # 锁定当前账户，把锁定的用户拉入黑名单 with open('./data/black.txt', 'a+', encoding='UTF-8') as fp: fp.write(username+'\\n') isLogin = False break else: print(f'您的密码输入有误, 您还能再尝试{errorNum}次。') else: ...login()---您的密码输入有误, 您还能再尝试2次您的密码输入有误, 您还能再尝试1次给你机会你不中用啊细狗。账户已锁定，请联系管理人员并上供品。 这样，当我们的账户被输入错误三次之后，就会得到神的审判：关入小黑屋了。需要上供才行。（谁家这么设计产品，估计死的会很快吧？） 好了，继续往后，我们只写入了黑名单还不行，这样在执行的时候，还是无法判断用户是否输入的是锁定的账户。接下来需要做的事情，就是读取这个黑名单的文件，然后把里面的用户名全部以列表形式传到这个变量中。 再然后，我们只要在用户输入用户名的时候判断此用户是否被关小黑屋了就行了： 12345678910111213141516171819202122232425262728293031323334353637383940# 搞定登录功能# 专门定义数据变量，存放已经注册的用户信息...blackUserList = [] # 定义一个小黑屋专用变量# 读取所有的注册信息with open('./data/user.txt', 'a+', encoding=&quot;utf-8&quot;) as fp: ...# 读取所有黑名单用户with open('./data/black.txt', 'a+', encoding='utf-8') as fp: fp.seek(0,0) res = fp.readlines() for i in res: r = i.strip() # blackUserList.append(r)# 封装登录函数def login(): ... # 创建循环 while isLogin: # 获取用户登录时输入的用户名 username = input('欢迎登录，请输入您的用户名：') # 检测当前用户名是否存在 if username in blackUserList: print('您的账户已经被锁定，并且还未给管理员上供品。') isLogin = False # 结束外循环 elif username in userlist: while True: ... else: # 用户名不存在 ... # 检测用户是否属于锁定状态login() 功能合并 现在，我们把注册和登录功能都分别完成了，那么我们现在就剩下最后一步，将两个功能合并在一起。 注册和登录的函数是现成的，那么现在我们要做的事情就是： 将读取数据的两个with方法封装到一个函数内 定义一个主函数，进入进程后就执行 告知用户选择登录还是注册 在执行登录注册函数之前，初始化数据（加载读取数据的函数） OK，让我们来实现吧： 首先是封装读取数据的函数： 12345678910111213141516171819202122# 定义一个读取所有用户数据的函数def readAllUsers(): # 读取所有的注册信息 with open('./data/user.txt', 'a+', encoding=&quot;utf-8&quot;) as fp: global userdict global userlist fp.seek(0,0) # 调整指针到文件头 res = fp.readlines() # 按照每一行读取所有用户数据 for i in res: # 循环读取每一行数据 r = i.strip() # 处理每一行尾部的换行符 mydict = r.split(':') userdict.update({mydict[0]:mydict[1]}) userlist = userdict.keys() # 读取所有黑名单用户 with open('./data/black.txt', 'a+', encoding='utf-8') as fp: global blackUserList fp.seek(0,0) res = fp.readlines() for i in res: r = i.strip() # blackUserList.append(r) 注意我们这里用了global, 因为我们需要使用外部定义的变量，所以必须要讲变量改成全局变量，改动才会有效。 注册登录函数不用改动，直接原封不动的粘贴过来就可以了。再来我们就需要直接进入主进程进行选择： 123456789101112131415161718192021222324252627282930# 判断当前脚本是否作为一个主进程脚本在执行if __name__ == '__main__': ''' 这里的代码，只有在使用Python解释器直接运行时才会执行 如果当前脚本作为了模块被其他文件导入后使用，那么这个地方的代码不会执行 因此这个地方的代码，适合写当前脚本中的一些测试，这样不会影响其他脚本 ''' # 调用初始化方法，加载数据 readAllUsers() isBegin = True while isBegin: myStr = ''' ====================== ** 登录（0） 注册（1）** ====================== ''' print(myStr) # 让用户选择对应的操作 num = input('请输入对应的序号，体验功能：') if num == '0': login() isBegin = False elif num == '1': register() isBegin = False else: print('后续功能还在开发中。') isBegin = False 这样当我们输入python3 file.py的时候，就会直接进入这个主程序内。然后执行加载数据的函数，打印用户选择内容，然后等待用户选择并执行后续操作，如图： 而这其中我们设定一个变量isBegin来控制循环，每次执行完一次函数就会结束循环，然后需要重新执行。主要原因就是因为加载数据的方法我们写在了while循环外面，所以当我们注册之后再去登录，新创建的用户并不在用户列表内，无法完成登录。所以干脆结束掉之后重新加载数据，就可以执行登录了。 能不能改善呢？可以。只是今天的课程目的已经完毕了，所以当作留给大家的练习题吧。如何优化整个程序让其更合理高效，大家试试看。然后记得在评论区给我留言。 好了，这节课到这里就结束了，相关代码可以在我的仓库里去找到。 下一节课中，我们会讲解模块，系统的内置模块。我们下次见。","link":"/Exercise-register-system/"},{"title":"15. 系统内置模块","text":"Hi，大家好。我是茶桁。 上一节中，在我们的学习到达一个阶段的时候，我们用之前所学过的知识创建了一个简单的注册登录系统。不知道小伙伴们有没有在课后自己实现一遍呢？编程这种事情，还是要多上手多练才行。 那么今天这节课，我们来学习一下Python系统内置模块。 系统内置模块就是安装完Python解释器之后，系统本身所提供的模块。我知道，咱们之前的课程里有学习系统的内置函数，这个模块和函数不是一个东西。模块这种东西，是需要导入后才可以使用的，比如：json, re, os等等。 行，废话不多说，让我们进入正题。 序列化模块 序列化，就是指可以把Python中的数据，以文本或者二进制的方式进行转换，并且还能反序列化为原来的数据。数据在程序和网络中进行传输和存储的时候，需要以更加方便的形式进行操作，因此需要对数据进行序列化。 对数据进行序列化主要有两种方法，一种呢是Python专用的二进制序列化模块：pickle， 还有一种呢，是互联网通用的文本序列化模块json。 pickle 按照官方的定义来讲 pickle实现了对一个Python对象结构的二进制序列化和反序列化 它提供了一些可供使用的函数，下面让我们来一一介绍一下： 12345678import picklemyStr = 'I love you'res = pickle.dumps(myStr)print(res, type(res))---b'\\x80\\x04\\x95\\x0e\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\nI love you\\x94.' &lt;class 'bytes'&gt; 当前是将一段字符串使用dumps()进行了转化，那其他数据类型是否可以呢？我们来一段列表试试看： 123456myList = [1, 2, 3, 4, 5]res = pickle.dumps(myList)print(res, type(res))---b'\\x80\\x04\\x95\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00]\\x94(K\\x01K\\x02K\\x03K\\x04K\\x05e.' &lt;class 'bytes'&gt; 可以看到，依然进行了转化，并且类型还是bytes。其他的诸如字典、元组等都可以进行这样的转化，我们就不一一的在这里展示了。结论为，我们使用pickle.dumps方法可以进行序列化成为一个二进制的数据。 再让我们来看看反序列化的效果，我在源码中还做过一个元组的序列化，并且给res进行了赋值，我们就拿最后一次的结果来做演示（res = b'\\x80\\x04\\x95\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00(K\\x01K\\x02K\\x03K\\x04K\\x05K\\x06t\\x94.'）： 12345res = pickle.loads(res)print(res, type(res))---(1, 2, 3, 4, 5, 6) &lt;class 'tuple'&gt; 可以看到, 之前序列化成二进制数据的元组被loads() 反序列化转化回来恢复成了元组，我们打印其类型，为tuple。 除了以上两个方法之外，还有另外两个方法dump()和load()， 这四者的区别如下： dumps(obj, protocol=None, *, fix_imports=True, buffer_callback=None): 序列化，可以把一个Python的任意对象序列化成为一个二进制，返回一个序列化后的二进制数据。 dump(obj, file, protocol=None, *, fix_imports=True, buffer_callback=None): 序列化，把一个数据对象进行序列化并写入到文件中。注意，demps是返回并不写到文件中，而dump者是写入到文件中。所以多一个必填参数file， 就是写入的文件对象。 loads(data, /, *, fix_imports=True, encoding='ASCII', errors='strict', buffers=None): 反序列化，可以把一个序列化后的二进制数据反序列化为Python的对象。返回一个反序列化后的Python对象。 load(file, *, fix_imports=True, encoding='ASCII', errors='strict', buffers=None): 反序列化， 在一个文件中读取序列化的数据，并且完成一个反序列化。和loads最大的不同是加载的是读取的文件对象file，而不是data。 可以看到，基本上来说，dump和load是对文件进行操作的方法，那能不能使用dumps和loads来完成呢？让我们来试试： 1234567# 定义数据myDict = {'name':'茶桁', 'age':32, 'sex':'male'}# 进行序列化res = pickle.dumps(myDict)# 写入文件with open('./data/data.txt', 'wb') as fp: fp.write(res) 然后我们看，文件夹中确实多了一个data.txt文件，当我想要打开的时候，提示我为二进制文件。 那基本上可以确定，咱们所作的操作确实成功了。 借用其他的支持二进制文件的编辑器打开看看： 再来，我们把一个反序列的二进制文件读取处理，并完成反序列化： 123456789with open('./data/data.txt', 'rb') as fp: res = fp.read()# 进行反序列化myDict = pickle.loads(res)print(myDict)---{'name': '茶桁', 'age': 32, 'sex': 'male'} 以上两个方式，我们其实完全可以使用pickle模块提供的方法来完成，dump和load: 12345678910myDict = {'name':'茶桁', 'age':32, 'sex':'male'}with open('./data/data2.txt', 'wb') as fp: pickle.dump(myDict, fp)with open('./data/data2.txt', 'rb') as fp: newdict = pickle.load(fp) print(newdict) ---{'name': '茶桁', 'age': 32, 'sex': 'male'} 我们又重新创建了一个序列化，保存数据到data2.txt中，然后反序列化再从文件中读取转化。和之前我们用到的方法得到的结果一样，但是方法我们用的却完全不同。 JSON序列化 JSON的全称为: JavaScript Object Notation, 是一个受JS的对象字面量语法启发的轻量级数据交换格式。其在JS语言中是一个对象的表示方法，和Python中的字典的定义规则和语法都很像。 JSON在互联网中又是一种通用的数据交换，传输，定义的一种数据格式。 和之前的pickle序列化方法一样，JSON序列化也有四种函数，其功能基本是一模一样。只是最后转化的数据格式不同： json.dumps(): 完成JSON格式数据的序列化 json.loads(): 完成JSON格式数据的反序列化 json.dump(): 和pickle模块的dump方法一致 json.load(): 和pickle模块的load方法一致 这里，我们先不着急写代码，我觉得需要对JSON简单了解一下，其实很简单，一说就明白了： 我们之前定义了一个字典：myDict = {'name':'茶桁', 'age':32, 'sex':'male'}， 这个格式的数据在Python中是字典，但是在JS中，这个玩意是一个对象(Object)，如果它放在一个.json文件中，这会是正常的json格式的数据。 我们来做一下操作，上几张图就明白了，为了说明，我们创建一个15_json.html文件和15_json.json， 大家来看： 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; ... &lt;title&gt;Document&lt;/title&gt; &lt;script&gt; let person = {'name':'茶桁', 'age':32, 'sex':'male'} console.log(person, typeof(person)) &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;/body&gt;&lt;/html&gt; 我们在脚本中定义了一个person，格式和Python中的字典一模一样，但是在JS中，它被称为对象。我们在浏览器的控制台中打印出来看看： 那如果在一个JSON文件中呢，它就是一个最普通的JSON数据格式。只是稍微需要注意一下，虽然我们这样写并不会报错，但是总会提示格式问题，JSON最正规的写法，是需要用“\"，尽量不要使用‘’。 提前说这么多JSON的知识点，是因为接下来，如果我们没有说清楚，可能正的会分辨不清。好了，让我们转回Python中： 1234567891011import jsonmyDict = {'name':'茶桁', 'age':32, 'sex':'male'}print(myDict, type(myDict))# 使用JSON模块的dumps方法进行JSON格式的转换res = json.dumps(myDict)print(res, type(res))---{'name': '茶桁', 'age': 32, 'sex': 'male'} &lt;class 'dict'&gt;{&quot;name&quot;: &quot;\\u8336\\u6841&quot;, &quot;age&quot;: 32, &quot;sex&quot;: &quot;male&quot;} &lt;class 'str'&gt; 我们在代码中执行了两次打印，第一次是定义完字典之后，第二次是转换为JSON之后。我们可以看到两次打印结果，几乎是一模一样。当然，中文部分转化的比较明显，但是如果我讲name的值设定为英文，比如Hivan， 那么可以说几乎看不出区别。比较明显的，是我们将类型打印了出来，一个是dict, 一个就是str。 当然，和pickle一样，loads方法将会进行反序列化。 12345newDict = json.loads(res)print(newDict, type(newDict))---{'name': '茶桁', 'age': 32, 'sex': 'male'} &lt;class 'dict'&gt; JSON的转化，基本Python中所有的数据类型都可以进行序列化而不会出现报错的情况，但是有些数据是真的转为了JSON格式的数据，但是有的则只是转为了字符串而已。 让我们尝试将一个复杂的结构数据写到一个JSON文件中： 123myList = [{'name':'admin','age':1000, 'sex':'female'},{'name':'du','age':34,'sex':'male'}]with open('./data/15_data.json', 'w') as fp: json.dump(myList, fp) 我们去查看一下文件的内容： 读取的话，当然也和pickle中的用法也是一致的： 123456with open('./data/15_data.json', 'r') as fp: newList = json.load(fp) print(newList)---[{'name': 'admin', 'age': 1000, 'sex': 'female'}, {'name': 'du', 'age': 34, 'sex': 'male'}] 数学与数值 数学模块 Math Python中的内置数学模块Math提供了很多的数学相关运算。整个模块中的方法都非常简单，直接调用就可以了，当然，前提是需要导入数学模块。 下面我们就简单的介绍一下相关方法(当然，其实我们之前已经介绍过一部分了。)： 向上取整 math.ceil() 1234567import math# math.ceil() 向上取整res = math.ceil(3.14)print(res)---4 这个函数让你想到了什么？不知道小伙伴们还记得前面学习的内容不，是不是特别像我们曾经学过的round()方法？不过这两个方法还有不一样的地方，让我们来看： 123456print(round(3.14))print(round(3.5))---34 看到区别了吗？round实际上是一个四舍五入的函数，而ceil只是向上取整 ，则不管你小数点后面的数字大小。当然，有向上取整: math.floor()，这肯定有对应的向下取整，为了很清晰的看出来，我选择了一个向上接近整数的小数： 1234print(math.floor(3.94))---3 接下来是求幂次方: math:pow(x,y)，这个方法会传入两个数值，前一个数值为底数，后一个则为指数，结果是浮点数。 1234print(math.pow(1.2, 2))---1.44 下面一个是求开平方，结果也是浮点数： math.sqrt() 1234print(math.sqrt(4))---2.0 math.fabs()能够计算绝对值,结果是浮点 1234print(math.fabs(-3.14))---3.14 math.modf(): 把一个数值拆分成小数和整数组成的元组 1234print(math.modf(3.1415))---(0.14150000000000018, 3.0) 至于最后的打印结果，不用太纠结。这并不是Python中的BUG，而是与计算机如何处理浮点数有关，因为计算机存储数据实际上都是二进制的，而二进制无法正确处理。 举个栗子：1/3， 在十进制下，这个数是无限循环小数对吧？3.33333333，二进制实际上也有这种问题，比如说1/10, 十进制下是0.1，可是二进制下呢，就成了无限循环小数：0.000111001100110011...。 其实大部分情况之下，这并不影响我们的使用，只是得到的数值需要处理一下四舍五入就可以了，但是也有不适用的情况。实际上，Python中有专门处理精度计算的模块：decimal，这个等我们以后再详细讲。 math.copysign(x,y): 把第二个参数的正负符号拷贝给第一个参数，结果为浮点数： 123456print(math.copysign(-3, 99))print(math.copysign(3, -2))---3.0-3.0 math.fsum() : 将一个容器类型数据中的元素进行一个求和运算，结果为浮点数 123456print(math.fsum((1, 2, 3)))print(math.fsum({1, 2, 3}))---6.06.0 这个方法的参数值需要注意一下，容器中的元素必须是可以运算的number类型。 math.factorial(x): 以一个整数返回x的阶乘 1234print(math.factorial(4))---24 除了运算函数外，还有一些常量函数。最典型的就是数学常数 π = 3.141592...: 1234print(math.pi)---3.141592653589793 基本可以看出来，Python的数学模块基本都属于很简单的工具类函数， 列举几个之后，大家基本上就都能上手了。其官方的文档地址可以看这里：相关文档地址为：https://docs.python.org/zh-cn/3.10/library/math.html#module-math。 随机模块 random 随机模块也是一个比较简单的模块，大部分时候，我们是使用它来产生随机值使用的。 random模块中的random函数直接使用会返回一个0-1之间的随机小数（左闭右开） 什么是左闭右开呢？通俗点说，就是random这个函数，有可能取到0这个值，而无论如何不会取到1这个值。 1234print(random.random())---0.7363473107012012 random.randrange([开始值]，结束值，[步进值]):随机获取指定范围内的整数。对于这种需要开始值，结束值和步进值的参数形式的函数我们应该都已经非常熟悉了对吧？ 这里有三个值，除了结束值是必选之外，另外两个值都是可选值。当只有一个参数时，默认就是从0到整数之间的值，存在两个参数时，就从开始值到结束值之间的随机数，而当有三个参数时，就会按照步进值从开始值到结束值之间产生一个随机数。需要记住一点，这三种参数取值方式，都是左闭右开的形式。也就是说，结束值是不会被取到的。 随机数大量应用在数字验证码，抽奖以及高并发下生成订单号等应用。 12345678print(random.randrange(5))print(random.randrange(3,6))print(random.randrange(4,10,2))---458 random.randint() 会随机产生指定范围内的随机整数 1234print(random.randint(5, 10))---6 可能有的小伙伴会发出疑问了：茶桁老师，你这个解释是不是写错位置了？将randrange的解释直接复制了下来。其实没有，这两个的功能几乎一模一样，说几乎的意思当然是还是有不同点，唯一一点不相同的是，randint产生的随机整数，是左闭右闭的模式，也就是说，它是可以取到结束值的。 当然我们不能只随机整数对吧？实际应用场景中我们也需要大量的浮点数： random.uniform() 获取指定返回内的随机小数, 实际应用中我们需要注意，这个函数是没有开始值和结束值的，只有范围值，也就是说，你最小值和最大值填入的先后顺序无所谓。 那有的小伙伴会想，如果两个值我填入的数值一样会如何？嗯，那就产生一个唯一的浮点值呗。 12345678print(random.uniform(5, 10))print(random.uniform(10, 5))print(random.uniform(5, 5))---9.5402588987387797.3431368358999895.0 random.choice()， 随机获取容器类型中的值。这个函数的应用范围就非常广了，我们在做数据分析的时候经常会用得到。因为大部分时候，我们说面对的应该都是容器类数据。 123456print(random.choice('123'))print(random.choice([1,2,3,4]))---32 random.shuffle() 随机打乱当前列表中的值，没有返回值，仅仅是打乱原数据： 1234567arr = [1, 2, 3, 4, 5]res = random.shuffle(arr)print(f'arr:{arr} \\nres:{res}')---arr:[4, 1, 5, 2, 3] res:None 当然，我说介绍的函数都只是一部分，目的是打个样，让大家知道这些函数是个怎么回事。大部分时候会用到的函数抽出来讲解一下，更多的内容，还需要参考官方文档：https://docs.python.org/zh-cn/3.10/library/random.html#module-random 系统操作相关模块 OS模块 OS模块，就是操作系统接口模块。这个模块提供了一些方便使用操作系统相关功能的函数。我们之前重点学习的open()，就是这个模块中的相关函数。现在让我们来看看除了open之外，还有哪些函数可供我们日常使用： os.getcwd()获取当前的工作目录,注意获取的不是当前脚本的目录 123456import osres = os.getcwd()print(res)---/Users/du/git/AI_Cheats/Python 不过需要注意一点是，这个函数并不是获取现在这个文件的所在目录，而是当前此文件的执行目录。这个怎么理解呢？我给大家举例说明一下： 我们首先需要知道一个，就是我们在Linux中执行cd和pwd的时候，一个是进入某个目录，一个是打印当前目录路径： 那么这里的pwd所执行的结果，是随着进入目录不同而变化的，比如我们进入我们当前的文件目录： 也就是说，我在哪个目录下执行pwd，那么返回结果就是当前执行的这个目录，而不是pwd这个执行文件本身所在的目录。 gwtcwd()文件，和pwd实际上就是相同的特点，如果在当前目录执行这个脚本文件，那么getcwd获取的就是当前的文件目录。如果这个时候我切换到了其他目录，但是写了getcwd()方法的文件没有挪动位置，那么此时我获取的返回值就是我切换的其他目录，而非文件所在的位置。下面我们可以测试一下： 123456os.chdir('/Users/du/git/')res = os.getcwd()print(res)---/Users/du/git 可以看到，我们执行了和上main相同的代码，但是这个时候res接收返回值发生了变化，其原因就是我使用了os.chdir来改变了一下当前的工作目录。不知道大家现在是否能理解gwtcwd()的工作原理？还是无法理解的，可以多自己写一下代码，做做尝试。连我这种笨人之前学习的时候都能很快理解，小伙伴们肯定更没有问题。 刚才我们的实验中，引出了另外一个方法： os.chdir(), 如上所见，其功能就是修改当前工作目录。 下面我们直接介绍其他的函数： os.listdir() 获取当前或指定目录中的所有项（文件，文件夹，隐藏文件），组成的列表 这个方法和Linux中的list命令就十分像了，让我们先将当前工作目录切回我们当前文件本来所在的目录，然后在来执行一下这个函数： 123456os.chdir('/Users/du/git/AI_Cheats/Python/')res = os.listdir()print(res)---['12.ipynb', '10.ipynb', '.DS_Store', '15_json.html', '14.ipynb', '8.ipynb', '14.py', '11.ipynb', 'globals.ipynb', '13.ipynb', 'Kalman.ipynb', '9.ipynb', '15.ipynb', '7.ipynb', '5.ipynb', '1.ipynb', '3.ipynb', '4.ipynb', '6.ipynb', 'data', '2.ipynb'] 可以看到，目录内所有的文件，包括隐藏文件.DS_Store和文件夹data都被放进了一个列表当中。 这是在不指定目录的情况下，默认为当前工作目录，当然，我们还可以指定目录来获取那个目录下的内容： 12345res = os.listdir('/Users/du/AI/')print(res)---['AIGC', '.DS_Store', 'GPT', 'AI_core_competence', 'stable-diffusion-webui'] 这样，我们就获取到了我们希望查找的目录下的所有内容。我们继续： os.mkdir(文件夹路径, 权限) 这个函数用来创建文件夹，其命令 和Linux中是一模一样，功能也是： 1234567print(os.listdir('./data/'))os.mkdir('./data/test', 0o777)print(os.listdir('./data/'))---['black.txt', 'user.txt', '13-2.txt', '13-1.txt', 'data2.txt', 'data3.json', '13-x+.txt', '15_data.json', 'data.txt']['black.txt', 'user.txt', 'test', '13-2.txt', '13-1.txt', 'data2.txt', 'data3.json', '13-x+.txt', '15_data.json', 'data.txt'] 可以看到我们两次打印结果的对比，确实多了一个test的目录。前一个参数很好理解，重点是后一个参数，什么是权限？ 关于系统中的文件权限，我下面所讲的仅限Linux系统，确切的说是unix，因为包括Mac一样通用： 来，我们先进入目录打印出来看看： 我们主要来看一下data目录的drwxr-xr-x，分别来介绍一下： 第一个字母d代表的是一个目录，如果是-呢，这表示这是一个文件 前三位的rwx代表当前目录（文件）对所有人(u)的权限 中间位置的r-x代表的所属组(g)的权限 末尾的三位r-x代表的是其他人(o)的权限 三个位置介绍完了我们来看字母所代表的意义： r,w,x代表不同的操作权限，其中: r就是可读，权限针对文件，表示可以查看文件内容，针对目录，表示可以ls查看目录中存在的文件名称。 w就是可写，针对文件，表示可以更改文件的内容，针对目录，表示是否可以删除目录中的子文件或者子目录。 x是访问权限，针对文件，表示是否可以开启文件当中记录的程序，针对目录，这表示是否可以进入该目录。 那为什么是777呢？那是因为r代表是4, w代表是2, x代表的是1, 那么7就可以理解了，就是所有数值相加的结果。那么为什么是三个7呢？因为这是在设置三个不同目标的权限，三个位数分别是所有人，所有组，其他。 不过，大家还要注意的一点是，无法使用Python去创建一个比自己这个进程权限还要高的文件。 mkdir()方法是只能创建一个文件夹，无法递归创建文件夹，而当我们需要进行递归创建该怎么办呢？也就是说，我们不仅仅是想要创建test文件夹，而是想创建/test/a/b/c/d/e该怎么办？ os.makedirs()可是进行递归创建文件夹。我们先看一下当前目录结构，直接Finder来看吧： 好，让我们执行一下代码： 1os.makedirs('./data/test/a/b/c/d/e/f/') 再来看看目录结构： 这样就能最直观的看到执行这个函数之后的结果了。 在创建完一些无用目录之后，我当然想着是怎么删除它们。 os.rmdir*() 删除空文件夹，比如我们尝试着删除一下刚才我们创建的目录： 1234os.rmdir('./data/test')---OSError: [Errno 66] Directory not empty: './data/test' 报错了，告知我们无法删除一个空目录，原因就是我们在test里创建了好几层文件夹，那现在test肯定不是空目录，那让我们从内层开始试试： 1os.rmdir('./data/test/a/b/c/d/e/f/') 没有报错，应该是成功了，我们来看看： 确实，f文件夹被删除了。不过太烦了，一个个删除到什么时候去了，还不如我到Finder中直接手动删除呢。 os.removedirs()就是一个递归删除空文件夹的函数，我们来试试： 1234os.removedirs('./data/test/')---OSError: [Errno 66] Directory not empty: './data/test/' 居然又报错，告诉我们非空目录，这... 原来，removedirs方法使用必须是从后往前递归的，也就是说，我们需要将需要删除的所有目录的层级关系给到这个方法，在执行过程中，向上递归，路径中的所有空目录都会被删除： 1os.removedirs('./data/test/a/b/c/d/e/') 再执行一次，这回没问题了。从test开始，下层的所有空目录都被删除了。 然后就是删除文件了 os.remove()就是删除文件，为了测试这个方法，我在data目录下创建了一个空文件test.txt 不过在删除之前，我们还是要先用一下这个文件，来看看如何改名： os.rename(): 用于修改文件或者文件夹的名字 1os.rename('./data/test.txt', './data/test_update.txt') 好了，文件用完了，现在让我们删除吧： 1os.remove('./data/test_update.txt') 顺利删掉了刚才创建的文件。 os.system() 执行操作系统中的命令 比如，我们刚才在命令行里执行过ls -al的命令用于查看当前目录下的所有文件及目录，包括其相关权限，那么我们在Python里可以执行吗？来试试看就知道了： 1os.system('ls -al') 执行效果如图： 这个方法实际上不止是让你在Python中执行系统命令用的，可以用于执行其他.py， 也就是Python文件。比如我们创建了一个hello.py文件，里面写了如下代码： 1print('Hello Python。') 那么，我在其他Python文件中使用os.system方法就可以执行这段代码，当然，前提是你得写对路径。 1os.system('/file_path/hello.py') 这样，你在其他文件内写的一个方法就被当前文件执行了。 os.path 路径模块 在Python创建的整个工程或者某一个函数里，路径操作也是经常要做的事情。比如： os.path.abspath() 就是将相对路径转化为绝对路径吗，多数时候，我们是需要获取文件的绝对路径的，更多的是为了获取当前工作目录的绝对路径。 123456print(os.path.abspath('./15.ipynb'))print(os.path.abspath('./'))---/Users/du/git/AI_Cheats/Python/15.ipynb/Users/du/git/AI_Cheats/Python os.path.basename()， 这个方法可以获取到路径后截取返回主体部分，来看代码，一看就明白了： 12345res = os.path.abspath('./15.ipynb')print(os.path.basename(res))---15.ipynb 截取了路径中最末尾的文件名和扩展名，如果路径上最末尾的是一个文件夹不包含文件，那获取的就是那最后一个文件夹名称。 os.path.dirname()， 返回路径中主体部分之前的内容 12res = os.path.dirname('/Users/du/git/AI_Cheats/Python/data/data.txt')print(res) 不过使用这个方法的时候需要注意，如果你填入的是一个相对路径，它并不能打印出绝对路径： 12345res = os.path.dirname('./1.ipynb')print(res)---. join() 链接多个路径，组成一个新的路径 12345res = os.path.join('./data/test/', '2.txt')print(res)---./data/test/2.txt 实际上，它更像是一个字符串拼接，因为这个方法并不会去验证路径的有效性。 split() 这个方法和join()正好相反，用于拆分路径，把路径拆分为路径和主体部分。然后返回一个元组。 12345res = os.path.split('./data/test/2.txt')print(res)---('./data/test', '2.txt') splitext()拆分路径，可以拆分文件后缀名 12345res = os.path.splitext('./data/test/2.jpg')print(res)---('./data/test/2', '.jpg') os.path.getsize()获取文件的大小 , 单位是字节数 12345res = os.path.getsize('./data/15_data.json')print(res)---91 os.path.isdir()会检测是否是一个文件夹，检测其是否存在，返回True或者False 12345res = os.path.isdir('./data/test')print(res)---False os.path.isfile()会检测文件是否存在，一样是返回True或者False 12345res = os.path.isfile('./data/test_update.txt')print(res)---True exists() 是一个通用函数，检测路径是否存在。和以上两个不同的是，也可以检测文件，也可以检测路径： 12345678res = os.path.exists('./data/test_update.txt')print(res)res = os.path.exists('./data/test')print(res)---TrueFalse 当我们有一个相对路径和一个绝对路径，而我们想看看两个路径是否指向一个目标位置的时候，是不是要先获取相对路径的绝对路径之后，再去对比呢？ 其实没有那么麻烦, 只需要os.path.samefile(a, b)就可以了。 1234567a = './data/data.txt'b = '/Users/du/git/AI_Cheats/Python/data/data.txt'res = os.path.samefile(a, b)print(res)---True 使用这个方法，我们需要填入的两个值是真实存在的路径。 当然，官方的文档内还有更多的函数，我这里仅仅是列出了一些常用的。大家可以去官方文档内去看看。 shutil高级操作模块 shutil模块对文件和文件集合提供了许多的高级操作。其中就有支持文件复制和删除的一些功能。 要说，其实shutil这个模块的很多方法和Unix里的shell util都一样，所以会用命令行的小伙伴，对这个模块应该是极其容易上手： shutil.copy()， 一看就明白是干什么的是吧？就是将文件拷贝到指定目录的。 1234shutil.copy('./data/data.txt', './data/test/data.txt')---FileNotFoundError: [Errno 2] No such file or directory: './data/test/data.txt' 报错了，咋回事？看提示，应该目录或文件不存在。嗯，这个方法要操作之前，目标路径中的目录是必须存在的，它无法自动创建目录。 让我们手动创建目录之后再试试， 还记得我们之前创建目录的命令怎么做吗？ 12345os.mkdir('./data/test')shutil.copy('./data/data.txt', './data/test/data.txt')---'./data/test/data.txt' 有返回值了，那我们操作应该完成了。走，去目录里看看： 确实，目录和文件都存在了，这里注意我标注的两个文件，copy这个命令指示拷贝了一个副本到目标目录中，原文件还是存在于原来的位置。但是注意到时间了吗？修改时间被更改了，改为了我们执行当前操作的时间。 copy2是另外一个拷贝方法，它所有功能和copy都一样，但是如果真是一模一样的方法，也就没必要多创建一个了对吧？这个方法最大的不同，就是保留了原文件的信息，包括操作时间和权限等。 再让我们操作一下试试： 1shutil.copy2('./data/data.txt', './data/test/data.txt') 除了这两个拷贝方法外，还有一个拷贝方法copyfile()， 专门用于拷贝文件中的内容，写入到新的文件中去。让我们在./data/test/中新建一个data2.txt文件来接收写入内容： 1shutil.copyfile('./data/data.txt', './data/test/data2.txt') 我们来打开data2.txt之后查看一下内容，确实写入了： shutil.copytree('./a', './b')方法看名字应该就猜到是干什么的，是将整个目录结构全部拷贝到指定目录中。使用的时候要多注意，指定的目标目录必须不存在同名目录。 shutil.rmtree()，我们之前有用到带rm的方法，那么看名字也就知道了，这个方法是删除整个文件夹，包括文件夹下的所有目录和文件，和之前我们使用的removedirs不同，这个方法并不是从下往上递归，而是直接全部删除。让我再创建一次多级目录才测试一下： 1os.makedirs('./data/test/a/b/c/d/e/f/') 目录创建完成后，来让我们将整个test文件夹全部删除： 1shutil.rmtree('./data/test') 执行成功，test整个目录及其内部文件全部删除了。 我们最后再来看一个用的非常多的方法shutil.move()， 我们都知道，windows中有剪切和复制两种菜单命令，然后到新的目标目录后进行粘贴，如果是剪切命令，这原目录中文件会在粘贴完成后删除，而如果是复制，不会执行删除。Linux的逻辑稍微有些不同，是先进行拷贝，然后在目标目录之后决定是粘贴还是移动，如果是粘贴的话就保留原目录的文件，如果是移动，则会在粘贴完成之后在原位置删除文件。 虽然逻辑上有些许不同，但是不管是Windows还是Linux(包括Mac)，如果是移动某个文件的时候都遵循的是先复制一份到目标目录之后，再把原文件删除的先后顺序。 其实move()命令也是一样的逻辑， 基于这个逻辑，move实际上也可以用于修改文件夹或文件的名称。 1shutil.move('./data/data.txt', './data/data_copy.txt') 然后我们去看一下： 可以看到，执行成功了。我们多注意一下就会发现，move命令也有和copy2相同的特性，将原文件的信息保留了下来。 zipfile压缩模块 ZIP文件格式基本是互联网上最通用的一种压缩格式，常见的存档和压缩标准。该模块提供了用于创建，读取，写入，附加和列出ZIP文件的工具。 在日常使用中，我们也会经常用到这个模块的相关功能。和之前介绍方法不同，我们这一部分按需求来介绍： 压缩文件 123456import zipfile, oswith zipfile.ZipFile('temp.zip', 'w') as myzip: myzip.write('./data/data_copy.txt') myzip.write('./data/black.txt') myzip.write('./data/user.txt') 有没有发现，其方法和我们在对文件进行读写操作的时候很像，逻辑就是先创建一个压缩包文件，然后往里面扔入对应的文件。 解压缩文件 压缩之后，我们这次解压缩来看看压缩包内的文件是不是我们刚才扔进去的内容： 12with zipfile.ZipFile('temp.zip', 'r') as myzip: myzip.extractall('./data2') 执行之后结果： 文件的确都是原来的文件。 批量压缩 不过之前压缩文件的时候也太麻烦了，文件一个一个的列出来扔进压缩包，那有没有办法将指定文件夹中的文件全部打包呢？ 当然没问题，既然我们嫌弃手动一个个添加文件名太麻烦，那我们直接用机器添加不就好了，怎么做呢？ 首先第一步当然是获取文件夹下所有的文件，应该还记得listdir()这个方法吧？才学的。 获取列表之后，我们直接用代码一个个的扔到压缩包内就可以了，用for循环吧： 123456789with zipfile.ZipFile('temp.zip','w', zipfile.ZIP_DEFLATED) as myzip: # 获取当前目录中的所有的项 arr = os.listdir('./') print(arr) for i in arr: myzip.write(i) ---['12.ipynb', '10.ipynb', '.DS_Store', '15_json.html', '14.ipynb', '8.ipynb', 'temp.zip', '14.py', '11.ipynb', 'globals.ipynb', '13.ipynb', 'Kalman.ipynb', '9.ipynb', '15.ipynb', '7.ipynb', '5.ipynb', '1.ipynb', '3.ipynb', '4.ipynb', '6.ipynb', 'data', '2.ipynb'] 通过将打印出来的arr我们可以看到所有被成功扔进压缩包的内容。 其他压缩方法 zipfile是Python中内置的专门用于压缩zip格式压缩包的方法，但是其实，我们并不限于压缩成zip格式。那是不是还有rarfile, 7zfile等模块呢？那真是想多了，我们刚才学过的shutil方法，就可以进行压缩操作。不过不一样的是，虽然效果是一样的，但是我们这个方法是创建归档: shutil.make_archive()， 用于创建一个压缩文档。这个方法中有三个比较重要的参数，一个是创建的归档文件名称，第二个是指定的归档格式，第三个则是要归档的文件或文件夹路径。 1shutil.make_archive('temp', 'tar', './data') 成功完成tar格式的归档。 除了以上的这些介绍的内置模块之外，我们平时应用中还会用到许多其他的模块，比如日历模块calendar，时间模块time等等。我上方讲解模块使用的同时，更多的是想向大家传递一个思想就是内置模块基本都很易上手，并且就算一时之间不太明白，可以多看看官方文档。从官方文档上学习是一个很好的习惯。我们可以从官方Python模块索引中去找到自己需要的模块。 好了，那这节课就先到这里了，下一节课中，我们利用日历和时间模块来做一个练习：万年历。大家要提前做预习，去官方文档好好学习一下其相关模块，包括calendar、datetime、time等. 那小伙伴们，让我们下节课练习再见吧。","link":"/System-built-in-modules/"},{"title":"16. 练习：万年历","text":"Hi, 大家好。我是茶桁。 上一节课最后，我让我家去预习一下日历和时间的相关模块，不知道大家有没有去预习。不管如何，这节课，让我们开始做一个练习：万年历。 没有预习的小伙伴也跟着一起，在本次练习完成的时候，相信你会对这些模块有了初步的了解。 好，让我们开始吧。 首先，我们需要来看看calendar.monthrange()这个函数，它属于calendar模块内，返回指定年份和月份的数据，月份的第一天是周几，和月份中的天数。 1234567import calendarres = calendar.monthrange(2023, 6)print(res)---(3, 30) 我们接收了返回值，但是这个3和30分别是什么意思呢？我们打开日历看一下就明白了： 如图所见，2023年的6月份一共是30天，第一天是周四。这也正是(3, 30)的含义。之所以是3而不是4，是因为是从0开始计算的，也就是说，周一是0。比如，2023年5月的第一天就是周一，我们来看看是不是这么回事： 12345res = calendar.monthrange(2023, 5)print(res)---(0, 31) 那有了这个，我们要做一个当月的日历就简单了，还记得我们之前做过一个星星的矩阵吗？是一样的概念，这是这次直接换成了数字而已， 来，让我们从最基本框架开始（还是以6月份数据来做）： 1234567891011121314151617days = res[1]week = res[0] + 1d = 1while d &lt;= days: # 循环周 for i in range(1, 8): print('{:0&gt;2d}'.format(d), end=&quot; &quot;) d+=1 print()---01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 这样，我们就将天数打印出来了。可是，明眼人一眼就看出了问题，这一月只有30天，怎么得到的35天的？让我们来修复一下这个问题： 1234567891011121314151617181920212223days = res[1]week = res[0] + 1print('一 二 三 四 五 六 日')d = 1while d &lt;= days: # 循环周 for i in range(1, 8): # 判断是否输出 if d &gt; days: print('', end='') else: print('{:0&gt;2d}'.format(d), end=&quot; &quot;) d+=1 print()---一 二 三 四 五 六 日01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 我们在代码中加了一层判断，如果循环中的d大于days了，那我们就直接输出空格，否则才正确输出格式化的数字，那么这样就可以不输出31-35了。 完成了，顺便还打印了一行星期几。可是问题是，没有和实际情况对齐对吧？没事，我们继续来改动。 1234567891011121314151617181920212223days = res[1]week = res[0] + 1print('一 二 三 四 五 六 日')d = 1while d &lt;= days: # 循环周 for i in range(1, 8): # 判断是否输出 if d &gt; days or (d==1 and i&lt;week): print(' ', end='') else: print('{:0&gt;2d}'.format(d), end=&quot; &quot;) d+=1 print()---一 二 三 四 五 六 日 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 我们在之前判断d大于days的判断上再加上一层，不仅如此，当d==1并且i小于week的时候，也都是出制表符，那自然最开始和最末尾不该出现数字的地方都被制表符补齐了。 我们再来多做一次实验，将月份改成7月来看看和实际情况是否相符, 并且，这次我们多加一些内容，将其中的年份和月份也都打印出来： 1234567891011121314151617181920212223242526272829303132year = 2023month = 7res = calendar.monthrange(year, month)days = res[1]week = res[0] + 1print(f'========= {year} 年 {month} 月 =========')print('一 二 三 四 五 六 日')print('='*32)d = 1while d &lt;= days: # 循环周 for i in range(1, 8): # 判断是否输出 if d &gt; days or (d==1 and i&lt;week): print(' ', end='') else: print('{:0&gt;2d}'.format(d), end=&quot; &quot;) d+=1 print()---========= 2023 年 7 月 =========一 二 三 四 五 六 日================================ 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 我们来看看实际情况是不是如此： 没错，确实如此。7月份的第一天从周六开始，一个月有31天，周一为最后一天。那说明，我们上面写的内容真实有效。 那现在要干嘛呢？当然是封装成一个函数，以year和month为参数，这样，不管我想要查询任意月份，只要我输入对应参数就可以了： 123456789101112131415161718192021222324252627282930313233def showdate(year, month): res = calendar.monthrange(year, month) days = res[1] # 当前月份的天数 week = res[0] + 1 # 当前月份第一天是周几 print(f'========= {year} 年 {month} 月 =========') print('一 二 三 四 五 六 日') print('='*32) # 实现日历信息的输出 d = 1 while d &lt;= days: # 循环周 for i in range(1, 8): # 判断是否输出 if d &gt; days or (d==1 and i&lt;week): print(' ', end='') else: print('{:0&gt;2d}'.format(d), end=&quot; &quot;) d+=1 print()showdate(2023, 12)---========= 2023 年 12 月 =========一 二 三 四 五 六 日================================ 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 我们尝试调用了一下封装好的函数，输出2023年12月份日历，大家可以看看自己手机里的日历，绝对真实可靠。 好了，现在我们要完成万年历的制作了。 万年历，自然是有一个初始值，那这个初始值必须是当前时间最妥当。不然你们试试打开你们的日历，看是不是打开默认都是指向的「今天」。 那么首先，让我们获取一下当前系统的年月，这个就需要用到我们的time模块里的localtime()方法，其返回参数如下： 1time.struct_time(tm_year=2023, tm_mon=8, tm_mday=13, tm_hour=1, tm_min=50, tm_sec=38, tm_wday=6, tm_yday=225, tm_isdst=0) 那我们如何从中拿到我需要的内容？我们接着看： 12345678910111213141516import timedd = time.localtime()year = dd.tm_yearmonth = dd.tm_monshowdate(year, month)---========= 2023 年 8 月 =========一 二 三 四 五 六 日================================ 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 很明显，我们用year和month两个变量从得到的localtime里获取了其中的年份和月份信息。然后重新调用showdate()封装函数，将其传入。也就打印出了我们当前月份的日历。 可是这都是静态的，我们总不能就只看我们当月的月份。所以，我们接着扩展这个程序。 123456789101112131415161718192021222324252627282930313233343536import time...while True: # 默认输出当前年月的日历信息 showdate(year, month) print(' &lt; 上一月 下一月 &gt; ') c = input('请输入您的选择 &quot;&lt;&quot; or &quot;&gt;&quot;：') # 判断用户的输入内容 if c == '&lt;': month -= 1 elif c == '&gt;': month += 1 else: print('您输入内容错误，请重新输入&quot;&lt;&quot;或者&quot;&gt;&quot;来选择。')---========= 2023 年 8 月 =========一 二 三 四 五 六 日================================ 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 &lt; 上一月 下一月 &gt; &gt;========= 2023 年 9 月 =========一 二 三 四 五 六 日================================ 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 &lt; 上一月 下一月 &gt; 我们在程序运行中没有图形界面，无法接收鼠标信息，那就用输入&lt;和&gt;来代替一下，其逻辑是相同的。 可以看到，我们做了一个判断，当输入&lt;的时候，我月份数字减少，当我们输入&gt;的时候，月份数字增加。所以当我们输入&gt;的时候，表示下一月，数字增加，也就打印出了9月份的月份信息。 可是问题又来了，我们总不能无限加或者无限减下去吧，12月份之后不可能是13月份吧。这又该怎么办呢？ 别着急，我们继续研究下该怎么改善： 1234567891011121314151617181920import time...while True: ... # 判断用户的输入内容 if c == '&lt;': month -= 1 if month &lt; 1: month = 12 year -= 1 elif c == '&gt;': month += 1 if month &gt; 12: month = 1 year += 1 elif c == 'exit': break else: print('您输入内容错误，请重新输入&quot;&lt;&quot;或者&quot;&gt;&quot;来选择。') 既然月份是固定的数字，那就是最好办的，我们让变量控制在范围内不就好了。如果超过数字了，那就改变年份，将月份回滚为最小值或者最大值不就好了。两个简单的if解决了问题。 这就完了吗？并没有。在打印的过程当中，我发现一个问题，就是我们的月份信息不断的叠加，那导致打印区变的过长，最终都没打印完全。这并不是我们想要的，如图： 所以，其实我都还没验证到底12月份之后是否正常变为2024年1月了。忍不了，这个问题也必须要解决。 那如何解决呢？我想起来，在Linux命令中有一个clear命令，其功能就是将当前窗口内容清理掉。那Python中又有很多和系统操作相同的功能，这次有没有呢？就算没有，我记得os.system()似乎可以调用系统命令的。 那，我们试试看： 12345import oswhile True: os.system('clear') # 默认输出当前年月的日历信息 ... 实际操作了一下，无法在Jupyter Notebook中实现，但是当你将代码存储成.py文件之后，在shell中执行是完全可以实现的。如下图： 至此，我们本次的练习「万年历」就完成了。 大家可以下载我的源码来研究，第16课，包含一个.ipynb笔记本文件和一个.py完整文件。 有什么问题，评论区留言。 好了，下课，咱们下节课再见。","link":"/Exercise-perpetual-calendar/"},{"title":"17. 正则表达式","text":"Hi，大家好。我是茶桁。 不知不觉中，咱们针对人工智能的Python课程已经过去了一半。相信大家这段时间也都有所进步了。 今天这节课呢，我给大家划一个重点。不仅仅是Python，很多语言里都是通用的，而且非常的强大。这就是我们的正则表达式。 说起正则表达式，很多程序员其实对其都不是很重视，但是学好它，必定在处理数据的时候事倍功半。虽然内容看似不多，但是市面上有一本经典的「精通电子表达式」整本书还是非常厚的。当然，它比咱们今天要讲的内容详尽的多了。听完我这节课之后想继续研究正则的小伙伴，推荐这本书（唯一推荐）。 正则表达式是什么呢？其实就是使用字符、转义字符和特殊字符组成一个规则，使用这个规则对文本的内容完成一个搜索或匹配或替换的功能。 正则表达式的组成 正则表达式内，包含了普通字符，转义字符，特殊字符以及匹配模式： 1234普通字符： 大小写字母，数字，符号...转义字符： \\w, \\W, \\d, \\D, \\s, \\S ...特殊字符： . * ? + ^ $ [] {} () 匹配模式： I U ... 接下来我们看几个栗子： 1234567891011# 定义字符串myStr = 'iloveyou521tosimida'# 定义正则myReg = 'love'# 调用正则函数方法res = re.findall(myReg, myStr)print(res)---['love'] 这样，我们就匹配了一个字符串。如果我们想匹配数字，那myReg=521就能够匹配了。不过，现在我有一个新需求，我们重新定义一下myStr = 'iloveyou521to123simida'， 我们可以看到，这个字符串十分的混乱，数字和字母都是混在一起的。现在，我就想把数字都单独的拎出来，又该怎么做呢？来，让我们试试看： 12345678myStr = &quot;iloveyou521to123simida&quot;myReg = '\\d'# 调用正则表达式相关函数res = re.findall(myReg, myStr)print(res)---['5', '2', '1', '1', '2', '3'] 这样，我们使用\\d这个转义字符匹配到了字符串内相关的所有数字，返回了一个列表。 可是这还是不符合我们的要求，我们想要的是将其中的数字组合匹配出来，而不是单独的数字。接着继续改： 12345678myStr = &quot;iloveyou521to123si7894mida&quot;myReg = '\\d\\d\\d'# 调用正则表达式相关函数res = re.findall(myReg, myStr)print(res)---['521', '123', '789'] 将原本一个\\d改为了三个叠在一起的\\d\\d\\d， 这样，我们匹配到了三位数字的组合。注意，我在原本的字符串内又加入了一个四位的数字组合7894，但是也只匹配出了789，那也就是说，这种数字匹配方式，有几个转义字符组合在一起，那就匹配出多少位; \\d这个转义字符就是代表单个的数字。 整个代码中，findall就是正则中的相关函数，除了findall之外，还有一些其他函数，我们一起来认识下： re模块的函数 match与search match和search经常是被放在一起来进行讨论的，因为这两个函数很像。具体它们有什么作用和区别呢？我们直接上代码，一点点讲： 123456print(re.match('www', 'www.hivan.me').span()) # 起始位置匹配print(re.match('me', 'www.hivan.me')) # 不在起始位置匹配---(0, 3)None 可以看到，虽然第二段打印中，我输入的me也可以从字符串中找到，但是因为不是从起始位置匹配的，所以返回了None。 再来看看search： 123456print(re.search('www', 'www.hivan.me').span()) # 起始位置匹配print(re.search('me', 'www.hivan.me').span()) # 不在起始位置匹配---(0, 3)(10, 12) search方法中，无论我们要匹配的字符是在起始位置还是结束位置，只要是能找到，都会返回其位置。 有小伙伴们可能会奇怪，我在成功返回的末尾都加了一个span(), 是不是这个影响的原因？我们来看看这两个方法匹配成功后的返回值就明白了： 1234print(re.search('me', 'www.hivan.me')) # 不在起始位置匹配---&lt;re.Match object; span=(10, 12), match='me'&gt; 其完整的返回值应该是这样的，我后面加的span()只是为了获取返回值中的span信息。 所以对于这两个函数，我们可以稍微归纳一下： re.match()函数： 会从头开始进行匹配，如果第一个就符合要求，那么匹配成功 如果第一个不符合规则，则返回None 匹配成功后返回Match对象 成功后可以使用group()和span()方法获取数据和下标区间 re.search()函数： 从字符串的开头开始进行搜索式的匹配 匹配成功则返回Match对象，匹配失败者返回None 成功后可以使用group()和span()方法获取数据和下标区间 两者的区别： match方法是从字符串的开头进行匹配，如果开始就不符合正则的要求，则匹配失败，返回None。 search方法是从字符串的开始位置一直搜索到字符串的最后，如果在整个字符串中都没有匹配到，则失败，返回None 在看完match和search之后，我们再来看看re模块的其他函数： re.findall() 这个函数在文章开头我们就用到了，但是并未给大家进行详解。现在我们就来认识一下： 123456myStr = &quot;iloveyou521to123si7894lovemida&quot;myReg = 'love'print(re.findall(myReg, myStr))---['love', 'love'] 可以看到，和一开头我们所写的不同，这次返回的参数列表内出现了两个love， 原因就是我对myStr这个变量又重新定义了一下，在接近尾部的地方多加了一个love。 从这我们也能看出来了，findall这个函数是按照正则表达式的规则在字符中匹配所有符合规则的元素，结果返回一个列表，如果没有找到的情况下，会返回一个空列表。 re.finditer() 12345res = re.finditer(myReg, myStr)print(res)---&lt;callable_iterator object at 0x107ab3790&gt; 从返回的结果中看到，这个函数返回的是一个迭代器。那让我们利用迭代器规则来试试看： 12345678print(next(res))print(next(res))print(next(res))---&lt;re.Match object; span=(1, 5), match='love'&gt;&lt;re.Match object; span=(22, 26), match='love'&gt;StopIteration: 在第三个next方法的时候报错了，那和我们使用findall结果是一致的，返回了两个love。并且，finditer方法会返回每一个匹配值的下标范围。使用span()来获取到这个范围。 re.sub() 这个函数方法和之前介绍的方法有些不太一样了，以上我们所使用的可以说都是搜索、查找。那这个函数就是修改了。其功能是按照正则表达式的规则，在字符串中找到需要被替换的字符串，完成一个替换。主要参数有三个： pattern: 正则表示的规则，匹配需要被替换的字符串 repl: 替换后的字符串 string: 被替换的原始字符串 1234567myStr = &quot;iloveyou521to123si7894lovemida&quot;myReg = 'love'res = re.sub(myReg, 'live', myStr)print(res)---iliveyou521to123si7894livemida 这样，我们对整个myStr就完成了特定字符串的替换，将其中的love全部替换成了live。 re.split() 这个方法会按照指定的正则规则，进行数据切割。 123456myStr = 'hello my name is chaheng'res = re.split(' ', myStr)print(res)---['hello', 'my', 'name', 'is', 'chaheng'] 在这段代码中，我们将原字符串以空格来进行风格，将整个字符串分割成了一个列表。其原理和我们将字符串时讲到的基本一致，这里就不详细讲解这个函数了。 compile() 这个函数可以直接将正则表达式定义为「正则对象」， 使用正则对象直接操作。 我们现在来看一个示例： 假如说，我有下面这样一个列表： 123456arr = [ 'i love 123 you', 'i love 234 you', 'i love 456 you', 'i love 678 you'] 我现在想要从中找到所有的数字，那么使用之前所学的内容，当然我们想到的一定是for循环。 12345678910for i in arr: newReg = '\\d\\d\\d' res = re.search(newReg, i) print(res.group()) ---123234456678 可以看到，我们确实正确的拿到了arr中的相关数字。 这里，我们稍微讲解一下正则表达式中的\\d{n}， 在正则中，\\d是转义字符我们之前学过了，但是其实，我们并不需要写三个\\d来去匹配三个数字，那如果真是这样的话，我们要匹配几十个数字的时候怎么办呢？这个时候我们可以用到\\d{n}这样的写法，大括号中的n表示的就是前面这个\\d的匹配连续匹配n次。那么，我们原本的newReg='\\d\\d\\d'就可以改为newRge='\\d{3}'。 让我们回过头来继续，刚才我们使用了for循环来完成了依次取值对吧。这个时候，让我们深入search()这个方法的源码去看看，其中是长这样的： 12def search(pattern, string, flags=0): return _compile(pattern, flags).search(string) 那么根据这个return的结构来看，我们是不是可以这样来写： 12345678910myReg = re.compile('\\d{3}')for i in arr: res = myReg.search(i).group() print(res) ---123234456678 确实，我们获得了我们想要的结果。 那上下两种写法的区别在于哪里呢？ 其实，我们第一种写法里，我们定义了一个正则规则，然后传给search(myReg, i)之后，search方法在其内部先是调用了一下_compile, 生成了一个正则对象，在这之后，才又传给了search方法，最后得到结果。 而我们在第二种写法中，直接用compile函数将规则定义成了一个对象，使用search直接得到了结果。 你们注意看结构，是不是我第二个写法里，for循环里的myReg实际上就是search方法内的_compile(pattern, flags)。 那我们这样写有什么意义呢？呃，实际上，从性能上来说虽然是快了一些，但是也不见得快多少。更多的是想让大家养成一个去方法源码中探究逻辑的好习惯。 那么接下来，才是这节课的重点。大家集中注意力，我们开始。 正则表达式的规则 在本文的最开头，我们就先给到了正则表达式的基本规则，我们拿下来再复习一下： 1234普通字符： 大小写字母，数字，符号...转义字符： \\w, \\W, \\d, \\D, \\s, \\S ...特殊字符： . * ? + ^ $ [] {} () 匹配模式： I U ... 那其实，我们之前介绍相关函数方法的时候，所使用到的基本都是普通字符，其中也用到了\\d这个转义字符，明白\\d就是去匹配数字。 普通字符 普通字符实际上就是最简单的匹配方式，你写什么就是什么。可以理解为，我在全文中去搜索一个单词或者数字。而且我们之前也使用过多次了，所以这部分我们就不再继续向西介绍了。 转义字符 转义字符包括：\\w, \\W, \\d, \\D, \\s, \\S ... 什么都是从代码里去理解最直接，让我们先来定义一个字符串待用： 1myStr = 'a2$_ilove5 21you' 然后我们一个一个来看： \\w, 这个转义字符匹配的内容是单个字母、数字、下划线 123456myReg = '\\w'res = re.findall(myReg, myStr)print(res)---['a', '2', '_', 'i', 'l', 'o', 'v', 'e', '5', '2', '1', 'y', 'o', 'u'] 我们看，在最后的结果中，我们匹配到了这个字符串中所有的字母，数字和下划线。其中的特殊字符和制表符都被过滤掉了。 \\W， 注意，现在这个W是大写的。那这个转义字符规则会去匹配单个「非」字母、数字，下划线。啥意思？简单，看代码： 1234567# 转义字符myReg = '\\W'res = re.findall(myReg, myStr)print(res)---['$', '\\t'] 看到区别了吧？和\\w(小写)完全相反，之前匹配到的字母、数字、下划线一个没匹配到，而之前没被匹配到的特殊字符和制表符，则被匹配后组成了一个列表。 \\d，这个转义字符规则会匹配单个的数字。这个我们之前用过了，这里就不演示了。 \\D， 这个转义字符实际也非常简单，就是匹配非数字。注意到了吧？所有的转义字符里，小写字母大写之后，其匹配的内容都是相反的。 123456myReg = '\\D'res = re.findall(myReg, myStr)print(res)---['a', '$', '_', 'i', 'l', 'o', 'v', 'e', '\\t', 'y', 'o', 'u'] 结果也是，除了数字其他的内容都被匹配到了。 \\s， 这个转义字符规则是匹配单个的空格符或制表符。 123456myReg = '\\s'res = re.findall(myReg, myStr)print(res)---['\\t'] 结果却是如此，唯一的制表符被匹配了出来。（初学时，我一度长期混乱的认为\\s是匹配所有字符串，大家别犯我一样的错误。） \\S，那这个大写字母的匹配规则不用说，一定是匹配空格或制表符之外的所有内容： 123456myReg = '\\S'res = re.findall(myReg, myStr)print(res)---['a', '2', '$', '_', 'i', 'l', 'o', 'v', 'e', '5', '2', '1', 'y', 'o', 'u'] 打印结果验证了我们的猜想。 \\w{4}\\d， 基本转义字符我们都介绍完了，这里我们来看看组合在一起会是什么样。其中的\\w{4}大家也应该明白其含义，就是\\w\\w\\w\\w。 123456myReg = '\\w{4}\\d'res = re.findall(myReg, myStr)print(res)---['love5'] 可以看到，这样组合之后匹配出来的就是4个字母、数字、下划线+一个数字。那是不是这次匹配我们也可以写成\\w{5}呢？反正最后匹配出来的love5不也就是5个w的组合么？ 那我们试试看就知道了： 123456myReg = '\\w{5}'res = re.findall(myReg, myStr)print(res)---['_ilov', '21you'] 没想道结果是这样的对吧？这是因为，\\w{5}中，最后一位可以是字母，数字或者下划线都可以，而我们用\\w{4}\\d则是最后一位必须是数字，不能是其他的。 特殊字符 特殊字符包括：. * ? + ^ $ [] {} () 这回，让我们再重新定义一段合适的字符串 1myStr = '@ _2i4l22oveyou' 然后，我们还是一个一个的来看： .，表示匹配单个的任意字符，当然也有例外，就是除了换行符。 123456myReg = '.'res = re.findall(myReg, myStr)print(res)---['@', '\\t', '_', '2', 'i', '4', 'l', '2', '2', 'o', 'v', 'e', 'y', 'o', 'u'] 字符串内的所有内容，我们都匹配了出来，然后组成了一个列表。 *, 这个特殊字符需要和其他的匹配进行组合使用，它表示的是任意次数。具体什么意思呢？我们还是从代码里去看看是如何表现的： 1234567myStr = 'like chaheng @ _2i4l22oveyou'myReg = '\\w'res = re.search(myReg, myStr)print(res.group())---l 这段代码我们应该是比较熟悉了，\\w和search方法我们都已经学过了。那这个其实就是在字符串中从开头去匹配\\w对吧？ 我们再来继续往下看： 1234567myStr = 'like chaheng @ _2i4l22oveyou'myReg = '\\w*'res = re.search(myReg, myStr)print(res.group())---like 我们看结果，这次search在匹配到第一个\\w之后，又继续向后匹配了，直到遇到空格才停了下来。就是因为\\w后面加了一个*, 所以就会一直匹配任意次数，直到\\w不再匹配了才结束。 这里有一个概念，就是*代表的匹配任意次数，为什么我要强调这个呢？就是任意次数其实是包含0次的。也就是说，我们只要使用了*这个特殊字符，那么就算没有符合匹配项，一样是酸是匹配成功了，只是返回的是空而已。我们来看看： 1234567myStr = ' like chaheng @ _2i4l22oveyou'myReg = '\\w*'res = re.match(myReg, myStr)print(res)---&lt;re.Match object; span=(0, 0), match=''&gt; 我们在之前的字符串前面加了一个空格，按道理说是不符合\\w匹配的。那么match从字符串的开头开始进行搜索式的匹配,没有就返回None对吧？可是这次，并没有返回None，我们看到match=''， 也就是说，它匹配成功了，只是成功了0次。所以按照*匹配任意次的规则，它不会返回None。这里比较绕，大家好好理解一下。我们继续。 +， 这又是一个和其他规则配合使用的特殊字符，和*一样，它也表示匹配次数，但是这个表示的是至少要求匹配一次。正好，我们之前改造的字符串最前面多加了一个空格，让我们来看看： 123456myReg = '\\w+'res = re.search(myReg, myStr)print(res.group())---like 按search从字符串的开头开始进行搜索式的匹配，是不是这里我们应该返回l了？然而并没有，却返回了like，这是为什么呢？ 原因就在于+这个特殊字符，它也并不是只拿到第一个就罢休了，和\\w*一样，一直往后匹配，直到第二个空格的时候，才宣告罢休。所以\\+是「至少」匹配一次，0次不干，而1之后如果可以连续，那就继续匹配。 ?, 这个特殊字符的作用是「拒绝贪婪」，看着很特殊是吧？其实就是，在?之前的规则，只要达成即可。 123456myReg = '\\w+?'res = re.search(myReg, myStr)print(res.group())---l 看，我们之前使用\\w+的时候，+这孩子遇到第一个还不满足，非要向后继续拿。可是这个时候我多加了一个家长?，勒令+既然目的已经达到了，就不要再继续了。这就是?字符「拒绝贪婪」的作用。 {}， 这个特殊字符咱们用过了，应该大家也都知道它的含义。就是重复多少次。 123456myReg = '\\w{5}'res = re.search(myReg, myStr)print(res.group())---chahe 从结果我们可以看到，因为前面的like只有四位，并不符合连续匹配五次的标准，所以最后被匹配出来的是chahe。 {}这个特殊字符其实还有一种用法，就是可以给定范围。 123456myReg = '\\w{1,4}'res = re.search(myReg, myStr)print(res.group())---like 我们知道，下标1到下标4这个范围内，正好是like。 ‘[]’， 这个特殊字符代表字符的范围。用于匹配范围内说包含的字符。使用[]我们可以更精准的筛选出我们需要匹配的内容。 12345678910111213141516171819myStr = 'like chaheng @ ShO _2i4l22LoveYou'myReg = '[A-Z]'res = re.findall(myReg, myStr)print(res)myReg = '[a-z]'res = re.findall(myReg, myStr)print(res)myReg = '[0-9]'res = re.findall(myReg, myStr)print(res)myReg = '[_]'res = re.findall(myReg, myStr)print(res)---['S', 'O', 'L', 'Y']['l', 'i', 'k', 'e', 'c', 'h', 'a', 'h', 'e', 'n', 'g', 'h', 'i', 'l', 'o', 'v', 'e', 'o', 'u']['2', '4', '2', '2']['_'] 可以看到，不同的组合匹配出了不同的范围内的单个字符。 [A-Za-z0-9_] 这个组合等价于\\w。 1234567myStr = 'like chaheng @ ShO _2i4l22LoveYou'myReg = '[A-Za-z0-9_]'res = re.findall(myReg, myStr)print(res)---['l', 'i', 'k', 'e', 'c', 'h', 'a', 'h', 'e', 'n', 'g', 'S', 'h', 'O', '_', '2', 'i', '4', 'l', '2', '2', 'L', 'o', 'v', 'e', 'Y', 'o', 'u'] (),这个特殊字符代表的子组，括号中的表达式首先作为整个正则的一部分，另外会把符合小阔中的内容单独提取一份。我们先看一段代码： 1234567myStr = 'like chaheng @ ShO _2ial2345LoveYou'myReg = '\\w+\\d{4}\\w+'res = re.findall(myReg, myStr)print(res)---['_2ial2345LoveYou'] 这个组合我们匹配到了_2ial2345LoveYou， 有且只有这一个组合了。不过这不是我想要的，我想要的是什么呢？是这段匹配出的字符串，并且，我还想要这段字符串中那段数字作为单独的匹配出来。那这个时候我们怎么做呢？ 1234567myStr = 'like chaheng @ ShO _2ial2345LoveYou'myReg = '(\\w+)(\\d{4})(\\w+)'res = re.findall(myReg, myStr)print(res)---[('_2ial', '2345', 'LoveYou')] 我们将用()将前中后包裹了起来，希望得到一个子组，而中间的部分，就是我们想要得到的4个数字的组合。 ^和$这两个特殊字符实际上属于「定位符」，^是匹配输入字符串开始的位置。$是匹配输入字符串结尾的位置。 那我们从一个案例中来了解一下这两个定位符的使用： 12345678myStr = '186301916675'# 定义一个匹配手机号的正则表达式myReg = '^1\\d{10}$'res = re.search(myReg, myStr)print(res)---None 返回了None， 这又是为什么呢？原因就在于，我们用^限制了必须是1开始，而用了$来限制了到结尾必须是后面有十位数字，而我们数一下，我们给定了12位数字，超出了一位，才会匹配不上。 如果我们去掉限制结尾$再来看看： 12345678myStr = '186301916675'# 定义一个匹配手机号的正则表达式myReg = '^1\\d{10}'res = re.search(myReg, myStr)print(res.group())---18630191667 可以看到，我们遵循了开头位1和后面跟十位数字的规则，但是原字符串中多出来的一位数字被直接过滤掉了。那我们并不知道，用户输入的数字中到底是哪个位置多输入了一个数对吧？ 再来看，我们把结尾限制加上，但是开头限制改一个数字： 12345678myStr = '186301916675'# 定义一个匹配手机号的正则表达式myReg = '^2\\d{10}$'res = re.search(myReg, myStr)print(res)---None 因为开头匹配不对，所以返回了None。 所以，争取的匹配方式至少有三个条件： 1开头， 当然，如果能把运营商的所有开头数字都拿到，那我们能够匹配的条件就变多了。 必须全部是数字 必须是11位 这样，我们可以加上开头限制和结尾限制，正好能满足一个简易的手机号匹配规则： 1'^1\\d{10}$' 正则的模式 在正则的模式中，包含了一下几个模式： re.I: 不区分大小写 re.L: 作本地化识别匹配 re.M: 多行匹配，会影响到^个$ re.S：使用.匹配包括换行在哪的所有字符 re.U：根据Unicode字符集解析字符。这个标志影响\\w, \\W, \\b, \\B re.X: 改标志通过给予更灵活的格式以便将正则表达式写的更易于理解。 实际上，我们虽然列出了这么多模式，真正常用的，也就是re.I这个模式： 1234567891011121314# 正常匹配，给定规则大写字母myStr = 'like chaheng @ ShO _2i4l22LoveYou'myReg = '[A-Z]'res = re.findall(myReg, myStr)print(res)# 使用模式're.I'， 其他不变myReg = '[A-Z]'res = re.findall(myReg, myStr,re.I)print(res)---['S', 'O', 'L', 'Y']['l', 'i', 'k', 'e', 'c', 'h', 'a', 'h', 'e', 'n', 'g', 'S', 'h', 'O', 'i', 'l', 'L', 'o', 'v', 'e', 'Y', 'o', 'u'] 可以看到，当我设定了不区分大小写的情况下。字符串中的所有英文字母都被匹配了出来。 练习： 这次，我们和以往不同，将练习和课程放在了一起。至于为什么嘛，只是因为我课程写完之后发现还有时间。^_^ 这次我们作这样一个练习： 1231. 定义一个正则表达式来验证邮箱是否正确。2. 完善手机号码的正则表达式3. 定义一个匹配IP的正则表达式 [255.255.255.254] 验证邮箱 首先让我们来看，邮箱的格式基本包含以下内容： 123456@qq.com 纯数字 chaheng@qq.com 纯字母 chaheng75@126.com 数字加字母 cha_heng@163.com 混合型 chaheng@vip.163.com 多级域名 chaheng@hivan.me 企业邮箱（企业域名） cha.heng@gmail.com 包含特殊字符. 好，让我们来看，我们以@来前后区分，那么我们先看左边，会包含的内容就是数字，字母，下划线，特殊字符，让我们先来写一下规则试一下： [a-zA-Z0-9]+([_\\.][a-zA-Z0-9])* 那么右边的部分呢？ @(\\w)+\\.[a-z]{2,6} 让我们结合其实试试看： 123456789101112def mailReg(mail): myReg = '[a-zA-Z0-9]+([_\\.][a-zA-Z0-9]+)*@(\\w)+\\.[a-z]{2,6}' res = re.search(myReg, mail) if res: return res.group() else: print(res) mailReg('123456@qq.com')---'123456@qq.com' 感觉是OK。那让我们多测试下试试： 12345678910mailReg('cha.heng@gmail.com')---'cha.heng@gmail.com'=========================mailReg('chaheng@vip.163.com')---None 多级域名的测试没有通过，在函数内打印出了None。我们回过头来看看： 1myReg = '[a-zA-Z0-9]+([_\\.][a-zA-Z0-9]+)*@(\\w)+\\.[a-z]{2,6}' 既然是多级域名出现了问题，那问题肯定出现在多出的那一个点上，我们这样改： 1myReg = '^[a-zA-Z0-9]+([_\\.][a-zA-Z0-9]+)*@(\\w+\\.)+[a-z]{2,6}$' 注意，我们加上了^和$符号。 然后我们再重新试试看，这次呢，我不想一个个实验了，让我们来定义一个数组来批量测试： 12345678910111213141516171819202122232425262728293031def mailReg(mail): myReg = '^[a-zA-Z0-9]+([_\\.][a-zA-Z0-9]+)*@(\\w+\\.)+[a-z]{2,6}$' for i in mail: res = re.search(myReg, i) if res: print(res.group()) else: print(res)emailarr = [ '123456@qq.com', 'chaheng@qq.com', 'chaheng75@126.com', 'cha_heng@163.com', 'chaheng@vip.163.com', 'chaheng@hivan.me', 'cha.heng@gmail.com', ' list@gmail.com ']mailReg(emailarr)---123456@qq.comchaheng@qq.comchaheng75@126.comcha_heng@163.comchaheng@vip.163.comchaheng@hivan.mecha.heng@gmail.comNone 这一下子我们就将刚才想到的格式都测试完了，最后我们还特意加了一个不符合的格式来测试下康康是否被过滤了出来。结果说明我们写的正则没有问题。 手机号码 就像我们上面写验证手机号正则提到过的，我们可以定义一个所有运营商可能的开头来做头部验证： 113[0-9],14[5-9],15[0-3,5-9],16[2,5,6,7,8],17[0-8],18[0-9],19[0-3,5-9] 实际上，我们还可以分的更细一点，区分运营商。不过在这个练习中，没必要分的这么细了。 来，我们实现一下： 12345678910111213141516171819202122232425myReg = '^(13[0-9]|14[5-9]|15[0-3,5-9]|16[2,5,6,7,8]|17[0-8]|18[0-9]|19[0-3,5-9])\\d{8}$'phonearr = [ '13728739429', '13128319520', '17729231234', '23210023421', '189232198341', '19123214421']for i in phonearr: res = re.search(myReg, i) if not res: print('手机号码不正确:', i) else: print(res.group()) ---137287394291312831952017729231234手机号码不正确: 23210023421手机号码不正确: 18923219834119123214421 正确的辨认并打印了出来，不正确的也辨认了出来。 匹配IP地址 我们来看看，一个正确的IP地址（IPv4），是由四个三位数来组成的，包含： 10.0.0.0 ~ 255.255.255.255 既然是这样一个格式，我们来思考一下， 首先，我们需要匹配0 ~ 199的范围，也就是0或者1开头,这个比较简单： 1[0-1]?\\d{1,2} 这一段匹配中： [0-1]?表示匹配0或1一次或者零次 \\d就是要匹配任意单个数字。0～9都可以 {1,2}， 给定范围1 ～ 2， 表示前面的\\d出现1次或者2次。 然后我们需要来匹配200 ~ 255范围, 这个范围内比较复杂，包含了两种情况，一种是201 ~250的情况，一种是251 ~ 255的情况 12((5[0-5])|([0-4][0-9])) 这一段匹配，一开表示数字2开头 然后2之后用异或限定了开头可以是5或者可以是0-4之间的任意数字。 是5的话，后面需要匹配0-5，是4的话，后面需要匹配0-9`。 既然一次的匹配已经有了，那剩下的和第一次也都一样，就好些了，我们再多加一个.的匹配： 1((2((5[0-5])|([0-4]\\d)))|([0-1]?\\d{1,2}))(\\.((2((5[0-5])|([0-4]\\d)))|([0-1]?\\d{1,2}))){3} 让我们来试试： 123456789101112131415161718192021222324252627282930myReg = '((2(5[0-5]|[0-4]\\d))|[0-1]?\\d{1,2})(\\.((2(5[0-5]|[0-4]\\d))|[0-1]?\\d{1,2})){3}'ipArr = [ '25.232.123.241', '123.242.211.221', '0.0.123.421', '123.421.4.5', '212.444.523', '0.0.0.0', '214.113.231.256', '54.214.213.265']for i in ipArr: res = re.search(myReg, i) if not res: print('IP 地址不正确:', i) else: print(res.group())---25.232.123.241123.242.211.2210.0.123.42IP 地址不正确: 123.421.4.5IP 地址不正确: 212.444.5230.0.0.0214.113.231.2554.214.213.26 从结果中可以看到，到底还是有漏网之鱼。第三个和最后两个判断都失误了。这是为什么呢？ 似乎最后一位被截断判断了，也就是说，它并没有判断三位数。 哦，我大概猜到了。让我们把头部匹配和结尾匹配加上再试试： 123456789101112131415161718192021222324252627282930myReg = '^((2(5[0-5]|[0-4]\\d))|[0-1]?\\d{1,2})(\\.((2(5[0-5]|[0-4]\\d))|[0-1]?\\d{1,2})){3}$'ipArr = [ '25.232.123.241', '123.242.211.221', '0.0.123.421', '123.421.4.5', '212.444.523', '0.0.0.0', '214.113.231.256', '54.214.213.265']for i in ipArr: res = re.search(myReg, i) if not res: print('IP 地址不正确:', i) else: print(res.group())---25.232.123.241123.242.211.221IP 地址不正确: 0.0.123.421IP 地址不正确: 123.421.4.5IP 地址不正确: 212.444.5230.0.0.0IP 地址不正确: 214.113.231.256IP 地址不正确: 54.214.213.265 这回没问题了。我们的正则匹配算是完成了。 那这节课下来之后，大家要多去理解，多去练习。这节课对于打算玩数据的人正的很重要。 好了，那我们下节课再见吧。","link":"/regular-expression/"},{"title":"18. Python中的模块与包","text":"Hi, 大家好。我是茶桁。 这一段Python之旅怎么样？还算顺利吧？ 之前我们都学习了些什么？有基本常识，流程，函数，不同类型的数据以及一些模块对吧？并且还做了一些练习来巩固所学过的内容。 那么今天，我们接着来学习模块。不过今天要学的模块和以往不太一样了，以前我们学习的都是Python内置的一些模块，而今天呢，我们自己来打包模块。 模块 简单点说，当我们定义一个Python文件，其后缀名为.py的时候，那么这个文件就被称为模块。 模块中通常呢会定义一些相似的类、函数等代码内容，提供给别的程序引入使用。那对于应用，之前我们已经用过很多次了对吧？我们曾多次应用系统模块来使用，那这次，我们还是从系统模块开始吧。 系统模块 系统模块实际上就是一个Python的程序脚本，专门提供给我们自己的程序使用。它们是在安装好Python环境时，就已经存在的，需要的时候可以使用 import 导入到程序中使用。比如： 1import os, re, time, json, calendar 自定义模块 那知道了系统模块是什么东西，在理解自定义模块就轻松多了对吧？其实就是我们自己创建一个Python脚本，定义一些类或方法，供别的脚本导入后使用。 由于本节课比较特殊，所以课程源码除了18.ipynb这个笔记本文件之外，还有有一个文件夹，路径为./Python/packages/file，然后内部会有多个.py文件。 比如我们定义一个self.py文件如下： 123456789101112# self.py# 定义类class MyException(): pass# 定义函数def func(): print('我是一个模块中的func函数')# 定义变量myStr = 'iloveyou' 然后让我们在笔记本中引用这个文件（模块）以及其他模块，让我们来看看，还记得我们是怎么引入模块的嘛？来，回忆一下： 123456# 先引入一个系统模块：timeimport timeprint(f'time:{time.time()}')---time:1692005247.144672 我们引入了一个系统模块time，然后执行了一下模块里的time()方法，并把最终结果打印了出来。 既然都已经有例子了，那我们有样学样来试试引入我们自己创建的文件： 1234import self---ModuleNotFoundError: No module named 'self' 报错了，告诉我们并没有self这个模块。这个... 还记得我们刚才说过的文件路径嘛？./Python/packages/file，而我们当前文件18.ipynb是放在Python目录下的，层级关系如下： 123456- .- Python/- ...- 18.ipynb- packages/ | - self.py 也就是说，我们要应用self.py， 需要找对路径才行。那我们将路径加上去： 12# 引入自定义模块import packages.self 这回执行之后是没报错了，应该没问题了。 那下面呢，让我们来操作一下文件内的类、函数之类的试试： 123456# 使用模块中定义的类obj = packages.self.MyException()print(obj)---&lt;packages.self.MyException object at 0x10468cb80&gt; 没毛病，确实获取到了相关类病打印了出来。 可是我们这也太麻烦了，每次使用这个模块不是抖要输入这么长一段吧？packages.self.xxx， 不知道之前的学习中大家有没有注意到一个关键字as，这个我之前课程中都没有特意讲解过，但是在我们引入模块的时候会经常的用到。所以这里顺带讲一下吧，比如，我们在操作文件的时候有如下代码： 12with open('./file', 'w') as fp: fp.read() 那这个as我们能猜到是什么作用吗？其实，就是讲as前的内容放入as后面的这个变量里，然后将as身后的这个变量改为一个对象而已，在这段代码里，我们打开了文件，并且将其放入了fp这个变量里，变成了一个fp对象。也可以理解为，我们将as之前的内容起了一个别名。 那么我们导入文件的时候可以这么操作吗？我们试试看： 123456import packages.self as selfobj = self.MyException()print(obj)---&lt;packages.self.MyException object at 0x1046ce2c0&gt; 嗯，看来我们没搞错，确实可以这么用。 那让我们再来试试文件中的那个函数吧，函数内应该是执行了一段打印方法： 1234self.func()---我是一个模块中的func函数 确实正确执行了。这也太顺利了，趁热打铁，让我们再来获取其中的变量： 1234print(self.myStr)---iloveyou 导入模块其实不是仅可导入模块，还能从一个模块中导入类，方法甚至是变量： 12345from packages.self import funcfrom packages.self import myStr as strfunc()print(str) 应该能看出这一段代码的含义吧？就是from（从）一个模块中import导入一个对象。 模块中的测试代码 在自定义模块中，通常我们只是去定义类或函数，变量等，并不调用。如果在指定模块中，想要写一些测试代码，在当前模块作为主程序使用时执行，而作为模块被别的程序导入时不执行，那么可以把测试代码写到下面的代码块中： 12if __name__ == '__main__': print('这个位置的代码只有当前脚本被直接运行时才会运行。') 那么这个模块再被别的程序调用之后，这段代码中的程序是不会被执行的。因为只有这个模块作为主程序运行时才会运行这段代码。我们来看下面这些操作就明白了： 12345import packages.self as selfself---&lt;module 'packages.self' from '/Users/du/git/AI_Cheats/Python/packages/self.py'&gt; 按道理，我们引入模块之后应该会拿到该模块内的所有方法，可是刚才我们写的打印并没有被执行。现在我们在命令行内直接大概这个.py文件来试试： 能看到，if里面的print被直接执行了，打印出了里面的字符串。 在这整段代码中，__name__是一个特殊的变量，这个变量在当前脚本作为模块被别的程序导入时__name__的值是当前这个模块的名称，也就是说，我在笔记本中导入的时候__name__就是self， 而我们在if条件中的设定，是只有当前脚本被作为主程序直接由Python解析时才会进入判断，也就是__name__这个变量的值为__main__时。 我们来看看是不是如此，我们在self.py中加上一段代码： 12name = __name__print(f'__name__: {name}') 然后我们直接让self.py在Python解释器中运行： 现在让我们在笔记本中重新引入一下模块中的变量name，再打印出来看看： 打印的第一段内容为引入模块的时候，模块内的print(f'__name__: {name}')执行了一次，第二段内容则是在笔记本中输入的方法print(name)。 这样，我们就很直观的看到了__name__在不同位置时存储了不同的值。 我们在写程序的时候要记得，不要想着把所有的方法定义在一个脚本文件内。 包 那什么是包呢？包并不是模块。你可以将包理解为一个文件夹，这个文件夹里面包含了多个Python文件。 包的结构 12345678910'''package/ # 包(文件夹)├── __init__.py # 包中的初始化文件├── a.py # 包中的模块├── b.py└── ps/ # 子包 ├── __init__.py ├── c.py └── d.py''' 包的使用方法 其实，我们在刚才所讲的内容中，已经给大家演示过了包的使用方法，不知道小伙伴们能不能反应过来到底是哪里？不知道也没有关系，让我们从头来好好的盘一下这件事。 我们之前在当前目录下创建了一个文件夹packages， 里面有我们self.py文件。实际上，这就是一个包了。 让我们将这个包搞的复杂一点，按照上面我们写的结构来增加一些文件，然后我们看看现在的目录结构： 我们可以看到，除了我们之前设定的文件之外，还有多出来一个文件夹__pycache__以及文件self.cpython-310.pyc， 这个文件夹和文件是当这个包内的文件存在引入关系的时候，自动生成的缓存文件。大家可以不用管。 下面我们来看具体的包使用方法，我们预先在a, b, c, d这四个文件内都写入了一模一样的代码： 12def funca(): print('a.py') 当然，方法名和打印的内容都和文件同名的。 然后我们回到18.ipynb这个笔记本文件内，开始操作使用： 12345import packages as papa.a.funca---AttributeError: module 'packages' has no attribute 'a' 似乎并不行，我们好像并不能引用包来直接使用。那我们怎么办呢？前面我们介绍过一个引用的方法from ... import，我们在使用包内的模块时，需要这样去引用。 1234567from packages import a, ba.funca()b. funcb()---a.pyb.py 可以看到，这回我们引用成功了。那我们之前也学到了，在引入模块的时候，也可以直接就引用模块内的方法和变量，模块在包内也可以如此使用： 12345from packages.a import funcafunca()---a.py 那既然我们得到了这种方式来导入模块内的内容，同样的，包内层如果还存在一个包，而我们要使用子包里的模块，也是这样的导入方法： 12345from packages.ps import cc.funcc()---c.py 看到了，同样能够正常使用。 那如果再过分点，我们要想导入c.py里的函数可以吗？试试就知道了, 再使用.多链接一层： 12345from packages.ps.d import funcdfuncd()---d.pys 呐，完全没问题。 然后我们再反过来看最开始，其实呢，我们的第一种方法直接引用包不是不可以，这需要用到我们这个包内的__init__.py文件。 __init__.py是一个包内的初始化文件，可以说，没有这个文件，这只是一个文件夹，只有有了这个文件，这才是一个包。在初始化的时候，就把包内的模块导入一次，在__init__.py中写下以下代码： 1import a 然后我们再回到笔记本文件中直接导入包来使用试试： 12345import packagesa.funca()---a.py 这样就可以了。 好了，那如果这个时候我packages这个包里一大堆的模块，我不想一个个的来导入，有什么办法吗？也是有的，我们需要用到__all__这个参数，在__init__.py中将包内所有的模块名做成一个列表，然后赋值给__all__这个变量，那么我们在引入包内的模块的时候，就可以使用`*来代表所有文件： __init__.py文件： 1__all__ = ['a', 'b'] 然后进行引入： 12345from packages import *b.funcb()---b.py 这样，我们就一次性导入了packages这个包里的所有文件。 导入方式的分类 之前我们讲的内容中，把导入的方式都过了一遍。到现在这个位置，我们应该总结一下了。 具体的导入方式，我们可以将其分为两个类别，分别是绝对导入和相对导入。那两者有什么区别呢？ 绝对导入 绝对导入的方式会使用「搜索路径」去查找和导入指定的包或模块，包括以下几种方式： import module 导入模块 import package 导入包 import package.module导入包.模块 from module import func 从模块中导入函数 from package import module从包中导入模块 from package.module import func 从包.模块中导入函数 关于「搜索路径」，我们先简单的理解一下就是，从当前文件夹中去找，如果找不到，就会去Python的安装环境中去寻找。 相对导入 ⚠️ 相对导入智能在非主程序的模块中使用，不需要直接运行的模块文件。比如： from .包名/模块名 import 模块/内容 from ..包名/模块名 import 模块/内容 .和..我们之前已经了解过了，.代表的就是当前这一级，..代表的就是上一级。 举个栗子好理解：假设我们现在去修改一下ps/c.py这个文件，在这个模块中如果需要当前包中的d模块: 1from .d import funcd 注意啊，我们这个时候不要在c.py中直接运行funcd()方法，这样会导致报错： 1ImportError: attempted relative import with no known parent package 那我们需要怎么运行呢？我们需要讲c.py导入到其他文件中再执行。比如我们进入到笔记本18.ipynb中导入执行。 12345import packages.ps.c as cc.funcd()---d.py 然后让我们再在c.py中加上一段内容： 1from ..a import funca 小伙伴们应该都看出来了，我是在引用c.py的上一级的a.py。 让我们再在笔记本中执行一下试试： 1234567import packages.ps.c as cc.funcd()c.funca()---d.pya.py 这样，在我们引入了模块c之后，我们同时也拥有了c.py引入的同级和上一级中的d.py、a.py。 搜索路径 刚才我们简单提到了一下「搜索路径」， 这里我们详细的来展开说一下。 「搜索路径」就是在导入模块或者包的时候，程序查找的路径。主要的搜索路径包含以下三部分： 当前导入模块的程序所在的文件 python的扩展目录中 python解释器指定的其它 第三方模块位置 /lib/sitepackages 当然，如果你像我一样，系统中安装了多个Python版本，并且使用虚拟环境。那么你的「搜索路径」就不一定是在哪里了。那么我们到底该如何查找呢？我们来看一下以下查找方法： 123456# 在当前脚本中查看包或者模块的搜索路径import sysprint(sys.path)---['/Users/du/git/AI_Cheats/Python', '/Users/du/miniforge3/envs/glm/lib/python310.zip', '/Users/du/miniforge3/envs/glm/lib/python3.10', '/Users/du/miniforge3/envs/glm/lib/python3.10/lib-dynload', '', '/Users/du/miniforge3/envs/glm/lib/python3.10/site-packages'] 我们看到找到的搜索路径被以列表的形式呈现出来。当然，我们找到搜索路径后，其实是可以向其中添加一个的。 1sys.path.append('/Users/du/AI/GPT') 单入口程序 那什么是单入口程序呢？顾名思义，这种程序就只有一个入口。那单入口程序就是指整个程序都是经过一个主程序文件在运行，其它程序都封装成了包或模块。 单入口文件是作为程序直接被运行的唯一文件，其他都是作为模块或者包，被导入单入口中去执行。打个比方说，我们要去做一个ATM机的程序，我们来实现一个单入口程序。那么可能的情况如下： 1234567891011ATM/|-- main.py # 当前程序的主入口文件，单入口文件,唯一直接运行的文件|-- package/ # 主要程序模块包|-- |--- __init__.py # 包的初始化文件|-- |--- View.py # 视图函数模块|-- |--- Controller.py# 控制器模块|-- |--- Card.py # 银行卡模块|-- |--- User.py # 用户模块|-- databases/ # 数据存储文件夹|-- |-- user.txt|-- |-- user_id_card.txt 那么这个程序中，main就是程序的主入口文件，会被直接作为主程序运行。所以main.py文件必须使用「绝对导入」的方式。 好，那讲到这里，我们今天的内容也就结束了。不知道小伙伴们理解了多少？ 本节课也不太好放练习，那我们这节课就免了。下去之后，大家去拉取我的源码好好的研究一下引入关系，然后讲包、模块的概念好好的理解透。 那小伙伴们，咱们下节课再见。","link":"/Modules-and-packages/"},{"title":"19. 第三方库的管理和虚拟环境","text":"Hi， 大家好。我是茶桁。 在我们之前的课程中，讲解了数据，函数，类，模块以及包。这些基本上已经构成了Python的全部了。 那么，我们在学习Python的包之后，有没有思考过，既然Python有内置模块，我们也可以自己写一些模块来使用，那一定有很多第三方写过相应的模块来供我们使用。那么，这些包该如何去找，找到以后如何使用和管理呢？今天，就让我们来看看这个问题。 第三方库的管理 现在很多编程语言都有第三方库的提供，比如Ruby, Node等。而Python的生态也是发展的最好的之一。Python中比较牛逼的地方就是由大量的第三方库提供给你使用。生态的蓬勃发展也是Python广为流行的最大的原因之一。 Python的第三方库的管理网站：https://pypi.org/。 如何安装第三方库？ pip就是Python得包管理工具，解决了包直接的依赖关系，可以方便的管理第三方库（包）。类似于PHP中的Composer, 或者Nodejs中的npm, 又或者Mac中的Homebrew。 我们可以使用pip install 包名（库名）来进行安装。而如果是有多个Python环境的情况下，可能需要使用pip3。比如说，我们要安装pymysql这个库： 1pip install pymysql 在安装命令过程中，有的时候我们可能对版本会有一定的要求，并不是越新的版本越好。这个时候，我们也可以安装指定版本的包 1pip install 包名==版本 现在包都已经安装到本地了，可是因为安装的内容太多，我们可能有的时候会忘记自己以前是否安装过这个包。为了避免重复再装一次，我们可以搜索一下看看： 1pip show 包名 这样，这个包的所有信息就会打印出来供我们查看： 当然，我们也有类似于想要查看本地安装的所有包的需求： 1pip list 这样，我们就可以把本地说安装的包名以及版本都列出来进行查看： 这些呢，就是我们在包管理经常用到的一些命令。 等等，大家在执行安装的时候，一定会遇到安装特别缓慢的情况。多数时候可能是因为我们所在的环境因为各种原因连接不上官方的源服务器。 不过别着急，我们可以切换到镜像源上，找一个速度快的来下载安装。 目前国内的安装源有以下几个可供选择： 阿里云 http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 豆瓣(douban) http://pypi.douban.com/simple/ 清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/ 中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/ 修改源的方式也很简单，包括了「临时修改」和「永久修改」两种。 临时修改，顾名思义，就是我们有的时候临时需要切换到其他源上进行下载了。 我们可以使用pip的时候在后面加上-i参数，指定pip源： 1pip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple/ 多数情况下，我还是习惯于将源永久修改为一个速度较快的镜像上，也懒得每次都输入长传的命令。那么永久修改的方式稍微复杂点，我们分系统来看： 在Linux中，我们找到pip.conf这个文件，一般来说，它的位置应该是~/.pip/pip.conf，如果没有的话，那就创建一个，然后写入如下内容： 1234[global]timeout = 6000index-url = https://pypi.tuna.tsinghua.edu.cn/simpletrusted-host = pypi.tuna.tsinghua.edu.cn 在Windows内原理也是一样的，不同的是路径和文件有所不同。 我们在user目录中创建一个pip目录，如：C:\\Users\\du\\pip, 然后在PIP目录下新建一个pip.ini文件，然后写入内容： 1234[global]timeout = 6000index-url = https://pypi.tuna.tsinghua.edu.cn/simpletrusted-host = pypi.tuna.tsinghua.edu.cn 这样，我们在之后安装第三方库的时候就会发现，速度快多了。 虚拟环境 虚拟环境呢，就是在当前的系统环境中，去配置另外一个Python得运行环境。我们理论上是可以创建多个不同的虚拟环境的，Python得虚拟环境相互独立，互相之间不会影响。 那么虚拟环境下呢，具有以下一些特点： 虚拟环境中可以在没有权限的情况下安装新的库（Linux系统中可能会出现的问题） 不同的应用可以使用不同的库或不同的版本。 虚拟环境中的库升级也不影响其它环境 虚拟环境可以作为一个项目的专有环境。在需要部署时，一键导出项目的所需要的包 默认方式 Python本身就支持创建和管理虚拟环境。可以使用以下方式进行创建： 1python -m venv 虚拟环境名 创建完成后，我们可以使用下面的方式进入虚拟环境，激活虚拟环境 Linux 12# 使用source命令去执行v1/bin/目录下的activatelocalhost:code yc$ source v1/bin/activate Windows 12345# Windows系统需要进入v1/Scripts/这个目录cd v1/Scripts/# 运行activate.bat文件activate.bat(v1) F:\\code 在这之后，我们现在就处于某一个虚拟环境中了，可以执行安装等操作： 123pip install pymysqlpip show pymysql 那么如果我的某一个环境被我给搞乱了，我也找不到原因，还不如重新安装来的快。这会儿我们可能就想要退出并删除这个虚拟环境： 如果想要退出虚拟环境，在Linux中，我们可以输入下面这个命令： 1234# 退出虚拟环境# Linuxdeactivate 那如果是Windows中呢就比较简单了，直接Ctrl+C就好了。 在退出之后，我们直接删除虚拟环境的文件夹，就搞定了。 以上是我们不想要当前虚拟环境的情况下，那也有一种可能是我们需要更换电脑等原因，但是我想带着我的环境一起迁移，这该怎么办？ 这也好办，我们可以到处当前环境所有安装过的包： 1234567891011# 查看所有安装的包pip list```package version------------------------Numpy 1.3.1........# 导出所有包到文件pip freeze &gt; ./requirements.txt 然后在下一个环境中，我们直接执行安装文件内所有包就可以了： 1pip install -r requirements.txt Conda 虽然Python中已经有了包管理的方法，但是还是不得不说，有时候第三方提供的方案确实香。 目前，我现在都是使用conda(Andaconda)来管理我本地的虚拟环境。其使用也是非常的简单了，我们可以去其官网上（https://www.anaconda.com/）下载并安装对应自己系统的版本。 安装完成后，Conda就会创建一个默认的base环境，我们之前一直使用的Jupyter Notebook也一并是安装在环境中的。 那么在Conda中，我们经常会用的命令如下： 1234567891011121314151617# 安装包conda install 包名称# 安全方式安装包conda install -c conda-forge 包名称# 创建环境conda create --name 环境名 python=3.10 #最后是输入当前环境要用的Python版本# 切换（激活）环境conda activate 环境名# 查看环境列表conda info --env# 删除某个环境conda remove --name 环境名 --all 对比下来，conda真的是非常的方便。推荐大家使用。 那么，今天的课程就到这里结束了。我知道，今天的课程似乎显的特别的少。em....不是似乎，是确实。 原因在于这一部分必须拿出一个章节来介绍一下，否则大家平时在使用的过程中万一将自己的默认环境搞乱了，但是又不知道该怎么办，确实挺烦人的。所以我专门拿一节出来，将环境的问题好好的跟大家介绍下，顺便也是告诉大家，不管你做什么实验和操作，尽量新建一个环境来操作。这样，即便你把当前环境搞的乱七八糟无法恢复，删了就好了。 好，那我们这节课到这里也就结束了，咱们下节课讲讲如何处理异常。","link":"/Third-party-libraries-and-virtual-environments/"},{"title":"20. 异常处理","text":"Hi，大家好。我是茶桁。 在我们日常使用Python或者其他编程语言的时候，不可避免的都会出现报错和异常。那么，我们今天就来谈谈异常。 什么是异常？ 异常异常，根据名字简单理解，那就是非正常，也就是没有达到预期目标。 异常呢，其实就是一个事件，并且这个异常事件在程序的运行过程中出现，会影响程序的正常执行。而一般来说，异常被分为两种： 语法错误导致的异常 逻辑错误导致的异常 比如： 12345varlist = [1, 2, 3]print(varlist[3])---IndexError: list index out of range 这个时候，系统抛出了异常，提示我们列表索引超出范畴。 这里我们需要知道，「异常」在Python中实际上也是一个对象，表示一个错误。当我们的程序无法继续正常进行时，就会被抛出。 我们来完整的看看这个报错信息： 1234567---------------------------------------------------------------------------IndexError Traceback (most recent call last)Cell In[2], line 2 1 varlist = [1, 2, 3]----&gt; 2 print(varlist[3])IndexError: list index out of range Python在遇到异常之后，首先会给出一个「错误回溯」， 然后给出具体哪一句代码出现了问题。 然后在最后给出异常分类和解释。那么IndexError告知我们，这是一个「索引错误」，并且给出了具体的描述「列出索引超出范围」。其中IndexError是我们的异常类， list index out of range是我们的异常信息。 在程序运行过程中，会出现各种各样的异常类，常见标准异常类，我放在最下面作为一个附录。 如何处理异常 可预知 如果错误发生的情况是我们可以预知的，那么就可以使用流程控制进行预防处理。比如，两个数字的运算，其中一个不是数字，运算就会出错，这个时候就可以判断来预防： 123456789n2 = '3'if isinstance(n2, int): res = 10+n2 print(res)else: print('非整型。')---非整形 在这一段代码中，我们使用isinstance方法来检测第一个参数是否是第二个参数的所属类型。这是一个用来检测的方法，返回True或者False。那我们在if中，只有真才会打印结果，假则会打印另外一则消息。 有些小伙伴会想，那既然知道不是整型就会出错，那前面限制传如整型不就好了，干嘛还要费劲去做非整判断。 你要知道，很多时候一个程序的编写和维护并不是单一一个人来做的，即便是一个人在做，也不能完全保证自己某个地方埋下了隐患。那么在每一段代码中，我们对可能预知的情况做妥善的预防是必须的。 不可预知 那可预知的情况我们避免了，可是在我们编写代码的时候，更多的情况是我们自己都不知道我们到底埋了什么雷，哪一段没有遵循规则或者逻辑。那这种情况就是不可预知的。 对于这种不可预知的情况我们该怎么办呢？我们又没办法预先判断。那这种情况下，我们可以使用try...except...语句，在错误发生时进行处理。相关语法如下： 123456try: 可能发生异常错误的代码except: 如果发生异常这进入except代码块进行处理异常被捕获之后程序继续向下执行 我们来看个示例，比如我们之前做过的一个注册、登录练习。其中我们有一段代码是要去读取列表中的所有用户。之前我们的练习中，有提到过文件不存在的情况，所以我们使用了a+的方法，当文件不存在的时候，就新建。 那么现在，我们假设我们就用了r的方法，当文件不存在的时候，一定会报错对吧？这个时候，我们可以使用两种方式来进行处理。 第一种方式，就可以在读取前先判断当前文件是否存在。 第二种方式，就可以使用try...except...在错误发生的时候进行处理。 那么这里，我们用第二种方式来做一下处理： 12345678910111213# 假设我们读取的文件不存在，会发生错误try: with open('./data/user5.txt', 'r') as fp: res = fp.read() print(res)except: print('文件不存在。')print('程序继续运行...')---文件不存在。程序继续运行... 可以看到，我们准确的捕获了错误，并且之后程序仍然继续往后执行了。 ⚠️ try...except... 是在错误发生后进行处理，并不是提前判断。也就是说，错误其实还是发生了。这和if实际上有根本性的区别。 try...except... 详解 首先，我们认识try...except的一个特性，就是它可以处理指定的异常，如果引发了非指定的异常，则无法处理。比如，我们下面人为制造一个异常： 12345s1 = 'hello'int(s1)---ValueError: invalid literal for int() with base 10: 'hello' 可以看到，我们这一段代码引发了一个ValueError异常。 现在我们来捕获一下, 但是这次，我们为这个异常指定一个异常类再来看看，先看看正常状态下： 12345678try: s1 = 'hello' int(s1)except: print('程序错误。') ---程序错误。 接着我们来看指定异常之后： 12345678try: s1 = 'hello' int(s1)except IndexError as e: print('程序错误。') ---ValueError: invalid literal for int() with base 10: 'hello' 这里我们指定了一个IndexError的异常类，显然我们之前看到了，程序报错是ValueError异常类，两者并不匹配。所以最后依然还是报错。 那么之前我们谈到过标准的异常类，并且也知道异常实际上也就是一个对象。而我们平时在使用的时候，except实际上就是去这个「标准的异常类」的列表里去查找，如果没有对应的异常类，它依然是无法捕获的。不过大部分时候，我们基本不会遇到标准异常类之外的异常。而这种处理指定的异常类的特性，平时也可以被我们使用。 其中一个使用方式，就是进行多分支处理异常类，不同的异常可以走不通的except进行处理： 123456789101112s1 = 'hello'try: s1[5] # IndexErrorexcept IndexError as e: print('这里是IndexError', e)except KeyError as e: print('这里是KeyError', e)except ValueError as e: print('这里是ValueError', e) ---这里是IndexError string index out of range 是不是和if...elif的分支形式很像？ 让我们继续，在我们说指定的异常类中，实际上会有一个万能的通用异常类。那就是Exception。 12345678s1 = 'world'try: int(s1)except Exception as e: print('Exception ===',e) ---Exception === invalid literal for int() with base 10: 'world' 基本上所有的异常，都可以走到这个异常类。在这段代码中，我们之前记得int(s1)是属于一个ValueError， 但是我们使用Exception依然可以获取到这个错误。可是如果这两种异常类同时被指定的情况下会如何？ 12345678910s1 = 'world'try: int(s1)except Exception as e: print('Exception ===',e)except ValueError as e: print('ValueError ===', e) ---Exception === invalid literal for int() with base 10: 'world' 我们看到，就是按照程序的从上至下的顺序在执行。 所以，其实我们可以这样理解，当我们进行多分支异常类+通用异常类的时候，Exception是最后的一个保底。 123456789101112131415s1 = 'hello'try: # int(s1) # ValueError s1[5] # IndexErrorexcept IndexError as e: print('IndexError',e)except KeyError as e: print('KeyError',e)except ValueError as e: print('ValueError',e)except Exception as e: print('Exception',e) ---IndexError string index out of range 除此之外，try...except是支持else的，当try里的代码顺利执行没有捕获到任何错误之后，还可以走到else之中额外执行分支内的代码： 12345678910111213141516s1 = 'hello'try: str(s1) print(s1)except IndexError as e: print('IndexError',e)except ValueError as e: print('ValueError',e)except Exception as e: print('Exception',e)else: print('try代码块中没有引发异常时，执行') ---hellotry代码块中没有引发异常时，执行 我们再来了解一下finally, 这个方法是无论是否引发异常都会执行。通常情况下用于执行一些清理工作： 123456789101112131415161718192021s1 = 'hello'try: int(s1) print('如果前面的代码引发了异常，这个代码块将不在继续执行。。')except IndexError as e: print('IndexError',e)except ValueError as e: print('ValueError',e)except Exception as e: print('Exception',e)else: print('try代码块中没有引发异常时，执行')finally: print('无论是否引发了异常，都会执行这个代码块')print('如果上面的代码有异常并且进行了处理，那么后面的代码将继续执行')---ValueError invalid literal for int() with base 10: 'hello'无论是否引发了异常，都会执行这个代码块如果上面的代码有异常并且进行了处理，那么后面的代码将继续执行 这段代码中，我们引发了一个异常，也被捕获了。但是依然执行了finally内的代码，并且也未影响程序继续往后执行。 在我们平常写代码的过程中还有一种情况，就是我们需要自己制作一个异常信息，然后抛出。这个时候，我们需要用raise， 来主动抛出异常。 1234567try: raise Exception('发生错误')except Exception as e: print('Exception', e) ---Exception 发生错误 除了上述的异常处理之外，其实还有另外一种方式，是直接判断逻辑是否成立，不成立抛出AssertionError错误。就是使用assert进行断言。它在表达式错误的时候，会直接抛出AssertionError错误，如果表达式正确，这什么都不做。 1234assert 2 &gt; 3---AssertionError: 自定义异常处理类 虽然系统已经给到了很多异常处理的方式，而我们在平时开发中也会经常的使用。但是实际上，很多时候我们都需要一些自己的处理要求。比如说，当异常出现的时候，我们要将异常信息写入日志，在日后我们从日志里查看日常信息或者做数据分析，就是我们最常使用的。 那我们接下来看看，如果做一个异常处理的自定义开发: 再最开始，我们需要归纳一下，我们到底要保存怎样一个格式： 123# 日志的基本格式：- 日期时间， 异常的级别- 异常信息：引发的异常类别，异常的信息，文件及行号。 在确定了日志格式后，我们可以进入开发了，首先我们需要导入两个所需的库 123# 先导入所需模块import tracebackimport logging 让我们先来人为创建一个日常，并用try语句来捕获它： 1234 int('aaa') ---ValueError: invalid literal for int() with base 10: 'aaa' 这句代码报了一个ValueError异常类。 1234567try: int('aaa')except: print('在此进行异常的处理') ---在此进行异常的处理 没问题，我们捕获了异常并且正确的进入了except。那么，我们可以通过traceback模块来获取异常信息, 替换一下打印信息我们来查看一下。 123456789101112try: int('aaa')except: # 通过traceback获取异常信息 errormsg = traceback.format_exc() print(errormsg) ---Traceback (most recent call last): File &quot;/var/folders/h4/7cr1cmpn7v5b3x20_9wz8m740000gn/T/ipykernel_39689/2534911191.py&quot;, line 2, in &lt;module&gt; int('aaa')ValueError: invalid literal for int() with base 10: 'aaa' 接下来，就轮到logging模块了。该模块定义了实现用于应用程序和库的灵活事件日志记录系统的函数和类。 12345logging.basicConfig( filename = './data/error.log', # 日志存储的文件及目录 format='%(asctime)s %(levelname)s \\n %(message)s', # 格式化存储的日志格式 datefmt = '%Y-%m-%d %H:%M:%S') 在定义了logging的基本信息之后，我们就可以定义一下将刚才的errormsg写入日志了： 12# 写入日志logging.error(traceback.format_exc()) 那么我们完善一下整个代码就是这样： 12345678logging.basicConfig( filename = './data/error.log', # 日志存储的文件及目录 format='%(asctime)s %(levelname)s \\n %(message)s', # 格式化存储的日志格式 datefmt = '%Y-%m-%d %H:%M:%S')# 写入日志logging.error(traceback.format_exc()) 我们需要在异常出发的时候，将错误写入到日志内。那么需要将这段代码放到except中。可是我们总不能每次都写这么长一段代码，那怎么办呢？嗯，没错，我们需要封装一个函数用于多次调用。 1234567891011121314151617def Myexception(): # logging的基本配置 logging.basicConfig( filename = './data/error.log', # 日志存储的文件及目录 format='%(asctime)s %(levelname)s \\n %(message)s', # 格式化存储的日志格式 datefmt = '%Y-%m-%d %H:%M:%S' ) # 写入日志 logging.error(traceback.format_exc())# 使用自定义异常处理类try: int('bb')except: print('在此处进行异常的处理') Myexception() # 在异常处理的代码块中去调用自定义异常类 然后我们将导入库的方法也写进去，这样在我们需要的时候才会进行导入，顺便，我们将这个函数封装成一个类。就便于更多的文件调用： 123456789101112131415161718192021# 自定义异常日志处理类class Myexception(): def __init__(self): import traceback import logging # logging的基本配置 logging.basicConfig( filename='./error.log',# 日志存储的文件及目录 format='%(asctime)s %(levelname)s \\n %(message)s',# 格式化存储的日志格式 datefmt='%Y-%m-%d %H:%M:%S' ) # 写入日志 logging.error(traceback.format_exc())# 使用自定义异常处理类try: int('bb')except: print('在此处进行异常的处理') Myexception() # 在异常处理的代码块中去调用自定义异常类 这样，一个自定义的获取异常之后写入日常的类就定义好了，我们可以在任意地方导入并调用这个类方法，以便获取以及日后查看整个程序中的异常。 附录 标准的异常类 异常名称 描述 BaseException 所有异常的基类 SystemExit 解释器请求退出 KeyboardInterrupt 用户中断执行(通常是输入^C) Exception 常规错误的基类 StopIteration 迭代器没有更多的值 GeneratorExit 生成器(generator)发生异常来通知退出 StandardError 所有的内建标准异常的基类 ArithmeticError 所有数值计算错误的基类 FloatingPointError 浮点计算错误 OverflowError 数值运算超出最大限制 ZeroDivisionError 除(或取模)零 (所有数据类型) AssertionError 断言语句失败 AttributeError 对象没有这个属性 EOFError 没有内建输入,到达EOF 标记 EnvironmentError 操作系统错误的基类 IOError 输入/输出操作失败 OSError 操作系统错误 WindowsError 系统调用失败 ImportError 导入模块/对象失败 LookupError 无效数据查询的基类 IndexError 序列中没有此索引(index) KeyError 映射中没有这个键 MemoryError 内存溢出错误(对于Python 解释器不是致命的) NameError 未声明/初始化对象 (没有属性) UnboundLocalError 访问未初始化的本地变量 ReferenceError 弱引用(Weak reference)试图访问已经垃圾回收了的对象 RuntimeError 一般的运行时错误 NotImplementedError 尚未实现的方法 SyntaxError Python 语法错误 IndentationError 缩进错误 TabError Tab 和空格混用 SystemError 一般的解释器系统错误 TypeError 对类型无效的操作 ValueError 传入无效的参数 UnicodeError Unicode 相关的错误 UnicodeDecodeError Unicode 解码时的错误 UnicodeEncodeError Unicode 编码时错误 UnicodeTranslateError Unicode 转换时错误 Warning 警告的基类 DeprecationWarning 关于被弃用的特征的警告 FutureWarning 关于构造将来语义会有改变的警告 OverflowWarning 旧的关于自动提升为长整型(long)的警告 PendingDeprecationWarning 关于特性将会被废弃的警告 RuntimeWarning 可疑的运行时行为(runtime behavior)的警告 SyntaxWarning 可疑的语法的警告 UserWarning 用户代码生成的警告 那么，这节课到这里也就结束了。各位小伙伴，下去以后记得勤加练习。下课。","link":"/Get-the-exception/"},{"title":"21. 面向对象及特性","text":"Hi，大家好。我是茶桁。 今天开始，我们要迈向Python的另外一个台阶了，那就是面向对象。 面向对象编程（Object Oriented Programming)，简称为OOP，是一种以对象为中心的程序设计思想。 与之相对的，就是面向过程编程（Procedure Oriented Programming), 简称为POP, 是一种以过程为中心的程序设计思想。 面向对象和面向过程 接下来，让我们先了解一下这两个编程思想到底有什么不同。还记得咱们之前讲过宋丹丹老师小品里的经典的「把大象装进冰箱分几步」吗？小品给出的答案是三步对吧？ 第一步：打开冰箱门 第二步：把大象装进去 第三步：关上冰箱门 设计思想的不同 那么这个答案，就是一种面向过程的思维，遇到问题之后，分析解决问题的步骤，然后一步步的去实现。 那么如果是面向对象的话，又该如何去做？ 面向对象是通过分析问题中需要的抽象模型，然后根据需要的功能分别去创建模型对象，最终由模型对象来完成程序。那这个「把大象装进冰箱分几步」的问题我们该如何去考虑呢？ 首先，面向对象要解决这个问题，需要先建立出抽象模型，比如： 打开冰箱门和关闭冰箱门，这都属于一个冰箱的功能， 大象走进去，这就是大象的功能。 到此时我们就出现了两个抽象模型，一个是冰箱，一个是大象。 冰箱具有 打开和关闭的功能，大象具有走路的能力。 分析到这里，就是面向对象的思想，具体完成的话，就是去创建冰箱和大象这两个对象，最终完成这个程序 冰箱对象-开门，大象对象-走进冰箱，冰箱对象-关门 这个问题解决了，我们再来思考一个新的问题：「想吃清蒸鱼怎么办？」 当然是按照做菜的顺序一步一步来对吧？这就是典型的面向过程思维： 买鱼，买料 杀鱼和清理，并且腌制 锅里烧水 把鱼放进去，开始蒸鱼。 十分钟后开盖，把鱼端出来，然后浇汁。 这样，一步一步的完成这个愿望，就是面向过程所作的事情。 轮到面向对象，又该如何呢？ 需要一个对象：大厨。 告诉大厨，我想吃清蒸鱼。 那么大厨呢，有可能是我们自己训练的，也有可能是其他五星酒店挖过来的。不管如何，这是一个已经完善建立好的对象，我们直接拿来用就可以了。面向对象呢，就是这样寻找具体的对象去解决问题。对于我们来说，调用了对象，而对象完成了这个过程。 当然，具体大厨这个对象里肯定还是一步一步的去完成过程，也就是说，最终面向对象中是由面向过程的体现的。但是思维方式，也就是设计思想是完全不同的。 优缺点 既然有不同之处，那必然是有优缺点的。因为有对比嘛。面向过程有其优点，当然，面向对象也有其缺点。 面向过程的核心是过程，过程就是指几觉问题的步骤。其优缺点非常明显： 优点： 将负责的问题流程化，进而实现简答。 缺点： 扩展性差（更新、维护、迭代） 而面向对象的核心是对象，是一个特征和功能的综合体，其优缺点如下： 优点：可扩展性高 缺点：编程复杂度相对面向过程高一些，这里的复杂度指的是计算机在执行面向对象的程序时性能表现一般。 那总结起来呢，在去完成一些简单的程序时，可以使用面向过程去解决。但是如果有复杂的程序或任务，而且需要不断的进行迭代和维护，那么肯定是优先选择面向对象的编程思想 如何学习面向对象编程 那我们后面如何去学习面向对象编程呢？其实就两步： 学习面向对象编程的思想 学习面向对象编程的语法 这两步中，其实难的是第一步，学习面向对象编程的思想。 不管如何，什么事情都需要有个开头，那我们就从类和对象的基本概念开始好了。 认识类与对象 类： 类是对象的一个抽象的概念 对象（实例）：对象就是由类创建的实例 那么这两者的关系其实就是「模具和铸件」之间的关系。 类是由对象总结而来的，总结的这个过程叫做抽象。 对象是由类具体实施出来的，这个过程叫做实例化。 是不是听着有些迷糊了？这里我们还是用实际例子来解释一下的好，我们思考下面的问题： 水果是一个对象还是一个类？ 汽车是一个对象还是一个类？ 手机是一个对象还是一个类？ 我们在说水果的时候，你能想到什么？香蕉、苹果、西瓜、榴莲等等。对吧？那我们想了这么多不一样的东西，是不是这些所有的都称为是「水果」？那么我们将这些内容都叫做水果的过程就称为「归类」的过程。这个「水果」就是一个类，刚才我们总结的这个过程就叫做抽象，我们想到的香蕉、苹果...等等，就是对象。 汽车其实是一个概念，你能想到什么？奔驰、野马、奥迪、别摸我？那我们见过的车，就会在我们脑海中浮现，而这些具体的车总结出来一个类的过程就是「抽象的过程」，我们最后总结出来的「汽车」就是一个类。那些在我们脑海里浮现的具体的汽车，就是对象。 单我们去开车上班的时候，那么我们就是应用一个具体的对象去发生特定的功能。 再来想一个问题，我现在给大家写这个教程，用的是Macbook Pro，那么请问我当前正在使用的这个MBP是对象还是一个类？ MBP的特征：金属外壳，优美的外观。 MBP的功能：给大家写教程，编辑代码，听音乐，作曲，画画.... 当我描述了这么多之后，这个MBP到底是一个类还是一个对象？ 面向对象的基本实现 如果我们需要实例一个对象，那么我们就需要先抽象一个类。 举了栗子： 我们现在需要创建一个汽车，或者千千万万个汽车用于销售。那在这之前我们要做什么？ 首先，我们需要抽象一个汽车类，也就是我们要在一个设计图纸上设计处这个汽车。 然后，我们由这个设计图纸去创建（实例）出来的真实汽车就是一个对象。 那么接下来，就让我们具体到代码里去实现看看。 还记得我们之前介绍的怎么去创建一个类嘛？有没有小伙伴还记得？ 123# 定义一个汽车的类class Cart(): pass 没错，就是使用class关键字来定义一个类。其书写规范如下： 12345'''类名的书写规范，建议使用驼峰命名法 大驼峰：MyCar ChaHeng 小驼峰：myCar chaHeng''' 那么我们在类里需要声明些什么内容呢？一个类需要有「特征」和「功能」两个内容组成： 特征就是一个描述：颜色：黑色， 品牌：野马，排量：2.4...； 特征在编程中就是一个变量，在类中称为属性 功能就是某一项能力： 拉货，代步，上班....； 功能在编程中就是一个函数，在类中称为方法 在类中，属性一般定义在前面，方法定义在后面。 12345678910111213141516# 定义一个汽车的类class Car(): # 属性 =&gt; 特征 =&gt; 变量 color = 'black' # 表示颜色属性 brand = 'mustang' #表示品牌属性 displacement = 2.4 # 表示排量属性 # 方法 =&gt; 功能 =&gt; 函数 def pulling(self): print('小汽车能拉货。') def rode(self): print('小汽车能代步。') def onDuty(self): print('小汽车能上班。') 现在，我们拥有了一个具体的类，里面包含了特征和功能。那么我们如何通过类实例化对象并最终使用它们呢？ 很简单，将其赋值给一个具体的变量就可以了，比如，我们现在去4S店实际购买一个野马汽车： 12# 实例化一个对象buyNewCar = Car() 这样，就简单的实例化了一个购买的新车，让我们查看一下它的类别和各项属性： 123456789101112print(buyNewCar, type(buyNewCar))# 查看对象的品牌print(buyNewCar.brand)# 调用对象的方法buyNewCar.rode()---&lt;__main__.Car object at 0x105e6fbb0&gt; &lt;class '__main__.Car'&gt;mustang小汽车能代步。。 这样，我们就能看到这个对象是由类Car实例化得来的。并且查看到了品牌属性，试用了一下其“代步”这个功能。 成员属性和方法的操作 一个对象通过实例化之后，在类中定义的属性和方法，可以使用实例化的对象进行操作。 类中定义的属性也称为成员属性，类中定义的方法，也称为成员方法。 我们直接拿之前定义的类来实例化两个对象观察一下： 12345678a = Car()b = Car()print(a)print(b)---&lt;__main__.Car object at 0x105d37190&gt;&lt;__main__.Car object at 0x105d36fe0&gt; 我们来看，a,b分别实例化之后，我们将其打印出来。看到两个对象都是通过Car来实例化的，但是后面不同。就是说，这两者在实例化之后，完全就是两个不同的对象。那我们可以这么说，一个类可以实例化处多个对象。 对象的成员操作 在类的外部，使用对象操作成员，比如，我们可以通过对象访问类中的属性： 12345res = a.colorprint(res)---black 还可以通过对象访问类中的方法： 1234a.rode()---小汽车能代步。 那除了访问，我们是否可以对其进行修改呢？来看看： 123456a.color = 'red'res = a.colorprint(res)---red 可以看到，我们修改了对象的属性。那么，这个时候我们另外一个实例化对象b里是什么情况？ 1234print(b.color)---black 依然还是black，并未收到a内属性变化的影响。 也就是说，我们操作单个对象进行属性修改，并不影响最初的类，也不会影响同一个类实例化出来的其他对象。 我们还可以给对象添加本来没有的属性来丰富这个对象： 12345a.name = 'AE86'print(a.name)---AE86 同样的，我们对单个对象进行的操作，一样不会影响原本的类以及其他实例化对象： 1234print(b.name)---AttributeError: 'Car' object has no attribute 'name' 不出所料的报错了，错误类型为属性错误。告知我们并没有name这个属性。 好，再让我们来看看删除这个动作。我们就直接删除a对象刚创建的name: 1234567print(a.name)del a.nameprint(a.name)---AE86AttributeError: 'Car' object has no attribute 'name' 程序显示打印了一次name的值，说明我们能正常获取，然后删除a对象中的这个属性，然后再打印来看，警告我们AttributeError类型错误。说明，这个时候的name已经不存在了。 好，让我们删除a继承下来的属性brand,不过这次为了让后续程序还能正常运行，我们使用try来捕获一下错误。 12345678910try: del a.brandexcept AttributeError as e: print('AttributeError:', e)print('a.brand: ', a.brand)---AttributeError: branda.brand: mustang 可以看到，我们在执行删除a.brand的时候报错了，后面打印的结果也证明了a.brand这个属性还存在，可以被打印出来。 那么问题来了，为什么之前的a.name可以被删除，而a.brand不行？这两个属性到底有什么区别？ 其实，单我们执行删除一个对象的属性时，只能删除当前这个对象自己的属性才可以。而我们执行的操作中，brand并不是a自己的属性，而是属于Car这个类的。因为无法进行删除。 a.name则不一样，是单独在a对象内创建的属性，因此可以删除。 访问成员属性，会先访问对象自己的属性，如果没有，则去访问这个对象的类的属性。 修改对象的属性值时，实际上等于给这个对象创建了一个对象自己的属性。 添加对象的属性，是给对象创建了自己独有的属性。 删除属性，只能删除这个对象自己的属性，包括给对象添加的和修改的。 接着，我们来看看在类的外部，操作对象的方法。 访问对象的方法：实际上如果这个对象没有自己独立的方法，那么会访问这个对象的类的方法。 1234a.rode()---小汽车能代步 我们来进行修改对象的方法：给这个对象的方法重新定义： 12345678def func(): print('这里是重新定义的一个方法')a.rode = funca.rode()---这里是重新定义的一个方法 这样，我们就完成了方法的重新定义。 访问、修改之后，我们能不能给对象添加新的方法呢？ 12345a.func2 = funca.func2()---这里是重新定义的一个方法 看来也是可以的，我们现在给这个对象自己新创建了一个方法。 来，删除一下方法试试： 1del a.func2 并未报错，我们继续执行下试试： 1234a.func2()---AttributeError: 'Car' object has no attribute 'func2' 看报错，说明我们删除成功了。 方法实际上和属性一样，我们可以删除对象自己的方法，但是无法删除对象的类的方法。 至此，我们可以总结如下： 一个类定义类成员属性和成员方法，那么通过这个类实例化的对象，也具备了这些方法和属性。 实际上，创建对象的时候，并不会把类中的属性的属性和方法复制一份给对象，而是在对象中应用父类的方法。因此在访问对象的属性时，会先去找对象自己的属性，如果没有就去找这个类的属性和方法。 一个对象由类创建以后，是一个独立的对象，会应用父类中的属性和方法。如果在对象创建后，给对象的属性或方法，进行修改或添加，那么此时等于给这个对象创建了一个自己的属性和方法。所以在删除时，只能删除对象呗修改或添加的成员。 除了在类的实例化对象中对类的成员进行操作之外，我们还可以直接在类上进行操作。比如，我们可以执行下列操作： 1Car.brand = 'BMW' 那现在提一个问题，在原始类的成员修改之后，这个类创建的实例化对象会如何？ 12345678910print(a.brand) # 先执行一次打印，原始属性Car.brand = 'BMW'b = Car() # 新创建一个实例化对象print(b.brand) # 打印新创建的对象的属性print(a.brand) # 打印修改之前创建的对象的属性---mustangBMWBMW 很明显，我们直接在类上进行操作修改成员之后，不管是hi新创建的实例化对象，还是早已存在的实例化对象，其中的成员属性都被修改了。删除和新加都遵循着这样一个特性。 对成员属性和方法的操作，我们也就可以总结成两种，一是「对象操作成员」， 一种是「类操作成员」。当然，由于类修改后会影响具体的实例化对象，所以并不推荐这么去做。 对象操作成员 1234567891011成员属性： 访问： 对象.成员属性名 修改： 对象.成员属性名法 = 新值。（此时等于给这个对象创建了一个自己的属性） 添加： 对象.新成员属性 = 值 (此时是给这个对象自己新建了一个属性) 删除： del 对象.成员属性 (注意：只能删除这个对象自己的属性) 成员方法： 访问： 对象.成员方法名() 修改： 对象.成员方法名 = func（此时等于给这个对象创建了一个自己的方法） 添加： 对象.方法名 = func (此时是给这个对象自己新建了一个方法) 删除： del 对象.方法名 (注意：只能删除这个对象自己的方法) 类操作成员（不推荐） 1234567891011成员属性： 访问： 类名.成员属性名 修改： 类名.成员属性名法 = 新值。（此时通过这个类创建的对象都具有这个属性） 添加： 类名.新成员属性 = 值 (此时通过这个类创建的对象都具有这个属性) 删除： del 类名.成员属性 (注意：删除这个类的属性后，这个类创建的对象也没有这几个属性了) 成员方法： 访问： 类名.成员方法名() 修改： 类名.成员方法名 = func（此时通过类创建的对象都被修改） 添加： 类名.方法名 = func (此时通过类创建的对象都被修改) 删除： del 类名.方法名 (注意：此时通过类创建的对象都被修改) 最终总结一下如下： 一个类可以实例化出多个对象，每个对象在内存中都独立存在的 当通过类实例化对象时，并不会把类中的成员复制一份给对象，而去给对象了一个引用 访问对象成员的时候，如果对象自己没有这个成员，对象会向实例化它的类去查找 对象成员的添加和修改，都只会影响当前对象自己，不会影响类和其它对象 删除对象的成员时，必须是该对象自己具备的成员才可以，不能删除类中引用的成员 对类的成员操作，会影响通过这个类创建的对象，包括之前创建的。 成员方法中的self self在方法中只是一个形参，并不是关键字。从它本身的意义上来说，是可以用其他的关键字去替换的，但是长久以来的惯例大家都一直在使用self。 其作为英文单词的本意是：自己。那么在类的方法中则代表的是「当前这个对象」。不太明白？让我们来看一个实际的例子： 让我们先定义一个「Person」类，然后实例化一个「张三」： 123456789101112131415161718192021222324# 定义人class Person(): # 成员属性 name = 'name' age = 0 sex = 'sex' # 成员方法 def sing(self): print('会唱歌') def dance(self): print('会跳舞') def rap(self): print('会饶舌')# 实例化对象zs = Person()print(zs.name)---name 成功打印出了name， 说明我们成功实例化了。 通过实例化的对象，我们可以在类的外部去访问成员属性和成员方法。（对象.成员）。 同样的，我们其实也可以在类的内部去访问成员属性和成员方法。让我们做一个实验，来说明一下self到底是什么： 123456789101112131415161718# 定义人class Person(): # 成员属性 ... # 成员方法 ... def func(self): print(self)# 实例化对象zs = Person()# print(zs.name)print(zs)zs.func()---&lt;__main__.Person object at 0x10a048c10&gt;&lt;__main__.Person object at 0x10a048c10&gt; 我们修改了这个类，在内部创建了一个方法func(self)， 然后打印了self这个参数。 然后我们在外面打印了实例化的zs，还通过这个具体的实例化对象执行了类内部的func方法。实际上就是打印了一下此刻的self。可以看到，两个打印结果完全一样，那说明，这两者本身就是一个东西。 self代表调用这个方法的对象，谁调用了这个方法，self就代表的是谁。self就可以在类的内部代替对象进行各种操作。 我们通过self来进行的操作，其实完全就是实例化的对象所作的操作。我们在类中修改func这个方法，让其打印name， 修改name， 调用方法rap来试试看： 123456789101112131415161718192021222324# 定义人class Person(): # 成员属性 ... # 成员方法 ... def func(self): print(self) print(self.name) self.name = '茶桁' print(self.name) self.rap()# 实例化对象zs = Person()zs.name = &quot;张三&quot;zs.func() ---&lt;__main__.Person object at 0x10a06bf40&gt;张三茶桁会饶舌 我们就可以很清晰的看到self代表的含义，谁调用，self就代表谁。也就是说，只要是对象能干的事情，self就可以代表对象去完成，比如成员的添加、删除、更新、访问、调用等等。 我们再来修改一下类里的方法，让其更清晰的显示这个特性： 12345678910111213141516171819202122# 定义人class Person(): # 成员属性 name = 'name' ... # 成员方法 ... def rap(self): print(f'我是{self.name}, 我会饶舌') def func(self): ... self.rap()# 实例化对象zs = Person()zs.name = &quot;张三&quot;zs.func() ---我是张三, 我会饶舌 在类中，我们修改了一下rap方法，让其调用self.name， 在类被定义的时候，这个类中的name是被赋值为name的。然后，我们在func方法中调用了一下self.rap(), 我们对其进行实例化一个对象zs，并且在这个实例中对name进行了重新赋值张三, 接着，调用了实例化对象中的func()方法。 我们清晰的看到，func()调用了self.rap()，然后将张三打印在了屏幕上。充分说明了，这个时候的self代表的就是调用它的zs这个实例化对象。 我们直接调用类中的方法试试看： 1234Person.func()---TypeError: Person.func() missing 1 required positional argument: 'self' 我们收到了报错，被告知缺少必须的位置参数self。 好，那让我们再来做两个实验，第一个实验中，我们测试一下如果在类中的方法没有使用self接受参数会怎样： 123456789101112class Person(): def func(): print('我是一个没有`self`的方法。')Person.func()a = Person()a.func()---我是一个没有`self`的方法。TypeError: Person.func() takes 0 positional arguments but 1 was given 可以看到，我们可以使用类直接调用这个方法有效，但是我们创建一个实例化对象之后，利用实例化对象去调用则会报错。这个是因为，我们在用实例化对象去调用类中的方法的时候会传入一个参数。但是现在类中的func()方法并没有可以接受的参数，那么必定会报错。 第二个实验，我们试试不用self，而是其他的参数是否可以成功： 12345678910class Person(): def func(vars): print(f'我是{vars.name}, 我使用了vars来接受参数。')a = Person()a.name = 'admin'a.func()---我是admin, 我使用了vars来接受参数。 可以看到，完全没有问题。也就是说，用实例化对象调用类中的方法时，是一定会将自己作为一个参数传给这个方法，需要一个具体的参数去接受。而参数的名称是什么则无所谓，只是大家在习惯上都是用self。区别如下： 含有self或者可以接受对象作为参数的方法： 非绑定类方法 不含self或者不能接受对象作为参数的方法：绑定类方法 非绑定类方法，可以使用对象去访问, 绑定类方法，只能通过类去访问。 魔术方法 魔术方法是什么呢？ 魔术方法也和普通方法一样都是类中定义的成员方法。这是一种不需要去手动调用的，在某种情况下，自动触发（自动执行）的方法。魔术方法特殊就特殊在定义的时候，多数的魔术方法 前后都有两个连续的下划线。但是切记，这个方法并不是我们自己定义的，而是系统定义好的，我们来使用而已。 __init__ 初始化方法 这个初始化方法是在通过类实例化对象之后，自动触发的一个方法。 123456789101112131415161718class Person(): name = None age = None sex = None # 初始化方法 def __init__(self): print('我是一个初始化方法。') # 成员方法 def say(self): print('大家好，我是茶桁。')# 实例化对象zs = Person()---我是一个初始化方法。 注意到了么？我们仅仅是实例化的对象而已，并没有进行任何调用，初始化方法就执行了一遍。那么，我们可以得到下面这些内容： __init__ 触发机制： 在通过类实例化对象后，自动触发的一个方法 作用：可以在对象实例化之后完成对象的初始化（属性的复制，方法的调用）。 应用场景：文件的打开，数据的获取。 干活之前，做好一些准备工作。 以下，我们改造一下这个类，然后再实例化的时候多做一些动作： 123456789101112131415161718192021222324class Person(): name = None age = None sex = None # 初始化方法 def __init__(self,name,age, sex): print('我是一个初始化方法。') # 完成对象属性的初始化赋值 self.name = name self.age = age self.sex = sex # 成员方法 def say(self): print('大家好，我是茶桁。')# 实例化对象zs = Person('张三', 41, 'male')print(f'我叫{zs.name}, 我今年{zs.age}岁，性别:{zs.sex}')---我是一个初始化方法。我叫张三, 我今年41岁，性别:male 当然，我们还可以再初始化方法中调用say方法，完成自我介绍： 123456def __init__(self, name, age, sex): ... self.say() def say(self): print(f'打击好，我是{self.name}。') __del__： 析构方法 和初始化方法一样，我们直接来解析一下这个方法的触发机制，作用以及注意点。 触发机制： 析构方法方法会在对象被销毁时自动触发。 作用：关闭一些开发的资源 注意：对象被销毁时触发了析构方法，而不是析构方法销毁了对象。 我们还是从代码里来观察这个方法。 我们来定义一个类，完成一个日志的记录，调用这个对象的时候，传递一个日志信息。这个对象会创建一个文件，开始写入，并在最后关闭这个文件。 12345678910111213141516171819202122232425262728293031import timeclass writeLog(): # 成员属性 # 文件的路径 fileurl = './data' # 日志文件的名称 filename = str(time.strftime('%Y-%m-%d'))+'.log' # 初始化 打开文件 def __init__(self): # 完成文件的打开 print('初始化方法触发类，完成文件的打开') self.fileobj = open(self.fileurl+self.filename, 'a+', encoding='utf-8') # 写日志的方法 def log(self,s): print(f'把日志{s}写入到文件中') # 析构方法 def __del__(self): print('析构方法触发了，关闭打开的文件') # 在对象被销毁时，关闭在初始化方法中打开的文件对象 self.fileobj.close()l = writeLog()l.log('today is good day.')del l---初始化方法触发类，完成文件的打开把日志today is good day.写入到文件中析构方法触发了，关闭打开的文件 这段代码中，我们实例化了writeLog()类，调用了初始化方法。在方法中我们打开了文件，因为我用的是变量创建，所以不一定是什么文件。当前我操作的文件为2023-08-16.log。 然后我们调用l.log()， 也就是实例化对象中的log方法来对该文件写入一段日志内容：today is good day.， 在执行之后，我们又使用了del l来销毁这个实例。在销毁实例的时候，就会调用__del__方法来执行其中的方法。 那么对象会在什么情况下被销毁呢？ 当程序执行完毕，内存中所有的资源都会被销毁释放 使用 del 删除时 对象没有被引用时，会自动销毁 面向对象的三大特性 面向对象有三大特性，分别是「封装、继承、多态」， 那么它们具体都是什么呢？下面让我们分别来解释。 封装 封装，就是使用特殊的语法，对成员属性和成员方法进行包装，达到保护和隐藏的目的。就像我们送礼的时候，会找东西把礼物包起来一样。 但是一定注意，不能把成员全部封装死，就失去意义了。就好比我们买的笔记本电脑，无论如何都会给你留下一些接口的，比如说电源接口，USB接口等等。只有有了这些接口，我们才能插上鼠标啊，移动硬盘等等来进行使用。 被封装的成员主要是供类的内部使用。被特殊语法封装的成员，会有不同的访问的权限。比如笔记本内的硬盘，内存等等，这些并不是不让你使用，而是提供给笔记本本身使用，我们可以操作笔记本电脑来达到间接使用它们的目的。 封装分为了几个不同的级别，一般情况下有三种： 公有的 public 受保护的 protected 私有的 private 被特殊语法封装的成员，会有不同的访问权限。 123456789101112131415161718192021222324252627282930313233343536373839class Person(): # 成员属性 name = None age = None sex = None # 初始化方法 def __init__(self, name, age, sex): self.name = name self.age = age self.sex = sex # 成员方法 def say(self): print('talk about life.') def sing(self): print('sing a song.') def kiss(self): print('come on...')# 实例化对象zs = Person('张三', 49, 'male')# 查看对象的所有成员print(Person.__dict__) # 获取当前类的所有成员信息print(zs.__dict__) # 获取当前对象的所有成员信息# 我们也可以直接访问对象所有的方法print(zs.name)zs.kiss()---{'__module__': '__main__', 'name': None, 'age': None, 'sex': None, '__init__': &lt;function Person.__init__ at 0x111a355a0&gt;, 'say': &lt;function Person.say at 0x111a35750&gt;, 'sing': &lt;function Person.sing at 0x111a357e0&gt;, 'kiss': &lt;function Person.kiss at 0x111a35870&gt;, '__dict__': &lt;attribute '__dict__' of 'Person' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'Person' objects&gt;, '__doc__': None}{'name': '张三', 'age': 49, 'sex': 'male'}张三come on... 在整段代码中，我们实例化对象的时候，基本可以访问Person类中所有的成员。我们说定义的属性和方法，都可以无障碍访问。那么，我们现在说定义的这些成员，就都是Public级别。 现在想象一个场景，我们走在美国街头上，遇到一个美女，然后我们上前询问人家的年龄，大多数时候我们得不到想要的答案。而如果我们上去询问性别（现在知道为什么我要设定为美国街头了吧？），我估计这个就是保密的了吧，有可能一种情况就是当事人在当时的情况下，自己都不知道自己是什么性别。 那这个时候，我们就需要改写一下这段代码了, 改写之前，我们需要理解一下Python中不同级别成员的定义方式，分别为： str =&gt; 公共的 _str =&gt; 受保护的（约定俗成，在Python中没有具体实现） __str =&gt; 私有的。 在了解了定义方法之后，我们可以着手来做实验了： 123456789101112131415161718192021222324252627282930313233class Person(): # 成员属性 name = None _age = None # 这是一个protected 成员属性 __sex = None # 这是一个 private 成员属性 # 初始化方法 def __init__(self, name, age, sex): self.name = name self._age = age self.__sex = sex # 成员方法 def say(self): print('talk about life.') def _sing(self): # 这是一个protected 成员方法 print('sing a song.') def __kiss(self): # 这是一个private 成员方法 print('come on...')# 实例化对象zs = Person('张三', 49, 'male')# 查看对象的所有成员print(Person.__dict__) # 获取当前类的所有成员信息print(zs.__dict__) # 获取当前对象的所有成员信息---{'__module__': '__main__', 'name': None, '_age': None, '_Person__sex': None, '__init__': &lt;function Person.__init__ at 0x111f99630&gt;, 'say': &lt;function Person.say at 0x111f996c0&gt;, 'sing': &lt;function Person.sing at 0x111f99870&gt;, 'kiss': &lt;function Person.kiss at 0x111f99c60&gt;, '__dict__': &lt;attribute '__dict__' of 'Person' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'Person' objects&gt;, '__doc__': None}{'name': '张三', '_age': 49, '_Person__sex': 'male'} 可以看到，我们调用实例化方法得到的结果已经和之前有所不同了。最终拿到的__sex成员属性是属于类的。 现在让我们逐一来调用一下： 123456print(zs._age)print(zs.__sex)---49AttributeError: 'Person' object has no attribute '__sex' 可以看到，_age作为受保护的成员属性可以调用，但是__sex作为私有成员属性则不允许。 实际上，受保护的成员属性也是不能调用的，但是Python中因为没有具体实现，所以唯独在Python中可以调用。 123456zs._sing()zs.__kiss()---sing a song.AttributeError: 'Person' object has no attribute '__kiss' 那么，作为受保护的成员方法_sing被正常调用了，但是室友的成员方法__kiss调用的时候报错。看来和成员属性是一致的。 那么我们现在就可以总结如下： 公有的(Public) 受保护的(Protected) 私有的(Private) 在类的内部 可以访问 可以访问 可以访问 在类的外部 可以访问 不可以访问（Python中可以） 不可以访问 在实现上我们总结如下： 公有的(Public) 受保护的(Protected) 私有的(Private) 定义 默认定义的成员都属于公有成员 在成员名称前面加一个下划线 _成员名称 在成员名称前面加两个下划线 __成员名称 特征 公有的成员可以在任何位置进行访问和操作 受保护的成员和公有成员一样可以在任何位置进行访问，但是一般不要随便访问和操作受保护成员 私有的成员只能在当前类的内部去访问和操作，不能在类的外部进行操作 ⚠️ 这里我们需要注意Python特殊的亮点： 在python中并没有实现受保护的封装，属于开发者的约定俗成。 python中的私有化封装是通过改名策略实现的，并不是真正的私有化 继承 继承是什么？我们是不是经常听到「文化的继承，技艺的继承，衣钵的继承...」等等这些。 那计算机的继承又是什么？ 在面向对象中，一个类去继承父类，那么这个类就拥有了父类中除了私有成员之外的所有成员，包括属性和方法。这个，就叫做继承。 在整个继承过程中，被其他类继承的类就称为「父类」， 也可以称为「基类」或者「超类」。那么继承其他类的类，就被称为「子类」， 也可以称为「派生类」。 那么我们继承又什么意义吗？继承的主要意义，就是为了提高代码的重用性，建立新的类与类的关系，方便其他逻辑的操作。 继承实现起来其实非常方便： 123456# 继承的语法格式class 父类(): passclass 子类(父类): pass 我们直接看代码来理解，比如，我有如下定义： 123456789101112131415161718192021222324# 定义猫科动物class Felidae(): # 属性 coatColor = 'orange' # 毛色 sex = 'M' # 定义性别 # 成员方法 def run(self): print('轻盈的跳跃') def walk(self): print('走的猫步')# 定义猫class Cat(): coatColor = ' white' sex = 'M' # 定义性别 # 成员方法 def run(self): print('轻盈的跳跃') def walk(self): print('走的猫步') 我们看，猫是不是也是属于猫科动物的一种动物？那么在猫科动物中定义的所有成员，其实在猫这边我也会有。不过这样重复定义是不是感觉特别繁琐？其实，我们在Cat中完全不需要再次输入这么多，完全可以这样写： 12345678910111213141516171819# 定义猫科动物class Felidae(): # 属性 coatColor = 'orange' # 毛色 sex = 'M' # 定义性别 # 成员方法 def run(self): print('轻盈的跳跃') def walk(self): print('走的猫步')# 定义猫class Cat(Felidae): passmimi = Cat()mimi.run() 这样，我在定义Cat的时候就完成了对Felidae的继承，然后我们实例化一个Cat,再调用这个实例化对象中的方法run()， 也就输出了原本是属于类Felidae中的run()方法。 我们再继承父类的时候，之类还可以写入自己独有的成员属性或方法。 123456789101112131415161718class Cat(Felidae): size = 'small' def eat(self): print('吃猫粮。') pass mimi = Cat()mimi.run()print(mimi.size)mimi.eat()Felidae.eat()---轻盈的跳跃small吃猫粮。AttributeError: type object 'Felidae' has no attribute 'eat' 我们定义Cat的时候，除了继承Felidae里的成员之外，还定义了一个size成员属性和一个eat成员方法。然后我们在实例化对象中进行调用，都正常运行。 这个时候我们反过来，使用父类Felidae来调用在之类Cat中定义的成员，则会报错。说明这个成员是独属于之类的。 我们不仅可以继承的时候进行扩展，还可以复写父类中的方法，使的它与父类方法产生差异化。其方法是在子类中将父类的方法重新定义一遍就可以了。 那有什么办法在我重写父类方法的时候，仍然可以调用父类方法吗？也是可以的，就是使用super().父类方法名()来进行操作： 12345678910111213141516171819202122232425262728293031# 定义猫科动物class Felidae(): # 属性 coatColor = 'orange' # 毛色 sex = 'M' # 定义性别 # 成员方法 def run(self): print('轻盈的跳跃') def walk(self): print('走的猫步')# 定义猫class Cat(Felidae): size = 'small' def run(self): super().run() print('更加轻盈的跳跃。') def eat(self): print('吃猫粮。') passmimi = Cat()mimi.run()---轻盈的跳跃更加轻盈的跳跃。 我们可以看到，在子类中我们重写了父类中的run方法，但是由于我们在重写的时候在内部使用了super().run()。 所以父类中的方法被完全调用了一遍。 所以，我们目前可以总结继承的特征如下： 在不指定继承的父类时，所有类都继承自object类（系统提供） 了解 子类继承了父类后，就拥有了父类中的所有成员包括魔术方法（除了私有成员） 子类继承父类后，并不会把父类的成员复制给子类，而去引用 子类继承父类后可以重写父类中的方法，叫做 重写 子类重写父类的方法，依然可以使用super().父类方法名()的方式调用父类的方法 子类中如果定义了父类中不存在的方法，称为对父类的扩展 一个父类可以被多个子类继承，还可以存在 链式继承 。 链式继承：A类继承了B类，B类继承了C类，C类继承了D类。。。 单继承和多继承 一个类只能继承一个父类的方式，就叫做单继承。如果一个类继承了多个父类的方式，就称为多继承。直接看例子， 123456789101112131415class Person(): print('人的样子。')class Chusheng(): print('畜生的特性。')class Japanese(Person, Chusheng): passc = Japanese()c---人的样子。畜生的特性。 像代码中定义的Japanese类，同时继承了Person和Chusheng， 那这个，就属于多继承。我们来区分一下语法特征： 12345678910111213141516# 单继承class 父类(): passclass 子类(父类): pass # 多继承class 父(): pass class 母(): passclass 子(父，母): pass 在多继承的关系里，有一个有意思的部分，我们来看看： 12345678910111213141516class Tiger(): def eat(self): print('大口撕咬食物...')class Cat(): def eat(self): print('小口吞咽食物...')class C(Tiger, Cat): def eat(self): super().eat() print('到底该怎么吃？')# 实例化对象c = C()c.eat() 我们现在看到这段代码是一个多继承关系，我在C这个类中继承了Tiger和Cat两个类，并且复写了eat()这个方法。按道理来说，我们实例化C类之后，打印的结果一定是复写的结果。但是我们在C类的eat方法里还调用了super().eat()， 我们知道super()是调用一遍父类的方法。那么这里到底是调用Tiger里的eat方法，还是Cat里的eat方法呢？ 让我们看打印结果： 123---大口撕咬食物...到底该怎么吃？ 打印结果有没有出乎你的意料？那么这个原因是什么呢？其实也不复杂，就是因为Tiger的调用在前面，Cat在后面。让我们重新改一下看看： 1234567891011121314151617181920class Tiger(): def eat(self): print('大口撕咬食物...')class Cat(): def eat(self): print('小口吞咽食物...')class C(Cat,Tiger): def eat(self): super().eat() print('到底该怎么吃？')# 实例化对象c = C()c.eat()---小口吞咽食物...到底该怎么吃？ 这就证实了，谁在前面就调用谁的方法。 菱形继承（钻石继承） 先来看一个图形： 123 AB C D 那我们先有一个A类，下面有B和C类，再下面还有一个D类。 看图可能还是不太明白，它们之间的关系是这样的：B和C继承了A类，然后D又多继承了B和C。 那么这种继承关系就叫做菱形继承。 那么我们现在面临的一个问题就是：在这种菱形继承关系中，类与类是什么关系？super()调用时的顺序是怎样的？ 12345678910111213141516171819202122232425262728293031323334353637383940414243# 菱形继承# 祖先class A(): num = 111 def eat(self): print('学着凭借本能寻找食物...')# 父亲class B(A): num = 222 def eat(self): super().eat() print(super().num) print('进化了，学会大口吃肉。。。')# 母亲class C(A): num = 333 def eat(self): super().eat() print(super().num) print('进化的另外一个分支，小口吞咽...')# 子class D(B, C): num = 444 def eat(self): super().eat() print(super().num) print('居然退化了，又忘了怎么吃...')d = D()d.eat()---学着凭借本能寻找食物...111进化的另外一个分支，小口吞咽...333进化了，学会大口吃肉。。。222居然退化了，又忘了怎么吃... 那么我们来看一下，究竟是怎样的一个顺序： 1D.super() =&gt; B.super() =&gt;C.super() =&gt; A.print() -&gt; C.print() -&gt; B.print() -&gt; D.print() 上边这一段中，=&gt;是继承关系，-&gt;是执行顺序。 好，那我们这个时候要清楚一个点是，我们使用的d这个实例化去执行的，那么在这所有的继承类中，self全部都是c这个实例化对象。让我们来看看到底是不是： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 菱形继承# 祖先class A(): num = 111 def eat(self): print(self.num) print(self) print('学着凭借本能寻找食物...')# 父亲class B(A): num = 222 def eat(self): print(self.num) print(self) super().eat() print(super().num) print('进化了，学会大口吃肉。。。')# 母亲class C(A): num = 333 def eat(self): print(self.num) print(self) super().eat() print(super().num) print('进化的另外一个分支，小口吞咽...')# 子class D(B, C): num = 444 def eat(self): super().eat() print(super().num) print('居然退化了，又忘了怎么吃...')d = D()d.eat()---444&lt;__main__.D object at 0x111b71540&gt;444&lt;__main__.D object at 0x111b71540&gt;444&lt;__main__.D object at 0x111b71540&gt;学着凭借本能寻找食物...111进化的另外一个分支，小口吞咽...333进化了，学会大口吃肉。。。222居然退化了，又忘了怎么吃... 打印的结果证实了我们刚才的说法。 这个地方可能比较让人意外的是之前那个继承关系上，明明我B继承的是A， 怎么变成C了？我们来看看原因： 1234567891011121314'''在定义类之后，程序会自动生成一个继承的列表MRO(Method Realtion Order)方法关系列表MRO列表生成原则：1. 子类永远在父类的前面2. 同一等级的类，按照之类中的继承顺序摆放3. 先之类，后父类的顺序原则，最终的类是系统提供的obejct类MRO的调用方法类名.mro()'''D.mro()---[__main__.D, __main__.B, __main__.C, __main__.A, object] super在调用时，并不是查找父类，而是去MRO列表上找上一个类。 super方法在调用时，会自动把当前self传入到上一级的类的方法中。 所以我们之前会呈现出D=&gt;B=&gt;C=&gt;A的顺序。 看着有点晕是吧？别着急，我们接下来介绍一个方法，能很方便的看到类关系。 issubclass()类关系检测 这个方法是检测一个类是否是另一个类的之类的方法。用起来也非常简单： 1234567891011121314res = issubclass(D, B)print(res)res = issubclass(D, C)print(res)res = issubclass(D, A)print(res)res = issubclass(A, D)print(res)---TrueTrueTrueFalse 多态 对于同一个方法，由于调用的对象不同，产生了不同形态的结果。这个就叫做多态。 比如说，我们现在的电脑上有一个USB接口，那么这个接口在接入不同的设备的时候，产生的结果也是不一样的。插入鼠标，我们可以点击。插入键盘我们可以输入，插入U盘呢，我们可以读取。对吧？对于这个USB接口来说。就属于多态。 好的，让我们来实现一下，直接看代码： 12345678910111213141516171819202122232425262728293031323334353637# 定义电脑类class Computer(): # 在电脑类中定义一个 sub 的规范的接口 方法 def usb(self,obj): obj.start()# 定义鼠标类class Mouse(): def start(self): print('鼠标启动成功，可以双击单击嗨起来。。。')# 定义键盘类class KeyBord(): def start(self): print('键盘启动成功了，赶紧输入666。。。')# 定义 U盘 类class Udisk(): def start(self): print('U盘启动了，赶紧检查一下我的种子还在不在。。。')# 实例化对象c = Computer() # 电脑对象m = Mouse() # 鼠标对象k = KeyBord() # 键盘对象u = Udisk() # u盘对象# 把不同的设备插入到电脑的usb的接口中c.usb(m)c.usb(k)c.usb(u)---鼠标启动成功，可以双击单击嗨起来。。。键盘启动成功了，赶紧输入666。。。U盘启动了，赶紧检查一下我的种子还在不在。。。 这样，我们就实现了一个多态的程序。 我们在实例化Computer()之后，利用实例化对象c调用类中的方法usb， 将实例化对象传入，并且还传入了不同的obj， 这里的obj是我们之前实例化过的m, k, u。 那这样，我们obj代表了不同的实例化对象，那也就会启动不同的类方法。 那这样呢，属于一个普通的方式来实现，其实对于这段程序，我们还可以使用继承关系来完成。 我们先定义一个接口规范类，其他类都继承这个类，并实现（重写）父类中的方法。由于每个对象实现父类的方式或者过程都不相同，最后的结果是不一样的形态。 1234567891011121314151617181920212223242526272829303132333435363738394041# 继承关系写多态# 定义USBclass USB(): ''' info: 这个类是一个接口规范类，需要子类继承并实现start方法 start方法不做任何具体功能的实现 ''' # 在usb类中定义一个规范的接口方法，但是不实现任何功能 def start(self): pass# 定义鼠标类class Mouse(USB): def start(self): print('鼠标启动成功，可以双击单击嗨起来。。。')# 定义键盘类class KeyBord(USB): def start(self): print('键盘启动成功了，赶紧输入666。。。')# 定义 U盘 类class Udisk(USB): def start(self): print('U盘启动了，赶紧检查一下我的种子还在不在。。。')# 实例化对象m = Mouse()k = KeyBord()u = Udisk()m.start()k.start()u.start()---鼠标启动成功，可以双击单击嗨起来。。。键盘启动成功了，赶紧输入666。。。U盘启动了，赶紧检查一下我的种子还在不在。。。 我们回来看这段代码，实际上，如果抛开USB类，我们单独去写后面的类，并且把继承关系去掉。最后是不是也可以进行打印？可以... 可是这样的话，那这三个方法中的satrt方法之间就毫无关系，继承了USB中的start方法，也就是继承了规范。 而且这个继承的形式，和我们之前实现的普通版本其实并无什么差别，虽然代码实现上有不同，可是逻辑上是完全相同的。 好了，关于面向对象，我们就先介绍到这里。不过别着急，并不是讲完了，我们下节课还要接着讲「面向对象」。讲解一些高级语法和思想。小伙伴们记得关注。 另外，面向对象这个东西，确实蛮难的，并不是看我这一两节课就能学懂的。虽然我尽力，但是我还是有自知之明。 在这里给大家推荐一本好书，有它在，你想不懂都难。 ^_^ 领取优惠券再购买：","link":"/Object-Oriented-Programming/"},{"title":"23. 描述符和设计模式","text":"Hi， 大家好，我是茶桁。 上一节课中，我们讲解了面向对象中的一些高阶应用，给大家介绍了一些魔术方法。并在最后，我们预告这节课内容会讲解描述符和设计模式。 好了，让我们开始吧。 描述符 这个玩意，怎么讲合适呢？这么说吧，当某一个类中，包含了三个魔术方法（__get__, __set__, __delete__)中的任意一个，或者全部都有时，那么这个类就被称为是「描述符类」。 描述符的作用就是对一个类中的某一个成员进行一个详细的惯例操作，包括获取、赋值以及删除。也就是，描述符代理了一个类中的成员的操作，描述符属于类，只能定义为类的属性。 魔术方法咱们前面有提到，这里让咱们先来看看三个特殊的魔术方法，： __get__(self, instance, owner) 触发机制：在访问对象成员属性时自动触发(当该成员已经交给描述符管理时) 作用：设置当前属性获取的值 参数：1. self 描述符对象 2.被管理成员的类的对象。3.被管理成员的类 返回值：返回值作为成员属性获取的值 注意事项：无 __set__(self, instance, value) 触发机制：在设置对象成员属性时自动触发(当该成员已经交给描述符管理时) 作用：对成员的赋值进行管理 参数：1. self 描述符对象 2.被管理成员的类的对象。3.要设置的值 返回值：无 注意事项：无 __delete__(self, instance) 触发机制：在删除对象成员属性时自动触发(当该成员已经交给描述符管理时) 作用：对成员属性的删除进行管理 参数：1. self 描述符对象 2.被管理成员的类的对象。 返回值：无 注意事项：无 让我们先来看一个基本的类和实例化： 123456789class Person(): name = 'name'# 实例化对象zs = Person()print(zs.name)---name 然后我们定义一个「描述符类」 123456789101112# 定义描述符类class PersonName(): __name = 'abc' def __get__(self, instance, owner): pass def __set__(self, instance, value): pass def __delete__(self, instance): pass 接着我们重新更改一下刚才定义的普通类， 将其中的name成员属性交给刚定义的描述符类来实现： 1234# 定义的普通类class Person(): # 把类中的一个成员属性交给一个描述符类来实现 name = PersonName() 这个时候我们实例化之后打印其中的成员属性会如何？ 123456# 实例化对象zs = Person()print(zs.name)---None 我们可以看到，结果为None。 现在让我们依次将类中的__get__方法的参数都打印出来观察一下： 1234567891011121314# 修改其中的`__get__`方法def __get__(self, instance, owner): print(self) print(instance) print(owner) # 实例化后打印print(zs.name)---&lt;__main__.PersonName object at 0x108931d20&gt;&lt;__main__.Person object at 0x108930250&gt;&lt;class '__main__.Person'&gt;None 现在，具体self, instance, owner各自分别是什么，就非常清楚了。 那，既然self是PersonName类本身，那我们在其中定义的name成员属性是不是就可以拿到了？ 123456789101112131415161718192021222324# 定义描述符类class PersonName(): __name = 'abc' def __get__(self, instance, owner): return self.__name def __set__(self, instance, value): pass def __delete__(self, instance): pass# 定义的普通类class Person(): # 把类中的一个成员属性交给一个描述符类来实现 name = PersonName()# 实例化对象zs = Person()print(zs.name)---abc 没错，我们确实拿到了PersonName中的__name。 我们现在可以这么理解，普通类中的一个成员属性交给了一个描述符类来实现，类中的成员的值是另一个描述符类的对象， 那么当对这个类中的成员进行操作时，可以理解为就是对另一个对象的操作。现在的PersonName这个描述符类，相对于一个代理人的角色。把当前的描述符类赋值给了一个需要代理的类中的成员属性。 既然我们看到了get方法的结果之后，那么剩下两个魔术方法的作用也就很容易想到了： 123456789101112131415161718# 定义描述符类class PersonName(): ... def __set__(self, instance, value): self.__name = value ...# 定义的普通类class Person(): ... # 实例化对象...zs.name = '张三丰'print(zs.name)---abc张三丰 这里容易理解吧？当我们执行zs.name = ‘张三丰’这个赋值操作的时候，其就是走到了__set__方法内。其中的self不言而喻，就是PersonName， 而value就是刚才我们进行赋值操作的那个值。这个时候，我们可以设置self.__name = value，那就是满足了这个赋值操作。当然，我们也可以不这样给，来让我们调戏一下这个赋值。 12345678910# 只改动`__set__`def __set__(self, instance, value): # self.__name = value self.__name = '茶桁'zs.name = '张三丰'print(zs.name)---茶桁 当我们这样去改的时候，那么无论我们怎样去赋值，最终的结果都是打印出茶桁。 那么__del__怎么用呢？我们接着看： 12345678# 前面的代码都不做改动del zs.nameprint(zs.name)---茶桁茶桁 那么第一个茶桁是刚才我们赋值后的打印结果，第二个茶桁呢？就是我们在执行del zs.name之后的打印结果。按道理来说，我们执行了del命令之后。zs这个对象的name成员已经被删除了，现在应该是打印出类中的原始值，也就是abc, 那为什么这里打印出来的还是茶桁呢？ 原因就在于我们的__del__方法内没有任何操作。我们来改一下__del__内部： 12345678910# 只改动`__del__`def __delete__(self, instance): # print('我就是不行删除，气死你') del self.__namedel zs.nameprint(zs.name)---abc 这样我们就执行了del本该有的操作。不过大家也看到了，我中间有一段代码注释了，现在让我们替换一下注释： 123456789# 只改动`__del__`def __delete__(self, instance): print('我就是不行删除，气死你') # del self.__namedel zs.name---我就是不行删除，气死你 当我们执行del zs.name的时候，触发方法内的打印命令。那么，这个时候再让我们打印一下zs.name来看看： 1234print(zs.name)---茶桁 毫无意外的，茶桁还在，并没有变成abc。 需要注意的是，同时具备三个魔术方法的类才是「数据描述符类」，没有同时具备三个魔术方法的类呢？很简单，就是「非数据描述符类」。两者的区别就是一个是完整的，一个是不完整的。可以不可以应用呢？部分可以，但是不完整，__get__, __set__, __delete__中总有某些功能无法实现。 一个描述符应用 了解了描述符的概念以及怎么使用之后，我们来试试实现一个应用：定义一个学生类，需要记录学员的id, 名字和分数。 让我们先来起一个框架： 12345678910111213141516class Student(): def __init__(self, id, name, score): self.id = id self.name = name self.score = score def __repr__(self): return f'学员编号:{self.id}\\n学员姓名:{self.name}\\n学员分数:{self.score}' # 实例化对象zs = Student(37, '张三丰', 98)print(zs)---学员编号:37学员姓名:张三丰学员分数:98 这里，我们对这个方法有一个要求，就是学员的分数只能在0-100范围中, 那其实很简单了对吧？ 我们先来看看第一种最普通的实现方法： 12345678910111213141516171819class Student(): def __init__(self, id, name, score): self.id = id self.name = name # 检测分数范围 if score &gt;= 0 and score &lt;= 100: self.score = score else: print('当前分数不符号要求。') def __repr__(self): return f'学员编号:{self.id}\\n学员姓名:{self.name}\\n学员分数:{self.score}' # 实例化对象zs = Student(37, '张三丰', 101)print(zs)---当前分数不符号要求。AttributeError: 'Student' object has no attribute 'score' 尝试一下，确实打印了“分数不符合要求”，同时报错。 先不说怎么解决报错的问题，这简单的解决方案只能适用于对象初始化的时候有效。如果我们是中间单独对成员属性进行赋值，那么就会失效了 12345678...zs.score = -1print(zs)---学员编号:37学员姓名:张三丰学员分数:-1 那这个时候，大家还记不记得咱们之前学过的一个魔术方法setattr? 我们来给中间加一个__setattr__： 12345678910111213141516171819202122def __setattr__(self, key, value): # 检测是否给score进行赋值操作 if key == 'score': print(key, value) # 检测分数范围 if value &gt;= 0 and value &lt;= 100: object.__setattr__(self, key, value) else: print('当前分数不符号要求。') else: object.__setattr__(self, key, value)def __repr__(self): info = f'学员编号:{self.id}\\n学员姓名:{self.name}\\n学员分数:{self.score}' return info...zs.score = -1---score -1当前分数不符合要求 我们这样就使用__setattr__方法，检测如果score分数进行赋值时候，进行了分数的检测判断。 那我们在看，现在的一个问题是，假如学员的分数不止一个，我需要赋值多个分数怎么办？当前学员有：语文，数学，英语分数。 另外就是当前这个类中的代码是否比较繁杂？ 现在，我们再来看，思考一下使用描述符来代理我们的分数这个成员属性。让我们先来实现一下框架： 123456789101112131415161718192021class Score(): __score = None def __get__(self, instance, owner): pass def __set__(self, instance, value): pass def __delete__(self, instance): del self.__scoreclass Student(): score = Score() def __init__(self, id, name, score): self.id = id self.name = name self.score = score def returnSelf(self): info = f'学员编号:{self.id}\\n学员姓名:{self.name}\\n学员分数:{self.score}' return info 框架就实现好了，我们将原始的普通类中的score代理给了描述符类Score()。 那现在让我们来完善一下整个类中的方法。 首先，当我们进行获取的时候，直接return现有的值就可以了，当我们进行设置的时候，就需要进行判断，如果不符合要求就打印一个不符合要求。 12345678910111213141516171819202122232425# 定义描述符类，代理分数的管理class Score(): __score = None def __get__(self, instance, owner): return self.__score def __set__(self, instance, value): if value &gt;= 0 and value &lt;= 100: self.__score = value else: print('分数不符合要求') def __delete__(self, instance): del self.__scoreclass Student(): score = Score() def __init__(self, id, name, score): self.id = id self.name = name self.score = score def returnSelf(self): info = f'学员编号:{self.id}\\n学员姓名:{self.name}\\n学员分数:{self.score}' return info 让我们对其进行一下检测，看看是不是符合要求： 1234567# 实例化对象zs = Student(37, '张三丰', 132)zs.returnSelf()---分数不符合要求'学员编号:37\\n学员姓名:张三丰\\n学员分数:None' 没毛病，被告知了当前赋值不符合要求，并且最后分数上也为None，并未进行赋值。 在看看单独赋值： 12345678zs.score = -1zs.score = 88zs.returnSelf()---分数不符合要求'学员编号:37\\n学员姓名:张三丰\\n学员分数:88' 当赋值为-1的时候也是提示不符合要求，再次赋值88之后，正确赋值。然后我们打印出来的结果也正确。 那么我们的代理就正确的完成了它的工作。基本工作流程如下： 定义Score描述符类 把学生类中的score这个成员交给描述符类进行代理 只要在代理的描述符中对分数进行判断和赋值就可以了。 那么现在，我们就完成了一个描述符的应用案例。不知道大家是否都理解了？那么在下面，我给大家介绍一下描述符的三种定义格式： 格式一： 通过定义描述符来实现（推荐） 12345678910class ScoreManage(): def __get__(self, instance, owner): pass def __set__(self, instance, value): pass def __delete__(self, instance): passclass Student(): score = ScoreManage() 格式二： 使用property函数来实现 1234567891011121314151617181920class Student(): def __init__(self, id, name, score): self.id = id self.name = name self._score = score def getScore(self): return self._score def setScore(self, score): self._score = score def delScore(self): del self._score # 在 property 函数中指定对应的三个方法 # 对应的方法 1. `__get__`，2. `__set__`, 3. `__delete__` # 当然，名称不是固定的，也可以定义成其他的方法名 # 不管定义成什么，`property`中的方法名必须一致。 # 注意在类中将成员属性重新定义，可以为受保护的或者私有属性，避免递归调用。 score = property(getscore,setscore,delscore) 格式三：使用@property装饰器语法来实现 1234567891011121314151617class Student(): __score = None @property def score(self): print('get') return self.__score @score.setter def score(self,value): print('set') self.__score = value @score.deleter def score(self): print('delete') del self.__score 设计模式 我们谈设计模式的时候，实际上是一个比较抽象的东西。 设计模式，就是前人完成某个功能或者需求，根据经验和总结，对实现的代码步骤和代码设计进行了总结及归纳。成为了实现某个需求的经典模式。 设计模式可以说并不是什么固定的代码格式，而是一种面向对象编程的设计。 让我们先从单例开始。 单例（单态）设计模式 在当前脚本中，同一个类只能创建一个对象去使用，这种情况就称为单例（单态）。 我们以一个实际的思考案例来进行讲解，现在让我们来想： 单例和婚姻法的关系，特别像，就是一个人只能有一个结婚对象。在社会中是如何完成一夫一妻制的？如果想要结婚，必须要到民政局登记，民政局需要检测两个人的户口本，看看上面是否属于已婚的状态。如果是已婚，肯定就被撵出去了对吧。如果没有结婚，就可以盖章登记了。 那么按照这样的思路，我们又该如何去实现Python中的单例设计模式呢？来看哈： 需要一个方法，可以去控制当前对象的创建过程： 构建方法__new__ 需要有一个标识来存储和表示是否有对象：创建一个私有属性进行存储，默认为None； 在创建对象的方法中去检测和判断是否有对象： 如果没有对象，则创建对象，并且将对象存储起来，返回对象。那如果存储的是对象，则直接返回对象，就不需要创建新的对象了。 让我们依照这样一个思路来完成代码，让我们还是从框架开始： 12345class Demo(): # 定义构造方法 def __new__(cls, *args, **kwargs): return cls.obj 第一步我们完成了，现在让我们来看第二部，我们需要定义一个私有属性用于存储对象。 12345678class Demo(): # 定义私有属性存储对象 __obj = None # 定义构造方法 def __new__(cls, *args, **kwargs): return cls.__obj 接着，就要进入判断了： 123456789101112class Demo(): # 定义私有属性存储对象 __obj = None # 定义构造方法 def __new__(cls, *args, **kwargs): # 创建对象的过程中，判断是否有对象 if not cls.__obj: # 如果没有，则创建，并且存储起来 cls.__obj = object.__new__(cls) return cls.__obj 类完成了，让我们来证实一下看看，是否只会创建一个对象。 123456789# 实例化对象a = Demo()b = Demo()print(a)print(b)---&lt;__main__.Demo object at 0x10434e950&gt;&lt;__main__.Demo object at 0x10434e950&gt; 看到打印结果中，两次实例化对象不同，但是地址相同。可以证明确实为一个对象。 Mixin类 Mixin 必须是表示一种功能，而不是一个对象。 Mixin 的功能必须单一，如果有多个功能，那就多定义Mixin类 python 中的Mixin是通过多继承实现的 Mixin 这个类通常不单独使用，而是混合到其它类中，去增加功能的 Mixin 类不依赖子类的实现，即便子类没有继承这个Mixin,子类也能正常运行，可能就是缺少了一些功能。。 那使用Mixin混入类有什么好处呢？ 这个混入类的设计模式，在不对类的内容修改的前提下，扩展了类的功能。也提高代码的重用性，使的代码结构更加的简单清晰。可以根据开发需要任意调整功能（也就是创建新的Mixin混入类），避免设计多层次的复杂的继承关系。 我们之前学习继承，知道继承需要有一个必要的前提，就是继承应该是一个is-a的关系。 比如，苹果可以去继承水果，因为苹果is a水果， 那苹果是不能继承午饭的，因为午饭可以有苹果，也可以没有。 再比如，汽车可以继承交通工具，又是因为汽车本身is a交通工具。 遵循这样的一个规律，我们来思考，交通工具都有哪些呢？ 汽车、飞机、直升飞机，这些都属于交通工具对吧？当然，高铁什么的也是，我们无法穷举出来，那样就太多了。 那么如何去设计这些类的关系呢？我们可以创建一个交通工具类，然后属于交通工具的都来继承，再去实现... 等等，我们再来思考一个问题：飞机、直升飞机都可以飞，可是汽车呢？汽车并不能飞行。那么交通工具中如果去定义飞行这个功能，是不是就不合适了？ 你们现在是不是在想：那就在飞机和直升飞机类中分别实现飞行这个功能。可以是可以，但是重复代码是不是过多了？代码无法重用。 那该怎么办？其实，让我们分别去定义交通工具和飞行器这两个父类，这样飞机和直升飞机就可以去继承这两个类。对吧？ 来，让我们开始实现，一样的，先来个框架 1234567891011121314# 定义交通工具class vehicle(): # 运输货物 def cargo(): print('货物') # 搭载乘客 def person(): print('人')# 定义飞行器class flying(): def fly(self): print('可以飞') 现在刚才的思考得以实现，我们定义了两个父类。接着是不是就要考虑继承了？ 1234567891011# 定义飞机class airplane(vehicle, flying): pass# 定义直升机class helicopter(vehicle, flying): pass# 定义汽车class car(vehicle): pass 根据我们之前学习的继承关系，这样就完成了子类对父类的继承关系。来让我们分析下： 此时去定义一个飞行器的类Flying, 让需要飞行的交通工具直接继承这个类，可以解决问题。但是又两个问题，出现的类多继承，就违背了is-a， 飞行器这个类很容易被误解。那怎么办？ 其实解决方案还是使用多继承，但是给飞行器这个类定义为一个Mixin混合类，此时就是等于把飞行器这个类，作为一个扩展的功能，来扩展其他类。 让我们来改一下： 1234567891011121314151617181920212223242526# 定义交通工具class vehicle(): # 运输货物 def cargo(): print('货物') # 搭载乘客 def person(): print('人')# 定义飞行器class flyingMixin(): def fly(self): print('可以飞')# 定义飞机class airplane(vehicle, flyingMixin): pass# 定义直升机class helicopter(vehicle, flyingMixin): pass# 定义汽车class car(vehicle): pass 嗯，你没看错，就是这么简单，改一下名称。 那么在这段代码中，虽然直升机和飞机都是用了多继承，也就是继承了flyingMixin，但是由于flyingMixin类加了Mixin这个名，就告诉了后面阅读代码的人，这个类是一个Mixin类。 我知道你们在想什么，这个是不是太随便了？其实并不是，我们目前在谈论的是「设计模式」，这个flyingMixin类中除了名称之外，还要遵循一些特定的惯例规则，就是这个类中的功能必须是单一的。 在名称的含义上，Mixin表示混入(mix-in)， Mixin必须是表示一种功能，而不是一个对象。Mixin的功能必须单一，如果有多个功能，那就需要多定义几个Mixin类。在Python中的Mixin是通过多继承实现的。Mixin类通常不单独使用，而是混合到其他类中，去增加功能的。Mixin类不依赖子类的实现，即便子类没有继承这个Mixin类，子类也能正常运行，只是可能缺少一些功能。 抽象类 首先我们要明白，抽象类也是一个类。但是，这又是一个特殊的类，抽象类不能用，不能直接实例化称为一个对象。抽象类包含了抽象方法，抽象方法就是没有实现代码的方法。抽象类需要子类继承，并重写父类的抽象方法，才可以使用。 抽象类，一般应用在程序设计，程序设计中一般是要对功能和需求进行规划，其中有一些需求是明确的并且可以完成的，但是也可能会有一些需求是不明确的，或者不确定具体需要怎么实现，此时就可以把这个不确定怎么实现或者需要后面再去实现的方法，定义为抽象方法（只定义方法名，不写具体代码）。 我们还是拿一个实例来讲解： 比如公司有一项新的产品需要开发，交给了开发部门的大拿，也就是你。那么你就开始去规划设计怎么去完成这个产品的开发。比如项目需要用到不同的技术，不同的人来完成。这样，你作为老大，自己完成了一部分功能，但是依然有一部分定义了需求，但是还没有具体实现，需要其他人来进行实现。 那么此时，你已经写完的部分就是普通方法，定义了需求但是未完成的就可以理解为是抽象方法。 还是来直接看代码： 123456789101112131415import abc# 必须使用metaclass, 属性必须是abc.ABCMetaclass WriteCode(metaclass=abc.ABCMeta): # 需要抽象方法，使用装饰器进行装饰 @abc.abstractmethod def write_swift(self): pass def write_java(self): print('实现了Java代码的开发') def write_python(self): print('实现了Python代码的开发') 这样我们就在一个抽象类中定义好了一个抽象方法，和几个普通方法。至于为什么必须metaclass=abc.ABCMeta，那就又要扩展着去讲了。这里先记住，就跟背单词一样，到这里了就这么用就可以了。 前面我们讲了，抽象类是不能直接实例化的，让我们试试看： 12345# 抽象类不能直接实例化对象obj = WriteCode()---TypeError: Can't instantiate abstract class WriteCode with abstract method write_swift 报错了，直接告诉我们无法实例化抽象类。 那么我们到底要怎么用呢？ 我们可以定义一个子类来继承，并实现抽象类中的抽象方法。 1234# 定义子类，继承抽象类，并实现抽象类中的抽象方法class Demo(WriteCode): def write_swift(self): print('实现了swift代码的开发') 好了，现在让我们实例化子类试试： 12345obj = Demo()print(obj)---&lt;__main__.Demo object at 0x104ade8c0&gt; 没有报错，似乎是完成了继承和实现。接着当然是一次执行子类中的方法来看看： 12345678obj.write_java()obj.write_python()obj.write_swift()---实现了Java代码的开发实现了Python代码的开发实现了swift代码的开发 没毛病，现在我们完成了整个代码。 那小伙伴们现在估计最大的疑问是：抽象类我要应用在什么地方呢？ 比如说，我们现在要开发一个框架，这个框架要有一大堆的功能，包括a,b,c（哎，我忽然理解导入的为什么是abc这样起名了）。但是呢，具体用这个框架开发什么样的产品我们并不清楚，因此这个框架中能否知道你要做什么样的开发吗？肯定不知道。 框架具备了一定的功能即可，剩下的，需要具体开发项目的人来实现自己的业务逻辑。 那这个时候，我们就要用到抽象类了。 好了，到目前为止，我们关于面向对象编程也就介绍的差不多了。而我们的Python课程基本上也到了尾声，在后面的课程中，我们会介绍一下Python中的装饰器。然后去学习一下几个用的特别广的库，包括matplotlib,numpy以及pandas。 小伙伴们，大家对于此前的Python基础一定要好好的理解，好好练习。这样，越往后我们才能越轻松的开展后面的学习。 行，本节课到这里就结束了，下课。","link":"/OOP-Descriptor-and-design-patterns/"},{"title":"22. 面向对象 - 高阶","text":"Hi，大家好。我是茶桁。 之前的课程里面，我们简单的接触了面向对象编程，也和大家讲解了其思想，优缺点。相信上节课程结束之后，大家对面向对象都有了一定的理解。 那么我们这节课，就进入面向对象的一些高阶部分，让我们继续来学习一些魔术方法以及Python的内置成员，然后再来学习一下描述符与设计模式。 内置成员 魔术方法 描述符 设计模式 好，正课走起。让我们开始。 内置成员 当我们创建一个类之后，即便我们还什么都没做，这个类里面就已经有内容了，我们来看一下： 123456789class Demo(): pass# 获取类/对象的所属成员res = Demo.__dict__print(res)---{'__module__': '__main__', '__dict__': &lt;attribute '__dict__' of 'Demo' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'Demo' objects&gt;, '__doc__': None} 上节课我们学过了__dict__， 这个是获取类或者对象的成员的方法。打印结果我们看到其中的成员。 让我们添加些内容再来观察一下： 12345678910111213class Demo(): name = 'a' age = 20 def say(self): print('say something')# 获取类/对象的所属成员res = Demo.__dict__print(res)---{'__module__': '__main__', 'name': 'a', 'age': 20, 'say': &lt;function Demo.say at 0x1117a3e20&gt;, '__dict__': &lt;attribute '__dict__' of 'Demo' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'Demo' objects&gt;, '__doc__': None} 看到我们刚才定义的成员属性和成员方法也都在列了。我们还可以实例化之后获取对象的成员： 12345obj = Demo()print(obj.__dict__)---{} 当我们实例化一个对象obj之后，打印发现其成员是空的。这是为什么？ 原因就在于，这个方法用处其实是打印其对象的专有成员。我们再来为这个实例化对象创建一些成员再来看看： 12345obj.sex = 'female'print(obj.__dict__)---{'sex': 'female'} 可以看到，我们获取了刚才创建的成员属性。 以上，就是我们使用__dict__获取了类和对象的所属成员，方法为：类/对象.__dict__。 除了获取所属成员，我们还有其他方法，比如获取「文档信息」， 获取「类名称」， 获取「所在文件名称」，获取「当前类的父类列表」以及获取「当前类的『继承链』。来让我们依次看一下： 还记得我们之前在创建函数的时候可以添加文档吗？ 1234def obj(): ''' 这里是文档内容 ''' 同样的，类当中我们一样可以添加文档内容。然后我们可以通过__doc__来获取： 12345678910class Demo(): ''' 这里是一个Demo类，主要用于测试 ''' passprint(Demo.__doc__)---这里是一个Demo类，主要用于测试 同样的，__doc__不仅可以获取类的文档信息，同样可以获取到对象的。 我们使用__name__来获取类名称「组成的字符串」, 这个方法无法对对象使用。 1234print(Demo.__name__)---Demo __module__可以用来获取类/对象所在的文件名称 123456print(Demo.__module__)print(obj.__module__)---__main____main__ 如果其所在文件为当前文件，那么这里就会显示为__main__。 然后是__base__， 这个方法是用来获取当前类的父类列表。这个方法有两个版本，一个是__base__, 一个是__bases__。这两个方法的区别在于一个是获取继承的第一个父类，一个是继承所有的父类的列表，为了呈现的更明显，我们建立一个继承类： 1234567891011class A(Demo): passclass B(A, Demo): passprint(B.__base__)print(B.__bases__)---&lt;class '__main__.A'&gt;(&lt;class '__main__.A'&gt;, &lt;class '__main__.Demo'&gt;) 还有一个就是我们上节课讲过的，MRO列表，也就是__mro__方法，用于获取当前类的继承链。 1234print(B.__mro__)---(&lt;class '__main__.B'&gt;, &lt;class '__main__.A'&gt;, &lt;class '__main__.Demo'&gt;, &lt;class 'object'&gt;) 到此为止，我们介绍的就是常用的一些内置成员获取的一些方法。当然，这里不是全部，除此之外还有很多，因为并不是常用，所以这里我们就不多介绍了。 方法的分类 接下来呢，我们来看下面向对象的分类，包括： 对象方法 类方法 绑定类方法 静态方法 对象方法 其特征为： 1. 在类中定义方法，含有self参数 2. 含有self的方法，只能使用对象进行调用。 3. 该方法会把调用的对象传给进来。 12345678910111213class Demo(): # 对象方法 def objFunc(self): print(self) print('this is objFunc')# 实例化对象obj = Demo()obj.objFunc()---&lt;__main__.Demo object at 0x1171460e0&gt;this is objFunc 这个方法不能直接使用类直接调用，但是其实也不是绝对的。当我们使用类直接调用的时候，需要传递一个参数，也就是必须要self有参数可接收。 12345Demo.objFunc('a')---athis is objFunc 类方法 类方法呢，和对象方法有不一样的地方，也有相同的地方。两者定义十分相似，不同之处是使用装饰器材： 其特征为： 在类中定义的方法，使用了@classmethod进行了装饰 方法中有形参cls 可以不用实例化对象，直接使用类进行调用 会把调用这个方法的类或对象传递进来 来直接看代码理解： 12345678910111213141516class Demo(): # 类方法 @classmethod # 装饰器 def clsFunc(cls): print(cls) print('this is cls function: clsFunc')Demo.clsFunc()obj.clsFunc()---&lt;class '__main__.Demo'&gt;this is cls function: clsFunc&lt;class '__main__.Demo'&gt;this is cls function: clsFunc 看结果可以看到，我们用类进行调用的时候并没有像对象方法一样传递一个参数进去，这是因为调用的时候会直接传递调用的类给到cls参数。 而我们说不需要实例化对象，并不是实例化对象不可调用。对象调用也是可以的。 至于什么是「装饰器」，我们以后会详细讲到，这里先记住这种形式就可以了。 绑定类方法 这个方法不传递任何对象和类。在定义的时候，不设定任何的形参： 1234567891011class Demo(): # 绑定类方法 def bindClassFunc(): print('this is bind Class function: bindClassFunc')# 调用Demo.bindClassFunc()---this is bind Class function: bindClassFunc 那么绑定类方法既然没有定义形参，那么这个方法是无法使用实例化对象来调用的。 1234obj.bindClassFunc()---TypeError: Demo.bindClassFunc() takes 0 positional arguments but 1 was given 其特征如下： 1. 在类中定义的方法，不必须设置形参。 2. 只能使用类进行调用。 3. 可以传递任意参数，但是不会将类作为参数传递进来。 静态方法 「静态类方法」和「类方法」相似，也需要一个装饰器。并且，静态类方法也是不需要设置形参的。 123456789101112class Demo(): # 静态类方法 @staticmethod def staticFunc(): print('this is static method func')Demo.staticFunc()obj.staticFunc()---this is static method functhis is static method func 那从结果中我们可以看到，「静态类方法」可以使用类和对象进行调用，并且调用的时候不需要传递任何参数。 其特征如下： 1. 在类中定义的方法，使用装饰器 @staticmethod 进行了装饰 2. 可以使用对象或者类进行调用 3. 不会将对象或者类作为参数传递进来 ⚠️ 注意：这里我们需要注意的，「静态类方法」只是可以不设置参数，并不是不能设置参数，并且，就算是设置了参数之后，也是不接受类和对象作为参数传递的。比如： 1234567891011121314@staticmethoddef staticFunc(a, b): print(f'a:{a}, b:{b}') print('this is static method func')# 调用Demo.staticFunc('static', 'class')obj.staticFunc('static', 'obj')---a:static, b:classthis is static method funca:static, b:objthis is static method func 我们分别用类和对象进行了调用并传递了两个参数进行打印，而打印结果正常，并且没有对象或者类被传递。 相应的，「绑定类方法」也是这种特性，只是「绑定类方法」只支持类调用，不支持对象调用。 常用函数 其实在之前，关于「常用函数」我们已经接触过了一些，比如：issubclass(子类，父类)。有些小伙伴可能还记得，这个函数是用于检测一个类是否为另一个类的子类。 那除了这个之外，Python中还有很多其他的一些针对类和对象的常用函数，下面让我们来详细看一下。 isinstance(对象，类)， 用于检测一个对象是否是该类或者该类的子类的实例化结果。 12345obj = D()print(isinstance(obj, D))---True 这个结果显而易见，那么我们思考一下，既然D类继承了B类和C类，那么obj对象是否也是B或者C的实例化结果呢？ 1234print(isinstance(obj, B))---True 可见，对于继承了父类的子类，其实例化对象和父类之间也会被检测为True。 hasattr(对象/类,'成员名称')， 这个函数是用于检测类/对象是否包含指定名称的成员。 12345B.name = '张三'print(hasattr(obj, 'name'))---True 在这段代码中，我们给父类B添加了一个成员属性name，因为obj是D的实例化对象（之前的代码中）。而D类是继承自B类的，所以自然obj中也是包含了name这个成员属性的。所以我们的检测结果必然为True。 来，我们做另外一个实验： 12345678objB = B()D.age = 20print(D.age)print(hasattr(objB, 'age'))---20False 我们重新用B类实例化了一个对象objB， 然后我们给D类添加了一个成员属性age，并且打印了一遍证实其存在。这个时候我们检测了一下objB中是否含有age， 因为D为B的子类，它说添加的成员属性为独有属性，并不会更改到B类里，那自然B的实例化对象objB中是不可能存在这个成员属性的，结果自然为False。 `` getattr(对象/类,'成员名称'), 用于获取类/对象的成员的值 那这个函数就好理解了，我们可以用之前建立好的实例化对象objB和obj来获取一下试试看： 123456print(getattr(obj, 'age'))print(getattr(objB, 'name'))---20张三 没问题，结果如我们所料一般。那如果是获取objB中的age的值会如何？我们前面已经知道，objB中并未存在age这个成员属性，所以必然会报错： 1234print(getattr(objB, 'age'))---AttributeError: 'B' object has no attribute 'age' setattr(对象/类,'成员名称','成员的值'), 这个函数用于设置类/对象的成员的属性值。 123456print(setattr(obj, 'name', 'du'))print(obj.name)---Nonedu 如结果所见，这个方法的返回值为None，但是我们通过打印obj.name可知，方法确实更改了obj中name的值。 delattr(类/对象,'成员名称') 这个函数可以删除类/对象的成员属性，和del直接删除对象的成员是一样的结果。 123456print(delattr(obj, 'name'))print(obj.name)---None张三 可见，这个方法也是没有返回值的，返回了None。 不过，既然我们已经删除了obj中的name， 为啥还能打印出张三呢？有没有小伙伴知道为什么？ 其实，我们删除的name是之前使用setattr为obj设定的专有成员，当它被删除之后，我们的obj的继承类D中还存在着name这个成员属性，所以现在打印出来的张三是从D类中继承过来的。 那如果是我们新添加的但是其他类中没有的属性就会直接报错了，来看： 12345678setattr(obj, 'size', 'small')print(obj.size)delattr(obj, 'size')print(obj.size)---smallAttributeError: 'D' object has no attribute 'size' 我们分别打印了两次，第一次setattr了一个成员属性size，并且打印验证了。然后我们执行delattr，删除了刚才设置的成员属性size，这次再打印来看，报错了。 dir()这个函数可以获取当前对象所有可以访问的成员的列表。正好，让我们来看看是否从还存在从B类中继承的成员属性name： 12345res = dir(obj)print(res)---['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'age', 'name'] 可以看到打印结果的最后面，确实还存在着name这个成员属性。 以上函数在讲解的过程中，我们使用的都是成员属性，而成员方法其实是一样的。因为这几个函数说针对的对象都是「成员」。 另外需要注意的一点是，以上所有这些常用函数，都是在可访问的情况下才可执行。我们还有一些不可访问的情况，比如说「私有成员属性」，这种成员是无法被访问或者操作的，我们随便拿个函数来举一个例子看看： 12345678910111213class D(): name = '张三' _age = 25 __sex = 'female' print(f'Sex:{__sex}')obj = D()getattr(obj, '__sex')---Sex:femaleAttributeError: 'D' object has no attribute '__sex' 可以看到，当我们意图用getattr来获取实例化对象obj中的__sex属性时报错了。无法正确访问。 12345res = dir(obj)print(res)---['_D__sex', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_age', 'name'] 当我们使用dir()来查看的时候，其中也并没有__sex这个成员属性，有的只是类的私有成员属性:_D__sex， 我们需要借助类从内部才可访问到： 1234getattr(obj, '_D__sex')---'female' 这样是可以的。 魔术方法 我们在这节课之前，讲到过「魔术方法」， 那我们已经了解，魔术方法是不需要手动调用就可以自动执行的方法。 那我们之前已经讲解过__init__，这是一个初始化方法。然后还有一个__del__方法，是一个销毁方法。 这两个方法除了功能上的不同之外，还有一个最大的不同点就是被触发的机制是不一样的。其实，魔术方法中，最重要的一点就是要了解方法的触发机制是什么。 让我们先列出来常用的魔术方法，包括其触发机制，作用以及参数等等... __init__， 初始化方法, ***** 触发机制：当实例化对象之后就会立即触发的方法 作用：为当前创建的对象完成一些初始化的操作，比如：成员属性的赋值， 方法的调用， 打开或者创建一些资源等等。 参数：一个self， 接收当前对象，其他参数根据需求进行定义即可。 返回值：无 注意事项：无 __new__，构造方法, **** 触发机制：实例化对象时自动触发（在__init__之前触发） 作用：管理控制对象创建的过程 参数：一个cls接收当前类，其它参数根据初始化方法的参数进行决定 返回值：必须返回object.__new__(cls)进行对象的创建，如果没有返回值，则实例化对象的结果为None 注意事项： __new__方法的参数和__init__方法的参数要保持一致，除了第一个参数。必须返回object.__new__(cls)进行对象的创建，如果没有返回值，则实例化对象的结果为None 应用场景：设计模式中的单例设计模式。 __del__，析构方法, ***** 触发机制：当该类对象被销毁时，自动触发 作用： 关闭或释放对象创建时打开或创建的一些资源 参数： 一个self，接受当前的对象 返回值：无 注意事项： 无 __call__ , *** 触发机制: 把对象当作函数直接调用时自动触发 作用: 一般用于归纳类或对象的操作步骤，方便调用 参数：一个self接收当前对象，其它参数根据调用需求缺点 返回值：可有可无 5.__len__ 触发机制: 当使用len函数去检测当前对象的时候自动触发 作用: 可以使用len函数检测当前对象中某个数据的信息 参数: 一个self接收当前对象 返回值：必须有，并且必须是一个整型 注意事项：len要获取什么属性的值，就在返回值中返回哪个属性的长度即可 6.__str__ 触发机制: 当使用str或者print函数对对象进行操作时自动触发 作用: 代码对象进行字符串的返回，可以自定义打印的信息 参数：一个self，接收当前对象 返回值：必须有，而去必须是字符串类型的值 7.__repr__ 触发机制: 在使用repr方法对当前对象进行转换时自动触发 作用: 可以设置repr函数操作对象的结果 参数： 一个self，接收当前对象 返回值： 必须有，而去必须是字符串类型的值 注意：正常情况下，如果没有__str__这个魔术方法，__repr__方法就会代替__str__魔术方法 8.__bool__ 触发机制: 当前使用bool函数转换当前对象时，自动触发.默认情况下，对象会转为True 作用: 可以代替对象进行bool类型的转换，可以转换任何数据 参数: 一个self接收对象 返回值： 必须是一个布尔类型的返回值 以上，我们把常用魔术方法都列出来之后，然后我们来些代码进行讲解。让我们先创建一个Person类，然后在其中协商构造方法，初始化方法和析构方法。 123456789101112131415161718192021222324252627# 定义一个人class Person(): # 构造方法 def __new__(cls, *args, **kwargs): print(args) print(kwargs) # 初始化方法 def __init__(self, name, age, sex): print('触发初始化方法:__init__') self.name = name self.age = age self.sex = sex # 析构方法 def __del__(self): print('触发了析构方法:__del__')# 实例化对象zs = Person('张三丰', 210, '男')print(zs)---('张三丰', 210, '男'){}None 当我们完成实例化的时候，「构造方法」先是用*args接收了所有传递的参数，并且使用存储了元组。我们可以看到，**kwargs什么都没接收到，所以打印为空。 当「构造方法」执行完之后，也并没有去执行「初始化方法」和「析构方法」，这又是为什么呢？这是因为如果在「构造方法」中没有返回对象，这对象无法创建。要想对象进行创建，这我们必须返回object.__new__(cls)进行对象的创建。这在之前「构造方法」的说明里有说明。这个cls参数是什么呢？我们直接来看代码： 12345678910111213141516171819202122232425262728293031# 定义一个人class Person(): # 构造方法 def __new__(cls, *args, **kwargs): print(args) # print(kwargs) print(cls) # 如果该方法中没有返回对象，则无法创建对象 return object.__new__(cls) # 初始化方法 def __init__(self, name, age, sex): print('触发初始化方法:__init__') self.name = name self.age = age self.sex = sex # 析构方法 def __del__(self): print('触发了析构方法:__del__')# 实例化对象zs = Person('张三丰', 210, '男')print(zs)---('张三丰', 210, '男')&lt;class '__main__.Person'&gt;触发初始化方法:__init__&lt;__main__.Person object at 0x107f3c070&gt; 现在可以看到，我们打印了cls， 实际上就是Person这个类。当我们返回object.__new__(cls)之后，可以看到__init__初始化方法正确运行了，执行了方法内的打印方法。然后最后，我们打印了zs这个实例化对象。那为什么__del__析构方法没有触发？因为我们是在Jupyter中执行，并未执行释放，此时我们如果del zs，则会触发析构方法，或者，我们讲上述代码保存为一个22.py文件，然后单独执行，这个时候Python的垃圾回收机制会执行，就会进行释放，从而触发析构方法。如下图： 我们接着上面写的代码在22.py中继续写： 1234zs()---TypeError: 'Person' object is not callable 报警，告知我们这个类当中没有cllable。那如果我们讲这个类改造下，加上__call__: 1234567891011121314151617181920212223# 魔术方法# 定义一个人class Person(): # 构造方法 def __new__(cls, *args, **kwargs): ... # 初始化方法 def __init__(self, name, age, sex): ... def __call__(slef, *args, **kwargs): print('你把对象当成了函数进行调用。') # 析构方法 def __del__(self): ...# 实例化对象...zs()---...你把对象当成了函数进行调用。触发了析构方法:__del__ 这样，我们直接执行zs()就没问题了，可以把对象当作函数直接调用时自动触发。 让我们继续，返回到22.ipynb笔记本文件中，让我们重新定义一个类： 123456789class Demo(): items = []# 实例化对象obj = Demo()len(obj)---TypeError: object of type 'Demo' has no len() 报错信息中可以看出，这个实例化对象是没有len()方法的。我们如果给它加上__len__之后就会让其拥有len()方法。 1234567891011class Demo(): items = [] def __len__(self): return len(self.items)# 实例化对象obj = Demo()print(len(obj))---0 因为当前我们在类中定义的items里面没有数据，所以返回的长度必然也是0。但是这个返回值是必须要有的，需要返回一个整型才行。 来，我们看看如果这个方法的返回值写死会如何： 123456789101112class Demo(): items = [] def __len__(self): return 1# 实例化对象obj = Demo()obj.items = [1, 2, 3, 4, 5, 6, 7]print(len(obj))---1 很明显，我们重新给obj.items进行了赋值，目前其长度是7, 可是返回值依然是1。说明__len__的返回值只要四个整型就行。那我们就需要注意了，len需要获取什么属性的值，就在返回值中返回哪个属性的长度即可。当我们用正确的方式返回的时候就会是这样： 123456789101112class Demo(): items = [] def __len__(self): return len(self.items)# 实例化对象obj = Demo()obj.items = [1, 2, 3, 4, 5, 6, 7]print(len(obj))---7 让我们继续接着这段代码来玩： 123456...res = str(obj)print(res)---&lt;__main__.Demo object at 0x110631270&gt; 发现没有，虽然我们使用了str()方法，可是最后返回的结果，和直接打印obj的结果是一样的。那为什么会这样呢？这是因为我们这个当前的方法其实是对obj对象进行了一个转化字符串操作，而其本身就返回了一个&lt;__main__.Demo object at 0x110631270&gt;的字符串。 其实这个返回的字符串我们也是可以自定义的, 使用__str__方法给一个返回值就可以了。 123456789101112class Demo(): items = [] ... def __str__(self): return '&lt;__Demo__, 此字符串返回的是茶桁自定义的结果。&gt;'# 实例化对象obj = Demo()...print(obj)---&lt;__Demo__, 此字符串返回的是茶桁自定义的结果。&gt; 我们直接打印了obj对象，因为__str__方法的存在，所以现在直接打印了返回的字符串。也就是说，该方法可以代替对象进行str或者print的字符串信息返回。 继续来看： 1234567891011121314class Demo(): # def __str__(self): # ... def __repr__(self): return '这是一个repr返回的内容'# 实例化对象obj = Demo()print(obj)---这是一个repr返回的内容 可以看到我在类中注释了__str__方法，这是因为只有其不存在的情况下，__repr__ 方法才会起作用，可以替代__str__方法。 那么到底__str__和__repr__两个到底有什么区别呢？让我们直接在代码里找答案： 1234567num = 521print(str(num))print(repr(num))---521521 这个时候两个的结果都是一样的，似乎并看不出两者到底有什么区别。别急，让我们继续往后做这个实验： 123456789num = 521r1 = str(num)r2 = repr(num)print(f'r1: {r1}, {type(r1)}')print(f'r2: {r2}, {type(r2)}')---r1: 521, &lt;class 'str'&gt;r2: 521, &lt;class 'str'&gt; 两者的类型都是一样的，返回了一个字符串类。难道这两者就正的毫无区别吗？Python得创建者吃饱了撑的没事做两个功能一模一样但是名字不同的方法？ 123456789s = '521'r1 = str(s)r2 = repr(s)print(f'r1: {r1}, {type(r1)}')print(f'r2: {r2}, {type(r2)}')---r1: 521, &lt;class 'str'&gt;r2: '521', &lt;class 'str'&gt; 仔细看，两者似乎有了细微的差别。repr解析的结果带着引号。 那么，str和repr函数都可以把其他类型的数据转为字符串类型。 str函数会把对象转为更适合人阅读的形式，repr函数会把对象转为解释器读取的形式。 如果数据对象并没有更明显的区别的话，str和repr的转化结果还真没什么区别。 这两者的区别，其实只要了解一下就可以了。大部分时候，并不需要那么较真。 接着让我继续来看看__bool__： 12345res = bool(obj)print(res)---True 当我们对obj使用bool()方法的时候，返回值为True。 那说明其中包含了一个bool机制，并且默认返回值为True。 这个时候让我们来定义一下看看： 12345678910111213class Demo(): items = [] ... def __bool__(self): return bool(self.items)# 实例化对象obj = Demo()res = bool(obj)print(res)---False 由于我们的items中设置为空值，而我们将前面定义obj的items的那段代码删掉了，所以这个时候，传入方法的self.items的值也为空，必然返回值就是False。也证明了，bool(obj)拿到的返回值就是类里定义的的__bool__中返回的对象。 介绍完常用的一些魔术方法之后，我们再来看一些其他的魔术方法。同样是魔术方法，为什么我要明显的区别开来讲呢？那是因为现在开始说讲的魔术方法都是针对成员的，是一些成员相关魔术方法。 __getattribute__: 优先级最高 触发机制: 当访问对象成员时，自动触发，无论当前成员是否存在 作用: 可以在获取对象成员时，对数据进行一些处理 参数: 1. self接收对象，2. item接收当前访问的成员名称 返回值: 可有可无，返回的值就是访问的结果 注意事项: 在当前的魔术方法中，禁止对当前对象的成员进行访问，会触发递归。如果想要在当前魔术方法中访问对象的成员必须使用object来进行访问。格式： object.__getattribute__(self,item) __getattr__ 触发机制：当访问对象中不存在的成员时，自动触发 作用：防止访问不存在的成员时报错，也可以为不存在的成员进行赋值操作 参数: 1. self接收当前对象，2. item接收当前访问的成员名称 返回值：可有可无 注意事项：当存在 getattribute 方法时，会去执行 getattribute 方法。也要注意，不要在当前的方法中再次去访问这个不存在的成员，会触发递归操作 __setattr__ 触发机制： 当给对象的成员进行赋值操作时会自动触发（包括添加，修改） 作用： 可以限制或管理对象成员的添加和修改操作 参数： 1. self接收当前对象 2. key设置的成员名 3. val设置的成员值 返回值： 无 注意事项：在当前的魔术方法中禁止给当前对象的成员直接进行赋值操作，会触发递归操作。如果想要给当前对象的成员进行赋值，需要借助 object 格式： object.__setattr__(self,key,value) __delattr__ 触发机制： 当删除对象成员时自动触发 作用： 可以去限制对象成员的删除，还可以删除不存在成员时防止报错 参数：1. self接收当前对象 2. item删除的成员名称 返回值： 无 注意事项： 在当前魔术方法中禁止直接删除对象的成员，会触发递归操作。如果想要删除当前对象的成员，那么需要借助 object。 格式： object.__delattr__(self,item) 好了，按照惯例，让我们上代码, 先让我们来定义一个最正常的类，并且实例化它： 12345678910111213141516171819202122class Person(): name = 'name' age = 0 sex = 'male' def __init__(self, name, age, sex): self.name = name self.age = age self.sex = sex def say(self): print('say something...') def sing(self): print('sing a song...')# 实例化对象obj = Person('张三丰', 280, '男')print(obj.name)---张三丰 这个时候，让我们在类中定义一个方法：__getattrbute__()。 1234567891011class Person(): ... def __getattribute__(self, item): pass# 实例化对象obj = Person('张三丰', 280, '男')print(obj.name)---None 可以看到，虽然我们在实例化对象的时候传入了成员值，但是当我们打印的时候返回值为None。如果我们这个时候修改一下这个魔术方法： 1234567891011class Person(): ... def __getattribute__(self, item): return 'abc'...print(obj.name)print(obj.sex)---abcabc 那我们拿到的就是得到的内容。不仅是name， 任意我们传入的成员，返回的值都为__getattribute__返回的值。当获取对象成员时，这个方法进行处罚，其中的item形参就是我们想要获取的成员属性。第一次是obj.name, 第二次是obj.sex，但是无论你调用的是什么成员，拿到的都是这个方法的返回值abc。 那既然这样，是不是我们返回对象的成员属性就可以了？ 12345class Person(): ... def __getattribute__(self, item): return self.name 千万不要这么做，这样会引起方法的无限递归调用，最终导致栈溢出。那么是不是我们就没办法了？也不是，我们需要使用object.__getattribute__(self, item)： 123456789101112131415class Person(): ... # 获取对象成员的时候触发 def __getattribute__(self, item): return object.__getattribute__(self, item)# 实例化对象obj = Person('张三丰', 280, '男')print(obj.name)print(obj.sex)---张三丰男 这样，我们就获取到了正确的返回值。我们这里讲解一个__getattribute__方法，限于篇幅的原因，我们其他的几个方法就不细致讲了。在我们先前的列表内，每一个方法的触发机制，作用，参数和注意事项我们都有写清楚。大家可以执行去看看，并做一些测试。让我们赶紧进入下一个阶段，不过在这之前呢，我们还是需要讲访问成员的顺序给大家强调一下，这个还是比较重要： 调用 __getattribute__魔术方法 调用数据描述符 调用当前对象的成员 调用当前类的成员 调用非数据描述符 调用父类的成员 调用__getattr__魔术方法 以上步骤是调用某个成员时的顺序，前面的能够调用成功，后面则不再执行。至于描述符，咱们下节课来详细讲。 好了，本节课到这里就结束了，让我们先预告一下，下节课呢，我们来讲讲面向对象中的「描述符和设计模式」。大家期待一下吧。 记得课后好好做练习，目前我们的课程稍微有些难度了，只有保持一定的练习量，才能理解并记住。 小伙伴们，下节课再见了。下课。","link":"/Object-Oriented-Programming-Higher-Level/"},{"title":"24. 装饰器语法与应用","text":"Hi, 大家好。我是茶桁。 在最近几期的课程中，相信小伙伴们都频繁的看到一个词：「装饰器」， 那到底什么是装饰器，又有什么作用呢？我们这节课，就来好好的来了解一下。 装饰器定义 装饰器就是在不改变原有函数代码，且保持原函数调用方法不变的情况下，给原函数增加新的功能（或者给类增加属性和方法）。 核心思想：用一个函数（或者类）去装饰一个旧函数（或者类），造出一个新函数（或者新类）。 应用场景：引入日子，函数执行时间的统计，执行函数钱的准备工作，执行函数后的处理工作，权限校验，缓存等。 语法规则：在原有的函数上加上@符，装饰器会把下面的函数当作参数传递到装饰器中，@符又被称为「语法糖」。 装饰器原型 装饰器其实就是利用闭包，把函数当作参数传递，并在在函数内去调用传递进来的函数，并返回一个函数。 来，我们还是用代码来学习，先让我们定义一个普通函数： 12345678# 定义一个普通函数def old(): print('我是一个普通的函数')old() # 作为普通函数直接调用---我是一个普通的函数 现在让我们定义一个嵌套函数，分为外函数和内函数两部分： 12345678# 定义外函数，接受一个函数作为参数def outer(f): # 定义内函数， 并且在内函数中调用了外函数的参数 def inner(): print('我是外函数中的内函数1') f() print('我是外函数中的内函数2') return inner 这里面我们在内函数中打印了两句话，在两句话中间执行了一次外函数的参数（传递进来一个函数）。最后讲内函数作为参数返回。 然后我们讲刚才的普通函数old作为参数传进去，然后再用外函数返回的inner内函数重新赋值普通函数old，最后让我们再执行一遍old函数, 这个时候，因为old被重新赋值，其实等同于调用了inner函数。来，我们看看结果： 1234567old = outer(old) # outer返回了inner函数，赋值给了oldold()---我是外函数中的内函数1我是一个普通的函数我是外函数中的内函数2 是不是稍显繁杂？那让我们换个思路，现在我们已经先定义好了outerhe inter，两者关系不变。还是之前那些代码，那么我们如何利用装饰器来进行调用呢？ 1234# 装饰器用法@outer # 此处将outer作为了装饰器def old(): print('我是一个普通的函数') 我们在定义old函数的时候，直接加上一个@语法糖，就将outer作为了装饰器。这个装饰器的作用就等同于old = outer(old)。那让我们打印看看结果： 123456old()---我是外函数中的内函数1我是一个普通的函数我是外函数中的内函数2 那我们现在完成了装饰器的用法，按照定义，我们在不改变old函数的代码，且保持了old函数调用方法不变的情况下，增加了新的方法outer。 old函数经过outer装饰器进行了装饰，代码和调用方法不变，但是函数的功能发生了改变。 你是不是这个时候又有疑问了，那装饰器要用在什么地方呢？让我们来实现一个应用： 装饰器应用：统计函数的执行时间 在正式写代码之前，我还是习惯带着大家先思考一遍。我们需要统计函数的执行时间，那我们需要什么关键点？ 开始时间， 结束时间 开始时间和结束时间之间，就是程序在运行的过程。 好的，让我们来开始写代码，先来一段简单的需要运行的程序，为了能顺利统计时间，我们给它设定两个东西，一个循环，一个停止运行时长。这样，我们不会因为程序运行过快而看不到结果： 1234567import time# 定义一个普通函数def func(): for i in range(5): print(i, end=&quot; &quot;) time.sleep(1) 函数写完之后，让我们来执行一下看看： 1234func()---0 1 2 3 4 没问题，确实是一秒打印一次。 现在，再来让我们完成要称为装饰器的统计函数： 12345678# 定义一个统计函数执行时间的装饰器def runtime(f): def inner(): start = time.perf_counter() f() end = time.perf_counter() - start print(f'\\n函数的调用执行时间为：{end}') return inner 在函数inner中，我们最终是打印了最终的时间end， 然后将整个inner函数返回。 那么，让我们来尝试执行一下看看吧： 123456func = runtime(func)func()---0 1 2 3 4 函数的调用执行时间为：5.017943917075172 这样，我们就得到了func这个函数最后执行的时间，那现在的问题是，统计时间的函数是一个通用函数，我们很多函数中都需要用到它进行统计。但是我们总不能所有的函数都要用这种方法重新赋值之后再调用吧？ 那我们就用装饰器来解决就好了： 123456789101112# 定义一个普通函数@runtimedef func(): for i in range(5): print(i, end=&quot; &quot;) time.sleep(1) func()---0 1 2 3 4 函数的调用执行时间为：5.017888417001814 当然，最终这种函数的调用执行时间并不会像现在这样打印到前台，而是会写进log变为日志存储起来，便于之后分析使用。 装饰器嵌套语法 在这一段代码中，我们来约个妹子，完成一场约会。从哪开始呢？就从找妹子要微信开始吧： 1234567891011121314151617def begin(f): def begin_inner(): print('找妹子要微信，成功...') f() print('送妹子回家...') return begin_inner @begindef love(): print('跟妹子畅谈人生和理想...')love()---找妹子要微信，成功...跟妹子畅谈人生和理想...送妹子回家... 那这样，我们实现了一段最普通装饰器的定义。现在让我们在下面再定义一个装饰器函数，为什么呢？因为我渐渐不满足于只谈理想和人生了，要有点实际的行动了,顺便，我们写了一个列表，把和妹子要做的事情都列了个顺序，再来看看： 123456789101112131415161718192021222324252627def begin(f): def begin_inner(): print('找妹子要微信，成功... 1') f() print('送妹子回家... 5') return begin_innerdef evolve(f): def evolve_inner(): print('和妹子一起吃了个大餐.. 2') f() print('和妹子看了一场夜场电影... 4') return evolve_inner@evolve@begindef love(): print('跟妹子畅谈人生和理想... 3')love()---和妹子一起吃了个大餐.. 2找妹子要微信，成功... 1跟妹子畅谈人生和理想... 3送妹子回家... 5和妹子看了一场夜场电影... 4 这个...顺序似乎不太对啊。让我们改变一下装饰器的顺序试试： 12345678910111213@begin@evolvedef love(): print('跟妹子畅谈人生和理想... 3')love()---找妹子要微信，成功... 1和妹子一起吃了个大餐.. 2跟妹子畅谈人生和理想... 3和妹子看了一场夜场电影... 4送妹子回家... 5 这回没错了，我们在最开始要妹子微信和最后送妹子回家中间，又进行了点什么。也算是有些进展了。那么，我们怎么去理解这个程序运行顺序呢？ 先使用离得最近的begin装饰器，装饰love函数，返回了一个begin_inner函数 在使用上面的evolve, 装饰了上一次返回的begin_inner函数，又返回了一个evolve_inner函数。 在调用完成之后，就是需要顺序执行了，其执行的嵌套关系和顺序如下： 那这，就是我们嵌套装饰器的用法。当然，这种嵌套装饰器的用法并不常见，可是一旦我们遇到了，要理解他的运行机制和顺序，避免不必要的麻烦。 装饰带有参数的函数 上一个部分，我们做了一个约会妹子的函数，并且使用装饰器进行了装饰。使的我们成功的按照进度依次执行了自己的计划。 但是问题来了，我们到目前为止约会了那么多妹子，都不知道谁是谁（海王体质），这可怎么办。这次，我们吸取教训，先要名字，既然之前的流程很成功，我们直接拿来使用就行了。但是繁杂的步骤我们都去掉，直奔主题： 123# 带有参数的函数def love(name): print(f'跟{name}妹子在___畅谈人生...') 一个简单的函数，并且是一个填空题，你愿意待谁去哪里畅谈人生，随便。比如我，找「露思」去，去了哪里，恕不奉告了： 1234love('露思')---跟露思妹子在___畅谈人生... 可是即便如此，该有的流程还是不能丢，总不能凭空变出个妹子吧，还是得把必要的流程加上，原本定义的装饰器函数似乎不能使用了： 1234567891011# 定义装饰器def outer(f): def inner(): print(f'找到妹子，成功的拿到了微信...') f() print(f'约妹子去看一场午夜电影...') return inner@outerdef love(name): print(f'跟{name}妹子在___畅谈人生...') 流程上现在是没问题了，可是我们执行一下发现，报错了。 1234love('露思')---TypeError: outer..inner() takes 0 positional arguments but 1 was given 进行不下去了吧？那没办法，谁叫你之前和之后都把人家名字忘了呢，海王也得有点职业道德才行。既然我们在执行的时候有参数，那你整个过程中都得带上才行。不能玩着玩着忘记人家名字，对吧。让我们改进一下，既然我们已经知道，在使用装饰器装饰过后的love()执行实际上是执行装饰器inner， 那我们尝试给inner加上参数进行传递，还有很重要的，我们之前和之后，得把妹子名字记清楚才行，所以执行的时候也记得加上： 12345678910111213141516171819# 定义装饰器def outer(f): def inner(var): print(f'找到{var}妹子，成功的拿到了微信...') f(var) print(f'约{var}妹子去看一场午夜电影...') return inner# 带有参数的函数@outerdef love(name): print(f'跟{name}妹子在___畅谈人生...')love('露思')---找到露思妹子，成功的拿到了微信...跟露思妹子在___畅谈人生...约露思妹子去看一场午夜电影... 嗯，这样一场和「露思」妹子之间完美的从认识到约会流程就完成了。我们总结一下： 如果装饰器带有参数的函数，需要在内函数中定义形参，并传递给调用的函数。因为调用原函数等于调用内函数。 装饰多参数的函数 上一个流程跑完之后呢，我觉得还是不太妥当。主要是其中有两个选择题，一个是谁，一个是去哪里。对吧。再说了，我也得跟妹子自我介绍一下，加强一点印象。 好，我们这次再多设置几个参数，让整个约会过程更完善一些，那我们从最初就要规划一下，需要的参数包括：我，妹子，地点，行为等等。还蛮多的。 1234567891011121314# 装饰带有多参数的函数def outer(f): def inner(man,name,*args,**kwargs): print(f'{man}要到了{name}妹子的微信...') f(man, name, *args, **kwargs) print('天色渐晚...') return inner# 定义多参数的函数@outerdef love(man, name, *args, **kwargs): print(f'{man}跟{name}畅谈人生...') print(f'带{name}妹子去吃了很多美食：', args) print(f'和{name}妹子看了夜场电影:', kwargs) 这样我们就定义好了，至于*args以及**kwargs是什么，可以翻看之前的教程。 让我们现在来执行一下： 12345678love('茶桁','露思', '火锅', '海鲜', '饭后甜点', mov='封神第一部')---茶桁要到了露思妹子的微信...茶桁跟露思畅谈人生...带着露思妹子去吃了很多美食： ('火锅', '海鲜', '饭后甜点')和露思妹子看了夜场电影: {'mov': '封神第一部'}天色渐晚... 这样，多道选择题就被我们一一的化解了。相信「露思」妹子对我们的整体安排也是相当的满意了。 带有参数的装饰器 在我们平时使用Python各种第三方库的时候，不可避免的会遇到带有参数的装饰器。比如说，Django框架中的@login_required(redirect_field_name=\"my_redirect_field\")。 这种带有参数的装饰器是干嘛的呢？还是拿我们之前的海王约会流程来举例。之前的流程是都没有什么问题，可是有没有发现，所有事情都是我们自己做主了，似乎妹子一直都没有反对过，也没有说自己想要什么。这是不是不太符合现实？ 没错，我们也要给妹子装上一个会思考的大脑，也要学会做判断，好，让我们来实现一下： 既然我们这节是学习带有参数的装饰器，那么必然装饰器上是带参数的。呃，不要认为这句话是废话，我们看代码： 123@put(var)def love(): print('畅谈人生...') 就像这样，我们给装饰器加上了参数。 那么现在问题就来了，我们来看看我们之前写的装饰器函数： 1234def outer(f): def inner(): pass return inner 发现有什么问题了么？虽然我们的外层函数outer是有形参的，但是我们之前的过程中了解到，这个形参f是为了接收当前执行函数的。那还有什么其他地方接收非函数的普通参数嘛？ 既然outer中很重要的作用，除了接收函数在内函数内执行，还有一个就是返回inner内函数，那么我们在不改变outer的基础之上，再加一个接收普通参数的函数不就行了。 123456789101112131415161718192021def put(var): def outer(f): def inner1(): print('妹子给了你微信') def inner2(): print('妹子给了你她闺蜜的微信') def inner3(): print('妹子送了你一句感人肺腑的话：滚...') # 装饰器壳的参数，可以用于在函数内去做流程控制。 if var == 1: return inner1 elif var == 2: return inner2 else: return inner3 return outer@put(2)def love(): print('畅谈人生...') 定义完成之后，我们来执行一下看看： 1234love()---妹子给了你她闺蜜的微信 家人们谁懂啊，妹子真把我当海王了嘛？最后，我还是老老实实的接受了妹子的好意。 从整段代码中我们可以看出来，如果装饰器中有参数，需要有一个外壳函数来接收参数，传参之后就会进入到下一层函数中，并且传递当前对象。再然后才会再进入下一层中去。当然，我们在这里，利用传递的参数写了一段if判断，用于确定妹子的决定是什么。然后我们就返回哪个决定的函数。最后别忘记，在外壳函数中，我们还需要讲outer函数返回出去。此时虽然love函数是outer函数，但是在之前，put装饰器已经将参数传递给了外壳函数put(var)。在装饰器函数的争端代码中，我们都没有再执行过传进来的参数，也就是函数love()， 所以此段代码中love函数中的打印方法并未执行。 其执行步骤为： 1put(var) =&gt; outer() =&gt; outer(love) =&gt; inner2() 用类装饰器装饰函数 之前我们所有的代码中，装饰器一直使用的都是函数装饰器。那我们能否用类来当装饰器装饰函数呢？ 试试不就知道了。 12345678910111213# 类装饰器装饰函数class Outer(): def __call__(self, func): self.func = func return self.inner def inner(self, who): print('拿到妹子的微信...') self.func(who) print('看一场午夜电影...')@Outer()def love(who): print(f'{who}和妹子谈理想与人生...') 写完了，这下我们省略了那么多杂七杂八的流程，因为我发现，那么多流程下来，最终感动的人只有自己。妹子愿意，怎么都愿意，不愿意的，无论做多少都不愿意。 来，让我们跑一下程序试试： 123456love('茶桁')---拿到妹子的微信...茶桁和妹子谈理想与人生...看一场午夜电影... 没问题，正确的执行。那这个时候的love函数到底是什么呢？我们来打印出来看看： 1234print(love)---&lt;bound method Outer.inner of &lt;__main__.Outer object at 0x106646c80&gt;&gt; 可以看到，此时的love函数就是Outer类中的inner函数。那我们怎么理解整个代码呢？ 我们在love函数上使用了装饰器Outer, 那么这个时候Outer就会实例化出来一个对象obj，然后这个@obj就等同于obj(love)。 然后我们实例化对象进入Outer()内部，进入之后遇到了魔术方法__call__, 它会把该类的对象当作函数调用时自动触发。也就是obj()触发。 还记得类的实例化么？会传入一个参数，也就是实例化对象本身：obj。并且，第二个参数func用来接收了传递进来的函数love， 设置了self.func = func, 把传进来的函数作为对象的成员方法。最后返回了一个函数inner， 这个返回的函数是类中定义好的，于是作为实例化对象也将这个成员方法继承了下来，所以self.inner可以直接被返回出去。 这个定义好的inner接收了两个形参，一个是实例化对象本身，一个就是传递进来的函数love的参数who。然后，中间执行了一下魔术方法__call__内定义好的self.func(who)，实际上也就是obj.love(who)。 看着迷糊？这样，我写一个注释过的完整版本。 1234567891011121314151617181920212223242526272829# 类装饰器装饰函数class Outer(): # 魔术方法：当把该类的对象当作函数调用时，自动触发obj() def __call__(self, func): # 把传进来的函数作为对象的成员方法 self.func = func # 返回一个函数 return self.inner # 在定义的需要返回的新方法中，去进行装饰和处理 def inner(self, who): print('拿到妹子的微信...') self.func(who) print('看一场午夜电影...')'''Outer() 实例化对象 =&gt; obj@obj 就等于 obj(love)进入类后 =&gt; __call__(love)接收返回参数`inner()`'''@Outer()def love(who): print(f'{who}和妹子谈理想与人生...')# inner('茶桁')love('茶桁')# 此时的love就是属于`Outer`类这个对象中的inner方法print(love) 不知道这段注释代码加上刚才的解说，大家能否看懂？有没有发现，用类做装饰器比起函数装饰器反而更清晰一点？不需要写那么多外层函数和内层函数。 让我们继续... 类方法装饰函数 刚才我们将整个类都用作了一个装饰器，那我们思考一下，是不是我们还可以用类中的方法来做装饰器呢？ 说干就干，直接上代码测试： 12345678910111213141516171819202122232425262728# 用类方法装饰函数class Outer(): def newinner(f): # 把传递进来的函数定义为类方法 Outer.func = f # 同时返回一个新的类方法 return Outer.inner def inner(): print('拿到妹子微信...') Outer.func() print('看一场午夜电影...')'''Outer.newinner(love) 接收返回参数：Outer.inner'''@Outer.newinnerdef love(): print('和妹子谈谈人生喝喝茶...')# love() 等于 Outer.inner()love()---拿到妹子微信...和妹子谈谈人生喝喝茶...看一场午夜电影... 在经历了几场约会之后，我们的耐心也渐渐没了。连是谁都不管，也没耐心去谈理想了。喝点茶聊聊天就直奔主题了都是。 到目前为止以上所有形式的装饰器，包括「函数装饰器」、「类装饰器」、「类方法装饰器」，都有一个共同特点：都是在给函数去进行装饰，增加功能。 那我们这个时候就不满足了，既然能装饰函数，那是否也能装饰类呢？ 用装饰器装饰类 还真有一种装饰器是专门装饰类的，也就是在类的定义的前面使用@装饰器这种语法。和装饰函数并无什么区别，只是放在了类前面而已： 123@装饰器class Demo(): pass 装饰器给函数进行装饰，目的是不改变函数调用和代码的情况下给原函数增加新的功能。 装饰器给类进行装饰，目的是不改变类的定义和调用的情况下给类增加新的成员（属性或者方法）。 来，让我们具体的看看： 函数装饰器装饰类 123456789101112131415161718192021222324# 定义函数，接收一个类。返回修改后的类def expand(cls): def func2(): print('我是在装饰器中追加的新方法，func2') cls.func2 = func2 # 把刚才定义的方法赋值给 类 cls.name = '我是在装饰器中追加的新属性 name' # 返回时，把追加类新成员的类返回去 return cls@expand # expand(Demo) ==&gt; cls ==&gt; Democlass Demo(): def func(): print('我是Demo类中定义的func方法')Demo.func() # 此时在调用的Demo类是通过装饰器，更新过的Demo类Demo.func2()print(Demo.name)---我是Demo类中定义的func方法我是在装饰器中追加的新方法，func2我是在装饰器中追加的新属性 name 这样，我们在原来的Demo这个类中，使用装饰器增加了一个成员方法func2，并且增加了一个成员属性name， 并最终返回到原始类中。从而扩展了这个原始类Demo中的方法和属性。 类装饰器装饰类 123456789101112131415161718192021222324252627282930# 使用类装饰器装饰类class expand(): def __call__(self, cls): # 把接收的类，赋值给当前对象，作为一个属性 self.cls = cls # 返回一个函数 return self.newfunc def newfunc(self): self.cls.name = '我是在类装饰器中追加的新属性 name' self.cls.func2 = self.func2 # 返回传递进来的类的实例化结果，obj return self.cls() def func2(self): print('我是在类装饰器中追加的新方法 func2')'''expand() ==&gt; obj ==&gt; @obj(Demo) ==&gt; __call__(Demo) ==&gt; newfunc'''@expand() class Demo(): def func(self): print('我是Demo类中定义的func方法')obj = Demo() # Demo() ==&gt; newfunc() ==&gt; objobj.func()obj.func2()print(obj.name) 在之前那么多案例过后，相信大家这一段代码应该能看的出来吧。 那我这个地方要处一个思考题了，请问：此时的 obj这个对象，是哪个类的对象。Demo还是expand? 1234print(obj)---??? 这个问题的答案，我放在源码中了，大家要记得思考之后再去看答案。 那么，本节课的内容到这里也就结束了。 课程进行到这里，我们Python本身的所有内容就已经介绍完了。下节课开始，我们就要考试讲第三方库。 好，下课。","link":"/Decorator-syntax-and-application/"},{"title":"25. matplotlib","text":"Hi, 大家好。我是茶桁。 在上一节课中，我们结束了Python正式的所有内容，但是咱们的Python课程还未结束。从这节课开始，我们要来学习一下Python的第三方库。 Python的生态非常完善也非常活跃，我们不太可能讲目前所有的第三方库全部都介绍一遍，只介绍几个有影响力并且和处理数据相关的。那今天第一Part，我们就先来学习matplotlib。 在之后的课程中，Python的基础理论我就不会细讲了，咱们重点是要快速的认识和学会第三方库。有什么语法上的问题，可以翻看前面几节的教程。 matplotlib是什么？ Matplotlib是一个Python 2D绘图库，它可以在各种平台上以各种硬拷贝格式和交互式环境生成出具有 出版品质的图形。 Matplotlib可用于Python脚本，Python和IPython shell，Jupyter笔记本，Web应用 程序服务器和四个图形用户界面工具包。 Matplotlib试图让简单的事情变得更简单，让无法实现的事情变得可能实现。 只需几行代码即可生成绘图，直方图，功率谱，条形图，错误图，散点图等。 为了简单绘图，pyplot模块提供了类似于MATLAB的界面，特别是与IPython结合使用时。 对于高级用户，您可以通过面向对象的界面或MATLAB用户熟悉的一组函数完全控制线条样式，字体属性，轴属性等。 那么，我们为什么要学习matplotlib呢？ 可视化是在整个数据挖掘的关键辅助工具，可以清晰的理解数据，从而调整我们的分析方法。 能将数据进行可视化,更直观的呈现 使数据更加客观、更具说服力 例如下面两个图为数字展示和图形展示： 以上两个图形中，第一组是完全的数据。我们基本很难看出这组数据到底谁大谁小，当然，从位数上我们还是可以比较容易辨认，但是比起第二张图呢？是不是第二张图就非常清晰的展示了数据的大小，一目了然？ 那既然我们要学习的是数据可视化，我们首先要做的，必定是要先了解一下常见数据图表，知道其种类和意义。 常见图形种类及意义 我们首先要先了解具体的图形，才能知道我们在什么情况下使用什么图形来表示。 折线图：以折线的上升或下降来表示统计数量的增减变化的统计图。特点是：能够显示数据的变化趋势，反映事物的变化情况。(变化) 散点图：用两组数据构成多个坐标点，考察坐标点的分布,判断两变量之间是否存在某种关联或总结坐标点的分布模式。特点是：判断变量之间是否存在数量关联趋势,展示离群点(分布规律) 柱状图：排列在工作表的列或行中的数据可以绘制到柱状图中。特点是：绘制连离散的数据,能够一眼看出各个数据的大小,比较数据之间的差别。(统计/对比) 直方图：由一系列高度不等的纵向条纹或线段表示数据分布的情况。 一般用横轴表示数据范围， 纵轴表示分布情况。特点是：绘制连续性的数据展示一组或者多组数据的分布状况(统计) 饼图：用于表示不同分类的占比情况，通过弧度大小来对比各种分类。特点是：分类数据的占比情况(占比)。 matplotlib画图实现 首先我们要知道，Python的第三方库几乎全部都需要额外安装才行。我们之前在讲Python环境的时候有提到如何创建虚拟环境以及如何安装第三方库。那我们现在，就来直接安装一下，还记得么？我用的环境是conda， 所以我的安装命令都是使用conda的，不过你将其咱们课程中还是要使用最普遍的方式，所以以下我都会替换成pip。 1pip install motplotlib 执行完毕后，motplotlib就安装到您的Python环境内了。 建议在Python环境内安装好Jupyter来进行学习，你会发现简直太方便了。如果是在Jupyter内，那么会在代码执行之后的下方直接显示出结果，而如果我们是使用python xx.py来执行，那Python会调用内部的绘图器来进行显示。 Jupyter for vscode Python绘图器 当然，你也可以自己在命令行内起一个Jupyter notebook服务，那就可以直接在浏览器上进行操作了。 Jupyter Notebook 当然几种方法中，我还是最推崇在VSCode中进行。毕竟我们还是需要代码提示的。 在正式开始之前，让我们对matplotlib的图像结构建立一个认识： 现在让我们来具体的实现一下，做一个简单的图形： 12345678# 导入模块import matplotlib.pyplot as plt# 这段代码会让之后的代码在Jupyter内执行的时候显示图片%matplotlib inline# 传入x,y, 通过plot画图plt.plot([1,0,9],[4,5,6])# 在执行程序的时候显示图形plt.show() 在这段代码中，我们使用plot来进行了绘制，x和y分别是plot的两个参数，代表了x轴和y轴。那么这两个轴分别接受了一个列表，那就是有三个点，第一个点是(1,4), 第二个点是(0,5)， 第三个点是(9,6)。最后，我们使用show()函数来进行最终呈现。 绘图折线图 首先，我们来绘制一个折线图，这次我们用变量存储数据的方式： 1234567# 绘制折线图import matplotlib.pyplot as pltx = range(1, 8) # x轴的位置y = [17, 17, 18, 15, 11, 11, 13]# 传入x,y，plot绘图plt.plot(x, y)plt.show() 然后，我们对这个折线图进行一下设置，修改颜色和形状： 1234567# 绘制折线图import matplotlib.pyplot as pltx = range(1, 8) # x轴的位置y = [17, 17, 18, 15, 11, 11, 13]# 传入x,y，plot绘图plt.plot(x, y, color='red', alpha=0.5, linestyle='--', linewidth=3)plt.show() 我们对plot函数传递了几个参数修改了折线的样式，其中color是折线的颜色，alpha是折线的透明度（0-1）， linestyle是折线的样式， linewidth是折线的宽度。 linestyle的几个值分别是：-实线(solid)，这个也是默认值；--短线(dashed)； -.短点相间线(dashdot)； :虚线点(dotted)。 关键点样式 让我们接着进行修改这段折线样式： 1234...# 传入x,y，plot绘图plt.plot(x, y, color='red', alpha=0.5, linestyle='--', linewidth=3, marker='o')plt.show() 我们这次只增加了一个参数，很明显，marker就是关键点的样式。 折线和关键点到底有哪些值呢？我们看一下下面这个表： 值 描述 - solid line style -- dashed line style -. dash-dot line style : dotted line style . point marker , pixel marker o circle marker v triangle_down marker ^ triangle_up marker &lt; triangle_left marker &gt; triangle_right marker 1 tri_down marker 2 tri_up marker 3 tri_left marker 4 tri_right marker s square marker p pentagon marker * star marker h hexagon1 marker H hexagon2 marker + plus marker x x marker D diamond marker d thin_diamond marker | vline marker _ hline marker 如果看不懂描述的小伙伴，最直接的办法就是放到代码里直接运行一下看看。 当然，我们还可以改变关键点的大小等参数： 1plt.plot(x, y, color='red', alpha=0.5, linestyle='--', linewidth=3, marker='o', markersize='10',markeredgecolor='blue', markeredgewidth=3) 那这里面，markersize是表示关键点的大小，markeredgecolor是关键点边框的颜色，markeredgewidth就是关键点边框的宽度。 既然图片渲染出来了，那我们总是需要进行保存的。那么下面，我们就看看如何将图片保存下来，在保存之前，我们还会根据需要设置一下图片的大小。 设置图片大小和保存 1234567891011from matplotlib import pyplot as pltimport randomx = range(2, 26, 2) # x轴的位置y = [random.randint(15, 30) for i in x]# 设置图片大小plt.figure(figsize=(20, 8), dpi=80)plt.plot(x,y)# plt.show()# 保存plt.savefig('./data/img/t1.png') 我们依次来看这段代码，里面有我们认识的也有我们不认识的。其中的random是为了生成随机数，这个我们就先不管了。直接看设置部分。 figsize这参数是为了指定figure的宽和高，单位为英寸。 dpi参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为80，1英寸等于2.5cm, A4纸为21*30cm的纸张。 然后我们继续往下看，savefig就是指定目录进行保存。这里我们需要注意两点： 如果保存的目录有路径不存在，则会报错无法保存。 我们需要savefig的时候尽量不要使用show方法，因为savefig也具备了展示图片的功能，并且，如果show存在的话，在展示完图片之后，会释放figuer资源，那么savefig保存下来的图片将会是空白的。就好比我们open('file', 'a+') as fp一个文件，在使用fp.write的时候没有往里面写入内容。因为这些内容被上面一个方法清空了，但是文件我还是会保存的，只是文件内没有任何内容。 然后我们回过头来继续看我们写的这段代码，其实savefig能存储的文件格式很多，包括能够存储svg格式的矢量图。只需要plt.savefig('./t1.svg')，保存的时候换一下后缀名就可以了。 现在让我们看下保存好的图片： 绘制轴上的刻度 有没有发现，虽然我们折线图是正常的，但是似乎x轴上的刻度区间太大了，并不是所有关键点都明显展示出来了。那现在我们就来设置一下x轴和y轴的刻度。 123456789...# 设置x轴的刻度plt.xticks(x)# # 设置y轴的刻度plt.yticks(y)# 设置图片大小plt.plot(x,y)plt.show() 这段代码中我们将保存文件的代码去掉了。因为主要是进行设置，所以我们展示一下看看正确与否就行了。 我们看到现在的图片，刻度上x轴遵循了我们之前对x的设定，(2, 26, 2)， 从2开始，到25， 并且步进值为2.y轴呢？因为是随机数，所以关键点的分布并不均匀。 这个时候，我就又需要进行修改了，一个是y轴的刻度要分配的更均匀，再有就是x轴上，我希望区间为1，而不是2。让我们来对其进行下修改： 123456789...# 设置x轴的刻度plt.xticks(range(1, 25))# 设置y轴的刻度plt.yticks(range(min(y), max(y)+1))# 设置图片大小plt.plot(x,y)plt.show() 似乎图不太一样，原因是因为我在代码中使用的是随机数函数random.randint来差生y轴的数据，所以每次生成的图片都会有些不同。 我们来好好看看轴线上的刻度。确实和关键点都对应上了。并且比起关键点来说更密集一点。原因就在于，我们将x的刻度点范围改为(1, 25)，无步进值。y轴在绘制的时候也做了定义，范围设置为(最小的y值, 最大的y值+1)，同样，也是没有步进值。这样，两个轴上的刻度分布就非常均匀了。 这里的关键知识点就是：我们可以使用xticks和yticks来生成x轴刻度或者y轴刻度，并且，在其中可以传递参数来对x轴上的刻度和y轴上的刻度进行定义。 不过我们现在这个还是无法满足需求，原因就在于我们这个折线图是为了显示不同时间点上的温度变化。那么我们就必须要让x轴显示时间，而y轴显示温度。 让我们继续修改一下,这里我们就只展示轴线代码的修改： 12345678# 构造x轴刻度标签x_ticks_label = [&quot;{}:00&quot;.format(i) for i in x]# 让字旋转45度plt.xticks(x, x_ticks_label, rotation=45)# 构造y轴的刻度标签y_ticks_label = [&quot;{}℃&quot;.format(i) for i in range(min(y), max(y)+1)]plt.yticks(range(min(y), max(y)+1), y_ticks_label) 最后生成的图片： 这里，我们使用了x_ticks_label来设置了x轴的刻度上显示的信息。当然，y轴也是相同的方式。然后将label传入轴刻度生成方法xticks中进行刻度生成，在生成的时候，我们还使用了xticks的参数rotation设置为45来完成了label的旋转。 设置显示中文 不过这并未结束，matplotlib默认是只显示英文的，无法显示中文。但是我们无论是刻度，图标题，很多时候都必须显示中文。该怎么办呢？ 接下来就让我们来看看如何修改matplotlib的默认字体。这一段，我们重新写一个需求，来看看2个小时内每分钟跳动变化。 1234567891011121314151617181920from matplotlib import pyplot as pltfrom matplotlib import font_managerimport randomx = range(0, 120)y = [random.randint(10, 30) for i in range(120)]plt.figure(figsize=(20, 8), dpi=80)plt.plot(x,y)# 设置字体和Labelmy_font = font_manager.FontProperties(fname='/System/Library/Fonts/PingFang.ttc', size=18)plt.xlabel('时间', fontproperties=my_font)plt.ylabel('次数', fontproperties=my_font)# 设置标题plt.title('每分钟跳动次数', fontproperties=my_font, color='red')plt.show() 我们引入了font_manager，然后利用它设置了我们需要用到的字体（必须是中文字体）给到一个变量my_font内。最后在设置label的时候，将字体设置为这个变量。 这样，我们就完成了中文字体的显示。 作为对比，我们来看看这样设置的是什么样： 123456...plt.xlabel('时间')plt.ylabel('次数')plt.title('每分钟跳动次数',color='red')plt.show() 一图多线 大多数时候，我们的一张图表上可能不仅需要一条线。而是两条线相互交错。这就形成了两组数据的对比，我们打个比方来说：我们正在和一位同事竞争销售额，需要查看去年（2022年）全年的数据对比： 12345678910111213141516171819202122232425# 销售额数据对比y1 = [20,10,10,20,40,30,40,40,50,60,50,40]y2 = [10,30,20,30,40,20,10,30,30,80,30,20]x = range(1, 13)# 设置图形plt.figure(figsize=(20, 8), dpi=80)plt.plot(x, y1, color='orange', label=&quot;茶桁&quot;)plt.plot(x, y2, color='green', label='同事')# 设置x轴刻度xtick_labels = ['{}月'.format(i) for i in x]my_font = font_manager.FontProperties(fname='/System/Library/Fonts/PingFang.ttc', size=24)plt.xticks(x, xtick_labels, fontproperties=my_font, rotation=45)# 绘制网格plt.grid(alpha=0.4) #网格也可以设置样式，这里透明度为0.4# 添加图例(注意：只有在这里需要添加prop参数是显示中文，其他的都用fontproperties)# 设置位置loc : upper left、 lower left、 center left、 upper centerplt.legend(prop=my_font, loc='upper right')plt.show() 我们在代码中设置两两组数据，分别为y1, y2。这两组数据一共12个，对应了12个月份。然后我们在plot方法中设置了线条的颜色，设置了这条线条对应的数据，并且添加了label。意在对这个线条写个说明。 在之后，我们添加了grid网格，意图让线条上的关键点更明显。 最后使用legend来完成plot中设置的label的显示，并在其中设置了显示所用字体。这里需要注意的是，在legend方法中设置字体所用的形参是prop而非fontproperties。在最后，我们使用loc设置了这两个label显示的位置，其中的关键字upper right代表的是两个方位「上，右」，来确定显示位置为右上角。 最后，我们绘制的图片显示如图： 从图上能明显看出来，我的销售数据是在稳步上升的，而同事起伏比较大。大部分时候我占优势，可是旺季时顶峰数据同事比我高很多，所以说基本上是各有千秋。 基于折线图，我们再来看几个拓展的部分。 首先，我们在绘图的时候，实际上是支持多个坐标系绘制在一张图上的。这也经常是数据图对比经常用到的方式： 123456789101112131415161718# 多个坐标系子图 add_subplot方法，给figure新增子图import matplotlib.pyplot as pltimport numpy as npx = np.arange(1, 100)fig = plt.figure(figsize=(20, 10), dpi=80)# 子图1ax1 = fig.add_subplot(2,2,1)ax1.plot(x,x)# 子图2ax2 = fig.add_subplot(2,2,2)ax2.plot(x, x**2)ax2.grid(color='r', linestyle='--', linewidth=1, alpha=0.3)# 子图3ax3 = fig.add_subplot(2,2,3)ax3.plot(x, np.log(x))plt.show() 我们利用add_subplot方法，在一个figure上添加了三个子图。 其次，有一些时候，我们需要对坐标轴范围进行设定。 123456789101112131415# 设定坐标轴范围x = np.arange(-10, 11, 1)y = x ** 2plt.plot(x,y)# plt.xlim([-5,5])# 单边调整# plt.xlim(xmin=-4)# plt.xlim(xmax=4)plt.ylim(ymin=0)plt.xlim(xmin=0)plt.show() 在这段代码中，我展示了三个调整范围的方式。 第一个是使用数组划定范围来进行调整。 第二个方式是分别设定一边的值（最小值或者最大值）。 最后一个方式是只设定x轴和y轴的最小值。下图展示的是第三个方式绘制的图： 当然，坐标轴并不会是一成不变的。有的时候我们可能需要y轴在x轴的正中间。所以我们需要改变坐标轴的默认显示方式 我们先来看看原本的图默认样式是什么样： 1234567# 改变坐标轴的默认显示方式y = range(0, 14, 2)x = [-3, -2, -1, 0, 1, 2, 3]plt.figure(figsize=(20,8), dpi=80)plt.plot(x,y)plt.show() 然后我们对这张图进行修改，获取图像之后设置四周边线，并且移动底边，移动到y轴的0位置 1234567891011121314151617181920# 改变坐标轴的默认显示方式y = range(0, 14, 2)x = [-3, -2, -1, 0, 1, 2, 3]plt.figure(figsize=(20,8), dpi=80)# 获得当前图表的图像ax = plt.gca()# 设置图形四周的边线ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.spines['bottom'].set_color('blue')ax.spines['left'].set_color('red')# 设置底边的移动范围，移动到y轴的0位置，`data`: 移动轴的位置到交叉轴的指定坐标ax.spines['bottom'].set_position(('data', 0))ax.spines['left'].set_position(('data', 0))plt.plot(x,y)plt.show() 代码中，我们使用gca()获取了图像赋值给变量ax，然后对其进行修改，spines可以修改四周边线，包括颜色和位置等。分别设置完颜色之后，我们分别对底边和左边使用了set_position进行了位移。移动到0点位置。那其实，底边对应的就是x轴，左边对应的就是y轴。 绘制散点图 我们拿到了一组数据，就是今年3月份每天的最高气温，现在我想在图表上进行展示。为了展示气温的分布，我们准备使用散点图进行展示。 12345678# 绘制散点图y = [11,17,16,11,12,11,12,6,6,7,8,9,12,15,14,17,18,21,16,17,20,14,15,15,15,19,21,22, 22,22,23]x = range(1,32)plt.figure(figsize=(20,8),dpi=80)# 使用scatter绘制散点图plt.scatter(x, y, label='3月份')plt.show() 本来这样就已经完成绘制了。不过我们看一下图表，虽然散点是绘制完成了，但是整张图上我们看不出太多信息，包括刻度值，月份等等，另外，我记得咱们之前加过图例，现在我们都加上： 1234567891011121314151617181920from matplotlib import font_manager# 绘制散点图y = [11,17,16,11,12,11,12,6,6,7,8,9,12,15,14,17,18,21,16,17,20,14,15,15,15,19,21,22, 22,22,23]x = range(1,32)plt.figure(figsize=(20,8),dpi=80)# 使用scatter绘制散点图plt.scatter(x, y, label='3月份')my_font = font_manager.FontProperties(fname='/System/Library/Fonts/PingFang.ttc', size=12)_xticks_labels = ['3月{}日'.format(i) for i in x]plt.xticks(x[::3], _xticks_labels[::3], fontproperties=my_font, rotation=45)plt.xlabel('日期', fontproperties=my_font)plt.ylabel('温度', fontproperties=my_font)# 图例plt.legend(prop=my_font)plt.show() 以上代码中的内容，基本都是咱们之前学过的内容，我就不多做解释了。其中，我们可以看到，plt.之后的方法就是绘制不同的图形，之前我们看到的plot是折线图，这次我们学到scatter是散点图。在图形绘制之后，剩下的就是对其进行修饰和设置。 绘制条形图 今天刷到一条新闻，说目前2023年暑期档电影的票房基本已经定型，目前全部累计票房已经超过2019年称为历史最高暑期档，而最引人瞩目的是，国产电影全线压制好莱坞大片。这真是一个值的骄傲的事情。那现在，我们就来展现一下暑期档电影票房的对比吧,我们一步一步来，最开始当然是拿到数据。 本数据来源于猫眼，2023年8月20日14:00的实时数据，后期未下线的电影数据可能会有变化。 12a = ['消失的她','碟中谍7:致命清算','芭比','八角笼中','茶啊二中','热烈','长安三万里','巨齿鲨2:深渊','封神第一部','孤注一掷']b = ['35.22','3.46','2.47','21.92','3.66','8.69','17.56','7.53','22.36','26.48'] 然后我们就可以开始绘制柱状图了： 123plt.figure(figsize=(20,8), dpi=80)plt.bar(range(len(a)), [float(i) for i in b], width=0.3)plt.show() 现在图形是绘制出来了，但是完全无法让人满意。我们并不知道哪个柱子是哪个电影的，并且对比之下，我们只能看出大概高低，并不知道具体的票房。那我们现在来进行修改, 先加上x轴和y轴上的刻度标识，并且将y轴刻度范围放大。 12plt.xticks(range(len(a)), a, fontproperties=my_font)plt.yticks(range(0,51,5), range(0,51,5)) 目前的图我们是能看出谁是谁了，而且比起刚才看起来y轴上也舒服了很多，没有顶天立地。 接下来，我们继续修改。给柱子加上颜色，好做区分。并且将绘制的图形赋值给到变量rects，在下面我们好对每一根柱子上写一个数字，将票房数值写上去。这样，具体的票房我们就能一目了然。 1234567891011121314a = ['消失的她','碟中谍7:致命清算','芭比','八角笼中','茶啊二中','热烈','长安三万里','巨齿鲨2:深渊','封神第一部','孤注一掷']b = ['35.22','3.46','2.47','21.92','3.66','8.69','17.56','7.53','22.36','26.48']plt.figure(figsize=(20,8), dpi=80)rects = plt.bar(range(len(a)), [float(i) for i in b], width=0.3, color=['r','g','b','r','g','b','r','g','b','r'])plt.xticks(range(len(a)), a, fontproperties=my_font)plt.yticks(range(0,51,5), range(0,51,5))# 加标注(水平居中)for rect in rects: height = rect.get_height() plt.text(rect.get_x()+rect.get_width() / 2, height+1, str(height), ha='center')plt.show() 这段代码中其他的都好理解，就是加标注这一段。我们使用for从rects中分别获取到每一根柱子，然后设定了一个高，这个高度就是柱子的实际高度。再设定一个text给到这根柱子，text的高度为柱子的定位为每根柱子的x轴位置+其宽度的二分之一，高度为柱子高度+1， 字符串为柱子高度本身，设置水平居中。 来，让我们看看效果： 现在，我们的显示虽然还是不完美，但是已经非常清晰了。 当然，柱状图除了竖向的，还有横向，但是横向就不能称之为柱状图，而是条形图： 123456789101112131415# 横向条形图a = ['消失的她','碟中谍7:致命清算','芭比','八角笼中','茶啊二中','热烈','长安三万里','巨齿鲨2:深渊','封神第一部','孤注一掷']b = ['35.22','3.46','2.47','21.92','3.66','8.69','17.56','7.53','22.36','26.48']plt.figure(figsize=(20,8), dpi=80)rects = plt.barh(range(len(a)), [float(i) for i in b], height=0.5, color=['r','g','b','r','g','b','r','g','b','r'])plt.xticks(range(0,51,5), range(0,51,5))plt.yticks(range(len(a)), a, rotation=45, fontproperties=my_font)# 加标注(水平居中)for rect in rects: width = rect.get_width() plt.text(width+1, rect.get_y()+0.5/2, str(width)+' 亿', va='center', fontproperties=my_font)plt.show() 其方法和原理都和柱状图是一样的，不同的点就在于柱状图的方法是bar，横向条形图的方法为barh。另外，别忘了切换x,y轴。 在这里，恭喜《封神》破22亿，加油... 我们平时看到的柱状图或者条形图，一定不只是这样一根一根的，还有那种两根或两根以上并列的对吧？其形式也很简单，就是在当前的柱子旁边多加一根而已： 12345678910# 多条并列index = np.arange(4)BJ = [50,55,53,60]SH = [44,66,55,41]# 并列plt.bar(index,BJ, width=0.3)plt.bar(index+0.3, SH, width=0.3, color='green')plt.xticks(index+0.3/2, index)plt.show() 直方图 这个图形的学习，我们还是拿电影数据来做，但是这次我们不拿刚才用过的数据了，我们拿到一个250部电影的时长数据，现在我们要统计处这些电影时长的分布状态。（比如，市场为100分钟到120分钟的电影数量，出现频率等），我们该如何去做呢？ 来，让咱们尝试一下，还是老样子，先落位数据： 12345times = [131,98,125,131,124,139,131,117,128,108,135,138,131,102,107,114,119,128,121,142,127,130,124,101,110,116,117,110,128,128,115,99,95,138,117,111,78,132,124,113,150,110,117,136,126,134,95,144,105,126,130,126,130,126,116,123,106,112,138,123,99,136,123,117,119,105,137,123,128,125,104,109,134,125,127,105,120,107,129,116,108,132,103,136,118,102,120,114,105,115,132,145,119,121,112,139,125,138,109,132,134,156,106,117,127,144,139,139,119,140,83,110,102,123,107,143,115,136,118,139,123,112,118,125,109,119,133,112,114,122,109,106,123,116,131,127,115,118,112,135,115,146,137,116,103,144,83,123,111,110,111,100,154,136,100,118,119,133,134,106,129,126,110,111,109,141,120,117,106,149,122,122,110,118,127,121,114,125,126,114,140,103,130,141,117,106,114,121,114,133,137,92,121,112,146,97,137,105,98,117,112,81,97,139,113,134,106,144,110,137,137,111,104,117,100,111,101,110,105,129,137,112,120,113,133,112,83,94,146,133,101,131,116,111,84,137,115,122,106,144,109,123,116,111,111,133,150,134,76,104] print(len(times))---250 没问题，250个数据齐了。现在让我们来开始绘图： 123456789101112plt.figure(figsize=(20,8), dpi=80)# 设置组距distance = 2# 计算组距group_num = int((max(times) - min(times)) / distance)plt.hist(times, bins=group_num)plt.xticks(range(min(times), max(times))[::2])plt.grid(linestyle='--', alpha=0.5)plt.xlabel('电影时长大小', fontproperties=my_font)plt.ylabel('电影的数据量', fontproperties=my_font)plt.show() 从代码中看，直方图的方法是hist, 其中参数为需要展示的数据和组距。 x轴刻度上，我们将最小的电影时长和最大的电影时长顺序分布，步进值为2。 现在我们从图中能看到，110 ～ 112这个时间段的电影数量是最多的，其次就是116 ~ 118分钟的。 现在，我们更清晰的感受了i库55起1422 直方图的作用。 饼状图 最后我们来看看饼状图，饼状图相信大家平时看的也很多。基本上，我们遇到比例，份额对比的时候会使用这个图形。 来，让我们先绘制一个基本图形： 12345size = [55, 35, 10] # 各部分大小plt.figure(figsize=(20,8), dpi=100)plt.pie(size)plt.show() 现在让我们在这个基础的饼状图上进行设置和修改，首先，我们需要这个图形各个区域都有个名称，然后是我们自己设定下他们的颜色，而不是用默认的，接着，我们需要将其中一部分突出显示。 12345# 修改生成饼图代码plt.pie(size, explode=[0, 0.05, 0], colors = ['r','g','b'], labels = ['第一部分','第二部分','第三部分']) 现在是按照我刚才希望的去变化了，但是并不让人满意。我们需要将文字设置成中文显示，然后整个图形旋转一下，让突出的部分部要向下，接着我们希望显示出百分比，并且有一个图例，分别标识出颜色和区域的文字。 12345678910111213141516171819202122232425262728size = [55, 35, 10] # 各部分大小plt.figure(figsize=(20,8), dpi=100)label_list = ['第一部分','第二部分','第三部分'] # 各部分标签color = ['red', 'green', 'blue'] # 各部分颜色explode = [0, 0.05, 0] # 各部分突出值patches, l_text, p_text = plt.pie(size, explode = explode, colors = color, labels = label_list, labeldistance=1.1, autopct='%1.1f%%', shadow=False, startangle=150, pctdistance=0.6)for t in l_text: print(dir(t)) t.set_fontproperties(my_font)for t in p_text: t.set_size(18)for i in patches: i.set_color('pink') breakplt.legend(prop=my_font)plt.show() 我们在这段代码中，将之前在pie方法中的三组列表数据拿出来赋值给了三个变量，然后增加了一些其他参数用于标签距离、设置旋转、百分比距离等。 然后将生成的图赋值给到三个变量，patches, l_text, p_text，这三个变量分别接受的参数为「饼的区域」、「饼的外部说明文字」、「饼的内部标识文字」。 然后，对这三个变量分别进行设置。 最后，将图例的字体设置一下，然后显示出来。 总结 今天的课程到这里也就结束了。最后让我们来总结一下 最后，我们留个作业吧，好久没留作业了。将咱们之前做的这个饼图加上阴影，扩大分离的那块区域的分离距离。如图： 小伙伴们，记得认真学习并且完成作业。 好，下课。","link":"/AI-Python-matplotlib/"},{"title":"26. NumPy","text":"Hi，大家好。我是茶桁。 上一节课中，我们学习了matplotlib. 实际上，我们已经进入了数据可视化阶段。 可是在上一节课中，所有的数据都是我们固定写好的，包括两个电影的数据展示的案例（柱状图和直方图），都是我们将数据手动写成了数据列表，然后直接使用。 在我们平时的工作中，不太有那么多的机会使用现成的数据，除非你跟我一样是一个数据产品经理，那倒是会有程序员们会将数据整理好递到你手上。可是即便如此，很多时候我还是需要自己亲手去处理数据，因为我不能为了一次数据验证或者一个什么想法就去惊动程序员为你服务。 更何况，我们现在的课程面向的是人工智能，那么处理数据就成了必须要有的手段，也是家常便饭常有的事。 那么今天，我们就来学习一下Python中科学计算的基础第三方库：「numpy」。 简单介绍一下NumPy Numpy的全称是：Numerical Python, 是一个开源的Python科学计算库，用于快速处理任意维度的数组。 Numpy支持常见的数组和矩阵操作，对于同样的数值计算任务，使用Numpy比直接使用Python要简洁的多。其中的ndarray对象用于处理多维数组，该对象是一个快速而灵活的大数据容器。 相对于直接使用Python，NumPy具有一下优势： 对于同样的数值计算任务，使用NumPy要比直接编写Python代码便捷得多； NumPy中的数组的存储效率和输入输出性能均远远优于Python中等价的基本数据结构，且其能够 提升的性能是与数组中的元素成比例的； NumPy的大部分代码都是用C语言写的，其底层算法在设计时就有着优异的性能，这使得NumPy 比纯Python代码高效得多. 说那么多我们不如直接来一次对比显得更直接一点，让我们来计算100000000个数字的加法运算。 123456789101112131415161718192021222324import randomimport timeimport numpy as npa = []for i in range(100000000): a.append(random.random())t1 = time.time()# Python 处理sum_py = sum(a)t2 = time.time()b = np.array(a)t4 = time.time()# NumPy 处理sum_np = np.sum(b)t5 = time.time()print(f'Python:{t2-t1}, NumPy:{t5-t4}')---Python:1.782839059829712, NumPy:0.2144303321838379 在以上代码中，t2-t1为使用python自带的求和函数消耗的时间，t5-t4为使用numpy求和消耗的时间。我们看到了时间对比，是不是为NumPy效率的提升感到惊讶？实际上，在一些更早一些配置差一点的电脑上，这个差距还会更大。记得我以前曾经做过一样的事情，时间大概为Python: 5s, NumPy:0.5s。差了有10倍左右。那么我们也能看出来了，ndarray的计算速度快了很多，为我们节约了大量时间。 ndarray对象 N维数组对象ndarray可以说是NumPy中最重要的一个特点了，它是一个系列同类型数据的集合，以0下标为开始进行集合中元素的索引。ndarray对象是用于存放同类型元素的多维数组。 让我尝试创建一些一维数组，Numpy创建数组有三种不同的方式，1. 直接传入列表的方式； 2. 传入range生成序列； 3. 直接使用np.arange()生成数组。让我们一一来实现下： 1234567891011121314151617181920212223# 创建数组的多种形式# 1. 直接传入列表的方式list1 = [1, 2, 3, 4]oneArray = np.array(list1)print()print(f'oneArray: {oneArray, type(oneArray)}')t1 = np.array([1, 2, 3, 4])print(f't1: {t1, type(t1)}')# 2. 传入range生成序列t2 = np.array(range(10))print(f't2: {t2, type(t2)}')# 3. 使用numpy自带的np.arange()生成数组t3 = np.arange(0, 10, 2)print(f't3: {t3, type(t3)}')---oneArray: (array([1, 2, 3, 4]), &lt;class 'numpy.ndarray'&gt;)t1: (array([1, 2, 3, 4]), &lt;class 'numpy.ndarray'&gt;)t2: (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), &lt;class 'numpy.ndarray'&gt;)t3: (array([0, 2, 4, 6, 8]), &lt;class 'numpy.ndarray'&gt;) 无论用哪种方式，我们可以看到最终打印的类型都是numpy.ndarray。 那以上是一维数组的创建，我们再来看看二维数组： 123456789# 二维数组list2 = [[1,2],[3,4],[5,6]]twoArray = np.array(list2)print(twoArray)---[[1 2] [3 4] [5 6]] 对于二位数组的理解，我们可以将其看成行跟列。如果是在Excel中，那么1,2就是一行，3,4是一行，5,6是一行。而[1,3,5]就是一列，[2,4,6]也是一列。 那么，我们在看到数据之前怎么知道这组数据的维度呢？可以使用方法ndim， 顺便，我们来学习一下一些常用属性。 12345678910111213# 获取数组的维度print(twoArray.ndim)# 获取数据的形状（行、列）print(twoArray.shape)# 获取数组的元素个数print(twoArray.size)---2(3, 2)6 这样，我们就可以看到数组的一些相关属性，拿来在处理数据前做参考。其中，获取到的shape是一个元组数据，而我们获取到的size可以看成是元组内的数据相乘。 现在我们来看，我们能否在numpy中调整数组的形状呢？来，尝试一下，当前我们有一组二维数组： 1[[1,2,3],[4,5,6]] 将其转变为ndarray并赋值给一个变量arr_1， 既然我们前面知道获取数据的形状是用shape， 那我们尝试直接更改它的shape看看是否可行： 12345678arr_1 = np.array([[1,2,3],[4,5,6]])arr_1.shape = (3, 2)print(arr_1)---[[1 2] [3 4] [5 6]] 居然可以。 不过虽然这样能够对数组形状进行修改，不过在NumPy中正确的修改方式应该是使用reshape： 123456789# 返回一个新的数组arr_1 = arr_1.reshape(arr_1.shape)print(f'\\narr_1:\\n{arr_1}')---arr_1:[[1 2] [3 4] [5 6]] 这次我们使用了数组第一次修改后的形状，所以整个和之前没有差别，让我们再试试其他的： 123456789101112# 将多维变成一维数组arr_2 = arr_1.reshape((arr_1.size), order='F')print(f'\\narr_2:\\n{arr_2}')arr_3 = arr_1.flatten(order='F')print(f'\\narr_3:\\n{arr_3}')---arr_2:[1 3 5 2 4 6]arr_3:[1 3 5 2 4 6] 后方那个形参order是一个可选参数，是读取元素的索引顺序，有C, F, A三个固定值。C为行有限，F为列有限, A为按数据存储顺序。 如果只是转为一维数组，使用reshape还需要知道数组的元素个数，不如flatten来的方便。这是一个专门用于将数组折叠成一维数组的方法。 让我们来看看reshape转换数组形状的其他几个例子： 12345678910111213141516171819202122232425262728293031323334353637# 数组的形状t = np.arange(24)print(f't:\\n{t}')print(t.shape)# 转换成二维t1 = t.reshape((4,6))print(f'\\nt1:\\n{t1}')print(t1.shape)# 转换成三维t2 = t1.reshape((2, 3, 4))print(f'\\nt2:\\n{t2}')print(t2.shape)---t:[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23](24,)t1:[[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]](4, 6)t2:[[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19][9, 12, 88, 14, 25] &lt;class 'list'&gt; [20 21 22 23]]](2, 3, 4) 在数据转为了ndarray之后，方便了我们在NumPy中进行计算等操作，不过很多时候，最终我们还是要转回Python内的List。如果需要进行转换，直接使用tolist()就可以了。 1234567# 将数组转为lista = np.array([9, 12, 88, 14, 25])items = a.tolist()print(items, type(items))---[9, 12, 88, 14, 25] &lt;class 'list'&gt; NumPy的数据类型 在之前的Python基础课程中，我用几节课给大家讲了一遍Python中的数据类型。同样的，在NumPy中也有一些不同的数据类型，大多数时候，他们都可以进行转换。 来直接上代码看几个例子： 1arr = np.array([1, 2, 3, 4, 5], dtype=np.int16) 这样，我们就在Numpy中生成了一个int16类型的数组。我们来看看这组数据的元素长度和类型： 12345678# 返回数组中每个元素的直接单位长度print(arr.itemsize)# 获取数据类型print(arr.dtype)---2int16 当然，就如我们之前说的，数据的类型之间是可以进行转换的，转换也十分方便，直接使用类型方法就可以了。 123456# 调整数据类型arr_2 = arr.astype(np.int64)print(arr_2.dtype)---int64 这里给大家多讲一个小技巧，让我们看看如何生成随机小数： 123456789# 使用Python语法，保留两位print(round(random.random(), 2))# Numpy生成数组arr_3 = np.round([random.random() for i in range(10)],2)print(arr_3)---0.47[0.97 0.81 0.1 0.23 0.66 0.98 0.06 0.44 0.33 0.14] 既然我们知道了dtype是numpy中的数据类型，那么对于数组来说，都有哪些类型呢？这里给大家一个表： 名称 描述 简写 np.bool 用一个字节存储的布尔类型（True或False） 'b' np.int8 一个字节大小，-128 至 127 （一个字节） 'i' np.int16 整数，-32768 至 32767 （2个字节） 'i2' np.int32 整数，-2 31 至 2 32 -1 （4个字节） 'i4' np.int64 整数，-2 63 至 2 63 - 1 （8个字节） 'i8' np.uint8 无符号整数，0 至 255 'u' np.uint16 无符号整数，0 至 65535 'u2' np.uint32 无符号整数，0 至 2 ** 32 - 1 'u4' np.uint64 无符号整数，0 至 2 ** 64 - 1 'u8' np.ﬂoat16 半精度浮点数：16位，正负号1位，指数5位，精度10位 'f2' np.ﬂoat32 单精度浮点数：32位，正负号1位，指数8位，精度23位 'f4' np.ﬂoat64 双精度浮点数：64位，正负号1位，指数11位，精度52位 'f8' np.complex64 复数，分别用两个32位浮点数表示实部和虚部 'c8' np.complex128 复数，分别用两个64位浮点数表示实部和虚部 'c16' np.object_ python对象 'O' np.string_ 字符串 'S' np.unicode_ unicode类型 'U' 数组的计算 numpy的广播机制在运算过程中，加减乘除的值被广播到所有的元素上面： 1234567891011121314151617181920212223242526272829303132333435t1 = np.arange(24).reshape((6,4))print('原数组：\\n', t1)print('加2:\\n', t1+2)print('乘2:\\n', t1*2)print('除2:\\n', t1/2)---原数组： [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15] [16 17 18 19] [20 21 22 23]]加2: [[ 2 3 4 5] [ 6 7 8 9] [10 11 12 13] [14 15 16 17] [18 19 20 21] [22 23 24 25]]乘2: [[ 0 2 4 6] [ 8 10 12 14] [16 18 20 22] [24 26 28 30] [32 34 36 38] [40 42 44 46]]除2: [[ 0. 0.5 1. 1.5] [ 2. 2.5 3. 3.5] [ 4. 4.5 5. 5.5] [ 6. 6.5 7. 7.5] [ 8. 8.5 9. 9.5] [10. 10.5 11. 11.5]] 除了和数字进行计算之外，同种形状的数组之间也是可以进行计算（对应位置进行计算操作）。 1234567891011121314151617181920t1 = np.arange(24).reshape(6,4)t2 = np.arange(100, 124).reshape(6,4)print('相加:\\n',t1+t2)print('相乘:\\n',t1*t2)---相加: [[100 102 104 106] [108 110 112 114] [116 118 120 122] [124 126 128 130] [132 134 136 138] [140 142 144 146]]相乘: [[ 0 101 204 309] [ 416 525 636 749] [ 864 981 1100 1221] [1344 1469 1596 1725] [1856 1989 2124 2261] [2400 2541 2684 2829]] 那么，不同形状的多维数组能否可以计算呢？来，一起试试看： 12345678910111213141516t1 = np.arange(24).reshape((4,6))t2 = np.arange(18).reshape((3,6))print(t1)print(t2)print(t1-t2)---[[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]][[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17]]ValueError: operands could not be broadcast together with shapes (4,6) (3,6) 做相加操作的时候报错了，所以我们平时在处理数据的时候一定要多注意这种情况发生。不同形状的多维数组是不能进行计算的。 我们继续来往后做实验看看，行数或者列数相同的一维数组和多维数组可以进行计算吗？ 先看看行形状相同的情况： 123456789t1 = np.arange(24).reshape(4,6)t2 = np.arange(0, 6)print(t1 - t2)---[[ 0 0 0 0 0 0] [ 6 6 6 6 6 6] [12 12 12 12 12 12] [18 18 18 18 18 18]] 看到结果我们就明白了，多维数组中的每一行数组都分别和一维数组中的数据进行操作，也就是会与每一行数组的对应位相操作。 那么列形状相同是否也是相同情况？ 123456789t1 = np.arange(24).reshape(4,6)t2 = np.arange(4).reshape(4,1)print(t1-t2)---[[ 0 1 2 3 4 5] [ 5 6 7 8 9 10] [10 11 12 13 14 15] [15 16 17 18 19 20]] 就跟预料的一样，每一列的数组的对应位都进行了操作。 数组的轴 在理解了一维数组和二维数组之后，我们来看看数组中的轴。 什么是轴？在Numpy中，我们可以讲轴理解为方向，使用0,1,2数字来表示，对于一个一维数组，只有一个0轴。二维数组（shape(2,2)）呢，就有0轴和1轴，那同理向上推断，三维数组(shape(2,2,3))会有0，1，2三个轴。 那么我们到底为什么要了解和学习轴呢？有了轴的概念之后，我们计算讲会更加方便，比如计算一个二维数组的平均值，必须制定是计算哪个方向上面的数字的平均值。 下图中，我列出了不同数组的轴，看着图相信会好理解很多： 一维数组： 二维数组： 三维数组： 看完图之后，让我们再放到代码里去理解一下轴的概念,我们先创建一个二维数组，然后用数组内的数据分别从0轴和1轴进行相加计算总和，得出的结果如下： 1234567a = np.array([[1,2,3], [4,5,6]])print(np.sum(a, axis=0))print(np.sum(a, axis=1))---[5 7 9][ 6 15] 也就是说，我们按0轴算总和的时候，是列相加(1+4,2+5,3+6)，我们按1轴相加算总和的时候，是行相加(1+2+3, 4+5+6) 再让我们看看后面形参上我们不给行列值，计算的总和会是多少？ 1234print(np.sum(a))---21 也就是这个数组内的所有数字相加得到的结果。 在理解了二维数组行列相加的不同之后，我们再来看看三维数组，让我们先创建一个三维数组： 123456789101112131415a = np.arange(27).reshape(3,3,3)print(a)---[[[ 0 1 2] [ 3 4 5] [ 6 7 8]] [[ 9 10 11] [12 13 14] [15 16 17]] [[18 19 20] [21 22 23] [24 25 26]]] 接着，我们来分别按不同数轴进行计算来看看： 1234567# axis = 0print(np.sum(a, axis=0))---[[27 30 33] [36 39 42] [45 48 51]] 1234567# axis = 1print(np.sum(a, axis=1))---[[ 9 12 15] [36 39 42] [63 66 69]] 1234567# axis = 2print(np.sum(a, axis=2))---[[ 3 12 21] [30 39 48] [57 66 75]] 虽然最后相加之后得到的都是3X3的二维数组，但是计算结果大相径庭。我们就不一一分析，只拿计算结果的第一个数字来看，就能明白，其他数字都是一样的计算方法： axis=0, 第一行的第一个数字得到过程为：0+9+18； axis=1, 第一行的第一个数字得到的过程为： 0+3+6； axis=2, 第一行的第一个数字得到的过程为:0+1+2； 那现在，我们是不是对于轴相加的计算过程就比较理解了？ 最后总结一下，在计算的时候可以想象成是每一个坐标轴，分别计算这个轴上面的每一个刻度上的值，或者在二维数组中记住0表示列1表示行。 索引和切片 在学习Python基础课程的时候，我们应该已经了解过索引和切片的概念，在Numpy数组中这个概念也并无不同，一样也是开始值，结束值，步进值(start:stop:step)来进行切片操作。 12345a = np.arange(10)print(a[2:7:2])---[2 4 6] 我们创建了一个一维数组，然后从索引2开始到索引7停止，间隔为2。 和Python中的下标索引方式相同，在这个以为数组中，我们使用[2]一样可以获取到下标为2的那个元素： 1234print(a[2], a)---2 [0 1 2 3 4 5 6 7 8 9] 还记得[2:]代表的含义吗？不太记得的话，我们来看看在Numpy的数组中操作会怎么样，其结果和Python对列表进行操作一样： 1234print(a[2:])---[2 3 4 5 6 7 8 9] 那以上这些操作我们都是对一维数组进行操作，大家可能还相对比较好理解。因为我们可以将其代入成一个Python列表，那对列表进行索引和切片的操作我们之前已经学过。 可是如果是多维数组进行操作，又和一维数组有什么不同呢？我们先来创建一个4行6列的二维数组： 12345678t1 = np.arange(24).reshape(4,6)print(t1)---[[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]] 然后我们现在来一样对其使用[2]来进行操作： 1234print(t1[2])---[12 13 14 15 16 17] 是不是和一维数组不太一样？在t1数组中，我们打印的数据应该是它按行往下数第3行，那其实就是行下标的2, 按行数从上往下数的下标分别为[0, 1, 2] 那么我们如果使用切片的话，以这种方式去思考，应该是从当前下标的行往后取，对吧？对不对我们直接来试试就知道了： 12345print(t1[2:])---[[12 13 14 15 16 17] [18 19 20 21 22 23]] 这是从当前行开始向下连续取多行。 那如果是取中间的连续部分呢？ 12345print(t1[1:3])---[[ 6 7 8 9 10 11] [12 13 14 15 16 17]] 这样，我们就取到了第二和第三行。 有的时候，我们可能需要隔行取值： 123456print(t1[[0,2,3]])---[[ 0 1 2 3 4 5] [12 13 14 15 16 17] [18 19 20 21 22 23]] 我们现在是知道了如何按行来获取数组中的数据，同样，我们还可以按列来获取。 在学习如何按列获取数据之前，我们先来看看如何获取到某行某列的那单个数据： 1234print(t1[2,3])---15 这样，表示的意思就是第二行第三列，我们如愿取到了15这个值。 而从这种方式我们基本就可以看的出来，在获取数据的时候，我们用到了[axis1,axis0]这样的形式去准确的定位。知道了这个，就简单了。 既然,号之前的数值代表行下标，那,之后的数值应该就是列下标。一样，从0开始。和取行不同的地方在于，我们在取行的时候可以在,号之后加:这个符号来表示所有列，当然也可以不加，这是一种默认的方式。而取列的时候，我们必须在前面加上行的下标，如果是全取值，得加上:。 比方说，我们之前按行取值的时候： 12print(t1[1:3])print(t1[1:3,:]) 这两种方式实际上是一样的。因为默认方式列上 :的取值方式。 就如我们刚才说到的，按列取值的时候，行就必须加上:才行。 1234print(t1[,2])---SyntaxError: invalid syntax 看，报错了。我们只有在前面将取所有行加上才行，又或者加上某一行的值： 1234print(t1[:,2])---[ 2 8 14 20] 这样，我们就取到了第三列的值。 和取行一样，列也可以连续取值： 1234567print(t1[:, 2:])---[[ 2 3 4 5] [ 8 9 10 11] [14 15 16 17] [20 21 22 23]] 我们如愿从第三列开始向后连续取值。 那如果是不连续的呢？也和行是一样的形式： 1234567print(t1[:,[0,2,3]])---[[ 0 2 3] [ 6 8 9] [12 14 15] [18 20 21]] 有没有发现，当我们了解了行列关系和取值方式代表的意义之后，那这个多维数组就任我们拿捏了。其原理显得非常简单。 好，让我们多加一个难度稍微高一点的取值，我们需要从行，从列都取不连续的值。那还不简单，不就是[[行,行,行...],[列,列,列...]]的方式吗。 1234print(t1[[0,1,1],[0,1,3]])---[0 7 9] 这，其实就是一种定点取值的方式了。 修改数组的值 在了解完如何进行查询查找之后，我们现在在来学习一下如何对其中的值进行修改。在平时我们处理数据的时候，替换修改数值是常有的事情,我们每次修改之后，都会将之前的值复原。 我们从修改行的值开始，因为这个比较好理解： 123456789# 修改一行的值t1[1] = 0print(t1)---[[ 0 1 2 3 4 5] [ 0 0 0 0 0 0] [12 13 14 15 16 17] [18 19 20 21 22 23]] 就是如此简单，定位好数值后，直接用=赋值就可以了。 那同理，修改某一列的数据也是如此操作。 123456789t1 = np.arange(24).reshape(4,6)t1[:,1] = 0print(t1)---[[ 0 0 2 3 4 5] [ 6 0 8 9 10 11] [12 0 14 15 16 17] [18 0 20 21 22 23]] 修改连续多行和修改连续多列的方式也是一样，这次让我们来一次联动操作，将其中的连续多行多列一起修改掉： 123456789t1 = np.arange(24).reshape(4,6)t1[1:3,1:4] = 0print(t1)---[[ 0 1 2 3 4 5] [ 6 0 0 0 10 11] [12 0 0 0 16 17] [18 19 20 21 22 23]] 当然，既然可以连续多行多列进行修改，那我们肯定也能够进行定点修改值 123456789t1 = np.arange(24).reshape(4,6)t1[[0,1],[0,3]] = 0print(t1)---[[ 0 1 2 3 4 5] [ 6 7 8 0 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]] 那基本上，如何按照行列寻找值并修改也就这么多内容，大家多操作完全可以轻易掌握。 别着急，还没结束，按行按列并不能完全满足我们平时的要求，有的时候，我们是要对数值按条件进行修改的。打个比方说，我们需要对所有大于2小于12的值进行修改，那么这又该如何实现？总不会在数组中可以用比较运算符来判断吧？嗯，没错，就是要用比较运算符符来判断，完全可以： 12345t1 = np.arange(24).reshape(4,6)t1[2&lt;t1 and t1&lt;12] = 0---ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() 报错了，怎么回事，不是说好的可以用比较运算符判断吗？ 原因其实就是，我们不能使用and,or这样的方式，我们来改变一下逻辑运算符的方式： 123456789t1 = np.arange(24).reshape(4,6)t1[(2&lt;t1)&amp;(t1&lt;12)] = 0print(t1)---[[ 0 1 2 0 0 0] [ 0 0 0 0 0 0] [12 13 14 15 16 17] [18 19 20 21 22 23]] 成功了，从3到11我们全部替换成了0。 其实，在NumPy中，也有相应的方法名来替代&amp;, |, ~这种逻辑运算符，我们来看： 123456789t1 = np.arange(24).reshape(4,6)t1[(np.logical_and(t1&gt;2, t1&lt;12))] = 0print(t1)---[[ 0 1 2 0 0 0] [ 0 0 0 0 0 0] [12 13 14 15 16 17] [18 19 20 21 22 23]] 这就是在NumPy中特殊的逻辑运算方法。「与、或、非」对应的方法如下： np.logical_and: 与&amp; np.logical_or: 或| np.logical_not: 非~ 大家可以依次去试试，我这里另外给大家拓展一个方式,就是三目运算方式，这种方式在今后的数据处理中，我们会经常看到，其形式为： 1三目运算 np.where(condition, x, y)，满足条件(condition)输出x, 不满足输出y 我们来看个例子： 12345678score = np.array([[80,88],[82,81],[75,81]])result = np.where(score&gt;80, True, False)print(result)---[[False True] [ True True] [False True]] 在我们平时处理数据的过程当中，数据并不是如此完美，除了修改之外，更多的时候是需要我们去添加、删除以及去重。接下来，就让我们看看这部分该如何操作： 数组的添加 numpy.append函数在数组的末尾添加值。 追加操作会分配整个数组，并把原来的数组复制到新数组中。 此外，输入数组的维度必须匹配否则将生成ValueError。 在其中的参数意义如下： arr: 输入数组 values：要向arr添加的值，需要和arr形状相同（除了要添加的轴）。 axis：默认为 None。当axis无定义时，是横向加成，返回总是为一维数组！当axis有定义的时候，分别为 0和1的时候。当axis有定义的时候，分别为0和1的时候（列数要相同）。当axis为1时，数组是加在右边（行数要相同）。 1234567891011121314151617181920212223# 数组的添加a = np.array([[1,2,3], [4,5,6]])print(f'第一个数组:\\n{a}\\n')print(f'向数组添加元素:\\n{np.append(a, [7,8,9])}\\n')print(f'沿轴0添加元素:\\n{np.append(a, [[7,8,9]], axis=0)}\\n')print(f'沿轴1添加元素:\\n{np.append(a, [[5,5,5],[7,8,9]], axis=1)}\\n')---第一个数组:[[1 2 3] [4 5 6]]向数组添加元素:[1 2 3 4 5 6 7 8 9]沿轴0添加元素:[[1 2 3] [4 5 6] [7 8 9]]沿轴1添加元素:[[1 2 3 5 5 5] [4 5 6 7 8 9]] 除了append之外，我们还可以使用insert进行添加。 numpy.insert：函数在给定索引之前，沿给定轴在输入数组中插入值。如果值的类型转换为要插入，则它与输入数组不同。 插入没有原地的，函数会返回一个新数组。 此外，如果未提供轴，则输入数组会被展开。 12345678910111213141516171819202122232425262728a = np.array([[1,2],[3,4],[5,6]])print(f'第一个数组:\\n{a}\\n')print(f'未传递Axis参数,在插入之前输入数组会被展开:\\n{np.insert(a,3,[11,12])}\\n')print('\\n传递了Axis参数,会广播值数组来配输入数组')print(f'沿轴0广播:\\n{np.insert(a,1,[11], axis=0)}\\n')print(f'沿轴1广播:\\n{np.insert(a,1,11, axis=1)}\\n')---第一个数组:[[1 2] [3 4] [5 6]]未传递Axis参数,在插入之前输入数组会被展开:[ 1 2 3 11 12 4 5 6]传递了Axis参数,会广播值数组来配输入数组沿轴0广播:[[ 1 2] [11 11] [ 3 4] [ 5 6]]沿轴1广播:[[ 1 11 2] [ 3 11 4] [ 5 11 6]] 从代码中我们可以看出来，insert相比较而言在arr和values之间多了一个形参，用于指示插入数据的下标位置。 接着，我们来看看如何在数组中操作删除。 npmpy.delete: 函数返回从输入数组中删除指定子数组的新数组。 与 insert()函数的情况一样，如果未提供轴参数，则输入数组将展开。 参数说明： arr：输入数组 obj：可以被切片，整数或者整数数组，表明要从输入数组删除的子数组 axis：沿着它删除给定子数组的轴，如果未提供，则输入数组会被展开。 123456789101112131415161718a = np.arange(12).reshape(3,4)print(f'第一个数组:\\n{a}\\n')print(f'未传递Axis参数。在删除之前输入数组会被展开:\\n{np.delete(a, 5)}\\n')print(f'删除每一行中的第二列：\\n{np.delete(a,1,axis=1)}\\n')---第一个数组:[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]未传递Axis参数。在删除之前输入数组会被展开:[ 0 1 2 3 4 6 7 8 9 10 11]删除每一行中的第二列：[[ 0 2 3] [ 4 6 7] [ 8 10 11]] 接下来，估计是我们清洗数据最常用到的操作，就是「去重」。 numpy.unique: 函数用于去除数组中的重复元素。 参数说明： arr：输入数组，如果不是一维数组则会展开. return_index：如果为true，返回新列表元素在旧列表中的位置（下标），并以列表形式储. return_inverse：如果为true，返回旧列表元素在新列表中的位置（下标），并以列表形式储 . return_counts：如果为true，返回去重数组中的元素在原数组中的出现次数. 12345678910111213141516171819202122232425262728293031323334353637a = np.array([5,2,6,2,7,5,6,7,2,9])print(f'第一个数组:\\n{a}\\n')print(f'第一个数组的去重值：\\n{np.unique(a)}\\n')print('去重数组对应原数组的索引下标数组：')u,indices = np.unique(a, return_index = True)print(indices)print('\\n我们可以看到每个和原数组下标对应的数值：')print(u)print('\\n原数组对应去重数组的下标数组：')u,indices = np.unique(a, return_inverse = True)print(u)print(indices)print ('\\n返回去重元素的重复数量：')u,indices = np.unique(a,return_counts = True) print (u)print (indices)---第一个数组:[5 2 6 2 7 5 6 7 2 9]第一个数组的去重值：[2 5 6 7 9]去重数组对应原数组的索引下标数组：[1 0 2 4 9]我们可以看到每个和原数组下标对应的数值：[2 5 6 7 9]原数组对应去重数组的下标数组：[2 5 6 7 9][1 0 2 0 3 1 2 3 0 4]返回去重元素的重复数量：[2 5 6 7 9][3 2 2 2 1] 我用图形来解释一下「第一个数组的去重值」和「去重数组的索引数组」，至于其他的，也是类似的分析方式。 在去重的方法中，我们都有两个变量去接收返回值，也就是方法返回了两个值，其中u这个变量就代表的是改变之后的数组，而indices则是重新组成的数组中的数值在原数组中第一次出现的下标位置，又或者是出现次数。以方法不同，但是u肯定是去重后的重新组成的数组。 我标识了一下原数组中的下标，并且用颜色区分了一下。在重新组成的去重的数组中，可以看到已经没有重复的数值了，然后对比原数组，每一个不重复的数值也都还在。而默认的排序方式就是从小到大的排序。所以重新组成的数值中的排序和原数组中不太一样。 然后我们再来看最下面「去重下标」部分，虽然这也是一个数组，但是实际上它是一个完全由下标位置组成的数组，然后我们来看去重数组，我们是拿第一次出现的下标来算。其中2对应原数组就的下标就是1， 5第一次出现是0的位置，6是2, 7是4，9这是最后一位，也就是下标9。 所以对重新组成的数组稍微一分析，就能明白每个方法的返回值说代表的意义。 相信到这里，大家应该也都能理解了。 接下来，我们要来看看重头戏，NumPy的计算。 NumPy的计算 NumPy原本就是一个科学计算的第三方库，所以计算能力应该算是NumPy里的重点。在原始方法中，有许许多多的方法用于数据的计算。包括求最大值，求最小值，平均值，累计和等等。除了计算整体之外，还支持在不同轴上进行计算。下面我们来看看，NumPy到底为我们都提供了哪些好用的方法，最开始，还是让我们来生成一组新的数据： 1234567score = np.array([[80,88],[82,81],[75,81]])score---array([[80, 88], [82, 81], [75, 81]]) 获取所有数据最大值 12345result = np.max(score)print(result)---88 获取某一个轴上的数据最大值 12345result = np.max(score,axis=0)print(result)---[82 88] 获取最小值 12345result = np.min(score)print(result)---75 获取某一个轴上的数据最小值 12345result = np.min(score,axis=1)print(result)---[80 81 75] 数据的比较 第一个参数中的每一个数与第二个参数比较返回大的 12345result = np.maximum([-2, -1, 0, 1, 2], 0)print(result)---[0 0 0 1 2] 第一个参数中的每一个数与第二个参数比较返回小的 12345result = np.minimum([-2, -1, 0, 1, 2], 0)print(result)---[-2 -1 0 0 0] 以上两组代码中，当方法内接受两个参数，当然也可以大小一致; 当第二个参数只是一个单独的值时，其实是用到了维度的广播机制。如果第二个参数是一个同样长度的数组，会分别比较不同位置。 12345result = np.maximum([-2, -1, 0, 1, 2], [1,2,3,4,5]) print(result)---[1 2 3 4 5] 咱们在这里稍微讲一下NumPy中的广播机制，其实是有点晦涩难懂。大家尝试理解一下看看。 广播机制是Numpy让两个不同shape的数组能够做一些运算，需要对参与运算的两个数组做一些处理或者说扩展，最终是参与运算的两个数组的shape一样，然后广播计算(对应位置数据进行某运算)得到结果。 以我们做数据比较的第一组为例，当我们去对比第一个参数和第二个参数返回大的，这个时候我们给到的两个参数分别是： [-2,-1,0,1,2]和0，但是由于广播机制的存在，在NumPy中实际上是这么处理的，第一个参数还是[-2,-1,0,1,2]， 而第二个参数实际上是[0,0,0,0,0]。 获取所有数据的平均值 12345result = np.mean(score)print(result)---81.16666666666667 获取某一行或一列的平均值 12345result = np.mean(score, axis=0)print(result)---[79. 83.33333333] 返回给定axis上的累计和 123456789t1 = np.array([[1,2,3],[4,5,6]])print(t1)print(t1.cumsum(0))---[[1 2 3] [4 5 6]][[1 2 3] [5 7 9]] 在来看值为axis=1时： 12345print(t1.cumsum(1))---[[ 1 3 6] [ 4 9 15]] 我们可以这样认为，当cumsum(0)，也就是axis=0时，是这样计算的： 12[1 2 3] --------&gt; |1 |2 |3 |[5 7 9] --------&gt; |5=1+4 |7=2+5 |9=3+6| 当cumsum(1)，也就是axis=1时，是这样计算的： 12[ 1 3 6] ------&gt; |1 |3=2+1 |6=3+2+1 |[ 4 9 15] ------&gt; |4 |9=4+5 |15=4+5+6 | argmin求最小值索引 12345result = np.argmin(score, axis=0)print(result)---[2 1] 我们看score这组数据中，75是第一列最小值，81是第二列最小值。当然，第二列的数据中有两个81。现在让我们讲数组的值换一下看看： 123456score[2,1] = 64result = np.argmin(score, axis=0)print(result)---[2 2] 很显然，我们第二列的最小值变成了64， 在那一列中，它的坐标为2。 求每一列的标准差 标准差是一组数据平均值分散程度的一种度量。一个较大的标准差，代表大部分数值和其平均值之间差异较大；一个较小的标准差，代表这些数据较接近平均值反应出数据的波动稳定情况，越大表示波动越大，越不稳定。 12345result = np.std(score, axis=0)print(result)---[ 2.94392029 10.07747764] 从结果中可以看出来，我们第二列的波动较大。其原因正是因为我把[2,2]这个位置的值替换成了64。和其他值拉大了差距。 极值 12345result = np.ptp(score,axis=None) print(result)---24 计算极值，其实也就是最大值和最小值的差。在score中，最大值为88，最小值为64。 除了我们目前测试的这些方法之外，NumPy中还有很多其他方法。比如，计算反差的var， 协方差cov，平均值average， 中位数median。在这里，我们就不一一测试了，方法都比较简单，拿来直接用的那种。 我将通用函数和解释在这里列一个表： 通用函数 解释 numpy.sqrt(array) 平方根函数 numpy.exp(array) e^array[i]的数组 numpy.abs/fabs(array) 计算绝对值 numpy.square(array) 计算各元素的平方 等于 array**2 numpy.log/log10/log2(array) 计算各元素的各种对数 numpy.sign(array) 计算各元素正负号 numpy.isnan(array 计算各元素是否为NaN numpy.isinf(array) 计算各元素是否为NaN numpy.cos/cosh/sin/sinh/tan/tanh(array) 三角函数 numpy.modf(array) 将array中值得整数和小数 分离，作两个数组返回 numpy.ceil(array) 向上取整,也就是取比这个 数大的整数 numpy.ﬂoor(array) 向下取整,也就是取比这个 数小的整数 numpy.rint(array) 四舍五入 numpy.trunc(array) 向0取整 numpy.cos(array) 正弦值 numpy.sin(array) 余弦值 numpy.tan(array) 正切值 numpy.add(array1,array2) 元素级加法 numpy.subtract(array1,array2) 元素级减法 numpy.multiply(array1,array2) 元素级乘法 numpy.divide(array1,array2) 元素级除法 array1./array2 numpy.power(array1,array2) 元素级指数 array1.^array2 numpy.maximum/minimum(array1,aray2) 元素级最大值 numpy.fmax/fmin(array1,array2) 元素级最大值，忽略NaN numpy.mod(array1,array2) 元素级求模 numpy.copysign(array1,array2) 将第二个数组中值得符号复 制给第一个数组中值 numpy.greater/greater_equal/less/less_equal/equal/not_equal (array1,array2) 元素级比较运算，产生布尔 数组 numpy.logical_end/logical_or/logic_xor(array1,array2) 元素级的真值逻辑运算 数组的拼接 有的时候我们需要将两个数据加起来一起研究分析，我们就可以将其进行拼接然后分析。 我们先创建两组数组： 12a = np.array([[1,2],[3,4]])b = np.array([[5,6],[7,8]]) 然后我们现在先根据轴连接的数组序列 先沿轴0连接两个数组： 1234567print(np.concatenate((a,b), axis=0))---[[1 2] [3 4] [5 6] [7 8]] 再沿轴1连接两个数组： 12345print(np.concatenate((a,b), axis=1))---[[1 2 5 6] [3 4 7 8]] 根据轴进行堆叠 沿轴0堆叠两个数组： 12345678print(np.stack((a,b), axis=0))---[[[1 2] [3 4]] [[5 6] [7 8]]] 沿轴1堆叠两个数组： 12345678print(np.stack((a,b), axis=1))---[[[1 2] [5 6]] [[3 4] [7 8]]] 矩阵垂直拼接 12345678910v1 = [[0,1,2,3,4,5], [6,7,8,9,10,11]]v2 = [[12,13,14,15,16,17],[18,19,20,21,22,23]]result = np.vstack((v1, v2))print(result)---[[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]] 矩阵水平拼接 123456result = np.hstack((v1, v2))print(result)---[[ 0 1 2 3 4 5 12 13 14 15 16 17] [ 6 7 8 9 10 11 18 19 20 21 22 23]] 数组的分割 将一个数组分割为多个子数组 参数说明： ary：被分割的数组 indices_or_sections：果是一个整数，就用该数平均切分，如果是一个数组，为沿轴切分的位置（左开右 闭） axis：沿着哪个维度进行切向，默认为0，横向切分。为1时，纵向切分 12345678arr = np.arange(9).reshape(3,3)print('将数组分成三个大小相等的子数组：')b = np.split(arr,3)print(b)---将数组分成三个大小相等的子数组：[array([[0, 1, 2]]), array([[3, 4, 5]]), array([[6, 7, 8]])] numpy.hsplit函数用于水平分割数组，通过指定要返回的相同形状的数组数量来拆分原数组。 1234567891011121314harr = np.floor(10 * np.random.random((2,6)))print(f'原array:\\n{harr}')print(f'\\n水平分割后:\\n{np.hsplit(harr, 3)}')---原array:[[1. 1. 8. 2. 3. 9.] [3. 1. 6. 6. 5. 6.]]水平分割后:[array([[1., 1.], [3., 1.]]), array([[8., 2.], [6., 6.]]), array([[3., 9.], [5., 6.]])] 这里我们说一下floor(), 这个方法会返回数值的下舍整数（舍去小数点求整型）。 numpy.vsplit会沿垂直轴分割 123456789101112131415a = np.arange(16).reshape(4,4)print(f'第一个数组：\\n{a}')print(f'\\n垂直分割之后：\\n{np.vsplit(a,2)}')---第一个数组：[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]]垂直分割之后：[array([[0, 1, 2, 3], [4, 5, 6, 7]]), array([[ 8, 9, 10, 11], [12, 13, 14, 15]])] nan和inf C 语言中表示最大的正整数值是0x7FFFFFFF，最小的负整数是0x80000000。inf 表示无穷大，需要使用 ﬂoat(‘inf’)函数来转化，那么对应的就有 ﬂoat('-inf')表示无穷小了。这样你就可以使用任意数来判断和它的关系了。 那什么时候会出现inf呢？ 比如一个数字除以0，Python中会报错，但是numpy中会是一个inf或者-inf 另外还有 nan，这种写法在pandas中常见，表示缺失的数据，所以一般用nan来表示。任何与其做运算结果都是nan。 12345678a = np.nanb = np.infprint(a, type(a))print(b, type(b))---nan &lt;class 'float'&gt;inf &lt;class 'float'&gt; 判断数组中为nan的个数(注意：float类型的数据才能赋值nan) 1t = np.arange(24,dtype=float).reshape(4,6) 可以使用np.count_nonzero()来判断非零的个数 1234print(np.count_nonzero(t))---23 将三行四列的数改成nan 12345t[3,4] = np.nanprint(t[3,4] != np.nan)---True 注意到没有，np.nan != np.nan居然是True。难道我们更改数据失败了？我们打出来看看： 1234567print(t)---[[ 0. 1. 2. 3. 4. 5.] [ 6. 7. 8. 9. 10. 11.] [12. 13. 14. 15. 16. 17.] [18. 19. 20. 21. nan 23.]] 没错，t[3,4]确实被改变了，那只能说明np.nan != np.nan是确实存在的。 所以，我们就可以使用这两个结合使用判断nan的个数： 1234print(np.count_nonzero(t != t))---1 我们之前讲过，nan和任何数计算都为nan 1234print(np.sum(t,axis=0))---[36. 40. 44. 48. nan 56.] 接下来，让我们做一个具体的练习，在练习中，我们将处理数组中的nan 123456789101112# 练习，处理数组中的nant = np.arange(24).reshape(4,6).astype('float')# 将数组中的一部分替换nant[1, 3:] = np.nanprint(t)---[[ 0. 1. 2. 3. 4. 5.] [ 6. 7. 8. nan nan nan] [12. 13. 14. 15. 16. 17.] [18. 19. 20. 21. 22. 23.]] 现在我们得到了一组包含nan的数组，接着我们来处理这组数据： 1234567891011121314151617181920212223# 尝试便利每一列，然后判断每一列是否有`nan`for i in range(t.shape[1]): # 获取当前列数据 temp_col = t[:, i] # 判断当前列的数据中是否含有nan nan_num = np.count_nonzero(temp_col != temp_col) # 条件成立说明含有nan if nan_num != 0: # 将这一列部位nan的数据拿出来 temp_col_not_nan = temp_col[temp_col == temp_col] # 将nan替换成这一列的平均值 temp_col[np.isnan(temp_col)] = np.mean(temp_col_not_nan)print(t)---[[ 0. 1. 2. 3. 4. 5.] [ 6. 7. 8. 13. 14. 15.] [12. 13. 14. 15. 16. 17.] [18. 19. 20. 21. 22. 23.]] 这样，我们就处理了这组数据中的nan，至于替换成平均值填补空缺数据，这个是清洗数据的通用做法。 二维数组的转置 针对二维数组的转置，也就是对换数组的维度。说的直白一点，就是行转列，列转行。 这在处理数据的时候，也是我们经常要做的操作： 123456789101112131415a = np.arange(12).reshape(3,4)print (f'原数组：\\n{a}') print (f'\\n对换数组：\\n{np.transpose(a)}') ---原数组：[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]对换数组：[[ 0 4 8] [ 1 5 9] [ 2 6 10] [ 3 7 11]] 让我们再来看一种处理方式，与transpose方法一致： 123456789101112131415a = np.arange(12).reshape(3,4)print (f'原数组：\\n{a}') print (f'\\n转置数组：\\n{a.T}') ---原数组：[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]转置数组：[[ 0 4 8] [ 1 5 9] [ 2 6 10] [ 3 7 11]] 接着我们尝试一个函数用于交换数组的两个轴 1234567891011121314151617181920t1 = np.arange(24).reshape(4,6)re = t1.swapaxes(1,0)print (f'\\n原数组：\\n{t1}') print (f'\\n调用 swapaxes 函数后的数组：\\n{re}') ---原数组：[[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]]调用 swapaxes 函数后的数组：[[ 0 6 12 18] [ 1 7 13 19] [ 2 8 14 20] [ 3 9 15 21] [ 4 10 16 22] [ 5 11 17 23]] 这几种方式都完成了转置操作，平时工作中，我们可以都尝试一下。 总结 最后，我们还是对NumPy整个的做一个总结： 然后，让我们留点作业吧： 练习矩阵相乘 练习数组索引 练习数组形状修改 大家要好好的完成作业。那本节课就到这里了，下课。","link":"/AI-Python-numpy/"},{"title":"27. Pandas","text":"Hi, 大家好。我是茶桁。 先跟小伙伴们打个招呼，今天这节课呢，就是我们Python基础课程的最后一节课了。 在本节课之前，我们讲解了Python的基础，介绍了两个第三方库。而今天一样会介绍一个第三方库：Pandas。 虽然是最后一节课了，但是本节课的任务却是一点也不轻松。相比较而言，如果你以后从事的是数据治理和分析工作，那么本节课的内容可能会是你在今后工作中用到的最多的内容。我们需要学习行列索引的操作，数据的处理，数据的合并，多层索引，时间序列，数据的分组聚合（重点）。最后，我们会有一个案例的展示。 听着是不是很兴奋？那我们就开始吧。 在开始讲解pandas之前，我们讲解一些关于数据的小知识。 我们大部分人应该都用过Excel表格，而我们从数据库中拿到的数据，也基本上和Excel上的数据差不多，都是由行列组成的。可以直接导出为csv文件。 也就是说，我们大部分时候要处理的数据，基本上都一组二维数据。例如，我们今天最后案例要用到的一个电影数据(部分)，如图： 这里面，我们就将数据通过行列来展示和定位。 了解了这一点之后，我们来开始学习pandas。 Pandas简介 在之前的介绍中，我们发现很多的操作似乎都似曾相识，在NumPy中我们好像都接触过。 有这种感觉很正常，Pandas本身就是基于NumPy的一种工具，该⼯具是为了解决数据分析任务⽽创建的。Pandas 纳⼊了⼤量库和⼀些标准的数据模型，提供了⾼效地操作⼤型数据集所需的⼯具。pandas提供了⼤量能使我们快速 便捷地处理数据的函数和⽅法。 Series对象 Pandas基于两种数据类型：series与dataframe。 Series是Pandas中最基本的对象，Series类似⼀种⼀维数组。事实上，Series基本上就是基于NumPy的数组对象来的。和 NumPy的数组不同，Series能为数据⾃定义标签，也就是索引（index），然后 通过索引来访问数组中的数据。 Dataframe是⼀个⼆维的表结构。Pandas的dataframe可以存储许多种不同的数据类型，并且每⼀个坐标轴都有⾃⼰的标签。你可以把它想象成⼀个series的字典项。 在开始基于代码进行学习之前，我们需要应用一些必要项。 1234import numpy as npimport pandas as pdfrom pandas import Seriesfrom pandas import DataFrame as df 现在我们来看看Series的一些基本操作： 创建Series对象并省略索引 index 参数是可省略的，你可以选择不输⼊这个参数。如果不带 index 参数，Pandas 会⾃动⽤默认 index 进⾏索引，类似数组，索引值是 [0, ..., len(data) - 1] 123456789sel = Series([1,2,3,4])print(sel)---0 11 22 33 4dtype: int64 我们之前在NumPy中学习了dtype, 以及它的相关数据类型。所以现在的dtype: int64我们应该能理解是什么意思了。 我们看打印的结果，在1,2,3,4前面，是Series默认生成的索引值。 通常我们会⾃⼰创建索引 123456789sel = Series(data=[1,2,3,4], index= list('abcd'))print(sel)---a 1b 2c 3d 4dtype: int64 这个时候，我们可以对这个Series对象操作分别获取内容和索引： 123456print(f'values: {sel.values}')print(sel.index)---values: [1 2 3 4]Index(['a', 'b', 'c', 'd'], dtype='object') 又或者，我们可以直接获取健值对（索引和值对）。 1234print(list(sel.iteritems()))---[('a', 1), ('b', 2), ('c', 3), ('d', 4)] 那么这种健值对的形式让你想到了什么？是字典对吧？ 我们完全可以将字典转为Series: 12345678910dict={&quot;red&quot;:100,&quot;black&quot;:400,&quot;green&quot;:300,&quot;pink&quot;:900}se3=Series(dict) print(se3)---red 100black 400green 300pink 900dtype: int64 Series数据获取 在Series拿到数据转为Series对象之后，诺大的数据中，我们如何定位并获取到我们想要的内容呢？ Series在获取数据上，支持位置、标签、获取不连续数据，使用切片等方式。我们一个个的看一下： Series对象同时⽀持位置和标签两种⽅式获取数据 123456print('索引下标',sel['c'])print('位置下标',sel[2])---索引下标 3位置下标 3 获取不连续的数据 1234567891011121314print('位置切⽚\\n',sel[1:3])# 左包含右不包含print('\\n索引切⽚\\n',sel['b':'d'])# 左右都包含---位置切⽚b 2c 3dtype: int64索引切⽚b 2c 3d 4dtype: int64 我们看到结果，发现两组数据数量不对，但是其实我们获取的位置都是一样的。这是因为，位置切片的方式会是「左包含右不包含」的，而索引切片则是「左右都包含」。 重新赋值索引的值 除了获取数据之外，我们还可以对数据进行重新索引。其实在之前我们将索引的时候，第二种自己赋值的方式实际上就是一个重新赋值了，将自己定义的值替换了默认值。这里让我们再来看一下： 123456789sel.index = list('dcba')print(sel)---d 1c 2b 3a 4dtype: int64 还有一种重新索引的方法reindex， 这会返回一个新的Series。调用reindex将会重新排序，缺失值这会用NaN填补。 123456789print(sel.reindex(['b','a','c','d','e']))---b 3.0a 4.0c 2.0d 1.0e NaNdtype: float64 在重新索引的时候，我们特意多增加了一个索引。在最后一位没有数据的情况下，缺失值被NaN填补上了。 丢弃指定轴上的项 123456789101112131415sel = pd.Series(range(10, 15))print(sel)print(sel.drop([2,3]))---0 101 112 123 134 14dtype: int640 101 114 14dtype: int64 使用drop，会丢弃掉轴上的项，例子中，我们将2，3进行了丢弃。 Series进⾏算术运算操作 对 Series 的算术运算都是基于 index 进⾏的。我们可以⽤加减乘除（+ - * /）这样的运算符对两个 Series 进⾏运算，Pandas 将会根据索引 index，对响应的数据进⾏计算，结果将会以浮点数的形式存储，以避免丢失精度。如果 Pandas 在两个 Series ⾥找不到相同的 index，对应的位置就返回⼀个空值 NaN。 1234567891011121314151617181920212223242526272829303132333435363738# Series 算数运算series1 = pd.Series([1,2,3,4],['London','HongKong','Humbai','lagos'])series2 = pd.Series([1,3,6,4],['London','Accra','lagos','Delhi'])print('series1-series2:')print(series1-series2) print('\\nseries1+series2:')print(series1+series2) print('\\nseries1*series2:')print(series1*series2)---series1-series2:Accra NaNDelhi NaNHongKong NaNHumbai NaNLondon 0.0lagos -2.0dtype: float64series1+series2:Accra NaNDelhi NaNHongKong NaNHumbai NaNLondon 2.0lagos 10.0dtype: float64series1*series2:Accra NaNDelhi NaNHongKong NaNHumbai NaNLondon 1.0lagos 24.0dtype: float64 除此之外，Series的算术运算操作同样也支持NumPy的数组运算 12345678910111213141516171819sel = Series(data = [1,6,3,5], index = list('abcd'))print(sel[sel&gt;3]) # 布尔数组过滤print(sel*2) # 标量乘法print(np.square(sel)) # 可以直接加⼊到numpy的数学函数---b 6d 5dtype: int64a 2b 12c 6d 10dtype: int64a 1b 36c 9d 25dtype: int64 DataFrame DataFrame（数据表）是⼀种2维数据结构，数据以表格的形式存储，分成若⼲⾏和列。通过 DataFrame，你能很⽅便地处理数据。常见的操作⽐如选取、替换⾏或列的数据，还能重组数据表、修改索引、多重筛选等。我们基本上可以把 DataFrame 理解成⼀组采⽤同样索引的 Series 的集合。调⽤ DataFrame()可以将多种格式的数据转换为DataFrame对象，它的的三个参数data、index和columns 分别为数据、⾏索引和列索引。 DataFrame的创建 我们可以使用二维数组 123456789df1 = df(np.random.randint(0,10,(4,4)),index=[1,2,3,4],columns=['a','b','c','d']) print(df1)--- a b c d1 9 6 3 02 6 2 7 03 3 1 6 54 6 6 7 1 也可以使用字典创建 行索引由index决定，列索引由字典的键决定 12345678910dict={'Province': ['Guangdong', 'Beijing', 'Qinghai','Fujian'],'pop': [1.3, 2.5, 1.1, 0.7], 'year': [2022, 2022, 2022, 2022]}df2= df(dict,index=[1,2,3,4]) print(df2)--- Province pop year1 Guangdong 1.3 20222 Beijing 2.5 20223 Qinghai 1.1 20224 Fujian 0.7 2022 使用from_dict 123456789dict2={&quot;a&quot;:[1,2,3],&quot;b&quot;:[4,5,6]}df6=df.from_dict(dict2) print(df6)--- a b0 1 41 2 52 3 6 索引相同的情况下，相同索引的值会相对应，缺少的值会添加NaN 1234567891011121314data = { 'Name':pd.Series(['zs','ls','we'],index=['a','b','c']), 'Age':pd.Series(['10','20','30','40'],index=['a','b','c','d']), 'country':pd.Series(['中国','⽇本','韩国'],index=['a','c','b'])}df = pd.DataFrame(data)print(df)--- Name Age countrya zs 10 中国b ls 20 韩国c we 30 ⽇本d NaN 40 NaN 看了那么多DataFrame的转换方式，那我们如何将数据转为字典呢？DataFrame有一个内置方法to_dict()能将DataFrame对象转换为字典： 12345dict = df.to_dict()print(dict)---{'Name': {'a': 'zs', 'b': 'ls', 'c': 'we', 'd': nan}, 'Age': {'a': '10', 'b': '20', 'c': '30', 'd': '40'}, 'country': {'a': '中国', 'b': '韩国', 'c': '⽇本', 'd': nan}} DataFrame对象常⽤属性 让我们先来生成一组数据备用： 123456789101112df_dict = {'name':['James','Curry','Iversion'],'age':['18','20','19'], 'national':['us','China','us'] }df = pd.DataFrame(data=df_dict,index=['0','1','2'])print(df)--- name age national0 James 18 us1 Curry 20 China2 Iversion 19 us 获取⾏数和列数 1234print(df.shape)---(3,3) 获取⾏索引 1234print(df.index.tolist())---['0', '1', '2'] 获取列索引 1234print(df.columns.tolist())---['name', 'age', 'national'] 获取数据的类型 1234567print(df.dtypes)---name objectage objectnational objectdtype: object 获取数据的维度 1234print(df.ndim)---2 values属性也会以⼆维ndarray的形式返回DataFrame的数据 123456print(df.values)---[['James' '18' 'us'] ['Curry' '20' 'China'] ['Iversion' '19' 'us']] 展示df的概览 1234567891011121314print(df.info())---&lt;class 'pandas.core.frame.DataFrame'&gt;Index: 3 entries, 0 to 2Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 name 3 non-null object 1 age 3 non-null object 2 national 3 non-null objectdtypes: object(3)memory usage: 96.0+ bytesNone 显示头⼏⾏,默认显示5⾏ 123456print(df.head(2))--- name age national0 James 18 us1 Curry 20 China 显示后⼏⾏ 12345print(df.tail(1))--- name age national2 Iversion 19 us 获取DataFrame的列 1234567print(df['name'])---0 James1 Curry2 IversionName: name, dtype: object 因为我们只获取⼀列，所以返回的就是⼀个Series 1234print(type(df['name']))---&lt;class 'pandas.core.series.Series'&gt; 如果获取多个列，那返回的就是⼀个DataFrame 类型： 123456789print(df[['name','age']])print(type(df[['name','age']]))--- name age0 James 181 Curry 202 Iversion 19&lt;class 'pandas.core.frame.DataFrame'&gt; 获取一行 12345print(df[0:1])--- name age national0 James 18 us 去多⾏ 123456print(df[1:3])--- name age national1 Curry 20 China2 Iversion 19 us 取多⾏⾥⾯的某⼀列（不能进⾏多⾏多列的选择） 123456print(df[1:3][['name','age']])--- name age1 Curry 202 Iversion 19 ⚠️ 注意：df[]只能进⾏⾏选择，或列选择，不能同时多⾏多列选择。比如在NumPy中的data[:,1:3]这种是不行的。当然，并不是没有办法获取，我们接着往下看: df.loc通过标签索引⾏数据;df.iloc通过位置获取⾏数据 获取某⼀⾏某⼀列的数据 1234print(df.loc['0','name'])---James ⼀⾏所有列 1234567print(df.loc['0',:])---name Jamesage 18national usName: 0, dtype: object 某⼀⾏多列的数据 123456print(df.loc['0',['name','age']])---name Jamesage 18Name: 0, dtype: object 选择间隔的多⾏多列 123456print(df.loc[['0','2'],['name','national']]) --- name national0 James us2 Iversion us 选择连续的多⾏和间隔的多列 1234567print(df.loc['0':'2',['name','national']])--- name national0 James us1 Curry China2 Iversion us 取⼀⾏ 1234567print(df.iloc[1])---name Curryage 20national ChinaName: 1, dtype: object 取连续多⾏ 123456print(df.iloc[0:2])--- name age national0 James 18 us1 Curry 20 China 取间断的多⾏ 123456print(df.iloc[[0,2],:])--- name age national0 James 18 us2 Iversion 19 us 取某⼀列 1234567print(df.iloc[:,1])---0 181 202 19Name: age, dtype: object 某⼀个值 1234print(df.iloc[1,0])---Curry 修改值 12345678df.iloc[0,0]='panda'print(df)--- name age national0 panda 18 us1 Curry 20 China2 Iversion 19 us dataframe中的排序⽅法 12345678df = df.sort_values(by='age', ascending*=False) print(df)--- name age national1 Curry 20 China2 Iversion 19 us0 panda 18 us ascending=False是降序排列，默认为True，也就是升序。 dataframe修改index、columns 一样，让我们先创建一组新的数据供我们使用： 12345678df1 = pd.DataFrame(np.arange(9).reshape(3, 3), index = ['bj', 'sh', 'gz'], columns=['a', 'b', 'c'])print(df1)--- a b cbj 0 1 2sh 3 4 5gz 6 7 8 修改 df1 的 index df1.index可以打印出df1的索引值，同时也可以为其赋值。 12345678910print(df1.index) # 可以打印出print的值，同时也可以为其赋值df1.index = ['beijing', 'shanghai', 'guangzhou']print(df1)---Index(['bj', 'sh', 'gz'], dtype='object') a b cbeijing 0 1 2shanghai 3 4 5guangzhou 6 7 8 ⾃定义map函数（x是原有的⾏列值） 123456789101112# ⾃定义map函数（x是原有的⾏列值） def test_map(x): return x+'_ABC'print(df1.rename(index=test_map, columns=test_map, inplace=True))print(df1)---None a_ABC b_ABC c_ABCbeijing_ABC 0 1 2shanghai_ABC 3 4 5guangzhou_ABC 6 7 8 其中的inplace传入一个布尔值，默认为False。指定是否返回新的DataFrame。如果为True，则在原df上修改， 返回值为None。 rename 可以传⼊字典，为某个 index 单独修改名称 12345678df3 = df1.rename(index={'beijing_ABC':'beijing'}, columns = {'a_ABC':'aa'})print(df3)--- aa b_ABC c_ABCbeijing 0 1 2shanghai_ABC 3 4 5guangzhou_ABC 6 7 8 列转化为索引 12345678910df1=pd.DataFrame({'X':range(5),'Y':range(5),'S':list(&quot;abcde&quot;),'Z': [1,1,2,2,2]})print(df1)--- X Y S Z0 0 0 a 11 1 1 b 12 2 2 c 23 3 3 d 24 4 4 e 2 指定⼀列为索引 (drop=False 指定同时保留作为索引的列) 1234567891011result = df1.set_index('S',drop=False)result.index.name=Noneprint(result)--- X Y S Za 0 0 a 1b 1 1 b 1c 2 2 c 2d 3 3 d 2e 4 4 e 2 ⾏转为列索引 1234567891011result = df1.set_axis(df1.iloc[0],axis=1,inplace=False)result.columns.name=Noneprint(result)--- 0 0 a 10 0 0 a 11 1 1 b 12 2 2 c 23 3 3 d 24 4 4 e 2 添加数据 先增加一组数据： 123456789df1 = pd.DataFrame([['Snow','M',22],['Tyrion','M',32],['Sansa','F',18], ['Arya','F',14]],columns=['name','gender','age'])print(df1)--- name gender age0 Snow M 221 Tyrion M 322 Sansa F 183 Arya F 14 在数据框最后加上score⼀列，注意增加列的元素个数要跟原数据列的个数⼀样。 123456789df1['score']=[80,98,67,90] print(df1)--- name gender age score0 Snow M 22 801 Tyrion M 32 982 Sansa F 18 673 Arya F 14 90 在具体某个位置插⼊⼀列可以⽤insert的⽅法, 语法格式：列表.insert(index, obj) index ---&gt;对象 obj 需要插⼊的索引位置。 obj---&gt; 要插⼊列表中的对象（列名） 将数据框的列名全部提取出来存放在列表⾥ 12345col_name=df1.columns.tolist() print(col_name)---['name', 'gender', 'age', 'score'] 在列索引为2的位置插⼊⼀列,列名为:city 12345col_name.insert(2,'city')print(col_name)---['name', 'gender', 'city', 'age', 'score'] 刚插⼊时不会有值，整列都是NaN,我们使用DataFrame.reindex()对原⾏/列索引重新构建索引值 123456789df1=df1.reindex(columns=col_name)print(df1)--- name gender city age score0 Snow M NaN 22 801 Tyrion M NaN 32 982 Sansa F NaN 18 673 Arya F NaN 14 90 给city列赋值 123456789df1['city']=['北京京','⼭⻄西','湖北北','澳⻔门'] print(df1)--- name gender city age score0 Snow M 北京京 22 801 Tyrion M ⼭⻄西 32 982 Sansa F 湖北北 18 673 Arya F 澳⻔门 14 90 df中的insert是插⼊⼀列。语法和关键参数为： df.insert(iloc,column,value) iloc:要插⼊的位置 colunm:列名 value:值 刚才我们插入city列的时候省略了value,所以新建列值全部为NaN，这次我们加上再看： 123456789df1.insert(2,'score2',[80,98,67,90]) print(df1)--- name gender score2 city age score0 Snow M 80 北京京 22 801 Tyrion M 98 ⼭⻄西 32 982 Sansa F 67 湖北北 18 673 Arya F 90 澳⻔门 14 90 插⼊⼀⾏ 1234567891011# 插⼊⼀⾏row=['111','222','333','444','555','666'] df1.iloc[1]=rowprint(df1)--- name gender score2 city age score0 Snow M 80 北京京 22 801 111 222 333 444 555 6662 Sansa F 67 湖北北 18 673 Arya F 90 澳⻔门 14 90 插入行的时候，列个数必须对应才可以，否则会报错。 目前这组数据已经被我们玩乱了，我们再重新生成一组数据来看后面的： 123456789df1 = pd.DataFrame([['Snow','M',22],['Tyrion','M',32],['Sansa','F',18],['Arya','F',14]],columns=['name','gender','age'])print(df1)--- name gender age0 Snow M 221 Tyrion M 322 Sansa F 183 Arya F 14 再继续创建一组数据，我们将尝试将两组数据进行合并, 新创建的这组数据⽤来增加进数据框的最后⼀⾏。 123456new=pd.DataFrame({'name':'lisa','gender':'F','age':19},index=[0])print(new)--- name gender age0 lisa F 19 在原数据框df1最后⼀⾏新增⼀⾏，⽤append⽅法 12345678910df1=df1.append(new,ignore_index=True) print(df1)--- name gender age0 Snow M 221 Tyrion M 322 Sansa F 183 Arya F 144 lisa F 19 ignore_index=False,表示不按原来的索引， 从0开始⾃动递增。 objs:合并对象 axis:合并⽅式，默认0表示按列合并，1表示按⾏合并 ignore_index:是否忽略索引 12345678910111213df1 = pd.DataFrame(np.arange(6).reshape(3,2),columns=['four','five'])df2 = pd.DataFrame(np.arange(6).reshape(2,3),columns=['one','two','three'])print(df1)print(df2)--- four five0 0 11 2 32 4 5 one two three0 0 1 21 3 4 5 按行合并 12345678result = pd.concat([df1,df2],axis=1)print(result)--- four five one two three0 0 1 0.0 1.0 2.01 2 3 3.0 4.0 5.02 4 5 NaN NaN NaN 按列合并 123456 four five one two three0 0.0 1.0 NaN NaN NaN1 2.0 3.0 NaN NaN NaN2 4.0 5.0 NaN NaN NaN3 NaN NaN 0.0 1.0 2.04 NaN NaN 3.0 4.0 5.0 看结果我们能看出来，在合并的时候，如果对应不到值，那么就会默认添加NaN值。 DataFrame的删除 12345678910111213141516df2 = pd.DataFrame(np.arange(9).reshape(3,3),columns=['one','two','three'])print(df2)df3=df2.drop(['one'],axis=1, inplace=True)print(df2)print(df3)--- one two three0 0 1 21 3 4 52 6 7 8 two three0 1 21 4 52 7 8None lables：要删除数据的标签 axis：0表示删除⾏，1表示删除列，默认0 inplace:是否在当前df中执⾏此操作 最后的返回值为None，原因是我们设置inplace为True，在当前df中执行操作。如果我们将其设置为False，则会将操作后的值进行返回，生成一个新的对象。 1234567891011df3=df2.drop([0,1],axis=0, inplace=False)print(df2)print(df3)--- one two three0 0 1 21 3 4 52 6 7 8 one two three2 6 7 8 数据处理 在我们查看完DataFrame的基础操作之后，我们现在来正式开始数据处理。 我们可以通过通过dropna()滤除缺失数据，先让我们创建一组数据： 12345678910se=pd.Series([4,NaN,8,NaN,5]) print(se)---0 4.01 NaN2 8.03 NaN4 5.0dtype: float64 尝试清除缺失数据，也就是NaN值： 1234567print(se.dropna())---0 4.02 8.04 5.0dtype: float64 在清除数据之前，我们有两个方法可以判断当前数据中是否有缺失数据，不过这两个方法的判断方式是相反的，一个是判断不是缺失数据，一个判断是缺失数据： 12345678910111213141516print(se.notnull())print(se.isnull())---0 True1 False2 True3 False4 Truedtype: bool0 False1 True2 False3 True4 Falsedtype: bool 那既然有方法可以进行判断当前数据是否为缺失数据，那么我们用之前的方法与其配合，一样可以做滤除操作： 1234567print(se[se.notnull()])---0 4.02 8.04 5.0dtype: float64 当然，除了Series对象之外，我们还需要进行处理DataFrame对象 123456789df1=pd.DataFrame([[1,2,3],[NaN,NaN,2],[NaN,NaN,NaN],[8,8,NaN]]) print(df1)--- 0 1 20 1.0 2.0 3.01 NaN NaN 2.02 NaN NaN NaN3 8.0 8.0 NaN 默认滤除所有包含NaN： 12345print(df1.dropna())--- 0 1 20 1.0 2.0 3.0 传⼊how=‘all’滤除全为NaN的⾏： 1234567print(df1.dropna(how='all')) --- 0 1 20 1.0 2.0 3.01 NaN NaN 2.03 8.0 8.0 NaN 可以看到，除了下标为2的那一行之外，其余含NaN值的行都被保留了。 之前操作最后只留下一行，原因是how的默认值为how='any'。只要是nan就删除 传⼊axis=1滤除列： 12345678print(df1.dropna(axis=1,how=&quot;all&quot;))--- 0 1 20 1.0 2.0 3.01 NaN NaN 2.02 NaN NaN NaN3 8.0 8.0 NaN 为什么没有变化？按列来查看，没有一列是全是NaN值的。 除了how值外，我们还可以可以使用thresh来精准操作，它可以传入一个数值n，会保留至少n个非NaN数据的行或列： 123456print(df1.dropna(thresh=2))--- 0 1 20 1.0 2.0 3.03 8.0 8.0 NaN 仅有一个非NaN的行和全部为NaN的行就都被滤除了。 那NaN是不是就只能被删除了呢？并不是，还记得我们之前操作的时候我提到过，我们大多数遇到NaN值的时候，基本都是用平均值来进行填充，这是一个惯例操作。 那么，我们来看看如何填充缺失数据 123456789df1=pd.DataFrame([[1,2,3],[NaN,NaN,2],[NaN,NaN,NaN],[8,8,NaN]]) print(df1)--- 0 1 20 1.0 2.0 3.01 NaN NaN 2.02 NaN NaN NaN3 8.0 8.0 NaN ⽤常数填充fillna 12345678print(df1.fillna(0))--- 0 1 20 1.0 2.0 3.01 0.0 0.0 2.02 0.0 0.0 0.03 8.0 8.0 0.0 传⼊inplace=True直接修改原对象： 123456789df1.fillna(0,inplace=True) print(df1)--- 0 1 20 1.0 2.0 3.01 0.0 0.0 2.02 0.0 0.0 0.03 8.0 8.0 0.0 通过字典填充不同的常数 12345678print(df1.fillna({0:10,1:20,2:30}))--- 0 1 20 1.0 2.0 3.01 10.0 20.0 2.02 10.0 20.0 30.03 8.0 8.0 30.0 还有我们之前提到过的，填充平均值： 12345678print(df1.fillna(df1.mean()))--- 0 1 20 1.0 2.0 3.01 4.5 5.0 2.02 4.5 5.0 2.53 8.0 8.0 2.5 当然，我们可以只填充某一列 12345678910print(df1.iloc[:,1].fillna(5,inplace = True)) print(df1)---None 0 1 20 1.0 2.0 3.01 NaN 5.0 2.02 NaN 5.0 NaN3 8.0 8.0 NaN 传⼊method=” “会改变插值⽅式，先来一组数据，并在其中加上NaN值 123456789101112df2=pd.DataFrame(np.random.randint(0,10,(5,5))) df2.iloc[1:4,3]=NaNdf2.iloc[2:4,4]=NaNprint(df2)--- 0 1 2 3 40 3 5 9 9.0 3.01 0 1 2 NaN 8.02 6 5 8 NaN NaN3 5 6 5 NaN NaN4 5 3 5 8.0 2.0 现在，我们用前面的值来填充， method ='ffill' 123456789print(df2.fillna(method='ffill'))--- 0 1 2 3 40 3 5 9 9.0 3.01 0 1 2 9.0 8.02 6 5 8 9.0 8.03 5 6 5 9.0 8.04 5 3 5 8.0 2.0 用后面的值来填充method='bfill': 123456789print(df2.fillna(method='bfill',limit=1))--- 0 1 2 3 40 7 1 8 0.0 0.01 6 8 4 NaN 5.02 6 2 5 NaN NaN3 4 8 0 1.0 3.04 8 0 2 1.0 3.0 以上代码中，我们还传入了limit， 用于限制填充行数。 当我们传入axis的时候，会修改填充方向： 123456789print(df2.fillna(method=&quot;ffill&quot;,limit=1,axis=1))--- 0 1 2 3 40 0.0 8.0 9.0 4.0 7.01 2.0 8.0 0.0 0.0 9.02 1.0 8.0 5.0 5.0 NaN3 0.0 0.0 3.0 3.0 NaN4 2.0 9.0 4.0 6.0 3.0 接着，我们再来看看如何移除重复数据，俗称「去重」： DataFrame中经常会出现重复⾏，利⽤duplicated()函数返回每⼀⾏判断是否重复的结果（重复则为 True) 12345678910111213141516171819202122df1=pd.DataFrame({'A':[1,1,1,2,2,3,1],'B':list(&quot;aabbbca&quot;)})print(df1)print(df1.duplicated())--- A B0 1 a1 1 a2 1 b3 2 b4 2 b5 3 c6 1 a0 False1 True2 False3 False4 True5 False6 Truedtype: bool 去除全部的重复⾏ 12345678print(df1.drop_duplicates())--- A B0 1 a2 1 b3 2 b5 3 c 指定列去除重复行 1234567print(df1.drop_duplicates(['A']))--- A B0 1 a3 2 b5 3 c 保留重复⾏中的最后⼀⾏ 12345678df1=pd.DataFrame({'A':[1,1,1,2,2,3,1],'B':list(&quot;aabbbca&quot;)})print(df1.drop_duplicates(['A'],keep='last'))--- A B4 2 b5 3 c6 1 a 去除重复的同时改变DataFrame对象 123456789df1.drop_duplicates(['A','B'],inplace=True) print(df1)--- A B0 1 a2 1 b3 2 b5 3 c 数据合并 我们平时会与拿到的数据可能存储在不同的数据表中，这需要我们对数据进行合并，然后再进行操作。 使⽤join合并，着重关注的是⾏的合并。 简单合并（默认是left左连接,以左侧df3为基础） 123456789101112131415161718192021df3=pd.DataFrame({'Red':[1,3,5],'Green':[5,0,3]},index=list('abc'))df4=pd.DataFrame({'Blue':[1,9,8],'Yellow':[6,6,7]},index=list('cde')) print(df3)print(df4)df3.join(df4,how='left')--- Red Greena 1 5b 3 0c 5 3 Blue Yellowc 1 6d 9 6e 8 7 Red Green Blue Yellowa 1 5 NaN NaNb 3 0 NaN NaNc 5 3 1.0 6.0 右链接 123456789df3.join(df4,how='outer')--- Red Green Blue Yellowa 1.0 5.0 NaN NaNb 3.0 0.0 NaN NaNc 5.0 3.0 1.0 6.0d NaN NaN 9.0 6.0e NaN NaN 8.0 7.0 合并多个DataFrame对象 123456789df5=pd.DataFrame({'Brown':[3,4,5],'White':[1,1,2]},index=list('aed')) df3.join([df4,df5])--- Red Green Blue Yellow Brown Whitea 1.0 5.0 NaN NaN 3.0 1.0b 3.0 0.0 NaN NaN NaN NaNc 5.0 3.0 1.0 6.0 NaN NaN 使⽤merge，着重关注的是列的合并。 我们先来构建两组数据： 12345678910111213df1=pd.DataFrame({'名字':list('ABCDE'),'性别':['男','⼥','男','男','⼥'],'职称': ['副教授','讲师','助教','教授','助教']},index=range(1001,1006))df1.columns.name='学院⽼师'df1.index.name='编号'df1---学院⽼师 名字 性别 职称编号 1001 A 男 副教授1002 B ⼥ 讲师1003 C 男 助教1004 D 男 教授1005 E ⼥ 助教 12345678910111213df2=pd.DataFrame({'名字':list('ABDAX'),'课程':['C++','计算机导论','汇编','数据结构','马克思原理'],'职称':['副教授','讲师','教授','副教授','讲师']},index= [1001,1002,1004,1001,3001])df2.columns.name='课程'df2.index.name='编号'print(df2)---课程 名字 课程 职称编号 1001 A C++ 副教授1002 B 计算机导论 讲师1004 D 汇编 教授1001 A 数据结构 副教授3001 X 马克思原理 讲师 默认下是根据左右对象中出现同名的列作为连接的键，且连接⽅式是how=’inner’ 12345678print(pd.merge(df1,df2)) # 返回匹配的--- 名字 性别 职称 课程0 A 男 副教授 C++1 A 男 副教授 数据结构2 B ⼥ 讲师 计算机导论3 D 男 教授 汇编 指定列名合并 123456789pd.merge(df1,df2,on='名字',suffixes=['_1','_2']) # 返回匹配的--- 名字 性别 职称_1 课程 职称_20 A 男 副教授 C++ 副教授1 A 男 副教授 数据结构 副教授2 B ⼥ 讲师 计算机导论 讲师3 D 男 教授 汇编 教授 连接⽅式，根据左侧为准 12345678910pd.merge(df1,df2,how='left')--- 名字 性别 职称 课程0 A 男 副教授 C++1 A 男 副教授 数据结构2 B ⼥ 讲师 计算机导论3 C 男 助教 NaN4 D 男 教授 汇编5 E ⼥ 助教 NaN 根据右侧为准 123456789pd.merge(df1,df2,how='right')--- 名字 性别 职称 课程0 A 男 副教授 C++1 B ⼥ 讲师 计算机导论2 D 男 教授 汇编3 A 男 副教授 数据结构4 X NaN 讲师 马克思原理 所有的数据 123456789101112pd.merge(df1,df2,how='outer')---名字 性别 职称 课程0 A 男 副教授 C++1 A 男 副教授 数据结构2 B ⼥ 讲师 计算机导论3 C 男 助教 NaN4 D 男 教授 汇编5 E ⼥ 助教 NaN6 X NaN 讲师 马克思原理 根据多个键进⾏连接 12345678pd.merge(df1,df2,*on*=['职称','名字'])--- 名字 性别 职称 课程0 A 男 副教授 C++1 A 男 副教授 数据结构2 B ⼥ 讲师 计算机导论3 D 男 教授 汇编 除此之外，我们还有一种轴向连接的方式：Concat Series对象的连接 1234567891011121314151617181920s1=pd.Series([1,2],index=list('ab'))s2=pd.Series([3,4,5],index=list('bde')) print(s1)print(s2)pd.concat([s1,s2])---a 1b 2dtype: int64b 3d 4e 5dtype: int64a 1b 2b 3d 4e 5dtype: int64 横向连接 12345678pd.concat([s1,s2],*axis*=1)--- 0 1a 1.0 NaNb 2.0 3.0d NaN 4.0e NaN 5.0 ⽤内连接求交集(连接⽅式，共有’inner’,’left’,right’,’outer’) 12345pd.concat([s1,s2],axis=1,join='inner')--- 0 1b 2 3 创建层次化索引 123456789pd.concat([s1,s2],keys=['A','B'])---A a 1 b 2B b 3 d 4 e 5dtype: int64 当纵向连接时keys为列名 12345678pd.concat([s1,s2],keys=['A','D'],axis=1)---- A Da 1.0 NaNb 2.0 3.0d NaN 4.0e NaN 5.0 DataFrame对象的连接 123456789101112df3=pd.DataFrame({'Red':[1,3,5],'Green':[5,0,3]},index=list('abd')) df4=pd.DataFrame({'Blue':[1,9],'Yellow':[6,6]},index=list('ce'))pd.concat([df3,df4])--- Red Green Blue Yellowa 1.0 5.0 NaN NaNb 3.0 0.0 NaN NaNd 5.0 3.0 NaN NaNc NaN NaN 1.0 6.0e NaN NaN 9.0 6.0 ⽤字典的⽅式连接同样可以创建层次化列索引 12345678910pd.concat({'A':df3,'B':df4},axis=1)--- A B Red Green Blue Yellowa 1.0 5.0 NaN NaNb 3.0 0.0 NaN NaNd 5.0 3.0 NaN NaNc NaN NaN 1.0 6.0e NaN NaN 9.0 6.0 多层索引（拓展） 创建多层索引 1234567891011s = Series(np.random.randint(0,150,size=6),index=list('abcdef'))print(s)---a 40b 122c 95d 40e 35f 27dtype: int64 123456789101112s = Series(np.random.randint(0,150,size=6),index=[['a','a','b','b','c','c'],['期中','期末','期中','期末','期中','期末']]) print(s)---a 期中 132 期末 145b 期中 33 期末 149c 期中 10 期末 145dtype: int64 DataFrame也可以创建多层索引 1234567891011121314# DataFrame创建多层索引df1 = df(np.random.randint(0,150,size=(6,4)), columns = ['zs','ls','ww','zl'], index =[['python','python','math','math','En','En'],['期中','期末','期中','期末','期中','期末']])print(df1)--- zs ls ww zlpython 期中 123 3 98 95 期末 9 36 15 126math 期中 86 86 73 115 期末 3 130 52 89En 期中 75 21 84 98 期末 56 46 111 147 特定结构 1234567891011121314class1=['python','python','math','math','En','En']class2=['期中','期末','期中','期末','期中','期末']m_index2=pd.MultiIndex.from_arrays([class1,class2])df2=df(np.random.randint(0,150,(6,4)),index=m_index2) print(df2)--- 0 1 2 3python 期中 94 36 6 19 期末 24 41 108 120math 期中 79 69 144 32 期末 138 100 42 38En 期中 110 90 123 75 期末 69 59 72 109 1234567891011121314class1=['期中','期中','期中','期末','期末','期末']class2=['python','math','En','python','math','En']m_index2=pd.MultiIndex.from_arrays([class1,class2])df2=df(np.random.randint(0,150,(6,4)),index=m_index2) print(df2)--- 0 1 2 3期中 python 96 15 135 5 math 66 78 143 93 En 70 27 120 63期末 python 147 77 92 97 math 121 81 137 102 En 18 12 134 113 product构造 1234567891011121314class1=['python','math','En']class2=['期中','期末']m_index2=pd.MultiIndex.from_product([class1,class2])df2=df(np.random.randint(0,150,(6,4)),index=m_index2) print(df2)--- 0 1 2 3python 期中 12 72 115 59 期末 36 51 94 111math 期中 44 14 9 61 期末 115 121 65 93En 期中 29 23 16 70 期末 30 73 77 53 多层索引对象的索引 让我们先来看看Series的操作 123456789101112s = Series(np.random.randint(0,150,size=6),index=[['a','a','b','b','c','c'],['期中','期末','期中','期末','期中','期末']])print(s)---a 期中 31 期末 4b 期中 101 期末 95c 期中 54 期末 126dtype: int64 取⼀个第⼀级索引 123456print(s['a'])---期中 31期末 4dtype: int64 取多个第⼀级索引 12345678print(s[['a','b']])---a 期中 31 期末 4b 期中 101 期末 95dtype: int64 根据索引获取值 1234print(s['a','期末'])---4 loc⽅法取值 1234567891011121314print(s.loc['a'])print(s.loc[['a','b']]) print(s.loc['a','期末'])---期中 31期末 4dtype: int64a 期中 31 期末 4b 期中 101 期末 95dtype: int644 iloc⽅法取值(iloc计算的事最内层索引) 123456789print(s.iloc[1])print(s.iloc[1:4])---4a 期末 4b 期中 101 期末 95dtype: int64 然后再让我们来看看DataFrame的操作 123456789101112131415# dataframeclass1=['python','math','En']class2=['期中','期末']m_index2=pd.MultiIndex.from_product([class1,class2])df2=df(np.random.randint(0,150,(6,4)),index=m_index2) print(df2)--- 0 1 2 3python 期中 88 69 82 28 期末 110 60 130 133math 期中 64 103 24 49 期末 23 41 10 61En 期中 124 139 65 115 期末 114 13 117 79 获取列 12345678910print(df2[0])---python 期中 88 期末 110math 期中 64 期末 23En 期中 124 期末 114Name: 0, dtype: int64 ⼀级索引 1234567print(df2.loc['python'])--- 0 1 2 3期中 88 69 82 28期末 110 60 130 133 多个⼀级索引 12345678print(df2.loc[['python','math']])--- 0 1 2 3python 期中 88 69 82 28 期末 110 60 130 133math 期中 64 103 24 49 期末 23 41 10 61 取⼀⾏ 12345678print(df2.loc['python','期末'])---0 1101 602 1303 133Name: (python, 期末), dtype: int64 取⼀值 1234print(df2.loc['python','期末'][0])---110 iloc是只取最内层的索引的 12345678print(df2.iloc[0])---0 881 692 823 28Name: (python, 期中), dtype: int64 时间序列 ⽣成⼀段时间范围 该函数主要⽤于⽣成⼀个固定频率的时间索引，在调⽤构造⽅法时，必须指定start、end、periods中的两个参数值，否则报错。 时间序列频率 解释 D ⽇历⽇的每天 B ⼯作⽇的每天 H 每⼩时 T或min 每分钟 S 每秒 L或ms 每毫秒 U 每微秒 M ⽇历⽇的⽉底⽇期 BM ⼯作⽇的⽉底⽇期 MS ⽇历⽇的⽉初⽇期 BMS ⼯作⽇的⽉初⽇期 12345678910111213date = pd.date_range(start='20190501',end='20190530')print(date)---DatetimeIndex(['2023-05-01', '2023-05-02', '2023-05-03', '2023-05-04', '2023-05-05', '2023-05-06', '2023-05-07', '2023-05-08', '2023-05-09', '2023-05-10', '2023-05-11', '2023-05-12', '2023-05-13', '2023-05-14', '2023-05-15', '2023-05-16', '2023-05-17', '2023-05-18', '2023-05-19', '2023-05-20', '2023-05-21', '2023-05-22', '2023-05-23', '2023-05-24', '2023-05-25', '2023-05-26', '2023-05-27', '2023-05-28', '2023-05-29', '2023-05-30'], dtype='datetime64[ns]', freq='D') req：⽇期偏移量，取值为string, 默认为'D'， periods：固定时期，取值为整数或None freq: 时间序列频率 12345678date = pd.date_range(start='20230501',periods=10,freq='10D')print(date)---DatetimeIndex(['2023-05-01', '2023-05-11', '2023-05-21', '2023-05-31', '2023-06-10', '2023-06-20', '2023-06-30', '2023-07-10', '2023-07-20', '2023-07-30'], dtype='datetime64[ns]', freq='10D') 根据closed参数选择是否包含开始和结束时间closed=None，left包含开始时间，不包含结束时间， right与之相反。 1234567data_time =pd.date_range(start='2023-08-09',end='2023-08-14',closed='left') print(data_time)---DatetimeIndex(['2023-08-09', '2023-08-10', '2023-08-11', '2023-08-12', '2023-08-13'], dtype='datetime64[ns]', freq='D') 时间序列在dataFrame中的作⽤ 可以将时间作为索引 12345678910111213141516index = pd.date_range(start='20230801',periods=10)df = pd.Series(np.random.randint(0,10,size = 10),index=index) print(df)---2023-08-01 72023-08-02 22023-08-03 52023-08-04 52023-08-05 22023-08-06 02023-08-07 22023-08-08 32023-08-09 62023-08-10 5Freq: D, dtype: int64 truncate这个函数将before指定⽇期之前的值全部过滤出去,after指定⽇期之前的值全部过滤出去. 12345678910111213after = df.truncate(after='2023-08-8')print(after)---2023-08-01 72023-08-02 22023-08-03 52023-08-04 52023-08-05 22023-08-06 02023-08-07 22023-08-08 3Freq: D, dtype: int64 12345678long_ts = pd.Series(np.random.randn(1000),index=pd.date_range('1/1/2021',periods=1000)) print(long_ts)---2021-01-01 -0.482811...2023-09-27 -0.108047Freq: D, Length: 1000, dtype: float64 根据年份获取 12345678result = long_ts['2022']print(result)---2022-01-01 -0.600007...2022-12-31 0.097874Freq: D, Length: 365, dtype: float64 年份和⽇期获取 12345678result = long_ts['2023-07'] print(result)---2023-07-01 -1.797582...2023-07-31 0.687787Freq: D, dtype: float64 使⽤切⽚ 123456789101112# 使⽤切⽚result = long_ts['2023-05-01':'2023-05-06']print(result)---2023-05-01 -2.3382182023-05-02 -2.1307802023-05-03 0.5829202023-05-04 -0.1825402023-05-05 0.1273632023-05-06 -0.032844Freq: D, dtype: float64 通过between_time()返回位于指定时间段的数据集 123456789index=pd.date_range(&quot;2023-03-17&quot;,&quot;2023-03-30&quot;,freq=&quot;2H&quot;)ts = pd.Series(np.random.randn(157),index=index) print(ts.between_time(&quot;7:00&quot;,&quot;17:00&quot;))---2023-03-17 08:00:00 -0.532254...2023-03-29 16:00:00 0.437697Length: 65, dtype: float64 这些操作也都适⽤于dataframe 12345678910111213141516index=pd.date_range('1/1/2023',periods=100)df = pd.DataFrame(np.random.randn(100,4),index=index) print(df.loc['2023-04'])--- 0 1 2 32023-04-01 -0.220090 0.335770 -0.086181 -0.0460452023-04-02 -1.046423 -0.347116 0.367099 -0.9793542023-04-03 -0.720944 -1.478932 0.220948 0.8018312023-04-04 1.359946 -1.239004 0.309747 -0.0479592023-04-05 -0.256502 2.224782 0.494740 -1.3224902023-04-06 1.488119 0.244942 0.614101 -0.1562012023-04-07 -1.815019 -1.935966 0.239024 -1.3885022023-04-08 1.106623 1.148805 2.120405 -0.7992902023-04-09 -1.902216 0.625965 -0.102506 -0.4305502023-04-10 -0.876382 -2.034205 -0.060846 2.442651 移位⽇期 123456789101112131415ts = pd.Series(np.random.randn(10),index=pd.date_range('1/1/2023',periods=10)) print(ts)---2023-01-01 -0.9769582023-01-02 -0.4874392023-01-03 0.1431042023-01-04 -0.9642362023-01-05 0.7583262023-01-06 -1.6508182023-01-07 0.7092312023-01-08 0.1987142023-01-09 -1.0434432023-01-10 0.220834Freq: D, dtype: float64 移动数据，索引不变，默认由NaN填充 periods: 移动的位数 负数是向上移动 fill_value: 移动后填充数据 freq： ⽇期偏移量 1234567891011121314ts.shift(periods=2,fill_value=100, freq='D')---2023-01-03 -0.9769582023-01-04 -0.4874392023-01-05 0.1431042023-01-06 -0.9642362023-01-07 0.7583262023-01-08 -1.6508182023-01-09 0.7092312023-01-10 0.1987142023-01-11 -1.0434432023-01-12 0.220834Freq: D, dtype: float64 通过tshift()将索引移动指定的时间： 1234567891011121314ts.tshift(2)---2023-01-03 -0.9769582023-01-04 -0.4874392023-01-05 0.1431042023-01-06 -0.9642362023-01-07 0.7583262023-01-08 -1.6508182023-01-09 0.7092312023-01-10 0.1987142023-01-11 -1.0434432023-01-12 0.220834Freq: D, dtype: float64 将时间戳转化成时间根式 1234pd.to_datetime(1688570740000,unit='ms')---Timestamp('2023-07-05 15:25:40') utc是协调世界时,时区是以UTC的偏移量的形式表示的,但是注意设置utc=True,是让pandas对象具有时区性质,对于⼀列进⾏转换的,会造成转换错误。 unit='ms'设置粒度是到毫秒级别的。 时区名字 12345import pytzprint(pytz.common_timezones)---['Africa/Abidjan', ..., 'US/Pacific', 'UTC'] 1234pd.to_datetime(1688570740000,unit='ms').tz_localize('UTC').tz_convert('Asia/Shanghai')---Timestamp('2023-07-05 23:25:40+0800', tz='Asia/Shanghai') 一个处理的例子： 12345678df = pd.DataFrame([1688570740000, 1688570800000, 1688570860000],columns = ['time_stamp'])pd.to_datetime(df['time_stamp'],unit='ms').dt.tz_localize('UTC').dt.tz_convert ('Asia/Shanghai')---0 2023-07-05 23:25:40+08:001 2023-07-05 23:26:40+08:002 2023-07-05 23:27:40+08:00Name: time_stamp, dtype: datetime64[ns, Asia/Shanghai] 先赋予标准时区,再转换到东⼋区。 处理中⽂ 1234pd.to_datetime('2023年7⽉5⽇',format='%Y年%m⽉%d⽇')---Timestamp('2023-07-05 00:00:00') 分组聚合 123456789101112131415161718df=pd.DataFrame({'name':['BOSS','Lilei','Lilei','Han','BOSS','BOSS','Han','BOSS'], 'Year':[2016,2016,2016,2016,2017,2017,2017,2017],'Salary':[999999,20000,25000,3000,9999999,999999,3500,999999],'Bonus':[100000,20000,20000,5000,200000,300000,3000,400000]})df--- name Year Salary Bonus0 BOSS 2016 999999 1000001 Lilei 2016 20000 200002 Lilei 2016 25000 200003 Han 2016 3000 50004 BOSS 2017 9999999 2000005 BOSS 2017 999999 3000006 Han 2017 3500 30007 BOSS 2017 999999 400000 根据name这⼀列进⾏分组 12345group_by_name=df.groupby('name') print(type(group_by_name))---&lt;class 'pandas.core.groupby.generic.DataFrameGroupBy'&gt; 查看分组 12345678910print(group_by_name.groups) # 分组后的数量print(group_by_name.count())---{'BOSS': [0, 4, 5, 7], 'Han': [3, 6], 'Lilei': [1, 2]} Year Salary Bonusname BOSS 4 4 4Han 2 2 2Lilei 2 2 2 查看分组的情况 1234567for name,group in group_by_name: print(name) ---BOSSHanLilei 组的名字 123456print(group) # 组具体内容--- name Year Salary Bonus1 Lilei 2016 20000 200002 Lilei 2016 25000 20000 可以选择分组 12345678print(group_by_name.get_group('BOSS'))--- name Year Salary Bonus0 BOSS 2016 999999 1000004 BOSS 2017 9999999 2000005 BOSS 2017 999999 3000007 BOSS 2017 999999 400000 按照某⼀列进⾏分组, 将name这⼀列作为分组的键，对year进⾏分组 123456789group_by_name=df['Year'].groupby(df['name'])print(group_by_name.count())---nameBOSS 4Han 2Lilei 2Name: Year, dtype: int64 按照多列进⾏分组 123456789101112131415161718192021222324group_by_name_year=df.groupby(['name','Year'])for name,group in group_by_name_year: print(name) # 组的名字 print(group) # 组具体内容 ---('BOSS', 2016) name Year Salary Bonus0 BOSS 2016 999999 100000('BOSS', 2017) name Year Salary Bonus4 BOSS 2017 9999999 2000005 BOSS 2017 999999 3000007 BOSS 2017 999999 400000('Han', 2016) name Year Salary Bonus3 Han 2016 3000 5000('Han', 2017) name Year Salary Bonus6 Han 2017 3500 3000('Lilei', 2016) name Year Salary Bonus1 Lilei 2016 20000 200002 Lilei 2016 25000 20000 可以选择分组 12345print(group_by_name_year.get_group(('BOSS',2016)))--- name Year Salary Bonus0 BOSS 2016 999999 100000 将某列数据按数据值分成不同范围段进⾏分组（groupby）运算 123456789101112df = pd.DataFrame({'Age': np.random.randint(20, 70, 100), 'Sex': np.random.choice(['M', 'F'], 100), })age_groups = pd.cut(df['Age'], bins=[19,40,65,100])print(df.groupby(age_groups).count())--- Age SexAge (19, 40] 35 35(40, 65] 54 54(65, 100] 11 11 按‘Age’分组范围和性别（sex）进⾏制作交叉表 123456789pd.crosstab(age_groups, df['Sex'])---Sex F MAge (19, 40] 18 22(40, 65] 25 27(65, 100] 3 5 聚合 我们先来看聚合函数的表格 聚合函数 解释 mean 计算分组平均值 count 分组中⾮NA值的数量 sum ⾮NA值的和 median ⾮NA值的算术中位数 std 标准差 var ⽅差 min ⾮NA值的最⼩值 max ⾮NA值的最⼤值 prod ⾮NA值的积 first 第⼀个⾮NA值 last 最后⼀个⾮NA值 mad 平均绝对偏差 mode 模 abs 绝对值 sem 平均值的标准误差 skew 样品偏斜度（三阶矩） kurt 样品峰度（四阶矩） quantile 样本分位数（百分位上的值） cumsum 累积总和 cumprod 累积乘积 cummax 累积最⼤值 cummin 累积最⼩值 12345678910111213df1=pd.DataFrame({'Data1':np.random.randint(0,10,5), 'Data2':np.random.randint(10,20,5), 'key1':list('aabba'), 'key2':list('xyyxy')})print(df1)--- Data1 Data2 key1 key20 4 17 a x1 4 13 a y2 0 12 b y3 5 16 b x4 8 10 a y 按key1分组，进⾏聚合计算 ⚠️ 注意：当分组后进⾏数值计算时，不是数值类的列（即麻烦列）会被清除 1234567print(df1.groupby('key1').sum())--- Data1 Data2key1 a 16 40b 5 28 只算data1 123456789101112print(df1['Data1'].groupby(df1['key1']).sum()) print(df1.groupby('key1')['Data1'].sum())---key1a 16b 5Name: Data1, dtype: int64key1a 16b 5Name: Data1, dtype: int64 使⽤agg()函数做聚合运算 1234567print(df1.groupby('key1').agg('sum'))--- Data1 Data2key1 a 16 40b 5 28 可以同时做多个聚合运算 12345678print(df1.groupby('key1').agg(['sum','mean','std']))--- Data1 Data2 sum mean std sum mean stdkey1 a 16 5.333333 2.309401 40 13.333333 3.511885b 5 2.500000 3.535534 28 14.000000 2.828427 可⾃定义函数，传⼊agg⽅法中 grouped.agg(func) 123456789101112def peak_range(df): &quot;&quot;&quot; 返回数值范围 &quot;&quot;&quot; return df.max() - df.min()print(df1.groupby('key1').agg(peak_range))--- Data1 Data2key1 a 4 7b 5 4 同时应⽤多个聚合函数 1234567891011121314print(df1.groupby('key1').agg(['mean', 'std', 'count', peak_range])) # 默认列名为函数名--- Data1 Data2 \\ mean std count peak_range mean std count key1 a 5.333333 2.309401 3 4 13.333333 3.511885 3 b 2.500000 3.535534 2 5 14.000000 2.828427 2 peak_range key1 a 7 b 4 通过元组提供新的列名 12345678print(df1.groupby('key1').agg(['mean', 'std', 'count', ('range', peak_range)])) --- Data1 Data2 mean std count range mean std count rangekey1 a 5.333333 2.309401 3 4 13.333333 3.511885 3 7b 2.500000 3.535534 2 5 14.000000 2.828427 2 4 给每列作⽤不同的聚合函数 123456789101112dict_mapping = { 'Data1':['mean','max'], 'Data2':'sum'}df1.groupby('key1').agg(dict_mapping)--- Data1 Data2 mean max sumkey1 a 5.333333 8 40b 2.500000 5 28 拓展apply()函数 apply函数是pandas⾥⾯所有函数中⾃由度最⾼的函数 123456789101112df1=pd.DataFrame({'sex':list('FFMFMMF'),'smoker':list('YNYYNYY'),'age': [21,30,17,37,40,18,26],'weight':[120,100,132,140,94,89,123]})print(df1)--- sex smoker age weight0 F Y 21 1201 F N 30 1002 M Y 17 1323 F Y 37 1404 M N 40 945 M Y 18 896 F Y 26 123 抽烟的年龄⼤于等18的 1234567891011121314151617def bin_age(age): if age &gt;=18: return 1 else: return 0 print(df1['age'].apply(bin_age))---0 11 12 03 14 15 16 1Name: age, dtype: int64 123456789101112df1['age'] = df1['age'].apply(bin_age) print(df1)--- sex smoker age weight0 F Y 1 1201 F N 1 1002 M Y 0 1323 F Y 1 1404 M N 1 945 M Y 1 896 F Y 1 123 取出抽烟和不抽烟的体重前⼆ 1234567891011def top(smoker,col,n=5): return smoker.sort_values(by=col)[-n:]df1.groupby('smoker').apply(top,col='weight',n=2)--- sex smoker age weightsmoker N 4 M N 1 94 1 F N 1 100 Y 2 M Y 0 132 3 F Y 1 140 按理来说，我们最后展示数据的时候，在用完age上0,1作为判断之后，要恢复成原本的年龄的数据。不过...就这样吧。因为马上，我们要做一个完整的分组案例，从一个csv文件获取数据，然后分组，最后进行数据可视化展示： 分组案例 我们先来读取数据，案例中使用到的数据会上传到我的Github仓库中。 12345678910111213141516171819202122232425262728293031data = pd.read_csv('./data/movie_metadata.csv')print('数据的形状：', data.shape) print(data.head())---数据的形状： (5043, 28) movie_title language country \\0 Avatar English USA 1 Pirates of the Caribbean: At World's End English USA 2 Spectre English UK 3 The Dark Knight Rises English USA 4 Star Wars: Episode VII - The Force Awakens ... NaN NaN content_rating title_year color duration genres \\0 PG-13 2009-02 Color 178.0 Action|Adventure|Fantasy|Sci-Fi 1 PG-13 2007-09 Color 169.0 Action|Adventure|Fantasy 2 PG-13 2015-11 Color 148.0 Action|Adventure|Thriller 3 PG-13 2012-08 Color 164.0 Action|Thriller 4 NaN NaN NaN NaN Documentary plot_keywords budget ... \\0 avatar|future|marine|native|paraplegic 237000000.0 ... 1 goddess|marriage ceremony|marriage proposal|pi... 300000000.0 ... 2 bomb|espionage|sequel|spy|terrorist 245000000.0 ... 3 deception|imprisonment|lawlessness|police offi... 250000000.0 ... 4 NaN NaN ... actor_2_facebook_likes actor_3_name actor_3_facebook_likes \\0 936.0 Wes Studi 855.0 1 5000.0 Jack Davenport 1000.0 ... 然后让我们来处理缺失值： 123456789101112131415161718192021222324252627282930data = data.dropna(how='any')print(data.head())--- movie_title language country content_rating \\0 Avatar English USA PG-13 1 Pirates of the Caribbean: At World's End English USA PG-13 2 Spectre English UK PG-13 3 The Dark Knight Rises English USA PG-13 5 John Carter English USA PG-13 title_year color duration genres \\0 2009-02 Color 178.0 Action|Adventure|Fantasy|Sci-Fi 1 2007-09 Color 169.0 Action|Adventure|Fantasy 2 2015-11 Color 148.0 Action|Adventure|Thriller 3 2012-08 Color 164.0 Action|Thriller 5 2012-07 Color 132.0 Action|Adventure|Sci-Fi plot_keywords budget ... \\0 avatar|future|marine|native|paraplegic 237000000.0 ... 1 goddess|marriage ceremony|marriage proposal|pi... 300000000.0 ... 2 bomb|espionage|sequel|spy|terrorist 245000000.0 ... 3 deception|imprisonment|lawlessness|police offi... 250000000.0 ... 5 alien|american civil war|male nipple|mars|prin... 263700000.0 ... actor_2_facebook_likes actor_3_name actor_3_facebook_likes \\0 936.0 Wes Studi 855.0 1 5000.0 Jack Davenport 1000.0 2 393.0 Stephanie Sigman 161.0 ... 接着，我们来查看一下票房收入统计 导演vs票房总收⼊ 1group_director = data.groupby(*by*='director_name')['gross'].sum() ascending升降序排列，True升序 12345678910111213141516171819result = group_director.sort_values() print(type(result))print(result)---&lt;class 'pandas.core.series.Series'&gt;director_nameEkachai Uekrongtham 1.620000e+02Frank Whaley 7.030000e+02Ricki Stern 1.111000e+03Alex Craig Mann 1.332000e+03Paul Bunnell 2.436000e+03 ... Sam Raimi 2.049549e+09Tim Burton 2.071275e+09Michael Bay 2.231243e+09Peter Jackson 2.592969e+09Steven Spielberg 4.114233e+09Name: gross, Length: 1660, dtype: float64 电影产量年份趋势 1234567891011from matplotlib import pyplot as plt import randomfrom matplotlib import font_managermovie_years = data.groupby('title_year')['movie_title']print(movie_years.count().index.tolist()) print(movie_years.count().values)---['1927-02', ..., '2016-12'][ 1 ... 6] 最后，我们利用之前学过的matplotlib进行数据可视化展示： 12345x = movie_years.count().index.tolist()y = movie_years.count().valuesplt.figure(figsize=(20,8),dpi=80)plt.plot(x,y)plt.show() 结尾 那小伙伴们，到今天为止，我们《AI秘籍》系列课程中的「Python部分」也就完全讲完了。就如我一开始向大家承诺的，这部分内容将完全免费。 而我们的《AI秘籍》课程也才刚刚开始，未来很长一段时间内，我们都将继续和这些基础内容频繁打交道。用它们来呈现我们之后要用到的所有课程。包括AI基础，CV，BI，NLP等课程。 不过在结束了一个阶段之后，我需要花点时间休整一下，也是为了好好的备课，找到最好的结构和顺序为大家编写后面的课程。本次课程的最后这两节课我都有些疲劳，为了赶快完成进度，编写过程当中可能有些粗糙或者遗漏，也请大家多包涵。日后，我可能会对这两节课进行更新，将一些细节的讲解补充完整。 免费部分结束了，日后的收费课程，也期望大家能一如即往的支持。 在这里，我也给大家推荐一些比较好的书籍，希望看到小伙伴们能够快速成长起来。 感谢大家，好了。本节课到此结束，下课了。","link":"/AI-Python-Pandas/"},{"title":"茶桁的AI秘籍 - 人工智能数学基础篇 导言","text":"Hi, 大家好。又见面了，我是茶桁。 在之前的一个多月前，我有了一个写一个AI系列的想法，起名为《茶桁的AI秘籍》，简单规划之后，于7月27日发出预告，然后历时二十多天将近一个月，完成了其中《Python篇》的写作。 不知道其中的内容对大家是否有帮助呢？ 那么今天我又回来了，根据规划，Python以及相关第三方科学计算库只是我们基础学习的一小部分，而很大一部分基础学习都还未进行。 那么这次，我依然给大家带来的是另外一篇基础部分，「人工智能数学基础篇」。 数学对于计算机编程来说重要性是毋庸置疑的，更何况我们现在不仅仅是编程，而是走在「人工智能」的路上。可以说，数学应该是最重要的基础。 我们在学习AI的过程当中可能会遇到的一些关于数学方面的一些东西，比如说线性代数里面的矩阵运算，比如说求导，还有一些概率统计，图论方面的一些东西。 如果您觉得自己对于微积分，线性代数，概率统计这些内容自认为掌握的还不错的同学，其实是可以不用看了。如果大家是从文科转过来或者说以前上的数学很多年了也忘的差不多了，那可以来学习一下这套课程。 你将会学到的 ✓ 掌握数据科学领域必备数学知识点 ✓ 掌握机器学习算法中常用数学 ✓ 通俗理解各项数学公式的作用 ✓ 掌握数学知识点应用领域与方法 ✓ 掌握高等数学 ✓ 掌握线性代数 ✓ 掌握概率论 ✓ 掌握统计分析方法 ✓ 掌握结合Python进行数学操作 课程内容 数学导论 数学导论概述 微积分基础（导数） 线性代数基础（矩阵） 概率&amp;统计基础（随机变量） 图论（图的概念） 微积分 函数 极限&amp;连续 导数 微分 链式法则 偏导数 梯度 积分 牛顿-莱布尼兹公式 泰勒展开 线性代数 线性方程组 行列式与克拉默法则 矩阵及其运算 神经网络中的矩阵/向量 矩阵的性质 矩阵与线性变换 线性变换的几何意义 特征值与特征向量 NumPy中矩阵的操作 概率&amp;统计 概率是什么 古典概型&amp;几何概型 条件概率&amp;联合概率 期望&amp;方差&amp;协方差 二项分布 高斯分布 中心极限定理 泊松分布 贝叶斯先验分布&amp;后验分布 机器学习分类指标 图论 图的由来 图的构成 图的表示 邻接矩阵 图的种类 最短路径问题 Dijkstra算法 树 最小生成树 图与人工智能 要求 有一定的数学基础学习起来更顺手 熟悉Python将更快上手进行统计分析 说明 本篇是系列《茶桁的AI秘籍》中的《基础数学篇》，旨在帮助同学们快速打下数学基础，通俗讲解其中每一个知识点。课程内容涉及高等数学，线性代数，概率论与统计学，同学们在学习过程中应当以理解为出发点并不需要死记每一个公式，快速掌握核心知识点。课程章节内容较多，零基础同学按顺序学习即可，有基础的同学们可以按照自己的需求来有选择的学习！ 此课程面向哪些人： 数据科学方向的同学们； 准备继续学习机器学习，深度学习等方向的同学； 准备面试及就业AI相关方向的同学 对此有需求的小伙伴，赶紧如下方式订阅起来： 扫码并关注微信号「坍缩的奇点」，然后搜索「数学篇」。","link":"/Math-Introduction/"}],"tags":[{"name":"产品经理","slug":"产品经理","link":"/tags/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"CV","slug":"CV","link":"/tags/CV/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"BI","slug":"BI","link":"/tags/BI/"},{"name":"LLM","slug":"LLM","link":"/tags/LLM/"},{"name":"Math","slug":"Math","link":"/tags/Math/"},{"name":"Neural Network","slug":"Neural-Network","link":"/tags/Neural-Network/"},{"name":"Chrome","slug":"Chrome","link":"/tags/Chrome/"},{"name":"ChatGPT","slug":"ChatGPT","link":"/tags/ChatGPT/"},{"name":"Mac","slug":"Mac","link":"/tags/Mac/"},{"name":"Stable Diffusion","slug":"Stable-Diffusion","link":"/tags/Stable-Diffusion/"},{"name":"Google","slug":"Google","link":"/tags/Google/"},{"name":"javascript","slug":"javascript","link":"/tags/javascript/"},{"name":"Gmail","slug":"Gmail","link":"/tags/Gmail/"},{"name":"Photoshop","slug":"Photoshop","link":"/tags/Photoshop/"},{"name":"Model","slug":"Model","link":"/tags/Model/"},{"name":"python","slug":"python","link":"/tags/python/"}],"categories":[{"name":"从零开始接触人工智能大模型","slug":"从零开始接触人工智能大模型","link":"/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"AI秘籍","slug":"AI秘籍","link":"/categories/AI%E7%A7%98%E7%B1%8D/"},{"name":"Python","slug":"AI秘籍/Python","link":"/categories/AI%E7%A7%98%E7%B1%8D/Python/"},{"name":"Math","slug":"AI秘籍/Math","link":"/categories/AI%E7%A7%98%E7%B1%8D/Math/"}],"pages":[]}