<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>31. 深度学习进阶 - 全连接层及网络结构 - 茶桁.MAMT</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="茶桁.MAMT"><meta name="msapplication-TileImage" content="https://qiniu.hivan.me/picGo/20230601174411.png?imgNote"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="茶桁.MAMT"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content=""><meta property="og:type" content="blog"><meta property="og:title" content="31. 深度学习进阶 - 全连接层及网络结构"><meta property="og:url" content="https://hivan.me/31.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%8F%8A%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"><meta property="og:site_name" content="茶桁.MAMT"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2023-12-19T23:30:00.000Z"><meta property="article:modified_time" content="2024-01-16T08:25:04.299Z"><meta property="article:author" content="Hivan Du"><meta property="article:tag" content="AI"><meta property="twitter:card" content="summary"><meta property="twitter:creator" content="@hivan"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hivan.me/31.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%8F%8A%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"},"headline":"31. 深度学习进阶 - 全连接层及网络结构","image":[],"datePublished":"2023-12-19T23:30:00.000Z","dateModified":"2024-01-16T08:25:04.299Z","author":{"@type":"Person","name":"Hivan Du"},"publisher":{"@type":"Organization","name":"茶桁.MAMT","logo":{"@type":"ImageObject","url":"https://hivan.me/img/logo.svg"}},"description":""}</script><link rel="canonical" href="https://hivan.me/31.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%8F%8A%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?f91b64734fdc7bfb999e48f9248d44dd";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-ZFB6CVWZFJ" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-ZFB6CVWZFJ');</script><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="茶桁.MAMT" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-12-19T23:30:00.000Z" title="12/20/2023, 7:30:00 AM">2023-12-20</time>发表</span><span class="level-item"><a class="link-muted" href="/categories/AI-%E7%A7%98%E7%B1%8D/">AI 秘籍</a><span> / </span><a class="link-muted" href="/categories/AI-%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/">核心能力基础</a></span></div></div><h1 class="title is-3 is-size-4-mobile">31. 深度学习进阶 - 全连接层及网络结构</h1><div class="content"><p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111936981.png"
alt="Alt text" /></p>
<span id="more"></span>
<p>Hi，你好。我是茶桁。</p>
<p>之前的课程咱们学习了卷积以及池化，那到底卷积是如何构成卷积神经网络的呢？我们这节课来好好讲一下。</p>
<h2 id="全连接层">全连接层</h2>
<p>整个卷积的运算就是经过卷积，再经过
pooling，再经过卷积。会把这个图形变的很小。然后再经过
pooling，又会一直把我们的特征变得越来越小，之后有一个很重要的层，这个层叫做全连接层。</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934379.png"
alt="Alt text" /></p>
<p>后面的几个柱状图就是它的线性变化，就是它的全连接层。</p>
<p>先是将图片卷积、池化变小，变成很小的高级特征，然后拉平之后进入全连接层进行线性变化。
这就是卷积操作的整个工作流，也是为什么卷积操作需要的参数少的原因。</p>
<p>我们在这里重点说一下全连接层。</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934380.png"
alt="Alt text" /></p>
<p>我们做了很多 pooling，
很多卷积之后，我们会生成一个很厚的一个值。把很厚的这个值给他拉平，在
PyTorch 里面直接就 flatten, 或者用 reshape 直接进行，把它拉平成一个 1
乘以 n 的一个向量。然后给这个 1 乘以 n 进行熟悉的<code>wx+b</code>。</p>
<p>我们对它进行线性变化，第一是对它的维度进行了变化。假如要给它变成一个
10 分类，纬度进行的变化。</p>
<p>另外一点，我们每一层都会有不同的特征点，这些特征点代表这图像不同的位置把它抽象成的值。然后一层一层的，又是不同的
filter
的结果，提取出来的不同的特征。比如横向，竖向之类的。机器可能还会自动提取一些颜色，形状等等。</p>
<p>那么现在我们要把这些东西进行一个综合考量，要把这些信息全部拿起来综合做个判断。比如我们有三个
filter, 也就是有三层，这三层里面拿出四个位置。 那么拉平的画，就变成 3
乘以 4，这里面有 12 个数值。这 12
个数值提取出来通过不同的方式了，关注点不同，提出来的 12 高级特征。</p>
<p>现在要把这 12
个高级特征全盘考虑、综合考虑。我们要给这些数据加一个不同的权重。就要给它做一个<code>wi * xi</code>，就给它这些全盘综合做了一个权重的这个赋值。</p>
<p>所以说，全连接不仅对维度进行了变化，它还对之前提取出来的局部信息进行了综合，这个就是全连接层的作用。既进行了变化又进行了维度信息的综合。</p>
<p>所以说，大家看一下</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934381.png"
alt="Alt text" /></p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934382.png"
alt="Alt text" /></p>
<p>这些不同的著名的网络结构，都是进行完之后要进行线性变化，线性变化之后把它变到我们期望的
target 上，就是最前面的这些东西进行综合。</p>
<p>算出来这个数值之后，然后用全连接层进行分类。但是全连接层不一定是只能进行分类，其实还进行特征的一个变化。</p>
<p>进行线性变化完了之后，通过 Softmax，然后再给它进行
cross-entropy，就可以求出它的 loss 值了。</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934383.png"
alt="Alt text" /></p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934384.png"
alt="Alt text" /></p>
<p>其实最近几年，就从 2019 年左右开始呢，其实大家慢慢的不用 Softmax 和
cross entropy 了，当然用这个也可以。为什么不用了呢？</p>
<p>比方我现在有三个图片，IMAGE1、IMAGE2 和 IMAGE3，对应的 label
分别是<code>3、5、6</code>。那么要做 cross-entropy 的时候，就要把 3
变成<code>[0,0,1,0,0,0,0,0]</code>，然后 5 和 6 都要进行变化。然后才能跟
Softmax 预测出来这个 probability 做
cross-entropy。也就是说，在这里要进行一次 one-hot
编码。结果后来就发现可以做一个简化操作，进行了 Softmax 之后给它前面加个
log。</p>
<p>假如说，Softmax 之后是<code>0.1, 0.3, 0.3, 0.2, 0.1</code>, 给它加个
log,就会是一个负的比较大的数字，越接近于 1，比方 0.99，越接近于 1
结果会越接近于 0，越远离 1，这个负的值会越大。</p>
<p>所以现在大家会有一个非线性变化，叫做 log
Softmax，出来的结果就是负的。然后还有一个 loss 叫做<code>NLLloss</code>,
<code>negative log likelihood loss</code>，这个在 PyTorch 里边也有。</p>
<p>这个有趣的地方就来了，如果我们它的 label 是 3，直接来看一下 log
之后的值是不是<code>-3</code>, 给它再取个负号，那么就直接说这个的 loss
是<code>3</code>。如果它的 label 是 5，那么 log
之后是另外一个值，假如说是<code>-0.7</code>，那么它取
5，我们发现结果是<code>-0.7</code>，加个负号，它的 loss
直接就是<code>0.7</code>。</p>
<p>这样就不需要进行 one-hot
编码了，而且也能达到一个效果，就是我们期望的地方越接近于 1，loss
越接近于 0。</p>
<p>所以，现在在工作中，我们看大量代码都开始这么做了，相当于是一个简化板的
Softmax。</p>
<p>那这个呢就是我们整个卷积神经网络的工作流程，全连接层的作用大家一定要知道。</p>
<p>好，我们做一个总结。第一节课，给大家讲解卷积的原理。那么什么是卷积神经网络呢？只要用了卷积(Conv)这个操作的网络,
它就叫卷积神经网络。所以理论上，你可以让一个图形先经过卷积，再经过
RNN，再经过卷积，再经过
RNN，都可以。这个你既可以叫它卷积网络，也可以叫它循环神经网络。</p>
<p>然后呢跟大家说了 CNN
可以用在很多地方，比方说分类，探测，还有分割，其实背后都是卷积神经网络在做。</p>
<p>还有给大家讲了 filters, padding, stride 和
channel，它的作用。除此之外，我们讲了 Parameters sharing 和 Location
Invariant。</p>
<p>在整个过程中，我们哪一层做卷积，哪一层做
pooling，线性变化做几层，是不是纯靠经验？说白了这个确实还是纯靠经验，所以有一个很重要的特点就是我们需要去借鉴，我们需要去借鉴前人的经验。</p>
<h2 id="几种神经网络结构">几种神经网络结构</h2>
<p>我们需要看前人的网络结构是怎么搭的，有几种比较重要的结构，LE-NET5，ALEX-NET。Alex
那个 net 结构就是 2012 年 ImageNet 取得第一名的，上面有图。</p>
<p>它的特点就是第一次用 Relu 去做了非线性变化,
作用就会进行的比较快，它还在 GPU 上进行运算。</p>
<p>Relu 就是一个非线性变化，如果把它做了卷积操作之后，给它再加个
Relu，可以把它值再进行一个非线性变化就可以了，就是把它卷积出来的结果做了一个非线性变化。</p>
<p>GPU 运算的作用是什么呢？GPU 为什么重要？</p>
<p>假设现在有一张 1 万 * 1 万的一张图，有 3 * 3
的卷积核，如果说原始的状态我们得先从左到右再从上到下的做。我们得进行 998
乘以 998 次移动。</p>
<p>有 GPU 的话，我们可以让其中一部分在 GPU
的某个地方进行计算，另外一部分同时在 GPU
的另外一个地方计算，就可以分布式的。因为 GPU
所做的事情就是把矩阵运算可以分布式的在不同的地方并行运算。</p>
<p>这就是为什么有 GPU 玩游戏不卡，因为加载图片的时候它一部分图片在 GPU
某个地方加载，另外一部分图片在 GPU
另外一个地方加载，这是同时一起加载的。</p>
<p>如果年龄在 30
岁以上的小伙伴应该知道，以前看网页的时候那个大的图片会一行一行显示出来，就
90 年代末那会儿，图片是一行一行一行显示出来的。而对于 GPU
的话，显示图片是一块一块一起去渲染的。</p>
<p>那么对于卷积神经网络来说，这一块一块的
filters，也是一起渲染一起计算的。所以说在做一层的计算的时候它就快了。而且如果你的
GPU 足够多，你还可以让它每一层的 filters 也并行计算。每一层的 filters
在每一块上又可以快速计算。</p>
<p>所以有了 GPU 的运行速度可以快十几倍，二十几倍，甚至上百倍都可以。</p>
<p>然后是 VGG-NET。</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934385.png"
alt="Alt text" /></p>
<p>VGG-NET
是第一个真正意义上的深度神经网络，我们看这张图，它门一层都向下做了一个下采样。不断的下采样的结果是可以获得一些非常深的
feature，或者一些非常高层次的 feature。</p>
<p>VGG 当时取得的效果也非常的好，也学的非常好。但是随着 VGG
正式的把我们带到深度神经网络这个过程中，我们就发现当网络特别深的时候会产生一个问题。</p>
<p>我们回忆一下，之前的课程中有讲过，当网络特别深的时候会产生什么问题？</p>
<p>我们之前课程里有说，当网络特别深的时候就会产生梯度消失。</p>
<p>首先做这个变化的时候它的体现倒不是说就是会梯度消失，而是和梯度消失很类似。就是这个图片在前面运行的特别长，如果这个
filter 有几个值比较小，那么值经过 filter 值会变得很小，再经过一个 filter
又会变得很小。</p>
<p>到最后，原来的图像区别还挺大的，经过几次卷积之后呢，就都变成了很小的一些数字，展示出来就近乎一张纯色的图片。</p>
<p>这个其实在哲学上也可以理解一下，当你的抽象层次特别特别高的时候，全世界的东西都一样。对吧，就很佛系，科学尽头是神学。当你的抽象层次极高的时候，你看全世界所有东西都一样，在
CNN 里也一样，当你的这个东西足够长的时候，最后得到的东西它都差不多。</p>
<p>所以为了解决这个问题，就提出来一个重要的神经网络叫做 RES-NET。</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934386.png"
alt="Alt text" /></p>
<p>这个叫做残差网络，这个残差网络是非常重要的，是微软亚研当年提出来的。</p>
<p>2015 年用了 RES-NET
造成了计算机视觉的识别率超过了人类眼睛的识别率，所以 2016 年是 AI
在产业中开始落地的第一年。</p>
<p>当然它的原理并不难，但是经过这样的一个修改，使得我们计算机识别网络的准确度超过了人类，然后开始了这个产业落地。</p>
<p>截图中是 RES-NET 的一个 Block。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResBlock</span>(nn.Module):<br>    ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        out = self.conv(x)<br>        out = self.batch_normal(out)<br>        out = torch.relu(out)<br><br>        <span class="hljs-keyword">return</span> out + x<br></code></pre></td></tr></table></figure>
<p>向前运算的时候输入 x，经过了卷积，之后再给它进行一个 Batch
normalization ，它的那个值就把小的变大，大的变小。然后再进行一个 Relu
非线性变化，输出的是 out 加了个 x。</p>
<p>这句话就是我们所谓的 Residual 的意思,就是理论上我们只要输出 out
就行了，但是为啥要加 x 呢？因为当经过很多层之后，out 可能会变成
0，变成一个纯色图片。所以把 x
加上，就是它还是保留了它的主要的图片信息，但是它在 out
上又有一些小的变化。这就是 RES-NET 的原理。</p>
<p>如果我们现在想做一个深的 RES-NET
的话怎么办？你给它输入一个三维的图片，比方说 32 个 filters。</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934387.png"
alt="Alt text" /></p>
<p>然后我们进行了一个 ResBlock:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NetResDeep</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">...</span>):<br>        ...<br>        self.resblocks = nn.Sequential(<br>            *[ResBlock(n_chans=n_chans) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_blocks)]<br>        )<br>        ...<br>    ...<br></code></pre></td></tr></table></figure>
<p>这个地方其实相当于是 ResBlock
之后，输出的<code>x+out</code>又给它输入到了一个
ResBlock，又是一个<code>x+out</code>。</p>
<p>我们这里<code>Sequential</code>的意思就是做完了这个，它的输出直接给下一个做输出。</p>
<p>在这个过程中，先让 x 进来做卷积、做非线性变化、做
pooling。然后把它送到一串 ResBlock:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NetResDeep</span>(...):<br>    ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        out = F.max_pool2d(torch.relu(self.conv(x)), <span class="hljs-number">2</span>)<br>        out = self.resblocks(out)<br>        out = F.max_pool2d(out, <span class="hljs-number">2</span>)<br>        out = out.view(-<span class="hljs-number">1</span>, <span class="hljs-number">8</span> * <span class="hljs-number">8</span> * self.n_chans)<br>        out = torch.relu(self.fc1(out))<br>        out = self.fc2(out)<br>        ...<br></code></pre></td></tr></table></figure>
<p>这一串 ResBlock, 它有很多个
ResBlock，一层一层运行下来。之后，做了一个 pooling,
之后做拉平，拉平之后在做一个全连接，就是对个权重进行线性变化，变化完了之后再加了非线性变化，最后再做一个线性变化。</p>
<p>这里的<code>fc2</code>，我们定义的维度是 10，意思就是把它要变成一个
10 分类的任务。</p>
<p>然后我们再给它做个 log Softmax，或者说 cross-entropy，或者是
NLL，就可以给它进行反向传播了。</p>
<p>这整个过程就是咱们的 RES-NET。</p>
<p>只要这个网络有 ResBlock，或者类似于 ResBlock 的，它都叫
RES-NET。就像只要有卷积这个单元的网络都叫卷积网络一样。</p>
<p>这句话的意思是说，RES-NET
其实有很多种。比方下面这张图，就是一个非常著名的 RES-NET。</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934388.png"
alt="Alt text" /></p>
<p>咱们刚才写的那个 ResBlock 是最简单化的 ResBlock, 这个 ResBlock 是 x
进来之后，首先有一个卷积，卷积之后又给它进行了一个 Batch
normalization，normalization 之后又进行了一个 Relu，然后又进行了一个
drop out，再之后再给它进行一个 Relu，然后再 sum，加上 x。</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934389.png"
alt="Alt text" /></p>
<p>这个是刚才我们写的 ResBlock 的一个更复杂的版本。这个网络结构是
RES-NET 的一个经典结构。</p>
<p>RES-NET
的经典结构一共有这么几种：ResNet-18、ResNet-34、ResNet-50、ResNet-101、ResNet-152
几种。ResNet-18 和 ResNet-34 的基本结构相同，属于相对浅层的网络，后面 3
种的基本结构不同于 ResNet-18 和 ResNet-34，属于更深层的网络。</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934390.png"
alt="Alt text" /></p>
<p>感兴趣的可以去看看这篇论文：<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a>。</p>
<p>这五种结构都可以实现，但是它们的具体实现方法不一样。</p>
<p>RES-NET
内理论上全部是卷积，没有全链接。全连接的部分其实是放在外边的。</p>
<p>RES-NET
它的实现过程含有一点工程上的东西，如果是想要做计算机视觉的小伙伴，就需要想起的去学习一下这个部分。之后我会有专门讲
CV 的部分，会更详细的讲解。</p>
<p>然后我们再来了解一个 Inception model，
直译的话称之为「初创模型」，一般大家都把它叫做 inception。它是 Google 在
RES-NET 提出来之后提出来的一个神经网络。</p>
<p>Google 的神经网络提出来的这个 Inception
机制有一个很很奇怪的点，就是它提出来了一个操作叫做<code>1*1 convolutional</code>,
意思就是我们把之前的卷积操作的那个<code>kernel_size</code>变成了
1*1，就是变成一个点点。变成一个点点之后再加了一个非线性变化。</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934391.png"
alt="Alt text" /></p>
<p>这个权重也是刚开始随机的，后来是学习出来的。</p>
<p>它相当于是把整个前面的图形，整体每个数字乘了一个数，然后再给它进行了一个非线性变化。</p>
<p>也就是说，如图 <code>1 * 1</code>的位置是
5，相当于把前面矩阵内所有的数字都乘了个
5，然后再进行了一个非线性变化。</p>
<p>假如现在有一个<code>8 * 8</code>的照片, 包含 RGB
就是<code>8 * 8 * 3</code>，现在有 5
个<code>1 * 1</code>的卷积核，那么得出的结果应该是多少？</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934392.png"
alt="Alt text" /></p>
<p>如图，也就是说，如果 A 为 5， 那么 B、C、D
应该等于多少？分别应该是<code>8 * 8 * 5</code>。</p>
<p>所以它其实起到了什么作用？首先我们知道了第一个功能就是改变通道数。</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934393.png"
alt="Alt text" /></p>
<p>改变了通道数之后，如果是<code>28 * 28 * 3</code>，这个 Inception
的机制是对每一层的输入要用多个不同的 kernel size
的卷积做操作，做完之后把这些值拼起来，把它再作为下一层的输出。</p>
<p>这个时候 padding
就很有用了，保证了值都是<code>28 * 28</code>，就可以连起来了，否则还要做各种
reshap 就很麻烦。</p>
<p>Inception 第一个操作是它有一个 1 *
1，什么都没干但它改变了通道数，第二个就是它使用了多个 kernel size
给一层做卷积，之后把它的结果全部连起来。</p>
<p>那么把所有连起来它会产生这样一个结果</p>
<p><img
src="https://raw.githubusercontent.com/hivandu/notes/main/img/202311111934395.png"
alt="Alt text" /></p>
<p>刚刚说了，inception 里面会把多个 kernel size
出来的结果连起来，因为有很多个 Kernel
size，这个结果就很长，我们希望把它变短，就可以用<code>1 * 1</code>的这个操作给它变短。</p>
<p>用了 16 个<code>1 * 1</code>的操作，就可以把这个 256 层的 channel
变成 16 层的 channel，变成 16
层之后再用<code>5 * 5</code>的卷积核去，得到了一个<code>28 * 28 * 32</code>的
channel。</p>
<p>而如果直接用<code>5 * 5</code>再 patting
的话也可以得到一个<code>28 * 28</code>的
channel，但是这两个是有区别的。如果直接用，就是<code>28 * 28</code>个
channel，有 32 个。那么所需要拟合的参数就是 160 million。</p>
<p>参数之所以大是因为连在一起的值特别大，特别的深。现在如果想把它变浅的话参数就少了。这个地方叫做
Bottleneck Network, 称为瓶颈网络。就是将之前连在一起而特别长的这个
channel 给它变得特别短，然后在这个短的 channel
上再做计算，所消耗的参数算下来就只有 13 million。</p>
<p>所以<code>1 * 1</code>的操作其实就是因为有了 inception
这种机制，所以会产生出特别长的结果。如果现在要对特别长的这个结果进行卷积的话，会需要的参数特别的多，而我们可以通过<code>1 * 1</code>的操作把它变短，之后再进行卷积操作，它的权重就少多了。这个就是这个
inseption 机制。</p>
<p>所以 Inseption 还是一样的道理，减少了参数的量，减少了 parameters
的数量，又降低了模型的复杂度，降低了过拟合，加快了计算速度。</p>
<p>那么我们卷积神经网络基本上到这里就给大家讲完了。关于更多卷积神经的应用后面会讲到专门的
CV 方面。</p>
<p>在这之前，接下来会用几节课分别讲解一下 CV、BI 和 NLP
的一些基础，给大家热热场子。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>31. 深度学习进阶 - 全连接层及网络结构</p><p><a href="https://hivan.me/31. 深度学习进阶 - 全连接层及网络结构/">https://hivan.me/31. 深度学习进阶 - 全连接层及网络结构/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hivan Du</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2023-12-20</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2024-01-16</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/AI/">AI</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=6479444288ae9600196fa98e&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://qiniu.hivan.me/picGo/20230601221633.jpeg" alt="支付宝"></span></a><a class="button donate" href="https://patreon.com/user?u=89473430" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://qiniu.hivan.me/IMG_4603.JPG" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/32.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20Transfer%20Learning/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">32. 深度学习进阶 - Transfer Learning</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/30.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E6%B1%A0%E5%8C%96/"><span class="level-item">30. 深度学习进阶 - 池化</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://hivan.me/31.%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6%20-%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%8F%8A%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/';
            this.page.identifier = '31. 深度学习进阶 - 全连接层及网络结构/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'hivan' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://www.gravatar.com/avatar/bdff168cf8a71c11d2712a1679a00c54?s=128" alt="茶桁"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">茶桁</p><p class="is-size-6 is-block">AI 游民</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shang Hai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">223</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">22</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hivandu" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hivan"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/hivan"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com/hivan"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4NzE4MDQzMg==&amp;action=getalbum&amp;album_id=2932504849574543360&amp;scene=173&amp;from_msgid=2648747980&amp;from_itemidx=1&amp;count=3&amp;nolastread=1&amp;token=1758883909&amp;lang=zh_CN#wechat_redirect"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.zhihu.com/column/c_1424326166602178560" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">塌缩的奇点</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li><li><a class="level is-mobile" href="https://www.zhihu.com/column/hivandu" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">茶桁-知乎</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI-%E7%A7%98%E7%B1%8D/"><span class="level-start"><span class="level-item">AI 秘籍</span></span><span class="level-end"><span class="level-item tag">81</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI-%E7%A7%98%E7%B1%8D/BI/"><span class="level-start"><span class="level-item">BI</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/AI-%E7%A7%98%E7%B1%8D/Math/"><span class="level-start"><span class="level-item">Math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/AI-%E7%A7%98%E7%B1%8D/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li><li><a class="level is-mobile" href="/categories/AI-%E7%A7%98%E7%B1%8D/%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%E5%9F%BA%E7%A1%80/"><span class="level-start"><span class="level-item">核心能力基础</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="level-start"><span class="level-item">从零开始接触人工智能大模型</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-02-27T23:30:00.000Z">2024-02-28</time></p><p class="title"><a href="/17.%20BI%20-%20Surprise%E5%B7%A5%E5%85%B7%E7%AE%B1%EF%BC%9ABaseline/">17. BI - Surprise 工具箱：Baseline</a></p><p class="categories"><a href="/categories/AI-%E7%A7%98%E7%B1%8D/">AI 秘籍</a> / <a href="/categories/AI-%E7%A7%98%E7%B1%8D/BI/">BI</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-02-24T23:30:00.000Z">2024-02-25</time></p><p class="title"><a href="/16.%20BI%20-%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B9%8BALS%E5%AE%9E%E7%8E%B0/">16. BI - 推荐系统之 ALS 实现</a></p><p class="categories"><a href="/categories/AI-%E7%A7%98%E7%B1%8D/">AI 秘籍</a> / <a href="/categories/AI-%E7%A7%98%E7%B1%8D/BI/">BI</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-02-20T23:30:00.000Z">2024-02-21</time></p><p class="title"><a href="/15.%20BI%20-%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B9%8BALS%E5%8E%9F%E7%90%86/">15. BI - 推荐系统之 ALS 原理</a></p><p class="categories"><a href="/categories/AI-%E7%A7%98%E7%B1%8D/">AI 秘籍</a> / <a href="/categories/AI-%E7%A7%98%E7%B1%8D/BI/">BI</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-02-15T23:30:00.000Z">2024-02-16</time></p><p class="title"><a href="/14.%20BI%20-%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/">14. BI - 推荐系统之矩阵分解</a></p><p class="categories"><a href="/categories/AI-%E7%A7%98%E7%B1%8D/">AI 秘籍</a> / <a href="/categories/AI-%E7%A7%98%E7%B1%8D/BI/">BI</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-02-10T23:30:00.000Z">2024-02-11</time></p><p class="title"><a href="/13.%20BI%20-%20%E5%8F%AF%E8%A7%86%E5%8C%96%E7%9C%8B%E6%9D%BF%E5%8F%91%E5%B8%83/">13. BI - 可视化看板发布</a></p><p class="categories"><a href="/categories/AI-%E7%A7%98%E7%B1%8D/">AI 秘籍</a> / <a href="/categories/AI-%E7%A7%98%E7%B1%8D/BI/">BI</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a><p class="is-size-7"><span>&copy; 2024 Hivan Du</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>