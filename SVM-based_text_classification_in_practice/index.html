<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>SVM-based Text Classification in Practice - 茶桁.MAMT</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="茶桁.MAMT"><meta name="msapplication-TileImage" content="https://qiniu.hivan.me/picGo/20230601174411.png?imgNote"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="茶桁.MAMT"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="The source code: SVM-based Text Classification in Practice   &amp;#39;cnews.train.txt&amp;#39; data cannot be uploaded because it is too large, so it needs to be decompressed and imported after compression.  Use SVM"><meta property="og:type" content="blog"><meta property="og:title" content="SVM-based Text Classification in Practice"><meta property="og:url" content="https://hivan.me/SVM-based_text_classification_in_practice/"><meta property="og:site_name" content="茶桁.MAMT"><meta property="og:description" content="The source code: SVM-based Text Classification in Practice   &amp;#39;cnews.train.txt&amp;#39; data cannot be uploaded because it is too large, so it needs to be decompressed and imported after compression.  Use SVM"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hivan.me/img/og_image.png"><meta property="article:published_time" content="2021-08-31T14:21:07.776Z"><meta property="article:modified_time" content="2023-06-02T03:26:49.841Z"><meta property="article:author" content="Hivan Du"><meta property="article:tag" content="AI,人工智能,代码,大语言模型"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://hivan.me/img/og_image.png"><meta property="twitter:creator" content="@hivan"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hivan.me/SVM-based_text_classification_in_practice/"},"headline":"SVM-based Text Classification in Practice","image":["https://hivan.me/img/og_image.png"],"datePublished":"2021-08-31T14:21:07.776Z","dateModified":"2023-06-02T03:26:49.841Z","author":{"@type":"Person","name":"Hivan Du"},"publisher":{"@type":"Organization","name":"茶桁.MAMT","logo":{"@type":"ImageObject","url":"https://hivan.me/img/logo.svg"}},"description":"The source code: SVM-based Text Classification in Practice   &#39;cnews.train.txt&#39; data cannot be uploaded because it is too large, so it needs to be decompressed and imported after compression.  Use SVM"}</script><link rel="canonical" href="https://hivan.me/SVM-based_text_classification_in_practice/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="茶桁.MAMT" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-08-31T14:21:07.776Z" title="8/31/2021, 10:21:07 PM">2021-08-31</time>发表</span></div></div><h1 class="title is-3 is-size-4-mobile">SVM-based Text Classification in Practice</h1><div class="content"><blockquote>
<p>The source code: <a
target="_blank" rel="noopener" href="https://github.com/hivandu/Colab/blob/master/AI_data/SVM-based%20text%20classification%20in%20practice.ipynb">SVM-based
Text Classification in Practice</a></p>
</blockquote>
<blockquote>
<p>'cnews.train.txt' data cannot be uploaded because it is too large, so
it needs to be decompressed and imported after compression.</p>
</blockquote>
<p>Use SVM to implement a simple text classification based on bag of
words and support vector machine.</p>
<h2 id="import-data">import data</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># import</span><br><span class="hljs-keyword">import</span> codecs<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> jieba<br></code></pre></td></tr></table></figure>
<p>Chinese news data is prepared as a sample data set. The number of
training data is 50,000 and the number of test data is 10,000. All data
is divided into 10 categories: sports, finance, real estate, home
furnishing, education, technology, fashion, current affairs, games and
entertainment . From the training text, you can load the code, view the
data format and samples:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><br>data_train = <span class="hljs-string">&#x27;./data/cnews.train.txt&#x27;</span> <span class="hljs-comment"># training data file name  </span><br>data_test = <span class="hljs-string">&#x27;./data/cnews.test.txt&#x27;</span>  <span class="hljs-comment"># test data file name</span><br>vocab = <span class="hljs-string">&#x27;./data/cnews.vocab.txt&#x27;</span> <span class="hljs-comment"># dictionary</span><br><br><span class="hljs-keyword">with</span> codecs.<span class="hljs-built_in">open</span>(data_train, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    lines = f.readlines()<br><br><span class="hljs-comment"># print sample content</span><br>label, content = lines[<span class="hljs-number">0</span>].strip(<span class="hljs-string">&#x27;\r\n&#x27;</span>).split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>content<br></code></pre></td></tr></table></figure>
<p>Take the first item of the training data as an example to segment the
loaded news data. Here I use the word segmentation function of LTP, you
can also use jieba, and the segmentation results are displayed separated
by "/" symbols.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># print word segment results</span><br>segment = jieba.cut(content)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;/&#x27;</span>.join(segment))<br></code></pre></td></tr></table></figure>
<p>To sort out the above logic a bit, implement a class to load training
and test data and perform word segmentation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># cut data</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_line</span>(<span class="hljs-params">idx, line</span>):<br>    data = <span class="hljs-built_in">tuple</span>(line.strip(<span class="hljs-string">&#x27;\r\n&#x27;</span>).split(<span class="hljs-string">&#x27;\t&#x27;</span>))<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">len</span>(data)==<span class="hljs-number">2</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>    content_segged = <span class="hljs-built_in">list</span>(jieba.cut(data[<span class="hljs-number">1</span>]))<br>    <span class="hljs-keyword">if</span> idx % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;line number: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(idx))<br>    <span class="hljs-keyword">return</span> (data[<span class="hljs-number">0</span>], content_segged)<br>    <br><span class="hljs-comment"># data loading method</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">file</span>):<br>    <span class="hljs-keyword">with</span> codecs.<span class="hljs-built_in">open</span>(file, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        lines = f.readlines()<br>    data_records = [process_line(idx, line) <span class="hljs-keyword">for</span> idx, line <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(lines)]<br>    data_records = [data <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> data_records <span class="hljs-keyword">if</span> data <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>]<br>    <span class="hljs-keyword">return</span> data_records<br><br><span class="hljs-comment"># load and process training data</span><br>train_data = load_data(data_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;first training data: label &#123;&#125; segment &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(train_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], <span class="hljs-string">&#x27;/&#x27;</span>.join(train_data[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>])))<br><span class="hljs-comment"># load and process testing data</span><br>test_data = load_data(data_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;first testing data: label &#123;&#125; segment &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(test_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], <span class="hljs-string">&#x27;/&#x27;</span>.join(test_data[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>])))<br></code></pre></td></tr></table></figure>
<p>After spending some time on word segmentation, you can start building
a dictionary. The dictionary is built from the training set and sorted
by word frequency.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_vocab</span>(<span class="hljs-params">train_data, thresh</span>):<br>    vocab = &#123;<span class="hljs-string">&#x27;&lt;UNK&gt;&#x27;</span>: <span class="hljs-number">0</span>&#125;<br>    word_count = &#123;&#125; <span class="hljs-comment"># word frequency</span><br>    <span class="hljs-keyword">for</span> idx, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_data):<br>        content = data[<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> content:<br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> word_count:<br>                word_count[word] += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                word_count[word] = <span class="hljs-number">1</span><br>    word_list = [(k, v) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> word_count.items()]<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;word list length: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(word_list)))<br>    word_list.sort(key = <span class="hljs-keyword">lambda</span> x : x[<span class="hljs-number">1</span>], reverse = <span class="hljs-literal">True</span>) <span class="hljs-comment"># sorted by word frequency</span><br>    word_list_filtered = [word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_list <span class="hljs-keyword">if</span> word[<span class="hljs-number">1</span>] &gt; thresh]<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;word list length after filtering: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(word_list_filtered)))<br>    <span class="hljs-comment"># construct vocab</span><br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_list_filtered:<br>        vocab[word[<span class="hljs-number">0</span>]] = <span class="hljs-built_in">len</span>(vocab)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;vocab size: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(vocab))) <span class="hljs-comment"># vocab size is word list size +1 due to unk token</span><br>    <span class="hljs-keyword">return</span> vocab<br><br>vocab = build_vocab(train_data, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>
<p>In addition, according to category, we know that the label itself
also has a "dictionary":</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_label_vocab</span>(<span class="hljs-params">cate_file</span>):<br>    label_vocab = &#123;&#125;<br>    <span class="hljs-keyword">with</span> codecs.<span class="hljs-built_in">open</span>(cate_file, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">for</span> lines <span class="hljs-keyword">in</span> f:<br>            line = lines.strip().split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>            label_vocab[line[<span class="hljs-number">0</span>]]  = <span class="hljs-built_in">int</span>(line[<span class="hljs-number">1</span>])<br>    <span class="hljs-keyword">return</span> label_vocab<br><br>label_vocab = build_label_vocab(<span class="hljs-string">&#x27;./data/cnews.category.txt&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;label vocab: <span class="hljs-subst">&#123;label_vocab&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p>Next, construct the id-based training and test sets, because we only
consider the bag of words, so the order of words is excluded.
Constructed to look like libsvm can eat. Note that because the bag of
word model</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">construct_trainable_matrix</span>(<span class="hljs-params">corpus, vocab, label_vocab, out_file</span>):<br>    records = []<br>    <span class="hljs-keyword">for</span> idx, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(corpus):<br>        <span class="hljs-keyword">if</span> idx % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;process &#123;&#125; data&#x27;</span>.<span class="hljs-built_in">format</span>(idx))<br>        label = <span class="hljs-built_in">str</span>(label_vocab[data[<span class="hljs-number">0</span>]]) <span class="hljs-comment"># label id</span><br>        token_dict = &#123;&#125;<br>        <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> data[<span class="hljs-number">1</span>]:<br>            token_id = vocab.get(token, <span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">if</span> token_id <span class="hljs-keyword">in</span> token_dict:<br>                token_dict[token_id] += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                token_dict[token_id] = <span class="hljs-number">1</span><br>        feature = [<span class="hljs-built_in">str</span>(<span class="hljs-built_in">int</span>(k) + <span class="hljs-number">1</span>) + <span class="hljs-string">&#x27;:&#x27;</span> + <span class="hljs-built_in">str</span>(v) <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> token_dict.items()]<br>        feature_text = <span class="hljs-string">&#x27; &#x27;</span>.join(feature)<br>        records.append(label + <span class="hljs-string">&#x27; &#x27;</span> + feature_text)<br>    <br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(out_file, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        f.write(<span class="hljs-string">&#x27;\n&#x27;</span>.join(records))<br><br>construct_trainable_matrix(train_data, vocab, label_vocab, <span class="hljs-string">&#x27;./data/train.svm.txt&#x27;</span>)<br>construct_trainable_matrix(test_data, vocab, label_vocab, <span class="hljs-string">&#x27;./data/test.svm.txt&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h2 id="training-process">Training process</h2>
<p>The remaining core model is simple: use libsvm to train the support
vector machine, let your svm eat the training and test files you have
processed, and then use the existing method of libsvm to train, we can
change different parameter settings . The documentation of libsvm can be
viewed <a target="_blank" rel="noopener" href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">here</a>,
where the "-s, -t, -c" parameters are more important, and they decide
what you choose Svm, your choice of kernel function, and your penalty
coefficient.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> libsvm <span class="hljs-keyword">import</span> svm<br><span class="hljs-keyword">from</span> libsvm.svmutil <span class="hljs-keyword">import</span> svm_read_problem,svm_train,svm_predict,svm_save_model,svm_load_model<br><br><span class="hljs-comment"># train svm</span><br>train_label, train_feature = svm_read_problem(<span class="hljs-string">&#x27;./data/train.svm.txt&#x27;</span>)<br><span class="hljs-built_in">print</span>(train_label[<span class="hljs-number">0</span>], train_feature[<span class="hljs-number">0</span>])<br>model=svm_train(train_label,train_feature,<span class="hljs-string">&#x27;-s 0 -c 5 -t 0 -g 0.5 -e 0.1&#x27;</span>)<br><br><span class="hljs-comment"># predict</span><br>test_label, test_feature = svm_read_problem(<span class="hljs-string">&#x27;./data/test.svm.txt&#x27;</span>)<br><span class="hljs-built_in">print</span>(test_label[<span class="hljs-number">0</span>], test_feature[<span class="hljs-number">0</span>])<br>p_labs, p_acc, p_vals = svm_predict(test_label, test_feature, model)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;accuracy: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(p_acc))<br></code></pre></td></tr></table></figure>
<p>After a period of training, we can observe the experimental results.
You can change different svm types, penalty coefficients, and kernel
functions to optimize the results.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>SVM-based Text Classification in Practice</p><p><a href="https://hivan.me/SVM-based_text_classification_in_practice/">https://hivan.me/SVM-based_text_classification_in_practice/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hivan Du</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-08-31</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2023-06-02</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=6479444288ae9600196fa98e&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="https://afdian.net/item/72907364008511ee904852540025c377" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://qiniu.hivan.me/picGo/20230601221633.jpeg" alt="支付宝"></span></a><a class="button donate" href="https://www.buymeacoffee.com/hivandu" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="https://patreon.com/user?u=89473430" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="doo@hivan.me"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://qiniu.hivan.me/IMG_4603.JPG" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/Lecture_1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Foundation of Artificial Intelligence - Lecture 1</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/auto_operation_weibo/"><span class="level-item">Auto operation Weibo</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://hivan.me/SVM-based_text_classification_in_practice/';
            this.page.identifier = 'SVM-based_text_classification_in_practice/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'hivan' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://www.gravatar.com/avatar/bdff168cf8a71c11d2712a1679a00c54?s=128" alt="茶桁"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">茶桁</p><p class="is-size-6 is-block">AI游民</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shang Hai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">170</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">21</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hivandu" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hivandu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hivan"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/hivan"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com/hivan"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4NzE4MDQzMg==&amp;action=getalbum&amp;album_id=2932504849574543360&amp;scene=173&amp;from_msgid=2648747980&amp;from_itemidx=1&amp;count=3&amp;nolastread=1&amp;token=1758883909&amp;lang=zh_CN#wechat_redirect"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.zhihu.com/column/c_1424326166602178560" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">塌缩的奇点</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li><li><a class="level is-mobile" href="https://www.zhihu.com/column/hivandu" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">茶桁-知乎</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI%E7%A7%98%E7%B1%8D/"><span class="level-start"><span class="level-item">AI秘籍</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI%E7%A7%98%E7%B1%8D/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%8E%A5%E8%A7%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="level-start"><span class="level-item">从零开始接触人工智能大模型</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-08T13:26:16.000Z">2023-08-08</time></p><p class="title"><a href="/file-operations/">13. Python的文件操作</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Python/">Python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-08T04:59:49.000Z">2023-08-08</time></p><p class="title"><a href="/Detailed-of-set/">12. 数据类型 - 集合详解</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Python/">Python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-07T11:40:50.000Z">2023-08-07</time></p><p class="title"><a href="/Detailed-of-dictonary/">11. 数据类型 - 字典</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Python/">Python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-07T03:44:38.000Z">2023-08-07</time></p><p class="title"><a href="/Detailed-of-tuple/">10. 数据类型 - 元组详解</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Python/">Python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-06T06:45:03.000Z">2023-08-06</time></p><p class="title"><a href="/Detailed-of-list/">9. 数据类型 - 列表详解</a></p><p class="categories"><a href="/categories/AI%E7%A7%98%E7%B1%8D/">AI秘籍</a> / <a href="/categories/AI%E7%A7%98%E7%B1%8D/Python/">Python</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="茶桁.MAMT" height="28"></a><p class="is-size-7"><span>&copy; 2023 Hivan Du</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/hivandu"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>